<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:41:55Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|71001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00644</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00644</id><created>2015-01-04</created><authors><author><keyname>Alsheikh</keyname><forenames>Mohammad Abu</forenames></author><author><keyname>Hoang</keyname><forenames>Dinh Thai</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Tan</keyname><forenames>Hwee-Pink</forenames></author><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author></authors><title>Markov Decision Processes with Applications in Wireless Sensor Networks:
  A Survey</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) consist of autonomous and resource-limited
devices. The devices cooperate to monitor one or more physical phenomena within
an area of interest. WSNs operate as stochastic systems because of randomness
in the monitored environments. For long service time and low maintenance cost,
WSNs require adaptive and robust methods to address data exchange, topology
formulation, resource and power optimization, sensing coverage and object
detection, and security challenges. In these problems, sensor nodes are to make
optimized decisions from a set of accessible strategies to achieve design
goals. This survey reviews numerous applications of the Markov decision process
(MDP) framework, a powerful decision-making tool to develop adaptive algorithms
and protocols for WSNs. Furthermore, various solution methods are discussed and
compared to serve as a guide for using MDPs in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00653</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00653</id><created>2015-01-04</created><authors><author><keyname>Biswas</keyname><forenames>Souham</forenames></author><author><keyname>Nene</keyname><forenames>Manisha J.</forenames></author></authors><title>Hostile Intent Identification by Movement Pattern Analysis: Using
  Artificial Neural Networks</title><categories>cs.AI cs.NE</categories><comments>To appear in IEEE Xplore as a part of the proceedings of the 3rd IEEE
  International Conference on Parallel, Distributed and Grid Computing, 2014,
  at Jaypee University of Information Technology, Solan</comments><doi>10.13140/2.1.4429.7281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, the problem of identifying suspicious behavior has
gained importance and identifying this behavior using computational systems and
autonomous algorithms is highly desirable in a tactical scenario. So far, the
solutions have been primarily manual which elicit human observation of entities
to discern the hostility of the situation. To cater to this problem statement,
a number of fully automated and partially automated solutions exist. But, these
solutions lack the capability of learning from experiences and work in
conjunction with human supervision which is extremely prone to error. In this
paper, a generalized methodology to predict the hostility of a given object
based on its movement patterns is proposed which has the ability to learn and
is based upon the mechanism of humans of learning from experiences. The
methodology so proposed has been implemented in a computer simulation. The
results show that the posited methodology has the potential to be applied in
real world tactical scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00656</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00656</id><created>2015-01-04</created><authors><author><keyname>Bourl&#xe8;s</keyname><forenames>Henri</forenames></author></authors><title>Algebraic Analysis Applied to the Theory of Linear Dynamical Systems</title><categories>math.OC cs.SY</categories><comments>in French</comments><msc-class>93B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expression &quot;Algebraic Analysis&quot; was coined by Mikio Sato. It consists of
using algebraic notions to solve analytic problem. The origin of Algebraic
Analysis is Algebraic Geometry as was developed by Alexander Grothendieck and
his school. Mimicking the introduction of Grothendieck's EGA (changing only a
few words) one obtains a good definition of the modern theory of linear
dynamical systems, as developed by Michel Fliess, Ian Willems, Ulrich Oberst
and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00657</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00657</id><created>2015-01-04</created><updated>2015-03-22</updated><authors><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author></authors><title>Cross-language Wikipedia Editing of Okinawa, Japan</title><categories>cs.CY cs.CL cs.SI</categories><comments>In Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems, CHI 2015. ACM</comments><acm-class>H.5.4, H.5.3</acm-class><doi>10.1145/2702123.2702346</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article analyzes users who edit Wikipedia articles about Okinawa, Japan,
in English and Japanese. It finds these users are among the most active and
dedicated users in their primary languages, where they make many large,
high-quality edits. However, when these users edit in their non-primary
languages, they tend to make edits of a different type that are overall smaller
in size and more often restricted to the narrow set of articles that exist in
both languages. Design changes to motivate wider contributions from users in
their non-primary languages and to encourage multilingual users to transfer
more information across language divides are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00665</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00665</id><created>2015-01-04</created><authors><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author><author><keyname>Sarrabezolles</keyname><forenames>Pauline</forenames></author></authors><title>Huge Unimodular N-Fold Programs</title><categories>math.OC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><journal-ref>SIAM Journal on Discrete Mathematics, 29:2277-2283, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization over $l\times m\times n$ integer $3$-way tables with given
line-sums is NP-hard already for fixed $l=3$, but is polynomial time solvable
with both $l,m$ fixed. In the {\em huge} version of the problem, the variable
dimension $n$ is encoded in {\em binary}, with $t$ {\em layer types}. It was
recently shown that the huge problem can be solved in polynomial time for fixed
$t$, and the complexity of the problem for variable $t$ was raised as an open
problem. Here we solve this problem and show that the huge table problem can be
solved in polynomial time even when the number $t$ of types is {\em variable}.
The complexity of the problem over $4$-way tables with variable $t$ remains
open. Our treatment goes through the more general class of {\em huge $n$-fold
integer programming problems}. We show that huge integer programs over $n$-fold
products of totally unimodular matrices can be solved in polynomial time even
when the number $t$ of brick types is variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00666</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00666</id><created>2015-01-04</created><authors><author><keyname>Lukyanchikov</keyname><forenames>Oleg</forenames></author><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Payain</keyname><forenames>Simon</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author></authors><title>Using Object-Relational Mapping to Create the Distributed Databases in a
  Hybrid Cloud Infrastructure</title><categories>cs.DB cs.DC cs.NI</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications, 2014, 5(12):61-64</journal-ref><doi>10.14569/IJACSA.2014.051208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the challenges currently problems in the use of cloud services is the
task of designing of specialized data management systems. This is especially
important for hybrid systems in which the data are located in public and
private clouds. Implementation monitoring functions querying, scheduling and
processing software must be properly implemented and is an integral part of the
system. To provide these functions is proposed to use an object-relational
mapping (ORM). The article devoted to presenting the approach of designing
databases for information systems hosted in a hybrid cloud infrastructure. It
also provides an example of the development of ORM library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00669</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00669</id><created>2015-01-04</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author></authors><title>Asynchronous Programming in a Prioritized Form</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous programming has appeared as a programming style that overcomes
undesired properties of concurrent programming. Typically in asynchronous
models of programming, methods are posted into a post list for latter
execution. The order of method executions is serial, but nondeterministic. This
paper presents a new and simple, yet powerful, model for asynchronous
programming. The proposed model consists of two components; a context-free
grammar and an operational semantics. The model is supported by the ability to
express important applications. An advantage of our model over related work is
that the model simplifies the way posted methods are assigned priorities.
Another advantage is that the operational semantics uses the simple concept of
singly linked list to simulate the prioritized process of methods posting and
execution. The simplicity and expressiveness make it relatively easy for
analysis algorithms to disclose the otherwise un-captured programming bugs in
asynchronous programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00671</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00671</id><created>2015-01-04</created><authors><author><keyname>Arvind</keyname><forenames>V.</forenames></author><author><keyname>Joglekar</keyname><forenames>Pushkar S</forenames></author><author><keyname>Rattan</keyname><forenames>Gaurav</forenames></author></authors><title>On the Complexity of Noncommutative Polynomial Factorization</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the complexity of factorization of polynomials in the
free noncommutative ring $\mathbb{F}\langle x_1,x_2,\dots,x_n\rangle$ of
polynomials over the field $\mathbb{F}$ and noncommuting variables
$x_1,x_2,\ldots,x_n$. Our main results are the following.
  Although $\mathbb{F}\langle x_1,x_2,\dots,x_n \rangle$ is not a unique
factorization ring, we note that variable-disjoint factorization in
$\mathbb{F}\langle x_1,x_2,\dots,x_n \rangle$ has the uniqueness property.
Furthermore, we prove that computing the variable-disjoint factorization is
polynomial-time equivalent to Polynomial Identity Testing (both when the input
polynomial is given by an arithmetic circuit or an algebraic branching
program). We also show that variable-disjoint factorization in the black-box
setting can be efficiently computed (where the factors computed will be also
given by black-boxes, analogous to the work [KT91] in the commutative setting).
  As a consequence of the previous result we show that homogeneous
noncommutative polynomials and multilinear noncommutative polynomials have
unique factorizations in the usual sense, which can be efficiently computed.
  Finally, we discuss a polynomial decomposition problem in $\mathbb{F}\langle
x_1,x_2,\dots,x_n\rangle$ which is a natural generalization of homogeneous
polynomial factorization and prove some complexity bounds for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00676</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00676</id><created>2015-01-04</created><authors><author><keyname>Anantharam</keyname><forenames>Venkatachalam</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek Shripad</forenames></author></authors><title>A variational formula for risk-sensitive reward</title><categories>math.OC cs.IT math.IT math.PR</categories><comments>35 pages</comments><msc-class>93E20, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a variational formula for the optimal growth rate of reward in the
infinite horizon risk-sensitive control problem for discrete time Markov
decision processes with compact metric state and action spaces, extending a
formula of Donsker and Varadhan for the Perron-Frobenius eigenvalue of a
positive operator. This leads to a concave maximization formulation of the
problem of determining this optimal growth rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00677</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00677</id><created>2015-01-04</created><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Dong</keyname><forenames>Yu-Wei</forenames></author><author><keyname>Shang</keyname><forenames>Mingsheng</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Group-based ranking method for online rating systems with spamming
  attacks</title><categories>cs.IR physics.soc-ph</categories><comments>6 pages, 5 figures, 2 tables</comments><doi>10.1209/0295-5075/110/28003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ranking problem has attracted much attention in real systems. How to design a
robust ranking method is especially significant for online rating systems under
the threat of spamming attacks. By building reputation systems for users, many
well-performed ranking methods have been applied to address this issue. In this
Letter, we propose a group-based ranking method that evaluates users'
reputations based on their grouping behaviors. More specifically, users are
assigned with high reputation scores if they always fall into large rating
groups. Results on three real data sets indicate that the present method is
more accurate and robust than correlation-based method in the presence of
spamming attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00680</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00680</id><created>2015-01-04</created><authors><author><keyname>Skliar</keyname><forenames>Osvaldo</forenames></author><author><keyname>Monge</keyname><forenames>Ricardo E.</forenames></author><author><keyname>Gapper</keyname><forenames>Sherry</forenames></author></authors><title>A New Method for Signal and Image Analysis: The Square Wave Method</title><categories>cs.NA cs.CV math.NA</categories><msc-class>94A12, 65F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A brief review is provided of the use of the Square Wave Method (SWM) in the
field of signal and image analysis and it is specified how results thus
obtained are expressed using the Square Wave Transform (SWT), in the frequency
domain. To illustrate the new approach introduced in this field, the results of
two cases are analyzed: a) a sequence of samples (that is, measured values) of
an electromyographic recording; and b) the classic image of Lenna.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00683</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00683</id><created>2015-01-04</created><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Flint</keyname><forenames>Ian</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Privault</keyname><forenames>Nicolas</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author></authors><title>Performance Analysis of Simultaneous Wireless Information and Power
  Transfer with Ambient RF Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>IEEE Wireless Communications and Networking Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advance in RF energy transfer and harvesting technique over the past
decade has enabled wireless energy replenishment for electronic devices, which
is deemed as a promising alternative to address the energy bottleneck of
conventional battery-powered devices. In this paper, by using a stochastic
geometry approach, we aim to analyze the performance of an RF-powered wireless
sensor in a downlink simultaneous wireless information and power transfer
(SWIPT) system with ambient RF transmitters. Specifically, we consider the
point-to-point downlink SWIPT transmission from an access point to a wireless
sensor in a network, where ambient RF transmitters are distributed as a Ginibre
?$\alpha$-determinantal point process (DPP), which becomes the Poisson point
process when $\alpha$? approaches zero. In the considered network, we focus on
analyzing the performance of a sensor equipped with the power-splitting
architecture. Under this architecture, we characterize the expected RF energy
harvesting rate of the sensor. Moreover, we derive the upper bound of both
power and transmission outage probabilities. Numerical results show that our
upper bounds are accurate for different value of ?$\alpha$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00687</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00687</id><created>2015-01-04</created><authors><author><keyname>Alkasassbeh</keyname><forenames>Mouhammd</forenames></author><author><keyname>Altarawneh</keyname><forenames>Ghada A.</forenames></author><author><keyname>Hassanat</keyname><forenames>Ahmad B. A.</forenames></author></authors><title>On Enhancing The Performance Of Nearest Neighbour Classifiers Using
  Hassanat Distance Metric</title><categories>cs.LG</categories><comments>Canadian Journal of Pure and Applied Sciences (CJPAS). volume 9,
  issue 1, Feb 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We showed in this work how the Hassanat distance metric enhances the
performance of the nearest neighbour classifiers. The results demonstrate the
superiority of this distance metric over the traditional and most-used
distances, such as Manhattan distance and Euclidian distance. Moreover, we
proved that the Hassanat distance metric is invariant to data scale, noise and
outliers. Throughout this work, it is clearly notable that both ENN and IINC
performed very well with the distance investigated, as their accuracy increased
significantly by 3.3% and 3.1% respectively, with no significant advantage of
the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted
from our results that there is no optimal algorithm that can solve all
real-life problems perfectly; this is supported by the no-free-lunch theorem
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00696</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00696</id><created>2015-01-04</created><authors><author><keyname>Metzler</keyname><forenames>Saskia</forenames></author><author><keyname>Miettinen</keyname><forenames>Pauli</forenames></author></authors><title>Clustering Boolean Tensors</title><categories>cs.NA cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor factorizations are computationally hard problems, and in particular,
are often significantly harder than their matrix counterparts. In case of
Boolean tensor factorizations -- where the input tensor and all the factors are
required to be binary and we use Boolean algebra -- much of that hardness comes
from the possibility of overlapping components. Yet, in many applications we
are perfectly happy to partition at least one of the modes. In this paper we
investigate what consequences does this partitioning have on the computational
complexity of the Boolean tensor factorizations and present a new algorithm for
the resulting clustering problem. This algorithm can alternatively be seen as a
particularly regularized clustering algorithm that can handle extremely
high-dimensional observations. We analyse our algorithms with the goal of
maximizing the similarity and argue that this is more meaningful than
minimizing the dissimilarity. As a by-product we obtain a PTAS and an efficient
0.828-approximation algorithm for rank-1 binary factorizations. Our algorithm
for Boolean tensor clustering achieves high scalability, high similarity, and
good generalization to unseen data with both synthetic and real-world data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00715</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00715</id><created>2015-01-04</created><authors><author><keyname>Wright</keyname><forenames>Mason</forenames></author><author><keyname>Vorobeychik</keyname><forenames>Yevgeniy</forenames></author></authors><title>Mechanism Design for Team Formation</title><categories>cs.GT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Team formation is a core problem in AI. Remarkably, little prior work has
addressed the problem of mechanism design for team formation, accounting for
the need to elicit agents' preferences over potential teammates. Coalition
formation in the related hedonic games has received much attention, but only
from the perspective of coalition stability, with little emphasis on the
mechanism design objectives of true preference elicitation, social welfare, and
equity. We present the first formal mechanism design framework for team
formation, building on recent combinatorial matching market design literature.
We exhibit four mechanisms for this problem, two novel, two simple extensions
of known mechanisms from other domains. Two of these (one new, one known) have
desirable theoretical properties. However, we use extensive experiments to show
our second novel mechanism, despite having no theoretical guarantees,
empirically achieves good incentive compatibility, welfare, and fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00720</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00720</id><created>2015-01-04</created><authors><author><keyname>Savinov</keyname><forenames>Alexandr</forenames></author></authors><title>Concept-oriented programming: from classes to concepts and from
  inheritance to inclusion</title><categories>cs.PL</categories><comments>12 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1409.3947</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past several decades, programmers have been modeling things in the
world with trees using hierarchies of classes and object-oriented programming
(OOP) languages. In this paper, we describe a novel approach to programming,
called concept-oriented programming (COP), which generalizes classes and
inheritance by introducing concepts and inclusion, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00721</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00721</id><created>2015-01-04</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Quanrud</keyname><forenames>Kent</forenames></author></authors><title>Approximation Algorithms for Low-Density Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the family of intersection graphs of low density objects in low
dimensional Euclidean space. This family is quite general, and includes planar
graphs. We prove that such graphs have small separators. Next, we present
efficient $(1+\varepsilon)$-approximation algorithms for these graphs, for
Independent Set, Set Cover, and Dominating Set problems, among others. We also
prove corresponding hardness of approximation for some of these optimization
problems, providing a characterization of their intractability in terms of
density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00728</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00728</id><created>2015-01-04</created><authors><author><keyname>Marghny</keyname><forenames>M. H.</forenames></author><author><keyname>ElAziz</keyname><forenames>Rasha M. Abd</forenames></author><author><keyname>Taloba</keyname><forenames>Ahmed I.</forenames></author></authors><title>Differential Search Algorithm-based Parametric Optimization of Fuzzy
  Generalized Eigenvalue Proximal Support Vector Machine</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machine (SVM) is an effective model for many classification
problems. However, SVM needs the solution of a quadratic program which require
specialized code. In addition, SVM has many parameters, which affects the
performance of SVM classifier. Recently, the Generalized Eigenvalue Proximal
SVM (GEPSVM) has been presented to solve the SVM complexity. In real world
applications data may affected by error or noise, working with this data is a
challenging problem. In this paper, an approach has been proposed to overcome
this problem. This method is called DSA-GEPSVM. The main improvements are
carried out based on the following: 1) a novel fuzzy values in the linear case.
2) A new Kernel function in the nonlinear case. 3) Differential Search
Algorithm (DSA) is reformulated to find near optimal values of the GEPSVM
parameters and its kernel parameters. The experimental results show that the
proposed approach is able to find the suitable parameter values, and has higher
classification accuracy compared with some other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00729</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00729</id><created>2015-01-04</created><authors><author><keyname>Romero</keyname><forenames>Jose Guadalupe</forenames></author><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author></authors><title>Two Globally Convergent Adaptive Speed Observers for Mechanical Systems</title><categories>math.DS cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A globally exponentially stable speed observer for mechanical systems was
recently reported in the literature, under the assumptions of known (or no)
Coulomb friction and no disturbances. In this note we propose and adaptive
version of this observer, which is robust vis--a--vis constant disturbances.
Moreover, we propose a new globally convergent speed observer that, besides
rejecting the disturbances, estimates some unknown friction coefficients for a
class of mechanical systems that contains several practical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00734</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00734</id><created>2015-01-04</created><updated>2015-03-26</updated><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Chan</keyname><forenames>Siu On</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author></authors><title>Sum of Squares Lower Bounds from Pairwise Independence</title><categories>cs.CC</categories><comments>27 Pages (including the title page) and 4 figures including appendix</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every $\epsilon&gt;0$ and predicate $P:\{0,1\}^k\rightarrow
\{0,1\}$ that supports a pairwise independent distribution, there exists an
instance $\mathcal{I}$ of the $\mathsf{Max}P$ constraint satisfaction problem
on $n$ variables such that no assignment can satisfy more than a
$\tfrac{|P^{-1}(1)|}{2^k}+\epsilon$ fraction of $\mathcal{I}$'s constraints but
the degree $\Omega(n)$ Sum of Squares semidefinite programming hierarchy cannot
certify that $\mathcal{I}$ is unsatisfiable. Similar results were previously
only known for weaker hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00738</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00738</id><created>2015-01-04</created><updated>2016-02-14</updated><authors><author><keyname>Lin</keyname><forenames>Henry W.</forenames></author><author><keyname>Loeb</keyname><forenames>Abraham</forenames></author></authors><title>Zipf's Law from Scale-free Geometry</title><categories>physics.soc-ph astro-ph.CO cond-mat.stat-mech cs.SI stat.AP</categories><comments>7 pages, 2 figures, accepted for publication in Physical Review E</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spatial distribution of people exhibits clustering across a wide range of
scales, from household ($\sim 10^{-2}$ km) to continental ($\sim 10^4$ km)
scales. Empirical data indicates simple power-law scalings for the size
distribution of cities (known as Zipf's law) and the population density
fluctuations as a function of scale. Using techniques from random field theory
and statistical physics, we show that these power laws are fundamentally a
consequence of the scale-free spatial clustering of human populations and the
fact that humans inhabit a two-dimensional surface. In this sense, the
symmetries of scale invariance in two spatial dimensions are intimately
connected to urban sociology. We test our theory by empirically measuring the
power spectrum of population density fluctuations and show that the logarithmic
slope $\alpha = 2.04 \pm 0.09$, in excellent agreement with our theoretical
prediction $\alpha = 2$. The model enables the analytic computation of many new
predictions by importing the mathematical formalism of random fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00742</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00742</id><created>2015-01-04</created><authors><author><keyname>Dousti</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>LEQA: Latency Estimation for a Quantum Algorithm Mapped to a Quantum
  Circuit Fabric</title><categories>quant-ph cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents LEQA, a fast latency estimation tool for evaluating the
performance of a quantum algorithm mapped to a quantum fabric. The actual
quantum algorithm latency can be computed by performing detailed scheduling,
placement and routing of the quantum instructions and qubits in a quantum
operation dependency graph on a quantum circuit fabric. This is, however, a
very expensive proposition that requires large amounts of processing time.
Instead, LEQA, which is based on computing the neighborhood population counts
of qubits, can produce estimates of the circuit latency with good accuracy
(i.e., an average of less than 3% error) with up to two orders of magnitude
speedup for mid-size benchmarks. This speedup is expected to increase
superlinearly as a function of circuit size (operation count).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00744</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00744</id><created>2015-01-04</created><authors><author><keyname>Zhang</keyname><forenames>Lanbo</forenames></author></authors><title>Identifying Relevant Document Facets for Keyword-Based Search Queries</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As structured documents with rich metadata (such as products, movies, etc.)
become increasingly prevalent, searching those documents has become an
important IR problem. Although advanced search interfaces are widely available,
most users still prefer to use keyword-based queries to search those documents.
Query keywords often imply some hidden restrictions on the desired documents,
which can be represented as document facet-value pairs. To achieve high
retrieval performance, it's important to be able to identify the relevant
facet-value pairs hidden in a query. In this paper, we study the problem of
identifying document facet-value pairs that are relevant to a keyword-based
search query. We propose a machine learning approach and a set of useful
features, and evaluate our approach using a movie data set from INEX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00747</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00747</id><created>2015-01-04</created><updated>2015-05-06</updated><authors><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Kaynar</keyname><forenames>Dilsun</forenames></author><author><keyname>Sharma</keyname><forenames>Divya</forenames></author><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author></authors><title>Programs as Actual Causes: A Building Block for Accountability</title><categories>cs.CR</categories><comments>This submission has been updated and is available at
  http://arxiv.org/abs/1505.01131</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An updated version of this paper is available at
http://arxiv.org/abs/1505.01131
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00752</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00752</id><created>2015-01-04</created><updated>2015-08-04</updated><authors><author><keyname>Shafiee</keyname><forenames>Mohammad</forenames></author><author><keyname>Azimifar</keyname><forenames>Zohreh</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>A Deep-structured Conditional Random Field Model for Object Silhouette
  Tracking</title><categories>cs.CV cs.LG stat.ML</categories><comments>17 pages</comments><doi>10.1371/journal.pone.0133036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we introduce a deep-structured conditional random field
(DS-CRF) model for the purpose of state-based object silhouette tracking. The
proposed DS-CRF model consists of a series of state layers, where each state
layer spatially characterizes the object silhouette at a particular point in
time. The interactions between adjacent state layers are established by
inter-layer connectivity dynamically determined based on inter-frame optical
flow. By incorporate both spatial and temporal context in a dynamic fashion
within such a deep-structured probabilistic graphical model, the proposed
DS-CRF model allows us to develop a framework that can accurately and
efficiently track object silhouettes that can change greatly over time, as well
as under different situations such as occlusion and multiple targets within the
scene. Experiment results using video surveillance datasets containing
different scenarios such as occlusion and multiple targets showed that the
proposed DS-CRF approach provides strong object silhouette tracking performance
when compared to baseline methods such as mean-shift tracking, as well as
state-of-the-art methods such as context tracking and boosted particle
filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00756</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00756</id><created>2015-01-04</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author><author><keyname>Raziperchikolaei</keyname><forenames>Ramin</forenames></author></authors><title>Hashing with binary autoencoders</title><categories>cs.LG cs.CV math.OC stat.ML</categories><comments>22 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An attractive approach for fast search in image databases is binary hashing,
where each high-dimensional, real-valued image is mapped onto a
low-dimensional, binary vector and the search is done in this binary space.
Finding the optimal hash function is difficult because it involves binary
constraints, and most approaches approximate the optimization by relaxing the
constraints and then binarizing the result. Here, we focus on the binary
autoencoder model, which seeks to reconstruct an image from the binary code
produced by the hash function. We show that the optimization can be simplified
with the method of auxiliary coordinates. This reformulates the optimization as
alternating two easier steps: one that learns the encoder and decoder
separately, and one that optimizes the code for each image. Image retrieval
experiments, using precision/recall and a measure of code utilization, show the
resulting hash function outperforms or is competitive with state-of-the-art
methods for binary hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00758</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00758</id><created>2015-01-04</created><authors><author><keyname>Ishii</keyname><forenames>Akira</forenames></author><author><keyname>Kitao</keyname><forenames>Akiko</forenames></author><author><keyname>Usui</keyname><forenames>Tsukasa</forenames></author><author><keyname>Uchiyama</keyname><forenames>Koki</forenames></author></authors><title>Mathematical model for hit phenomena and its application to analyze
  popularity of weekly tv drama</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 12 figures, submitted to International Journal of Modern
  Physics B: Special issue: Advances on Statistical Physics of Complex Systems
  as a conference paper of International Conference on Statisitical Physics,
  Rhodes, Greece, 7-11 July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical model for hit phenomena presented by A Ishii et al in 2012 has
been extended to analyze and predict a lot of hit subject using social network
system. The equation for each individual consumers is assumed and the equation
of social response to each hit subject is derived as stochastic process of
statistical physics. The advertisement effect is included as external force and
the communication effects are included as two-body and three-body interaction.
The applications of this model are demonstrated for analyzing population of
weekly TV drama. Including both the realtime view data and the playback view
data, we found that the indirect communication correlate strongly to the TV
viewing rate data for recent Japanese 20 TV drama.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00777</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00777</id><created>2015-01-05</created><authors><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Chang</keyname><forenames>Heyou</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>Sparse Deep Stacking Network for Image Classification</title><categories>cs.CV cs.LG cs.NE</categories><comments>8 pages, 3 figures, AAAI-2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sparse coding can learn good robust representation to noise and model more
higher-order representation for image classification. However, the inference
algorithm is computationally expensive even though the supervised signals are
used to learn compact and discriminative dictionaries in sparse coding
techniques. Luckily, a simplified neural network module (SNNM) has been
proposed to directly learn the discriminative dictionaries for avoiding the
expensive inference. But the SNNM module ignores the sparse representations.
Therefore, we propose a sparse SNNM module by adding the mixed-norm
regularization (l1/l2 norm). The sparse SNNM modules are further stacked to
build a sparse deep stacking network (S-DSN). In the experiments, we evaluate
S-DSN with four databases, including Extended YaleB, AR, 15 scene and
Caltech101. Experimental results show that our model outperforms related
classification methods with only a linear classifier. It is worth noting that
we reach 98.8% recognition accuracy on 15 scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00802</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00802</id><created>2015-01-05</created><authors><author><keyname>Dewan</keyname><forenames>Prateek</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Detecting Malicious Content on Facebook</title><categories>cs.SI cs.CY</categories><comments>9 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks (OSNs) witness a rise in user activity whenever an
event takes place. Malicious entities exploit this spur in user-engagement
levels to spread malicious content that compromises system reputation and
degrades user experience. It also generates revenue from advertisements,
clicks, etc. for the malicious entities. Facebook, the world's biggest social
network, is no exception and has recently been reported to face much abuse
through scams and other type of malicious content, especially during news
making events. Recent studies have reported that spammers earn $200 million
just by posting malicious links on Facebook. In this paper, we characterize
malicious content posted on Facebook during 17 events, and discover that
existing efforts to counter malicious content by Facebook are not able to stop
all malicious content from entering the social graph. Our findings revealed
that malicious entities tend to post content through web and third party
applications while legitimate entities prefer mobile platforms to post content.
In addition, we discovered a substantial amount of malicious content generated
by Facebook pages. Through our observations, we propose an extensive feature
set based on entity profile, textual content, metadata, and URL features to
identify malicious content on Facebook in real time and at zero-hour. This
feature set was used to train multiple machine learning models and achieved an
accuracy of 86.9%. The intent is to catch malicious content that is currently
evading Facebook's detection techniques. Our machine learning model was able to
detect more than double the number of malicious posts as compared to existing
malicious content detection techniques. Finally, we built a real world solution
in the form of a REST based API and a browser plug-in to identify malicious
Facebook posts in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00805</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00805</id><created>2015-01-05</created><updated>2015-01-22</updated><authors><author><keyname>Tarighati</keyname><forenames>Alla</forenames></author><author><keyname>Jalden</keyname><forenames>Joakim</forenames></author></authors><title>A General Method for the Design of Tree Networks Under Communication
  Constraints</title><categories>cs.IT math.IT</categories><comments>Information Fusion 2014, Accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a distributed detection system with communication constraints,
where several nodes are arranged in an arbitrary tree topology, under the
assumption of conditionally independent observations. We propose a cyclic
design procedure using the minimum expected error probability as a design
criterion while adopting a person-by-person methodology. We design each node
jointly together with the fusion center, while other nodes are kept fixed, and
show that the design of each node using the person-by-person methodology is
analogous to the design of a network with two nodes, a network which we refer
to as the restricted model. We further show how the parameters in the
restricted model for the design of a node in the tree network can be found in a
computationally efficient manner. The proposed numerical methodology can be
applied for the design of nodes arranged in arbitrary tree topologies with
arbitrary channel rates for the links between nodes and for a general M-ary
hypothesis testing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00820</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00820</id><created>2015-01-05</created><updated>2016-02-14</updated><authors><author><keyname>Hegna</keyname><forenames>Odell</forenames></author></authors><title>Software Safety Demonstration and Idemnification</title><categories>cs.LO</categories><comments>50 pages</comments><acm-class>D.2.4; F.1.1; D.2.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In cyber-physical systems, software may control safety-significant
operations. This report discusses a method to structure software testing to
measure the confidence that algorithms are true to their intended design. The
subject matter appears in two main parts: theory, which shows the relationship
between discrete systems theory, software, and the actuated automaton; and
application, which discusses safety demonstration and indemnification, a safety
assurance metric.
  The recommended form of statistical testing involves sampling algorithmic
behavior in a specific area of safety risk known as a hazard. When this sample
is random, it is known as a safety demonstration. It provides evidence for
indemnification, a statistic expressing a probabilistic upper bound for
accident frequency. The method obtains results efficiently from practical
sample sizes.
  Keywords: software, safety, hazard, demonstration, operational profile,
automata, confidence, statistics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00825</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00825</id><created>2015-01-05</created><authors><author><keyname>Wang</keyname><forenames>Jianfeng</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Kankanhalli</keyname><forenames>Mohan S</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author></authors><title>Group $K$-Means</title><categories>cs.CV</categories><comments>The developed algorithm is similar with &quot;Christopher F. Barnes, A new
  multiple path search technique for residual vector quantizers, 1994&quot;, but we
  conduct the research independently and apply it in data/feature compression
  and image retrieval</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to learn multiple dictionaries from a dataset, and approximate
any data point by the sum of the codewords each chosen from the corresponding
dictionary. Although theoretically low approximation errors can be achieved by
the global solution, an effective solution has not been well studied in
practice. To solve the problem, we propose a simple yet effective algorithm
\textit{Group $K$-Means}. Specifically, we take each dictionary, or any two
selected dictionaries, as a group of $K$-means cluster centers, and then deal
with the approximation issue by minimizing the approximation errors. Besides,
we propose a hierarchical initialization for such a non-convex problem.
Experimental results well validate the effectiveness of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00828</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00828</id><created>2015-01-05</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Cariowa</keyname><forenames>Galina</forenames></author></authors><title>A new algorithm for multiplying two Dirac numbers</title><categories>cs.DS cs.NA</categories><comments>14 pages, 1 figure</comments><msc-class>15B33, 11R52, 65F30, 65Y20, 68W35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work a rationalized algorithm for Dirac numbers multiplication is
presented. This algorithm has a low computational complexity feature and is
well suited to FPGA implementation. The computation of two Dirac numbers
product using the na\&quot;ive method takes 256 real multiplications and 240 real
additions, while the proposed algorithm can compute the same result in only 88
real multiplications and 256 real additions. During synthesis of the discussed
algorithm we use the fact that Dirac numbers product may be represented as
vector-matrix product. The matrix participating in the product has unique
structural properties that allow performing its advantageous decomposition.
Namely this decomposition leads to significant reducing of the computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00834</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00834</id><created>2015-01-05</created><authors><author><keyname>Tanaka</keyname><forenames>Kazuyuki</forenames></author><author><keyname>Kataoka</keyname><forenames>Shun</forenames></author><author><keyname>Yasuda</keyname><forenames>Muneki</forenames></author><author><keyname>Ohzeki</keyname><forenames>Masayuki</forenames></author></authors><title>Inverse Renormalization Group Transformation in Bayesian Image
  Segmentations</title><categories>cs.CV cond-mat.stat-mech stat.ML</categories><comments>6 pages, 2 figures</comments><journal-ref>Journal of the Physical Society of Japan 84 (2015) 045001</journal-ref><doi>10.7566/JPSJ.84.045001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new Bayesian image segmentation algorithm is proposed by combining a loopy
belief propagation with an inverse real space renormalization group
transformation to reduce the computational time. In results of our experiment,
we observe that the proposed method can reduce the computational time to less
than one-tenth of that taken by conventional Bayesian approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00841</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00841</id><created>2015-01-05</created><authors><author><keyname>Lynch</keyname><forenames>Gerard</forenames></author><author><keyname>Vogel</keyname><forenames>Carl</forenames></author></authors><title>Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama
  in translation</title><categories>cs.CL</categories><comments>6 pages</comments><journal-ref>Digital Humanities 2009 Proceedings, University of Maryland,
  College Park, MD, USA, pages 192-195</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research into the stylistic properties of translations is an issue which has
received some attention in computational stylistics. Previous work by Rybicki
(2006) on the distinguishing of character idiolects in the work of Polish
author Henryk Sienkiewicz and two corresponding English translations using
Burrow's Delta method concluded that idiolectal differences could be observed
in the source texts and this variation was preserved to a large degree in both
translations. This study also found that the two translations were also highly
distinguishable from one another. Burrows (2002) examined English translations
of Juvenal also using the Delta method, results of this work suggest that some
translators are more adept at concealing their own style when translating the
works of another author whereas other authors tend to imprint their own style
to a greater extent on the work they translate. Our work examines the writing
of a single author, Norwegian playwright Henrik Ibsen, and these writings
translated into both German and English from Norwegian, in an attempt to
investigate the preservation of characterization, defined here as the
distinctiveness of textual contributions of characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00850</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00850</id><created>2015-01-05</created><updated>2015-05-20</updated><authors><author><keyname>Shchesnovich</keyname><forenames>V. S.</forenames></author></authors><title>Tight bound on trace distance between a realistic device with partially
  indistinguishable bosons and the ideal Boson Sampling</title><categories>quant-ph cond-mat.stat-mech cs.CC</categories><comments>8 pages, no figures. Small changes from version 3: title and Abstract
  revised. [This submission, since version 3, is restricted to identical
  detectors. Arbitrary detectors were treated in version 1. Those results
  remain valid, but the derivation contains a flaw. Corrected version 1 will be
  resubmitted as a separate article generalizing this one.]</comments><journal-ref>Phys. Rev. A 91, 063842 (2015)</journal-ref><doi>10.1103/PhysRevA.91.063842</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the closeness of an experimental unitary bosonic network with only
partially indistinguishable bosons in an arbitrary mixed input state, in
particular an experimental realization of the Boson Sampling, to the ideal
bosonic network, where the measure of closeness of two networks is the trace
distance between the output probability distributions. An upper bound on the
trace distance to the ideal bosonic network is proven and also a bound on the
difference between probabilities of an output configuration. Moreover, the
upper bound on the trace distance is tight, provided that a physically
transparent distinguishability conjecture is true. For a small
distinguishability error it is shown that a realistic device with $N$ bosons is
at a constant trace distance to the ideal Boson Sampling under the
$O(N^{-1})$-scaling of the mismatch of internal states of bosons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00857</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00857</id><created>2015-01-05</created><authors><author><keyname>Fauvel</keyname><forenames>Mathieu</forenames></author><author><keyname>Dechesne</keyname><forenames>Clement</forenames></author><author><keyname>Zullo</keyname><forenames>Anthony</forenames></author><author><keyname>Ferraty</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Fast forward feature selection for the nonlinear classification of
  hyperspectral images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast forward feature selection algorithm is presented in this paper. It is
based on a Gaussian mixture model (GMM) classifier. GMM are used for
classifying hyperspectral images. The algorithm selects iteratively spectral
features that maximizes an estimation of the classification rate. The
estimation is done using the k-fold cross validation. In order to perform fast
in terms of computing time, an efficient implementation is proposed. First, the
GMM can be updated when the estimation of the classification rate is computed,
rather than re-estimate the full model. Secondly, using marginalization of the
GMM, sub models can be directly obtained from the full model learned with all
the spectral features. Experimental results for two real hyperspectral data
sets show that the method performs very well in terms of classification
accuracy and processing time. Furthermore, the extracted model contains very
few spectral channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00881</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00881</id><created>2015-01-05</created><authors><author><keyname>Abdellah</keyname><forenames>Zaaloul</forenames></author><author><keyname>Abdelkrim</keyname><forenames>Haqiq</forenames></author></authors><title>Analysis of Performance Parameters in Wireless Networks by using Game
  Theory for the non Cooperative Slotted Aloha Enhanced by ZigZag Decoding
  Mechanism</title><categories>cs.GT cs.NI</categories><comments>9 pages, 8 figures, journal in World of Computer Science and
  Information Technology Journal (WCSIT) 2014. arXiv admin note: text overlap
  with arXiv:1001.1948, arXiv:1210.2911 by other authors</comments><msc-class>68M07</msc-class><acm-class>K.6.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In wireless communication networks, when the workload increases, sources
become more aggressive at the equilibrium in the game setting in comparison
with the team problem by using slotted Aloha mechanism. Consequently, more
packets are in collision and are lost. To reduce these phenomena and to enhance
the performance of the networks, we propose to combine ZigZag decoding approach
with non cooperative slotted Aloha mechanism. This approach was introduced in
our previous work based on the cooperative slotted Aloha mechanism. The
obtained results showed that this approach has significantly improved the
cooperative slotted Aloha mechanism and gave best results for the throughput
and delay. In this paper, we analyze the impact of combining non cooperative
slotted Aloha and ZigZag Decoding. We model the system by a two dimensional
Markov chain that integrates the effect of ZigZag decoding. The states of the
Markov chain describe the number of backlogged packets among the users. We use
a stochastic game to achieve our objective; we evaluate and compare the
performances parameters of the proposed approach with those of a simple slotted
Aloha mechanism. All found results show that our approach improves the
performance parameters of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00882</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00882</id><created>2014-12-19</created><authors><author><keyname>Grafenhofer</keyname><forenames>Dominik</forenames></author><author><keyname>Kuhle</keyname><forenames>Wolgang</forenames></author></authors><title>Observing Each Other's Observations in the Electronic Mail Game</title><categories>q-fin.EC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a Bayesian coordination game where agents receive private
information on the game's payoff structure. In addition, agents receive private
signals on each other's private information. We show that once agents possess
these different types of information, there exists a coordination game in the
evaluation of this information. And even though the precisions of both signal
types is exogenous, the precision with which agents predict each other's
actions at equilibrium turns out to be endogenous. As a consequence, we find
that there exist multiple equilibria if the private signals' precision is high.
These equilibria differ with regard to the way that agents weight their private
information to reason about each other's actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00892</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00892</id><created>2015-01-05</created><authors><author><keyname>Demirel</keyname><forenames>Burak</forenames></author><author><keyname>Gupta</keyname><forenames>Vijay</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>On the trade-off between control performance and communication cost in
  event-triggered control</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider a stochastic system where the communication between the
controller and the actuator is triggered by a threshold-based rule. The
communication is performed across an unreliable link that stochastically erases
transmitted packets. To decrease the communication burden, and as a partial
protection against dropped packets, the controller sends a sequence of control
commands to the actuator in each packet. These commands are stored in a buffer
and applied sequentially until the next control packet arrives. In this
context, we study dead-beat control laws and compute the expected
linear-quadratic loss of the closed-loop system for any given event-threshold.
Furthermore, we provide analytical expressions that quantify the trade-off
between the communication cost and the control performance of event-triggered
control systems. Numerical examples demonstrate the effectiveness of the
proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00894</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00894</id><created>2015-01-05</created><authors><author><keyname>Avanzini</keyname><forenames>Martin</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author></authors><title>On Sharing, Memoization, and Polynomial Time (Long Version)</title><categories>cs.CC</categories><acm-class>F.1.3; F.3.2; F.4.1; F.4.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study how the adoption of an evaluation mechanism with sharing and
memoization impacts the class of functions which can be computed in polynomial
time. We first show how a natural cost model in which lookup for an already
computed value has no cost is indeed invariant. As a corollary, we then prove
that the most general notion of ramified recurrence is sound for polynomial
time, this way settling an open problem in implicit computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00901</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00901</id><created>2015-01-05</created><updated>2015-04-29</updated><authors><author><keyname>Deng</keyname><forenames>Yubin</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Learning to Recognize Pedestrian Attribute</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to recognize pedestrian attributes at far distance is a challenging
problem in visual surveillance since face and body close-shots are hardly
available; instead, only far-view image frames of pedestrian are given. In this
study, we present an alternative approach that exploits the context of
neighboring pedestrian images for improved attribute inference compared to the
conventional SVM-based method. In addition, we conduct extensive experiments to
evaluate the informativeness of background and foreground features for
attribute recognition. Experiments are based on our newly released pedestrian
attribute dataset, which is by far the largest and most diverse of its kind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00909</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00909</id><created>2015-01-05</created><authors><author><keyname>Liang</keyname><forenames>Pengpeng</forenames></author><author><keyname>Liao</keyname><forenames>Chunyuan</forenames></author><author><keyname>Mei</keyname><forenames>Xue</forenames></author><author><keyname>Ling</keyname><forenames>Haibin</forenames></author></authors><title>Adaptive Objectness for Object Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object tracking is a long standing problem in vision. While great efforts
have been spent to improve tracking performance, a simple yet reliable prior
knowledge is left unexploited: the target object in tracking must be an object
other than non-object. The recently proposed and popularized objectness measure
provides a natural way to model such prior in visual tracking. Thus motivated,
in this paper we propose to adapt objectness for visual object tracking.
Instead of directly applying an existing objectness measure that is generic and
handles various objects and environments, we adapt it to be compatible to the
specific tracking sequence and object. More specifically, we use the newly
proposed BING objectness as the base, and then train an object-adaptive
objectness for each tracking task. The training is implemented by using an
adaptive support vector machine that integrates information from the specific
tracking target into the BING measure. We emphasize that the benefit of the
proposed adaptive objectness, named ADOBING, is generic. To show this, we
combine ADOBING with seven top performed trackers in recent evaluations. We run
the ADOBING-enhanced trackers with their base trackers on two popular
benchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton Tracking
Benchmark (100 sequences). On both benchmarks, our methods not only
consistently improve the base trackers, but also achieve the best known
performances. Noting that the way we integrate objectness in visual tracking is
generic and straightforward, we expect even more improvement by using
tracker-specific objectness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00919</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00919</id><created>2015-01-05</created><updated>2016-01-28</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>A General Design Framework for MIMO Wireless Energy Transfer with
  Limited Feedback</title><categories>cs.IT math.IT</categories><comments>This is a longer version of a paper to appear in IEEE Transactions on
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-antenna or multiple-input multiple-output (MIMO) technique can
significantly improve the efficiency of radio frequency (RF) signal enabled
wireless energy transfer (WET). To fully exploit the energy beamforming gain at
the energy transmitter (ET), the knowledge of channel state information (CSI)
is essential, which, however, is difficult to be obtained in practice due to
the hardware limitation of the energy receiver (ER). To overcome this
difficulty, under a point-to-point MIMO WET setup, this paper proposes a
general design framework for a new type of channel learning method based on the
ER's energy measurement and feedback. Specifically, the ER measures and encodes
the harvested energy levels over different training intervals into bits, and
sends them to the ET via a feedback link of limited rate. Based on the
energy-level feedback, the ET adjusts transmit beamforming in subsequent
training intervals and obtains refined estimates of the MIMO channel by
leveraging the technique of analytic center cutting plane method (ACCPM) in
convex optimization. Under this general design framework, we further propose
two specific feedback schemes termed energy quantization and energy comparison,
where the feedback bits at each interval are generated at the ER by quantizing
the measured energy level at the current interval and comparing it with those
in the previous intervals, respectively. Numerical results are provided to
compare the performance of the two feedback schemes. It is shown that energy
quantization performs better when the number of feedback bits per interval is
large, while energy comparison is more effective with small number of feedback
bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00923</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00923</id><created>2015-01-05</created><authors><author><keyname>Zaaloul</keyname><forenames>Abdellah</forenames></author><author><keyname>Aattar</keyname><forenames>Mohamed Ben El</forenames></author><author><keyname>Hanini</keyname><forenames>Mohamed</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Boulmalaf</keyname><forenames>Mohammed</forenames></author></authors><title>Sharing Channel In IEEE 802.16 Using The Cooperative Model Of Slotted
  ALOHA</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 4 figures in International Journal of Computer &amp;
  Organization Trends 2013</comments><msc-class>60J20</msc-class><acm-class>I.6.0</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the main problems in WIMAX is to share the medium by multiple users
who compete for access. Various random access mechanisms, such as ALOHA and its
corresponding variations have been widely studied as efficient methods to
coordinate the medium access among competing users. In this paper the slotted
ALOHA protocol is implemented as a two-state system. Using Markov Models, we
evaluate the channel utilization and we analyze the throughput under different
fairness conditions. The cooperative team problem is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00933</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00933</id><created>2015-01-05</created><updated>2015-04-10</updated><authors><author><keyname>Held</keyname><forenames>Stephan</forenames></author><author><keyname>K&#xe4;mmerling</keyname><forenames>Nicolas</forenames></author></authors><title>Two-Level Rectilinear Steiner Trees</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of terminals in the plane and a partition of $P$ into $k$
subsets $P_1, ..., P_k$, a two-level rectilinear Steiner tree consists of a
rectilinear Steiner tree $T_i$ connecting the terminals in each set $P_i$
($i=1,...,k$) and a top-level tree $T_{top}$ connecting the trees $T_1, ...,
T_k$. The goal is to minimize the total length of all trees. This problem
arises naturally in the design of low-power physical implementations of parity
functions on a computer chip.
  For bounded $k$ we present a polynomial time approximation scheme (PTAS) that
is based on Arora's PTAS for rectilinear Steiner trees after lifting each
partition into an extra dimension. For the general case we propose an algorithm
that predetermines a connection point for each $T_i$ and $T_{top}$
($i=1,...,k$).
  Then, we apply any approximation algorithm for minimum rectilinear Steiner
trees in the plane to compute each $T_i$ and $T_{top}$ independently.
  This gives us a $2.37$-factor approximation with a running time of
$\mathcal{O}(|P|\log|P|)$ suitable for fast practical computations. The
approximation factor reduces to $1.63$ by applying Arora's approximation scheme
in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00943</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00943</id><created>2015-01-05</created><authors><author><keyname>Hogan</keyname><forenames>Emilie</forenames></author><author><keyname>Cotilla-Sanchez</keyname><forenames>Eduardo</forenames></author><author><keyname>Halappanavar</keyname><forenames>Mahantesh</forenames></author><author><keyname>Huang</keyname><forenames>Zhenyu</forenames></author><author><keyname>Lin</keyname><forenames>Guang</forenames></author><author><keyname>Lu</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Shaobu</forenames></author></authors><title>Comparative Studies of Clustering Techniques for Real-Time Dynamic Model
  Reduction</title><categories>physics.soc-ph cs.SY</categories><comments>8 pages, 13 figures, submitted to IEEE transactions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic model reduction in power systems is necessary for improving
computational efficiency. Traditional model reduction using linearized models
or offline analysis would not be adequate to capture power system dynamic
behaviors, especially the new mix of intermittent generation and intelligent
consumption makes the power system more dynamic and non-linear. Real-time
dynamic model reduction emerges as an important need. This paper explores the
use of clustering techniques to analyze real-time phasor measurements to
determine generator groups and representative generators for dynamic model
reduction. Two clustering techniques -- graph clustering and evolutionary
clustering -- are studied in this paper. Various implementations of these
techniques are compared and also compared with a previously developed Singular
Value Decomposition (SVD)-based dynamic model reduction approach. Various
methods exhibit different levels of accuracy when comparing the reduced model
simulation against the original model. But some of them are consistently
accurate. From this comparative perspective, this paper provides a good
reference point for practical implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00950</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00950</id><created>2015-01-05</created><updated>2015-04-09</updated><authors><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author></authors><title>Influence of Behavioral Models on Multiuser Channel Capacity</title><categories>cs.IT math.IT physics.optics</categories><doi>10.1109/JLT.2015.2438951</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to characterize the channel capacity of a wavelength channel in a
wavelength-division multiplexed (WDM) system, statistical models are needed for
the transmitted signals on the other wavelengths. For example, one could assume
that the transmitters for all wavelengths are configured independently of each
other, that they use the same signal power, or that they use the same
modulation format. In this paper, it is shown that these so-called behavioral
models have a profound impact on the single-wavelength achievable information
rate. This is demonstrated by establishing, for the first time, upper and lower
bounds on the maximum achievable rate under various behavioral models, for a
rudimentary WDM channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00960</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00960</id><created>2015-01-05</created><updated>2015-05-07</updated><authors><author><keyname>Pechenick</keyname><forenames>Eitan Adam</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Characterizing the Google Books corpus: Strong limits to inferences of
  socio-cultural and linguistic evolution</title><categories>physics.soc-ph cond-mat.stat-mech cs.CL stat.AP</categories><comments>13 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is tempting to treat frequency trends from the Google Books data sets as
indicators of the &quot;true&quot; popularity of various words and phrases. Doing so
allows us to draw quantitatively strong conclusions about the evolution of
cultural perception of a given topic, such as time or gender. However, the
Google Books corpus suffers from a number of limitations which make it an
obscure mask of cultural popularity. A primary issue is that the corpus is in
effect a library, containing one of each book. A single, prolific author is
thereby able to noticeably insert new phrases into the Google Books lexicon,
whether the author is widely read or not. With this understood, the Google
Books corpus remains an important data set to be considered more lexicon-like
than text-like. Here, we show that a distinct problematic feature arises from
the inclusion of scientific texts, which have become an increasingly
substantive portion of the corpus throughout the 1900s. The result is a surge
of phrases typical to academic articles but less common in general, such as
references to time in the form of citations. We highlight these dynamics by
examining and comparing major contributions to the statistical divergence of
English data sets between decades in the period 1800--2000. We find that only
the English Fiction data set from the second version of the corpus is not
heavily affected by professional texts, in clear contrast to the first version
of the fiction data set and both unfiltered English data sets. Our findings
emphasize the need to fully characterize the dynamics of the Google Books
corpus before using these data sets to draw broad conclusions about cultural
and linguistic evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00976</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00976</id><created>2015-01-05</created><authors><author><keyname>Abdellah</keyname><forenames>Zaaloul</forenames></author><author><keyname>Abdelkrim</keyname><forenames>Haqiq</forenames></author></authors><title>Enhanced Slotted Aloha Mechanism by Introducing ZigZag Decoding</title><categories>cs.IT cs.NI math.IT</categories><comments>11 pages, 10 figures in Journal of mathematics and computer science
  2014. arXiv admin note: substantial text overlap with arXiv:1501.00881</comments><msc-class>60J20</msc-class><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various random access mechanisms, such as Aloha protocol and its
corresponding variants have been widely studied as efficient methods to
coordinate the medium access among competing users. But when two or more
wireless users transmit packets at the same time over the same channel a
collisions occur. When this happens, the received packets are discarded and
retransmissions are required, which is a waste of power and bandwidth. In such
a situation one of the most important objectives is to find techniques to
improve these protocols to reduce the number of collisions or to avoid them.
Several studies have contributed to this problem. In this paper, we propose a
new approach named ZigZag decoding to enhance slotted Aloha mechanism by
reducing the loss rate of packets colliding. We model the system by a Markov
chain witch the number of backlogged packets is taken as the system state. We
use a stochastic game to achieve our objective. We evaluate and compare the
performances parameters of the proposed approach with those of slotted Aloha
mechanism. All found results show that our approach is more efficient than the
slotted Aloha mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00994</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00994</id><created>2015-01-05</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author><author><keyname>Hoiles</keyname><forenames>William</forenames></author></authors><title>Online Reputation and Polling Systems: Data Incest, Social Learning and
  Revealed Preferences</title><categories>cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1412.4171</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers online reputation and polling systems where individuals
make recommendations based on their private observations and recommendations of
friends. Such interaction of individuals and their social influence is modelled
as social learning on a directed acyclic graph. Data incest (misinformation
propagation) occurs due to unintentional re-use of identical actions in the
for- mation of public belief in social learning; the information gathered by
each agent is mistakenly considered to be independent. This results in
overconfidence and bias in estimates of the state. Necessary and sufficient
conditions are given on the structure of information exchange graph to mitigate
data incest. Incest removal algorithms are presented. Experimental results on
human subjects are presented to illustrate the effect of social influence and
data incest on decision making. These experimental results indicate that social
learning protocols require careful design to handle and mitigate data incest.
The incest removal algorithms are illustrated in an expectation polling system
where participants in a poll respond with a summary of their friends' beliefs.
Finally, the principle of revealed preferences arising in micro-economics
theory is used to parse Twitter datasets to determine if social sensors are
utility maximizers and then determine their utility functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01013</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01013</id><created>2014-12-30</created><authors><author><keyname>Levaillant</keyname><forenames>Claire I.</forenames></author></authors><title>Making a circulant 2-qubit entangling gate</title><categories>quant-ph cs.IT math.GR math.IT math.QA math.RT</categories><comments>21 pages, 33 figures</comments><msc-class>81P68, 20F36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a way to physically realize a circulant 2-qubit entangling gate in
the Kauffman-Jones version of SU(2) Chern-Simons theory at level 4. Our
approach uses qubit and qutrit ancillas, braids, fusions and interferometric
measurements. Our qubit is formed by four anyons of topological charges 1221.
Among other 2-qubit entangling gates we generate in the present paper, we
produce in particular the circulant gate CEG = 1/4 I + I sqrt(3)/4 J - 3/4 J^2
+ I sqrt(3)/4 J^3, where J denotes the permutation matrix associated with the
cycle (1432) and I denotes the identity matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01019</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01019</id><created>2014-12-28</created><authors><author><keyname>Levaillant</keyname><forenames>Claire</forenames></author></authors><title>Protocol for making a $2$-qutrit entangling gate in the Kauffman-Jones
  version of $SU(2)_4$</title><categories>quant-ph cs.IT math.IT math.QA math.RT</categories><comments>8 pages, 4 figures</comments><msc-class>81P68, 68Q05, 20F36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following paper provides a protocol to physically generate a $2$-qutrit
entangling gate in the Kauffman-Jones version of $SU(2)$ Chern-Simons theory at
level $4$. The protocol uses elementary operations on anyons consisting of
braids, interferometric measurements, fusions and unfusions and ancilla pair
creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01039</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01039</id><created>2015-01-05</created><authors><author><keyname>Krug</keyname><forenames>Joseph</forenames></author><author><keyname>Peterson</keyname><forenames>Jack</forenames></author></authors><title>Sidecoin: a snapshot mechanism for bootstrapping a blockchain</title><categories>cs.CR</categories><comments>3 pages</comments><doi>10.13140/2.1.4577.1841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sidecoin is a mechanism that allows a snapshot to be taken of Bitcoin's
blockchain. We compile a list of Bitcoin's unspent transaction outputs, then
use these outputs and their corresponding balances to bootstrap a new
blockchain. This allows the preservation of Bitcoin's economic state in the
context of a new blockchain, which may provide new features and technical
innovations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01042</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01042</id><created>2015-01-05</created><authors><author><keyname>Peterson</keyname><forenames>Jack</forenames></author><author><keyname>Krug</keyname><forenames>Joseph</forenames></author></authors><title>Augur: a decentralized, open-source platform for prediction markets</title><categories>cs.CR</categories><comments>13 pages, 5 figures</comments><doi>10.13140/2.1.1431.4563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augur is a trustless, decentralized platform for prediction markets. It is an
extension of Bitcoin Core's source code which preserves as much of Bitcoin's
proven code and security as possible. Each feature required for prediction
markets is constructed from Bitcoin's input/output-style transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01062</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01062</id><created>2015-01-05</created><updated>2015-07-15</updated><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author></authors><title>Optimal Data-Dependent Hashing for Approximate Near Neighbors</title><categories>cs.DS</categories><comments>36 pages, 5 figures, an extended abstract appeared in the proceedings
  of the 47th ACM Symposium on Theory of Computing (STOC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an optimal data-dependent hashing scheme for the approximate near
neighbor problem. For an $n$-point data set in a $d$-dimensional space our data
structure achieves query time $O(d n^{\rho+o(1)})$ and space $O(n^{1+\rho+o(1)}
+ dn)$, where $\rho=\tfrac{1}{2c^2-1}$ for the Euclidean space and
approximation $c&gt;1$. For the Hamming space, we obtain an exponent of
$\rho=\tfrac{1}{2c-1}$.
  Our result completes the direction set forth in [AINR14] who gave a
proof-of-concept that data-dependent hashing can outperform classical Locality
Sensitive Hashing (LSH). In contrast to [AINR14], the new bound is not only
optimal, but in fact improves over the best (optimal) LSH data structures
[IM98,AI06] for all approximation factors $c&gt;1$.
  From the technical perspective, we proceed by decomposing an arbitrary
dataset into several subsets that are, in a certain sense, pseudo-random.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01070</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01070</id><created>2015-01-05</created><authors><author><keyname>Kllapi</keyname><forenames>Herald</forenames></author><author><keyname>Sakkos</keyname><forenames>Panos</forenames></author><author><keyname>Delis</keyname><forenames>Alex</forenames></author><author><keyname>Gunopulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ioannidis</keyname><forenames>Yannis</forenames></author></authors><title>Elastic Processing of Analytical Query Workloads on IaaS Clouds</title><categories>cs.DB cs.DC</categories><comments>12 pages</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern applications require the evaluation of analytical queries on
large amounts of data. Such queries entail joins and heavy aggregations that
often include user-defined functions (UDFs). The most efficient way to process
these specific type of queries is using tree execution plans. In this work, we
develop an engine for analytical query processing and a suite of specialized
techniques that collectively take advantage of the tree form of such plans. The
engine executes these tree plans in an elastic IaaS cloud infrastructure and
dynamically adapts by allocating and releasing pertinent resources based on the
query workload monitored over a sliding time window. The engine offers its
services for a fee according to service-level agreements (SLAs) associated with
the incoming queries; its management of cloud resources aims at maximizing the
profit after removing the costs of using these resources. We have fully
implemented our algorithms in the Exareme dataflow processing system. We
present an extensive evaluation that demonstrates that our approach is very
efficient (exhibiting fast response times), elastic (successfully adjusting the
cloud resources it uses as the engine continually adapts to query workload
changes), and profitable (approximating very well the maximum difference
between SLA-based income and cloud-based expenses).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01073</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01073</id><created>2015-01-05</created><authors><author><keyname>Chelloug</keyname><forenames>Samia Allaoua</forenames></author></authors><title>Impact of the Temperature and Humidity Variations on Link Quality of
  xm1000 Mote Sensors</title><categories>cs.NI</categories><comments>9 pages in International Journal of Ad hoc, Sensor &amp; Ubiquitous
  Computing (IJASUC) Vol.5, No.6, December 2014</comments><doi>10.5121/ijasuc.2014.5603</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The core motivations of deploying a sensor network for a specific application
come from the autonomy of sensors, their reduced size, and their capabilities
for computing and communicating in a short range. However, many challenges for
sensor networks still exist: minimizing energy consumption, and ensuring the
performance of communication that may be affected by many parameters. The work
described in this paper covers mainly the analysis of the impact of the
temperature and humidity variations on link quality of XM1000 operating under
TinyOS. Two-way ANOVA test has been applied and the obtained results show that
both the temperature and humidity variations impact RSSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01075</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01075</id><created>2015-01-05</created><authors><author><keyname>Abuzaghleh</keyname><forenames>Omar</forenames></author><author><keyname>Faezipour</keyname><forenames>Miad</forenames></author><author><keyname>Barkana</keyname><forenames>Buket D.</forenames></author></authors><title>Skincure: An Innovative Smart Phone-Based Application To Assist In
  Melanoma Early Detection And Prevention</title><categories>cs.CV cs.CY</categories><comments>appears in Signal &amp; Image Processing : An International Journal
  (SIPIJ) Vol.5, No.6, December 2014</comments><doi>10.5121/sipij.2014.5601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Melanoma spreads through metastasis, and therefore it has been proven to be
very fatal. Statistical evidence has revealed that the majority of deaths
resulting from skin cancer are as a result of melanoma. Further investigations
have shown that the survival rates in patients depend on the stage of the
infection; early detection and intervention of melanoma implicates higher
chances of cure. Clinical diagnosis and prognosis of melanoma is challenging
since the processes are prone to misdiagnosis and inaccuracies due to doctors
subjectivity. This paper proposes an innovative and fully functional
smart-phone based application to assist in melanoma early detection and
prevention. The application has two major components; the first component is a
real-time alert to help users prevent skin burn caused by sunlight; a novel
equation to compute the time for skin to burn is thereby introduced. The second
component is an automated image analysis module which contains image
acquisition, hair detection and exclusion, lesion segmentation, feature
extraction, and classification. The proposed system exploits PH2 Dermoscopy
image database from Pedro Hispano Hospital for development and testing
purposes. The image database contains a total of 200 dermoscopy images of
lesions, including normal, atypical, and melanoma cases. The experimental
results show that the proposed system is efficient, achieving classification of
the normal, atypical and melanoma images with accuracy of 96.3%, 95.7% and
97.5%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01076</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01076</id><created>2015-01-05</created><updated>2015-03-11</updated><authors><author><keyname>Du</keyname><forenames>Jian</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoli</forenames></author><author><keyname>Wu</keyname><forenames>Yishan</forenames></author></authors><title>The Effects of Research Level and Article Type on the Differences
  between Citation Metrics and F1000 Recommendations</title><categories>cs.DL</categories><comments>15 pages. arXiv admin note: text overlap with arXiv:1404.0359,
  arXiv:1303.3875 by other authors; Journal of the Association for Information
  Science and Technology, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  F1000 recommendations have been validated as a potential data source for
research evaluation, but reasons for differences between F1000 Article Factor
(FFa scores) and citations remain to be explored. By linking 28254 publications
in F1000 to citations in Scopus, we investigated the effect of research level
and article type on the internal consistency of assessments based on citations
and FFa scores. It turns out that research level has little impact, while
article type has big effect on the differences. These two measures are
significantly different for two groups: non-primary research or evidence-based
research publications are more highly cited rather than highly recommended,
however, translational research or transformative research publications are
more highly recommended by faculty members but gather relatively lower
citations. This can be expected because citation activities are usually
practiced by academic authors while the potential for scientific revolutions
and the suitability for clinical practice of an article should be investigated
from the practitioners' points of view. We conclude with a policy relevant
recommendation that the application of bibliometric approaches in research
evaluation procedures should include the proportion of three types of
publications: evidence-based research, transformative research, and
translational research. The latter two types are more suitable to be assessed
through peer review.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01079</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01079</id><created>2015-01-06</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author></authors><title>Note on Perfect Forests</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spanning subgraph $F$ of a graph $G$ is called perfect if $F$ is a forest,
the degree $d_F(x)$ of each vertex $x$ in $F$ is odd, and each tree of $F$ is
an induced subgraph of $G$. We provide a short proof of the following theorem
of A.D. Scott (Graphs &amp; Combin., 2001): a connected graph $G$ contains a
perfect forest if and only if $G$ has an even number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01083</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01083</id><created>2015-01-06</created><authors><author><keyname>Mohana</keyname><forenames>S. H.</forenames></author><author><keyname>Prabhakar</keyname><forenames>C. J.</forenames></author></authors><title>Stem-Calyx Recognition of an Apple using Shape Descriptors</title><categories>cs.CV</categories><comments>15 pages, 10 figures and 2 tables in Signal &amp; Image Processing : An
  International Journal (SIPIJ) Vol.5, No.6, December 2014</comments><doi>10.5121/sipij.2014.5602</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a novel method to recognize stem - calyx of an apple
using shape descriptors. The main drawback of existing apple grading techniques
is that stem - calyx part of an apple is treated as defects, this leads to poor
grading of apples. In order to overcome this drawback, we proposed an approach
to recognize stem-calyx and differentiated from true defects based on shape
features. Our method comprises of steps such as segmentation of apple using
grow-cut method, candidate objects such as stem-calyx and small defects are
detected using multi-threshold segmentation. The shape features are extracted
from detected objects using Multifractal, Fourier and Radon descriptor and
finally stem-calyx regions are recognized and differentiated from true defects
using SVM classifier. The proposed algorithm is evaluated using experiments
conducted on apple image dataset and results exhibit considerable improvement
in recognition of stem-calyx region compared to other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01084</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01084</id><created>2015-01-06</created><updated>2015-02-16</updated><authors><author><keyname>Huang</keyname><forenames>Cupjin</forenames></author><author><keyname>Tan</keyname><forenames>Zihan</forenames></author><author><keyname>Yang</keyname><forenames>Shenghao</forenames></author></authors><title>Upper Bound on Function Computation in Directed Acyclic Networks</title><categories>cs.IT math.IT</categories><comments>7 pages, full version of a conference paper accepted by ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Function computation in directed acyclic networks is considered, where a sink
node wants to compute a target function with the inputs generated at multiple
source nodes. The network links are error-free but capacity-limited, and the
intermediate network nodes perform network coding. The target function is
required to be computed with zero error. The computing rate of a network code
is measured by the average number of times that the target function can be
computed for one use of the network. We propose a cut-set bound on the
computing rate using an equivalence relation associated with the inputs of the
target function. Our bound holds for general target functions and network
topologies. We also show that our bound is tight for some special cases where
the computing capacity can be characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01086</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01086</id><created>2015-01-06</created><authors><author><keyname>Soundararajan</keyname><forenames>Vasanthakumar</forenames></author></authors><title>A Novel Design of a Parallel Machine Learnt Generational Garbage
  Collector</title><categories>cs.PL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Generational Garbage collection involves organizing the heap into
different divisions of memory space in-order to filter long-lived objects from
short-lived objects through moving the surviving object of each generation
Garbage Collection cycle to another memory space updating its age and
reclaiming space from the dead ones. The problem in this method is that the
longer an object is alive during its initial generations the longer the garbage
collector will have to deal with it by checking for its reachability from the
root and promoting it to other space divisions where as the ultimate goal of
the Garbage Collector is to reclaim memory from unreachable objects at a
minimal time possible. This paper is a proposal of a method where the lifetime
of every object getting into the heap will be predicted and will be placed in
heap accordingly for the garbage collector to deal more with reclaiming space
from dead objects and less in promoting the live ones to the higher level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01090</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01090</id><created>2015-01-06</created><authors><author><keyname>Mohana</keyname><forenames>S. H.</forenames></author><author><keyname>Prabhakar</keyname><forenames>C. J.</forenames></author></authors><title>A Novel Technique for Grading of Dates using Shape and Texture Features</title><categories>cs.CV</categories><comments>15 pages, 3 figures and 6 tables in Machine Learning and
  Applications: An International Journal (MLAIJ) Vol.1, No.2, December 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a novel method to grade the date fruits based on the
combination of shape and texture features. The method begins with reducing the
specular reflection and small noise using a bilateral filter. Threshold based
segmentation is performed for background removal and fruit part selection from
the given image. Shape features is extracted using the contour of the date
fruit and texture features are extracted using Curvelet transform and Local
Binary Pattern (LBP) from the selected date fruit region. Finally, combinations
of shape and texture features are fused to grade the dates into six grades.
k-Nearest Neighbour(k-NN) classifier yields the best grading rate compared to
other two classifiers such as Support Vector Machine (SVM) and Linear
Discriminant(LDA) classifiers. The experiment result shows that our technique
achieves highest accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01106</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01106</id><created>2015-01-06</created><authors><author><keyname>Golestani</keyname><forenames>Hossein Bakhshi</forenames></author><author><keyname>Joneidi</keyname><forenames>Mohsen</forenames></author><author><keyname>Sadeghi</keyname><forenames>Mostafa</forenames></author></authors><title>A Study on Clustering for Clustering Based Image De-Noising</title><categories>cs.CV</categories><comments>9 pages, 8 figures, Journal of Information Systems and
  Telecommunications (JIST)</comments><journal-ref>Journal of Information Systems and Telecommunications (JIST), vol.
  2, no. 4, pp. 196-204, December 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of de-noising of an image contaminated with
Additive White Gaussian Noise (AWGN) is studied. This subject is an open
problem in signal processing for more than 50 years. Local methods suggested in
recent years, have obtained better results than global methods. However by more
intelligent training in such a way that first, important data is more effective
for training, second, clustering in such way that training blocks lie in
low-rank subspaces, we can design a dictionary applicable for image de-noising
and obtain results near the state of the art local methods. In the present
paper, we suggest a method based on global clustering of image constructing
blocks. As the type of clustering plays an important role in clustering-based
de-noising methods, we address two questions about the clustering. The first,
which parts of the data should be considered for clustering? and the second,
what data clustering method is suitable for de-noising.? Then clustering is
exploited to learn an over complete dictionary. By obtaining sparse
decomposition of the noisy image blocks in terms of the dictionary atoms, the
de-noised version is achieved. In addition to our framework, 7 popular
dictionary learning methods are simulated and compared. The results are
compared based on two major factors: (1) de-noising performance and (2)
execution time. Experimental results show that our dictionary learning
framework outperforms its competitors in terms of both factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01109</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01109</id><created>2015-01-06</created><authors><author><keyname>Mashud</keyname><forenames>M. A. A.</forenames></author><author><keyname>Hossain</keyname><forenames>M. R.</forenames></author><author><keyname>Zaman</keyname><forenames>Mustari</forenames></author><author><keyname>Razzaque</keyname><forenames>M. A.</forenames></author></authors><title>PC Guided Automatic Vehicle System</title><categories>cs.OH</categories><comments>10 pages, International Journal on Cybernetics &amp; Informatics
  (IJCI);2014</comments><doi>10.5121/ijci.2014.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper is to design and develop an automatic
vehicle, fully controlled by a computer system. The vehicle designed in the
present work can move in a pre-determined path and work automatically without
the need of any human operator and it also controlled by human operator. Such a
vehicle is capable of performing wide variety of difficult tasks in space
research, domestic, scientific and industrial fields. For this purpose, an IBM
compatible PC with Pentium microprocessor has been used which performed the
function of the system controller. Its parallel printer port has been used as
data communication port to interface the vehicle. A suitable software program
has been developed for the system controller to send commands to the vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01118</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01118</id><created>2015-01-06</created><authors><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames><affiliation>INRIA</affiliation></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames><affiliation>INRIA</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>INRIA</affiliation></author></authors><title>*-Continuous Kleene $\omega$-Algebras</title><categories>cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define and study basic properties of *-continuous Kleene $\omega$-algebras
that involve a *-continuous Kleene algebra with a *-continuous action on a
semimodule and an infinite product operation that is also *-continuous. We show
that *-continuous Kleene $\omega$-algebras give rise to iteration
semiring-semimodule pairs. We show how our work can be applied to solve certain
energy problems for hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01138</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01138</id><created>2015-01-06</created><updated>2015-01-07</updated><authors><author><keyname>Li</keyname><forenames>Jiyou</forenames></author><author><keyname>Wan</keyname><forenames>Daqing</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author></authors><title>On the minimum distance of elliptic curve codes</title><categories>cs.IT math.CO math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the minimum distance of a linear code is one of the fundamental
problems in algorithmic coding theory. Vardy [14] showed that it is an \np-hard
problem for general linear codes. In practice, one often uses codes with
additional mathematical structure, such as AG codes. For AG codes of genus $0$
(generalized Reed-Solomon codes), the minimum distance has a simple explicit
formula. An interesting result of Cheng [3] says that the minimum distance
problem is already \np-hard (under \rp-reduction) for general elliptic curve
codes (ECAG codes, or AG codes of genus $1$). In this paper, we show that the
minimum distance of ECAG codes also has a simple explicit formula if the
evaluation set is suitably large (at least $2/3$ of the group order). Our
method is purely combinatorial and based on a new sieving technique from the
first two authors [8]. This method also proves a significantly stronger version
of the MDS (maximum distance separable) conjecture for ECAG codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01139</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01139</id><created>2015-01-06</created><authors><author><keyname>Taylor</keyname><forenames>M. B.</forenames></author><author><keyname>Boch</keyname><forenames>T.</forenames></author><author><keyname>Taylor</keyname><forenames>J.</forenames></author></authors><title>SAMP, the Simple Application Messaging Protocol: Letting applications
  talk to each other</title><categories>astro-ph.IM cs.SE</categories><comments>12 pages, 3 figures. Accepted for Virtual Observatory special issue
  of Astronomy and Computing</comments><doi>10.1016/j.ascom.2014.12.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SAMP, the Simple Application Messaging Protocol, is a hub-based communication
standard for the exchange of data and control between participating client
applications. It has been developed within the context of the Virtual
Observatory with the aim of enabling specialised data analysis tools to
cooperate as a loosely integrated suite, and is now in use by many and varied
desktop and web-based applications dealing with astronomical data. This paper
reviews the requirements and design principles that led to SAMP's
specification, provides a high-level description of the protocol, and discusses
some of its common and possible future usage patterns, with particular
attention to those factors that have aided its success in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01144</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01144</id><created>2015-01-06</created><updated>2016-01-02</updated><authors><author><keyname>Hamada</keyname><forenames>Katsuhiko</forenames></author><author><keyname>Mori</keyname><forenames>Hiromu</forenames></author><author><keyname>Shinoda</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Airborne Ultrasonic Tactile Display BCI</title><categories>q-bio.NC cs.HC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.4184</comments><acm-class>H.5.2, H.1.2</acm-class><journal-ref>Brain-Computer Interface Research - A State-of-the-Art Summary 4.
  SpringerBriefs in Electrical and Computer Engineering. 2015. p. 57-65</journal-ref><doi>10.1007/978-3-319-25190-5_6</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This chapter presents results of our project, which studied whether
contactless and airborne ultrasonic tactile display (AUTD) stimuli delivered to
a user's palms could serve as a platform for a brain computer interface (BCI)
paradigm. We used six palm positions to evoke combined somatosensory brain
responses to implement a novel contactless tactile BCI. This achievement was
awarded the top prize in the Annual BCI Research Award 2014 competition. This
chapter also presents a comparison with a classical attached vibrotactile
transducer-based BCI paradigm. Experiment results from subjects performing
online experiments validate the novel BCI paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01149</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01149</id><created>2015-01-06</created><authors><author><keyname>Wuest</keyname><forenames>Thorsten</forenames></author><author><keyname>Tinscher</keyname><forenames>Rainer</forenames></author><author><keyname>Porzel</keyname><forenames>Robert</forenames></author><author><keyname>Thoben</keyname><forenames>Klaus-Dieter</forenames></author></authors><title>Experimental Research Data Quality In Materials Science</title><categories>cs.DB cond-mat.mtrl-sci cs.DL</categories><journal-ref>International Journal of Advanced Information Technology, Vol. 4,
  No. 6, Dec. 2014</journal-ref><doi>10.5121/ijait.2014.4601</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In materials sciences, a large amount of research data is generated through a
broad spectrum of different experiments. As of today, experimental research
data including meta-data in materials science is often stored decentralized by
the researcher(s) conducting the experiments without generally accepted
standards on what and how to store data. The conducted research and experiments
often involve a considerable investment from public funding agencies that
desire the results to be made available in order to increase their impact. In
order to achieve the goal of citable and (openly) accessible materials science
experimental research data in the future, not only an adequate infrastructure
needs to be established but the question of how to measure the quality of the
experimental research data also to be addressed. In this publication, the
authors identify requirements and challenges towards a systematic methodology
to measure experimental research data quality prior to publication and derive
different approaches on that basis. These methods are critically discussed and
assessed by their contribution and limitations towards the set goals.
Concluding, a combination of selected methods is presented as a systematic,
functional and practical quality measurement and assurance approach for
experimental research data in materials science with the goal of supporting the
accessibility and dissemination of existing data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01152</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01152</id><created>2015-01-06</created><authors><author><keyname>Roman'kov</keyname><forenames>Vitali&#x12d;</forenames></author></authors><title>Linear decomposition attack on public key exchange protocols using
  semidirect products of (semi)groups</title><categories>cs.CR math.GR</categories><msc-class>20F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a linear decomposition attack based on the decomposition method
introduced by the author works by finding the exchanged secret keys in all main
protocols using semidirect products of (semi)grops proposed by Kahrobaei,
Shpilrain, Habeeb, Koupparis and Lam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01170</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01170</id><created>2015-01-06</created><authors><author><keyname>Delahaye</keyname><forenames>David</forenames><affiliation>CEDRIC, CNAM Paris</affiliation></author><author><keyname>Jacquel</keyname><forenames>M&#xe9;lanie</forenames></author></authors><title>Recovering Intuition from Automated Formal Proofs using Tableaux with
  Superdeduction</title><categories>cs.LO</categories><comments>see https://php.radford.edu/~ejmt/ContentIndex.php#v7n2</comments><proxy>ccsd</proxy><journal-ref>electronic Journal of Mathematics and Technology, 2013, 7 (2),
  pp.1 - 20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an automated deduction method which allows us to produce proofs
close to the human intuition and practice. This method is based on tableaux,
which generate more natural proofs than similar methods relying on clausal
forms, and uses the principles of superdeduction, among which the theory is
used to enrich the deduction system with new deduction rules. We present two
implementations of this method, which consist of extensions of the Zenon
automated theorem prover. The first implementation is a version dedicated to
the set theory of the B formal method, while the second implementation is a
generic version able to deal with any first order theory. We also provide
several examples of problems, which can be handled by these tools and which
come from different theories, such as the B set theory or theories of the TPTP
library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01178</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01178</id><created>2015-01-06</created><updated>2015-02-25</updated><authors><author><keyname>Negrevergne</keyname><forenames>Benjamin</forenames></author><author><keyname>Guns</keyname><forenames>Tias</forenames></author></authors><title>Constraint-based sequence mining using constraint programming</title><categories>cs.AI</categories><comments>In Integration of AI and OR Techniques in Constraint Programming
  (CPAIOR), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of constraint-based sequence mining is to find sequences of symbols
that are included in a large number of input sequences and that satisfy some
constraints specified by the user. Many constraints have been proposed in the
literature, but a general framework is still missing. We investigate the use of
constraint programming as general framework for this task. We first identify
four categories of constraints that are applicable to sequence mining. We then
propose two constraint programming formulations. The first formulation
introduces a new global constraint called exists-embedding. This formulation is
the most efficient but does not support one type of constraint. To support such
constraints, we develop a second formulation that is more general but incurs
more overhead. Both formulations can use the projected database technique used
in specialised algorithms. Experiments demonstrate the flexibility towards
constraint-based settings and compare the approach to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01181</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01181</id><created>2015-01-06</created><updated>2015-08-04</updated><authors><author><keyname>Vezhnevets</keyname><forenames>Alexander</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Object localization in ImageNet by looking out of the window</title><categories>cs.CV</categories><comments>in BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for annotating the location of objects in ImageNet.
Traditionally, this is cast as an image window classification problem, where
each window is considered independently and scored based on its appearance
alone. Instead, we propose a method which scores each candidate window in the
context of all other windows in the image, taking into account their similarity
in appearance space as well as their spatial relations in the image plane. We
devise a fast and exact procedure to optimize our scoring function over all
candidate windows in an image, and we learn its parameters using structured
output regression. We demonstrate on 92000 images from ImageNet that this
significantly improves localization over recent techniques that score windows
in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01184</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01184</id><created>2015-01-06</created><updated>2015-02-04</updated><authors><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Nielsen</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>&#x10c;edomir</forenames></author></authors><title>Interference Spins: Scheduling of Multiple Interfering Two-Way Wireless
  Links</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way is a dominant mode of communication in wireless systems. Departing
from the tradition to optimize each transmission direction separately, recent
work has demonstrated that, for time-division duplex (TDD) systems, optimizing
the schedule of the two transmission directions depending on traffic load and
interference condition leads to performance gains. In this letter, a general
network of multiple interfering two-way links is studied under the assumption
of a balanced load in the two directions for each link. Using the notion of
interference spin, we introduce an algebraic framework for the optimization of
two-way scheduling, along with an efficient optimization algorithm that is
based on the pruning of a properly defined topology graph and dynamic
programming. Numerical results demonstrate multi-fold rate gains with respect
to baseline solutions, especially for worst-case (5%-ile) rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01186</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01186</id><created>2015-01-06</created><updated>2016-01-27</updated><authors><author><keyname>Kalogeiton</keyname><forenames>Vicky</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Analysing domain shift factors between videos and images for object
  detection</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is one of the most important challenges in computer vision.
Object detectors are usually trained on bounding-boxes from still images.
Recently, video has been used as an alternative source of data. Yet, for a
given test domain (image or video), the performance of the detector depends on
the domain it was trained on. In this paper, we examine the reasons behind this
performance gap. We define and evaluate different domain shift factors: spatial
location accuracy, appearance diversity, image quality and aspect distribution.
We examine the impact of these factors by comparing performance before and
after factoring them out. The results show that all four factors affect the
performance of the detectors and their combined effect explains nearly the
whole performance gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01193</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01193</id><created>2015-01-06</created><authors><author><keyname>Zouinkhi</keyname><forenames>Ahmed</forenames></author><author><keyname>Mekki</keyname><forenames>Kais</forenames></author><author><keyname>Abdelkrim</keyname><forenames>Mohamed Naceur</forenames></author></authors><title>Application and network layers design for wireless sensor network to
  supervise chemical active product warehouse</title><categories>cs.NI</categories><comments>20 pages in International Journal of Computer Science, Engineering
  and Applications (IJCSEA), Vol.4, No.6, December 2014, pp. 53-72</comments><doi>10.5121/ijcsea.2014.4605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks have profound effects on many application fields
like security management which need an immediate and fast system reaction.
Indeed, the monitoring of a dangerous product warehouse is a major issue in
chemical industry field. This paper describes the design of chemical warehouse
security system using the concept of active products and wireless sensor
networks. A security application layer is developed to supervise and exchange
messages between nodes and the control center to prevent industrial accident.
Different security rules are proposed on this layer to monitor the internal
state and incompatible products distance. If a critical event is detected, the
application generates alert message which need a short end to end delay and low
packet loss rate constraints by network layer. Thus, a QoS routing protocol is
also developed in the network layer. The proposed solution is implemented in
Castalia/OMNeT++ simulator. Simulation results show that the system reacts
perfectly for critical event and can meet the QoS constraints of alert message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01199</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01199</id><created>2015-01-06</created><updated>2016-01-25</updated><authors><author><keyname>Sitova</keyname><forenames>Zdenka</forenames></author><author><keyname>Sedenka</keyname><forenames>Jaroslav</forenames></author><author><keyname>Yang</keyname><forenames>Qing</forenames></author><author><keyname>Peng</keyname><forenames>Ge</forenames></author><author><keyname>Zhou</keyname><forenames>Gang</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author><author><keyname>Balagani</keyname><forenames>Kiran</forenames></author></authors><title>HMOG: New Behavioral Biometric Features for Continuous Authentication of
  Smartphone Users</title><categories>cs.CR</categories><journal-ref>IEEE Transactions on Information Forensics and Security, PP(99):
  1-1,2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Hand Movement, Orientation, and Grasp (HMOG), a set of
behavioral features to continuously authenticate smartphone users. HMOG
features unobtrusively capture subtle micro-movement and orientation dynamics
resulting from how a user grasps, holds, and taps on the smartphone. We
evaluated authentication and biometric key generation (BKG) performance of HMOG
features on data collected from 100 subjects typing on a virtual keyboard. Data
was collected under two conditions: sitting and walking. We achieved
authentication EERs as low as 7.16% (walking) and 10.05% (sitting) when we
combined HMOG, tap, and keystroke features. We performed experiments to
investigate why HMOG features perform well during walking. Our results suggest
that this is due to the ability of HMOG features to capture distinctive body
movements caused by walking, in addition to the hand-movement dynamics from
taps. With BKG, we achieved EERs of 15.1% using HMOG combined with taps. In
comparison, BKG using tap, key hold, and swipe features had EERs between 25.7%
and 34.2%. We also analyzed the energy consumption of HMOG feature extraction
and computation. Our analysis shows that HMOG features extracted at 16Hz sensor
sampling rate incurred a minor overhead of 7.9% without sacrificing
authentication accuracy. Two points distinguish our work from current
literature: 1) we present the results of a comprehensive evaluation of three
types of features (HMOG, keystroke, and tap) and their combinations under the
same experimental conditions, and 2) we analyze the features from three
perspectives (authentication, BKG, and energy consumption on smartphones).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01202</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01202</id><created>2015-01-06</created><updated>2015-01-09</updated><authors><author><keyname>Mattern</keyname><forenames>Christopher</forenames></author></authors><title>On Probability Estimation by Exponential Smoothing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probability estimation is essential for every statistical data compression
algorithm. In practice probability estimation should be adaptive, recent
observations should receive a higher weight than older observations. We present
a probability estimation method based on exponential smoothing that satisfies
this requirement and runs in constant time per letter. Our main contribution is
a theoretical analysis in case of a binary alphabet for various smoothing rate
sequences: We show that the redundancy w.r.t. a piecewise stationary model with
$s$ segments is $O\left(s\sqrt n\right)$ for any bit sequence of length $n$, an
improvement over redundancy $O\left(s\sqrt{n\log n}\right)$ of previous
approaches with similar time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01203</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01203</id><created>2015-01-06</created><authors><author><keyname>Majumder</keyname><forenames>Saikat</forenames></author><author><keyname>Verma</keyname><forenames>Shrish</forenames></author></authors><title>Iterative network-channel decoding with cooperative space-time
  transmission</title><categories>cs.IT cs.MM math.IT</categories><comments>11 pages in International Journal of Ad hoc, Sensor &amp; Ubiquitous
  Computing (IJASUC) Vol.5, No.6, December 2014</comments><doi>10.5121/ijasuc.2014.5602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most efficient methods of exploiting space diversity for portable
wireless devices is cooperative communication utilizing space-time block codes.
In cooperative communication, users besides communicating their own
information, also relay the information of other users. In this paper we
investigate a scheme where cooperation is achieved using two methods, namely,
distributed space-time coding and network coding. Two cooperating users utilize
Alamouti space time code for inter-user cooperation and in addition utilize a
third relay which performs network coding. The third relay does not have any of
its information to be sent. In this paper we propose a scheme utilizing
convolutional code based network coding, instead of conventional XOR based
network code and utilize iterative joint network-channel decoder for efficient
decoding. Extrinsic information transfer (EXIT) chart analysis is performed to
investigate the convergence property of the proposed decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01207</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01207</id><created>2015-01-05</created><authors><author><keyname>Hsieh</keyname><forenames>Samuel C.</forenames></author></authors><title>Two Answers to a Common Question on Diagonalization</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common question from students on the usual diagonalization proof for the
uncountability of the set of real numbers is: when a representation of real
numbers, such as the decimal expansions of real numbers, allows us to use the
diagonalization argument to prove that the set of real numbers is uncountable,
why can't we similarly apply the diagonalization argument to rational numbers
in the same representation? why doesn't the argument similarly prove that the
set of rational numbers is uncountable too? We consider two answers to this
question. We first discuss an answer that is based on the familiar decimal
expansions. We then present an unconventional answer that is based on continued
fractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01209</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01209</id><created>2014-12-11</created><authors><author><keyname>Gharehshiran</keyname><forenames>Omid Namvar</forenames></author><author><keyname>Hoiles</keyname><forenames>William</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author></authors><title>Reinforcement Learning and Nonparametric Detection of Game-Theoretic
  Equilibrium Play in Social Networks</title><categories>cs.GT cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies two important signal processing aspects of equilibrium
behavior in non-cooperative games arising in social networks, namely,
reinforcement learning and detection of equilibrium play. The first part of the
paper presents a reinforcement learning (adaptive filtering) algorithm that
facilitates learning an equilibrium by resorting to diffusion cooperation
strategies in a social network. Agents form homophilic social groups, within
which they exchange past experiences over an undirected graph. It is shown
that, if all agents follow the proposed algorithm, their global behavior is
attracted to the correlated equilibria set of the game. The second part of the
paper provides a test to detect if the actions of agents are consistent with
play from the equilibrium of a concave potential game. The theory of revealed
preference from microeconomics is used to construct a non-parametric decision
test and statistical test which only require the probe and associated actions
of agents. A stochastic gradient algorithm is given to optimize the probe in
real time to minimize the Type-II error probabilities of the detection test
subject to specified Type-I error probability. We provide a real-world example
using the energy market, and a numerical example to detect malicious agents in
an online social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01239</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01239</id><created>2015-01-06</created><updated>2015-04-30</updated><authors><author><keyname>Zhao</keyname><forenames>Han</forenames></author><author><keyname>Melibari</keyname><forenames>Mazen</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author></authors><title>On the Relationship between Sum-Product Networks and Bayesian Networks</title><categories>cs.AI stat.ML</categories><comments>Full version of the same paper to appear at ICML-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish some theoretical connections between Sum-Product
Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be
converted into a BN in linear time and space in terms of the network size. The
key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent
the local conditional probability distributions at each node in the resulting
BN by exploiting context-specific independence (CSI). The generated BN has a
simple directed bipartite graphical structure. We show that by applying the
Variable Elimination algorithm (VE) to the generated BN with ADD
representations, we can recover the original SPN where the SPN can be viewed as
a history record or caching of the VE inference process. To help state the
proof clearly, we introduce the notion of {\em normal} SPN and present a
theoretical analysis of the consistency and decomposability properties. We
conclude the paper with some discussion of the implications of the proof and
establish a connection between the depth of an SPN and a lower bound of the
tree-width of its corresponding BN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01242</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01242</id><created>2015-01-06</created><updated>2015-01-12</updated><authors><author><keyname>Heim</keyname><forenames>Eric</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Berger</keyname><forenames>Matthew</forenames><affiliation>Air Force Research Laboratory, Information Directorate</affiliation></author><author><keyname>Seversky</keyname><forenames>Lee M.</forenames><affiliation>Air Force Research Laboratory, Information Directorate</affiliation></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames><affiliation>University of Pittsburgh</affiliation></author></authors><title>Efficient Online Relative Comparison Kernel Learning</title><categories>cs.LG</categories><comments>Extended version of the paper appearing in The Proceedings of the
  2015 SIAM International Conference on Data Mining (SDM15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a kernel matrix from relative comparison human feedback is an
important problem with applications in collaborative filtering, object
retrieval, and search. For learning a kernel over a large number of objects,
existing methods face significant scalability issues inhibiting the application
of these methods to settings where a kernel is learned in an online and timely
fashion. In this paper we propose a novel framework called Efficient online
Relative comparison Kernel LEarning (ERKLE), for efficiently learning the
similarity of a large set of objects in an online manner. We learn a kernel
from relative comparisons via stochastic gradient descent, one query response
at a time, by taking advantage of the sparse and low-rank properties of the
gradient to efficiently restrict the kernel to lie in the space of positive
semidefinite matrices. In addition, we derive a passive-aggressive online
update for minimally satisfying new relative comparisons as to not disrupt the
influence of previously obtained comparisons. Experimentally, we demonstrate a
considerable improvement in speed while obtaining improved or comparable
accuracy compared to current methods in the online learning setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01243</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01243</id><created>2015-01-06</created><authors><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author><author><keyname>Ramirez</keyname><forenames>Javier</forenames></author><author><keyname>da Cunha</keyname><forenames>Iria</forenames></author></authors><title>Un r\'esumeur \`a base de graphes, ind\'ep\'endant de la langue</title><categories>cs.CL</categories><comments>8 pages, in French, 2 figures; International Workshop on African
  Human Language Technologies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present REG, a graph-based approach for study a fundamental
problem of Natural Language Processing (NLP): the automatic text summarization.
The algorithm maps a document as a graph, then it computes the weight of their
sentences. We have applied this approach to summarize documents in three
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01252</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01252</id><created>2015-01-06</created><authors><author><keyname>Mathias</keyname><forenames>Mayeul</forenames></author><author><keyname>Moussa</keyname><forenames>Assema</forenames></author><author><keyname>Zhou</keyname><forenames>Fen</forenames></author><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author><author><keyname>Poli</keyname><forenames>Marie-Sylvie</forenames></author><author><keyname>Josselin</keyname><forenames>Didier</forenames></author><author><keyname>El-B&#xe8;ze</keyname><forenames>Marc</forenames></author><author><keyname>Linhares</keyname><forenames>Andr&#xe9;a Carneiro</forenames></author><author><keyname>Rigat</keyname><forenames>Francoise</forenames></author></authors><title>Optimisation using Natural Language Processing: Personalized Tour
  Recommendation for Museums</title><categories>cs.AI cs.CL</categories><comments>8 pages, 4 figures; Proceedings of the 2014 Federated Conference on
  Computer Science and Information Systems pp. 439-446</comments><doi>10.15439/2014F336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method to provide personalized tour recommendation
for museum visits. It combines an optimization of preference criteria of
visitors with an automatic extraction of artwork importance from museum
information based on Natural Language Processing using textual energy. This
project includes researchers from computer and social sciences. Some results
are obtained with numerical experiments. They show that our model clearly
improves the satisfaction of the visitor who follows the proposed tour. This
work foreshadows some interesting outcomes and applications about on-demand
personalized visit of museums in a very near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01254</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01254</id><created>2015-01-06</created><authors><author><keyname>Jayaweera</keyname><forenames>A. J. P. M. P.</forenames></author><author><keyname>Dias</keyname><forenames>N. G. J.</forenames></author></authors><title>Unknown Words Analysis in POS tagging of Sinhala Language</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Part of Speech (POS) is a very vital topic in Natural Language Processing
(NLP) task in any language, which involves analysing the construction of the
language, behaviours and the dynamics of the language, the knowledge that could
be utilized in computational linguistics analysis and automation applications.
In this context, dealing with unknown words (words do not appear in the lexicon
referred as unknown words) is also an important task, since growing NLP systems
are used in more and more new applications. One aid of predicting lexical
categories of unknown words is the use of syntactical knowledge of the
language. The distinction between open class words and closed class words
together with syntactical features of the language used in this research to
predict lexical categories of unknown words in the tagging process. An
experiment is performed to investigate the ability of the approach to parse
unknown words using syntactical knowledge without human intervention. This
experiment shows that the performance of the tagging process is enhanced when
word class distinction is used together with syntactic rules to parse sentences
containing unknown words in Sinhala language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01266</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01266</id><created>2015-01-06</created><updated>2016-02-15</updated><authors><author><keyname>Oeding</keyname><forenames>Luke</forenames></author></authors><title>The Quadrifocal Variety</title><categories>math.AG cs.CV</categories><comments>20 pages, updates to references, introduction, and main conjecture</comments><msc-class>Primary 13Pxx, 14Qxx, Secondary 15A69, 15A72, 68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-view Geometry is reviewed from an Algebraic Geometry perspective and
multi-focal tensors are constructed as equivariant projections of the
Grassmannian. A connection to the principal minor assignment problem is made by
considering several flatlander cameras. The ideal of the quadrifocal variety is
computed up to degree 8 (and partially in degree 9) using the representations
of $\operatorname{GL}(3)^{\times 4}$ in the polynomial ring on the space of $3
\times 3 \times 3 \times 3$ tensors. Further representation-theoretic analysis
gives a lower bound for the number of minimal generators. We conjecture that
the ideal of the quadrifocal variety is minimally generated in degree at most
9.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01270</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01270</id><created>2015-01-06</created><authors><author><keyname>Chua</keyname><forenames>Freddy Chong Tat</forenames></author><author><keyname>Oentaryo</keyname><forenames>Richard J.</forenames></author><author><keyname>Lim</keyname><forenames>Ee-Peng</forenames></author></authors><title>Using Linear Dynamical Topic Model for Inferring Temporal Social
  Correlation in Latent Space</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abundance of online user data has led to a surge of interests in
understanding the dynamics of social relationships using computational methods.
Utilizing users' items adoption data, we develop a new method to compute the
Granger-causal (GC) relationships among users. In order to handle the high
dimensional and sparse nature of the adoption data, we propose to model the
relationships among users in latent space instead of the original data space.
We devise a Linear Dynamical Topic Model (LDTM) that can capture the dynamics
of the users' items adoption behaviors in latent (topic) space. Using the time
series of temporal topic distributions learned by LDTM, we conduct Granger
causality tests to measure the social correlation relationships between pairs
of users. We call the combination of our LDTM and Granger causality tests as
Temporal Social Correlation. By conducting extensive experiments on
bibliographic data, where authors are analogous to users, we show that the
ordering of authors' name on their publications plays a statistically
significant role in the interaction of research topics among the authors. We
also present a case study to illustrate the correlational relationships between
pairs of authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01273</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01273</id><created>2015-01-06</created><authors><author><keyname>Akhtar</keyname><forenames>Nadeem</forenames></author><author><keyname>Ghori</keyname><forenames>Aisha Shafique</forenames></author><author><keyname>Salamat</keyname><forenames>Nadeem</forenames></author></authors><title>Requirement analysis, Architectural design and Formal verification of a
  multi-agent based University Information Management System</title><categories>cs.SE</categories><comments>9 pages, journal paper in International Journal of Computer Science &amp;
  Information Technology (IJCSIT), December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach based on the analysis, design, and formal
verification of a multi-agent based university Information Management System
(IMS). University IMS accesses information, creates reports and facilitates
teachers as well as students. An orchestrator agent manages the coordination
between all agents. It also manages the database connectivity for the whole
system. The proposed IMS is based on BDI agent architecture, which models the
system based on belief, desire, and intentions. The correctness properties of
safety and liveness are specified by First-order predicate logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01276</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01276</id><created>2015-01-06</created><authors><author><keyname>Awedh</keyname><forenames>Mohammad</forenames></author><author><keyname>Mueen</keyname><forenames>Ahmed</forenames></author><author><keyname>Zafar</keyname><forenames>Bassam</forenames></author><author><keyname>Manzoor</keyname><forenames>Umar</forenames></author></authors><title>Using Socrative and Smartphones for the support of collaborative
  learning</title><categories>cs.CY</categories><comments>appears in International Journal on Integrating Technology in
  Education (IJITE) Vol.3, No.4, December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of new technologies in the classrooms opens new possibilities
for the teaching and learning process. Technologies such as student response
system (e.g. Clicker) are getting popularity among teachers due to its effects
on student learning performance. In this study, our primary objective is to
investigate the effect of Socrative with combination of smartphones on student
learning performance. We also observed the benefits of interactivity between
the teacher and the students and among classmates, which positively influences
collaborative learning and engagement of students in the class. We test these
relationships experimentally in a community college class environment using
data from a survey answered by students in information technology associate
degree. The results of our study reveal that collaborative learning and
engagement of student in the class improves student learning performance. We
highly recommend these tools in educational settings to support the learning
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01294</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01294</id><created>2015-01-06</created><authors><author><keyname>Abuelenin</keyname><forenames>Sherif M.</forenames></author></authors><title>Design and simulation of a hybrid controller for a multi-input
  multi-output magnetic suspension system</title><categories>cs.SY</categories><comments>6 pages, 10 figures, 6 tables</comments><journal-ref>In Proceedings of IEEE International Conference on Fuzzy Systems,
  2009. FUZZ-IEEE 2009, pp. 676-681. IEEE, 2009</journal-ref><doi>10.1109/FUZZY.2009.5277404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a Fuzzy Logic control approach designed to stabilize
a multi-input multi-output magnetic suspension system. The system has four
cubic floaters and four actuators that apply magnetic forces on the floaters,
the suspension is performed by changing the voltages applied on the actuators,
hence changing their currents, producing vertical magnetic forces that balance
with the gravitational force. A fuzzy logic controller is used to control each
actuator; the system is nonlinear and sensitive to initial conditions. Another
fuzzy logic controller is used as a supervisory controller in order to increase
the dynamic range of the system, enabling it to stabilize the floaters when the
initial displacements are relatively big. Another design consideration was to
keep the four floaters in the same plane as much as possible, to perform that
task, a PD controller was set to modulate the currents of the four actuators in
order to minimize an error signal measuring the relative vertical displacement
of all the four floaters. Simulation results show that the designed control
scheme stabilized the system for the design constrains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01300</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01300</id><created>2014-12-20</created><authors><author><keyname>Paulson</keyname><forenames>Elisabeth</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author></authors><title>Computational Complexity of the Minimum State Probabilistic Finite State
  Learning Problem on Finite Data Sets</title><categories>cs.FL cs.DS math.OC</categories><msc-class>90C27, 90C60, 90C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of determining a minimum state
probabilistic finite state machine capable of generating statistically
identical symbol sequences to samples provided. This problem is qualitatively
similar to the classical Hidden Markov Model problem and has been studied from
a practical point of view in several works beginning with the work presented
in: Shalizi, C.R., Shalizi, K.L., Crutchfield, J.P. (2002) [arXiv:cs/0210025].
In this paper, we show that the underlying problem is $\mathrm{NP}$-hard and
thus all existing polynomial time algorithms must be approximations on finite
data sets. Using our $\mathrm{NP}$-hardness proof, we show how to construct a
provably correct algorithm for constructing a minimum state probabilistic
finite state machine given data and empirically study its running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01301</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01301</id><created>2015-01-06</created><authors><author><keyname>Jacquel</keyname><forenames>M&#xe9;lanie</forenames><affiliation>CEDRIC, CNAM Paris, INRIA</affiliation></author><author><keyname>Berkani</keyname><forenames>Karim</forenames><affiliation>CEDRIC, CNAM Paris, INRIA</affiliation></author><author><keyname>Delahaye</keyname><forenames>David</forenames><affiliation>CEDRIC, CNAM Paris, INRIA</affiliation></author><author><keyname>Dubois</keyname><forenames>Catherine</forenames><affiliation>CEDRIC, INRIA, ENSIIE</affiliation></author></authors><title>Tableaux Modulo Theories Using Superdeduction</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1501.01170</comments><proxy>ccsd</proxy><journal-ref>Global Journal of Advanced Software Engineering (GJASE), Avanti
  Publishers, 2014, 1, pp.1 - 13</journal-ref><doi>10.1007/978-3-642-31365-3_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method that allows us to develop tableaux modulo theories using
the principles of superdeduction, among which the theory is used to enrich the
deduction system with new deduction rules. This method is presented in the
framework of the Zenon automated theorem prover, and is applied to the set
theory of the B method. This allows us to provide another prover to Atelier B,
which can be used to verify B proof rules in particular. We also propose some
benchmarks, in which this prover is able to automatically verify a part of the
rules coming from the database maintained by Siemens IC-MOL. Finally, we
describe another extension of Zenon with superdeduction, which is able to deal
with any first order theory, and provide a benchmark coming from the TPTP
library, which contains a large set of first order problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01318</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01318</id><created>2015-01-06</created><authors><author><keyname>Odeh</keyname><forenames>Ashraf</forenames></author><author><keyname>Abu-Errub</keyname><forenames>Aymen</forenames></author><author><keyname>Shambour</keyname><forenames>Qusai</forenames></author><author><keyname>Turab</keyname><forenames>Nidal</forenames></author></authors><title>Arabic Text Categorization Algorithm using Vector Evaluation Method</title><categories>cs.IR cs.CL</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 6, December 2014</journal-ref><doi>10.5121/ijcsit.2014.6606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text categorization is the process of grouping documents into categories
based on their contents. This process is important to make information
retrieval easier, and it became more important due to the huge textual
information available online. The main problem in text categorization is how to
improve the classification accuracy. Although Arabic text categorization is a
new promising field, there are a few researches in this field. This paper
proposes a new method for Arabic text categorization using vector evaluation.
The proposed method uses a categorized Arabic documents corpus, and then the
weights of the tested document's words are calculated to determine the document
keywords which will be compared with the keywords of the corpus categorizes to
determine the tested document's best category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01321</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01321</id><created>2015-01-06</created><authors><author><keyname>Pereira</keyname><forenames>Silas Santiago Lopes</forenames></author><author><keyname>Maia</keyname><forenames>Jos&#xe9; Everardo Bessa</forenames></author><author><keyname>Silva</keyname><forenames>Jorge Luiz de Castro e</forenames></author></authors><title>ITCM: A Real Time Internet Traffic Classifier Monitor</title><categories>cs.NI cs.LG</categories><comments>16 pages, 3 figures, 7 tables, International Journal of Computer
  Science &amp; Information Technology (IJCSIT) Vol 6, No 6, December 2014</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 6, December 2014</journal-ref><doi>10.5121/ijcsit.2014.6602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continual growth of high speed networks is a challenge for real-time
network analysis systems. The real time traffic classification is an issue for
corporations and ISPs (Internet Service Providers). This work presents the
design and implementation of a real time flow-based network traffic
classification system. The classifier monitor acts as a pipeline consisting of
three modules: packet capture and pre-processing, flow reassembly, and
classification with Machine Learning (ML). The modules are built as concurrent
processes with well defined data interfaces between them so that any module can
be improved and updated independently. In this pipeline, the flow reassembly
function becomes the bottleneck of the performance. In this implementation, was
used a efficient method of reassembly which results in a average delivery delay
of 0.49 seconds, approximately. For the classification module, the performances
of the K-Nearest Neighbor (KNN), C4.5 Decision Tree, Naive Bayes (NB), Flexible
Naive Bayes (FNB) and AdaBoost Ensemble Learning Algorithm are compared in
order to validate our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01323</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01323</id><created>2015-01-05</created><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author></authors><title>Recent Developments in Cloud Based Systems: State of Art</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is the new buzzword in the head of the techies round the
clock these days. The importance and the different applications of cloud
computing are overwhelming and thus, it is a topic of huge significance. It
provides several astounding features like Multitenancy, on demand service, pay
per use etc. This manuscript presents an exhaustive survey on cloud computing
technology and potential research issues in cloud computing that needs to be
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01327</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01327</id><created>2015-01-06</created><authors><author><keyname>Bandi</keyname><forenames>Rama Krishna</forenames></author><author><keyname>Bhaintwal</keyname><forenames>Maheshanand</forenames></author></authors><title>Cyclic codes over $\mathbb{Z}_4+u\mathbb{Z}_4$</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1412.3751</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have studied cyclic codes over the ring
$R=\mathbb{Z}_4+u\mathbb{Z}_4$, $u^2=0$. We have considered cyclic codes of odd
lengths. A sufficient condition for a cyclic code over $R$ to be a
$\mathbb{Z}_4$-free module is presented. We have provided the general form of
the generators of a cyclic code over $R$ and determined a formula for the ranks
of such codes. In this paper we have mainly focused on principally generated
cyclic codes of odd length over $R$. We have determined a necessary condition
and a sufficient condition for cyclic codes of odd lengths over $R$ to be
$R$-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01331</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01331</id><created>2015-01-06</created><authors><author><keyname>Maximov</keyname><forenames>Yura</forenames></author></authors><title>DNF complexity of complete boolean functions</title><categories>math.CO cs.CC</categories><comments>19 pages, 1 figure, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyse the complexity of boolean functions takes value 0 on
a sufficiently small number of points. For many functions this leads to the
analysis of a single function attains 0 only on unsigned representation of
numbers from 1 to d for various d. Here we obtain a tight bounds on the DNF
complexity of complete functions in terms of the number of literals and
conjunctions. The method is based on a certain efficient approximation of the
hypercube covering problem related to DNF complexity of a given boolean
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01345</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01345</id><created>2015-01-06</created><authors><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>A General Utility Optimization Framework for Energy Harvesting Based
  Wireless Communications</title><categories>cs.IT math.IT</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, wireless communication systems are expected to achieve more
cost-efficient and sustainable operations by replacing conventional fixed power
supplies such as batteries with energy harvesting (EH) devices, which could
provide electric energy from renewable energy sources (e.g., solar and wind).
Such EH power supplies, however, are random and instable in nature, and as a
result impose new challenges on reliable communication design and have
triggered substantial research interests in EH based wireless communications.
Building upon existing works, in this article, we develop a general
optimization framework to maximize the utility of EH wireless communication
systems. Our framework encapsulates a variety of design problems, such as
throughput maximization and outage probability minimization in single-user and
multiuser setups, and provides useful guidelines to the practical design of
general EH based communication systems with different assumptions over the
knowledge of time-varying wireless channels and EH rates at the transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01348</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01348</id><created>2015-01-06</created><authors><author><keyname>Zamparo</keyname><forenames>Lee</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaolei</forenames></author></authors><title>Deep Autoencoders for Dimensionality Reduction of High-Content Screening
  Data</title><categories>cs.LG</categories><comments>5 pages, 3 figures. Submitted to MLCB 2014 (NIPS workshop, Machine
  Learning in Computational Biology)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-content screening uses large collections of unlabeled cell image data to
reason about genetics or cell biology. Two important tasks are to identify
those cells which bear interesting phenotypes, and to identify sub-populations
enriched for these phenotypes. This exploratory data analysis usually involves
dimensionality reduction followed by clustering, in the hope that clusters
represent a phenotype. We propose the use of stacked de-noising auto-encoders
to perform dimensionality reduction for high-content screening. We demonstrate
the superior performance of our approach over PCA, Local Linear Embedding,
Kernel PCA and Isomap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01352</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01352</id><created>2015-01-06</created><updated>2015-02-01</updated><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Zhang</keyname><forenames>Liang</forenames></author></authors><title>Iso-Orthogonality and Type II Duadic Constacyclic Codes</title><categories>cs.IT math.IT</categories><msc-class>12E20, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalizing even-like duadic cyclic codes and Type-II duadic negacyclic
codes, we introduce even-like (i.e.,Type-II) and odd-like duadic constacyclic
codes, and study their properties and existence. We show that even-like duadic
constacyclic codes are isometrically orthogonal, and the duals of even-like
duadic constacyclic codes are odd-like duadic constacyclic codes. We exhibit
necessary and sufficient conditions for the existence of even-like duadic
constacyclic codes. A class of even-like duadic constacyclic codes which are
alternant MDS-codes is constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01360</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01360</id><created>2015-01-06</created><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Shi</keyname><forenames>Minjia</forenames></author><author><keyname>Wu</keyname><forenames>Tingting</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>On double cyclic codes over Z_4</title><categories>cs.IT math.IT math.RA</categories><comments>16</comments><msc-class>11T71, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R=\mathbb{Z}_4$ be the integer ring mod $4$. A double cyclic code of
length $(r,s)$ over $R$ is a set that can be partitioned into two parts that
any cyclic shift of the coordinates of both parts leaves invariant the code.
These codes can be viewed as $R[x]$-submodules of $R[x]/(x^r-1)\times
R[x]/(x^s-1)$. In this paper, we determine the generator polynomials of this
family of codes as $R[x]$-submodules of $R[x]/(x^r-1)\times R[x]/(x^s-1)$.
Further, we also give the minimal generating sets of this family of codes as
$R$-submodules of $R[x]/(x^r-1)\times R[x]/(x^s-1)$. Some optimal or suboptimal
nonlinear binary codes are obtained from this family of codes. Finally, we
determine the relationship of generators between the double cyclic code and its
dual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01361</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01361</id><created>2015-01-06</created><authors><author><keyname>Liu</keyname><forenames>Changhchang</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author></authors><title>LinkMirage: How to Anonymize Links in Dynamic Social Systems</title><categories>cs.SI</categories><comments>19 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network based trust relationships present a critical foundation for
designing trustworthy systems, such as Sybil defenses, secure routing, and
anonymous/censorshipresilient communications. A key issue in the design of such
systems, is the revelation of users' trusted social contacts to an
adversary-information that is considered sensitive in today's society.
  In this work, we focus on the challenge of preserving the privacy of users'
social contacts, while still enabling the design of social trust based
applications. First, we propose LinkMirage, a community detection based
algorithm for anonymizing links in social network topologies; LinkMirage
preserves community structures in the social topology while anonymizing links
within the communities. LinkMirage considers the evolution of the social
network topologies, and minimizes privacy leakage due to temporal dynamics of
the system.
  Second, we define metrics for quantifying the privacy and utility of a time
series of social topologies with anonymized links. We analyze the privacy and
utility provided by LinkMirage both theoretically, as well as using real world
social network topologies: a Facebook dataset with 870K links and a large-scale
Google+ dataset with 940M links. We find that our approach significantly
outperforms the existing state-of-art.
  Finally, we demonstrate the applicability of LinkMirage in real-world
applications such as Sybil defenses, reputation systems, anonymity systems and
vertex anonymity. We also prototype LinkMirage as a Facebook application such
that real world systems can bootstrap privacy-preserving trust relationships
without the cooperation of the OSN operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01363</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01363</id><created>2015-01-06</created><authors><author><keyname>Volkstorf</keyname><forenames>Charles</forenames></author></authors><title>Program Synthesis from Axiomatic Proof of Correctness</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program Synthesis is the mapping of a specification of what a computer
program is supposed to do, into a computer program that does what the
specification says to do. This is equivalent to constructing any computer
program and a sound proof that it meets the given specification.
  We axiomatically prove statements of the form: program PROG meets
specification SPEC. We derive 7 axioms from the definition of the PHP
programming language in which the programs are to be written. For each
primitive function or process described, we write a program that uses only that
feature (function or process), and we have an axiom that this program meets the
specification described. Generic ways to alter or combine programs, that meet
known specifications, into new programs that meet known specifications, are our
7 rules of inference.
  To efficiently prove statements that some program meets a given
specification, we work backwards from the specification. We apply the inverses
of the rules to the specifications that we must meet, until we reach axioms
that are combined by these rules to prove that a particular program meets the
given specification. Due to their distinct nature, typically few inverse rules
apply. To avoid complex wff and program manipulation algorithms, we advocate
the use of simple table maintenance and look-up functions to simulate these
complexities as a prototype.
  Examples Include:
  &quot;$B=FALSE ; for ($a=1;!($j&lt;$a);++$a){ $A=FALSE ; if (($a*$i)==$j) $A=TRUE ;
if ($A) $B=TRUE ; } ; echo $B ;&quot; and &quot;echo ($j % $i) == 0&quot; : Is one number a
factor of another?
  &quot;for ($a=1 ; !($i&lt;$a) ;++$a) {if (($i%$a) == 0) echo $a ; }&quot; : List the
factors of I.
  &quot;$A=FALSE ; for ($a=1;$a&lt;$i;++$a){ if (1&lt;$a) { if (($i % $a) == 0) $A=TRUE ;
} ; } ; echo (!($A)) &amp;&amp; (!($i&lt;2)) ;&quot; : Is I a prime number?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01364</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01364</id><created>2015-01-06</created><authors><author><keyname>Vaitheeswaran</keyname><forenames>S. M.</forenames></author><author><keyname>K.</keyname><forenames>Bharath M.</forenames></author><author><keyname>M</keyname><forenames>Gokul</forenames></author></authors><title>Leader Follower Formation Control of Ground Vehicles Using Camshift
  Based Guidance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous ground vehicles have been designed for the purpose of that relies
on ranging and bearing information received from forward looking camera on the
Formation control . A visual guidance control algorithm is designed where real
time image processing is used to provide feedback signals. The vision subsystem
and control subsystem work in parallel to accomplish formation control. A
proportional navigation and line of sight guidance laws are used to estimate
the range and bearing information from the leader vehicle using the vision
subsystem. The algorithms for vision detection and localization used here are
similar to approaches for many computer vision tasks such as face tracking and
detection that are based color-and texture based features, and non-parametric
Continuously Adaptive Mean-shift algorithms to keep track of the leader. This
is being proposed for the first time in the leader follower framework. The
algorithms are simple but effective for real time and provide an alternate
approach to traditional based approaches like the Viola Jones algorithm.
Further to stabilize the follower to the leader trajectory, the sliding mode
controller is used to dynamically track the leader. The performance of the
results is demonstrated in simulation and in practical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01367</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01367</id><created>2015-01-06</created><authors><author><keyname>Xu</keyname><forenames>Qiang</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Device Fingerprinting in Wireless Networks: Challenges and Opportunities</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Node forgery or impersonation, in which legitimate cryptographic credentials
are captured by an adversary, constitutes one major security threat facing
wireless networks. The fact that mobile devices are prone to be compromised and
reverse engineered significantly increases the risk of such attacks in which
adversaries can obtain secret keys on trusted nodes and impersonate the
legitimate node. One promising approach toward thwarting these attacks is
through the extraction of unique fingerprints that can provide a reliable and
robust means for device identification. These fingerprints can be extracted
from transmitted signal by analyzing information across the protocol stack. In
this paper, the first unified and comprehensive tutorial in the area of
wireless device fingerprinting for security applications is presented. In
particular, we aim to provide a detailed treatment on developing novel wireless
security solutions using device fingerprinting techniques. The objectives are
three-fold: (i) to introduce a comprehensive taxonomy of wireless features that
can be used in fingerprinting, (ii) to provide a systematic review on
fingerprint algorithms including both white-list based and unsupervised
learning approaches, and (iii) to identify key open research problems in the
area of device fingerprinting and feature extraction, as applied to wireless
security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01370</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01370</id><created>2015-01-07</created><authors><author><keyname>Vucha</keyname><forenames>Mahendra</forenames></author><author><keyname>Rajawat</keyname><forenames>Arvind</forenames></author></authors><title>A Case Study: Task Scheduling Methodologies for High Speed Computing
  Systems</title><categories>cs.OS cs.PF cs.SY</categories><comments>12 pages</comments><doi>10.5121/ijesa.2014.4401</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  High Speed computing meets ever increasing real-time computational demands
through the leveraging of flexibility and parallelism. The flexibility is
achieved when computing platform designed with heterogeneous resources to
support multifarious tasks of an application where as task scheduling brings
parallel processing. The efficient task scheduling is critical to obtain
optimized performance in heterogeneous computing Systems (HCS). In this paper,
we brought a review of various application scheduling models which provide
parallelism for homogeneous and heterogeneous computing systems. In this paper,
we made a review of various scheduling methodologies targeted to high speed
computing systems and also prepared summary chart. The comparative study of
scheduling methodologies for high speed computing systems has been carried out
based on the attributes of platform &amp; application as well. The attributes are
execution time, nature of task, task handling capability, type of host &amp;
computing platform. Finally a summary chart has been prepared and it
demonstrates that the need of developing scheduling methodologies for
Heterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging high
speed computing platform for real time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01372</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01372</id><created>2015-01-07</created><updated>2015-12-03</updated><authors><author><keyname>Xie</keyname><forenames>Yuan</forenames></author></authors><title>Weighted Schatten $p$-Norm Minimization for Image Denoising with Local
  and Nonlocal Regularization</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial
  therotical error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a patch-wise low-rank based image denoising method with
constrained variational model involving local and nonlocal regularization. On
one hand, recent patch-wise methods can be represented as a low-rank matrix
approximation problem whose convex relaxation usually depends on nuclear norm
minimization (NNM). Here, we extend the NNM to the nonconvex schatten p-norm
minimization with additional weights assigned to different singular values,
which is referred to as the Weighted Schatten p-Norm Minimization (WSNM). An
efficient algorithm is also proposed to solve the WSNM problem. The proposed
WSNM not only gives better approximation to the original low-rank assumption,
but also considers physical meanings of different data components. On the other
hand, due to the naive aggregation schema which integrates all the denoised
patches into a whole image, current patch-wise denoising methods always produce
various degree of artifacts in denoised results. Therefore, to further reduce
artifacts, a data-driven regularizer called Steering Total Variation (STV)
combined with nonlocal TV is derived for a variational model, which imposes
local and nonlocal consistency constraints on the patch-wise denoised image. A
highly simple but efficient algorithm is proposed to solve this variational
model with convergence guarantee. Both WSNM and local \&amp; nonlocal consistent
regularization are integrated into an iterative restoration framework to
produce final results. Extensive experimental testing shows, both qualitatively
and quantitatively, that the proposed method can effectively remove noise, as
well as reduce artifacts compared with state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01376</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01376</id><created>2015-01-07</created><authors><author><keyname>Harjito</keyname><forenames>Bambang</forenames></author><author><keyname>Potdar</keyname><forenames>Vidyasagar</forenames></author></authors><title>Secure Transmission in Wireless Sensor Networks Data Using Linear
  Kolmogorov Watermarking Technique</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Wireless sensor networks (WSNs), All communications between different
nodes are sent out in a broadcast fashion. These networks are used in a variety
of applications including military, environmental, and smart spaces. Sensors
are susceptible to various types of attack, such as data modification, data
insertion and deletion, or even physical capture and sensor replacement. Hence
security becomes important issue in WSNs. However given the fact that sensors
are resources constrained, hence the traditional intensive security algorithms
are not well suited for WSNs. This makes traditional security techniques, based
on data encryption, not very suitable for WSNs. This paper proposes Linear
Kolmogorov watermarking technique for secure data communication in WSNs. We
provide a security analysis to show the robustness of the proposed techniques
against various types of attacks. This technique is robust against data
deletion, packet replication and Sybil attacks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01386</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01386</id><created>2015-01-07</created><authors><author><keyname>Daud</keyname><forenames>Misbah</forenames></author><author><keyname>Khan</keyname><forenames>Rafiullah</forenames></author><author><keyname>Mohibullah</keyname></author><author><keyname>Daud</keyname><forenames>Aitazaz</forenames></author></authors><title>Roman Urdu Opinion Mining System (RUOMiS)</title><categories>cs.CL cs.IR</categories><comments>8 pages, 2 figures, 4 tables, Computer Science &amp; Engineering: An
  International Journal (CSEIJ)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convincing a customer is always considered as a challenging task in every
business. But when it comes to online business, this task becomes even more
difficult. Online retailers try everything possible to gain the trust of the
customer. One of the solutions is to provide an area for existing users to
leave their comments. This service can effectively develop the trust of the
customer however normally the customer comments about the product in their
native language using Roman script. If there are hundreds of comments this
makes difficulty even for the native customers to make a buying decision. This
research proposes a system which extracts the comments posted in Roman Urdu,
translate them, find their polarity and then gives us the rating of the
product. This rating will help the native and non-native customers to make
buying decision efficiently from the comments posted in Roman Urdu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01387</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01387</id><created>2015-01-07</created><authors><author><keyname>Blel</keyname><forenames>Ilhem</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Alamouti OFDM/OQAM systems with time reversal technique</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal Frequency Division Multiplexing with Offset Quadrature Amplitude
Modulation (OFDM/OQAM) is a multicarrier modulation scheme that can be
considered as an alternative to the conventional Orthogonal Frequency Division
Multiplexing (OFDM) with Cyclic Prefix (CP) for transmission over multipath
fading channels. In this paper, we investigate the combination of the OFDM/OQAM
with Alamouti system with Time Reversal (TR) technique. TR can be viewed as a
precoding scheme which can be combined with OFDM/OQAM and easily carried out in
a Multiple Input Single Output (MISO) context such as Alamouti system. We
present the simulation results of the performance of OFDM/OQAM system in SISO
case compared with the conventional CP-OFDM system and the performance of the
combination Alamouti OFDM/OQAM with TR compared to Alamouti CP-OFDM. The
performance is derived by computing the Bit Error Rate (BER) as a function of
the transmit signal-to-noise ratio (SNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01392</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01392</id><created>2015-01-07</created><authors><author><keyname>Badry</keyname><forenames>Pallavi</forenames></author><author><keyname>Satyam</keyname><forenames>Neelima</forenames></author></authors><title>DSSI for pile supported asymmetrical buildings : a review</title><categories>cs.CE</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the reference of the several documents in the field of soil structure
interaction a document of present and past literature has been made with the
including a main focus on interaction of pile supported frames. This study
focuses on the complexity and excessive simplification of the model for
foundation system and structures, and should be carried forward for its
significance. The review is carried out including analytical, experimental and
numerical approaches considered in the past study. The perusal of literature
reveals that very few studies investigated on asymmetrical buildings supported
on pile foundations. In this paper, an attempt is made to understand research
carried out in pile soil structure interaction and research gap along with the
scope of research has been identified to carry out the present research work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01405</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01405</id><created>2015-01-07</created><authors><author><keyname>Passerat-Palmbach</keyname><forenames>Jonathan</forenames><affiliation>ISIMA, LIMOS, UBP</affiliation></author><author><keyname>Caux</keyname><forenames>Jonathan</forenames><affiliation>LIMOS, UBP, IBC, ISIMA</affiliation></author><author><keyname>Siregar</keyname><forenames>Pridi</forenames><affiliation>IBC</affiliation></author><author><keyname>Mazel</keyname><forenames>Claude</forenames><affiliation>LIMOS, UBP, ISIMA</affiliation></author><author><keyname>Hill</keyname><forenames>David</forenames><affiliation>UBP, LIMOS, ISIMA</affiliation></author></authors><title>Warp-Level Parallelism: Enabling Multiple Replications In Parallel on
  GPU</title><categories>cs.DC</categories><comments>Best paper award. from European Simulation and Modelling, Oct 2011,
  Guimaraes, Portugal</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic simulations need multiple replications in order to build
confidence intervals for their results. Even if we do not need a large amount
of replications, it is a good practice to speed-up the whole simulation time
using the Multiple Replications In Parallel (MRIP) approach. This approach
usually supposes to have access to a parallel computer such as a symmetric
mul-tiprocessing machine (with many cores), a computing cluster or a computing
grid. In this paper, we propose Warp-Level Parallelism (WLP), a GP-GPU-enabled
solution to compute MRIP on GP-GPUs (General-Purpose Graphics Processing
Units). These devices display a great amount of parallel computational power at
low cost, but are tuned to process efficiently the same operation on several
data, through different threads. Indeed, this paradigm is called Single
Instruction, Multiple Threads (SIMT). Our approach proposes to rely on small
threads groups, called warps, to perform independent computations such as
replications. We have benchmarked WLP with three different models: it allows
MRIP to be computed up to six times faster than with the SIMT computing
paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01422</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01422</id><created>2015-01-07</created><authors><author><keyname>Mawlawi</keyname><forenames>Baher</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes, CEA - LETI, INSA Lyon</affiliation></author><author><keyname>Dor&#xe9;</keyname><forenames>Jean-Baptiste</forenames><affiliation>CEA - LETI</affiliation></author></authors><title>CSMA/CA Bottleneck Remediation in Saturation Mode with New Backoff
  Strategy</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>6th International Workshop on Multiple Access Communications,
  2013, pp.70 - 81</journal-ref><doi>10.1007/978-3-319-03871-1_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern wireless networks integrate carrier sense mul-tiple
access/collision avoidance (CSMA/CA) with exponential backoff as medium access
control (MAC) technique. In order to decrease the MAC overhead and the
collision probability, we propose in this paper a new backoff strategy leading
to better saturation throughput and access de-lay performance comparing to the
classical protocol. We investigate the CSMA/CA with RTS/CTS technique, and we
show that our strategy reaches better saturation throughput and access delay
especially in dense networks. This proposed strategy distributes users over all
the backoff stages to solve the bottleneck problem present in the first backoff
stage. Finally, we analyze our strategy and we compare it to the classical one
modeled by Markov chain. Analytical and simulation results show the improvment
in term of saturation throughput. Cumulative density func-tion (CDF) of the
access delay illustrates the important gain obtained by the proposed strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01426</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01426</id><created>2015-01-07</created><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author><author><keyname>Sethi</keyname><forenames>Shuchi</forenames></author></authors><title>Analysis and Clustering of Workload in Google Cluster Trace based on
  Resource Usage</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has gained interest amongst commercial organizations,
research communities, developers and other individuals during the past few
years.In order to move ahead with research in field of data management and
processing of such data, we need benchmark datasets and freely available data
which are publicly accessible. Google in May 2011 released a trace of a cluster
of 11k machines referred as Google Cluster Trace.This trace contains cell
information of about 29 days.This paper provides analysis of resource usage and
requirements in this trace and is an attempt to give an insight into such kind
of production trace similar to the ones in cloud environment.The major
contributions of this paper include Statistical Profile of Jobs based on
resource usage, clustering of Workload Patterns and Classification of jobs into
different types based on k-means clustering.Though there have been earlier
works for analysis of this trace, but our analysis provides several new
findings such as jobs in a production trace are trimodal and there occurs
symmetry in the tasks within a long job type
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01427</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01427</id><created>2015-01-07</created><authors><author><keyname>Elkabbany</keyname><forenames>Ghada F.</forenames></author><author><keyname>Aslan</keyname><forenames>Heba K.</forenames></author><author><keyname>Rasslan</keyname><forenames>Mohamed N.</forenames></author></authors><title>A Design of a Fast Parallel-Pipelined Implementation of AES: Advanced
  Encryption Standard</title><categories>cs.CR</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT), Vol. 6, No. 6, Dec. 2014</journal-ref><doi>10.5121/ijcsit</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Advanced Encryption Standard (AES) algorithm is a symmetric block cipher
which operates on a sequence of blocks each consists of 128, 192 or 256 bits.
Moreover, the cipher key for the AES algorithm is a sequence of 128, 192 or 256
bits. AES algorithm has many sources of parallelism. In this paper, a design of
parallel AES on the multiprocessor platform is presented. While most of the
previous designs either use pipelined parallelization or take advantage of the
Mix_Column parallelization, our design is based on combining pipelining of
rounds and parallelization of Mix_Column and Add_Round_Key transformations.
This model is divided into two levels: the first is pipelining different
rounds, while the second is through parallelization of both the Add_Round_Key
and the Mix_Column transformations. Previous work proposed for pipelining AES
algorithm was based on using nine stages, while, we propose the use of eleven
stages in order to exploit the sources of parallelism in both initial and final
round. This enhances the system performance compared to previous designs. Using
two-levels of parallelization benefits from the highly independency of
Add_Round_Key and Mix_Column/ Inv_Mix_Colum transformations. The analysis shows
that the parallel implementation of the AES achieves a better performance. The
analysis shows that using pipeline increases significantly the degree of
improvement for both encryption and decryption by approximately 95%. Moreover,
parallelizing Add_Round_Key and Mix_Column/ Inv_Mix_Column transformations
increases the degree of improvement by approximately 98%. This leads to the
conclusion that the proposed design is scalable and is suitable for real-time
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01429</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01429</id><created>2015-01-07</created><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Lecroq</keyname><forenames>Thierry</forenames></author><author><keyname>Lefebvre</keyname><forenames>Arnaud</forenames></author><author><keyname>Prieur-Gaston</keyname><forenames>&#xc9;lise</forenames></author></authors><title>Online Computation of Abelian Runs</title><categories>cs.FL cs.DS</categories><comments>To appear in the Proceedings of LATA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a word $w$ and a Parikh vector $\mathcal{P}$, an abelian run of period
$\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having
abelian period $\mathcal{P}$. We give an algorithm that finds all the abelian
runs of period $\mathcal{P}$ in a word of length $n$ in time $O(n\times
|\mathcal{P}|)$ and space $O(\sigma+|\mathcal{P}|)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01430</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01430</id><created>2015-01-07</created><authors><author><keyname>Mawlawi</keyname><forenames>Baher</forenames><affiliation>INSA Lyon, CEA - LETI, CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Do</keyname><forenames>Jean-Baptiste</forenames><affiliation>CEA - LETI</affiliation></author><author><keyname>Lebedev</keyname><forenames>Nikolai</forenames><affiliation>INSA Lyon, CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames><affiliation>INSA Lyon, CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>Multiband CSMA/CA with RTS-CTS strategy</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>Wimob 2014: IEEE International Conference on Wireless and Mobile
  Computing, Networking and Communications, 2014, pp.628 - 633</journal-ref><doi>10.1109/WiMOB.2014.6962236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a new medium access control (MAC) scheme devoted to
orthogonal frequency division multiple access (OFDMA) systems which aims at
reducing collision probabilities during the channel request period. The
proposed MAC relies on the classical carrier sense multiple access/collision
avoidance (CSMA/CA) protocol with RTS / CTS (&quot;Request To Send&quot; / &quot;Clear To
Send&quot;) mechanism. The proposed method focus on the collision probability of RTS
messages exploiting a multi-channel configuration for these messages while
using the whole band for data transmissions. The protocol may be interpreted as
an asynchronous frequency multiplexing of RTS messages. This method achieves
strong performance gains in terms of throughput and latency especially in
crowded networks. Index Terms-Carrier sense multiple access/collision avoidance
(CSMA/CA), multiband, throughput, MAC protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01432</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01432</id><created>2015-01-07</created><authors><author><keyname>Zhou</keyname><forenames>Kuang</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Pan</keyname><forenames>Quan</forenames></author></authors><title>Evidential-EM Algorithm Applied to Progressively Censored Observations</title><categories>cs.AI stat.ME</categories><proxy>ccsd</proxy><journal-ref>15th International Conference on Information Processing and
  Management of Uncertainty in Knowledge-Based Systems, Jul 2014, Montpellier,
  France. pp.180 - 189</journal-ref><doi>10.1007/978-3-319-08852-5_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidential-EM (E2M) algorithm is an effective approach for computing maximum
likelihood estimations under finite mixture models, especially when there is
uncertain information about data. In this paper we present an extension of the
E2M method in a particular case of incom-plete data, where the loss of
information is due to both mixture models and censored observations. The prior
uncertain information is expressed by belief functions, while the
pseudo-likelihood function is derived based on imprecise observations and prior
knowledge. Then E2M method is evoked to maximize the generalized likelihood
function to obtain the optimal estimation of parameters. Numerical examples
show that the proposed method could effectively integrate the uncertain prior
infor-mation with the current imprecise knowledge conveyed by the observed
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01436</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01436</id><created>2015-01-07</created><updated>2015-06-12</updated><authors><author><keyname>He</keyname><forenames>Jianghao</forenames></author><author><keyname>Liew</keyname><forenames>Soung-Chang</forenames></author></authors><title>ARQ for Physical-layer Network Coding</title><categories>cs.NI</categories><comments>34 pages, 6 figures, 10 Tables,Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates ARQ (Automatic Repeat request) designs for PNC
(Physical-layer Network Coding) systems. We have previously found that, besides
TWRC (Two-Way Relay Channel) operated on the principle of PNC, there are many
other PNC building blocks--building blocks are simple small network structures
that can be used to construct a large network. In some of these PNC building
blocks, receivers can obtain side information through overhearing. Although
such overheard information is not the target information that the receivers
desire, the receivers can exploit the overheard information together with a
network-coded packet received to obtain a desired native packet. This leads to
throughput gain. Our previous study, however, assumed what is sent always get
received. In practice, that is not the case. Error control is needed to ensure
reliable communication. This paper focuses on the use of ARQ to ensure reliable
PNC communication. The availability of overheard Information and its potential
exploitation make the ARQ design of a network-coded system different from that
of a non-network-coded system. In this paper, we lay out the fundamental
considerations for such ARQ design: 1) We address how to track the stored coded
packets and overheard packets to increase the chance of packet extraction, and
derive the throughput gain achieved by tracking 2) We investigate two
variations of PNC ARQ, coupled and non-coupled ARQs, and prove that non-coupled
ARQ is more efficient; 3) We show how to optimize parameters in PNC
ARQ--specifically the window size and ACK frequency--to minimize the throughput
degradation caused by ACK feedback overhead and wasteful retransmissions due to
lost ACK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01450</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01450</id><created>2015-01-07</created><authors><author><keyname>Fang</keyname><forenames>Bin</forenames></author><author><keyname>Zhou</keyname><forenames>Wuyang</forenames></author></authors><title>An Effective Handover Analysis for the Randomly Distributed
  Heterogeneous Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>20 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Handover rate is one of the most import metrics to instruct mobility
management and resource management in wireless cellular networks. In the
literature, the mathematical expression of handover rate has been derived for
homogeneous cellular network by both regular hexagon coverage model and
stochastic geometry model, but there has not been any reliable result for
heterogeneous cellular networks (HCNs). Recently, stochastic geometry modeling
has been shown to model well the real deployment of HCNs and has been
extensively used to analyze HCNs. In this paper, we give an effective handover
analysis for HCNs by stochastic geometry modeling, derive the mathematical
expression of handover rate by employing an infinitesimal method for a
generalized multi-tier scenario, discuss the result by deriving some meaningful
corollaries, and validate the analysis by computer simulation with multiple
walking models. By our analysis, we find that in HCNs the handover rate is
related to many factors like the base stations' densities and transmitting
powers, user's velocity distribution, bias factor, pass loss factor and etc.
Although our analysis focuses on the scenario of multi-tier HCNs, the
analytical framework can be easily extended for more complex scenarios, and may
shed some light for future study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01457</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01457</id><created>2015-01-07</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>I&#xf1;aki Fern&#xe1;ndez</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Boumaza</keyname><forenames>Amine</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Charpillet</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Comparison of Selection Methods in On-line Distributed Evolutionary
  Robotics</title><categories>cs.AI cs.MA cs.NE cs.RO</categories><proxy>ccsd</proxy><journal-ref>ALIFE 14, Jul 2014, New York, United States. Artificial Life 14 in
  Complex Adaptive Systems, MIT Press, Artificial Life 14</journal-ref><doi>10.7551/978-0-262-32621-6-ch046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the impact of selection methods in the context of
on-line on-board distributed evolutionary algorithms. We propose a variant of
the mEDEA algorithm in which we add a selection operator, and we apply it in a
taskdriven scenario. We evaluate four selection methods that induce different
intensity of selection pressure in a multi-robot navigation with obstacle
avoidance task and a collective foraging task. Experiments show that a small
intensity of selection pressure is sufficient to rapidly obtain good
performances on the tasks at hand. We introduce different measures to compare
the selection methods, and show that the higher the selection pressure, the
better the performances obtained, especially for the more challenging food
foraging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01460</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01460</id><created>2015-01-07</created><authors><author><keyname>Zhou</keyname><forenames>Kuang</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Pan</keyname><forenames>Quan</forenames></author><author><keyname>Liu</keyname><forenames>Zhun-Ga</forenames></author></authors><title>Median evidential c-means algorithm and its application to community
  detection</title><categories>cs.AI cs.SI</categories><proxy>ccsd</proxy><journal-ref>Knowledge-Based Systems, Elsevier, 2015, 74, pp.69 - 88</journal-ref><doi>10.1016/j.knosys.2014.11.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Median clustering is of great value for partitioning relational data. In this
paper, a new prototype-based clustering method, called Median Evidential
C-Means (MECM), which is an extension of median c-means and median fuzzy
c-means on the theoretical framework of belief functions is proposed. The
median variant relaxes the restriction of a metric space embedding for the
objects but constrains the prototypes to be in the original data set. Due to
these properties, MECM could be applied to graph clustering problems. A
community detection scheme for social networks based on MECM is investigated
and the obtained credal partitions of graphs, which are more refined than crisp
and fuzzy ones, enable us to have a better understanding of the graph
structures. An initial prototype-selection scheme based on evidential
semi-centrality is presented to avoid local premature convergence and an
evidential modularity function is defined to choose the optimal number of
communities. Finally, experiments in synthetic and real data sets illustrate
the performance of MECM and show its difference to other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01463</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01463</id><created>2015-01-07</created><authors><author><keyname>Martinez-Gonzalez</keyname><forenames>Ricardo Francisco</forenames></author><author><keyname>Diaz-Mendez</keyname><forenames>Jose Alejandro</forenames></author></authors><title>Implementation of a Stream Cipher Based on Bernoulli's Map</title><categories>cs.CR</categories><comments>9 Pages, 6 Figures and 1 Table</comments><msc-class>94A60, 68P25, 14G50</msc-class><acm-class>C.2.0; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stream cipher was implemented on a FPGA. The keystream, for some authors
the most important element, was developed using an algorithm based on
Bernoullis chaotic map. When dynamic systems are digitally implemented, a
normal degradation appears and disturbs their behavior; for such reason, a
mechanism was needed. The proposed mechanism gives a solution for degradation
issue and its implementation is not complicated. Finally, the implemented
cipher includes 8 stages and 2 pseudo-random number generators (PRNG), such
cipher was tested using NIST testes. Once its designing stage, it was
implemented using a developing FPGA board.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01468</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01468</id><created>2015-01-07</created><authors><author><keyname>Ribaud</keyname><forenames>Vincent</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Saliou</keyname><forenames>Philippe</forenames></author></authors><title>The Cost of Problem-Based Learning: An Example in Information Systems
  Engineering</title><categories>cs.SE cs.CY</categories><proxy>ccsd</proxy><journal-ref>2013 IEEE 26th Conference on Software Engineering Education and
  Training (CSEE\&amp;T), May 2013, San Fransisco, United States. pp.259-263</journal-ref><doi>10.1109/CSEET.2013.6595257</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-quality education helps in finding a job - but student skills
heterogeneity and student reluctance to move towards a professional attitude
are important barriers to employability. We re-engineered some of the technical
courses of a Masters in software development using a Problem-Based Learning
(PBL) approach. Although initial results are encouraging, the cost of using PBL
must be taken into account. Two aspects are particularly expensive: (i) set-up
of the software development practicum, a mid-sized information system and its
environment; (ii) screenwriting of problem-based learning scenarios, including
procurement of input artefacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01471</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01471</id><created>2015-01-07</created><updated>2015-03-17</updated><authors><author><keyname>Ahmad</keyname><forenames>Rami Ali</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Arnal</keyname><forenames>Fabrice</forenames></author><author><keyname>Gineste</keyname><forenames>Mathieu</forenames></author><author><keyname>Clarac</keyname><forenames>Laurence</forenames></author></authors><title>Enhanced HARQ for Delay Tolerant Services in Mobile Satellite
  Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 5 figures, The Seventh International Conference on Advances
  in Satellite and Space Communications SPACOMM 2015. arXiv admin note:
  substantial text overlap with arXiv:1411.3625</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of our paper is to improve efficiency (in terms of throughput
or system capacity) for mobile satellite communications. In this context, we
propose an enhanced Hybrid Automatic Repeat reQuest (HARQ) for delay tolerant
services. Our proposal uses the estimation of the mutual information. We
evaluate the performance of the proposed method for a land mobile satellite
channel by means of simulations. Results are compared with those obtained with
a classical incremental redundancy (IR) HARQ scheme. The technique we propose,
shows a better performance in terms of efficiency while maintaining an
acceptable delay for services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01495</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01495</id><created>2015-01-07</created><updated>2015-04-03</updated><authors><author><keyname>Fehenberger</keyname><forenames>Tobias</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author><author><keyname>Hanik</keyname><forenames>Norbert</forenames></author></authors><title>On Achievable Rates for Long-Haul Fiber-Optic Communications</title><categories>cs.IT math.IT</categories><comments>Hard decision mutual information analysis added, two typos corrected</comments><doi>10.1364/OE.23.009183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lower bounds on mutual information (MI) of long-haul optical fiber systems
for hard-decision and soft-decision decoding are studied. Ready-to-use
expressions to calculate the MI are presented. Extensive numerical simulations
are used to quantify how changes in the optical transmitter, receiver, and
channel affect the achievable transmission rates of the system. Special
emphasis is put to the use of different quadrature amplitude modulation
formats, channel spacings, digital back-propagation schemes and probabilistic
shaping. The advantages of using MI over the prevailing $Q$-factor as a figure
of merit of coded optical systems are also highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01496</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01496</id><created>2015-01-07</created><updated>2015-07-28</updated><authors><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>IEEE 802.11ax: High-Efficiency WLANs</title><categories>cs.NI</categories><comments>in IEEE Wireless Communications Magazine, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEEE 802.11ax-2019 will replace both IEEE 802.11n-2009 and IEEE 802.11ac-2013
as the next high-throughput Wireless Local Area Network (WLAN) amendment. In
this paper, we review the expected future WLAN scenarios and use-cases that
justify the push for a new PHY/MAC IEEE 802.11 amendment. After that, we
overview a set of new technical features that may be included in the IEEE
802.11ax-2019 amendment and describe both their advantages and drawbacks.
Finally, we discuss some of the network-level functionalities that are required
to fully improve the user experience in next-generation WLANs and note their
relation with other on-going IEEE 802.11 amendments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01501</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01501</id><created>2015-01-07</created><authors><author><keyname>Schneider</keyname><forenames>Chris</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author><author><keyname>Dobson</keyname><forenames>Simon</forenames></author></authors><title>Autonomous Fault Detection in Self-Healing Systems using Restricted
  Boltzmann Machines</title><categories>cs.AI cs.SE</categories><comments>Published and presented in the 11th IEEE International Conference and
  Workshops on Engineering of Autonomic and Autonomous Systems (EASe 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomously detecting and recovering from faults is one approach for
reducing the operational complexity and costs associated with managing
computing environments. We present a novel methodology for autonomously
generating investigation leads that help identify systems faults, and extends
our previous work in this area by leveraging Restricted Boltzmann Machines
(RBMs) and contrastive divergence learning to analyse changes in historical
feature data. This allows us to heuristically identify the root cause of a
fault, and demonstrate an improvement to the state of the art by showing
feature data can be predicted heuristically beyond a single instance to include
entire sequences of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01526</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01526</id><created>2015-01-07</created><authors><author><keyname>Picot-Cl&#xe9;mente</keyname><forenames>Romain</forenames></author><author><keyname>Bothorel</keyname><forenames>C&#xe9;cile</forenames></author><author><keyname>Jullien</keyname><forenames>Nicolas</forenames></author></authors><title>Social Interactions vs Revisions, What is important for Promotion in
  Wikipedia?</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In epistemic community, people are said to be selected on their knowledge
contribution to the project (articles, codes, etc.) However, the socialization
process is an important factor for inclusion, sustainability as a contributor,
and promotion. Finally, what does matter to be promoted? being a good
contributor? being a good animator? knowing the boss? We explore this question
looking at the process of election for administrator in the English Wikipedia
community. We modeled the candidates according to their revisions and/or social
attributes. These attributes are used to construct a predictive model of
promotion success, based on the candidates's past behavior, computed thanks to
a random forest algorithm.
  Our model combining knowledge contribution variables and social networking
variables successfully explain 78% of the results which is better than the
former models. It also helps to refine the criterion for election. If the
number of knowledge contributions is the most important element, social
interactions come close second to explain the election. But being connected
with the future peers (the admins) can make the difference between success and
failure, making this epistemic community a very social community too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01537</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01537</id><created>2015-01-07</created><authors><author><keyname>Kh&#xe9;diri</keyname><forenames>Nouha</forenames></author><author><keyname>Zaghdoud</keyname><forenames>Montaceur</forenames></author></authors><title>Survey of Uncertainty Handling in Cloud Service Discovery and
  Composition</title><categories>cs.SE</categories><comments>10 pages,2 tables</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  With the spread of services related to cloud environment, it is tiresome and
time consuming for users to look for the appropriate service that meet with
their needs. Therefore, finding a valid and reliable service is essential.
However, in case a single cloud service cannot fulfil every user requirements,
a composition of cloud services is needed. In addition, the need to treat
uncertainty in cloud service discovery and composition induces a lot of
concerns in order to minimize the risk. Risk includes some sort of either loss
or damage which is possible to be received by a target (i.e., the environment,
cloud providers or customers). In this paper, we will focus on the uncertainty
application for cloud service discovery and composition. A set of existing
approaches in literature are reviewed and categorized according to the risk
modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01539</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01539</id><created>2015-01-07</created><authors><author><keyname>Zafari</keyname><forenames>Faheem</forenames></author><author><keyname>Papapanagiotou</keyname><forenames>Ioannis</forenames></author><author><keyname>Christidis</keyname><forenames>Konstantinos</forenames></author></authors><title>Micro-location for Internet of Things equipped Smart Buildings</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro-location is the process of locating any entity with high accuracy
(possibly in centimeters), while geofencing is the process of creating a
virtual fence around a so-called Point of Interest (PoI). In this paper, we
present an insight into various micro-location enabling technologies and
services. We also discuss how these can accelerate the incorporation of
Internet of Things (IoT) in smart buildings. We argue that micro-location based
location-aware solutions can play a significant role in facilitating the
tenants of an IoT equipped smart building. Also, such advanced technologies
will enable the smart building control system through minimal actions performed
by the tenants. We also highlight the existing and envisioned services to be
provided by using micro-location enabling technologies. We describe the
challenges and propose some potential solutions such that micro-location
enabling technologies and services are thoroughly integrated with IoT equipped
smart building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01545</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01545</id><created>2015-01-07</created><authors><author><keyname>Osipov</keyname><forenames>Andrei</forenames></author></authors><title>On decoding of digital data sent over a noisy MIMO channel</title><categories>cs.IT math.IT math.OC</categories><comments>40 pages</comments><report-no>Yale CS Technical Report #1503</report-no><msc-class>65C99, 90C27, 94A12, 94B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transmission of digital data is one of the principal tasks in modern
wireless communication. Classically, the communication channel consists of one
transmitter and one receiver; however, due to the constantly increasing demand
in higher transmission rates, the popularity of using several receivers and
transmitters has been rapidly growing.
  In this paper, we combine a number of fairly standard techniques from
numerical linear algebra and probability to develop several (apparently novel)
randomized schemes for the decoding of digital messages sent over a noisy
multivariate Gaussian channel.
  We use a popular mathematical model for such channels to illustrate the
performance of our schemes via numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01548</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01548</id><created>2015-01-07</created><updated>2015-01-17</updated><authors><author><keyname>Thangarajah</keyname><forenames>Akilan</forenames></author><author><keyname>Wongkaew</keyname><forenames>Buddhapala</forenames></author><author><keyname>Ekpanyapong</keyname><forenames>Mongkol</forenames></author></authors><title>Implementation of Auto Monitoring and Short-Message-Service System via
  GSM Modem</title><categories>cs.CV</categories><comments>7 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auto-Monitoring and Short-Messaging-Service System is a real-time monitoring
system for any critical operational environments. It detects an undesired event
occurring in the environment, generates an alert with detailed message and
sends it to the user to prevent hazards. This system employs a Friendly ARM as
main controller while, sensors and terminals to interact with the real world. A
GSM network is utilized to bridge the communication between monitoring system
and user. This paper presents details of prototyping the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01549</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01549</id><created>2015-01-07</created><authors><author><keyname>Salvail</keyname><forenames>Louis</forenames></author><author><keyname>Schaffner</keyname><forenames>Christian</forenames></author><author><keyname>Sotakova</keyname><forenames>Miroslava</forenames></author></authors><title>Quantifying the Leakage of Quantum Protocols for Classical Two-Party
  Cryptography</title><categories>quant-ph cs.CR</categories><comments>38 pages, completely supersedes arXiv:0902.4036</comments><journal-ref>Int. J. Quantum Inform. 12, 1450041 (2014)</journal-ref><doi>10.1142/S0219749914500415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study quantum protocols among two distrustful parties. By adopting a
rather strict definition of correctness - guaranteeing that honest players
obtain their correct outcomes only - we can show that every strictly correct
quantum protocol implementing a non-trivial classical primitive necessarily
leaks information to a dishonest player. This extends known impossibility
results to all non-trivial primitives. We provide a framework for quantifying
this leakage and argue that leakage is a good measure for the privacy provided
to the players by a given protocol. Our framework also covers the case where
the two players are helped by a trusted third party. We show that despite the
help of a trusted third party, the players cannot amplify the cryptographic
power of any primitive. All our results hold even against quantum
honest-but-curious adversaries who honestly follow the protocol but purify
their actions and apply a different measurement at the end of the protocol. As
concrete examples, we establish lower bounds on the leakage of standard
universal two-party primitives such as oblivious transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01563</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01563</id><created>2015-01-07</created><authors><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>A New Proof for the DoF Region of the MIMO Networks with No CSIT</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Communications Letters. arXiv admin note:
  substantial text overlap with arXiv:1401.5676</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new proof for the degrees of freedom (DoF) region of the
K-user multiple-input multiple-output (MIMO) broadcast channel (BC) with no
channel state information at the transmitter (CSIT) and perfect channel state
information at the receivers (CSIR) is provided. Based on this proof, the
capacity region of a certain class of MIMO BC with channel distribution
information at the transmitter (CDIT) and perfect CSIR is derived. Finally, an
outer bound for the DoF region of the K-user MIMO interference channel (IC)
with no CSIT is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01571</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01571</id><created>2015-01-07</created><authors><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>An Introduction to Matrix Concentration Inequalities</title><categories>math.PR cs.DS cs.IT cs.NA math.IT stat.ML</categories><comments>163 pages. To appear in Foundations and Trends in Machine Learning</comments><msc-class>Primary: 60B20. Secondary: 60F10, 60G50, 60G42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, random matrices have come to play a major role in
computational mathematics, but most of the classical areas of random matrix
theory remain the province of experts. Over the last decade, with the advent of
matrix concentration inequalities, research has advanced to the point where we
can conquer many (formerly) challenging problems with a page or two of
arithmetic. The aim of this monograph is to describe the most successful
methods from this area along with some interesting examples that these
techniques can illuminate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01576</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01576</id><created>2015-01-07</created><updated>2015-09-16</updated><authors><author><keyname>Tafaghodi</keyname><forenames>Mohammad</forenames></author><author><keyname>Ghaffari</keyname><forenames>Meysam</forenames></author><author><keyname>Latif</keyname><forenames>Alimohammad</forenames></author><author><keyname>Mousavi</keyname><forenames>Seyed Rasoul</forenames></author></authors><title>Improving image watermarking based on Tabu search by Chaos</title><categories>cs.MM cs.AI</categories><comments>This paper has been withdrawn by arXiv. arXiv admin note: author list
  truncated due to disputed authorship and content</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the fast development of communication and multimedia technology, the
rights of the owners of multimedia products is vulnerable to the unauthorized
copies and watermarking is one of the best known methods for proving the
ownership of a product. In this paper we prosper the previous watermarking
method which was based on Tabu search by Chaos. The modification applied in the
permutation step of watermarking and the initial population generation of the
Tabu search. We analyze our method on some well known images and experimental
results shows the improvement in the quality and speed of the proposed
watermarking method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01578</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01578</id><created>2015-01-07</created><authors><author><keyname>Gil</keyname><forenames>A.</forenames></author><author><keyname>Segura</keyname><forenames>J.</forenames></author><author><keyname>Temme</keyname><forenames>N. M.</forenames></author></authors><title>GammaCHI: a package for the inversion and computation of the gamma and
  chi-square cumulative distribution functions (central and noncentral)</title><categories>cs.MS math.CA math.NA</categories><comments>To appear in Computer Physics Communications</comments><doi>10.1016/j.cpc.2015.01.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Fortran 90 module (GammaCHI) for computing and inverting the gamma and
chi-square cumulative distribution functions (central and noncentral) is
presented. The main novelty of this package are the reliable and accurate
inversion routines for the noncentral cumulative distribution functions.
Additionally, the package also provides routines for computing the gamma
function, the error function and other functions related to the gamma function.
The module includes the routines cdfgamC, invcdfgamC, cdfgamNC, invcdfgamNC,
errorfunction, inverfc, gamma, loggam, gamstar and quotgamm for the computation
of the central gamma distribution function (and its complementary function),
the inversion of the central gamma distribution function, the computation of
the noncentral gamma distribution function (and its complementary function),
the inversion of the noncentral gamma distribution function, the computation of
the error function and its complementary function, the inversion of the
complementary error function, the computation of: the gamma function, the
logarithm of the gamma function, the regulated gamma function and the ratio of
two gamma functions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01579</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01579</id><created>2015-01-07</created><authors><author><keyname>Fantacci</keyname><forenames>C.</forenames></author><author><keyname>Vo</keyname><forenames>B. -N.</forenames></author><author><keyname>Vo</keyname><forenames>B. -T.</forenames></author><author><keyname>Battistelli</keyname><forenames>G.</forenames></author><author><keyname>Chisci</keyname><forenames>L.</forenames></author></authors><title>Consensus Labeled Random Finite Set Filtering for Distributed
  Multi-Object Tracking</title><categories>cs.SY stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses distributed multi-object tracking over a network of
heterogeneous and geographically dispersed nodes with sensing, communication
and processing capabilities. The main contribution is an information-theoretic
approach to distributed estimation based on labeled Random Finite Sets (RFSs)
and dynamic Bayesian inference, which enables the development of two novel
consensus tracking filters, namely a Consensus Marginalized
$\delta$-Generalized Labeled Multi-Bernoulli (CM$\delta$GLMB) and consensus
Labeled Multi-Bernoulli (CLMB) tracking filter. The proposed algorithms provide
fully distributed, scalable and computationally efficient solutions for
multi-object tracking. Simulation experiments via Gaussian mixture
implementations confirm the effectiveness of the proposed approach on
challenging scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01582</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01582</id><created>2015-01-07</created><authors><author><keyname>Egan</keyname><forenames>Malcolm</forenames></author><author><keyname>Jakob</keyname><forenames>Michal</forenames></author></authors><title>Market Mechanism Design for Profitable On-Demand Transport Services</title><categories>cs.CY cs.GT</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On-demand transport services in the form of dial-a-ride and taxis are crucial
parts of the transport infrastructure in all major cities. However, not all
on-demand transport services are equal. In particular, not-for-profit
dial-a-ride services with coordinated drivers significantly differ from
profit-motivated taxi services with uncoordinated drivers. As such, there are
two key threads of research for efficient scheduling, routing, and pricing for
passengers: dial-a-ride services (first thread); and taxi services (second
thread). Unfortunately, there has been only limited development of algorithms
for joint optimization of scheduling, routing, and pricing; largely due to the
widespread assumption of fixed pricing. In this paper, we introduce another
thread: profit-motivated on-demand transport services with coordinated drivers.
To maximize provider profits and the efficiency of the service, we propose a
new market mechanism for this new thread of on-demand transport services, where
passengers negotiate with the service provider. In contrast to previous work,
our mechanism jointly optimizes scheduling, routing, and pricing. Ultimately,
we demonstrate that our approach can lead to higher profits, compared with
standard fixed price approaches, while maintaining comparable efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01588</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01588</id><created>2015-01-07</created><authors><author><keyname>Akhtar</keyname><forenames>Nadeem</forenames></author><author><keyname>Akhtar</keyname><forenames>Anique</forenames></author></authors><title>KitRobot: A multi-platform graphical programming IDE to program
  mini-robotic agents</title><categories>cs.PL cs.HC cs.RO</categories><comments>9 pages, IISTE - Computer Engineering and Intelligent Systems, ISSN
  2222-1719 (Paper) ISSN 2222-2863 (Online) Vol.5, No.3, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis, design and development of a graphical programming IDE for
mini-robotic agents allows novice users to program robotic agents by a
graphical drag and drop interface, without knowing the syntax and semantics of
the intermediate programming language. Our work started with the definition of
the syntax and semantics of the intermediate programming language. The major
work is the definition of grammar for this language. The use of a graphical
drag and drop interface for programming mini-robots offers a user-friendly
interface to novice users. The user can program graphically by drag and drop
program parts without having expertise of the intermediate programming
language. The IDE is highly flexible as it uses xml technology to store program
objects (i.e. loops, conditions) and robot objects (i.e. sensors, actuators).
Use of xml technology allows making major changes and updating the interface
without modifying the underlying design and programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01598</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01598</id><created>2015-01-07</created><authors><author><keyname>Brown-Cohen</keyname><forenames>Jonah</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author></authors><title>Combinatorial Optimization Algorithms via Polymorphisms</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An elegant characterization of the complexity of constraint satisfaction
problems has emerged in the form of the the algebraic dichotomy conjecture of
[BKJ00]. Roughly speaking, the characterization asserts that a CSP {\Lambda} is
tractable if and only if there exist certain non-trivial operations known as
polymorphisms to combine solutions to {\Lambda} to create new ones. In an
entirely separate line of work, the unique games conjecture yields a
characterization of approximability of Max-CSPs. Surprisingly, this
characterization for Max-CSPs can also be reformulated in the language of
polymorphisms.
  In this work, we study whether existence of non-trivial polymorphisms implies
tractability beyond the realm of constraint satisfaction problems, namely in
the value-oracle model. Specifically, given a function f in the value-oracle
model along with an appropriate operation that never increases the value of f ,
we design algorithms to minimize f . In particular, we design a randomized
algorithm to minimize a function f that admits a fractional polymorphism which
is measure preserving and has a transitive symmetry.
  We also reinterpret known results on MaxCSPs and thereby reformulate the
unique games conjecture as a characterization of approximability of max-CSPs in
terms of their approximate polymorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01657</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01657</id><created>2015-01-07</created><authors><author><keyname>Asudeh</keyname><forenames>Abolfazl</forenames></author><author><keyname>Zaruba</keyname><forenames>Gergely V.</forenames></author><author><keyname>Das</keyname><forenames>Sajal K.</forenames></author></authors><title>A General Model for MAC Protocol Selection in Wireless Sensor Networks</title><categories>cs.NI</categories><journal-ref>Ad Hoc Networks 2016</journal-ref><doi>10.1016/j.adhoc.2015.07.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are being deployed for different
applications, each having its own structure, goals and requirements. Medium
access control (MAC) protocols play a significant role in WSNs and hence should
be tuned to the applications. However, there is no for selecting MAC protocols
for different situations. Therefore, it is hard to decide which MAC protocol is
good for a given situation. Having a precise model for each MAC protocol, on
the other hand, is almost impossible. Using the intuition that the protocols in
the same behavioral category perform similarly, our goal in this paper is to
introduce a general model that selects the protocol(s) that satisfy the given
requirements from the category that performs better for a given context. We
define the Combined Performance Function (CPF) to demonstrate the performance
of different categories protocols for different contexts. Having the general
model, we then discuss the model scalability for adding new protocols,
categories, requirements, and performance criteria. Considering energy
consumption and delay as the initial performance criteria of the model, we
focus on deriving mathematical models for them. The results extracted from CPF
are the same as the well-known rule of thumb for the MAC protocols that
verifies our model. We validate our models with the help of simulation study.
We also implemented the current CPF model in a web page to make the model
online and useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01661</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01661</id><created>2015-01-07</created><updated>2015-01-24</updated><authors><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Kim</keyname><forenames>Kyu-Han</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Provably Delay Efficient Data Retrieving in Storage Clouds</title><categories>cs.DC cs.IT math.IT</categories><comments>17 pages, 4 figures. This is the technical report for a conference
  paper accepted by IEEE INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One key requirement for storage clouds is to be able to retrieve data
quickly. Recent system measurements have shown that the data retrieving delay
in storage clouds is highly variable, which may result in a long latency tail.
One crucial idea to improve the delay performance is to retrieve multiple data
copies by using parallel downloading threads. However, how to optimally
schedule these downloading threads to minimize the data retrieving delay
remains to be an important open problem. In this paper, we develop
low-complexity thread scheduling policies for several important classes of data
downloading time distributions, and prove that these policies are either
delay-optimal or within a constant gap from the optimum delay performance.
These theoretical results hold for an arbitrary arrival process of read
requests that may contain finite or infinite read requests, and for
heterogeneous MDS storage codes that can support diverse storage redundancy and
reliability requirements for different data files. Our numerical results show
that the delay performance of the proposed policies is significantly better
than that of First-Come- First-Served (FCFS) policies considered in prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01666</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01666</id><created>2015-01-07</created><authors><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Rossi</keyname><forenames>Luca</forenames></author></authors><title>Towards effective visual analytics on multiplex and multilayer networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we discuss visualisation strategies for multiplex networks.
Since Moreno's early works on network analysis, visualisation has been one of
the main ways to understand networks thanks to its ability to summarise a
complex structure into a single representation highlighting multiple properties
of the data. However, despite the large renewed interest in the analysis of
multiplex networks, no study has proposed specialised visualisation approaches
for this context and traditional methods are typically applied instead. In this
paper we initiate a critical and structured discussion of this topic, and claim
that the development of specific visualisation methods for multiplex networks
will be one of the main drivers pushing current research results into daily
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01668</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01668</id><created>2015-01-07</created><authors><author><keyname>Sadr</keyname><forenames>Sanam</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj S.</forenames></author></authors><title>Handoff Rate and Coverage Analysis in Multi-tier Heterogeneous Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the impact of user mobility in multi-tier heterogeneous
networks. We begin by obtaining the handoff rate for a mobile user in an
irregular cellular network with the access point locations modeled as a
homogeneous Poisson point process. The received signal-to-interference-ratio
(SIR) distribution along with a chosen SIR threshold is then used to obtain the
probability of coverage. To capture potential connection failures due to
mobility, we assume that a fraction of handoffs result in such failures.
Considering a multi-tier network with orthogonal spectrum allocation among
tiers and the maximum biased average received power as the tier association
metric, we derive the probability of coverage for two cases: 1) the user is
stationary (i.e., handoffs do not occur, or the system is not sensitive to
handoffs); 2) the user is mobile, and the system is sensitive to handoffs. We
derive the optimal bias factors to maximize the coverage. We show that when the
user is mobile, and the network is sensitive to handoffs, both the optimum tier
association and the probability of coverage depend on the user's speed; a
speed-dependent bias factor can then adjust the tier association to effectively
improve the coverage, and hence system performance, in a fully-loaded network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01675</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01675</id><created>2015-01-07</created><authors><author><keyname>Mulder</keyname><forenames>Henk</forenames></author></authors><title>Derivative coordinates for analytic tree fractals and fractal
  engineering</title><categories>cs.CG cs.CE math.GN</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce an alternative coordinate system based on derivative polar and
spherical coordinate functions and construct a root-to-canopy analytic
formulation for tree fractals. We develop smooth tree fractals and demonstrate
the equivalence of their canopies with iterative straight lined tree fractals.
We then consider implementation and application of the analytic formulation
from a computational perspective. Finally we formulate the basis for
concatenation and composition of fractal trees as a basis for fractal
engineering of which we provide some examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01676</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01676</id><created>2015-01-07</created><authors><author><keyname>Bothorel</keyname><forenames>Cecile</forenames></author><author><keyname>Cruz</keyname><forenames>Juan David</forenames></author><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Micenkova</keyname><forenames>Barbora</forenames></author></authors><title>Clustering attributed graphs: models, measures and methods</title><categories>cs.SI physics.soc-ph</categories><comments>Accepted for publication, Network Science journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering a graph, i.e., assigning its nodes to groups, is an important
operation whose best known application is the discovery of communities in
social networks. Graph clustering and community detection have traditionally
focused on graphs without attributes, with the notable exception of edge
weights. However, these models only provide a partial representation of real
social systems, that are thus often described using node attributes,
representing features of the actors, and edge attributes, representing
different kinds of relationships among them. We refer to these models as
attributed graphs. Consequently, existing graph clustering methods have been
recently extended to deal with node and edge attributes. This article is a
literature survey on this topic, organizing and presenting recent research
results in a uniform way, characterizing the main existing clustering methods
and highlighting their conceptual differences. We also cover the important
topic of clustering evaluation and identify current open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01678</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01678</id><created>2015-01-07</created><authors><author><keyname>Zhang</keyname><forenames>Changwang</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Chain</keyname><forenames>Benjamin M.</forenames></author></authors><title>LeoTask: a fast, flexible and reliable framework for computational
  research</title><categories>cs.SE cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LeoTask is a Java library for computation-intensive and time-consuming
research tasks. It automatically executes tasks in parallel on multiple CPU
cores on a computing facility. It uses a configuration file to enable automatic
exploration of parameter space and flexible aggregation of results, and
therefore allows researchers to focus on programming the key logic of a
computing task. It also supports reliable recovery from interruptions, dynamic
and cloneable networks, and integration with the plotting software Gnuplot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01689</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01689</id><created>2015-01-07</created><authors><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author><author><keyname>Zadimoghaddam</keyname><forenames>Morteza</forenames></author></authors><title>Sparse Solutions to Nonnegative Linear Systems and Applications</title><categories>cs.DS cs.IT cs.LG math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an efficient algorithm for finding sparse approximate solutions to
linear systems of equations with nonnegative coefficients. Unlike most known
results for sparse recovery, we do not require {\em any} assumption on the
matrix other than non-negativity. Our algorithm is combinatorial in nature,
inspired by techniques for the set cover problem, as well as the multiplicative
weight update method.
  We then present a natural application to learning mixture models in the PAC
framework. For learning a mixture of $k$ axis-aligned Gaussians in $d$
dimensions, we give an algorithm that outputs a mixture of $O(k/\epsilon^3)$
Gaussians that is $\epsilon$-close in statistical distance to the true
distribution, without any separation assumptions. The time and sample
complexity is roughly $O(kd/\epsilon^3)^{d}$. This is polynomial when $d$ is
constant -- precisely the regime in which known methods fail to identify the
components efficiently.
  Given that non-negativity is a natural assumption, we believe that our result
may find use in other settings in which we wish to approximately explain data
using a small number of a (large) candidate set of components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01692</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01692</id><created>2015-01-07</created><updated>2016-01-26</updated><authors><author><keyname>Tochimani</keyname><forenames>Azucena</forenames></author><author><keyname>Pinto</keyname><forenames>Maria Vaz</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Direct products in projective Segre codes</title><categories>math.AC cs.IT math.AG math.IT</categories><msc-class>13P25, 15A78, 14G50, 94B27</msc-class><journal-ref>Finite Fields Appl. 39 (2016), 96--110</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let K=Fq be a finite field. We introduce a family of projective
Reed-Muller-type codes called projective Segre codes. Using commutative algebra
and linear algebra methods, we study their basic parameters and show that they
are direct products of projective Reed-Muller-type codes. As a consequence we
recover some results on projective Reed-Muller-type codes over the Segre
variety and over projective tori.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01693</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01693</id><created>2015-01-07</created><authors><author><keyname>Escobar</keyname><forenames>Santiago</forenames><affiliation>Universitat Polit&#xe9;cnica de Val&#xe9;ncia</affiliation></author></authors><title>Proceedings XIV Jornadas sobre Programaci\'on y Lenguajes</title><categories>cs.PL cs.LO cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015</journal-ref><doi>10.4204/EPTCS.173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains a selection of the papers presented at the XIV Jornadas
sobre Programaci\'on y Lenguajes (PROLE 2014), held at C\'adiz, Spain, during
September 17th-19th, 2014. Previous editions of the workshop were held in
Madrid (2013), Almer\'ia (2012), A Coru\~na (2011), Val\'encia (2010), San
Sebasti\'an (2009), Gij\'on (2008), Zaragoza (2007), Sitges (2006), Granada
(2005), M\'alaga (2004), Alicante (2003), El Escorial (2002), and Almagro
(2001).
  Programming languages provide a conceptual framework which is necessary for
the development, analysis, optimization and understanding of programs and
programming tasks. The aim of the PROLE series of conferences (PROLE stems from
the spanish PROgramaci\'on y LEnguajes) is to serve as a meeting point for
spanish research groups which develop their work in the area of programming and
programming languages. The organization of this series of events aims at
fostering the exchange of ideas, experiences and results among these groups.
Promoting further collaboration is also one of the main goals of PROLE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01694</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01694</id><created>2015-01-07</created><authors><author><keyname>Kejriwal</keyname><forenames>Mayank</forenames></author><author><keyname>Miranker</keyname><forenames>Daniel P.</forenames></author></authors><title>A DNF Blocking Scheme Learner for Heterogeneous Datasets</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity Resolution concerns identifying co-referent entity pairs across
datasets. A typical workflow comprises two steps. In the first step, a blocking
method uses a one-many function called a blocking scheme to map entities to
blocks. In the second step, entities sharing a block are paired and compared.
Current DNF blocking scheme learners (DNF-BSLs) apply only to structurally
homogeneous tables. We present an unsupervised algorithmic pipeline for
learning DNF blocking schemes on RDF graph datasets, as well as structurally
heterogeneous tables. Previous DNF-BSLs are admitted as special cases. We
evaluate the pipeline on six real-world dataset pairs. Unsupervised results are
shown to be competitive with supervised and semi-supervised baselines. To the
best of our knowledge, this is the first unsupervised DNF-BSL that admits RDF
graphs and structurally heterogeneous tables as inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01695</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01695</id><created>2015-01-07</created><authors><author><keyname>Han</keyname><forenames>Bin</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>Robustness Properties of Dimensionality Reduction with Gaussian Random
  Matrices</title><categories>cs.IT math.FA math.IT math.NA math.PR</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the robustness properties of dimensionality reduction
with Gaussian random matrices having arbitrarily erased rows. We first study
the robustness property against erasure for the almost norm preservation
property of Gaussian random matrices by obtaining the optimal estimate of the
erasure ratio for a small given norm distortion rate. As a consequence, we
establish the robustness property of Johnson-Lindenstrauss lemma and the
robustness property of restricted isometry property with corruption for
Gaussian random matrices. Secondly, we obtain a sharp estimate for the optimal
lower and upper bounds of norm distortion rates of Gaussian random matrices
under a given erasure ratio. This allows us to establish the strong restricted
isometry property with the almost optimal RIP constants, which plays a central
role in the study of phaseless compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01696</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01696</id><created>2015-01-07</created><authors><author><keyname>Kejriwal</keyname><forenames>Mayank</forenames></author><author><keyname>Miranker</keyname><forenames>Daniel P.</forenames></author></authors><title>On the Complexity of Sorted Neighborhood</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Record linkage concerns identifying semantically equivalent records in
databases. Blocking methods are employed to avoid the cost of full pairwise
similarity comparisons on $n$ records. In a seminal work, Hernandez and Stolfo
proposed the Sorted Neighborhood blocking method. Several empirical variants
have been proposed in recent years. In this paper, we investigate the
complexity of the Sorted Neighborhood procedure on which the variants are
built. We show that achieving maximum performance on the Sorted Neighborhood
procedure entails solving a sub-problem, which is shown to be NP-complete by
reducing from the Travelling Salesman Problem. We also show that the
sub-problem can occur in the traditional blocking method. Finally, we draw on
recent developments concerning approximate Travelling Salesman solutions to
define and analyze three approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01697</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01697</id><created>2015-01-07</created><updated>2015-02-04</updated><authors><author><keyname>Ongie</keyname><forenames>Greg</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Super-resolution MRI Using Finite Rate of Innovation Curves</title><categories>cs.CV</categories><comments>Conference paper accepted to ISBI 2015. 4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a two-stage algorithm for the super-resolution of MR images from
their low-frequency k-space samples. In the first stage we estimate a
resolution-independent mask whose zeros represent the edges of the image. This
builds off recent work extending the theory of sampling signals of finite rate
of innovation (FRI) to two-dimensional curves. We enable its application to MRI
by proposing extensions of the signal models allowed by FRI theory, and by
developing a more robust and efficient means to determine the edge mask. In the
second stage of the scheme, we recover the super-resolved MR image using the
discretized edge mask as an image prior. We evaluate our scheme on simulated
single-coil MR data obtained from analytical phantoms, and compare against
total variation reconstructions. Our experiments show improved performance in
both noiseless and noisy settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01701</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01701</id><created>2015-01-07</created><authors><author><keyname>Enyioha</keyname><forenames>Chinwendu</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Preciado</keyname><forenames>Victor</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Distributed Resource Allocation for Epidemic control</title><categories>cs.SY math.OC physics.soc-ph</categories><comments>8 pages, 6 figures; submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed resource allocation strategy to control an epidemic
outbreak in a networked population based on a Distributed Alternating Direction
Method of Multipliers (D-ADMM) algorithm. We consider a linearized Susceptible-
Infected-Susceptible (SIS) epidemic spreading model in which agents in the
network are able to allocate vaccination resources (for prevention) and
antidotes (for treatment) in the presence of a contagion. We express our
epidemic control condition as a spectral constraint involving the
Perron-Frobenius eigenvalue, and formulate the resource allocation problem as a
Geometric Program (GP). Next, we separate the network-wide optimization problem
into subproblems optimally solved by each agent in a fully distributed way. We
conclude the paper by illustrating performance of our solution framework with
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01706</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01706</id><created>2015-01-07</created><authors><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Error Performance Analysis of the Symbol-Decision SC Polar Decoder</title><categories>cs.IT math.IT</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are the first provably capacity-achieving forward error
correction codes. To improve decoder throughput, the symbol-decision SC
algorithm makes hard-decision for multiple bits at a time. In this paper, we
prove that for polar codes, the symbol-decision SC algorithm is better than the
bit-decision SC algorithm in terms of the frame error rate (FER) performance
because the symbol-decision SC algorithm performs a local maximum likelihood
decoding within a symbol. Moreover, the bigger the symbol size, the better the
FER performance. Finally, simulation results over both the additive white
Gaussian noise channel and the binary erasure channel confirm our theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01708</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01708</id><created>2015-01-07</created><authors><author><keyname>Mo</keyname><forenames>Qun</forenames></author></authors><title>A Sharp Restricted Isometry Constant Bound of Orthogonal Matching
  Pursuit</title><categories>cs.IT math.IT</categories><comments>8 pages, submitted to the IEEE Transactions on Information Theory</comments><msc-class>15A23, 15A54, 42C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We shall show that if the restricted isometry constant (RIC)
$\delta_{s+1}(A)$ of the measurement matrix $A$ satisfies $$ \delta_{s+1}(A) &lt;
\frac{1}{\sqrt{s + 1}}, $$ then the greedy algorithm Orthogonal Matching
Pursuit(OMP) will succeed. That is, OMP can recover every $s$-sparse signal $x$
in $s$ iterations from $b = Ax$. Moreover, we shall show the upper bound of RIC
is sharp in the following sense. For any given $s \in \N$, we shall construct a
matrix $A$ with the RIC $$ \delta_{s+1}(A) = \frac{1}{\sqrt{s + 1}} $$ such
that OMP may not recover some $s$-sparse signal $x$ in $s$ iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01711</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01711</id><created>2015-01-07</created><updated>2015-04-21</updated><authors><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Frequent Directions : Simple and Deterministic Matrix Sketching</title><categories>cs.DS</categories><comments>28 pages , This paper contains Frequent Directions algorithm (see
  arXiv:1206.0594) and relative error bound on it (see arXiv:1307.7454)</comments><msc-class>68W40 (Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm called Frequent Directions for deterministic
matrix sketching in the row-updates model. The algorithm is presented an
arbitrary input matrix $A \in R^{n \times d}$ one row at a time. It performed
$O(d \times \ell)$ operations per row and maintains a sketch matrix $B \in
R^{\ell \times d}$ such that for any $k &lt; \ell$
  $\|A^TA - B^TB \|_2 \leq \|A - A_k\|_F^2 / (\ell-k)$ and $\|A -
\pi_{B_k}(A)\|_F^2 \leq \big(1 + \frac{k}{\ell-k}\big) \|A-A_k\|_F^2 $ .
  Here, $A_k$ stands for the minimizer of $\|A - A_k\|_F$ over all rank $k$
matrices (similarly $B_k$) and $\pi_{B_k}(A)$ is the rank $k$ matrix resulting
from projecting $A$ on the row span of $B_k$. We show both of these bounds are
the best possible for the space allowed. The summary is mergeable, and hence
trivially parallelizable. Moreover, Frequent Directions outperforms exemplar
implementations of existing streaming algorithms in the space-error tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01720</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01720</id><created>2015-01-07</created><authors><author><keyname>Ling</keyname><forenames>Jeffrey</forenames></author><author><keyname>Xiao</keyname><forenames>Kai</forenames></author><author><keyname>Yang</keyname><forenames>Dai</forenames></author></authors><title>Online Algorithms Modeled After Mousehunt</title><categories>cs.DS</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a variety of novel online algorithm problems inspired
by the game Mousehunt. We consider a number of basic models that approximate
the game, and we provide solutions to these models using Markov Decision
Processes, deterministic online algorithms, and randomized online algorithms.
We analyze these solutions' performance by deriving results on their
competitive ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01721</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01721</id><created>2015-01-07</created><authors><author><keyname>Wolfe</keyname><forenames>Nathan</forenames></author><author><keyname>Zou</keyname><forenames>Ethan</forenames></author><author><keyname>Ren</keyname><forenames>Ling</forenames></author><author><keyname>Yu</keyname><forenames>Xiangyao</forenames></author></authors><title>Optimizing Path ORAM for Cloud Storage Applications</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live in a world where our personal data are both valuable and vulnerable
to misappropriation through exploitation of security vulnerabilities in online
services. For instance, Dropbox, a popular cloud storage tool, has certain
security flaws that can be exploited to compromise a user's data, one of which
being that a user's access pattern is unprotected. We have thus created an
implementation of Path Oblivious RAM (Path ORAM) for Dropbox users to obfuscate
path access information to patch this vulnerability. This implementation
differs significantly from the standard usage of Path ORAM, in that we
introduce several innovations, including a dynamically growing and shrinking
tree architecture, multi-block fetching, block packing and the possibility for
multi-client use. Our optimizations together produce about a 77% throughput
increase and a 60% reduction in necessary tree size; these numbers vary with
file size distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01723</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01723</id><created>2015-01-07</created><authors><author><keyname>Abdelsamea</keyname><forenames>M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Marghny H.</forenames></author><author><keyname>Bamatraf</keyname><forenames>Mohamed</forenames></author></authors><title>An Effective Image Feature Classiffication using an improved SOM</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image feature classification is a challenging problem in many computer vision
applications, specifically, in the fields of remote sensing, image analysis and
pattern recognition. In this paper, a novel Self Organizing Map, termed
improved SOM (iSOM), is proposed with the aim of effectively classifying
Mammographic images based on their texture feature representation. The main
contribution of the iSOM is to introduce a new node structure for the map
representation and adopting a learning technique based on Kohonen SOM
accordingly. The main idea is to control, in an unsupervised fashion, the
weight updating procedure depending on the class reliability of the node,
during the weight update time. Experiments held on a real Mammographic images.
Results showed high accuracy compared to classical SOM and other state-of-art
classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01725</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01725</id><created>2015-01-07</created><authors><author><keyname>Hong</keyname><forenames>Seung-Eun</forenames></author><author><keyname>Oh</keyname><forenames>Kyoung-Sub</forenames></author></authors><title>Load-Modulated Single-RF MIMO Transmission for Spatially Multiplexed QAM
  Signals</title><categories>cs.IT math.IT</categories><comments>5 pages with 2-column format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, MIMO has become an indispensable scheme for providing significant
spectral efficiency in wireless communication and for future wireless system,
recently, it goes to two extremes: massive MIMO and single-RF MIMO. This paper,
which is put in the latter, utilizes load-modulated arrays with only reactance
loads for single-RF transmission of spatially multiplexed QAM signals. To
alleviate the need for iterative processes while considering mutual coupling in
the compact antenna, we present a novel design methodology for the loading
network, which enables the exact computation of the three reactance loads per
antenna element and also the perfect matching to the source with the
opportunity to select appropriate analog tunable loads. We verify the design
methodology by comparing the calculated values for some key parameters with the
values from the circuit simulation. In addition, as an evaluation of the
proposed architecture, we perform the bit error rate (BER) comparison which
shows that our scheme with ideal loading is comparable to the conventional
MIMO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01726</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01726</id><created>2015-01-07</created><authors><author><keyname>Jin</keyname><forenames>Hu</forenames></author><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author></authors><title>Opportunistic Interference Alignment for Random Access Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 8 figures, to appear in IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interference management problem among multiple overlapped random access
networks (RANs) is investigated, each of which operates with slotted ALOHA
protocol. Assuming that access points and users have multiple antennas, a novel
opportunistic interference alignment~(OIA) is proposed to mitigate interference
among overlapped RANs. The proposed technique intelligently combines the
transmit beamforming technique at the physical layer and the opportunistic
packet transmission at the medium access control layer. The transmit
beamforming is based on interference alignment and the opportunistic packet
transmission is based on the generating interference of users to other RANs,
which can be regarded as a joint optimization of the physical layer and the
medium access control layer. It is shown that the proposed OIA protocol
significantly outperforms the conventional schemes such as multi-packet
reception and interference nulling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01728</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01728</id><created>2015-01-07</created><updated>2015-04-14</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Optimized Training for Net Energy Maximization in Multi-Antenna Wireless
  Energy Transfer over Frequency-Selective Channel</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the training design problem for multiple-input
single-output (MISO) wireless energy transfer (WET) systems in
frequency-selective channels, where the frequency-diversity and
energy-beamforming gains can be both reaped to maximize the transferred energy
by efficiently learning the channel state information (CSI) at the energy
transmitter (ET). By exploiting channel reciprocity, a new two-phase channel
training scheme is proposed to achieve the diversity and beamforming gains,
respectively. In the first phase, pilot signals are sent from the energy
receiver (ER) over a selected subset of the available frequency sub-bands,
through which the ET determines a certain number of &quot;strongest&quot; sub-bands with
largest antenna sum-power gains and sends their indices to the ER. In the
second phase, the selected sub-bands are further trained by the ER, so that the
ET obtains a refined estimate of the corresponding MISO channels to implement
energy beamforming for WET. A training design problem is formulated and
optimally solved, which takes into account the channel training overhead by
maximizing the net harvested energy at the ER, defined as the average harvested
energy offset by that consumed in the two-phase training. Moreover, asymptotic
analysis is obtained for systems with a large number of antennas or a large
number of sub-bands to gain useful insights on the optimal training design.
Finally, numerical results are provided to corroborate our analysis and show
the effectiveness of the proposed scheme that optimally balances the diversity
and beamforming gains achieved in MISO WET systems with limited-energy
training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01741</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01741</id><created>2015-01-08</created><updated>2015-02-27</updated><authors><author><keyname>Kenter</keyname><forenames>Franklin</forenames></author><author><keyname>Radcliffe</keyname><forenames>Mary</forenames></author></authors><title>A linear k-fold Cheeger inequality</title><categories>math.CO cs.DM cs.DS math.PR math.SP</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G$, the classical Cheeger constant, $h_G$,
measures the optimal partition of the vertices into 2 parts with relatively few
edges between them based upon the sizes of the parts. The well-known Cheeger's
inequality states that $2 \lambda_1 \le h_G \le \sqrt {2 \lambda_1}$ where
$\lambda_1$ is the minimum nontrivial eigenvalue of the normalized Laplacian
matrix.
  Recent work has generalized the concept of the Cheeger constant when
partitioning the vertices of a graph into $k &gt; 2$ parts. While there are
several approaches, recent results have shown these higher-order Cheeger
constants to be tightly controlled by $\lambda_{k-1}$, the $(k-1)$-th
nontrivial eigenvalue, to within a quadratic factor.
  We present a new higher-order Cheeger inequality with several new
perspectives. First, we use an alternative higher-order Cheeger constant which
considers an &quot;average case&quot; approach. We show this measure is related to the
average of the first $k-1$ nontrivial eigenvalues of the normalized Laplacian
matrix. Further, using recent techniques, our results provide linear
inequalities using the $\infty$-norms of the corresponding eigenvectors.
Consequently, unlike previous results, this result is relevant even when
$\lambda_{k-1} \to 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01742</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01742</id><created>2015-01-08</created><authors><author><keyname>Fehenberger</keyname><forenames>Tobias</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Hanik</keyname><forenames>Norbert</forenames></author></authors><title>LDPC Coded Modulation with Probabilistic Shaping for Optical Fiber
  Systems</title><categories>cs.IT math.IT</categories><comments>3 pages, 3 figures. Paper is accepted for presentation at OFC 2015.
  Following the submission to OFC, the blue box in Fig. 1 has been corrected
  and reference [3] has been updated</comments><doi>10.1364/OFC.2015.Th2A.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An LDPC coded modulation scheme with probabilistic shaping, optimized
interleavers and noniterative demapping is proposed. Full-field simulations
show an increase in transmission distance by 8% compared to uniformly
distributed input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01744</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01744</id><created>2015-01-08</created><authors><author><keyname>Yuan</keyname><forenames>Wenjia</forenames></author><author><keyname>Wengrowski</keyname><forenames>Eric</forenames></author><author><keyname>Dana</keyname><forenames>Kristin J.</forenames></author><author><keyname>Ashok</keyname><forenames>Ashwin</forenames></author><author><keyname>Gruteser</keyname><forenames>Marco</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan</forenames></author></authors><title>Optimal Radiometric Calibration for Camera-Display Communication</title><categories>cs.CV</categories><comments>10 pages, Submitted to CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method for communicating between a camera and display by
embedding and recovering hidden and dynamic information within a displayed
image. A handheld camera pointed at the display can receive not only the
display image, but also the underlying message. These active scenes are
fundamentally different from traditional passive scenes like QR codes because
image formation is based on display emittance, not surface reflectance.
Detecting and decoding the message requires careful photometric modeling for
computational message recovery. Unlike standard watermarking and steganography
methods that lie outside the domain of computer vision, our message recovery
algorithm uses illumination to optically communicate hidden messages in real
world scenes. The key innovation of our approach is an algorithm that performs
simultaneous radiometric calibration and message recovery in one convex
optimization problem. By modeling the photometry of the system using a
camera-display transfer function (CDTF), we derive a physics-based kernel
function for support vector machine classification. We demonstrate that our
method of optimal online radiometric calibration (OORC) leads to an efficient
and robust algorithm for computational messaging between nine commercial
cameras and displays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01745</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01745</id><created>2015-01-08</created><authors><author><keyname>Guttenberg</keyname><forenames>Nicholas</forenames></author><author><keyname>Laneuville</keyname><forenames>Matthieu</forenames></author><author><keyname>Ilardo</keyname><forenames>Melissa</forenames></author><author><keyname>Aubert-Kato</keyname><forenames>Nathanael</forenames></author></authors><title>Transferable measurements of Heredity in models of the Origins of Life</title><categories>q-bio.PE cs.CE nlin.AO</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a metric which can be used to compute the amount of heritable
variation enabled by a given dynamical system. A distribution of selection
pressures is used such that each pressure selects a particular fixed point via
competitive exclusion in order to determine the corresponding distribution of
potential fixed points in the population dynamics. This metric accurately
detects the number of species present in artificially prepared test systems,
and furthermore can correctly determine the number of heritable sets in
clustered transition matrix models in which there are no clearly defined
genomes. Finally, we apply our metric to the GARD model and show that it
accurately reproduces prior measurements of the model's heritability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01755</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01755</id><created>2015-01-08</created><authors><author><keyname>Golestani</keyname><forenames>Hossein Bakhshi</forenames></author><author><keyname>Ghanbari</keyname><forenames>Mohammed</forenames></author></authors><title>Minimization of image watermarking side effects through subjective
  optimization</title><categories>cs.MM cs.CR</categories><comments>17 pages,11 figures, IET Image Processing Journal</comments><journal-ref>IET Image Processing, vol. 7, no. 8, pp. 733-741, 2013</journal-ref><doi>10.1049/iet-ipr.2013.0086</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the use of Structural Similaritys (SSIM) index on the
minimized side effect to image watermarking. For fast implementation and more
compatibility with the standard DCT based codecs, watermark insertion is
carried out on the DCT coefficients and hence a SSIM model for DCT based
watermarking is developed. For faster implementation, the SSIM index is
maximized over independent 4x4 non-overlapped blocks but the disparity between
the adjacent blocks reduces the overall image quality. This problem is resolved
through optimization of overlapped blocks, but, the higher image quality is
achieved at a cost of high computational complexity. To reduce the
computational complexity while preserving the good quality, optimization of
semi-overlapped blocks is introduced. We show that while SSIM-based
optimization over overlapped blocks has as high as 64 times the complexity of
the 4x4 non-overlapped method, with semi-overlapped optimization the high
quality of overlapped method is preserved only at a cost of less than 8 times
the non-overlapped method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01758</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01758</id><created>2015-01-08</created><authors><author><keyname>Golestani</keyname><forenames>Hossein Bakhshi</forenames></author><author><keyname>Ghaemmaghami</keyname><forenames>Shahrokh</forenames></author></authors><title>Enhance Robustness of Image-in-Image Watermarking through Data
  Partitioning</title><categories>cs.MM cs.CR</categories><comments>5 pages, 7 figures, IEEE TENCON2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vulnerability of watermarking schemes against intense signal processing
attacks is generally a major concern, particularly when there are techniques to
reproduce an acceptable copy of the original signal with no chance for
detecting the watermark. In this paper, we propose a two-layer, data
partitioning (DP) based, image in image watermarking method in the DCT domain
to improve the watermark detection performance. Truncated singular value
decomposition, binary wavelet decomposition and spatial scalability idea in
H.264/SVC are analyzed and employed as partitioning methods. It is shown that
the proposed scheme outperforms its two recent competitors in terms of both
data payload and robustness to intense attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01773</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01773</id><created>2015-01-08</created><authors><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author></authors><title>Estimates for the growth of inverse determinant sums of quasi-orthogonal
  and number field lattices</title><categories>cs.IT math.IT math.NT</categories><comments>This is an extended and corrected version of R. Vehkalahti and L.
  Luzzi,&quot;Measuring the growth of inverse determinants sums of a family of
  quasi-orthogonal codes&quot;, which appeared in proc. IZS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse determinant sums appear naturally as a tool for analyzing performance
of space-time codes in Rayleigh fading channels. This work will analyze the
growth of inverse determinant sums of a family of quasi-orthogonal codes and
will show that the growths are in logarithmic class. This is considerably lower
than that of comparable number field codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01779</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01779</id><created>2015-01-08</created><updated>2015-04-22</updated><authors><author><keyname>Mizera</keyname><forenames>Andrzej</forenames></author><author><keyname>Pang</keyname><forenames>Jun</forenames></author><author><keyname>Yuan</keyname><forenames>Qixia</forenames></author></authors><title>Reviving the Two-state Markov Chain Approach (Technical Report)</title><categories>cs.CE cs.LO</categories><comments>24 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Boolean networks (PBNs) is a well-established computational
framework for modelling biological systems. The steady-state dynamics of PBNs
is of crucial importance in the study of such systems. However, for large PBNs,
which often arise in systems biology, obtaining the steady-state distribution
poses a significant challenge. In fact, statistical methods for steady-state
approximation are the only viable means when dealing with large networks. In
this paper, we revive the two-state Markov chain approach presented in the
literature. We first identify a problem of generating biased results, due to
the size of the initial sample with which the approach needs to start and we
propose a few heuristics to avoid such a pitfall. Second, we conduct an
extensive experimental comparison of the two-state Markov chain approach and
another approach based on the Skart method and we show that statistically the
two-state Markov chain has a better performance. Finally, we apply this
approach to a large PBN model of apoptosis in hepatocytes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01780</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01780</id><created>2015-01-08</created><authors><author><keyname>Zhou</keyname><forenames>Kuang</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Pan</keyname><forenames>Quan</forenames></author></authors><title>Evidential Communities for Complex Networks</title><categories>cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>15th International Conference on Information Processing and
  Management of Uncertainty in Knowledge-Based Systems, Jul 2014, Montpellier,
  France. pp.557 - 566</journal-ref><doi>10.1007/978-3-319-08795-5_57</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is of great importance for understand-ing graph structure
in social networks. The communities in real-world networks are often
overlapped, i.e. some nodes may be a member of multiple clusters. How to
uncover the overlapping communities/clusters in a complex network is a general
problem in data mining of network data sets. In this paper, a novel algorithm
to identify overlapping communi-ties in complex networks by a combination of an
evidential modularity function, a spectral mapping method and evidential
c-means clustering is devised. Experimental results indicate that this
detection approach can take advantage of the theory of belief functions, and
preforms good both at detecting community structure and determining the
appropri-ate number of clusters. Moreover, the credal partition obtained by the
proposed method could give us a deeper insight into the graph structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01783</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01783</id><created>2015-01-08</created><authors><author><keyname>Bonichon</keyname><forenames>Nicolas</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames><affiliation>SOC</affiliation></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames><affiliation>SOC</affiliation></author><author><keyname>Perkovi&#x107;</keyname><forenames>Ljubomir</forenames><affiliation>SOC</affiliation></author><author><keyname>Van Renssen</keyname><forenames>Andr&#xe9;</forenames><affiliation>NII</affiliation></author></authors><title>Upper and Lower Bounds for Competitive Online Routing on Delaunay
  Triangulations</title><categories>cs.CG cs.DM cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a weighted graph G where vertices are points in the plane and edges
are line segments. The weight of each edge is the Euclidean distance between
its two endpoints. A routing algorithm on G has a competitive ratio of c if the
length of the path produced by the algorithm from any vertex s to any vertex t
is at most c times the length of the shortest path from s to t in G. If the
length of the path is at most c times the Euclidean distance from s to t, we
say that the routing algorithm on G has a routing ratio of c.We present an
online routing algorithm on the Delaunay triangulation with competitive and
routing ratios of 5.90. This improves upon the best known algorithm that has
competitive and routing ratio 15.48. The algorithm is a generalization of the
deterministic 1-local routing algorithm by Chew on the L1-Delaunay
triangulation. When a message follows the routing path produced by our
algorithm, its header need only contain the coordinates of s and t. This is an
improvement over the currently known competitive routing algorithms on the
Delaunay triangulation, for which the header of a message must additionally
contain partial sums of distances along the routing path.We also show that the
routing ratio of any deterministic k-local algorithm is at least 1.70 for the
Delaunay triangulation and 2.70 for the L1-Delaunay triangulation. In the case
of the L1-Delaunay triangulation, this implies that even though there exists a
path between two points x and y whose length is at most 2.61|[xy]| (where
|[xy]| denotes the length of the line segment [xy]), it is not always possible
to route a message along a path of length less than 2.70|[xy]|. From these
bounds on the routing ratio, we derive lower bounds on the competitive ratio of
1.23 for Delaunay triangulations and 1.12 for L1-Delaunay triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01792</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01792</id><created>2015-01-08</created><updated>2015-01-16</updated><authors><author><keyname>Pierzchlewski</keyname><forenames>Jacek</forenames></author><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author></authors><title>Frequency Selective Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>ieee notice page + 4 pages + references page, 7 figures, submitted to
  IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the authors describe the problem of acquisition of interfered
signals and formulate a filtering problem. A frequency-selective compressed
sensing technique is proposed as a solution to this problem. Signal acquisition
is critical in facilitating frequency-selective compressed sensing. The authors
propose a filtering compressed sensing parameter, which allows to assess if a
given acquisition process makes frequency-selective compressed sensing possible
for a given filtering problem. A numerical experiment which shows how the
described method works in practice is conducted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01797</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01797</id><created>2015-01-08</created><authors><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Kamilov</keyname><forenames>Ulugbek</forenames></author></authors><title>Inference for Generalized Linear Models via Alternating Directions and
  Bethe Free Energy Minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized Linear Models (GLMs), where a random vector $\mathbf{x}$ is
observed through a noisy, possibly nonlinear, function of a linear transform
$\mathbf{z}=\mathbf{Ax}$ arise in a range of applications in nonlinear
filtering and regression. Approximate Message Passing (AMP) methods, based on
loopy belief propagation, are a promising class of approaches for approximate
inference in these models. AMP methods are computationally simple, general, and
admit precise analyses with testable conditions for optimality for large i.i.d.
transforms $\mathbf{A}$. However, the algorithms can easily diverge for general
$\mathbf{A}$. This paper presents a convergent approach to the generalized AMP
(GAMP) algorithm based on direct minimization of a large-system limit
approximation of the Bethe Free Energy (LSL-BFE). The proposed method uses a
double-loop procedure, where the outer loop successively linearizes the LSL-BFE
and the inner loop minimizes the linearized LSL-BFE using the Alternating
Direction Method of Multipliers (ADMM). The proposed method, called ADMM-GAMP,
is similar in structure to the original GAMP method, but with an additional
least-squares minimization. It is shown that for strictly convex, smooth
penalties, ADMM-GAMP is guaranteed to converge to a local minima of the
LSL-BFE, thus providing a convergent alternative to GAMP that is stable under
arbitrary transforms. Simulations are also presented that demonstrate the
robustness of the method for non-convex penalties as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01809</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01809</id><created>2015-01-08</created><updated>2015-12-18</updated><authors><author><keyname>Rathgeber</keyname><forenames>Florian</forenames></author><author><keyname>Ham</keyname><forenames>David A.</forenames></author><author><keyname>Mitchell</keyname><forenames>Lawrence</forenames></author><author><keyname>Lange</keyname><forenames>Michael</forenames></author><author><keyname>Luporini</keyname><forenames>Fabio</forenames></author><author><keyname>McRae</keyname><forenames>Andrew T. T.</forenames></author><author><keyname>Bercea</keyname><forenames>Gheorghe-Teodor</forenames></author><author><keyname>Markall</keyname><forenames>Graham R.</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author></authors><title>Firedrake: automating the finite element method by composing
  abstractions</title><categories>cs.MS cs.NA math.NA</categories><comments>Revisions in response to referee reports, new experimental results</comments><acm-class>G.1.8; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Firedrake is a new tool for automating the numerical solution of partial
differential equations. Firedrake adopts the domain-specific language for the
finite element method of the FEniCS project, but with a pure Python
runtime-only implementation centred on the composition of several existing and
new abstractions for particular aspects of scientific computing. The result is
a more complete separation of concerns which eases the incorporation of
separate contributions from computer scientists, numerical analysts and
application specialists. These contributions may add functionality, or improve
performance.
  Firedrake benefits from automatically applying new optimisations. This
includes factorising mixed function spaces, transforming and vectorising inner
loops, and intrinsically supporting block matrix operations. Importantly,
Firedrake presents a simple public API for escaping the UFL abstraction. This
allows users to implement non--finite-element operations such as flux-limiters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01811</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01811</id><created>2015-01-08</created><updated>2015-02-09</updated><authors><author><keyname>Yaroslavsky</keyname><forenames>L.</forenames></author></authors><title>Is &quot;Compressed Sensing&quot; compressive? Can it beat the Nyquist Sampling
  Approach?</title><categories>physics.optics cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data compression capability of &quot;Compressed sensing (sampling)&quot; in signal
discretization is numerically evaluated and found to be far from the
theoretical upper bound defined by signal sparsity. It is shown that, for the
cases when ordinary sampling with subsequent data compression is prohibitive,
there is at least one more efficient, in terms of data compression capability,
and more simple and intuitive alternative to Compressed sensing: random sparse
sampling and restoration of image band-limited approximations based on energy
compaction capability of transforms. It is also shown that assertions that
&quot;Compressed sensing&quot; can beat the Nyquist sampling approach are rooted in
misinterpretation of the sampling theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01817</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01817</id><created>2015-01-08</created><updated>2015-01-12</updated><authors><author><keyname>Gogacz</keyname><forenames>Tomasz</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jerzy</forenames></author></authors><title>The Hunt for a Red Spider: Conjunctive Query Determinacy Is Undecidable</title><categories>cs.DB</categories><doi>10.1109/LICS.2015.35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve a well known, long-standing open problem in relational databases
theory, showing that the conjunctive query determinacy problem (in its
&quot;unrestricted&quot; version) is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01819</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01819</id><created>2015-01-08</created><updated>2015-06-15</updated><authors><author><keyname>Manoussakis</keyname><forenames>George</forenames></author></authors><title>New algorithms for $k$-degenerate graphs</title><categories>cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is $k$-degenerate if any induced subgraph has a vertex of degree at
most $k$. In this paper we prove new algorithms for cliques and similar
structures for these graphs. We design linear time Fixed-Parameter Tractable
algorithms for induced and non induced bicliques. We prove an algorithm listing
all maximal bicliques in time $\mathcal{O}(k^{3}(n-k)2^{k})$, improving the
result of [D. Eppstein, Arboricity and bipartite subgraph listing algorithms,
Information Processing Letters, (1994)]. We construct an algorithm listing all
cliques of size $l$ in time $\mathcal{O}(l(n-k)k(k-1)^{l-2})$, improving a
result of [N. Chiba and T. Nishizeki, Arboricity and subgraph listing
algorithms, SIAM, (1985)]. As a consequence we can list all triangles in such
graphs in time $\mathcal{O}((n-k)k^{2})$ improving the previous bound of
$\mathcal{O}(nk^2)$. We show another optimal algorithm listing all maximal
cliques in time $\mathcal{O}(k(n-k)3^{k/3})$, matching the best possible
complexity proved in [D. Eppstein, M. L\&quot;offler, and D. Strash, Listing all
maximal cliques in large sparse real-world graphs, JEA, (2013)]. Finally we
prove $(2-\frac{1}{k})$ and $\mathcal{O}(k(\log\log k)^{2}\slash (\log
k)^{3})$-approximation algorithms for the minimum vertex cover and the maximum
clique problems, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01822</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01822</id><created>2015-01-08</created><authors><author><keyname>van Kreveld</keyname><forenames>Marc</forenames></author><author><keyname>Loffler</keyname><forenames>Maarten</forenames></author><author><keyname>Staals</keyname><forenames>Frank</forenames></author></authors><title>Central Trajectories</title><categories>cs.CG</categories><comments>Full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important task in trajectory analysis is clustering. The results of a
clustering are often summarized by a single representative trajectory and an
associated size of each cluster. We study the problem of computing a suitable
representative of a set of similar trajectories. To this end we define a
central trajectory $\mathcal{C}$, which consists of pieces of the input
trajectories, switches from one entity to another only if they are within a
small distance of each other, and such that at any time $t$, the point
$\mathcal{C}(t)$ is as central as possible. We measure centrality in terms of
the radius of the smallest disk centered at $\mathcal{C}(t)$ enclosing all
entities at time $t$, and discuss how the techniques can be adapted to other
measures of centrality. We first study the problem in $\mathbb{R}^1$, where we
show that an optimal central trajectory $\mathcal{C}$ representing $n$
trajectories, each consisting of $\tau$ edges, has complexity $\Theta(\tau
n^2)$ and can be computed in $O(\tau n^2 \log n)$ time. We then consider
trajectories in $\mathbb{R}^d$ with $d\geq 2$, and show that the complexity of
$\mathcal{C}$ is at most $O(\tau n^{5/2})$ and can be computed in $O(\tau n^3)$
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01825</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01825</id><created>2015-01-08</created><updated>2015-04-12</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Unified Convex Optimization Approach to Super-Resolution Based on
  Localized Kernels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of resolving the fine details of a signal from its coarse scale
measurements or, as it is commonly referred to in the literature, the
super-resolution problem arises naturally in engineering and physics in a
variety of settings. We suggest a unified convex optimization approach for
super-resolution. The key is the construction of an interpolating polynomial
based on localized kernels. We also show that the localized kernels act as the
connecting thread to another wide-spread problem of stream of pulses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01826</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01826</id><created>2015-01-08</created><updated>2015-08-05</updated><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Designing Securely and Reliably Connected Wireless Sensor Networks</title><categories>cs.CR cs.DM cs.IT math.IT</categories><comments>A critical error is found in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor networks, the $q$-composite key predistribution scheme is
a widely recognized way to secure communications. Although connectivity
properties of secure sensor networks with the $q$-composite scheme have been
studied in the literature, few results address physical transmission
constraints since it is challenging to analyze the network connectivity in
consideration of both the $q$-composite scheme and transmission constraints
together. These transmission constraints reflect real-world implementations of
sensor networks in which two sensors have to be within a certain distance from
each other to communicate. In this paper, we rigorously derive conditions for
connectivity in sensor networks employing the $q$-composite scheme under
transmission constraints. Furthermore, we extend the analysis to consider the
unreliability of wireless links by modeling each link being independently
active with some probability. Our results provide useful guidelines for
designing securely and reliably connected sensor networks. We also present
numerical experiments to confirm the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01829</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01829</id><created>2015-01-08</created><updated>2015-06-09</updated><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>Performance Analysis and Optimal Filter Design for Sigma-Delta
  Modulation via Duality with DPCM</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling above the Nyquist rate is at the heart of sigma-delta modulation,
where the increase in sampling rate is translated to a reduction in the overall
(mean-squared-error) reconstruction distortion. This is attained by using a
feedback filter at the encoder, in conjunction with a low-pass filter at the
decoder. The goal of this work is to characterize the optimal trade-off between
the per-sample quantization rate and the resulting mean-squared-error
distortion, under various restrictions on the feedback filter. To this end, we
establish a duality relation between the performance of sigma-delta modulation,
and that of differential pulse-code modulation when applied to (discrete-time)
band-limited inputs. As the optimal trade-off for the latter scheme is fully
understood, the full characterization for sigma-delta modulation, as well as
the optimal feedback filters, immediately follow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01858</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01858</id><created>2015-01-08</created><authors><author><keyname>Gangula</keyname><forenames>Rajeev</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>Optimization of Energy Harvesting MISO Communication System with
  Feedback</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization of a point-to-point (p2p) multipleinput single-output (MISO)
communication system is considered when both the transmitter (TX) and the
receiver (RX) have energy harvesting (EH) capabilities. The RX is interested in
feeding back the channel state information (CSI) to the TX to help improve the
transmission rate. The objective is to maximize the throughput by a deadline,
subject to the EH constraints at the TX and the RX. The throughput metric
considered is an upper bound on the ergodic rate of the MISO channel with
beamforming and limited feedback. Feedback bit allocation and transmission
policies that maximize the upper bound on the ergodic rate are obtained. Tools
from majorization theory are used to simplify the formulated optimization
problems. Optimal policies obtained for the modified problem outperform the
naive scheme in which no intelligent management of energy is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01862</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01862</id><created>2015-01-08</created><authors><author><keyname>Tran-Ha</keyname><forenames>Vu</forenames></author><author><keyname>Vu</keyname><forenames>Quang-Doanh</forenames></author><author><keyname>Hong</keyname><forenames>Een-Kee</forenames></author></authors><title>Time Reversal-based Transmissions with Distributed Power Allocation for
  Two-Tier Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio pollution and power consumption problems lead to innovative development
of green heterogeneous networks (HetNet). Time reversal (TR) technique which
has been validated from wide- to narrow-band transmissions is evaluated as one
of most prominent linear precoders with superior capability of harvesting
signal energy. In this paper, we consider a new HetNet model, in which
TR-employed femtocell is proposed to attain saving power benefits whereas
macrocell utilizes the beam-forming algorithm based on zero-forcing principle,
over frequency selective channels. In the considered HetNet, the practical case
of limited signaling information exchanged via backhaul connections is also
taken under advisement. We hence organize a distributed power loading strategy,
in which macrocell users are treated with a superior priority compared to
femtocell users. By Monte-Carlo simulation, the obtained results show that TR
is preferred to zero-forcing in the perspective of beamforming technique for
femtocell environments due to very high achievable gain in saving energy, and
the validity of power loading strategy is verified over multipath channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01864</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01864</id><created>2015-01-08</created><authors><author><keyname>Dai</keyname><forenames>Mingbo</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Transmit Beamforming for MISO Broadcast Channels with Statistical and
  Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transaction on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on linear beamforming design and power allocation strategy
for ergodic rate optimization in a two-user Multiple-Input-Single-Output (MISO)
system with statistical and delayed channel state information at the
transmitter (CSIT). We propose a transmission strategy, denoted as Statistical
Alternative MAT (SAMAT), which exploits both channel statistics and delayed
CSIT. Firstly, with statistical CSIT only, we focus on statistical beamforming
(SBF) design that maximizes a lower bound on the ergodic sum-rate. Secondly,
relying on both statistical and delayed CSIT, an iterative algorithm is
proposed to compute the precoding vectors of Alternative MAT (AMAT), originally
proposed by Yang et al., which maximizes an approximation of the ergodic
sum-rate with equal power allocation. Finally, via proper power allocation, the
SAMAT framework is proposed to softly bridge between SBF and AMAT for an
arbitrary number of transmit antennas and signal-to-noise ratio (SNR). A
necessary condition for the power allocation optimization is identified from
the Karush-Kuhn-Tucker (KKT) conditions. The optimum power allocation to
maximize an ergodic sum-rate approximation is computed using Sequential
Quadratic Programming (SQP). Simulation results show that the proposed SAMAT
scheme yields a significant sum-rate enhancement over both SBF and AMAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01866</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01866</id><created>2015-01-08</created><authors><author><keyname>Roorda</keyname><forenames>Dirk</forenames></author></authors><title>The Hebrew Bible as Data: Laboratory - Sharing - Experiences</title><categories>cs.CL cs.DL</categories><comments>12 pages, 5 figures, follow up on the workshop Biblical Scholarship
  and Humanities Computing: Data Types, Text, Language and Interpretation, held
  at the Lorentz Centre Leiden from 6 Feb 2012 through 10 Feb 2012,
  http://www.lorentzcenter.nl/lc/web/2012/480/report.php3?wsid=480&amp;venue=Oort</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The systematic study of ancient texts including their production,
transmission and interpretation is greatly aided by the digital methods that
started taking off in the 1970s. But how is that research in turn transmitted
to new generations of researchers? We tell a story of Bible and computer across
the decades and then point out the current challenges: (1) finding a stable
data representation for changing methods of computation; (2) sharing results in
inter- and intra-disciplinary ways, for reproducibility and
cross-fertilization. We report recent developments in meeting these challenges.
The scene is the text database of the Hebrew Bible, constructed by the Eep
Talstra Centre for Bible and Computer (ETCBC), which is still growing in detail
and sophistication. We show how a subtle mix of computational ingredients
enable scholars to research the transmission and interpretation of the Hebrew
Bible in new ways: (1) a standard data format, Linguistic Annotation Framework
(LAF); (2) the methods of scientific computing, made accessible by
(interactive) Python and its associated ecosystem. Additionally, we show how
these efforts have culminated in the construction of a new, publicly accessible
search engine SHEBANQ, where the text of the Hebrew Bible and its underlying
data can be queried in a simple, yet powerful query language MQL, and where
those queries can be saved and shared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01868</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01868</id><created>2015-01-08</created><authors><author><keyname>Dardouri</keyname><forenames>Samia</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Performance Analysis Of Resource Scheduling In LTE Femtocells Networks</title><categories>cs.NI</categories><comments>11 pages, 9 figures, 3 tables, The Sixth International Conference on
  Networks &amp; Communications (NETCOM - 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3GPP has introduced LTE Femtocells to manipulate the traffic for indoor users
and to minimize the charge on the Macro cells. A key mechanism in the LTE
traffic handling is the packet scheduler which is in charge of allocating
resources to active flows in both the frequency and time dimension. So several
scheduling algorithms need to be analyzed for femtocells networks. In this
paper we introduce a performance analysis of three distinct scheduling
algorithms of mixed type of traffic flows in LTE femtocells networks. The
particularly study is evaluated in terms of throughput, packet loss ratio,
fairness index and spectral efficiency
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01875</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01875</id><created>2015-01-08</created><authors><author><keyname>Ali</keyname><forenames>Abdalha</forenames></author><author><keyname>Alrasheedi</keyname><forenames>Muasaad</forenames></author><author><keyname>Ouda</keyname><forenames>Abdelkader</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A study of the interface usability issues of mobile learning
  applications for smart phones from the users perspective</title><categories>cs.CY cs.HC</categories><comments>16 pages, 7 figures, 5 tables, and 31 conferences. in International
  Journal on Integrating Technology in Education (IJITE) Vol.3, No.4, December
  2014</comments><doi>10.5121/ijite.2014.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conceptual framework for measuring the usability characteristics of mobile
learning (m-Learning) application has been developed. Furthermore, a software
prototype for smartphones to assess usability issues of m-Learning applications
has also been designed and implemented. This prototype has been developed,
using Java language and the Android Software Development Kit, based on the
recommended guidelines of the proposed conceptual framework. The usability of
the proposed model was compared to a generally available similar mobile
application (based on the Blackboard) by conducting a questionnairebased survey
at Western University. The two models were evaluated in terms of ease of use,
user satisfaction, attractiveness, and learnability. The results of the
questionnaire showed that the participants considered the user interface based
on our proposed framework more user-friendly as compared to the
Blackboard-based user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01894</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01894</id><created>2015-01-08</created><authors><author><keyname>Rajan</keyname><forenames>Vinodh</forenames></author></authors><title>Quantifying Scripts: Defining metrics of characters for quantitative and
  descriptive analysis</title><categories>cs.CL</categories><comments>Manuscript submitted to Literary and Linguistic Computing Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of scripts plays an important role in paleography and in
quantitative linguistics. Especially in the field of digital paleography
quantitative features are much needed to differentiate glyphs. We describe an
elaborate set of metrics that quantify qualitative information contained in
characters and hence indirectly also quantify the scribal features. We broadly
divide the metrics into several categories and describe each individual metric
with its underlying qualitative significance. The metrics are largely derived
from the related area of gesture design and recognition. We also propose
several novel metrics. The proposed metrics are soundly grounded on the
principles of handwriting production and handwriting analysis. These computed
metrics could serve as descriptors for scripts and also be used for comparing
and analyzing scripts. We illustrate some quantitative analysis based on the
proposed metrics by applying it to the paleographic evolution of the medieval
Tamil script from Brahmi. We also outline future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01901</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01901</id><created>2015-01-08</created><authors><author><keyname>Abraham</keyname><forenames>Subil</forenames></author><author><keyname>Nair</keyname><forenames>Suku</forenames></author></authors><title>Predictive Cyber-security Analytics Framework: A non-homogenous Markov
  model for Security Quantification</title><categories>cs.CR</categories><comments>16 pages, 6 Figures in International Conference of Security, Privacy
  and Trust Management 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous security metrics have been proposed in the past for protecting
computer networks. However we still lack effective techniques to accurately
measure the predictive security risk of an enterprise taking into account the
dynamic attributes associated with vulnerabilities that can change over time.
In this paper we present a stochastic security framework for obtaining
quantitative measures of security using attack graphs. Our model is novel as
existing research in attack graph analysis do not consider the temporal aspects
associated with the vulnerabilities, such as the availability of exploits and
patches which can affect the overall network security based on how the
vulnerabilities are interconnected and leveraged to compromise the system.
Gaining a better understanding of the relationship between vulnerabilities and
their lifecycle events can provide security practitioners a better
understanding of their state of security. In order to have a more realistic
representation of how the security state of the network would vary over time, a
nonhomogeneous model is developed which incorporates a time dependent
covariate, namely the vulnerability age. The daily transition-probability
matrices are estimated using Frei's Vulnerability Lifecycle model. We also
leverage the trusted CVSS metric domain to analyze how the total exploitability
and impact measures evolve over a time period for a given network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01903</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01903</id><created>2015-01-08</created><authors><author><keyname>D'Agostino</keyname><forenames>Gregorio</forenames></author><author><keyname>D'Antonio</keyname><forenames>Fulvio</forenames></author><author><keyname>De Nicola</keyname><forenames>Antonio</forenames></author><author><keyname>Tucci</keyname><forenames>Salvatore</forenames></author></authors><title>Interests Diffusion in Social Networks</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>30 pages 13 figs 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding cultural phenomena on Social Networks (SNs) and exploiting the
implicit knowledge about their members is attracting the interest of different
research communities both from the academic and the business side. The
community of complexity science is devoting significant efforts to define laws,
models, and theories, which, based on acquired knowledge, are able to predict
future observations (e.g. success of a product). In the mean time, the semantic
web community aims at engineering a new generation of advanced services by
defining constructs, models and methods, adding a semantic layer to SNs. In
this context, a leapfrog is expected to come from a hybrid approach merging the
disciplines above. Along this line, this work focuses on the propagation of
individual interests in social networks. The proposed framework consists of the
following main components: a method to gather information about the members of
the social networks; methods to perform some semantic analysis of the Domain of
Interest; a procedure to infer members' interests; and an interests evolution
theory to predict how the interests propagate in the network. As a result, one
achieves an analytic tool to measure individual features, such as members'
susceptibilities and authorities. Although the approach applies to any type of
social network, here it is has been tested against the computer science
research community.
  The DBLP (Digital Bibliography and Library Project) database has been elected
as test-case since it provides the most comprehensive list of scientific
production in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01905</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01905</id><created>2015-01-08</created><authors><author><keyname>Ugalde</keyname><forenames>U.</forenames></author><author><keyname>Anduaga</keyname><forenames>J.</forenames></author><author><keyname>Martinez</keyname><forenames>F.</forenames></author><author><keyname>Iturrospe</keyname><forenames>A.</forenames></author></authors><title>SHM method for damage localization based on substructuring and VARX
  models</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel damage localization method is proposed, which is based on a
substructuring approach and makes use of Vector Auto-Regressive with eXogenous
input (VARX) models. The substructuring approach aims to divide the monitored
structure into several multi-DOF isolated substructures. Later, each individual
substructure is modeled by a VARX model, and the health of each substructure is
determined analyzing the variation of the VARX model. The method allows to
detect whether the isolated substructure is damaged, and besides allows to
locate the damage within the substructure. Only measured displacement data is
required to estimate the isolated substructure's VARX model. Moreover, it is
not necessary to have a priori knowledge of the structural model. The proposed
method is validated by simulations of an eight-storey shear building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01906</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01906</id><created>2015-01-08</created><authors><author><keyname>Li</keyname><forenames>Yu</forenames></author></authors><title>What is NP? - Interpretation of a Chinese paradox &quot;white horse is not
  horse&quot;</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of nondeterminism has disappeared from the current definition of
NP, which has led to ambiguities in understanding NP, and caused fundamental
difficulties in studying the relation P versus NP. In this paper, we question
the equivalence of the two definitions of NP, the one defining NP as the class
of problems solvable by a nondeterministic Turing machine in polynomial time,
and the other defining NP as the class of problems verifiable by a
deterministic Turing machine in polynomial time, and reveal cognitive biases in
this equivalence. Inspired from a famous Chinese paradox white horse is not
horse, we further analyze these cognitive biases. The work shows that these
cognitive biases arise from the confusion between different levels of
nondeterminism and determinism, due to the lack of understanding about the
essence of nondeterminism. Therefore, we argue that fundamental difficulties in
understanding P versus NP lie firstly at cognition level, then logic level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01909</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01909</id><created>2015-01-08</created><authors><author><keyname>Miyauchi</keyname><forenames>Atsushi</forenames></author><author><keyname>Kawase</keyname><forenames>Yasushi</forenames></author></authors><title>Z-score-based modularity for community detection in networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 10 figures</comments><journal-ref>PLOS ONE 11, e0147805 (2016)</journal-ref><doi>10.1371/journal.pone.0147805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying community structure in networks is an issue of particular
interest in network science. The modularity introduced by Newman and Girvan
[Phys. Rev. E 69, 026113 (2004)] is the most popular quality function for
community detection in networks. In this study, we identify a problem in the
concept of modularity and suggest a solution to overcome this problem.
Specifically, we obtain a new quality function for community detection. We
refer to the function as Z-modularity because it measures the Z-score of a
given division with respect to the fraction of the number of edges within
communities. Our theoretical analysis shows that Z-modularity mitigates the
resolution limit of the original modularity in certain cases. Computational
experiments using both artificial networks and well-known real-world networks
demonstrate the validity and reliability of the proposed quality function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01910</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01910</id><created>2015-01-08</created><authors><author><keyname>Zhou</keyname><forenames>JianMing</forenames></author><author><keyname>Li</keyname><forenames>Yu</forenames></author></authors><title>What is Cook's theorem?</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we make a preliminary interpretation of Cook's theorem
presented in [1]. This interpretation reveals cognitive biases in the proof of
Cook's theorem that arise from the attempt of constructing a formula in CNF to
represent a computation of a nondeterministic Turing machine. Such cognitive
biases are due to the lack of understanding about the essence of
nondeterminism, and lead to the confusion between different levels of
nondeterminism and determinism, thus cause the loss of nondeterminism from the
NP-completeness theory. The work shows that Cook's theorem is the origin of the
loss of nondeterminism in terms of the equivalence of the two definitions of
NP, the one defining NP as the class of problems solvable by a nondeterministic
Turing machine in polynomial time, and the other defining NP as the class of
problems verifiable by a deterministic Turing machine in polynomial time.
Therefore, we argue that fundamental difficulties in understanding P versus NP
lie firstly at cognition level, then logic level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01914</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01914</id><created>2015-01-08</created><authors><author><keyname>Freedman</keyname><forenames>Roy S.</forenames></author></authors><title>Some New Results on Binary Relations</title><categories>cs.DM</categories><comments>13 pages, 7 figures, 1 appendix</comments><msc-class>97E60</msc-class><acm-class>G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that if a function from set A to set B has a right inverse
then the function is a surjection and the right inverse is an injection. For
finite sets, the number of functions, injections, and surjections can also be
counted. Relations generalize functions: do similar results exist for
relations? This paper proves several new results concerning binary relations.
For finite sets, we derive formulas for the number of right total, right
unique, left total, and left unique relations. We also provide formulas that
count the number of relations that are both right unique and left unique; right
unique and right total; and left unique and left total. We conclude by
discussing the probability that a relation selected at random is right unique
or right total.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01924</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01924</id><created>2015-01-08</created><authors><author><keyname>Rayana</keyname><forenames>Shebuti</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author></authors><title>Less is More: Building Selective Anomaly Ensembles</title><categories>cs.DB cs.LG</categories><comments>14 pages, 5 pages Appendix, 10 Figures, 15 Tables, to appear at SDM
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble techniques for classification and clustering have long proven
effective, yet anomaly ensembles have been barely studied. In this work, we tap
into this gap and propose a new ensemble approach for anomaly mining, with
application to event detection in temporal graphs. Our method aims to combine
results from heterogeneous detectors with varying outputs, and leverage the
evidence from multiple sources to yield better performance. However, trusting
all the results may deteriorate the overall ensemble accuracy, as some
detectors may fall short and provide inaccurate results depending on the nature
of the data in hand. This suggests that being selective in which results to
combine is vital in building effective ensembles---hence &quot;less is more&quot;.
  In this paper we propose SELECT; an ensemble approach for anomaly mining that
employs novel techniques to automatically and systematically select the results
to assemble in a fully unsupervised fashion. We apply our method to event
detection in temporal graphs, where SELECT successfully utilizes five base
detectors and seven consensus methods under a unified ensemble framework. We
provide extensive quantitative evaluation of our approach on five real-world
datasets (four with ground truth), including Enron email communications, New
York Times news corpus, and World Cup 2014 Twitter news feed. Thanks to its
selection mechanism, SELECT yields superior performance compared to individual
detectors alone, the full ensemble (naively combining all results), and an
existing diversity-based ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01930</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01930</id><created>2015-01-06</created><authors><author><keyname>Emami</keyname><forenames>Anahita</forenames></author><author><keyname>Khaleghian</keyname><forenames>Seyedmeysam</forenames></author><author><keyname>Jahromi</keyname><forenames>Mohammad Mahjoob</forenames></author></authors><title>Design, Analysis, and Simulation of a Pipe-Welding Robot with Fixed
  Plinth</title><categories>cs.OH</categories><comments>6 pages, 11 figures, 3rd International Conference on Manufacturing
  Engineering, ICME2011, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial requirements concerning the increased efficiency and high rate of
manufacturing result in the development of manufacturer robots, and a vast
group of these types of robots is used for welding. This study presented the
design, analysis, and simulation of a pipe-welding robot with fixed plinth for
a constant circular welding around the pipes. Design of a welding robot capable
of keeping the electrode orientation, welding speed, and distance between
electrode and pipe surface constant can improve the quality of welding; thus, a
five-linked articulated robot was designed for this purpose. Solving of direct
and diverse kinematics and dynamics equations of the robot was done by means of
Matlab software. The robot was also simulated using a program written in Matlab
and the diagrams of angles, velocities, and accelerations of all the arms, and
the applied force and torque of each arm required for drive the mechanism were
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01939</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01939</id><created>2015-01-08</created><authors><author><keyname>Chan</keyname><forenames>Hau</forenames></author><author><keyname>Han</keyname><forenames>Shuchu</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author></authors><title>Where Graph Topology Matters: The Robust Subgraph Problem</title><categories>cs.SI cs.DS</categories><comments>13 pages, 10 Figures, 3 Tables, to appear at SDM 2015 (9 pages only)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness is a critical measure of the resilience of large networked
systems, such as transportation and communication networks. Most prior works
focus on the global robustness of a given graph at large, e.g., by measuring
its overall vulnerability to external attacks or random failures. In this
paper, we turn attention to local robustness and pose a novel problem in the
lines of subgraph mining: given a large graph, how can we find its most robust
local subgraph (RLS)?
  We define a robust subgraph as a subset of nodes with high communicability
among them, and formulate the RLS-PROBLEM of finding a subgraph of given size
with maximum robustness in the host graph. Our formulation is related to the
recently proposed general framework for the densest subgraph problem, however
differs from it substantially in that besides the number of edges in the
subgraph, robustness also concerns with the placement of edges, i.e., the
subgraph topology. We show that the RLS-PROBLEM is NP-hard and propose two
heuristic algorithms based on top-down and bottom-up search strategies.
Further, we present modifications of our algorithms to handle three practical
variants of the RLS-PROBLEM. Experiments on synthetic and real-world graphs
demonstrate that we find subgraphs with larger robustness than the densest
subgraphs even at lower densities, suggesting that the existing approaches are
not suitable for the new problem setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01941</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01941</id><created>2015-01-08</created><updated>2015-02-11</updated><authors><author><keyname>Crainiceanu</keyname><forenames>Adina</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author></authors><title>Bloofi: Multidimensional Bloom Filters</title><categories>cs.DB cs.DS</categories><journal-ref>Information Systems Volume 54, December 2015, Pages 311-324</journal-ref><doi>10.1016/j.is.2015.01.002</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Bloom filters are probabilistic data structures commonly used for approximate
membership problems in many areas of Computer Science (networking, distributed
systems, databases, etc.). With the increase in data size and distribution of
data, problems arise where a large number of Bloom filters are available, and
all them need to be searched for potential matches. As an example, in a
federated cloud environment, each cloud provider could encode the information
using Bloom filters and share the Bloom filters with a central coordinator. The
problem of interest is not only whether a given element is in any of the sets
represented by the Bloom filters, but which of the existing sets contain the
given element. This problem cannot be solved by just constructing a Bloom
filter on the union of all the sets. Instead, we effectively have a
multidimensional Bloom filter problem: given an element, we wish to receive a
list of candidate sets where the element might be.
  To solve this problem, we consider 3 alternatives. Firstly, we can naively
check many Bloom filters. Secondly, we propose to organize the Bloom filters in
a hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally,
we propose another data structure that packs the Bloom filters in such a way as
to exploit bit-level parallelism, which we call Flat-Bloofi.
  Our theoretical and experimental results show that Bloofi and Flat-Bloofi
provide scalable and efficient solutions alternatives to search through a large
number of Bloom filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01944</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01944</id><created>2015-01-08</created><updated>2015-05-21</updated><authors><author><keyname>Frieze</keyname><forenames>Alan</forenames></author><author><keyname>Pegden</keyname><forenames>Wesley</forenames></author></authors><title>Separating subadditive Euclidean functionals</title><categories>math.PR cs.DM math.CO</categories><comments>32 pages, 5 figures. Branch and bound theorem is now unconditional</comments><msc-class>60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If we are given $n$ random points in the hypercube $[0,1]^d$, then the
minimum length of a Traveling Salesperson Tour through the points, the minimum
length of a spanning tree, and the minimum length of a matching, etc., are
known to be asymptotically $\beta n^{\frac{d-1}{d}}$ a.s., where $\beta$ is an
absolute constant in each case. We prove separation results for these
constants. In particular, concerning the constants $\beta_{\mathrm{TSP}}^d$,
$\beta_{\mathrm{MST}}^d$, $\beta_{\mathrm{MM}}^d$, and $\beta_{\mathrm{TF}}^d$
from the asymptotic formulas for the minimum length TSP, spanning tree,
matching, and 2-factor, respectively, we prove that
$\beta_{\mathrm{MST}}^d&lt;\beta_{\mathrm{TSP}}^d$,
$2\beta_{\mathrm{MM}}^d&lt;\beta_{\mathrm{TSP}}^d$, and
$\beta_{\mathrm{TF}}^d&lt;\beta_{\mathrm{TSP}}^d$ for all $d\geq 2$. We also
asymptotically separate the TSP from its linear programming relaxation in this
setting. Our results have some computational relevance, showing that a certain
natural class of simple algorithms cannot solve the random Euclidean TSP
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01946</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01946</id><created>2015-01-08</created><authors><author><keyname>Suarez</keyname><forenames>D.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Sengupta</keyname><forenames>A.</forenames></author><author><keyname>Kulasekera</keyname><forenames>S.</forenames></author><author><keyname>Madanayake</keyname><forenames>A.</forenames></author></authors><title>Multi-Beam RF Aperture Using Multiplierless FFT Approximation</title><categories>stat.ME cs.NA</categories><comments>8 pages, 3 figures, 2 tables, sfg corrected</comments><journal-ref>Electronics Letters, volume 50, issue 24, pages 1788-1790, 2014</journal-ref><doi>10.1049/el.2014.3561</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple independent radio frequency (RF) beams find applications in
communications, radio astronomy, radar, and microwave imaging. An $N$-point FFT
applied spatially across an array of receiver antennas provides $N$-independent
RF beams at $\frac{N}{2}\log_2N$ multiplier complexity. Here, a low-complexity
multiplierless approximation for the 8-point FFT is presented for RF
beamforming, using only 26 additions. The algorithm provides eight beams that
closely resemble the antenna array patterns of the traditional FFT-based
beamformer albeit without using multipliers. The proposed FFT-like algorithm is
useful for low-power RF multi-beam receivers; being synthesized in 45 nm CMOS
technology at 1.1 V supply, and verified on-chip using a Xilinx Virtex-6 Lx240T
FPGA device. The CMOS simulation and FPGA implementation indicate bandwidths of
588 MHz and 369 MHz, respectively, for each of the independent receive-mode RF
beams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01957</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01957</id><created>2015-01-08</created><updated>2015-08-09</updated><authors><author><keyname>Devassy</keyname><forenames>Rahul</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>&#xd6;stman</keyname><forenames>Johan</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Eftimov</keyname><forenames>Tome</forenames></author><author><keyname>Utkovski</keyname><forenames>Zoran</forenames></author></authors><title>Finite-SNR Bounds on the Sum-Rate Capacity of Rayleigh Block-Fading
  Multiple-Access Channels with no a Priori CSI</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures, to appear in IEEE Transactions on
  Communications, 2015. Numerical routines implementing the bounds described in
  the paper can be downloaded at
  https://github.com/infotheorychalmers/mac_capacity_bounds</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide nonasymptotic upper and lower bounds on the sum-rate capacity of
Rayleigh block-fading multiple-access channels for the setup where a priori
channel state information is not available. The upper bound relies on a dual
formula for channel capacity and on the assumption that the users can cooperate
perfectly. The lower bound is derived assuming a noncooperative scenario, where
each user employs unitary space-time modulation (independently from the other
users). Numerical results show that the gap between the upper and the lower
bound is small already at moderate SNR values. This suggests that the sum-rate
capacity gains obtainable through user cooperation are minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.01996</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.01996</id><created>2015-01-08</created><authors><author><keyname>Javari</keyname><forenames>Amin</forenames></author><author><keyname>Jalili</keyname><forenames>Mahdi</forenames></author></authors><title>A probabilistic model to resolve diversity-accuracy challenge of
  recommendation systems</title><categories>cs.IR</categories><comments>19 pages, 5 figures</comments><journal-ref>Knowledge and Information Systems, 1-19 (2014)</journal-ref><doi>10.1007/s10115-014-0779-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems have wide-spread applications in both academia and
industry. Traditionally, performance of recommendation systems has been
measured by their precision. By introducing novelty and diversity as key
qualities in recommender systems, recently increasing attention has been
focused on this topic. Precision and novelty of recommendation are not in the
same direction, and practical systems should make a trade-off between these two
quantities. Thus, it is an important feature of a recommender system to make it
possible to adjust diversity and accuracy of the recommendations by tuning the
model. In this paper, we introduce a probabilistic structure to resolve the
diversity-accuracy dilemma in recommender systems. We propose a hybrid model
with adjustable level of diversity and precision such that one can perform this
by tuning a single parameter. The proposed recommendation model consists of two
models: one for maximization of the accuracy and the other one for
specification of the recommendation list to tastes of users. Our experiments on
two real datasets show the functionality of the model in resolving
accuracy-diversity dilemma and outperformance of the model over other classic
models. The proposed method could be extensively applied to real commercial
systems due to its low computational complexity and significant performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02012</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02012</id><created>2015-01-08</created><updated>2015-03-31</updated><authors><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Full Diversity Unitary Precoded Integer-Forcing</title><categories>cs.IT math.IT</categories><comments>12 pages, 8 figures, to appear in IEEE-TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a point-to-point flat-fading MIMO channel with channel state
information known both at transmitter and receiver. At the transmitter side, a
lattice coding scheme is employed at each antenna to map information symbols to
independent lattice codewords drawn from the same codebook. Each lattice
codeword is then multiplied by a unitary precoding matrix ${\bf P}$ and sent
through the channel. At the receiver side, an integer-forcing (IF) linear
receiver is employed. We denote this scheme as unitary precoded integer-forcing
(UPIF). We show that UPIF can achieve full-diversity under a constraint based
on the shortest vector of a lattice generated by the precoding matrix ${\bf
P}$. This constraint and a simpler version of that provide design criteria for
two types of full-diversity UPIF. Type I uses a unitary precoder that adapts at
each channel realization. Type II uses a unitary precoder, which remains fixed
for all channel realizations. We then verify our results by computer
simulations in $2\times2$, and $4\times 4$ MIMO using different QAM
constellations. We finally show that the proposed Type II UPIF outperform the
MIMO precoding X-codes at high data rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02016</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02016</id><created>2015-01-08</created><authors><author><keyname>Gao</keyname><forenames>Qian</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Xu</keyname><forenames>Zhengyuan</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author></authors><title>DC-Informative Joint Color-Frequency Modulation for Visible Light
  Communications</title><categories>cs.IT math.IT</categories><comments>submitted to Journal of Lightwave Technology, Aug. 5th 2014</comments><doi>10.1109/JLT.2015.2408620</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of constellation design for a visible
light communication (VLC) system using red/green/blue light-emitting diodes
(RGB LED), and propose a method termed DC-informative joint color-frequency
modulation (DCI-JCFM). This method jointly utilizes available diversity
resources including different optical wavelengths, multiple baseband
subcarriers, and adaptive DC-bias. Constellation is designed in a high
dimensional space, where the compact sphere packing advantage over lower
dimensional counterparts is utilized. Taking into account multiple practical
illumination constraints, a non-convex optimization problem is formulated,
seeking the least error rate with a fixed spectral efficiency. The proposed
scheme is compared with a decoupled scheme, where constellation is designed
separately for each LED. Notable gains for DCI-JCFM are observed through
simulations where balanced, unbalanced and very unbalanced color illuminations
are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02018</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02018</id><created>2015-01-08</created><authors><author><keyname>Peng</keyname><forenames>Jigen</forenames></author><author><keyname>Yue</keyname><forenames>Shigang</forenames></author><author><keyname>Li</keyname><forenames>Haiyang</forenames></author></authors><title>$NP/CLP$ Equivalence: A Phenomenon Hidden Among Sparsity Models for
  Information Processing</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory in June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved in this paper that to every underdetermined linear system $Ax=b$
there corresponds a constant $p(A,b)&gt;0$ such that every solution to the
$l_p$-norm minimization problem also solves the $l_0$-norm minimization problem
whenever $0&lt;p&lt;p(A,b)$. This phenomenon is named $NP/CLP$ equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02030</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02030</id><created>2015-01-08</created><authors><author><keyname>Adalid</keyname><forenames>Dami&#xe1;n</forenames><affiliation>University of M&#xe1;laga</affiliation></author><author><keyname>Gallardo</keyname><forenames>Mar&#xed;a del Mar</forenames><affiliation>University of M&#xe1;laga</affiliation></author><author><keyname>Titolo</keyname><forenames>Laura</forenames><affiliation>University of M&#xe1;laga</affiliation></author></authors><title>Modeling Hybrid Systems in the Concurrent Constraint Paradigm</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 1-15</journal-ref><doi>10.4204/EPTCS.173.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems, which combine discrete and continuous dynamics, require
quality modeling languages to be either described or analyzed. The Concurrent
Constraint paradigm (ccp) is an expressive declarative paradigm, characterized
by the use of a common constraint store to communicate and synchronize
concurrent agents. In this paradigm, the information is stated in the form of
constraints, in contrast to the variable/value style typical of imperative
languages. Several extensions of ccp have been proposed in order to model
reactive systems. One of these extensions is the Timed Concurrent Constraint
Language (tccp) that adds to ccp a notion of discrete time and new features to
model time-out and preemption actions. The goal of this paper is to explore the
expressive power of tccp to describe hybrid systems. We introduce the language
Hy-tccp as a conservative extension of tccp, by adding a notion of continuous
time and new constructs to describe the continuous dynamics of hybrid systems.
In this paper, we present the syntax and the operational semantics of Hy-tccp
together with some examples that show the expressive power of our new language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02031</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02031</id><created>2015-01-08</created><authors><author><keyname>Alarte</keyname><forenames>Juli&#xe1;n</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author><author><keyname>Insa</keyname><forenames>David</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author><author><keyname>Silva</keyname><forenames>Josep</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author><author><keyname>Tamarit</keyname><forenames>Salvador</forenames><affiliation>Universidad Polit&#xe9;cnica de Madrid</affiliation></author></authors><title>Web Template Extraction Based on Hyperlink Analysis</title><categories>cs.IR</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 16-26</journal-ref><doi>10.4204/EPTCS.173.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web templates are one of the main development resources for website
engineers. Templates allow them to increase productivity by plugin content into
already formatted and prepared pagelets. For the final user templates are also
useful, because they provide uniformity and a common look and feel for all
webpages. However, from the point of view of crawlers and indexers, templates
are an important problem, because templates usually contain irrelevant
information such as advertisements, menus, and banners. Processing and storing
this information is likely to lead to a waste of resources (storage space,
bandwidth, etc.). It has been measured that templates represent between 40% and
50% of data on the Web. Therefore, identifying templates is essential for
indexing tasks. In this work we propose a novel method for automatic template
extraction that is based on similarity analysis between the DOM trees of a
collection of webpages that are detected using menus information. Our
implementation and experiments demonstrate the usefulness of the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02032</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02032</id><created>2015-01-08</created><authors><author><keyname>Albors</keyname><forenames>Javier</forenames><affiliation>Universidad del Pa&#xed;s Vasco</affiliation></author><author><keyname>Navarro</keyname><forenames>Marisa</forenames><affiliation>Universidad del Pa&#xed;s Vasco</affiliation></author></authors><title>SpecSatisfiabilityTool: A tool for testing the satisfiability of
  specifications on XML documents</title><categories>cs.LO cs.SE</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><acm-class>D,2,4; D.2.5; F.3.1</acm-class><journal-ref>EPTCS 173, 2015, pp. 27-40</journal-ref><doi>10.4204/EPTCS.173.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a prototype that implements a set of logical rules to prove the
satisfiability for a class of specifications on XML documents. Specifications
are given by means of constrains built on Boolean XPath patterns. The main goal
of this tool is to test whether a given specification is satisfiable or not,
and justify the decision showing the execution history. It can also be used to
test whether a given document is a model of a given specification and, as a
by-product, it permits to look for all the relations (monomorphisms) between
two patterns and to combine patterns in different ways. The results of these
operations are visually shown and therefore the tool makes these operations
more understandable. The implementation of the algorithm has been written in
Prolog but the prototype has a Java interface for an easy and friendly use. In
this paper we show how to use this interface in order to test all the desired
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02033</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02033</id><created>2015-01-08</created><authors><author><keyname>Almendros-Jim&#xe9;nez</keyname><forenames>Jes&#xfa;s M.</forenames><affiliation>Universidad de Almer&#xed;a</affiliation></author></authors><title>XQOWL: An Extension of XQuery for OWL Querying and Reasoning</title><categories>cs.PL cs.DB cs.LO</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 41-55</journal-ref><doi>10.4204/EPTCS.173.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main aims of the so-called Web of Data is to be able to handle
heterogeneous resources where data can be expressed in either XML or RDF. The
design of programming languages able to handle both XML and RDF data is a key
target in this context. In this paper we present a framework called XQOWL that
makes possible to handle XML and RDF/OWL data with XQuery. XQOWL can be
considered as an extension of the XQuery language that connects XQuery with
SPARQL and OWL reasoners. XQOWL embeds SPARQL queries (via Jena SPARQL engine)
in XQuery and enables to make calls to OWL reasoners (HermiT, Pellet and
FaCT++) from XQuery. It permits to combine queries against XML and RDF/OWL
resources as well as to reason with RDF/OWL data. Therefore input data can be
either XML or RDF/OWL and output data can be formatted in XML (also using
RDF/OWL XML serialization).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02034</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02034</id><created>2015-01-08</created><authors><author><keyname>Juli&#xe1;n-Iranzo</keyname><forenames>Pascual</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author><author><keyname>Moreno</keyname><forenames>Gin&#xe9;s</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author><author><keyname>Penabad</keyname><forenames>Jaime</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author><author><keyname>V&#xe1;zquez</keyname><forenames>Carlos</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author></authors><title>A Fuzzy Logic Programming Environment for Managing Similarity and Truth
  Degrees</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 71-86</journal-ref><doi>10.4204/EPTCS.173.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FASILL (acronym of &quot;Fuzzy Aggregators and Similarity Into a Logic Language&quot;)
is a fuzzy logic programming language with implicit/explicit truth degree
annotations, a great variety of connectives and unification by similarity.
FASILL integrates and extends features coming from MALP (Multi-Adjoint Logic
Programming, a fuzzy logic language with explicitly annotated rules) and
Bousi~Prolog (which uses a weak unification algorithm and is well suited for
flexible query answering). Hence, it properly manages similarity and truth
degrees in a single framework combining the expressive benefits of both
languages. This paper presents the main features and implementations details of
FASILL. Along the paper we describe its syntax and operational semantics and we
give clues of the implementation of the lattice module and the similarity
module, two of the main building blocks of the new programming environment
which enriches the FLOPER system developed in our research group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02035</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02035</id><created>2015-01-08</created><authors><author><keyname>Riesco</keyname><forenames>Adri&#xe1;n</forenames><affiliation>Universidad Complutense de Madrid,</affiliation></author><author><keyname>Rodr&#xed;guez-Hortal&#xe1;</keyname><forenames>Juan</forenames><affiliation>Lambdoop Solutions</affiliation></author></authors><title>Lifting Term Rewriting Derivations in Constructor Systems by Using
  Generators</title><categories>cs.PL</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 87-99</journal-ref><doi>10.4204/EPTCS.173.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Narrowing is a procedure that was first studied in the context of equational
E-unification and that has been used in a wide range of applications. The
classic completeness result due to Hullot states that any term rewriting
derivation starting from an instance of an expression can be &quot;lifted&quot; to a
narrowing derivation, whenever the substitution employed is normalized. In this
paper we adapt the generator- based extra-variables-elimination transformation
used in functional-logic programming to overcome that limitation, so we are
able to lift term rewriting derivations starting from arbitrary instances of
expressions. The proposed technique is limited to left-linear constructor
systems and to derivations reaching a ground expression. We also present a
Maude-based implementation of the technique, using natural rewriting for the
on-demand evaluation strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02036</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02036</id><created>2015-01-08</created><authors><author><keyname>S&#xe1;enz-P&#xe9;rez</keyname><forenames>Fernando</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author></authors><title>Improving the Deductive System DES with Persistence by Using SQL DBMS's</title><categories>cs.LO cs.AI cs.DB</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 100-114</journal-ref><doi>10.4204/EPTCS.173.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents how persistent predicates have been included in the
in-memory deductive system DES by relying on external SQL database management
systems. We introduce how persistence is supported from a user-point of view
and the possible applications the system opens up, as the deductive expressive
power is projected to relational databases. Also, we describe how it is
possible to intermix computations of the deductive engine and the external
database, explaining its implementation and some optimizations. Finally, a
performance analysis is undertaken, comparing the system with current
relational database systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02038</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02038</id><created>2015-01-08</created><authors><author><keyname>Berzal</keyname><forenames>Fernando</forenames><affiliation>Universidad de Granada</affiliation></author><author><keyname>Cortijo</keyname><forenames>Francisco J.</forenames><affiliation>Universidad de Granada</affiliation></author><author><keyname>Cubero</keyname><forenames>Juan-Carlos</forenames><affiliation>Universidad de Granada</affiliation></author><author><keyname>Quesada</keyname><forenames>Luis</forenames><affiliation>Universidad de Granada</affiliation></author></authors><title>The ModelCC Model-Driven Parser Generator</title><categories>cs.PL cs.FL</categories><comments>In Proceedings PROLE 2014, arXiv:1501.01693</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 173, 2015, pp. 56-70</journal-ref><doi>10.4204/EPTCS.173.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syntax-directed translation tools require the specification of a language by
means of a formal grammar. This grammar must conform to the specific
requirements of the parser generator to be used. This grammar is then annotated
with semantic actions for the resulting system to perform its desired function.
In this paper, we introduce ModelCC, a model-based parser generator that
decouples language specification from language processing, avoiding some of the
problems caused by grammar-driven parser generators. ModelCC receives a
conceptual model as input, along with constraints that annotate it. It is then
able to create a parser for the desired textual syntax and the generated parser
fully automates the instantiation of the language conceptual model. ModelCC
also includes a reference resolution mechanism so that ModelCC is able to
instantiate abstract syntax graphs, rather than mere abstract syntax trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02046</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02046</id><created>2015-01-09</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Multiuser MIMO Wireless Energy Transfer With Coexisting Opportunistic
  Communication</title><categories>cs.IT math.IT</categories><comments>submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter considers spectrum sharing between a primary multiuser
multiple-input multiple-output (MIMO) wireless energy transfer (WET) system and
a coexisting secondary point-to-point MIMO wireless information transmission
(WIT) system, where WET generates interference to WIT and degrades its
throughput performance. We show that due to the interference, the WIT system
suffers from a loss of the degrees of freedom (DoF) proportional to the number
of energy beams sent by the energy transmitter (ET), which, in general, needs
to be larger than one in order to optimize the multiuser WET with user fairness
consideration. To minimize the DoF loss in WIT, we further propose a new
single-beam energy transmission scheme based on the principle of time sharing,
where the ET transmits one of the optimal energy beams at each time. This new
scheme achieves the same optimal performance for the WET system, and minimizes
the impact of its interference to the WIT system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02056</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02056</id><created>2015-01-09</created><updated>2015-02-10</updated><authors><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt, MSR - INRIA</affiliation></author><author><keyname>Lindsten</keyname><forenames>Fredrik</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt, MSR - INRIA</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt, MSR - INRIA</affiliation></author></authors><title>Sequential Kernel Herding: Frank-Wolfe Optimization for Particle
  Filtering</title><categories>stat.ML cs.LG</categories><comments>in 18th International Conference on Artificial Intelligence and
  Statistics (AISTATS), May 2015, San Diego, United States. 38, JMLR Workshop
  and Conference Proceedings</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the Frank-Wolfe optimization algorithm was suggested as a procedure
to obtain adaptive quadrature rules for integrals of functions in a reproducing
kernel Hilbert space (RKHS) with a potentially faster rate of convergence than
Monte Carlo integration (and &quot;kernel herding&quot; was shown to be a special case of
this procedure). In this paper, we propose to replace the random sampling step
in a particle filter by Frank-Wolfe optimization. By optimizing the position of
the particles, we can obtain better accuracy than random or quasi-Monte Carlo
sampling. In applications where the evaluation of the emission probabilities is
expensive (such as in robot localization), the additional computational cost to
generate the particles through optimization can be justified. Experiments on
standard synthetic examples as well as on a robot localization task indicate
indeed an improvement of accuracy over random and quasi-Monte Carlo sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02058</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02058</id><created>2015-01-09</created><authors><author><keyname>Kachouane</keyname><forenames>M.</forenames><affiliation>USTHB</affiliation></author><author><keyname>Sahki</keyname><forenames>S.</forenames><affiliation>CDTA, USTHB</affiliation></author><author><keyname>Lakrouf</keyname><forenames>M.</forenames><affiliation>CDTA, USTHB</affiliation></author><author><keyname>Ouadah</keyname><forenames>N.</forenames><affiliation>CDTA</affiliation></author></authors><title>HOG based Fast Human Detection</title><categories>cs.RO cs.CV cs.LG</categories><proxy>ccsd</proxy><journal-ref>24th International Conference on Microelectronics (ICM), 2012, Dec
  2012, Alger, Algeria. pp.1 - 4</journal-ref><doi>10.1109/ICM.2012.6471380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objects recognition in image is one of the most difficult problems in
computer vision. It is also an important step for the implementation of several
existing applications that require high-level image interpretation. Therefore,
there is a growing interest in this research area during the last years. In
this paper, we present an algorithm for human detection and recognition in
real-time, from images taken by a CCD camera mounted on a car-like mobile
robot. The proposed technique is based on Histograms of Oriented Gradient (HOG)
and SVM classifier. The implementation of our detector has provided good
results, and can be used in robotics tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02062</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02062</id><created>2015-01-09</created><authors><author><keyname>Bagaria</keyname><forenames>Sankalp</forenames></author></authors><title>New Hashing Algorithm for Use in TCP Reassembly Module of IPS</title><categories>cs.DS cs.CR cs.NI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since last decade, IDS/ IPS has gained popularity in protecting large
networks. They can employ signature based techniques and/or flow-based
techniques to prevent intrusion from outside/ inside the network they are
trying to protect. Signature based IDS/ IPS can be stateless or stateful.
Stateful IDS can store the state of the protocol and use it for better
detection of malware. In the case of TCP/IP networks, an attacker can also
launch an attack such that the malicious code is distributed over many packets.
These packets pass through the traditional IDS/ IPS and reassemble inside the
network. Once re-assembled inside the network by the TCP/IP layer, the
malicious code launches an attack.
  The TCP state and a copy of last few packets for each active connection has
to be maintained in IDS/IPS. In TCP re-assembly, packets are re-assembled at
IDS/IPS and searched for signature matches. A connection table has to be
maintained for active connections and their list of last few (atmost 11)
packets that have already arrived. We need data structures for searching the
connection that the latest incoming packet belongs to. Popular hashing
algorithms like CRC, XOR, summing tuple, taking modulus are inefficient as hash
keys are not evenly distributed in hash-key space. Thus we show how an
algorithm based on cryptography concepts can be used for efficient hashing in
network connection management. We also show how to use full four tuple for
calculating hash key instead of simply summing the tuple and taking the modulus
of the sum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02069</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02069</id><created>2015-01-09</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh</forenames></author><author><keyname>Aronis</keyname><forenames>Stavros</forenames></author><author><keyname>Atig</keyname><forenames>Mohammed Faouzi</forenames></author><author><keyname>Jonsson</keyname><forenames>Bengt</forenames></author><author><keyname>Leonardsson</keyname><forenames>Carl</forenames></author><author><keyname>Sagonas</keyname><forenames>Konstantinos</forenames></author></authors><title>Stateless Model Checking for TSO and PSO</title><categories>cs.LO</categories><acm-class>D.1.3; D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for efficient stateless model checking of programs
that execute under the relaxed memory models TSO and PSO. The basis for our
technique is a novel representation of executions under TSO and PSO, called
chronological traces. Chronological traces induce a partial order relation on
relaxed memory executions, capturing dependencies that are needed to represent
the interaction via shared variables. They are optimal in the sense that they
only distinguish computations that are inequivalent under the widely-used
representation by Shasha and Snir. This allows an optimal dynamic partial order
reduction algorithm to explore a minimal number of executions while still
guaranteeing full coverage. We apply our techniques to check, under the TSO and
PSO memory models, LLVM assembly produced for C/pthreads programs. Our
experiments show that our technique reduces the verification effort for relaxed
memory models to be almost that for the standard model of sequential
consistency. In many cases, our implementation significantly outperforms other
comparable tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02084</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02084</id><created>2015-01-09</created><authors><author><keyname>Mart&#xed;n-Mart&#xed;n</keyname><forenames>Alberto</forenames></author><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayll&#xf3;n</keyname><forenames>Juan Manuel</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Reviving the past: the growth of citations to old documents</title><categories>cs.DL</categories><comments>13 pages, 4 tables, 3 figures</comments><report-no>EC3 Google Scholar's Digest Reviews 04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Digest we review a recent study released by the Google Scholar team
on the apparently increasing fraction of citations to old articles from studies
published in the last 24 years (1990-2013). First, we describe the main
findings of their article. Secondly, we conduct an analogue study, using a
different data source as well as different measures which throw very similar
results, thus confirming the phenomenon. Lastly, we discuss the possible causes
of this phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02113</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02113</id><created>2015-01-09</created><authors><author><keyname>Thai</keyname><forenames>Duy Hoang</forenames></author><author><keyname>Huckemann</keyname><forenames>Stephan</forenames></author><author><keyname>Gottschlich</keyname><forenames>Carsten</forenames></author></authors><title>Filter Design and Performance Evaluation for Fingerprint Image
  Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprint recognition plays an important role in many commercial
applications and is used by millions of people every day, e.g. for unlocking
mobile phones. Fingerprint image segmentation is typically the first processing
step of most fingerprint algorithms and it divides an image into foreground,
the region of interest, and background. Two types of error can occur during
this step which both have a negative impact on the recognition performance:
'true' foreground can be labeled as background and features like minutiae can
be lost, or conversely 'true' background can be misclassified as foreground and
spurious features can be introduced. The contribution of this paper is
threefold: firstly, we propose a novel factorized directional bandpass (FDB)
segmentation method for texture extraction based on the directional Hilbert
transform of a Butterworth bandpass (DHBB) filter interwoven with
soft-thresholding. Secondly, we provide a manually marked ground truth
segmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a
systematic performance comparison between the FDB method and four of the most
often cited fingerprint segmentation algorithms showing that the FDB
segmentation method clearly outperforms these four widely used methods. The
benchmark and the implementation of the FDB method are made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02125</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02125</id><created>2015-01-09</created><authors><author><keyname>Franz</keyname><forenames>Bernd</forenames></author><author><keyname>Ali</keyname><forenames>Liaquat</forenames></author><author><keyname>Schmalen</keyname><forenames>Laurent</forenames></author><author><keyname>Idler</keyname><forenames>Wilfried</forenames></author></authors><title>High Speed Data Transmission over GI-MMF Using Mode Group Division
  Multiplexing</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We transmitted 4x28 Gbit/s over 5-km standard GI-MMF using four different
mode groups in combination with direct detection and on-off-keying format
(OOK). Due to the square-law detection in the receiver MIMO processing is not
effective. Therefore proper mode group selective multiplexing and
de-multiplexing is essential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02128</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02128</id><created>2015-01-09</created><authors><author><keyname>Tan</keyname><forenames>Ying</forenames></author><author><keyname>Li</keyname><forenames>Junzhi</forenames></author><author><keyname>Zheng</keyname><forenames>Zhongyang</forenames></author></authors><title>Introduction and Ranking Results of the ICSI 2014 Competition on Single
  Objective Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report includes the introduction and ranking results of the
ICSI 2014 Competition on Single Objective Optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02134</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02134</id><created>2015-01-09</created><authors><author><keyname>Ponciano</keyname><forenames>Lesandro</forenames></author><author><keyname>Brasileiro</keyname><forenames>Francisco</forenames></author></authors><title>Finding Volunteers' Engagement Profiles in Human Computation for Citizen
  Science Projects</title><categories>cs.HC cs.CY</categories><comments>3 tables, and 4 figures</comments><journal-ref>Human Computation vol. 1, no. 2, pp. 245-264 (2014)</journal-ref><doi>10.15346/hc.v1i2.12</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Human computation is a computing approach that draws upon human cognitive
abilities to solve computational tasks for which there are so far no
satisfactory fully automated solutions even when using the most advanced
computing technologies available. Human computation for citizen science
projects consists in designing systems that allow large crowds of volunteers to
contribute to scientific research by executing human computation tasks.
Examples of successful projects are Galaxy Zoo and FoldIt. A key feature of
this kind of project is its capacity to engage volunteers. An important
requirement for the proposal and evaluation of new engagement strategies is
having a clear understanding of the typical engagement of the volunteers;
however, even though several projects of this kind have already been completed,
little is known about this issue. In this paper, we investigate the engagement
pattern of the volunteers in their interactions in human computation for
citizen science projects, how they differ among themselves in terms of
engagement, and how those volunteer engagement features should be taken into
account for establishing the engagement encouragement strategies that should be
brought into play in a given project. To this end, we define four quantitative
engagement metrics to measure different aspects of volunteer engagement, and
use data mining algorithms to identify the different volunteer profiles in
terms of the engagement metrics. Our study is based on data collected from two
projects: Galaxy Zoo and The Milky Way Project. The results show that the
volunteers in such projects can be grouped into five distinct engagement
profiles that we label as follows: hardworking, spasmodic, persistent, lasting,
and moderate. The analysis of these profiles provides a deeper understanding of
the nature of volunteers' engagement in human computation for citizen science
projects
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02143</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02143</id><created>2015-01-09</created><authors><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>St&#xf6;ckel</keyname><forenames>Morten</forenames></author></authors><title>Association Rule Mining using Maximum Entropy</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendations based on behavioral data may be faced with ambiguous
statistical evidence. We consider the case of association rules, relevant
e.g.~for query and product recommendations. For example: Suppose that a
customer belongs to categories A and B, each of which is known to have positive
correlation with buying product C, how do we estimate the probability that she
will buy product C?
  For rare terms or products there may not be enough data to directly produce
such an estimate --- perhaps we never directly observed a connection between A,
B, and C. What can we do when there is no support for estimating the
probability by simply computing the observed frequency? In particular, what is
the right thing to do when A and B give rise to very different estimates of the
probability of C?
  We consider the use of maximum entropy probability estimates, which give a
principled way of extrapolating probabilities of events that do not even occur
in the data set! Focusing on the basic case of three variables, our main
technical contributions are that (under mild assumptions): 1) There exists a
simple, explicit formula that gives a good approximation of maximum entropy
estimates, and 2) Maximum entropy estimates based on a small number of samples
are provably tightly concentrated around the true maximum entropy frequency
that arises if we let the number of samples go to infinity.
  Our empirical work demonstrates the surprising precision of maximum entropy
estimates, across a range of real-life transaction data sets. In particular we
observe the average absolute error on maximum entropy estimates is a factor
$3$--$14$ less compared to using independence or extrapolation estimates, when
the data used to make the estimates has low support. We believe that the same
principle can be used to synthesize probability estimates in many settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02144</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02144</id><created>2015-01-09</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Algorithms for two variants of Satisfaction Approval Voting</title><categories>cs.GT cs.MA</categories><msc-class>91A12, 68Q15</msc-class><acm-class>J.4; I.2.11; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-winner voting rules based on approval ballots have received increased
attention in recent years. In particular Satisfaction Approval Voting (SAV) and
its variants have been proposed. In this note, we show that the winning set can
be determined in polynomial time for two prominent and natural variants of SAV.
We thank Arkadii Slinko for suggesting these problems in a talk at the Workshop
on Challenges in Algorithmic Social Choice, Bad Belzig, October 11, 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02155</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02155</id><created>2015-01-09</created><authors><author><keyname>Hales</keyname><forenames>Thomas</forenames></author><author><keyname>Adams</keyname><forenames>Mark</forenames></author><author><keyname>Bauer</keyname><forenames>Gertrud</forenames></author><author><keyname>Dang</keyname><forenames>Dat Tat</forenames></author><author><keyname>Harrison</keyname><forenames>John</forenames></author><author><keyname>Hoang</keyname><forenames>Truong Le</forenames></author><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Magron</keyname><forenames>Victor</forenames></author><author><keyname>McLaughlin</keyname><forenames>Sean</forenames></author><author><keyname>Nguyen</keyname><forenames>Thang Tat</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Quang</forenames></author><author><keyname>Nipkow</keyname><forenames>Tobias</forenames></author><author><keyname>Obua</keyname><forenames>Steven</forenames></author><author><keyname>Pleso</keyname><forenames>Joseph</forenames></author><author><keyname>Rute</keyname><forenames>Jason</forenames></author><author><keyname>Solovyev</keyname><forenames>Alexey</forenames></author><author><keyname>Ta</keyname><forenames>An Hoai Thi</forenames></author><author><keyname>Tran</keyname><forenames>Trung Nam</forenames></author><author><keyname>Trieu</keyname><forenames>Diep Thi</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Vu</keyname><forenames>Ky Khac</forenames></author><author><keyname>Zumkeller</keyname><forenames>Roland</forenames></author></authors><title>A formal proof of the Kepler conjecture</title><categories>math.MG cs.LO</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes a formal proof of the Kepler conjecture on dense
sphere packings in a combination of the HOL Light and Isabelle proof
assistants. This paper constitutes the official published account of the now
completed Flyspeck project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02162</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02162</id><created>2015-01-09</created><authors><author><keyname>Court&#xe8;s</keyname><forenames>Ludovic</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Design and Implementation of rowe, a Web-Friendly Communication Library</title><categories>cs.DC cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The INDES project-team of Inria has been developing HOP, a multi-tier
language for Web programming. As part of the RAPP FP7 European project, the
team has set out to use HOP as the lingua franca of the robotics applications
developed within that project. Part of the challenge lies in the integration of
existing robotics code, written using ROS or custom libraries, with HOP-based
application.
  This document reports on the implementation of rowe, a communication library
designed the fill the gap between low-level robotics C components on one hand,
and other C, C++, ROS, or HOP components on the other. The library aims to be a
lightweight, high-performance, &quot;Web-friendly&quot; communication library. It
implements a socket-like interface that allows programs to exchange JSON
objects over WebSockets. We describe the rationale, design, and implementation
of rowe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02165</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02165</id><created>2015-01-09</created><authors><author><keyname>Perrin</keyname><forenames>Matthieu</forenames></author><author><keyname>Mostefaoui</keyname><forenames>Achour</forenames></author><author><keyname>Jard</keyname><forenames>Claude</forenames></author></authors><title>Update Consistency for Wait-free Concurrent Objects</title><categories>cs.DC</categories><comments>appears in International Parallel and Distributed Processing
  Symposium, May 2015, Hyderabad, India</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large scale systems such as the Internet, replicating data is an essential
feature in order to provide availability and fault-tolerance. Attiya and Welch
proved that using strong consistency criteria such as atomicity is costly as
each operation may need an execution time linear with the latency of the
communication network. Weaker consistency criteria like causal consistency and
PRAM consistency do not ensure convergence. The different replicas are not
guaranteed to converge towards a unique state. Eventual consistency guarantees
that all replicas eventually converge when the participants stop updating.
However, it fails to fully specify the semantics of the operations on shared
objects and requires additional non-intuitive and error-prone distributed
specification techniques. This paper introduces and formalizes a new
consistency criterion, called update consistency, that requires the state of a
replicated object to be consistent with a linearization of all the updates. In
other words, whereas atomicity imposes a linearization of all of the
operations, this criterion imposes this only on updates. Consequently some read
operations may return out-dated values. Update consistency is stronger than
eventual consistency, so we can replace eventually consistent objects with
update consistent ones in any program. Finally, we prove that update
consistency is universal, in the sense that any object can be implemented under
this criterion in a distributed system where any number of nodes may crash.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02175</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02175</id><created>2015-01-09</created><authors><author><keyname>Perrin</keyname><forenames>Matthieu</forenames></author><author><keyname>Most&#xe9;faoui</keyname><forenames>Achour</forenames></author><author><keyname>Jard</keyname><forenames>Claude</forenames></author></authors><title>Brief Announcement: Update Consistency in Partitionable Systems</title><categories>cs.DC</categories><comments>in DISC14 - 28th International Symposium on Distributed Computing,
  Oct 2014, Austin, United States</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data replication is essential to ensure reliability, availability and
fault-tolerance of massive distributed applications over large scale systems
such as the Internet. However, these systems are prone to partitioning, which
by Brewer's CAP theorem [1] makes it impossible to use a strong consistency
criterion like atomicity. Eventual consistency [2] guaranties that all replicas
eventually converge to a common state when the participants stop updating.
However, it fails to fully specify shared objects and requires additional
non-intuitive and error-prone distributed specification techniques, that must
take into account all possible concurrent histories of updates to specify this
common state [3]. This approach, that can lead to specifications as complicated
as the implementations themselves, is limited by a more serious issue. The
concurrent specification of objects uses the notion of concurrent events. In
message-passing systems, two events are concurrent if they are enforced by
different processes and each process enforced its event before it received the
notification message from the other process. In other words, the notion of
concurrency depends on the implementation of the object, not on its
specification. Consequently, the final user may not know if two events are
concurrent without explicitly tracking the messages exchanged by the processes.
A specification should be independent of the system on which it is implemented.
We believe that an object should be totally specified by two facets: its
abstract data type, that characterizes its sequential executions, and a
consistency criterion, that defines how it is supposed to behave in a
distributed environment. Not only sequential specification helps repeal the
problem of intention, it also allows to use the well studied and understood
notions of languages and automata. This makes possible to apply all the tools
developed for sequential systems, from their simple definition using structures
and classes to the most advanced techniques like model checking and formal
verification. Eventual consistency (EC) imposes no constraint on the convergent
state, that very few depends on the sequential specification. For example, an
implementation that ignores all the updates is eventually consistent, as all
replicas converge to the initial state. We propose a new consistency criterion,
update consistency (UC), in which the convergent state must be obtained by a
total ordering of the updates, that contains the sequential order of each
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02183</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02183</id><created>2015-01-09</created><authors><author><keyname>Martinsson</keyname><forenames>Anders</forenames></author></authors><title>An improved energy argument for the Hegselmann-Krause model</title><categories>cs.SY math.CO</categories><msc-class>93A14, 39A60, 91D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the freezing time of the $d$-dimensional Hegselmann-Krause model
is $O(n^4)$ where $n$ is the number of agents. This improves the best known
upper bound whenever $d\geq 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02185</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02185</id><created>2015-01-02</created><updated>2015-05-10</updated><authors><author><keyname>D'Alberto</keyname><forenames>Paolo</forenames></author></authors><title>Multiple-Campaign Ad-Targeting Deployment: Parallel Response Modeling,
  Calibration and Scoring Without Personal User Information</title><categories>stat.AP cs.OH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a vertical introduction to campaign optimization; that is, the
ability to predict the user response to an ad campaign without any users'
profiles on average and for each exposed ad. In practice, we present an
approach to build a polytomous model, multi response, composed by several
hundred binary models using generalized linear models. The theory has been
introduced twenty years ago and it has been applied in different fields since
then. Here, we show how we optimize hundreds campaigns and how this large
number of campaigns may overcome a few characteristic caveats of single
campaign optimization. We discuss the problem and solution of training and
calibration at scale. We present statistical performance as {\em coverage},
{\em precision} and {\em recall} used in classification. We present also a
discussion about the potential performance as throughput: how many decisions
can be done per second streaming the bid auctions also by using dedicated
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02190</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02190</id><created>2015-01-09</created><updated>2015-02-19</updated><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author></authors><title>Equational axioms associated with finite automata for fixed point
  operations in cartesian categories</title><categories>cs.LO</categories><comments>Accepted for publication in MSCS</comments><msc-class>18C10, 68Q55, 68Q65</msc-class><acm-class>F.3.2</acm-class><doi>10.1017/S0960129515000031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The axioms of iteration theories, or iteration categories, capture the
equational properties of fixed point operations in several computationally
significant categories. Iteration categories may be axiomatized by the Conway
identities and identities associated with finite automata. We show that in
conjunction with the Conway identities, each identity associated with a finite
automaton implies the identity associated with any input extension of the
automaton. We conclude that the Conway identities and the identities associated
with the members of a subclass $\cQ$ of finite automata is complete for
iteration categories iff for every finite simple group $G$ there is an
automaton $\bQ \in \cQ$ such that $G$ is a quotient of a group in the monoid
$M(\bQ)$ of the automaton $\bQ$. We also prove a stronger result that concerns
identities associated with finite automata with a distinguished initial state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02192</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02192</id><created>2015-01-09</created><authors><author><keyname>Alhawarat</keyname><forenames>M.</forenames></author><author><keyname>Scheper</keyname><forenames>T. Olde</forenames></author><author><keyname>Crook</keyname><forenames>N. T.</forenames></author></authors><title>Investigation of a chaotic spiking neuron model</title><categories>cs.NE cs.AI</categories><journal-ref>International Journal of Computer Applications 99(17):1-8, August
  2014</journal-ref><doi>10.5120/17462-8258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaos provides many interesting properties that can be used to achieve
computational tasks. Such properties are sensitivity to initial conditions,
space filling, control and synchronization. Chaotic neural models have been
devised to exploit such properties. In this paper, a chaotic spiking neuron
model is investigated experimentally. This investigation is performed to
understand the dynamic behaviours of the model.
  The aim of this research is to investigate the dynamics of the nonlinear
dynamic state neuron (NDS) experimentally. The experimental approach has
revealed some quantitative and qualitative properties of the NDS model such as
the control mechanism, the reset mechanism, and the way the model may exhibit
dynamic behaviours in phase space. It is shown experimentally in this paper
that both the reset mechanism and the self-feed back control mechanism are
important for the NDS model to work and to stabilise to one of the large number
of available unstable periodic orbits (UPOs) that are embedded in its
attractor. The experimental investigation suggests that the internal dynamics
of the NDS neuron provide a rich set of dynamic behaviours that can be
controlled and stabilised. These wide range of dynamic behaviours may be
exploited to carry out information processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02211</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02211</id><created>2015-01-09</created><authors><author><keyname>Borgohain</keyname><forenames>Tuhin</forenames></author><author><keyname>Kumar</keyname><forenames>Uday</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Survey of Security and Privacy Issues of Internet of Things</title><categories>cs.CR</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a general survey of all the security issues existing in the
Internet of Things (IoT) along with an analysis of the privacy issues that an
end-user may face as a consequence of the spread of IoT. The majority of the
survey is focused on the security loopholes arising out of the information
exchange technologies used in Internet of Things. No countermeasure to the
security drawbacks has been analyzed in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02212</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02212</id><created>2015-01-09</created><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>Efficient Computation by Three Counter Machines</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that multiplication can be done in polynomial time on a three counter
machine that receives its input as the contents of two counters. The technique
is generalized to functions of two variables computable by deterministic Turing
machines in linear space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02223</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02223</id><created>2015-01-09</created><authors><author><keyname>Capone</keyname><forenames>Antonio</forenames></author><author><keyname>Filippini</keyname><forenames>Ilario</forenames></author><author><keyname>Sciancalepore</keyname><forenames>Vincenzo</forenames></author></authors><title>Context-based Cell Search in Millimeter Wave 5G Networks</title><categories>cs.NI</categories><comments>5 pages, 5 figures, Submitted VTC2015-Spring - Massive MIMO and
  Millimeter-waves for 5G Networks Workshop (mmW5G-WS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploitation of the mm-wave bands is one of the most promising solutions
for 5G mobile radio networks. However, the use of mm-wave technologies in
cellular networks is not straightforward due to mm-wave severe propagation
conditions that limit access availability. In order to overcome this obstacle,
hybrid network architectures are being considered where mmwave small cells can
exploit an overlay coverage layer based on legacy technology. The additional
mm-wave layer can also take advantage of a functional split between control and
user plane, that allows to delegate most of the signaling functions to legacy
base stations and to gather context information from users for resource
optimization. However, mm-wave technology requires multiple antennas and highly
directional transmissions to compensate for high path loss and limited power.
Directional transmissions must be also used for the cell discovery and
synchronization process, and this can lead to a non negligible delay due to
need to scan the cell area with multiple transmissions in different angles. In
this paper, we propose to exploit the context information related to user
position, provided by the separated control plane, to improve the cell search
procedure and minimize delay. We investigate the fundamental trade-offs of the
cell discovery process with directional antennas and the effects of the context
information accuracy on its performance. Numerical results are provided to
validate our observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02224</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02224</id><created>2015-01-07</created><updated>2015-04-20</updated><authors><author><keyname>Park</keyname><forenames>Hyoshin</forenames></author><author><keyname>Shafahi</keyname><forenames>Ali</forenames></author><author><keyname>Haghani</keyname><forenames>Ali</forenames></author></authors><title>Stochastic Emergency Response Units (ERUs) Allocation Considering
  Secondary Incident Occurrences</title><categories>cs.SY math.OC math.PR</categories><comments>26 pages, 6 figures, 6 tables. This paper has been withdrawn by the
  author due to a crucial sign error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location of depots and routing of emergency response units are assumed to be
interdependent in the incident management system. System costs will be
excessive if delay regarding routing decisions is ignored when locating
response units. This paper presents an integrated method to solve location and
routing problem of emergency response units on freeways. The principle is to
begin with a location phase for managing initial incidents and to progress
through a routing phase for managing the stochastic occurrence of next
incidents. Previous models used the frequency of independent incidents and
ignored scenarios in which two incidents occurred within proximal regions and
intervals. The proposed analytical model relaxes the structural assumptions of
Poisson process (independent increments) and incorporates evolution of primary
and secondary incident probabilities over time. The proposed mathematical model
overcomes several limiting assumptions of the previous models, such as no
waiting-time and returning rule to original depot. Our stochastic programming
method hedges well against a wide range of scenarios in which probabilities of
a sequence of incidents are assigned. The initial non-linear stochastic model
is linearized. As a long-term strategy, the model incorporates flexibility in
choosing the locations. The temporal locations flexible to a future
policy-change are compared with current practice that locates all units in one
permanent depot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02237</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02237</id><created>2015-01-09</created><updated>2015-03-01</updated><authors><author><keyname>Chen</keyname><forenames>Tianran</forenames></author><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author></authors><title>Parallel degree computation for solution space of binomial systems with
  an application to the master space of $\mathcal{N}=1$ gauge theories</title><categories>math.AG cs.SC hep-th math-ph math.MP</categories><comments>27 pages, 5 figures. Improved data with further computation</comments><report-no>ADP-15-2/T904</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of solving a system of polynomial equations is one of the most
fundamental problems in applied mathematics. Among them, the problem of solving
a system of binomial equations form a important subclass for which specialized
techniques exist. For both theoretic and applied purposes, the degree of the
solution set of a system of binomial equations often plays an important role in
understanding the geometric structure of the solution set. Its computation,
however, is computationally intensive. This paper proposes a specialized
parallel algorithm for computing the degree on GPUs that takes advantage of the
massively parallel nature of GPU devices. The preliminary implementation shows
remarkable efficiency and scalability when compared to the closest CPU-based
counterpart. Applied to the &quot;master space problem of $\mathcal{N}=1$ gauge
theories&quot; the GPU-based implementation achieves nearly 30 fold speedup over its
CPU-only counterpart enabling the discovery of previously unknown results.
Equally important to note is the far superior scalability: with merely 3 GPU
devices on a single workstation, the GPU-based implementation shows better
performance, on certain problems, than a small cluster totaling 100 CPU cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02243</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02243</id><created>2015-01-09</created><updated>2016-02-14</updated><authors><author><keyname>Savani</keyname><forenames>Rahul</forenames></author><author><keyname>von Stengel</keyname><forenames>Bernhard</forenames></author></authors><title>Unit Vector Games</title><categories>cs.GT</categories><comments>final version as published in IJET</comments><msc-class>91A05</msc-class><journal-ref>International Journal of Economic Theory 12 (2016), 7-27</journal-ref><doi>10.1111/ijet.12077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  McLennan and Tourky (2010) showed that &quot;imitation games&quot; provide a new view
of the computation of Nash equilibria of bimatrix games with the Lemke-Howson
algorithm. In an imitation game, the payoff matrix of one of the players is the
identity matrix. We study the more general &quot;unit vector games&quot;, which are
already known, where the payoff matrix of one player is composed of unit
vectors. Our main application is a simplification of the construction by Savani
and von Stengel (2006) of bimatrix games where two basic equilibrium-finding
algorithms take exponentially many steps: the Lemke-Howson algorithm, and
support enumeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02245</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02245</id><created>2015-01-06</created><authors><author><keyname>Khaleghian</keyname><forenames>Seyedmeysam</forenames></author><author><keyname>Emami</keyname><forenames>Anahita</forenames></author><author><keyname>Soltani</keyname><forenames>Nasser</forenames></author></authors><title>Image Processing Code for Sharpening Photoelastic Fringe Patterns and
  Its Usage in Determination of Stress Intensity Factors in a Sample Contact
  Problem</title><categories>cond-mat.mtrl-sci cs.OH physics.optics</categories><comments>4 pages, 5 figures, ICME 2011, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presented a type of image processing code which is used for
sharpening photoelastic fringe patterns of transparent materials in
photoelastic experiences to determine the stress distribution. C-Sharp software
was utilized for coding the algorithm of this image processing method. For
evaluation of this code, the results of a photoelastic experience of a sample
contact problem between a half-plane with an oblique edge crack and a tilted
wedge using this image processing method was compared with the FEM results of
the same problem in order to obtain the stress intensity factors (SIF) of the
specimen. A good agreement between experimental results extracted from this
method of image processing and computational results was observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02246</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02246</id><created>2015-01-06</created><authors><author><keyname>Khaleghian</keyname><forenames>Seyedmeysam</forenames></author><author><keyname>Emami</keyname><forenames>Anahita</forenames></author><author><keyname>Yadegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Soltani</keyname><forenames>Nasser</forenames></author></authors><title>The Effect of Wedge Tip Angles on Stress Intensity Factors in the
  Contact Problem between Tilted Wedge and a Half Plane with an Edge Crack
  Using Digital Image Correlation</title><categories>cond-mat.mtrl-sci cs.CV physics.optics</categories><comments>12 pages, 11 figures, The International Conference on Experimental
  Solid Mechanics and Dynamics (X-MECH-2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first and second mode stress intensity factors (SIFs) of a contact
problem between a half-plane with an edge crack and an asymmetric tilted wedge
were obtained using experimental method of Digital Image Correlation (DIC). In
this technique, displacement and strain fields can be measured using two
digital images of the same sample at different stages of loading. However,
several images were taken consequently in each stage of this experiment to
avoid the noise effect. A pair of images of each stage was compared to each
other. Then, the correlation coefficients between them were studied using a
computer code. The pairs with the correlation coefficient higher than 0.8 were
selected as the acceptable match for displacement measurements near the crack
tip. Subsequently, the SIFs of specimens were calculated using displacement
fields obtained from DIC method. The effect of wedge tips angle on their SIFs
was also studied. Moreover, the results of DIC method were compared with the
results of photoelasticity method and a close agreement between them was
observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02250</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02250</id><created>2015-01-09</created><authors><author><keyname>Hanikov&#xe1;</keyname><forenames>Zuzana</forenames></author><author><keyname>Savicky</keyname><forenames>Petr</forenames></author></authors><title>Term satisfiability in FL$_\mathrm{ew}$-algebras</title><categories>cs.LO math.LO</categories><msc-class>03B47</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FL$_\mathrm{ew}$-algebras form the algebraic semantics of the full Lambek
calculus with exchange and weakening. We investigate two relations, called
satisfiability and positive satisfiability, between FL$_\mathrm{ew}$-terms and
FL$_\mathrm{ew}$-algebras. For each FL$_\mathrm{ew}$-algebra, the sets of its
satisfiable and positively satisfiable terms can be viewed as fragments of its
existential theory; we identify and investigate the complements as fragments of
its universal theory. We offer characterizations of those algebras that
(positively) satisfy just those terms that are satisfiable in the two-element
Boolean algebra providing its semantics to classical propositional logic. In
case of positive satisfiability, these algebras are just the nontrivial weakly
contractive algebras. In case of satisfiability, we give a characterization by
means of another property of the algebra, the existence of a two-element
congruence. Further, we argue that (positive) satisfiability problems in
FL$_\mathrm{ew}$-algebras are computationally hard. Some previous results in
the area of term satisfiabilty in MV-algebras or BL-algebras, are thus brought
to a common footing with, e.g., known facts on satisfiability in Heyting
algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02252</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02252</id><created>2014-12-26</created><authors><author><keyname>Song</keyname><forenames>Junxiao</forenames></author><author><keyname>Babu</keyname><forenames>Prabhu</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Optimization Methods for Designing Sequences with Low Autocorrelation
  Sidelobes</title><categories>math.OC cs.IT math.IT stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unimodular sequences with low autocorrelations are desired in many
applications, especially in the area of radar and code-division multiple access
(CDMA). In this paper, we propose a new algorithm to design unimodular
sequences with low integrated sidelobe level (ISL), which is a widely used
measure of the goodness of a sequence's correlation property. The algorithm
falls into the general framework of majorization-minimization (MM) algorithms
and thus shares the monotonic property of such algorithms. In addition, the
algorithm can be implemented via fast Fourier transform (FFT) operations and
thus is computationally efficient. Furthermore, after some modifications the
algorithm can be adapted to incorporate spectral constraints, which makes the
design more flexible. Numerical experiments show that the proposed algorithms
outperform existing algorithms in terms of both the quality of designed
sequences and the computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02282</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02282</id><created>2015-01-09</created><authors><author><keyname>Prat</keyname><forenames>David</forenames></author><author><keyname>Ortega</keyname><forenames>Cristobal</forenames></author><author><keyname>Casas</keyname><forenames>Marc</forenames></author><author><keyname>Moret&#xf3;</keyname><forenames>Miquel</forenames></author><author><keyname>Valero</keyname><forenames>Mateo</forenames></author></authors><title>Adaptive and application dependent runtime guided hardware prefetcher
  reconfiguration on the IBM POWER7</title><categories>cs.DC</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware data prefetcher engines have been extensively used to reduce the
impact of memory latency. However, microprocessors' hardware prefetcher engines
do not include any automatic hardware control able to dynamically tune their
operation. This lacking architectural feature causes systems to operate with
prefetchers in a fixed configuration, which in many cases harms performance and
energy consumption.
  In this paper, a piece of software that solves the discussed problem in the
context of the IBM POWER7 microprocessor is presented. The proposed solution
involves using the runtime software as a bridge that is able to characterize
user applications' workload and dynamically reconfigure the prefetcher engine.
The proposed mechanisms has been deployed over OmpSs, a state-of-the-art
task-based programming model. The paper shows significant performance
improvements over a representative set of microbenchmarks and High Performance
Computing (HPC) applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02285</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02285</id><created>2015-01-09</created><updated>2015-02-04</updated><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author></authors><title>Interval Selection in the Streaming Model</title><categories>cs.DS</categories><comments>Minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of intervals is independent when the intervals are pairwise disjoint.
In the interval selection problem we are given a set $\mathbb{I}$ of intervals
and we want to find an independent subset of intervals of largest cardinality.
Let $\alpha(\mathbb{I})$ denote the cardinality of an optimal solution. We
discuss the estimation of $\alpha(\mathbb{I})$ in the streaming model, where we
only have one-time, sequential access to the input intervals, the endpoints of
the intervals lie in $\{1,...,n \}$, and the amount of the memory is
constrained.
  For intervals of different sizes, we provide an algorithm in the data stream
model that computes an estimate $\hat\alpha$ of $\alpha(\mathbb{I})$ that, with
probability at least $2/3$, satisfies $\tfrac 12(1-\varepsilon)
\alpha(\mathbb{I}) \le \hat\alpha \le \alpha(\mathbb{I})$. For same-length
intervals, we provide another algorithm in the data stream model that computes
an estimate $\hat\alpha$ of $\alpha(\mathbb{I})$ that, with probability at
least $2/3$, satisfies $\tfrac 23(1-\varepsilon) \alpha(\mathbb{I}) \le
\hat\alpha \le \alpha(\mathbb{I})$. The space used by our algorithms is bounded
by a polynomial in $\varepsilon^{-1}$ and $\log n$. We also show that no better
estimations can be achieved using $o(n)$ bits of storage.
  We also develop new, approximate solutions to the interval selection problem,
where we want to report a feasible solution, that use $O(\alpha(\mathbb{I}))$
space. Our algorithms for the interval selection problem match the optimal
results by Emek, Halld{\'o}rsson and Ros{\'e}n [Space-Constrained Interval
Selection, ICALP 2012], but are much simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02287</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02287</id><created>2015-01-09</created><authors><author><keyname>Flamini</keyname><forenames>Vittoria</forenames></author><author><keyname>DeAnda</keyname><forenames>Abe</forenames></author><author><keyname>Griffith</keyname><forenames>Boyce E.</forenames></author></authors><title>Immersed boundary-finite element model of fluid-structure interaction in
  the aortic root</title><categories>cs.CE math.NA q-bio.TO</categories><doi>10.1007/s00162-015-0374-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has long been recognized that aortic root elasticity helps to ensure
efficient aortic valve closure, but our understanding of the functional
importance of the elasticity and geometry of the aortic root continues to
evolve as increasingly detailed in vivo imaging data become available. Herein,
we describe fluid-structure interaction models of the aortic root, including
the aortic valve leaflets, the sinuses of Valsalva, the aortic annulus, and the
sinotubular junction, that employ a version of Peskin's immersed boundary (IB)
method with a finite element (FE) description of the structural elasticity. We
develop both an idealized model of the root with three-fold symmetry of the
aortic sinuses and valve leaflets, and a more realistic model that accounts for
the differences in the sizes of the left, right, and noncoronary sinuses and
corresponding valve cusps. As in earlier work, we use fiber-based models of the
valve leaflets, but this study extends earlier IB models of the aortic root by
employing incompressible hyperelastic models of the mechanics of the sinuses
and ascending aorta using a constitutive law fit to experimental data from
human aortic root tissue. In vivo pressure loading is accounted for by a
backwards displacement method that determines the unloaded configurations of
the root models. Our models yield realistic cardiac output at physiological
pressures, with low transvalvular pressure differences during forward flow,
minimal regurgitation during valve closure, and realistic pressure loads when
the valve is closed during diastole. Further, results from high-resolution
computations demonstrate that IB models of the aortic valve are able to produce
essentially grid-converged dynamics at practical grid spacings for the
high-Reynolds number flows of the aortic root.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02307</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02307</id><created>2015-01-09</created><authors><author><keyname>Cao</keyname><forenames>Yu</forenames></author><author><keyname>Blostein</keyname><forenames>Steven D.</forenames></author><author><keyname>Chan</keyname><forenames>Wai-Yip</forenames></author></authors><title>Optimization of Unequal Error Protection Rateless Codes for Multimedia
  Multicasting</title><categories>cs.IT cs.MM math.IT</categories><comments>11 pages, 7 figures, 2 tables, to appear in Journal of Communications
  and Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rateless codes have been shown to be able to provide greater flexibility and
efficiency than fixed-rate codes for multicast applications. In the following,
we optimize rateless codes for unequal error protection (UEP) for multimedia
multicasting to a set of heterogeneous users. The proposed designs have the
objectives of providing either guaranteed or best-effort quality of service
(QoS). A randomly interleaved rateless encoder is proposed whereby users only
need to decode symbols up to their own QoS level. The proposed coder is
optimized based on measured transmission properties of standardized raptor
codes over wireless channels. It is shown that a guaranteed QoS problem
formulation can be transformed into a convex optimization problem, yielding a
globally optimal solution. Numerical results demonstrate that the proposed
optimized random interleaved UEP rateless coder's performance compares
favorably with that of other recently proposed UEP rateless codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02309</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02309</id><created>2015-01-09</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Range Queries on Uncertain Data</title><categories>cs.CG cs.DB cs.DS</categories><comments>26 pages. A preliminary version of this paper appeared in ISAAC 2014.
  In this full version, we also present solutions to the most general case of
  the problem (i.e., the histogram bounded case), which were left as open
  problems in the preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ uncertain points on the real line, each represented by
its one-dimensional probability density function, we consider the problem of
building data structures on $P$ to answer range queries of the following three
types for any query interval $I$: (1) top-$1$ query: find the point in $P$ that
lies in $I$ with the highest probability, (2) top-$k$ query: given any integer
$k\leq n$ as part of the query, return the $k$ points in $P$ that lie in $I$
with the highest probabilities, and (3) threshold query: given any threshold
$\tau$ as part of the query, return all points of $P$ that lie in $I$ with
probabilities at least $\tau$. We present data structures for these range
queries with linear or nearly linear space and efficient query time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02311</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02311</id><created>2015-01-09</created><updated>2015-01-28</updated><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author><author><keyname>Zhu</keyname><forenames>Zhen</forenames></author><author><keyname>Li</keyname><forenames>Kate</forenames></author></authors><title>Building Mini-Categories in Product Networks</title><categories>cs.SI</categories><comments>Accepted to CompleNet, March 2015, NYC, NY, USA; 12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We constructed a product network based on the sales data collected and
provided by a Fortune 500 speciality retailer. The structure of the network is
dominated by small isolated components, dense clique-based communities, and
sparse stars and linear chains and pendants. We used the identified structural
elements (tiles) to organize products into mini-categories -- compact
collections of potentially complementary and substitute items. The
mini-categories extend the traditional hierarchy of retail products (group -
class - subcategory) and may serve as building blocks towards exploration of
consumer projects and long-term customer behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02315</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02315</id><created>2015-01-10</created><updated>2015-10-26</updated><authors><author><keyname>Toulis</keyname><forenames>Panos</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author></authors><title>Long-term causal effects in multiagent economies</title><categories>stat.ME cs.AI cs.MA</categories><comments>Fixed errors with citation style</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effect of a treatment in a multiagent economy, e.g., a price increase, is
causal if the treated economy would be different, e.g., in terms of revenue,
relative to the control economy. Causal effects measured in an equilibrium of
the economy, the long-term causal effects, are more representative of the value
of such treatments. However, the statistical estimation of long-term causal
effects is difficult because it has to rely, for practical reasons, on
experimental data where agents are randomly assigned to the treated or the
control economy, and their actions are observed before an equilibrium is
reached. We propose a methodology to define and estimate long-term causal
effects, which relies on a model of agent behaviors that plays a two-fold role.
First, it predicts how agents would behave under different assignments, and,
second, it predicts how agents would behave in equilibrium. These two
prediction tasks enable the estimation of long-term causal effects under
suitable assumptions, which we state explicitly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02317</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02317</id><created>2015-01-10</created><updated>2015-01-31</updated><authors><author><keyname>Zhong</keyname><forenames>Xiaoxiong</forenames></author><author><keyname>Qin</keyname><forenames>Yang</forenames></author><author><keyname>Yang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author></authors><title>CROR: Coding-Aware Opportunistic Routing in Multi-Channel Cognitive
  Radio Networks</title><categories>cs.NI</categories><comments>6 pages, 8 figures, to appear in Proc. of IEEE GlobeCom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio (CR) is a promising technology to improve spectrum
utilization. However, spectrum availability is uncertain which mainly depends
on primary user's (PU's) behaviors. This makes it more difficult for most
existing CR routing protocols to achieve high throughput in multi-channel
cognitive radio networks (CRNs). Inter-session network coding and opportunistic
routing can leverage the broadcast nature of the wireless channel to improve
the performance for CRNs. In this paper we present a coding aware opportunistic
routing protocol for multi-channel CRNs, cognitive radio opportunistic routing
(CROR) protocol, which jointly considers the probability of successful spectrum
utilization, packet loss rate, and coding opportunities. We evaluate and
compare the proposed scheme against three other opportunistic routing protocols
with multichannel. It is shown that the CROR, by integrating opportunistic
routing with network coding, can obtain much better results, with respect to
throughput, the probability of PU-SU packet collision and spectrum utilization
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02320</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02320</id><created>2015-01-10</created><updated>2015-04-03</updated><authors><author><keyname>Jog</keyname><forenames>Varun</forenames></author><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author></authors><title>On model misspecification and KL separation for Gaussian graphical
  models</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>Accepted to ISIT 2015</comments><msc-class>62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish bounds on the KL divergence between two multivariate Gaussian
distributions in terms of the Hamming distance between the edge sets of the
corresponding graphical models. We show that the KL divergence is bounded below
by a constant when the graphs differ by at least one edge; this is essentially
the tightest possible bound, since classes of graphs exist for which the edge
discrepancy increases but the KL divergence remains bounded above by a
constant. As a natural corollary to our KL lower bound, we also establish a
sample size requirement for correct model selection via maximum likelihood
estimation. Our results rigorize the notion that it is essential to estimate
the edge structure of a Gaussian graphical model accurately in order to
approximate the true distribution to close precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02323</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02323</id><created>2015-01-10</created><authors><author><keyname>Pandey</keyname><forenames>Pradumn Kumar</forenames></author><author><keyname>Adhikari</keyname><forenames>Bibhas</forenames></author></authors><title>Context dependent preferential attachment model for complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>07 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a growing random complex network model, which we
call context dependent preferential attachment model (CDPAM), when the
preference of a new node to get attached to old nodes is determined by the
local and global property of the old nodes. We consider that local and global
properties of a node as the degree and relative average degree of the node
respectively. We prove that the degree distribution of complex networks
generated by CDPAM follow power law with exponent lies in the interval [2, 3]
and the expected diameter grows logarithmically with the size of new nodes
added in the initial small network. Numerical results show that the expected
diameter stabilizes when alike weights to the local and global properties are
assigned by the new nodes. Computing various measures including clustering
coefficient, assortativity, number of triangles, algebraic connectivity,
spectral radius, we show that the proposed model replicates properties of real
networks better than BA model for all these measures when alike weights are
given to local and global property. Finally, we observe that the BA model is a
limiting case of CDPAM when new nodes tend to give large weight to the local
property compared to the weight given to the global property during link
formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02330</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02330</id><created>2015-01-10</created><authors><author><keyname>Xu</keyname><forenames>Huanle</forenames></author><author><keyname>Lau</keyname><forenames>Wing Cheong</forenames></author></authors><title>Task-Cloning Algorithms in a MapReduce Cluster with Competitive
  Performance Bounds</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Job scheduling for a MapReduce cluster has been an active research topic in
recent years. However, measurement traces from real-world production
environment show that the duration of tasks within a job vary widely. The
overall elapsed time of a job, i.e. the so-called flowtime, is often dictated
by one or few slowly-running tasks within a job, generally referred as the
&quot;stragglers&quot;. The cause of stragglers include tasks running on
partially/intermittently failing machines or the existence of some localized
resource bottleneck(s) within a MapReduce cluster. To tackle this online job
scheduling challenge, we adopt the task cloning approach and design the
corresponding scheduling algorithms which aim at minimizing the weighted sum of
job flowtimes in a MapReduce cluster based on the Shortest Remaining Processing
Time scheduler (SRPT). To be more specific, we first design a 2-competitive
offline algorithm when the variance of task-duration is negligible. We then
extend this offline algorithm to yield the so-called SRPTMS+C algorithm for the
online case and show that SRPTMS+C is $(1+\epsilon)-speed$
$o(\frac{1}{\epsilon^2})-competitive$ in reducing the weighted sum of job
flowtimes within a cluster. Both of the algorithms explicitly consider the
precedence constraints between the two phases within the MapReduce framework.
We also demonstrate via trace-driven simulations that SRPTMS+C can
significantly reduce the weighted/unweighted sum of job flowtimes by cutting
down the elapsed time of small jobs substantially. In particular, SRPTMS+C
beats the Microsoft Mantri scheme by nearly 25% according to this metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02336</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02336</id><created>2015-01-10</created><authors><author><keyname>Nandi</keyname><forenames>Chandrakana</forenames></author><author><keyname>Monot</keyname><forenames>Aurelien</forenames></author><author><keyname>Oriol</keyname><forenames>Manuel</forenames></author></authors><title>Stochastic Contracts for Runtime Checking of Component-based Real-time
  Systems</title><categories>cs.SE</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new technique for dynamic verification of
component-based real-time systems based on statistical inference. Verifying
such systems requires checking two types of properties: functional and
real-time. For functional properties, a standard approach for ensuring
correctness is Design by Contract: annotating programs with executable pre- and
postconditions. We extend contracts for specifying real-time properties. In the
industry, components are often bought from vendors and meant to be used
off-the-shelf which makes it very difficult to determine their execution times
and express related properties. We present a solution to this problem by using
statistical inference for estimating the properties. The contract framework
allows application developers to express contracts like &quot;the execution time of
component $X$ lies within $\gamma$ standard deviations from the mean execution
time&quot;. Experiments based on industrial case studies show that this framework
can be smoothly integrated into existing control applications, thereby
increasing their reliability while having an acceptable execution time overhead
(less than 10%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02344</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02344</id><created>2015-01-10</created><authors><author><keyname>Hcine</keyname><forenames>Marwane Ben</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Fitting the Log Skew Normal to the Sum of Independent Lognormals
  Distribution</title><categories>cs.IT math.IT</categories><comments>15 pages, 7 figures, NeTCoM, CSIT, GRAPH-HOC, SPTM - 2014</comments><doi>10.5121/csit.2014.41305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sums of lognormal random variables (RVs) occur in many important problems in
wireless communications especially in interferences calculation. Several
methods have been proposed to approximate the lognormal sum distribution. Most
of them requires lengthy Monte Carlo simulations, or advanced slowly converging
numerical integrations for curve fitting and parameters estimation. Recently,
it has been shown that the log skew normal distribution can offer a tight
approximation to the lognormal sum distributed RVs. We propose a simple and
accurate method for fitting the log skew normal distribution to lognormal sum
distribution. We use moments and tails slope matching technique to find optimal
log skew normal distribution parameters. We compare our method with those in
literature in terms of complexity and accuracy. We conclude that our method has
same accuracy than other methods but more simple. To further validate our
approach, we provide an example for outage probability calculation in lognormal
shadowing environment based on log skew normal approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02347</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02347</id><created>2015-01-10</created><authors><author><keyname>Hcine</keyname><forenames>Marwane Ben</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Highly Accurate Log Skew Normal Approximation to the Sum of Correlated
  Lognormals</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures, NeTCoM, CSIT, GRAPH-HOC, SPTM - 2014</comments><doi>10.5121/csit.2014.41304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several methods have been proposed to approximate the sum of correlated
lognormal RVs. However the accuracy of each method relies highly on the region
of the resulting distribution being examined, and the individual lognormal
parameters, i.e., mean and variance. There is no such method which can provide
the needed accuracy for all cases. This paper propose a universal yet very
simple approximation method for the sum of correlated lognormals based on log
skew normal approximation. The main contribution on this work is to propose an
analytical method for log skew normal parameters estimation. The proposed
method provides highly accurate approximation to the sum of correlated
lognormal distributions over the whole range of dB spreads for any correlation
coefficient. Simulation results show that our method outperforms all previously
proposed methods and provides an accuracy within 0.01 dB for all cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02357</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02357</id><created>2015-01-10</created><authors><author><keyname>Peters</keyname><forenames>James F.</forenames></author></authors><title>Visibility in Proximal Delaunay Meshes</title><categories>math.MG cs.CG</categories><comments>6 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1411.6260</comments><msc-class>65D18, 54E05, 52C20, 52C22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a visibility relation $v$ (and the strong visibility
relation $\mathop{v}\limits^{\doublewedge}$) on proximal Delaunay meshes. A
main result in this paper is that the visibility relation $v$ is equivalent to
Wallman proximity. In addition, a Delaunay triangulation region endowed with
the visibility relation $v$ has a local Leader uniform topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02361</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02361</id><created>2015-01-10</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Assessing the true role of coauthors in the h-index measure of an author
  scientific impact</title><categories>cs.DL physics.soc-ph</categories><comments>13 pages ; 40 refs</comments><journal-ref>Physica A 422 (2015) 136-142</journal-ref><doi>10.1016/j.physa.2014.12.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method based on the classical principal component analysis leads to
demonstrate that the role of co-authors should give a h-index measure to a
group leader higher than usually accepted. The method rather easily gives what
is usually searched for, i.e. an estimate of the role (or &quot;weight&quot;) of
co-authors, as the additional value to an author papers' popularity. The
construction of the co-authorship popularity H-matrix is exemplified and the
role of eigenvalues and the main eigenvector component are discussed. An
example illustrates the points and serves as the basis for suggesting a
generally practical application of the concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02365</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02365</id><created>2015-01-10</created><authors><author><keyname>Lal</keyname><forenames>Nidhi</forenames></author><author><keyname>Singh</keyname><forenames>Anurag Prakash</forenames></author><author><keyname>Kumar</keyname><forenames>Shishupal</forenames></author></authors><title>Modified Trial Division Algorithm Using KNJ-Factorization Method To
  Factorize RSA Public Key Encryption</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of RSA algorithm depends upon the positive integer N, which is
the multiple of two precise large prime numbers. Factorization of such great
numbers is a problematic process. There are many algorithms has been
implemented in the past years. The offered KNJ -Factorization algorithm
contributes a deterministic way to factorize RSA. The algorithm limits the
search by only considering the prime values. Subsequently prime numbers are odd
numbers accordingly it also requires smaller number steps to factorize RSA. In
this paper, the anticipated algorithm is very simple besides it is very easy to
understand and implement. The main concept of this KNJ factorization algorithm
is, to check only those factors which are odd and prime. The proposed KNJ-
Factorization algorithm works very efficiently on those factors; which are
adjoining and close to N. The proposed factorization method can speed up if we
can reduce the time for primality testing. It fundamentally decreases the time
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02372</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02372</id><created>2015-01-10</created><updated>2015-07-04</updated><authors><author><keyname>Uss</keyname><forenames>M.</forenames></author><author><keyname>Vozel</keyname><forenames>B.</forenames></author><author><keyname>Lukin</keyname><forenames>V.</forenames></author><author><keyname>Chehdi</keyname><forenames>K.</forenames></author></authors><title>Efficient Rotation-Scaling-Translation Parameters Estimation Based on
  Fractal Image Model</title><categories>cs.CV</categories><comments>42 pages, 8 figures, 7 tables. Journal paper</comments><doi>10.1109/TGRS.2015.2453126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with area-based subpixel image registration under
rotation-isometric scaling-translation transformation hypothesis. Our approach
is based on a parametrical modeling of geometrically transformed textural image
fragments and maximum likelihood estimation of transformation vector between
them. Due to the parametrical approach based on the fractional Brownian motion
modeling of the local fragments texture, the proposed estimator MLfBm (ML
stands for &quot;Maximum Likelihood&quot; and fBm for &quot;Fractal Brownian motion&quot;) has the
ability to better adapt to real image texture content compared to other methods
relying on universal similarity measures like mutual information or normalized
correlation. The main benefits are observed when assumptions underlying the fBm
model are fully satisfied, e.g. for isotropic normally distributed textures
with stationary increments. Experiments on both simulated and real images and
for high and weak correlation between registered images show that the MLfBm
estimator offers significant improvement compared to other state-of-the-art
methods. It reduces translation vector, rotation angle and scaling factor
estimation errors by a factor of about 1.75...2 and it decreases probability of
false match by up to 5 times. Besides, an accurate confidence interval for
MLfBm estimates can be obtained from the Cramer-Rao lower bound on
rotation-scaling-translation parameters estimation error. This bound depends on
texture roughness, noise level in reference and template images, correlation
between these images and geometrical transformation parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02376</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02376</id><created>2015-01-10</created><authors><author><keyname>Ahmad</keyname><forenames>Muhammad Zubair</forenames></author><author><keyname>Akhtar</keyname><forenames>Ayyaz</forenames></author><author><keyname>Khan</keyname><forenames>Abdul Qadeer</forenames></author><author><keyname>Khan</keyname><forenames>Amir A.</forenames></author></authors><title>Simplified vision based automatic navigation for wheat harvesting in low
  income economies</title><categories>cs.RO cs.CV cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recent developments in the domain of agricultural robotics have resulted in
development of complex and efficient systems. Most of the land owners in the
South Asian region are low income farmers. The agricultural experience for them
is still a completely manual process. However, the extreme weather conditions,
heat and flooding, often combine to put a lot of stress on these small land
owners and the associated labor. In this paper, we propose a prototype for an
automated power reaper for the wheat crop. This automated vehicle is navigated
using a simple vision based approach employing the low-cost camera and assisted
GPS. The mechanical platform is driven by three motors controlled through an
interface between the proposed vision algorithm and the electrical drive. The
proposed methodology is applied on some real field scenarios to demonstrate the
efficiency of the vision based algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02377</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02377</id><created>2015-01-10</created><updated>2015-12-18</updated><authors><author><keyname>Iwen</keyname><forenames>Mark</forenames></author><author><keyname>Viswanathan</keyname><forenames>Aditya</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author></authors><title>Fast Phase Retrieval from Local Correlation Measurements</title><categories>math.NA cs.IT math.IT</categories><comments>Title changed from first draft's title - 'Fast Phase Retrieval For
  High-Dimensions'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a fast phase retrieval method which can utilize a large class of
local phaseless correlation-based measurements in order to recover a given
signal ${\bf x} \in \mathbb{C}^d$ (up to an unknown global phase) in
near-linear $\mathcal{O} \left(d\log^4 d \right)$-time. Accompanying
theoretical analysis proves that the proposed algorithm is guaranteed to
deterministically recover all signals ${\bf x}$ satisfying a natural flatness
(i.e., non-sparsity) condition for a particular choice of deterministic
correlation-based measurements. A randomized version of these same measurements
are then shown to provide nonuniform probabilistic recovery guarantees for
arbitrary signals ${\bf x} \in \mathbb{C}^d$. Numerical experiments demonstrate
the method's speed, accuracy, and robustness in practice -- all code is made
publicly available. Finally, we develop an extension of the proposed method to
the sparse phase retrieval problem, specifically, we demonstrate a
sublinear-time compressive phase retrieval algorithm which is guaranteed to
recover a given $s$-sparse vector ${\bf x} \in \mathbb{C}^d$ with high
probability in just $\mathcal{O}(s \log^5 s \cdot \log d)$-time using only
$\mathcal{O}(s \log^4 s \cdot \log d)$ magnitude measurements. In doing so we
prove the existence of compressive phase retrieval algorithms with near-optimal
linear-in-sparsity runtime complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02378</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02378</id><created>2015-01-10</created><authors><author><keyname>Ahmad</keyname><forenames>Muhammad Zubair</forenames></author><author><keyname>Akhtar</keyname><forenames>Ayyaz</forenames></author><author><keyname>Khan</keyname><forenames>Abdul Qadeer</forenames></author><author><keyname>Khan</keyname><forenames>Amir Ali</forenames></author><author><keyname>Khan</keyname><forenames>Muhammad Murtaza</forenames></author></authors><title>Low Cost Semi-Autonomous Agricultural Robots In Pakistan-Vision Based
  Navigation Scalable methodology for wheat harvesting</title><categories>cs.RO cs.CV cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Robots have revolutionized our way of life in recent years.One of the domains
that has not yet completely benefited from the robotic automation is the
agricultural sector. Agricultural Robotics should complement humans in the
arduous tasks during different sub-domains of this sector. Extensive research
in Agricultural Robotics has been carried out in Japan, USA, Australia and
Germany focusing mainly on the heavy agricultural machinery. Pakistan is an
agricultural rich country and its economy and food security are closely tied
with agriculture in general and wheat in particular. However, agricultural
research in Pakistan is still carried out using the conventional methodologies.
This paper is an attempt to trigger the research in this modern domain so that
we can benefit from cost effective and resource efficient autonomous
agricultural methodologies. This paper focuses on a scalable low cost
semi-autonomous technique for wheat harvest which primarily focuses on the
farmers with small land holdings. The main focus will be on the vision part of
the navigation system deployed by the proposed robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02379</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02379</id><created>2015-01-10</created><authors><author><keyname>Khan</keyname><forenames>Abdul Qadeer</forenames></author><author><keyname>Akhtar</keyname><forenames>Ayyaz</forenames></author><author><keyname>Ahmad</keyname><forenames>Muhammad Zubair</forenames></author></authors><title>Autonomous Farm Vehicles: Prototype of Power Reaper</title><categories>cs.RO cs.CV cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Chapter 2 will begin with introduction of Agricultural Robotics. There will
be a literature review of the mechanical structure, vision and control
algorithms. In chapter 3 we will discuss the methodology in detail using block
diagrams and flowcharts. The results of the tested and the proposed algorithms
will also be displayed. In chapter 4 we will discuss the results in detail and
how they are of significance in our work. In chapter 5 we will conclude our
work and discuss some future perspectives. In appendices we will provide some
background information necessary regarding this project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02388</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02388</id><created>2015-01-10</created><updated>2015-03-26</updated><authors><author><keyname>Blum</keyname><forenames>Christian</forenames></author><author><keyname>Raidl</keyname><forenames>G&#xfc;nther R.</forenames></author></authors><title>Computational Performance Evaluation of Two Integer Linear Programming
  Models for the Minimum Common String Partition Problem</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1405.5646 This paper
  version replaces the one submitted on January 10, 2015, due to detected error
  in the calculation of the variables involved in the ILP models</comments><msc-class>G.1.6</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the minimum common string partition (MCSP) problem two related input
strings are given. &quot;Related&quot; refers to the property that both strings consist
of the same set of letters appearing the same number of times in each of the
two strings. The MCSP seeks a minimum cardinality partitioning of one string
into non-overlapping substrings that is also a valid partitioning for the
second string. This problem has applications in bioinformatics e.g. in
analyzing related DNA or protein sequences. For strings with lengths less than
about 1000 letters, a previously published integer linear programming (ILP)
formulation yields, when solved with a state-of-the-art solver such as CPLEX,
satisfactory results. In this work, we propose a new, alternative ILP model
that is compared to the former one. While a polyhedral study shows the linear
programming relaxations of the two models to be equally strong, a comprehensive
experimental comparison using real-world as well as artificially created
benchmark instances indicates substantial computational advantages of the new
formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02393</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02393</id><created>2015-01-10</created><authors><author><keyname>Vemulapalli</keyname><forenames>Raviteja</forenames></author><author><keyname>Jacobs</keyname><forenames>David W.</forenames></author></authors><title>Riemannian Metric Learning for Symmetric Positive Definite Matrices</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, symmetric positive definite (SPD) matrices have been
receiving considerable attention from computer vision community. Though various
distance measures have been proposed in the past for comparing SPD matrices,
the two most widely-used measures are affine-invariant distance and
log-Euclidean distance. This is because these two measures are true geodesic
distances induced by Riemannian geometry. In this work, we focus on the
log-Euclidean Riemannian geometry and propose a data-driven approach for
learning Riemannian metrics/geodesic distances for SPD matrices. We show that
the geodesic distance learned using the proposed approach performs better than
various existing distance measures when evaluated on face matching and
clustering tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02398</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02398</id><created>2015-01-10</created><updated>2015-01-29</updated><authors><author><keyname>Shestakov</keyname><forenames>Denis</forenames></author><author><keyname>Moise</keyname><forenames>Diana</forenames></author></authors><title>Scalable high-dimensional indexing and searching with Hadoop</title><categories>cs.IR cs.DC cs.MM</categories><comments>This paper has been withdrawn by the authors. The manuscript has been
  withdrawn as having no new material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While high-dimensional search-by-similarity techniques reached their maturity
and in overall provide good performance, most of them are unable to cope with
very large multimedia collections. The 'big data' challenge however has to be
addressed as multimedia collections have been explosively growing and will grow
even faster than ever within the next few years. Luckily, computational
processing power has become more available to researchers due to easier access
to distributed grid infrastructures. In this paper, we show how
high-dimensional indexing and searching methods can be used on scientific grid
environments and present a scalable workflow for indexing and searching over 30
billion SIFT descriptors using a cluster running Hadoop. Besides its
scalability, the proposed scheme not only provides good search quality, but
also achieves a stable throughput of around 210ms per image when searching a
100M image collection. Our findings could help other researchers and
practitioners to cope with huge multimedia collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02405</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02405</id><created>2015-01-10</created><authors><author><keyname>Razavi</keyname><forenames>Alireza</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Covariance-Based OFDM Spectrum Sensing with Sub-Nyquist Samples</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures</comments><journal-ref>Signal Processing, Volume 109, April 2015, Pages 261-268</journal-ref><doi>10.1016/j.sigpro.2014.11.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a feature-based method for spectrum sensing of OFDM
signals from sub-Nyquist samples over a single band. We exploit the structure
of the covariance matrix of OFDM signals to convert an underdetermined set of
covariance-based equations to an overdetermined one. The statistical properties
of sample covariance matrix are analyzed and then based on that an approximate
Generalized Likelihood Ratio Test (GLRT) for detection of OFDM signals from
sub-Nyquist samples is derived. The method is also extended to the
frequency-selective channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02410</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02410</id><created>2015-01-10</created><authors><author><keyname>Semiari</keyname><forenames>Omid</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author></authors><title>Matching Theory for Backhaul Management in Small Cell Networks with
  mmWave Capabilities</title><categories>cs.IT cs.GT math.IT</categories><comments>In Proc. of the IEEE International Conference on Communications
  (ICC), Mobile and Wireless Networks Symposium, London, UK, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing cost-effective and scalable backhaul solutions is one of the main
challenges for emerging wireless small cell networks (SCNs). In this regard,
millimeter wave (mmW) communication technologies have recently emerged as an
attractive solution to realize the vision of a high-speed and reliable wireless
small cell backhaul network (SCBN). In this paper, a novel approach is proposed
for managing the spectral resources of a heterogeneous SCBN that can exploit
simultaneously mmW and conventional frequency bands via carrier aggregation. In
particular, a new SCBN model is proposed in which small cell base stations
(SCBSs) equipped with broadband fiber backhaul allocate their frequency
resources to SCBSs with wireless backhaul, by using aggregated bands. One
unique feature of the studied model is that it jointly accounts for both
wireless channel characteristics and economic factors during resource
allocation. The problem is then formulated as a one-to-many matching game and a
distributed algorithm is proposed to find a stable outcome of the game. The
convergence of the algorithm is proven and the properties of the resulting
matching are studied. Simulation results show that under the constraints of
wireless backhauling, the proposed approach achieves substantial performance
gains, reaching up to $30 \%$ compared to a conventional best-effort approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02411</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02411</id><created>2015-01-10</created><authors><author><keyname>Li</keyname><forenames>Haojun</forenames></author></authors><title>A Gaussian Particle Filter Approach for Sensors to Track Multiple Moving
  Targets</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of problems, the number and state of multiple moving targets are
unknown and are subject to be inferred from their measurements obtained by a
sensor with limited sensing ability. This type of problems is raised in a
variety of applications, including monitoring of endangered species, cleaning,
and surveillance. Particle filters are widely used to estimate target state
from its prior information and its measurements that recently become available,
especially for the cases when the measurement model and the prior distribution
of state of interest are non-Gaussian. However, the problem of estimating
number of total targets and their state becomes intractable when the number of
total targets and the measurement-target association are unknown. This paper
presents a novel Gaussian particle filter technique that combines Kalman filter
and particle filter for estimating the number and state of total targets based
on the measurement obtained online. The estimation is represented by a set of
weighted particles, different from classical particle filter, where each
particle is a Gaussian distribution instead of a point mass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02419</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02419</id><created>2015-01-11</created><authors><author><keyname>Wildman</keyname><forenames>Jeffrey</forenames></author><author><keyname>Osmanlioglu</keyname><forenames>Yusuf</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author><author><keyname>Shokoufandeh</keyname><forenames>Ali</forenames></author></authors><title>Delay Minimizing User Association in Cellular Networks via
  Hierarchically Well-Separated Trees</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 5 figures. Submitted on 2013-10-03 to the 2015 IEEE
  International Conference on Communications (ICC). Accepted on 2015-01-09 to
  the 2015 IEEE International Conference on Communications (ICC)</comments><doi>10.1109/ICC.2015.7248950</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study downlink delay minimization within the context of cellular user
association policies that map mobile users to base stations. We note the delay
minimum user association problem fits within a broader class of network utility
maximization and can be posed as a non-convex quadratic program. This
non-convexity motivates a split quadratic objective function that captures the
original problem's inherent tradeoff: association with a station that provides
the highest signal-to-interference-plus-noise ratio (SINR) vs. a station that
is least congested. We find the split-term formulation is amenable to
linearization by embedding the base stations in a hierarchically well-separated
tree (HST), which offers a linear approximation with constant distortion. We
provide a numerical comparison of several problem formulations and find that
with appropriate optimization parameter selection, the quadratic reformulation
produces association policies with sum delays that are close to that of the
original network utility maximization. We also comment on the more difficult
problem when idle base stations (those without associated users) are
deactivated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02428</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02428</id><created>2015-01-11</created><updated>2015-08-25</updated><authors><author><keyname>Chang</keyname><forenames>Tofar C. -Y.</forenames></author><author><keyname>Su</keyname><forenames>Yu T.</forenames></author></authors><title>Dynamic Weighted Bit-Flipping Decoding Algorithms for LDPC Codes</title><categories>cs.IT math.IT</categories><doi>10.1109/TCOMM.2015.2469780</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bit-flipping (BF) decoding of low-density parity-check codes is of low
complexity but gives inferior performance in general. To improve performance
and provide new BF decoder options for complexity-performance tradeoffs, we
propose new designs for the flipping function (FF), the flipped bit selection
(FBS) rule and the checksum weight updating schedule. The new FF adjusts the
checksum weights in every iteration while our FBS rules take more information
into account. These two modifications represent efforts to track more closely
the evolutions of both check and variable nodes' reliabilities. Two selective
update schedules are proposed to offer more performance and complexity
tradeoffs.
  The combinations of the new FBS rule and known FFs result in new BF decoders
with improved performance and a modest complexity increase. On the other hand,
combining the new FF and FBS rule gives a new decoder with performance
comparable to that of the normalized min-sum algorithm while if we use a much
simpler FBS rule instead, the decoder suffers little performance loss with
reduced complexity. We also present a simple decision-theoretical argument to
justify the new checksum weight formula and a time-expanded factor graph model
to explain the proposed selective weight-updating schedules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02429</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02429</id><created>2015-01-11</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Chen</keyname><forenames>Hsiao-Hwa</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author></authors><title>Enhancing Wireless Information and Power Transfer by Exploiting
  Multi-Antenna Techniques</title><categories>cs.IT math.IT</categories><comments>18 pages, 6 figures, IEEE Communications Magazine, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews emerging wireless information and power transfer (WIPT)
technique with an emphasis on its performance enhancement employing
multi-antenna techniques. Compared to traditional wireless information
transmission, WIPT faces numerous challenges. First, it is more susceptible to
channel fading and path loss, resulting in a much shorter power transfer
distance. Second, it gives rise to the issue on how to balance spectral
efficiency for information transmission and energy efficiency for power
transfer in order to obtain an optimal tradeoff. Third, there exists a security
issue for information transmission in order to improve power transfer
efficiency. In this context, multi-antenna techniques, e.g., energy
beamforming, are introduced to solve these problems by exploiting spatial
degree of freedom. This article provides a tutorial on various aspects of
multi-antenna based WIPT techniques, with a focus on tackling the challenges by
parameter optimization and protocol design. In particular, we investigate the
WIPT tradeoffs based on two typical multi-antenna techniques, namely limited
feedback multi-antenna technique for short-distance transfer and large-scale
multiple-input multiple-output (LS-MIMO, also known as massive MIMO) technique
for long-distance transfer. Finally, simulation results validate the
effectiveness of the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02431</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02431</id><created>2015-01-11</created><authors><author><keyname>Paithankar</keyname><forenames>Rashmi</forenames></author><author><keyname>Tidke</keyname><forenames>Bharat</forenames></author></authors><title>A H-K Clustering Algorithm For High Dimensional Data Using Ensemble
  Learning</title><categories>cs.DB</categories><comments>9 pages, 1 table, 2 figures, International Journal of Information
  Technology Convergence and Services (IJITCS) Vol.4, No.5/6, December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances made to the traditional clustering algorithms solves the various
problems such as curse of dimensionality and sparsity of data for multiple
attributes. The traditional H-K clustering algorithm can solve the randomness
and apriority of the initial centers of K-means clustering algorithm. But when
we apply it to high dimensional data it causes the dimensional disaster problem
due to high computational complexity. All the advanced clustering algorithms
like subspace and ensemble clustering algorithms improve the performance for
clustering high dimension dataset from different aspects in different extent.
Still these algorithms will improve the performance form a single perspective.
The objective of the proposed model is to improve the performance of
traditional H-K clustering and overcome the limitations such as high
computational complexity and poor accuracy for high dimensional data by
combining the three different approaches of clustering algorithm as subspace
clustering algorithm and ensemble clustering algorithm with H-K clustering
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02432</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02432</id><created>2015-01-11</created><authors><author><keyname>Jayadeva</keyname></author><author><keyname>Batra</keyname><forenames>Sanjit Singh</forenames></author><author><keyname>Sabharwal</keyname><forenames>Siddarth</forenames></author></authors><title>Learning a Fuzzy Hyperplane Fat Margin Classifier with Minimum VC
  dimension</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1410.4573</comments><msc-class>68T05, 68T10, 68Q32</msc-class><acm-class>I.5.1; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning
machine, and a low VC dimension leads to good generalization. The recently
proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by
minimizing an exact bound on the VC dimension. This paper extends the MCM
classifier to the fuzzy domain. The use of a fuzzy membership is known to
reduce the effect of outliers, and to reduce the effect of noise on learning.
Experimental results show, that on a number of benchmark datasets, the the
fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of
generalization, and that the fuzzy MCM uses fewer support vectors. On several
benchmark datasets, the fuzzy MCM classifier yields excellent test set
accuracies while using one-tenth the number of support vectors used by SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02444</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02444</id><created>2015-01-11</created><updated>2015-05-22</updated><authors><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Unified Scaling of Polar Codes: Error Exponent, Scaling Exponent,
  Moderate Deviations, and Error Floors</title><categories>cs.IT math.IT</categories><comments>23 pages, 4 figures, submitted to IEEE Trans. Inform. Theory and
  accepted in part at ISIT'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider transmission of a polar code of block length $N$ and rate $R$ over a
binary memoryless symmetric channel $W$ and let $P_e$ be the error probability
under successive cancellation decoding. In this paper, we develop new bounds
that characterize the relationship among the parameters $R$, $N$, $P_e$, and
the quality of the channel $W$ quantified by its capacity $I(W)$ and its
Bhattacharyya parameter $Z(W)$. In previous works, two main regimes have been
studied. In the error exponent regime, the channel $W$ and the rate $R&lt;I(W)$
are fixed, and it has been proved that the error probability $P_e$ scales
roughly as $2^{-\sqrt{N}}$. In the scaling exponent approach, the channel $W$
and the error probability $P_e$ are fixed and it has been proved that the gap
to capacity $I(W)-R$ scales as $N^{-1/\mu}$, where $\mu$ is called scaling
exponent. A heuristic computation for the binary erasure channel (BEC) gives
$\mu=3.627$ and it has been shown that, for any channel $W$, $3.579 \le \mu \le
5.702$.
  The original contributions of this paper are as follows. First, we provide
the tigher upper bound $\mu \le 4.714$ valid for any $W$. With the same
technique, we obtain an upper bound for the scaling exponent of the BEC which
very closely approaches its heuristically derived value $\mu =3.639$. Secondly,
we develop a trade-off between the gap to capacity $I(W)-R$ and the error
probability $P_e$, as functions of the block length $N$. In other words, we
consider a moderate deviations regime in which we study how fast both
quantities simultaneously go to 0 as $N$ goes large. Thirdly, we prove that
polar codes are not affected by error floors. To do so, we fix a polar code of
block length $N$ and rate $R$. We then vary the channel $W$ and we show that
the error probability $P_e$ scales as the Bhattacharyya parameter $Z(W)$ raised
to a power which scales like ${\sqrt{N}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02472</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02472</id><created>2015-01-11</created><updated>2015-07-31</updated><authors><author><keyname>Sanatkar</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>White</keyname><forenames>Warren N.</forenames></author><author><keyname>Natarajan</keyname><forenames>Balasubramaniam</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author><author><keyname>Garrett</keyname><forenames>Karren A.</forenames></author></authors><title>Epidemic Threshold of an SIS Model in Dynamic Switching Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Published in IEEE Transactions on Systems, Man and Cybernetics</comments><doi>10.1109/TSMC.2015.2448061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze dynamic switching networks, wherein the networks
switch arbitrarily among a set of topologies. For this class of dynamic
networks, we derive an epidemic threshold, considering the SIS epidemic model.
First, an epidemic probabilistic model is developed assuming independence
between states of nodes. We identify the conditions under which the epidemic
dies out by linearizing the underlying dynamical system and analyzing its
asymptotic stability around the origin. The concept of joint spectral radius is
then used to derive the epidemic threshold, which is later validated using
several networks (Watts-Strogatz, Barabasi-Albert, MIT reality mining graphs,
Regular, and Gilbert). A simplified version of the epidemic threshold is
proposed for undirected networks. Moreover, in the case of static networks, the
derived epidemic threshold is shown to match conventional analytical results.
Then, analytical results for the epidemic threshold of dynamic networksare
proved to be applicable to periodic networks. For dynamic regular networks, we
demonstrate that the epidemic threshold is identical to the epidemic threshold
for static regular networks. An upper bound for the epidemic spread probability
in dynamic Gilbert networks is also derived and verified using simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02473</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02473</id><created>2015-01-11</created><authors><author><keyname>Vangala</keyname><forenames>Harish</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author></authors><title>A Comparative Study of Polar Code Constructions for the AWGN Channel</title><categories>cs.IT math.IT</categories><comments>9 pages, submitted, under revision of an IEEE journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a comparative study of the performance of various polar code
constructions in an additive white Gaussian noise (AWGN) channel. A polar code
construction is any algorithm that selects $K$ best among $N$ possible polar
bit-channels at the design signal-to-noise-ratio (design-SNR) in terms of bit
error rate (BER). Optimal polar code construction is hard and therefore many
suboptimal polar code constructions have been proposed at different
computational complexities. Polar codes are also non-universal meaning the code
changes significantly with the design-SNR. However, it is not known which
construction algorithm at what design-SNR constructs the best polar codes. We
first present a comprehensive survey of all the well-known polar code
constructions along with their full implementations. We then propose a
heuristic algorithm to find the best design-SNR for constructing best possible
polar codes from a given construction algorithm. The proposed algorithm
involves a search among several possible design-SNRs. We finally use our
algorithm to perform a comparison of different construction algorithms using
extensive simulations. We find that all polar code construction algorithms
generate equally good polar codes in an AWGN channel, if the design-SNR is
optimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02475</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02475</id><created>2015-01-11</created><authors><author><keyname>Pinheiro</keyname><forenames>Eduardo G.</forenames></author><author><keyname>Alberto</keyname><forenames>T&#xfa;lio C.</forenames></author></authors><title>Teleoperando Rob\^os Pioneer Utilizando Android</title><categories>cs.RO</categories><comments>in Portuguese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an application with ROS, Aria and RosAria to control a
ModelSim simulated Pioneer 3-DX robot. The navigation applies a simple
autonomous algorithm and a teleoperation control using an Android device
sending the gyroscope generated information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02476</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02476</id><created>2015-01-11</created><authors><author><keyname>Mian</keyname><forenames>Salman</forenames></author><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author><author><keyname>Koskivaara</keyname><forenames>Eija</forenames></author></authors><title>Open-Source Software Implications in the Competitive Mobile Platforms
  Market</title><categories>cs.SE</categories><comments>As presented on the 11th IFIP WG 6.11 Conference on e-Business,
  e-Services, and e-Society, I3E 2011, Kaunas, Lithuania, October 12-14, IFIP
  Advances in Information and Communication Technology Volume 353 Year 2011</comments><doi>10.1007/978-3-642-55128-4_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The era of the PC platform left a legacy of competitive strategies for the
future technologies to follow. However, this notion became more complicated,
once the future grew out to be a present with huge bundle of innovative
technologies, Internet capabilities, communication possibilities, and ease in
life. A major step of moving from a product phone to a smart phone, eventually
to a mobile device has created a new industry with humongous potential for
further developments. The current mobile platform market is witnessing a
platforms-war with big players such as Apple, Google, Nokia and Microsoft in a
major role. An important aspect of today's mobile platform market is the
contributions made through open source initiatives which promote innovation.
This paper gives an insight into the open-source software strategies of the
leading players and its implications on the market. It first gives a precise
overview of the past leading to the current mobile platform market share state.
Then it briefs about the open-source software components used and released by
Apple, Google and Nokia platforms, leading to their mobile platform strategies
with regard to open source. Finally, the paper assesses the situation from the
point of view of communities of software developers complementing each
platform. The authors identified relevant implications of the open-source
phenomenon in the mobile-industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02478</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02478</id><created>2015-01-11</created><updated>2016-02-23</updated><authors><author><keyname>Luo</keyname><forenames>Yuan</forenames></author><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>HySIM: A Hybrid Spectrum and Information Market for TV White Space
  Networks</title><categories>cs.GT</categories><comments>This manuscript serves as the online technical report of the article
  published in IEEE International Conference on Computer Communications
  (INFOCOM), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a hybrid spectrum and information market for a database-assisted
TV white space network, where the geo-location database serves as both a
spectrum market platform and an information market platform. We study the
inter- actions among the database operator, the spectrum licensee, and
unlicensed users systematically, using a three-layer hierarchical model. In
Layer I, the database and the licensee negotiate the commission fee that the
licensee pays for using the spectrum market platform. In Layer II, the database
and the licensee compete for selling information or channels to unlicensed
users. In Layer III, unlicensed users determine whether they should buy the
exclusive usage right of licensed channels from the licensee, or the
information regarding unlicensed channels from the database. Analyzing such a
three-layer model is challenging due to the co-existence of both positive and
negative network externalities in the information market. We characterize how
the network externalities affect the equilibrium behaviours of all parties
involved. Our numerical results show that the proposed hybrid market can
improve the network profit up to 87%, compared with a pure information market.
Meanwhile, the achieved network profit is very close to the coordinated
benchmark solution (the gap is less than 4% in our simulation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02480</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02480</id><created>2015-01-11</created><updated>2016-02-23</updated><authors><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Hou</keyname><forenames>Fen</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Providing Long-Term Participation Incentive in Participatory Sensing</title><categories>cs.GT cs.NI</categories><comments>This manuscript serves as the online technical report of the article
  published in IEEE International Conference on Computer Communications
  (INFOCOM), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing an adequate long-term participation incentive is important for a
participatory sensing system to maintain enough number of active users
(sensors), so as to collect a sufficient number of data samples and support a
desired level of service quality. In this work, we consider the sensor
selection problem in a general time-dependent and location-aware participatory
sensing system, taking the long-term user participation incentive into explicit
consideration. We study the problem systematically under different information
scenarios, regarding both future information and current information
(realization). In particular, we propose a Lyapunov-based VCG auction policy
for the on-line sensor selection, which converges asymptotically to the optimal
off-line benchmark performance, even with no future information and under
(current) information asymmetry. Extensive numerical results show that our
proposed policy outperforms the state-of-art policies in the literature, in
terms of both user participation (e.g., reducing the user dropping probability
by 25% to 90%) and social performance (e.g., increasing the social welfare by
15% to 80%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02482</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02482</id><created>2015-01-11</created><authors><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author><author><keyname>Baiyere</keyname><forenames>Abayomi</forenames></author></authors><title>Crafting a Systematic Literature Review on Open-Source Platforms</title><categories>cs.SE</categories><comments>As presented in 10th IFIP WG 2.13 International Conference on Open
  Source Systems, OSS 2014, San Jos\'e, Costa Rica, May 6-9, 2014</comments><acm-class>D.2.9</acm-class><journal-ref>IFIP Advances in Information and Communication Technology vol 427
  2014</journal-ref><doi>10.1007/978-3-642-55128-4_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This working paper unveils the crafting of a systematic literature review on
open-source platforms. The high-competitive mobile devices market, where
several players such as Apple, Google, Nokia and Microsoft run a platforms- war
with constant shifts in their technological strategies, is gaining increasing
attention from scholars. It matters, then, to review previous literature on
past platforms-wars, such as the ones from the PC and game-console industries,
and assess its implications to the current mobile devices platforms-war. The
paper starts by justifying the purpose and rationale behind this literature
review on open-source platforms. The concepts of open-source software and
computer-based platforms were then discussed both individually and in unison,
in order to clarify the core-concept of 'open-source platform' that guides this
literature review. The detailed design of the employed methodological strategy
is then presented as the central part of this paper. The paper concludes with
preliminary findings organizing previous literature on open-source platforms
for the purpose of guiding future research in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02483</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02483</id><created>2015-01-11</created><updated>2015-06-01</updated><authors><author><keyname>Tarighati</keyname><forenames>Alla</forenames></author><author><keyname>Farhadi</keyname><forenames>Hamed</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>Design of LDPC Codes Robust to Noisy Message-Passing Decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address noisy message-passing decoding of lowdensity parity-check (LDPC)
codes over additive white Gaussian noise channels. Message-passing decoders in
which certain processing units iteratively exchange messages are common for
decoding LDPC codes. The exchanged messages are in general subject to internal
noise in hardware implementation of these decoders. We model the internal
decoder noise as additive white Gaussian noise (AWGN) degrading exchanged
messages. Using Gaussian approximation of the exchanged messages, we perform a
two-dimensional density evolution analysis for the noisy LDPC decoder. This
makes it possible to track both the mean, and the variance of the exchanged
message densities, and hence, to quantify the threshold of the LDPC code in the
presence of internal decoder noise. The numerical and simulation results are
presented that quantify the performance loss due to the internal decoder noise.
To partially compensate this performance loss, we propose a simple method,
based on EXIT chart analysis, to design robust irregular LDPC codes. The
simulation results indicate that the designed codes can indeed compensate part
of the performance loss due to the internal decoder noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02484</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02484</id><created>2015-01-11</created><authors><author><keyname>Hamm</keyname><forenames>Jihun</forenames></author><author><keyname>Champion</keyname><forenames>Adam</forenames></author><author><keyname>Chen</keyname><forenames>Guoxing</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Xuan</keyname><forenames>Dong</forenames></author></authors><title>Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart
  Devices</title><categories>cs.LG cs.CR cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart devices with built-in sensors, computational capabilities, and network
connectivity have become increasingly pervasive. The crowds of smart devices
offer opportunities to collectively sense and perform computing tasks in an
unprecedented scale. This paper presents Crowd-ML, a privacy-preserving machine
learning framework for a crowd of smart devices, which can solve a wide range
of learning problems for crowdsensing data with differential privacy
guarantees. Crowd-ML endows a crowdsensing system with an ability to learn
classifiers or predictors online from crowdsensing data privately with minimal
computational overheads on devices and servers, suitable for a practical and
large-scale employment of the framework. We analyze the performance and the
scalability of Crowd-ML, and implement the system with off-the-shelf
smartphones as a proof of concept. We demonstrate the advantages of Crowd-ML
with real and simulated experiments under various conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02487</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02487</id><created>2015-01-11</created><authors><author><keyname>Saeed</keyname><forenames>Muhammad Omer Bin</forenames></author></authors><title>A Unified Analysis Approach for LMS-based Variable Step-Size Algorithms</title><categories>cs.DS</categories><comments>5 pages, 1 figure, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The least-mean-squares (LMS) algorithm is the most popular algorithm in
adaptive filtering. Several variable step-size strategies have been suggested
to improve the performance of the LMS algorithm. These strategies enhance the
performance of the algorithm but a major drawback is the complexity in the
theoretical analysis of the resultant algorithms. Researchers use several
assumptions to find closed-form analytical solutions. This work presents a
unified approach for the analysis of variable step-size LMS algorithms. The
approach is then applied to several variable step-size strategies and
theoretical and simulation results are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02492</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02492</id><created>2015-01-11</created><authors><author><keyname>Babenko</keyname><forenames>Maxim</forenames></author><author><keyname>Goldberg</keyname><forenames>Andrew V.</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Savchenko</keyname><forenames>Ruslan</forenames></author><author><keyname>Weller</keyname><forenames>Mathias</forenames></author></authors><title>On the Complexity of Hub Labeling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hub Labeling (HL) is a data structure for distance oracles. Hierarchical HL
(HHL) is a special type of HL, that received a lot of attention from a
practical point of view. However, theoretical questions such as NP-hardness and
approximation guarantee for HHL algorithms have been left aside. In this paper
we study HL and HHL from the complexity theory point of view. We prove that
both HL and HHL are NP-hard, and present upper and lower bounds for the
approximation ratios of greedy HHL algorithms used in practice. We also
introduce a new variant of the greedy HHL algorithm and a proof that it
produces small labels for graphs with small highway dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02500</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02500</id><created>2015-01-11</created><authors><author><keyname>Maximov</keyname><forenames>Yura</forenames></author></authors><title>Lower bounds on the DNF exception problem for short exception lists and
  related problems</title><categories>math.CO cs.CC</categories><comments>in Russian, 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prowide lower bounds on the complexity of the DNF exception
problem for short exception lists and hypercube covering problem. The method
proposed is based on the relaxation of the initial problem to a certain linear
programming problem. Some explicit bounds are provided for the case when
exception list size is bounded above by a logarithm of dimension. The bound
provided in this case is significantly stronger than the bounds known before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02516</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02516</id><created>2015-01-11</created><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Gkatzikis</keyname><forenames>Lazaros</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Beam-searching and Transmission Scheduling in Millimeter Wave
  Communications</title><categories>cs.IT cs.PF math.IT</categories><comments>5 figures, 7 pages, accepted in ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmW) wireless networks are capable to support multi-gigabit
data rates, by using directional communications with narrow beams. However,
existing mmW communications standards are hindered by two problems: deafness
and single link scheduling. The deafness problem, that is, a misalignment
between transmitter and receiver beams, demands a time consuming beam-searching
operation, which leads to an alignment-throughput tradeoff. Moreover, the
existing mmW standards schedule a single link in each time slot and hence do
not fully exploit the potential of mmW communications, where directional
communications allow multiple concurrent transmissions. These two problems are
addressed in this paper, where a joint beamwidth selection and power allocation
problem is formulated by an optimization problem for short range mmW networks
with the objective of maximizing effective network throughput. This
optimization problem allows establishing the fundamental alignment-throughput
tradeoff, however it is computationally complex and requires exact knowledge of
network topology, which may not be available in practice. Therefore, two
standard-compliant approximation solution algorithms are developed, which rely
on underestimation and overestimation of interference. The first one exploits
directionality to maximize the reuse of available spectrum and thereby
increases the network throughput, while imposing almost no computational
complexity. The second one is a more conservative approach that protects all
active links from harmful interference, yet enhances the network throughput by
100% compared to the existing standards. Extensive performance analysis
provides useful insights on the directionality level and the number of
concurrent transmissions that should be pursued. Interestingly, extremely
narrow beams are in general not optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02524</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02524</id><created>2015-01-11</created><authors><author><keyname>Goudarzi</keyname><forenames>Hadi</forenames></author><author><keyname>Dousti</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Shafaei</keyname><forenames>Alireza</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Design of a Universal Logic Block for Fault-Tolerant Realization of any
  Logic Operation in Trapped-Ion Quantum Circuits</title><categories>quant-ph cs.ET</categories><journal-ref>Quantum Information Processing 13, no. 5 (2014): 1267-1299</journal-ref><doi>10.1007/s11128-013-0725-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a physical mapping tool for quantum circuits, which
generates the optimal Universal Logic Block (ULB) that can perform any logical
fault-tolerant (FT) quantum operations with the minimum latency. The operation
scheduling, placement, and qubit routing problems tackled by the quantum
physical mapper are highly dependent on one another. More precisely, the
scheduling solution affects the quality of the achievable placement solution
due to resource pressures that may be created as a result of operation
scheduling whereas the operation placement and qubit routing solutions
influence the scheduling solution due to resulting distances between
predecessor and current operations, which in turn determines routing latencies.
The proposed flow for the quantum physical mapper captures these dependencies
by applying (i) a loose scheduling step, which transforms an initial quantum
data flow graph into one that explicitly captures the no-cloning theorem of the
quantum computing and then performs instruction scheduling based on a modified
force-directed scheduling approach to minimize the resource contention and
quantum circuit latency, (ii) a placement step, which uses timing-driven
instruction placement to minimize the approximate routing latencies while
making iterative calls to the aforesaid force-directed scheduler to correct
scheduling levels of quantum operations as needed, and (iii) a routing step
that finds dynamic values of routing latencies for the qubits. In addition to
the quantum physical mapper, an approach is presented to determine the single
best ULB size for a target quantum circuit by examining the latency of
different FT quantum operations mapped onto different ULB sizes and using
information about the occurrence frequency of operations on critical paths of
the target quantum algorithm to weigh these latencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02527</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02527</id><created>2015-01-11</created><authors><author><keyname>Suresh</keyname><forenames>Harini</forenames></author><author><keyname>Locascio</keyname><forenames>Nicholas</forenames></author></authors><title>Autodetection and Classification of Hidden Cultural City Districts from
  Yelp Reviews</title><categories>cs.CL cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic models are a way to discover underlying themes in an otherwise
unstructured collection of documents. In this study, we specifically used the
Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to
classify restaurants based off of their reviews. Furthermore, we hypothesize
that within a city, restaurants can be grouped into similar &quot;clusters&quot; based on
both location and similarity. We used several different clustering methods,
including K-means Clustering and a Probabilistic Mixture Model, in order to
uncover and classify districts, both well-known and hidden (i.e. cultural areas
like Chinatown or hearsay like &quot;the best street for Italian restaurants&quot;)
within a city. We use these models to display and label different clusters on a
map. We also introduce a topic similarity heatmap that displays the similarity
distribution in a city to a new restaurant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02528</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02528</id><created>2015-01-11</created><authors><author><keyname>Chen</keyname><forenames>Changsheng</forenames></author><author><keyname>Mow</keyname><forenames>Wai Ho</forenames></author></authors><title>A Systematic Scheme for Measuring the Performance of the Display-Camera
  Channel</title><categories>cs.MM</categories><comments>8 pages, preliminary conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Display-camera communication has become a promising direction in both
computer vision and wireless communication communities. However, the
consistency of the channel measurement is an open issue since precise
calibration of the experimental setting has not been fully studied in the
literatures. This paper focuses on establishing a scheme for precise
calibration of the display-camera channel performance. To guarantee high
consistency of the experiment, we propose an accurate measurement scheme for
the geometric parameters, and identify some unstable channel factors, e.g.,
Moire effect, rolling shutter effect, blocking artifacts, inconsistency in
auto-focus, trembling and vibration. In the experiment, we first define the
consistency criteria according to the error-prone region in bit error rate
(BER) plots of the channel measurements. It is demonstrated that the
consistency of the experimental result can be improved by the proposed precise
calibration scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02530</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02530</id><created>2015-01-11</created><authors><author><keyname>Rohrbach</keyname><forenames>Anna</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Tandon</keyname><forenames>Niket</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>A Dataset for Movie Description</title><categories>cs.CV cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Descriptive video service (DVS) provides linguistic descriptions of movies
and allows visually impaired people to follow a movie along with their peers.
Such descriptions are by design mainly visual and thus naturally form an
interesting data source for computer vision and computational linguistics. In
this work we propose a novel dataset which contains transcribed DVS, which is
temporally aligned to full length HD movies. In addition we also collected the
aligned movie scripts which have been used in prior work and compare the two
different sources of descriptions. In total the Movie Description dataset
contains a parallel corpus of over 54,000 sentences and video snippets from 72
HD movies. We characterize the dataset by benchmarking different approaches for
generating video descriptions. Comparing DVS to scripts, we find that DVS is
far more visual and describes precisely what is shown rather than what should
happen according to the scripts created prior to movie production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02549</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02549</id><created>2015-01-12</created><authors><author><keyname>N.</keyname><forenames>Ajaykrishnan</forenames></author><author><keyname>Prem</keyname><forenames>Navya S.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Critical Database Size for Effective Caching</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replicating or caching popular content in memories distributed across the
network is a technique to reduce peak network loads. Conventionally, the
performance gain of caching was thought to result from making part of the
requested data available closer to end users. Recently, it has been shown that
by using a carefully designed technique to store the contents in the cache and
coding across data streams a much more significant gain can be achieved in
reducing the network load. Inner and outer bounds on the network load v/s cache
memory tradeoff were obtained in (Maddah-Ali and Niesen, 2012). We give an
improved outer bound on the network load v/s cache memory tradeoff. We address
the question of to what extent caching is effective in reducing the server load
when the number of files becomes large as compared to the number of users. We
show that the effectiveness of caching become small when the number of files
becomes comparable to the square of the number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02555</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02555</id><created>2015-01-12</created><updated>2015-07-21</updated><authors><author><keyname>Qin</keyname><forenames>Xiaoqian</forenames></author><author><keyname>Tan</keyname><forenames>Xiaoyang</forenames></author><author><keyname>Chen</keyname><forenames>Songcan</forenames></author></authors><title>Tri-Subject Kinship Verification: Understanding the Core of A Family</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major challenge in computer vision is to go beyond the modeling of
individual objects and to investigate the bi- (one-versus-one) or tri-
(one-versus-two) relationship among multiple visual entities, answering such
questions as whether a child in a photo belongs to given parents. The
child-parents relationship plays a core role in a family and understanding such
kin relationship would have fundamental impact on the behavior of an artificial
intelligent agent working in the human world. In this work, we tackle the
problem of one-versus-two (tri-subject) kinship verification and our
contributions are three folds: 1) a novel relative symmetric bilinear model
(RSBM) introduced to model the similarity between the child and the parents, by
incorporating the prior knowledge that a child may resemble a particular parent
more than the other; 2) a spatially voted method for feature selection, which
jointly selects the most discriminative features for the child-parents pair,
while taking local spatial information into account; 3) a large scale
tri-subject kinship database characterized by over 1,000 child-parents
families. Extensive experiments on KinFaceW, Family101 and our newly released
kinship database show that the proposed method outperforms several previous
state of the art methods, while could also be used to significantly boost the
performance of one-versus-one kinship verification when the information about
both parents are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02560</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02560</id><created>2015-01-12</created><authors><author><keyname>Maalel</keyname><forenames>Wiem</forenames><affiliation>IRISA</affiliation></author><author><keyname>Zhou</keyname><forenames>Kuang</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Elouedi</keyname><forenames>Zied</forenames></author></authors><title>Belief Hierarchical Clustering</title><categories>cs.AI cs.DB</categories><proxy>ccsd</proxy><journal-ref>3rd International Conference on Belief Functions, Sep 2014,
  Oxford, United Kingdom. pp.68 - 76</journal-ref><doi>10.1007/978-3-319-11191-9_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the data mining field many clustering methods have been proposed, yet
standard versions do not take into account uncertain databases. This paper
deals with a new approach to cluster uncertain data by using a hierarchical
clustering defined within the belief function framework. The main objective of
the belief hierarchical clustering is to allow an object to belong to one or
several clusters. To each belonging, a degree of belief is associated, and
clusters are combined based on the pignistic properties. Experiments with real
uncertain data show that our proposed method can be considered as a propitious
tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02565</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02565</id><created>2015-01-12</created><updated>2015-05-19</updated><authors><author><keyname>Revaud</keyname><forenames>Jerome</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Weinzaepfel</keyname><forenames>Philippe</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author></authors><title>EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical
  Flow</title><categories>cs.CV</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for optical flow estimation , targeted at large
displacements with significant oc-clusions. It consists of two steps: i) dense
matching by edge-preserving interpolation from a sparse set of matches; ii)
variational energy minimization initialized with the dense matches. The
sparse-to-dense interpolation relies on an appropriate choice of the distance,
namely an edge-aware geodesic distance. This distance is tailored to handle
occlusions and motion boundaries -- two common and difficult issues for optical
flow computation. We also propose an approximation scheme for the geodesic
distance to allow fast computation without loss of performance. Subsequent to
the dense interpolation step, standard one-level variational energy
minimization is carried out on the dense matches to obtain the final flow
estimation. The proposed approach, called Edge-Preserving Interpolation of
Correspondences (EpicFlow) is fast and robust to large displacements. It
significantly outperforms the state of the art on MPI-Sintel and performs on
par on Kitti and Middlebury.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02573</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02573</id><created>2015-01-12</created><updated>2015-01-16</updated><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author><author><keyname>Koenighofer</keyname><forenames>Bettina</forenames></author><author><keyname>Koenighofer</keyname><forenames>Robert</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>Shield Synthesis: Runtime Enforcement for Reactive Systems</title><categories>cs.LO</categories><comments>This is an extended version of [5], featuring an additional appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalability issues may prevent users from verifying critical properties of a
complex hardware design. In this situation, we propose to synthesize a &quot;safety
shield&quot; that is attached to the design to enforce the properties at run time.
Shield synthesis can succeed where model checking and reactive synthesis fail,
because it only considers a small set of critical properties, as opposed to the
complex design, or the complete specification in the case of reactive
synthesis. The shield continuously monitors the input/output of the design and
corrects its erroneous output only if necessary, and as little as possible, so
other non-critical properties are likely to be retained. Although runtime
enforcement has been studied in other domains such as action systems, reactive
systems pose unique challenges where the shield must act without delay. We thus
present the first shield synthesis solution for reactive hardware systems and
report our experimental results. This is an extended version of [5], featuring
an additional appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02581</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02581</id><created>2015-01-12</created><updated>2016-01-14</updated><authors><author><keyname>Shabouei</keyname><forenames>M.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>Mechanics-based solution verification for porous media models</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach to verify accuracy of computational
simulations. We develop mathematical theorems which can serve as robust a
posteriori error estimation techniques to identify numerical pollution, check
the performance of adaptive meshes, and verify numerical solutions. We
demonstrate performance of this methodology on problems from flow thorough
porous media. However, one can extend it to other models. We construct
mathematical properties such that the solutions to Darcy and Darcy-Brinkman
equations satisfy them. The mathematical properties include the total minimum
mechanical power, minimum dissipation theorem, reciprocal relation, and maximum
principle for the vorticity. All the developed theorems have firm mechanical
bases and are independent of numerical methods. So, these can be utilized for
solution verification of finite element, finite volume, finite difference,
lattice Boltzmann methods and so forth. In particular, we show that, for a
given set of boundary conditions, Darcy velocity has the minimum total
mechanical power of all the kinematically admissible vector fields. We also
show that a similar result holds for Darcy-Brinkman velocity. We then show for
a conservative body force, the Darcy and Darcy-Brinkman velocities have the
minimum total dissipation among their respective kinematically admissible
vector fields. Using numerical examples, we show that the minimum dissipation
and total mechanical power theorems can be utilized to identify pollution
errors in numerical solutions. The solutions to Darcy and Darcy-Brinkman
equations are shown to satisfy a reciprocal relation, which has the potential
to identify errors in the numerical implementation of boundary conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02588</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02588</id><created>2015-01-12</created><authors><author><keyname>Cai</keyname><forenames>Ning</forenames></author><author><keyname>Diao</keyname><forenames>Chen</forenames></author><author><keyname>Khan</keyname><forenames>M. Junaid</forenames></author></authors><title>A Novel Clustering Approach Based on Group Quasi-Consensus of Unstable
  Dynamic Linear High-Order Multi-Agent Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel approach of clustering, which is based on group
consensus of dynamic linear high-order multi-agent systems. The graph topology
is associated with a selected multi-agent system, with each agent corresponding
to one vertex. In order to reveal the cluster structure, the agents belonging
to a similar cluster are expected to aggregate together. As theoretical
foundation, a necessary and sufficient condition is given to check the group
consensus. Two numerical instances are shown to illustrate the process of
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02592</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02592</id><created>2015-01-12</created><authors><author><keyname>Hermans</keyname><forenames>Michiel</forenames></author><author><keyname>Soriano</keyname><forenames>Miguel</forenames></author><author><keyname>Dambre</keyname><forenames>Joni</forenames></author><author><keyname>Bienstman</keyname><forenames>Peter</forenames></author><author><keyname>Fischer</keyname><forenames>Ingo</forenames></author></authors><title>Photonic Delay Systems as Machine Learning Implementations</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear photonic delay systems present interesting implementation platforms
for machine learning models. They can be extremely fast, offer great degrees of
parallelism and potentially consume far less power than digital processors. So
far they have been successfully employed for signal processing using the
Reservoir Computing paradigm. In this paper we show that their range of
applicability can be greatly extended if we use gradient descent with
backpropagation through time on a model of the system to optimize the input
encoding of such systems. We perform physical experiments that demonstrate that
the obtained input encodings work well in reality, and we show that optimized
systems perform significantly better than the common Reservoir Computing
approach. The results presented here demonstrate that common gradient descent
techniques from machine learning may well be applicable on physical
neuro-inspired analog computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02594</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02594</id><created>2015-01-12</created><updated>2015-08-21</updated><authors><author><keyname>Shim</keyname><forenames>Taehyoung</forenames></author><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Ko</keyname><forenames>Seung-Woo</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author><author><keyname>Lee</keyname><forenames>Beom Hee</forenames></author><author><keyname>Choi</keyname><forenames>Jin Gu</forenames></author></authors><title>Traffic Convexity Aware Cellular Networks: A Vehicular Heavy User
  Perspective</title><categories>cs.NI cs.IT math.IT</categories><comments>15 pages, 5 figures, 1 table, to appear in IEEE Wireless
  Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rampant mobile traffic increase in modern cellular networks is mostly caused
by large-sized multimedia contents. Recent advancements in smart devices as
well as radio access technologies promote the consumption of bulky content,
even for people in moving vehicles, referred to as vehicular heavy users. In
this article the emergence of vehicular heavy user traffic is observed by field
experiments conducted in 2012 and 2015 in Seoul, Korea. The experiments reveal
that such traffic is becoming dominant, captured by the 8.62 times increase in
vehicular heavy user traffic while the total traffic increased 3.04 times. To
resolve this so-called vehicular heavy user problem (VHP), we propose a cell
association algorithm that exploits user demand diversity for different
velocities. This user traffic pattern is discovered first by our field trials,
which is convex-shaped over velocity, i.e. walking user traffic is less than
stationary or vehicular user traffic. As the VHP becomes severe, numerical
evaluation verifies the proposed user convexity aware association outperforms a
well-known load balancing association in practice, cell range expansion (CRE).
In addition to the cell association, several complementary techniques are
suggested in line with the technical trend toward 5G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02598</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02598</id><created>2015-01-12</created><updated>2015-03-12</updated><authors><author><keyname>Lazaridou</keyname><forenames>Angeliki</forenames></author><author><keyname>Pham</keyname><forenames>Nghia The</forenames></author><author><keyname>Baroni</keyname><forenames>Marco</forenames></author></authors><title>Combining Language and Vision with a Multimodal Skip-gram Model</title><categories>cs.CL cs.CV cs.LG</categories><comments>accepted at NAACL 2015, camera ready version, 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual
information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)
build vector-based word representations by learning to predict linguistic
contexts in text corpora. However, for a restricted set of words, the models
are also exposed to visual representations of the objects they denote
(extracted from natural images), and must predict linguistic and visual
features jointly. The MMSKIP-GRAM models achieve good performance on a variety
of semantic benchmarks. Moreover, since they propagate visual information to
all words, we use them to improve image labeling and retrieval in the zero-shot
setup, where the test concepts are never seen during model training. Finally,
the MMSKIP-GRAM models discover intriguing visual properties of abstract words,
paving the way to realistic implementations of embodied theories of meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02601</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02601</id><created>2015-01-12</created><authors><author><keyname>Toorani</keyname><forenames>Mohsen</forenames></author></authors><title>On Vulnerabilities of the Security Association in the IEEE 802.15.6
  Standard</title><categories>cs.CR</categories><msc-class>94A60</msc-class><acm-class>E.3; K.6.5; D.4.6; K.6.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Body Area Networks (WBAN) support a variety of real-time health
monitoring and consumer electronics applications. The latest international
standard for WBAN is the IEEE 802.15.6. The security association in this
standard includes four elliptic curve-based key agreement protocols that are
used for generating a master key. In this paper, we challenge the security of
the IEEE 802.15.6 standard by showing vulnerabilities of those four protocols
to several attacks. We perform a security analysis on the protocols, and show
that they all have security problems, and are vulnerable to different attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02607</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02607</id><created>2015-01-12</created><updated>2015-06-26</updated><authors><author><keyname>Carreiro</keyname><forenames>Facundo</forenames></author></authors><title>Characterization theorems for PDL and FO(TC)</title><categories>cs.LO</categories><comments>Technical Report, 70 pages. arXiv admin note: text overlap with
  arXiv:1401.4374</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our main contributions can be divided in three parts: (1) Fixpoint extensions
of first-order logic: we give a precise syntactic and semantic characterization
of the relationship between $\mathrm{FO(TC^1)}$ and $\mathrm{FO(LFP)}$; (2)
Automata and expressiveness on trees: we introduce a new class of parity
automata which, on trees, captures the expressive power of $\mathrm{FO(TC^1)}$
and WCL (weak chain logic). The latter logic is a variant of MSO which
quantifies over finite chains; and (3) Expressiveness modulo bisimilarity: we
show that PDL is expressively equivalent to the bisimulation-invariant fragment
of both $\mathrm{FO(TC^1)}$ and WCL. In particular, point (3) closes the open
problems of the bisimulation-invariant characterizations of PDL,
$\mathrm{FO(TC^1)}$ and WCL all at once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02620</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02620</id><created>2015-01-12</created><updated>2015-05-19</updated><authors><author><keyname>Mao</keyname><forenames>Yuyi</forenames></author><author><keyname>Luo</keyname><forenames>Yaming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Energy Harvesting Small Cell Networks: Feasibility, Deployment and
  Operation</title><categories>cs.IT math.IT</categories><comments>19 pages, 5 figures, to appear in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cell networks (SCNs) have attracted great attention in recent years due
to their potential to meet the exponential growth of mobile data traffic and
the increasing demand for better quality of service and user experience in
mobile applications. Nevertheless, a wide deployment of SCNs has not happened
yet because of the complexity in the network planning and optimization, as well
as the high expenditure involved in deployment and operation. In particular, it
is difficult to provide grid power supply to all the small cell base stations
(SCBSs) in a cost effective way. Moreover, a dense deployment of SCBSs, which
is needed to meet the capacity and coverage of the next generation wireless
networks, will increase operators' electricity bills and lead to significant
carbon emission. Thus, it is crucial to exploit off-grid and green energy
sources to power SCNs, for which energy harvesting (EH) technology is a viable
solution. In this article, we will conduct a comprehensive study of EH-SCNs,
and investigate important aspects, including the feasibility analysis, network
deployment, and network operation issues. The advantages, as well as unique
challenges, of EH-SCNs will be highlighted, together with potential solutions
and effective design methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02623</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02623</id><created>2015-01-12</created><updated>2015-01-13</updated><authors><author><keyname>Bizjak</keyname><forenames>Ale&#x161;</forenames><affiliation>Aarhus University, Denmark</affiliation></author><author><keyname>Birkedal</keyname><forenames>Lars</forenames><affiliation>Aarhus University, Denmark</affiliation></author></authors><title>Step-Indexed Logical Relations for Probability (long version)</title><categories>cs.LO</categories><comments>Extended version with appendix of a FoSSaCS'15 paper</comments><doi>10.1007/978-3-662-46678-0_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that constructing models of higher-order probabilistic
programming languages is challenging. We show how to construct step-indexed
logical relations for a probabilistic extension of a higher-order programming
language with impredicative polymorphism and recursive types. We show that the
resulting logical relation is sound and complete with respect to the contextual
preorder and, moreover, that it is convenient for reasoning about concrete
program equivalences. Finally, we extend the language with dynamically
allocated first-order references and show how to extend the logical relation to
this language. We show that the resulting relation remains useful for reasoning
about examples involving both state and probabilistic choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02627</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02627</id><created>2015-01-12</created><updated>2015-09-01</updated><authors><author><keyname>Serang</keyname><forenames>Oliver</forenames></author></authors><title>A fast numerical method for max-convolution and the application to
  efficient max-product inference in Bayesian networks</title><categories>cs.NA math.NA stat.CO stat.ME stat.ML</categories><journal-ref>Journal of Computational Biology. August 2015, 22(8): 770-783</journal-ref><doi>10.1089/cmb.2015.0013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observations depending on sums of random variables are common throughout many
fields; however, no efficient solution is currently known for performing
max-product inference on these sums of general discrete distributions
(max-product inference can be used to obtain maximum a posteriori estimates).
The limiting step to max-product inference is the max-convolution problem
(sometimes presented in log-transformed form and denoted as &quot;infimal
convolution&quot;, &quot;min-convolution&quot;, or &quot;convolution on the tropical semiring&quot;),
for which no O(k log(k)) method is currently known. Here I present a O(k
log(k)) numerical method for estimating the max-convolution of two nonnegative
vectors (e.g., two probability mass functions), where k is the length of the
larger vector. This numerical max-convolution method is then demonstrated by
performing fast max-product inference on a convolution tree, a data structure
for performing fast inference given information on the sum of n discrete random
variables in O(n k log(n k) log(n) ) steps (where each random variable has an
arbitrary prior distribution on k contiguous possible states). The numerical
max-convolution method can be applied to specialized classes of hidden Markov
models to reduce the runtime of computing the Viterbi path from n k^2 to n k
log(k), and has potential application to the all-pairs shortest paths problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02629</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02629</id><created>2015-01-12</created><updated>2015-10-21</updated><authors><author><keyname>Cl&#xe9;men&#xe7;on</keyname><forenames>St&#xe9;phan</forenames></author><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Colin</keyname><forenames>Igor</forenames></author></authors><title>Scaling-up Empirical Risk Minimization: Optimization of Incomplete
  U-statistics</title><categories>stat.ML cs.LG</categories><comments>30 pages. v2: minor correction to Theorem 4 and its proof, added 1
  reference. v3: typo corrected in Proposition 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wide range of statistical learning problems such as ranking, clustering
or metric learning among others, the risk is accurately estimated by
$U$-statistics of degree $d\geq 1$, i.e. functionals of the training data with
low variance that take the form of averages over $k$-tuples. From a
computational perspective, the calculation of such statistics is highly
expensive even for a moderate sample size $n$, as it requires averaging
$O(n^d)$ terms. This makes learning procedures relying on the optimization of
such data functionals hardly feasible in practice. It is the major goal of this
paper to show that, strikingly, such empirical risks can be replaced by
drastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms
only, usually referred to as incomplete $U$-statistics, without damaging the
$O_{\mathbb{P}}(1/\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)
procedures. For this purpose, we establish uniform deviation results describing
the error made when approximating a $U$-process by its incomplete version under
appropriate complexity assumptions. Extensions to model selection, fast rate
situations and various sampling techniques are also considered, as well as an
application to stochastic gradient descent for ERM. Finally, numerical examples
are displayed in order to provide strong empirical evidence that the approach
we promote largely surpasses more naive subsampling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02633</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02633</id><created>2015-01-12</created><authors><author><keyname>van Delft</keyname><forenames>Bart</forenames></author><author><keyname>Hunt</keyname><forenames>Sebastian</forenames></author><author><keyname>Sands</keyname><forenames>David</forenames></author></authors><title>Very Static Enforcement of Dynamic Policies</title><categories>cs.CR cs.PL</categories><comments>Technical Report of publication under the same name in Principles of
  Security and Trust (POST) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security policies are naturally dynamic. Reflecting this, there has been a
growing interest in studying information-flow properties which change during
program execution, including concepts such as declassification, revocation, and
role-change.
  A static verification of a dynamic information flow policy, from a semantic
perspective, should only need to concern itself with two things: 1) the
dependencies between data in a program, and 2) whether those dependencies are
consistent with the intended flow policies as they change over time. In this
paper we provide a formal ground for this intuition. We present a
straightforward extension to the principal flow-sensitive type system
introduced by Hunt and Sands (POPL '06, ESOP '11) to infer both end-to-end
dependencies and dependencies at intermediate points in a program. This allows
typings to be applied to verification of both static and dynamic policies. Our
extension preserves the principal type system's distinguishing feature, that
type inference is independent of the policy to be enforced: a single, generic
dependency analysis (typing) can be used to verify many different dynamic
policies of a given program, thus achieving a clean separation between (1) and
(2).
  We also make contributions to the foundations of dynamic information flow.
Arguably, the most compelling semantic definitions for dynamic security
conditions in the literature are phrased in the so-called knowledge-based
style. We contribute a new definition of knowledge-based termination
insensitive security for dynamic policies. We show that the new definition
avoids anomalies of previous definitions and enjoys a simple and useful
characterisation as a two-run style property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02646</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02646</id><created>2015-01-12</created><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Frommholz</keyname><forenames>Ingo</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author></authors><title>Bibliometric-enhanced Information Retrieval: 2nd International BIR
  Workshop</title><categories>cs.IR cs.DL</categories><comments>4 pages, 37th European Conference on Information Retrieval, BIR
  workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This workshop brings together experts of communities which often have been
perceived as different once: bibliometrics / scientometrics / informetrics on
the one side and information retrieval on the other. Our motivation as
organizers of the workshop started from the observation that main discourses in
both fields are different, that communities are only partly overlapping and
from the belief that a knowledge transfer would be profitable for both sides.
Bibliometric techniques are not yet widely used to enhance retrieval processes
in digital libraries, although they offer value-added effects for users. On the
other side, more and more information professionals, working in libraries and
archives are confronted with applying bibliometric techniques in their
services. This way knowledge exchange becomes more urgent. The first workshop
set the research agenda, by introducing in each other methods, reporting about
current research problems and brainstorming about common interests. This
follow-up workshop continues the overall communication, but also puts one
problem into the focus. In particular, we will explore how statistical
modelling of scholarship can improve retrieval services for specific
communities, as well as for large, cross-domain collections like Mendeley or
ResearchGate. This second BIR workshop continues to raise awareness of the
missing link between Information Retrieval (IR) and bibliometrics and
contributes to create a common ground for the incorporation of
bibliometric-enhanced services into retrieval at the scholarly search engine
interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02652</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02652</id><created>2015-01-12</created><authors><author><keyname>Roussakis</keyname><forenames>Yannis</forenames></author><author><keyname>Chrysakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Stefanidis</keyname><forenames>Kostas</forenames></author><author><keyname>Flouris</keyname><forenames>Giorgos</forenames></author><author><keyname>Stavrakas</keyname><forenames>Yannis</forenames></author></authors><title>A Flexible Framework for Defining, Representing and Detecting Changes on
  the Data Web</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic nature of Web data gives rise to a multitude of problems related
to the identification, computation and management of the evolving versions and
the related changes. In this paper, we consider the problem of change
recognition in RDF datasets, i.e., the problem of identifying, and when
possible give semantics to, the changes that led from one version of an RDF
dataset to another. Despite our RDF focus, our approach is sufficiently general
to engulf different data models that can be encoded in RDF, such as relational
or multi-dimensional. In fact, we propose a flexible, extendible and
data-model-independent methodology of defining changes that can capture the
peculiarities and needs of different data models and applications, while being
formally robust due to the satisfaction of the properties of completeness and
unambiguity. Further, we propose an ontology of changes for storing the
detected changes that allows automated processing and analysis of changes,
cross-snapshot queries (spanning across different versions), as well as queries
involving both changes and data. To detect changes and populate said ontology,
we propose a customizable detection algorithm, which is applicable to different
data models and applications requiring the detection of custom, user-defined
changes. Finally, we provide a proof-of-concept application and evaluation of
our framework for different data models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02655</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02655</id><created>2015-01-12</created><updated>2015-06-01</updated><authors><author><keyname>Sagel</keyname><forenames>Alexander</forenames></author><author><keyname>Meyer</keyname><forenames>Dominik</forenames></author><author><keyname>Shen</keyname><forenames>Hao</forenames></author></authors><title>Texture Retrieval via the Scattering Transform</title><categories>cs.IR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the problem of content-based image retrieval, specifically,
texture retrieval. It focuses on feature extraction and similarity measure for
texture images. Our approach employs a recently developed method, the so-called
Scattering transform, for the process of feature extraction in texture
retrieval. It shares a distinctive property of providing a robust
representation, which is stable with respect to spatial deformations. Recent
work has demonstrated its capability for texture classification, and hence as a
promising candidate for the problem of texture retrieval.
  Moreover, we adopt a common approach of measuring the similarity of textures
by comparing the subband histograms of a filterbank transform. To this end we
derive a similarity measure based on the popular Bhattacharyya Kernel. Despite
the popularity of describing histograms using parametrized probability density
functions, such as the Generalized Gaussian Distribution, it is unfortunately
not applicable for describing most of the Scattering transform subbands, due to
the complex modulus performed on each one of them. In this work, we propose to
use the Weibull distribution to model the Scattering subbands of descendant
layers.
  Our numerical experiments demonstrated the effectiveness of the proposed
approach, in comparison with several state of the arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02659</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02659</id><created>2015-01-12</created><authors><author><keyname>Chatzidimitris</keyname><forenames>Thomas</forenames></author><author><keyname>Gavalas</keyname><forenames>Damianos</forenames></author><author><keyname>Kasapakis</keyname><forenames>Vlasios</forenames></author></authors><title>PacMap: Transferring PacMan to the Physical Realm</title><categories>cs.MM</categories><comments>6 pages, 3 figures, Proceedings of the International Conference on
  Pervasive Games (PERGAMES'2014), Rome, Italy, 27 October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the implementation of the pervasive game PacMap.
Openness and portability have been the main design objectives for PacMap. We
elaborate on programming techniques which may be applicable to a broad range of
location-based games that involve the movement of virtual characters over map
interfaces. In particular, we present techniques to execute shortest path
algorithms on spatial environments bypassing the restrictions imposed by
commercial mapping services. Last, we present ways to improve the movement and
enhance the intelligence of virtual characters taking into consideration the
actions and position of players in location-based games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02661</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02661</id><created>2015-01-12</created><authors><author><keyname>Kasapakis</keyname><forenames>Vlasios</forenames></author><author><keyname>Gavalas</keyname><forenames>Damianos</forenames></author><author><keyname>Chatzidimitris</keyname><forenames>Thomas</forenames></author></authors><title>Evaluation of Pervasive Games: Recruitment of Qualified Participants
  through Preparatory Game Phases</title><categories>cs.HC</categories><comments>7 pages, 4 Tables, Proceedings of the International Conference on
  Pervasive Games (PERGAMES'2014), Rome, Italy, 27 October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the evaluation process for Barbarossa, a pervasive
role playing game. Barbarossa involves an invitational (preparatory) and a main
execution phase. The former is freely available though Google Play store and
may be played anytime/ anywhere. The latter defines three inter-dependent
player roles acted by players who need to collaborate in a treasure hunting
game. The eligibility of players for participating in the main game phase is
restricted among those ranked relatively high in the invitational phase.
Herein, we investigate the impact of the invitational game mode on the players
overall game experience. The main hypothesis tested is that game awareness
(gained from participating in a preliminary game phase) may serve as a means
for recruiting the most suitable subjects for user trials on pervasive game
research prototypes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02662</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02662</id><created>2015-01-12</created><updated>2015-01-12</updated><authors><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Filho</keyname><forenames>Henrique Parra Parra</forenames></author><author><keyname>de Luna</keyname><forenames>Rodrigo Bandeira</forenames></author><author><keyname>Martins</keyname><forenames>Ricardo Augusto Poppi</forenames></author><author><keyname>Amanqui</keyname><forenames>Flor Karina Mamani</forenames></author><author><keyname>Moreira</keyname><forenames>Dilvan de Abreu</forenames></author></authors><title>Social Participation Ontology: community documentation, enhancements and
  use examples</title><categories>cs.CY cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Participatory democracy advances in virtually all governments. South America
presents a prominent context with mixed culture and social predisposition. In
2012, civil, academic and governmental parties started elaborating the &quot;Common
Vocabulary of Social Participation&quot; (VCPS from the Brazilian name Vocabul\'ario
Comum de Participa\c{c}\~ao Social), as a public and online process. By May,
2013, first reference documents were publicized, together with a preliminary
OWL code, logos, and a diagram of a general &quot;public consultation&quot;. The Corais
platform kept online records of the process, like discussions and preparation
of texts. This article exposes this material and proposes an elementary
unfolding: the &quot;Social Participation Ontology&quot; (OPS from the Brazilian name
Ontologia de Participa\c{c}\~ao Social). To exhibit this new ontology, these
steps were considered: correction of ontological contradictions and OWL
protocol use errors; completion of VCPS OWL code into a preliminary version of
the OPS; translations and standardizations; enhancements of class names and
labels in Portuguese, Spanish and English; a toy expansion of the ontology by
further specifying classes; linked data examples regarding dereferencing, a
SparQL endpoint and participatory instances; use cases by researchers and
public managers. Ongoing work involves further adoption of OPS by the official
Brazilian federal portal of social participation, further adoption by civil
participatory organizations, and linkage to other participatory ontologies. OPS
is being used as an upper ontology, and all classes linked further to FOAF and
BFO as higher upper ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02670</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02670</id><created>2015-01-12</created><authors><author><keyname>Gyllensten</keyname><forenames>Amaru Cuba</forenames></author><author><keyname>Sahlgren</keyname><forenames>Magnus</forenames></author></authors><title>Navigating the Semantic Horizon using Relative Neighborhood Graphs</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with nearest neighbor search in distributional
semantic models. A normal nearest neighbor search only returns a ranked list of
neighbors, with no information about the structure or topology of the local
neighborhood. This is a potentially serious shortcoming of the mode of querying
a distributional semantic model, since a ranked list of neighbors may conflate
several different senses. We argue that the topology of neighborhoods in
semantic space provides important information about the different senses of
terms, and that such topological structures can be used for word-sense
induction. We also argue that the topology of the neighborhoods in semantic
space can be used to determine the semantic horizon of a point, which we define
as the set of neighbors that have a direct connection to the point. We
introduce relative neighborhood graphs as method to uncover the topological
properties of neighborhoods in semantic models. We also provide examples of
relative neighborhood graphs for three well-known semantic models; the PMI
model, the GloVe model, and the skipgram model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02680</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02680</id><created>2015-01-12</created><updated>2015-07-27</updated><authors><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>McKay</keyname><forenames>Brendan D.</forenames></author></authors><title>Recursive generation of IPR fullerenes</title><categories>math.CO cs.DM</categories><comments>19 pages; to appear in Journal of Mathematical Chemistry</comments><doi>10.1007/s10910-015-0513-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new construction algorithm for the recursive generation of all
non-isomorphic IPR fullerenes. Unlike previous algorithms, the new algorithm
stays entirely within the class of IPR fullerenes, that is: every IPR fullerene
is constructed by expanding a smaller IPR fullerene unless it belongs to
limited class of irreducible IPR fullerenes that can easily be made separately.
The class of irreducible IPR fullerenes consists of 36 fullerenes with up to
112 vertices and 4 infinite families of nanotube fullerenes. Our implementation
of this algorithm is faster than other generators for IPR fullerenes and we
used it to compute all IPR fullerenes up to 400 vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02683</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02683</id><created>2015-01-12</created><authors><author><keyname>Bouajjani</keyname><forenames>Ahmed</forenames></author><author><keyname>Calin</keyname><forenames>Georgel</forenames></author><author><keyname>Derevenetc</keyname><forenames>Egor</forenames></author><author><keyname>Meyer</keyname><forenames>Roland</forenames></author></authors><title>Lazy TSO Reachability</title><categories>cs.PL</categories><comments>accepted to FASE 2015</comments><msc-class>68Q60</msc-class><acm-class>D.2.4; D.1.3; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of checking state reachability for programs running
under Total Store Order (TSO). The problem has been shown to be decidable but
the cost is prohibitive, namely non-primitive recursive. We propose here to
give up completeness. Our contribution is a new algorithm for TSO reachability:
it uses the standard SC semantics and introduces the TSO semantics lazily and
only where needed. At the heart of our algorithm is an iterative refinement of
the program of interest. If the program's goal state is SC-reachable, we are
done. If the goal state is not SC-reachable, this may be due to the fact that
SC under-approximates TSO. We employ a second algorithm that determines TSO
computations which are infeasible under SC, and hence likely to lead to new
states. We enrich the program to emulate, under SC, these TSO computations.
Altogether, this yields an iterative under-approximation that we prove sound
and complete for bug hunting, i.e., a semi-decision procedure halting for
positive cases of reachability. We have implemented the procedure as an
extension to the tool Trencher and compared it to the Memorax and CBMC model
checkers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02686</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02686</id><created>2015-01-12</created><authors><author><keyname>Chroni</keyname><forenames>Maria</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Stavros D.</forenames></author></authors><title>Watermarking PDF Documents using Various Representations of
  Self-inverting Permutations</title><categories>cs.MM</categories><comments>17 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1003.1796 by other authors</comments><acm-class>D.2.2; D.4.6; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides to web users copyright protection of their Portable
Document Format (PDF) documents by proposing efficient and easily implementable
techniques for PDF watermarking; our techniques are based on the ideas of our
recently proposed watermarking techniques for software, image, and audio,
expanding thus the digital objects that can be efficiently watermarked through
the use of self-inverting permutations. In particular, we present various
representations of a self-inverting permutation $\pi^*$ namely
1D-representation, 2D-representation, and RPG-representation, and show that
theses representations can be efficiently applied to PDF watermarking. Indeed,
we first present an audio-based technique for marking a PDF document $T$ by
exploiting the 1D-representation of a permutation $\pi^*$, and then, since
pages of a PDF document $T$ are 2D objects, we present an image-based algorithm
for encoding $\pi^*$ into $T$ by first mapping the elements of $\pi^*$ into a
matrix $A^*$ and then using the information stored in $A^*$ to mark invisibly
specific areas of PDF document $T$. Finally, we describe a graph-based
watermarking algorithm for embedding a self-inverting permutation $\pi^*$ into
the document structure of a PDF file $T$ by exploiting the RPG-representation
of $\pi^*$ and the structure of a PDF document. We have evaluated the embedding
and extracting algorithms by testing them on various and different in
characteristics PDF documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02699</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02699</id><created>2015-01-12</created><authors><author><keyname>Engelmann</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Olderog</keyname><forenames>Ernst-R&#xfc;diger</forenames></author><author><keyname>Flick</keyname><forenames>Nils Erik</forenames></author></authors><title>Closing the Gap -- Formally Verifying Dynamically Typed Programs like
  Statically Typed Ones Using Hoare Logic -- Extended Version --</title><categories>cs.PL cs.LO cs.SE</categories><comments>includes all appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamically typed object-oriented languages enable programmers to write
elegant, reusable and extensible programs. However, with the current
methodology for program verification, the absence of static type information
creates significant overhead. Our proposal is two-fold:
  First, we propose a layer of abstraction hiding the complexity of dynamic
typing when provided with sufficient type information. Since this essentially
creates the illusion of verifying a statically-typed program, the effort
required is equivalent to the statically-typed case.
  Second, we show how the required type information can be efficiently derived
for all type-safe programs by integrating a type inference algorithm into Hoare
logic, yielding a semi-automatic procedure allowing the user to focus on those
typing problems really requiring his attention. While applying type inference
to dynamically typed programs is a well-established method by now, our approach
complements conventional soft typing systems by offering formal proof as a
third option besides modifying the program (static typing) and accepting the
presence of runtime type errors (dynamic typing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02702</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02702</id><created>2015-01-12</created><authors><author><keyname>Nan</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Joseph</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Max-Cost Discrete Function Evaluation Problem under a Budget</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose novel methods for max-cost Discrete Function Evaluation Problem
(DFEP) under budget constraints. We are motivated by applications such as
clinical diagnosis where a patient is subjected to a sequence of (possibly
expensive) tests before a decision is made. Our goal is to develop strategies
for minimizing max-costs. The problem is known to be NP hard and greedy methods
based on specialized impurity functions have been proposed. We develop a broad
class of \emph{admissible} impurity functions that admit monomials, classes of
polynomials, and hinge-loss functions that allow for flexible impurity design
with provably optimal approximation bounds. This flexibility is important for
datasets when max-cost can be overly sensitive to &quot;outliers.&quot; Outliers bias
max-cost to a few examples that require a large number of tests for
classification. We design admissible functions that allow for accuracy-cost
trade-off and result in $O(\log n)$ guarantees of the optimal cost among trees
with corresponding classification accuracy levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02713</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02713</id><created>2015-01-12</created><authors><author><keyname>Casbeer</keyname><forenames>David W.</forenames></author><author><keyname>Cao</keyname><forenames>Yongcan</forenames></author><author><keyname>Garcia</keyname><forenames>Eloy</forenames></author><author><keyname>Milutinovic</keyname><forenames>Dejan</forenames></author></authors><title>Bridge Consensus: Ignoring Initial Inessentials</title><categories>cs.SY cs.MA</categories><comments>Submitted to 2015 American Control Conference</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, the problem of bridge consensus is presented and solved.
Bridge consensus consists of a network of nodes, some of whom are participating
and others are non-participating. The objective is for all the agents to reach
average consensus of the participating nodes initial values in a distributed
and scalable manner. To do this, the nodes must use the network connections of
the non-participating nodes, which act as bridges for information and ignore
the initial values of the non-participating nodes. The solution to this problem
is made by merging the ideas from estimation theory and consensus theory. By
considering the participating nodes has having equal information and the
non-participating nodes as having no information, the nodes initial values are
transformed into information space. Two consensus filters are run in parallel
on the information state and information matrix. Conditions ensuring that the
product of the inverse information matrix and the information state of each
agent reaches average consensus of the participating agents' initial values is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02714</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02714</id><created>2015-01-12</created><updated>2015-03-24</updated><authors><author><keyname>Lazaridou</keyname><forenames>Angeliki</forenames></author><author><keyname>Dinu</keyname><forenames>Georgiana</forenames></author><author><keyname>Liska</keyname><forenames>Adam</forenames></author><author><keyname>Baroni</keyname><forenames>Marco</forenames></author></authors><title>From Visual Attributes to Adjectives through Decompositional
  Distributional Semantics</title><categories>cs.CL cs.CV</categories><comments>accepted at Transactions of the Association for Computational
  Linguistics (TACL), 3/2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As automated image analysis progresses, there is increasing interest in
richer linguistic annotation of pictures, with attributes of objects (e.g.,
furry, brown...) attracting most attention. By building on the recent
&quot;zero-shot learning&quot; approach, and paying attention to the linguistic nature of
attributes as noun modifiers, and specifically adjectives, we show that it is
possible to tag images with attribute-denoting adjectives even when no training
data containing the relevant annotation are available. Our approach relies on
two key observations. First, objects can be seen as bundles of attributes,
typically expressed as adjectival modifiers (a dog is something furry, brown,
etc.), and thus a function trained to map visual representations of objects to
nominal labels can implicitly learn to map attributes to adjectives. Second,
objects and attributes come together in pictures (the same thing is a dog and
it is brown). We can thus achieve better attribute (and object) label retrieval
by treating images as &quot;visual phrases&quot;, and decomposing their linguistic
representation into an attribute-denoting adjective and an object-denoting
noun. Our approach performs comparably to a method exploiting manual attribute
annotation, it outperforms various competitive alternatives in both attribute
and object annotation, and it automatically constructs attribute-centric
representations that significantly improve performance in supervised object
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02716</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02716</id><created>2015-01-12</created><authors><author><keyname>Biely</keyname><forenames>Martin</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author><author><keyname>Schwarz</keyname><forenames>Manfred</forenames></author><author><keyname>Winkler</keyname><forenames>Kyrill</forenames></author></authors><title>Gracefully Degrading Consensus and $k$-Set Agreement in Directed Dynamic
  Networks</title><categories>cs.DC cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1204.0641</comments><msc-class>68W15</msc-class><acm-class>C.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed agreement in synchronous directed dynamic networks,
where an omniscient message adversary controls the availability of
communication links. We prove that consensus is impossible under a message
adversary that guarantees weak connectivity only, and introduce vertex-stable
root components (VSRCs) as a means for circumventing this impossibility: A
VSRC(k, d) message adversary guarantees that, eventually, there is an interval
of $d$ consecutive rounds where every communication graph contains at most $k$
strongly (dynamic) connected components consisting of the same processes, which
have at most outgoing links to the remaining processes. We present a consensus
algorithm that works correctly under a VSRC(1, 4H + 2) message adversary, where
$H$ is the dynamic causal network diameter. On the other hand, we show that
consensus is impossible against a VSRC(1, H - 1) or a VSRC(2, $\infty$) message
adversary, revealing that there is not much hope to deal with stronger message
adversaries.
  However, we show that gracefully degrading consensus, which degrades to
general $k$-set agreement in case of unfavourable network conditions, is
feasible against stronger message adversaries: We provide a $k$-uniform $k$-set
agreement algorithm, where the number of system-wide decision values $k$ is not
encoded in the algorithm, but rather determined by the actual power of the
message adversary in a run: Our algorithm guarantees at most $k$ decision
values under a VSRC(n, d) + MAJINF(k) message adversary, which combines VSRC(n,
d) (for some small $d$, ensuring termination) with some information flow
guarantee MAJINF(k) between certain VSRCs (ensuring $k$-agreement). Our results
provide a significant step towards the exact solvability/impossibility border
of general $k$-set agreement in directed dynamic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02724</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02724</id><created>2015-01-12</created><authors><author><keyname>Subramaniam</keyname><forenames>Balaji</forenames></author><author><keyname>Feng</keyname><forenames>Wu-chun</forenames></author></authors><title>Towards Energy-Proportional Computing Using Subsystem-Level Power
  Management</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive data centers housing thousands of computing nodes have become
commonplace in enterprise computing, and the power consumption of such data
centers is growing at an unprecedented rate. Adding to the problem is the
inability of the servers to exhibit energy proportionality, i.e., provide
energy-efficient execution under all levels of utilization, which diminishes
the overall energy efficiency of the data center. It is imperative that we
realize effective strategies to control the power consumption of the server and
improve the energy efficiency of data centers. With the advent of Intel Sandy
Bridge processors, we have the ability to specify a limit on power consumption
during runtime, which creates opportunities to design new power-management
techniques for enterprise workloads and make the systems that they run on more
energy-proportional.
  In this paper, we investigate whether it is possible to achieve energy
proportionality for enterprise-class server workloads, namely SPECpower_ssj2008
and SPECweb2009 benchmarks, by using Intel's Running Average Power Limit (RAPL)
interfaces. First, we analyze the average power consumption of the full system
as well as the subsystems and describe the energy proportionality of these
components. We then characterize the instantaneous power profile of these
benchmarks within different subsystems using the on-chip energy meters exposed
via the RAPL interfaces. Finally, we present the effects of power limiting on
the energy proportionality, performance, power and energy efficiency of
enterprise-class server workloads. Our observations and results shed light on
the efficacy of the RAPL interfaces and provide guidance for designing
power-management techniques for enterprise-class workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02728</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02728</id><created>2015-01-12</created><updated>2015-10-19</updated><authors><author><keyname>de Arruda</keyname><forenames>Henrique Ferraz</forenames></author><author><keyname>Comin</keyname><forenames>Cesar Henrique</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Minimal paths between communities induced by geographical networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1088/1742-5468/2016/02/023403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the betweenness centrality in geographical
networks and its relationship with network communities. We show that vertices
with large betweenness define what we call characteristic betweenness paths in
both modeled and real-world geographical networks. We define a geographical
network model that possess a simple topology while still being able to present
such betweenness paths. Using this model, we show that such paths represent
pathways between entry and exit points of highly connected regions, or
communities, of geographical networks. By defining a new network, containing
information about community adjacencies in the original network, we describe a
means to characterize the mesoscale connectivity provided by such
characteristic betweenness paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02729</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02729</id><created>2015-01-12</created><authors><author><keyname>Subramaniam</keyname><forenames>Balaji</forenames></author><author><keyname>Feng</keyname><forenames>Wu-chun</forenames></author></authors><title>On the Energy Proportionality of Scale-Out Workloads</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our increasing reliance on the cloud has led to the emergence of scale-out
workloads. These scale-out workloads are latency-sensitive as they are user
driven. In order to meet strict latency constraints, they require massive
computing infrastructure, which consume significant amount of energy and
contribute to operational costs. This cost is further aggravated by the lack of
energy proportionality in servers. As Internet services become even more
ubiquitous, scale-out workloads will need increasingly larger cluster
installations. As such, we desire an investigation into the energy
proportionality and the mechanisms to improve the power consumption of
scale-out workloads.
  Therefore, in this paper, we study the energy proportionality and power
consumption of clusters in the context of scale-out workloads. Towards this
end, we evaluate the potential of power and resource provisioning to improve
the energy proportionality for this class of workloads. Using data serving, web
searching and data caching as our representative workloads, we first analyze
the component-level power distribution on a cluster. Second, we characterize
how these workloads utilize the cluster. Third, we analyze the potential of
power provisioning techniques (i.e., active low-power, turbo and idle low-power
modes) to improve the energy proportionality of scale-out workloads. We then
describe the ability of active low-power modes to provide trade-offs in power
and latency. Finally, we compare and contrast power provisioning and resource
provisioning techniques. Our study reveals various insights which will help
improve the energy proportionality and power consumption of scale-out
workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02732</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02732</id><created>2015-01-12</created><authors><author><keyname>Galyardt</keyname><forenames>April</forenames></author><author><keyname>Goldin</keyname><forenames>Ilya</forenames></author></authors><title>Predicting Performance During Tutoring with Models of Recent Performance</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In educational technology and learning sciences, there are multiple uses for
a predictive model of whether a student will perform a task correctly or not.
For example, an intelligent tutoring system may use such a model to estimate
whether or not a student has mastered a skill. We analyze the significance of
data recency in making such predictions, i.e., asking whether relatively more
recent observations of a student's performance matter more than relatively
older observations. We develop a new Recent-Performance Factors Analysis model
that takes data recency into account. The new model significantly improves
predictive accuracy over both existing logistic-regression performance models
and over novel baseline models in evaluations on real-world and synthetic
datasets. As a secondary contribution, we demonstrate how the widely used
cross-validation with 0-1 loss is inferior to AIC and to cross-validation with
L1 prediction error loss as a measure of model performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02741</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02741</id><created>2015-01-05</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author><author><keyname>Jiang</keyname><forenames>Huaizu</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author></authors><title>Salient Object Detection: A Benchmark</title><categories>cs.CV</categories><journal-ref>Image Processing, IEEE Transactions on (Volume:24, Issue: 12),
  2015</journal-ref><doi>10.1109/TIP.2015.2487833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extensively compare, qualitatively and quantitatively, 40 state-of-the-art
models (28 salient object detection, 10 fixation prediction, 1 objectness, and
1 baseline) over 6 challenging datasets for the purpose of benchmarking salient
object detection and segmentation methods. From the results obtained so far,
our evaluation shows a consistent rapid progress over the last few years in
terms of both accuracy and running time. The top contenders in this benchmark
significantly outperform the models identified as the best in the previous
benchmark conducted just two years ago. We find that the models designed
specifically for salient object detection generally work better than models in
closely related areas, which in turn provides a precise definition and suggests
an appropriate treatment of this problem that distinguishes it from other
problems. In particular, we analyze the influences of center bias and scene
complexity in model performance, which, along with the hard cases for
state-of-the-art models, provide useful hints towards constructing more
challenging large scale datasets and better saliency models. Finally, we
propose probable solutions for tackling several open problems such as
evaluation scores and dataset bias, which also suggest future research
directions in the rapidly-growing field of salient object detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02755</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02755</id><created>2015-01-12</created><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Yinghong</forenames></author></authors><title>Computation of Differential Chow Forms for Prime Differential Ideals</title><categories>math.AG cs.SC</categories><comments>24 pages</comments><msc-class>12H05, 14C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose algorithms to compute differential Chow forms for
prime differential ideals which are given by their characteristic sets. The
main algorithm is based on an optimal bound for the order of a prime
differential ideal in terms of its characteristic set under an arbitrary
ranking, which shows the Jacobi bound conjecture holds in this case. Apart from
the order bound, we also give a degree bound for the differential Chow form. In
addition, for prime differential ideals given by their characteristic sets
under an orderly ranking, a much more simpler algorithm is given to compute its
differential Chow form. The computational complexity of both is single
exponential in terms of the Jacobi number, the maximal degree of the
differential polynomials in the characteristic set and the number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02758</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02758</id><created>2015-01-12</created><authors><author><keyname>Gauvin</keyname><forenames>Laetitia</forenames></author><author><keyname>Panisson</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Cattuto</keyname><forenames>Ciro</forenames></author></authors><title>Revealing latent factors of temporal networks for mesoscale intervention
  in epidemic spread</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The customary perspective to reason about epidemic mitigation in temporal
networks hinges on the identification of nodes with specific features or
network roles. The ensuing individual-based control strategies, however, are
difficult to carry out in practice and ignore important correlations between
topological and temporal patterns. Here we adopt a mesoscopic perspective and
present a principled framework to identify collective features at multiple
scales and rank their importance for epidemic spread. We use tensor
decomposition techniques to build an additive representation of a temporal
network in terms of mesostructures, such as cohesive clusters and
temporally-localized mixing patterns. This representation allows to determine
the impact of individual mesostructures on epidemic spread and to assess the
effect of targeted interventions that remove chosen structures. We illustrate
this approach using high-resolution social network data on face-to-face
interactions in a school and show that our method affords the design of
effective mesoscale interventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02785</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02785</id><created>2015-01-12</created><updated>2015-09-15</updated><authors><author><keyname>Lotfi</keyname><forenames>Mohammad Hassan</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author><author><keyname>Sundaresan</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Khojastepour</keyname><forenames>Mohammad Ali</forenames></author></authors><title>The Economics of Quality Sponsored Data in Non-Neutral Networks</title><categories>cs.NI cs.GT</categories><comments>Submitted to IEEE/ACM Transaction on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing demand for data has driven the Service Providers (SPs) to provide
differential treatment of traffic to generate additional revenue streams from
Content Providers (CPs). While SPs currently only provide best-effort services
to their CPs, it is plausible to envision a model in near future, where CPs are
willing to sponsor quality of service for their content in exchange of sharing
a portion of their profit with SPs. This quality sponsoring becomes invaluable
especially when the available resources are scarce such as in wireless
networks, and can be accommodated in a non-neutral network. In this paper, we
consider the problem of Quality-Sponsored Data (QSD) in a non-neutral network.
In our model, SPs allow CPs to sponsor a portion of their resources, and price
it appropriately to maximize their payoff. The payoff of the SP depends on the
monetary revenue and the satisfaction of end-users both for the non-sponsored
and sponsored content, while CPs generate revenue through advertisement. We
analyze the market dynamics and equilibria in two different frameworks, i.e.
sequential and bargaining game frameworks, and provide strategies for (i) SPs:
to determine if and how to price resources, and (ii) CPs: to determine if and
what quality to sponsor. The frameworks characterize different sets of
equilibrium strategies and market outcomes depending on the parameters of the
market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02795</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02795</id><created>2015-01-11</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cortijo</keyname><forenames>Francisco J.</forenames></author></authors><title>Scanning and Parsing Languages with Ambiguities and Constraints: The
  Lamb and Fence Algorithms</title><categories>cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1111.3970, arXiv:1110.1470</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional language processing tools constrain language designers to
specific kinds of grammars. In contrast, model-based language processing tools
decouple language design from language processing. These tools allow the
occurrence of lexical and syntactic ambiguities in language specifications and
the declarative specification of constraints for resolving them. As a result,
these techniques require scanners and parsers able to parse context-free
grammars, handle ambiguities, and enforce constraints for disambiguation. In
this paper, we present Lamb and Fence. Lamb is a scanning algorithm that
supports ambiguous token definitions and the specification of custom pattern
matchers and constraints. Fence is a chart parsing algorithm that supports
ambiguous context-free grammars and the definition of constraints on
associativity, composition, and precedence, as well as custom constraints. Lamb
and Fence, in conjunction, enable the implementation of the ModelCC model-based
language processing tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02825</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02825</id><created>2015-01-12</created><authors><author><keyname>Bambach</keyname><forenames>Sven</forenames></author></authors><title>A Survey on Recent Advances of Computer Vision Algorithms for Egocentric
  Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent technological advances have made lightweight, head mounted cameras
both practical and affordable and products like Google Glass show first
approaches to introduce the idea of egocentric (first-person) video to the
mainstream. Interestingly, the computer vision community has only recently
started to explore this new domain of egocentric vision, where research can
roughly be categorized into three areas: Object recognition, activity
detection/recognition, video summarization. In this paper, we try to give a
broad overview about the different problems that have been addressed and
collect and compare evaluation results. Moreover, along with the emergence of
this new domain came the introduction of numerous new and versatile benchmark
datasets, which we summarize and compare as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02834</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02834</id><created>2015-01-12</created><authors><author><keyname>Adamek</keyname><forenames>Jiri</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Myers</keyname><forenames>Robert</forenames></author><author><keyname>Urbat</keyname><forenames>Henning</forenames></author></authors><title>Generalized Eilenberg Theorem I: Local Varieties of Languages</title><categories>cs.FL cs.LO math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the duality between algebraic and coalgebraic recognition of
languages to derive a generalization of the local version of Eilenberg's
theorem. This theorem states that the lattice of all boolean algebras of
regular languages over an alphabet {\Sigma} closed under derivatives is
isomorphic to the lattice of all pseudovarieties of {\Sigma}-generated monoids.
By applying our method to different categories, we obtain three related
results: one, due to Gehrke, Grigorieff and Pin, weakens boolean algebras to
distributive lattices, one weakens them to join-semilattices, and the last one
considers vector spaces over the binary field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02854</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02854</id><created>2015-01-12</created><authors><author><keyname>Zhao</keyname><forenames>Ye</forenames></author><author><keyname>Paine</keyname><forenames>Nicholas</forenames></author><author><keyname>Kim</keyname><forenames>Kwan Suk</forenames></author><author><keyname>Sentis</keyname><forenames>Luis</forenames></author></authors><title>Stability and Performance Limits of Latency-Prone Distributed Feedback
  Controllers</title><categories>cs.SY cs.RO</categories><comments>13 pages, 10 figures, 2 tables, 31 reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic control systems are increasingly relying on distributed feedback
controllers to tackle complex sensing and decision problems such as those found
in highly articulated human-centered robots. These demands come at the cost of
a growing computational burden and, as a result, larger controller latencies.
To maximize robustness to mechanical disturbances by maximizing control
feedback gains, this paper emphasizes the necessity for compromise between
high- and low-level feedback control effort in distributed controllers.
Specifically, the effect of distributed impedance controllers is studied where
damping feedback effort is executed in close proximity to the control plant and
stiffness feedback effort is executed in a latency-prone centralized control
process. A central observation is that the stability of high impedance
distributed controllers is very sensitive to damping feedback delay but much
less to stiffness feedback delay. This study pursues a detailed analysis of
this observation that leads to a physical understanding of the disparity. Then
a practical controller breakdown gain rule is derived to aim at enabling
control designers to consider the benefits of implementing their control
applications in a distributed fashion. These considerations are further
validated through the analysis, simulation and experimental testing on high
performance actuators and on an omnidirectional mobile base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02855</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02855</id><created>2015-01-12</created><authors><author><keyname>Kim</keyname><forenames>Donghyun</forenames></author><author><keyname>Zhao</keyname><forenames>Ye</forenames></author><author><keyname>Thomas</keyname><forenames>Gray</forenames></author><author><keyname>Sentis</keyname><forenames>Luis</forenames></author></authors><title>Assessing Whole-Body Operational Space Control in a Point-Foot Series
  Elastic Biped: Balance on Split Terrain and Undirected Walking</title><categories>cs.RO cs.SY</categories><comments>17 pages, 9 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present advancements in control and trajectory generation
for agile behavior in bipedal robots. We demonstrate that Whole-Body
Operational Space Control (WBOSC), developed a few years ago, is well suited
for achieving two types of agile behaviors, namely, balancing on a high pitch
split terrain and achieving undirected walking on flat terrain. The work
presented here is the first implementation of WBOSC on a biped robot, and more
specifically a biped robot with series elastic actuators. We present and
analyze a new algorithm that dynamically balances point foot robots by choosing
footstep placements. Dealing with the naturally unstable dynamics of these type
of systems is a difficult problem that requires both the controller and the
trajectory generation algorithm to operate quickly and efficiently. We put
forth a comprehensive development and integration effort: the design and
construction of the biped system and experimental infrastructure, a
customization of WBOSC for the agile behaviors, and new trajectory generation
algorithms. Using this custom built controller, we conduct, for first time, an
experiment in which a biped robot balances in a high pitch split terrain,
demonstrating our ability to precisely regulate internal forces using force
sensing feedback techniques. Finally, we demonstrate the stabilizing
capabilities of our online trajectory generation algorithm in the physics-based
simulator and through physical experiments with a planarized locomotion setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02859</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02859</id><created>2015-01-12</created><authors><author><keyname>Ravishankar</keyname><forenames>Saiprasad</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>$\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates
  and Convergence Guarantees</title><categories>stat.ML cs.LG</categories><comments>Accepted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2405503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications in signal processing benefit from the sparsity of signals
in a certain transform domain or dictionary. Synthesis sparsifying dictionaries
that are directly adapted to data have been popular in applications such as
image denoising, inpainting, and medical image reconstruction. In this work, we
focus instead on the sparsifying transform model, and study the learning of
well-conditioned square sparsifying transforms. The proposed algorithms
alternate between a $\ell_0$ &quot;norm&quot;-based sparse coding step, and a non-convex
transform update step. We derive the exact analytical solution for each of
these steps. The proposed solution for the transform update step achieves the
global minimum in that step, and also provides speedups over iterative
solutions involving conjugate gradients. We establish that our alternating
algorithms are globally convergent to the set of local minimizers of the
non-convex transform learning problems. In practice, the algorithms are
insensitive to initialization. We present results illustrating the promising
performance and significant speed-ups of transform learning over synthesis
K-SVD in image denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02866</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02866</id><created>2015-01-12</created><updated>2015-10-28</updated><authors><author><keyname>Shahrivar</keyname><forenames>Ebrahim Moradi</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>The Strategic Formation of Multi-Layer Networks</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the strategic formation of multi-layer networks, where each layer
represents a different type of relationship between the nodes in the network
and is designed to maximize some utility that depends on the topology of that
layer and those of the other layers. We start by generalizing distance-based
network formation to the two-layer setting, where edges are constructed in one
layer (with fixed cost per edge) to minimize distances between nodes that are
neighbors in another layer. We show that designing an optimal network in this
setting is NP-hard. Despite the underlying complexity of the problem, we
characterize certain properties of the optimal networks. We then formulate a
multi-layer network formation game where each layer corresponds to a player
that is optimally choosing its edge set in response to the edge sets of the
other players. We consider utility functions that view the different layers as
strategic substitutes. By applying our results about optimal networks, we show
that players with low edge costs drive players with high edge costs out of the
game, and that hub-and-spoke networks that are commonly observed in
transportation systems arise as Nash equilibria in this game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02869</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02869</id><created>2015-01-12</created><authors><author><keyname>Zhong</keyname><forenames>Xiaoxiong</forenames></author><author><keyname>Qin</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author></authors><title>Transport Protocols in Cognitive Radio Networks: A Survey</title><categories>cs.NI</categories><comments>to appear in KSII Transactions on Internet and Information Systems</comments><doi>10.3837/tiis.2014.11.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio networks (CRNs) have emerged as a promising solution to
enhance spectrum utilization by using unused or less used spectrum in radio
environments. The basic idea of CRNs is to allow secondary users (SUs) access
to licensed spectrum, under the condition that the interference perceived by
the primary users (PUs) is minimal. In CRNs, the channel availability is
uncertainty due to the existence of PUs, resulting in intermittent
communication. Transmission control protocol (TCP) performance may
significantly degrade in such conditions. To address the challenges, some
transport protocols have been proposed for reliable transmission in CRNs. In
this paper we survey the state-of-the-art transport protocols for CRNs. We
firstly highlight the unique aspects of CRNs, and describe the challenges of
transport protocols in terms of PU behavior, spectrum sensing, spectrum
changing and TCP mechanism itself over CRNs. Then, we provide a summary and
comparison of existing transport protocols for CRNs. Finally, we discuss
several open issues and research challenges. To the best of our knowledge, our
work is the first survey on transport protocols for CRNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02870</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02870</id><created>2015-01-12</created><authors><author><keyname>Ma</keyname><forenames>Meijie</forenames></author></authors><title>Topological properties on the diameters of the integer simplex</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wide diameter $d_\omega(G)$ and fault-diameter $D_\omega(G)$ of an
interconnection network $G$ have been recently studied by many authors. We
determine the wide diameter and fault-diameter of the integer simplex $T_m^n$.
Note that $d_1(T_m^n)=D_1(T_m^n)= d(T_m^n)$, where $d(T_m^n)$ is the diameter
of $T_m^n$. We prove that $d_\omega(T_m^n)=D_\omega(T_m^n)= d(T_m^n)+1$ when
$2\leq\omega\leq n$. Since a triangular pyramid $TP_L$ is $T_L^3$, we have
$d_\omega(TP_L)=D_\omega(TP_L)= d(TP_L)+1$ when $2\leq\omega\leq 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02876</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02876</id><created>2015-01-12</created><updated>2015-07-05</updated><authors><author><keyname>Wu</keyname><forenames>Ren</forenames></author><author><keyname>Yan</keyname><forenames>Shengen</forenames></author><author><keyname>Shan</keyname><forenames>Yi</forenames></author><author><keyname>Dang</keyname><forenames>Qingqing</forenames></author><author><keyname>Sun</keyname><forenames>Gang</forenames></author></authors><title>Deep Image: Scaling up Image Recognition</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the authors due to a mistake related
  to ImageNet server submissions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a state-of-the-art image recognition system, Deep Image, developed
using end-to-end deep learning. The key components are a custom-built
supercomputer dedicated to deep learning, a highly optimized parallel algorithm
using new strategies for data partitioning and communication, larger deep
neural network models, novel data augmentation approaches, and usage of
multi-scale high-resolution images. Our method achieves excellent results on
multiple challenging computer vision benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02885</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02885</id><created>2015-01-12</created><authors><author><keyname>Thomborson</keyname><forenames>Clark</forenames></author></authors><title>Benchmarking Obfuscators of Functionality</title><categories>cs.CR cs.SE</categories><comments>8 pp., submitted to SPRO 2015 (https://aspire-fp7.eu/spro)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a set of benchmarks for evaluating the practicality of software
obfuscators which rely on provably-secure methods for functional obfuscation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02886</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02886</id><created>2015-01-11</created><authors><author><keyname>Hoang</keyname><forenames>Dinh Thai</forenames></author><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Applications of Repeated Games in Wireless Networks: A Survey</title><categories>cs.GT cs.NI</categories><comments>32 pages, 15 figures, 5 tables, 168 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A repeated game is an effective tool to model interactions and conflicts for
players aiming to achieve their objectives in a long-term basis. Contrary to
static noncooperative games that model an interaction among players in only one
period, in repeated games, interactions of players repeat for multiple periods;
and thus the players become aware of other players' past behaviors and their
future benefits, and will adapt their behavior accordingly. In wireless
networks, conflicts among wireless nodes can lead to selfish behaviors,
resulting in poor network performances and detrimental individual payoffs. In
this paper, we survey the applications of repeated games in different wireless
networks. The main goal is to demonstrate the use of repeated games to
encourage wireless nodes to cooperate, thereby improving network performances
and avoiding network disruption due to selfish behaviors. Furthermore, various
problems in wireless networks and variations of repeated game models together
with the corresponding solutions are discussed in this survey. Finally, we
outline some open issues and future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02887</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02887</id><created>2015-01-11</created><authors><author><keyname>VL</keyname><forenames>Lajish</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Online Handwritten Devanagari Stroke Recognition Using Extended
  Directional Features</title><categories>cs.CV</categories><comments>8th International Conference on Signal Processing and Communication
  Systems 15 - 17 December 2014, Gold Coast, Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new feature set, called the extended directional
features (EDF) for use in the recognition of online handwritten strokes. We use
EDF specifically to recognize strokes that form a basis for producing
Devanagari script, which is the most widely used Indian language script. It
should be noted that stroke recognition in handwritten script is equivalent to
phoneme recognition in speech signals and is generally very poor and of the
order of 20% for singing voice. Experiments are conducted for the automatic
recognition of isolated handwritten strokes. Initially we describe the proposed
feature set, namely EDF and then show how this feature can be effectively
utilized for writer independent script recognition through stroke recognition.
Experimental results show that the extended directional feature set performs
well with about 65+% stroke level recognition accuracy for writer independent
data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02889</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02889</id><created>2015-01-13</created><authors><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Chae</keyname><forenames>Sung Ho</forenames></author><author><keyname>Lim</keyname><forenames>Sung Hoon</forenames></author></authors><title>Degrees of Freedom of Full-Duplex Multiantenna Cellular Networks</title><categories>cs.IT math.IT</categories><comments>21 pages, 16 figures, a shorter version of this paper has been
  submitted to the IEEE International Symposium on Information Theory (ISIT)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the degrees of freedom (DoF) of cellular networks in which a full
duplex (FD) base station (BS) equipped with multiple transmit and receive
antennas communicates with multiple mobile users. We consider two different
scenarios. In the first scenario, we study the case when half duplex (HD)
users, partitioned to either the uplink (UL) set or the downlink (DL) set,
simultaneously communicate with the FD BS. In the second scenario, we study the
case when FD users simultaneously communicate UL and DL data with the FD BS.
Unlike conventional HD only systems, inter-user interference (within the cell)
may severely limit the DoF, and must be carefully taken into account. With the
goal of providing theoretical guidelines for designing such FD systems, we
completely characterize the sum DoF of each of the two different FD cellular
networks by developing an achievable scheme and obtaining a matching upper
bound. The key idea of the proposed scheme is to carefully allocate UL and DL
information streams using interference alignment and beamforming techniques. By
comparing the DoFs of the considered FD systems with those of the conventional
HD systems, we establish the DoF gain by enabling FD operation in various
configurations. As a consequence of the result, we show that the DoF can
approach the two-fold gain over the HD systems when the number of users becomes
large enough as compared to the number of antennas at the BS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02892</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02892</id><created>2015-01-13</created><authors><author><keyname>Binu</keyname><forenames>V. P.</forenames></author><author><keyname>Sreekumar</keyname><forenames>A.</forenames></author></authors><title>Lossless Secret Image Sharing Schemes</title><categories>cs.CR</categories><comments>International Journal of Computational Intelligence and Information
  Security Vol. 4 April 2013, ISSN: 1837-7823</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret image sharing deals with splitting confidential images into several
shares and the original image can be reconstructed from the qualified subset of
the shares. Secret sharing schemes are used in transmission and storage of
private medical images and military secrets. Increased confidentiality and
availability are the major achievements. We propose an efficient (2, 2) scheme
and (2, 3) scheme for secret image sharing. The scheme is lossless and also the
share size is same as the secret size. The sharing and revealing phase uses
simple modular arithmetic which can be very easily implemented. Experimental
results on Binary and Gray scale images show that the proposed scheme is secure
and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02894</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02894</id><created>2015-01-13</created><updated>2015-01-15</updated><authors><author><keyname>Salarian</keyname><forenames>Mehdi.</forenames></author><author><keyname>Mohamadinia</keyname><forenames>Babak.</forenames></author><author><keyname>Rasekhi</keyname><forenames>Jalil</forenames></author></authors><title>A Modified No Search Algorithm for Fractal Image Compression</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractal image compression has some desirable properties like high quality at
high compression ratio, fast decoding, and resolution independence. Therefore
it can be used for many applications such as texture mapping and pattern
recognition and image watermarking. But it suffers from long encoding time due
to its need to find the best match between sub blocks. This time is related to
the approach that is used. In this paper we present a fast encoding Algorithm
based on no search method. Our goal is that more blocks are covered in initial
step of quad tree algorithm. Experimental result has been compared with other
new fast fractal coding methods, showing it is better in term of bit rate in
same condition while the other parameters are fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02905</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02905</id><created>2015-01-13</created><updated>2015-12-18</updated><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Xu</keyname><forenames>Yinlong</forenames></author><author><keyname>Li</keyname><forenames>Zhipeng</forenames></author></authors><title>Sampling Online Social Networks via Heterogeneous Statistics</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most sampling techniques for online social networks (OSNs) are based on a
particular sampling method on a single graph, which is referred to as a
statistics. However, various realizing methods on different graphs could
possibly be used in the same OSN, and they may lead to different sampling
efficiencies, i.e., asymptotic variances. To utilize multiple statistics for
accurate measurements, we formulate a mixture sampling problem, through which
we construct a mixture unbiased estimator which minimizes asymptotic variance.
Given fixed sampling budgets for different statistics, we derive the optimal
weights to combine the individual estimators; given fixed total budget, we show
that a greedy allocation towards the most efficient statistics is optimal. In
practice, the sampling efficiencies of statistics can be quite different for
various targets and are unknown before sampling. To solve this problem, we
design a two-stage framework which adaptively spends a partial budget to test
different statistics and allocates the remaining budget to the inferred best
statistics. We show that our two-stage framework is a generalization of 1)
randomly choosing a statistics and 2) evenly allocating the total budget among
all available statistics, and our adaptive algorithm achieves higher efficiency
than these benchmark strategies in theory and experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02911</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02911</id><created>2015-01-13</created><authors><author><keyname>Ajtai</keyname><forenames>Miklos</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author></authors><title>Sorting and Selection with Imprecise Comparisons</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple model of imprecise comparisons: there exists some
$\delta&gt;0$ such that when a subject is given two elements to compare, if the
values of those elements (as perceived by the subject) differ by at least
$\delta$, then the comparison will be made correctly; when the two elements
have values that are within $\delta$, the outcome of the comparison is
unpredictable. This model is inspired by both imprecision in human judgment of
values and also by bounded but potentially adversarial errors in the outcomes
of sporting tournaments.
  Our model is closely related to a number of models commonly considered in the
psychophysics literature where $\delta$ corresponds to the {\em just noticeable
difference unit (JND)} or {\em difference threshold}. In experimental
psychology, the method of paired comparisons was proposed as a means for
ranking preferences amongst $n$ elements of a human subject. The method
requires performing all $\binom{n}{2}$ comparisons, then sorting elements
according to the number of wins. The large number of comparisons is performed
to counter the potentially faulty decision-making of the human subject, who
acts as an imprecise comparator.
  We show that in our model the method of paired comparisons has optimal
accuracy, minimizing the errors introduced by the imprecise comparisons.
However, it is also wasteful, as it requires all $\binom{n}{2}$. We show that
the same optimal guarantees can be achieved using $4 n^{3/2}$ comparisons, and
we prove the optimality of our method. We then explore the general tradeoff
between the guarantees on the error that can be made and number of comparisons
for the problems of sorting, max-finding, and selection. Our results provide
strong lower bounds and close-to-optimal solutions for each of these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02917</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02917</id><created>2015-01-13</created><updated>2015-01-15</updated><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Kasparick</keyname><forenames>Martin</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>Spline Waveforms and Interference Analysis for 5G Random Access with
  Short Message Support</title><categories>cs.IT math.IT</categories><comments>This version contains minor corrections and the title of the paper
  was changed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main drivers for new waveforms in future 5G wireless communication
systems is to handle efficiently the variety of traffic types and requirements.
In this paper, we introduce a new random access within the standard acquisition
procedures to support sporadic traffic as an enabler of the Internet of Things
(IoT). The major challenge hereby is to cope with the highly asynchronous
access of different devices and to allow transmission of control signaling and
payload &quot;in one shot&quot;. We address this challenge by using a waveform design
approach based on bi-orthogonal frequency division multiplexing where transmit
orthogonality is replaced in favor of better temporal and spectral properties.
We show that this approach allows data transmission in frequencies that
otherwise have to remain unused. More precisely, we utilize frequencies
previously used as guard bands, located towards the standard synchronous
communication pipes as well as in between the typically small amount of
resources used by each IoT device. We demonstrate the superiority of this
waveform approach over the conventional random access using a novel
mathematical approach and numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02918</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02918</id><created>2015-01-13</created><updated>2015-01-14</updated><authors><author><keyname>Mitra</keyname><forenames>Shubhadip</forenames></author><author><keyname>Ranu</keyname><forenames>Sayan</forenames></author><author><keyname>Kolar</keyname><forenames>Vinay</forenames></author><author><keyname>Telang</keyname><forenames>Aditya</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Arnab</forenames></author><author><keyname>Kokku</keyname><forenames>Ravi</forenames></author><author><keyname>Raghavan</keyname><forenames>Sriram</forenames></author></authors><title>Trajectory Aware Macro-cell Planning for Mobile Users</title><categories>cs.NI</categories><comments>Published in INFOCOM 2015</comments><acm-class>C.2.3; C.2.1; G.1.6; G.1.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design and evaluate algorithms for efficient user-mobility driven
macro-cell planning in cellular networks. As cellular networks embrace
heterogeneous technologies (including long range 3G/4G and short range WiFi,
Femto-cells, etc.), most traffic generated by static users gets absorbed by the
short-range technologies, thereby increasingly leaving mobile user traffic to
macro-cells. To this end, we consider a novel approach that factors in the
trajectories of mobile users as well as the impact of city geographies and
their associated road networks for macro-cell planning. Given a budget k of
base-stations that can be upgraded, our approach selects a deployment that
impacts the most number of user trajectories. The generic formulation
incorporates the notion of quality of service of a user trajectory as a
parameter to allow different application-specific requirements, and operator
choices.We show that the proposed trajectory utility maximization problem is
NP-hard, and design multiple heuristics. We evaluate our algorithms with real
and synthetic data sets emulating different city geographies to demonstrate
their efficacy. For instance, with an upgrade budget k of 20%, our algorithms
perform 3-8 times better in improving the user quality of service on
trajectories in different city geographies when compared to greedy
location-based base-station upgrades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02921</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02921</id><created>2015-01-13</created><authors><author><keyname>Xu</keyname><forenames>W.</forenames></author><author><keyname>Wu</keyname><forenames>M.</forenames></author><author><keyname>Zhang</keyname><forenames>H.</forenames></author><author><keyname>You</keyname><forenames>X.</forenames></author><author><keyname>Zhao</keyname><forenames>C.</forenames></author></authors><title>ACO-OFDM-Specified Recoverable Upper Clipping With Efficient Detection
  for Optical Wireless Communications</title><categories>cs.IT math.IT</categories><comments>appear in IEEE Photonics Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high peak-to-average-power ratio (PAPR) of orthogonal frequency-division
multiplexing (OFDM) degrades the performance in optical wireless communication
systems. This paper proposes a modified asymmetrically clipped optical OFDM
(ACOOFDM) with low PAPR via introducing a recoverable upper-clipping (RoC)
procedure. Although some information is clipped by a predetermined peak
threshold, the clipped error information is kept and repositioned in our
proposed scheme, which is named RoC-ACO-OFDM, instead of simply being dropped
in conventional schemes. The proposed method makes full use of the specific
structure of ACO-OFDM signals in the time domain, where half of the positions
are forced to zeros within an OFDM symbol. The zero-valued positions are
utilized to carry the clipped error information. Moreover, we accordingly
present an optimal maximum a posteriori (MAP) detection for the RoC-ACO-OFDM
system. To facilitate the usage of RoC-ACO-OFDM in practical applications, an
efficient detection method is further developed with near-optimal performance.
Simulation results show that the proposed RoC-ACO-OFDM achieves a significant
PAPR reduction, while maintaining a competitive bit-error rate performance
compared with the conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02923</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02923</id><created>2015-01-13</created><updated>2015-10-22</updated><authors><author><keyname>Ravishankar</keyname><forenames>Saiprasad</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Efficient Blind Compressed Sensing Using Sparsifying Transforms with
  Convergence Guarantees and Application to MRI</title><categories>cs.LG stat.ML</categories><comments>This work has been accepted for publication in the SIAM Journal on
  Imaging Sciences. It also appears in Saiprasad Ravishankar's PhD thesis, that
  was deposited with the University of Illinois on December 05, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural signals and images are well-known to be approximately sparse in
transform domains such as Wavelets and DCT. This property has been heavily
exploited in various applications in image processing and medical imaging.
Compressed sensing exploits the sparsity of images or image patches in a
transform domain or synthesis dictionary to reconstruct images from
undersampled measurements. In this work, we focus on blind compressed sensing,
where the underlying sparsifying transform is a priori unknown, and propose a
framework to simultaneously reconstruct the underlying image as well as the
sparsifying transform from highly undersampled measurements. The proposed block
coordinate descent type algorithms involve highly efficient optimal updates.
Importantly, we prove that although the proposed blind compressed sensing
formulations are highly nonconvex, our algorithms are globally convergent
(i.e., they converge from any initialization) to the set of critical points of
the objectives defining the formulations. These critical points are guaranteed
to be at least partial global and partial local minimizers. The exact point(s)
of convergence may depend on initialization. We illustrate the usefulness of
the proposed framework for magnetic resonance image reconstruction from highly
undersampled k-space measurements. As compared to previous methods involving
the synthesis dictionary model, our approach is much faster, while also
providing promising reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02925</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02925</id><created>2015-01-13</created><updated>2015-01-15</updated><authors><author><keyname>Clouston</keyname><forenames>Ranald</forenames></author><author><keyname>Bizjak</keyname><forenames>Ale&#x161;</forenames></author><author><keyname>Grathwohl</keyname><forenames>Hans Bugge</forenames></author><author><keyname>Birkedal</keyname><forenames>Lars</forenames></author></authors><title>Programming and Reasoning with Guarded Recursion for Coinductive Types</title><categories>cs.PL cs.LO</categories><comments>Version of FoSSaCS 2015 paper with appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the guarded lambda-calculus, an extension of the simply typed
lambda-calculus with guarded recursive and coinductive types. The use of
guarded recursive types ensures the productivity of well-typed programs.
Guarded recursive types may be transformed into coinductive types by a
type-former inspired by modal logic and Atkey-McBride clock quantification,
allowing the typing of acausal functions. We give a call-by-name operational
semantics for the calculus, and define adequate denotational semantics in the
topos of trees. The adequacy proof entails that the evaluation of a program
always terminates. We demonstrate the expressiveness of the calculus by showing
the definability of solutions to Rutten's behavioural differential equations.
We introduce a program logic with L\&quot;{o}b induction for reasoning about the
contextual equivalence of programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02940</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02940</id><created>2015-01-13</created><authors><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author></authors><title>Low Complexity Transceiver Design for GFDM</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its attractive properties, generalized frequency division multiplexing
(GFDM) is recently being discussed as a candidate waveform for the fifth
generation of wireless communication systems (5G). GFDM is introduced as a
generalized form of the widely used orthogonal frequency division multiplexing
(OFDM) modulation scheme and since it uses only one cyclic prefix (CP) for a
group of symbols rather than a CP per symbol, it is more bandwidth efficient
than OFDM. In this paper, we propose novel transceiver structures for GFDM by
taking advantage of the particular structure in the modulation matrix. Our
proposed transmitter is based on modulation matrix sparsification through
application of fast Fourier transform (FFT) to reduce the implementation
complexity. A unified receiver structure for matched filter (MF), zero forcing
(ZF) and minimum mean square error (MMSE) receivers is also derived. The
proposed receiver techniques harness the special block circulant property of
the matrices involved in the demodulation stage to reduce the computational
cost of the system implementation. We have derived the closed forms for the ZF
and MMSE receiver filters. Additionally, our algorithms do not incur any
performance loss as they maintain the optimal performance. The computational
costs of our proposed techniques are analyzed in detail and are compared with
the existing solutions that are known to have the lowest complexity. It is
shown that through application of our transceiver structure a substantial
amount of computational complexity reduction can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02954</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02954</id><created>2015-01-13</created><authors><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>P&#xf6;chacker</keyname><forenames>Manfred</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Complexity of Power Draws for Load Disaggregation</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Intrusive Load Monitoring (NILM) is a technology offering methods to
identify appliances in homes based on their consumption characteristics and the
total household demand. Recently, many different novel NILM approaches were
introduced, tested on real-world data and evaluated with a common evaluation
metric. However, the fair comparison between different NILM approaches even
with the usage of the same evaluation metric is nearly impossible due to
incomplete or missing problem definitions. Each NILM approach typically is
evaluated under different test scenarios. Test results are thus influenced by
the considered appliances, the number of used appliances, the device type
representing the appliance and the pre-processing stages denoising the
consumption data. This paper introduces a novel complexity measure of
aggregated consumption data providing an assessment of the problem complexity
affected by the used appliances, the appliance characteristics and the
appliance usage over time. We test our load disaggregation complexity on
different real-world datasets and with a state-of-the-art NILM approach. The
introduced disaggregation complexity measure is able to classify the
disaggregation problem based on the used appliance set and the considered
measurement noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02967</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02967</id><created>2015-01-13</created><authors><author><keyname>Bui</keyname><forenames>Thanh</forenames></author></authors><title>Analysis of Docker Security</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, the use of virtualization technologies has increased
dramatically. This makes the demand for efficient and secure virtualization
solutions become more obvious. Container-based virtualization and
hypervisor-based virtualization are two main types of virtualization
technologies that have emerged to the market. Of these two classes,
container-based virtualization is able to provide a more lightweight and
efficient virtual environment, but not without security concerns. In this
paper, we analyze the security level of Docker, a well-known representative of
container-based approaches. The analysis considers two areas: (1) the internal
security of Docker, and (2) how Docker interacts with the security features of
the Linux kernel, such as SELinux and AppArmor, in order to harden the host
system. Furthermore, the paper also discusses and identifies what could be done
when using Docker to increase its level of security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02973</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02973</id><created>2015-01-13</created><authors><author><keyname>Sun</keyname><forenames>Wanlu</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author><author><keyname>Sui</keyname><forenames>Yutao</forenames></author><author><keyname>Sou</keyname><forenames>Kin Cheong</forenames></author></authors><title>D2D-based V2V Communications with Latency and Reliability Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct device-to-device (D2D) communication has been proposed as a possible
enabler for vehicle-to-vehicle (V2V) applications, where the incurred
intra-cell interference and the stringent latency and reliability requirements
are challenging issues. In this paper, we investigate the radio resource
management problem for D2D-based V2V communications. Firstly, we analyze and
mathematically model the actual requirements for vehicular communications and
traditional cellular links. Secondly, we propose a problem formulation to
fulfill these requirements, and then a Separate Resource Block allocation and
Power control (SRBP) algorithm to solve this problem. Finally, simulations are
presented to illustrate the improved performance of the proposed SRBP scheme
compared to some other existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02988</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02988</id><created>2015-01-13</created><authors><author><keyname>Pradhan</keyname><forenames>Hrusikesha</forenames></author><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>Sensing-Throughput Tradeoff in Cognitive Radio With Random Arrivals and
  Departures of Multiple Primary Users</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter analyzes the sensing-throughput tradeoff for a secondary user
(SU) under random arrivals and departures of multiple primary users (PUs). We
first study the case where PUs change their status only during SU's sensing
period. We then generalize to a case where PUs change status anytime during SU
frame, and compare the latter case with the former in terms of the optimal
sensing time and SU throughput. We also investigate the effects of PU traffic
parameters and the number of PUs on the sensing-throughput tradeoff for SU.
Results show that, though the increase in the number of PUs reduces the optimal
sensing time for SU, the opportunity to find a vacant PU channel reduces
simultaneously, in turn, reducing SU throughput. Finally, we validate the
analysis by Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02990</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02990</id><created>2015-01-13</created><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Xiong</keyname><forenames>Momiao</forenames></author><author><keyname>Jin</keyname><forenames>Li</forenames></author></authors><title>Random Bits Regression: a Strong General Predictor for Big Data</title><categories>stat.ML cs.LG</categories><comments>20 pages,1 figure, 2 tables, research article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve accuracy and speed of regressions and classifications, we present
a data-based prediction method, Random Bits Regression (RBR). This method first
generates a large number of random binary intermediate/derived features based
on the original input matrix, and then performs regularized linear/logistic
regression on those intermediate/derived features to predict the outcome.
Benchmark analyses on a simulated dataset, UCI machine learning repository
datasets and a GWAS dataset showed that RBR outperforms other popular methods
in accuracy and robustness. RBR (available on
https://sourceforge.net/projects/rbr/) is very fast and requires reasonable
memories, therefore, provides a strong, robust and fast predictor in the big
data era.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02995</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02995</id><created>2015-01-13</created><authors><author><keyname>Potluri</keyname><forenames>U. S.</forenames></author><author><keyname>Madanayake</keyname><forenames>A.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Kulasekera</keyname><forenames>S.</forenames></author><author><keyname>Edirisuriya</keyname><forenames>A.</forenames></author></authors><title>Improved 8-point Approximate DCT for Image and Video Compression
  Requiring Only 14 Additions</title><categories>cs.MM cs.CV cs.NA stat.ME</categories><comments>30 pages, 7 figures, 5 tables</comments><journal-ref>Circuits and Systems I: Regular Papers, IEEE Transactions on,
  Volume 61, Issue 6, June 2014, 1727--1740</journal-ref><doi>10.1109/TCSI.2013.2295022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video processing systems such as HEVC requiring low energy consumption needed
for the multimedia market has lead to extensive development in fast algorithms
for the efficient approximation of 2-D DCT transforms. The DCT is employed in a
multitude of compression standards due to its remarkable energy compaction
properties. Multiplier-free approximate DCT transforms have been proposed that
offer superior compression performance at very low circuit complexity. Such
approximations can be realized in digital VLSI hardware using additions and
subtractions only, leading to significant reductions in chip area and power
consumption compared to conventional DCTs and integer transforms. In this
paper, we introduce a novel 8-point DCT approximation that requires only 14
addition operations and no multiplications. The proposed transform possesses
low computational complexity and is compared to state-of-the-art DCT
approximations in terms of both algorithm complexity and peak signal-to-noise
ratio. The proposed DCT approximation is a candidate for reconfigurable video
standards such as HEVC. The proposed transform and several other DCT
approximations are mapped to systolic-array digital architectures and
physically realized as digital prototype circuits using FPGA technology and
mapped to 45 nm CMOS technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.02997</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.02997</id><created>2015-01-13</created><updated>2016-02-12</updated><authors><author><keyname>Fijalkow</keyname><forenames>Nathana&#xeb;l</forenames></author></authors><title>Characterisation of an Algebraic Algorithm for Probabilistic Automata
  Characterisation of an Algebraic Algorithm for Probabilistic Automata</title><categories>cs.FL cs.LO</categories><comments>STACS, Symposium on Theoretical Aspects of Computer Science, Feb
  2016, Orl{\'e}ans, France</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the value 1 problem for probabilistic automata over finite words:
it asks whether a given probabilistic automaton accepts words with probability
arbitrarily close to 1. This problem is known to be undecidable. However,
different algorithms have been proposed to partially solve it; it has been
recently shown that the Markov Monoid algorithm, based on algebra, is the most
correct algorithm so far. The first contribution of this paper is to give a
characterisation of the Markov Monoid algorithm. The second contribution is to
develop a profinite theory for probabilistic automata, called the prostochastic
theory. This new framework gives a topological account of the value 1 problem,
which in this context is cast as an emptiness problem. The above
characterisation is reformulated using the prostochastic theory, allowing to
give a modular proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03001</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03001</id><created>2015-01-13</created><authors><author><keyname>Laviolette</keyname><forenames>Francois</forenames><affiliation>LHC</affiliation></author><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LHC</affiliation></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames></author><author><keyname>Roy</keyname><forenames>Jean-Francis</forenames></author></authors><title>On Generalizing the C-Bound to the Multiclass and Multi-label Settings</title><categories>stat.ML cs.LG</categories><comments>NIPS 2014 Workshop on Representation and Learning Methods for Complex
  Outputs, Dec 2014, Montr{\'e}al, Canada</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The C-bound, introduced in Lacasse et al., gives a tight upper bound on the
risk of a binary majority vote classifier. In this work, we present a first
step towards extending this work to more complex outputs, by providing
generalizations of the C-bound to the multiclass and multi-label settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03002</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03002</id><created>2015-01-13</created><authors><author><keyname>Germain</keyname><forenames>Pascal</forenames><affiliation>LHC</affiliation></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames><affiliation>LHC</affiliation></author><author><keyname>Laviolette</keyname><forenames>Francois</forenames><affiliation>LHC</affiliation></author><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LHC</affiliation></author></authors><title>An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context</title><categories>stat.ML cs.LG</categories><comments>NIPS 2014 Workshop on Transfer and Multi-task learning: Theory Meets
  Practice, Dec 2014, Montr{\'e}al, Canada</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a theoretical analysis of domain adaptation based on the
PAC-Bayesian theory. We propose an improvement of the previous domain
adaptation bound obtained by Germain et al. in two ways. We first give another
generalization bound tighter and easier to interpret. Moreover, we provide a
new analysis of the constant term appearing in the bound that can be of high
interest for developing new algorithmic solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03015</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03015</id><created>2015-01-13</created><authors><author><keyname>Zimmermann</keyname><forenames>Albrecht</forenames></author><author><keyname>Bringmann</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author></authors><title>Exploring the efficacy of molecular fragments of different complexity in
  computational SAR modeling</title><categories>cs.CE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important first step in computational SAR modeling is to transform the
compounds into a representation that can be processed by predictive modeling
techniques. This is typically a feature vector where each feature indicates the
presence or absence of a molecular fragment. While the traditional approach to
SAR modeling employed size restricted fingerprints derived from path fragments,
much research in recent years focussed on mining more complex graph based
fragments. Today, there seems to be a growing consensus in the data mining
community that these more expressive fragments should be more useful. We
question this consensus and show experimentally that fragments of low
complexity, i.e. sequences, perform better than equally large sets of more
complex ones, an effect we explain by pairwise correlation among fragments and
the ability of a fragment set to encode compounds from different classes
distinctly. The size restriction on these sets is based on ordering the
fragments by class-correlation scores. In addition, we also evaluate the
effects of using a significance value instead of a length restriction for path
fragments and find a significant reduction in the number of features with
little loss in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03016</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03016</id><created>2015-01-13</created><authors><author><keyname>Rao</keyname><forenames>Shravas</forenames></author><author><keyname>Shinkar</keyname><forenames>Igor</forenames></author></authors><title>On Lipschitz Bijections between Boolean Functions</title><categories>cs.DM math.CO math.MG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For two functions $f,g:\{0,1\}^n\to\{0,1\}$ a mapping
$\psi:\{0,1\}^n\to\{0,1\}^n$ is said to be a $\textit{mapping from $f$ to $g$}$
if it is a bijection and $f(z)=g(\psi(z))$ for every $z\in\{0,1\}^n$. In this
paper we study Lipschitz mappings between boolean functions.
  Our first result gives a construction of a $C$-Lipschitz mapping from the
${\sf Majority}$ function to the ${\sf Dictator}$ function for some universal
constant $C$. On the other hand, there is no $n/2$-Lipschitz mapping in the
other direction, namely from the ${\sf Dictator}$ function to the ${\sf
Majority}$ function. This answers an open problem posed by Daniel Varga in the
paper of Benjamini et al. (FOCS 2014).
  We also show a mapping from ${\sf Dictator}$ to ${\sf XOR}$ that is 3-local,
2-Lipschitz, and its inverse is $O(\log(n))$-Lipschitz, where by $L$-local
mapping we mean that each of its output bits depends on at most $L$ input bits.
  Next, we consider the problem of finding functions such that any mapping
between them must have large \emph{average stretch}, where the average stretch
of a mapping $\phi$ is defined as ${\sf avgStretch}(\phi) = {\mathbb
E}_{x,i}[dist(\phi(x),\phi(x+e_i)]$. We show that any mapping $\phi$ from ${\sf
XOR}$ to ${\sf Majority}$ must satisfy ${\sf avgStretch}(\phi) \geq
\Omega(\sqrt{n})$. In some sense, this gives a &quot;function analogue&quot; to the
question of Benjamini et al. (FOCS 2014), who asked whether there exists a set
$A \subset \{0,1\}^n$ of density 0.5 such that any bijection from
$\{0,1\}^{n-1}$ to $A$ has large average stretch.
  Finally, we show that for a random balanced function
$f:\{0,1\}^n\to\{0,1\}^n$ with high probability there is a mapping $\phi$ from
${\sf Dictator}$ to $f$ such that both $\phi$ and $\phi^{-1}$ have constant
average stretch. In particular, this implies that one cannot obtain lower
bounds on average stretch by taking uniformly random functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03018</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03018</id><created>2015-01-13</created><authors><author><keyname>Macias</keyname><forenames>Nicholas J.</forenames></author></authors><title>Context-Dependent Functions: Narrowing the Realm of Turing's Halting
  Problem</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Turing's Halting Problem (HP), and reviews the classic
proof that no function exists that can solve HP. The concept of a
&quot;Context-Dependent Function&quot; (CDF), whose behavior varies based on seemingly
irrelevant changes to a program calling that function, is introduced, and the
proof of HP's undecidability is re-examined in light of CDFs. The existence of
CDFs is established via a pair of examples of such functions. The conclusion of
the proof of HP's undecidability is thus shown to be overly strong, as it
doesn't show that no solution to HP exists, but rather that a solution must be
a CDF. A higher-level analysis of this work is given, followed by conclusions
and comments on future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03024</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03024</id><created>2015-01-13</created><updated>2015-05-20</updated><authors><author><keyname>Vinckier</keyname><forenames>Quentin</forenames></author><author><keyname>Duport</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Smerieri</keyname><forenames>Anteo</forenames></author><author><keyname>Vandoorne</keyname><forenames>Kristof</forenames></author><author><keyname>Bienstman</keyname><forenames>Peter</forenames></author><author><keyname>Haelterman</keyname><forenames>Marc</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author></authors><title>High performance photonic reservoir computer based on a coherently
  driven passive cavity</title><categories>physics.optics cs.ET</categories><comments>none</comments><msc-class>68T05</msc-class><journal-ref>Optica 2, Issue 5, pp. 438-446 (2015)</journal-ref><doi>10.1364/OPTICA.2.000438</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir computing is a recent bio-inspired approach for processing
time-dependent signals. It has enabled a breakthrough in analog information
processing, with several experiments, both electronic and optical,
demonstrating state-of-the-art performances for hard tasks such as speech
recognition, time series prediction and nonlinear channel equalization. A
proof-of-principle experiment using a linear optical circuit on a photonic chip
to process digital signals was recently reported. Here we present a photonic
implementation of a reservoir computer based on a coherently driven passive
fiber cavity processing analog signals. Our experiment has error rate as low or
lower than previous experiments on a wide variety of tasks, and also has lower
power consumption. Furthermore, the analytical model describing our experiment
is also of interest, as it constitutes a very simple high performance reservoir
computer algorithm. The present experiment, given its good performances, low
energy consumption and conceptual simplicity, confirms the great potential of
photonic reservoir computing for information processing applications ranging
from artificial intelligence to telecommunications
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03028</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03028</id><created>2015-01-13</created><updated>2015-11-03</updated><authors><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author><author><keyname>Tao</keyname><forenames>Jia</forenames></author></authors><title>Knowledge in Communication Networks</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates epistemic properties of information flow under
communication protocols with a given topological structure of the communication
network. The main result is a sound and complete logical system that describes
all such properties. The system consists of a variation of the multi-agent
epistemic logic S5 extended by a new network-specific Gateway axiom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03032</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03032</id><created>2015-01-13</created><updated>2015-03-09</updated><authors><author><keyname>Gospodarczyk</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Lewanowicz</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Wo&#x17a;ny</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>$G^{k,l}$-constrained multi-degree reduction of B\'ezier curves</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to the problem of $G^{k,l}$-constrained ($k,l \leq
3$) multi-degree reduction of B\'{e}zier curves with respect to the least
squares norm. First, to minimize the least squares error, we consider two
methods of determining the values of geometric continuity parameters. One of
them is based on quadratic and nonlinear programming, while the other uses some
simplifying assumptions and solves a system of linear equations. Next, for
prescribed values of these parameters, we obtain control points of the
multi-degree reduced curve, using the properties of constrained dual Bernstein
basis polynomials. Assuming that the input and output curves are of degree $n$
and $m$, respectively, we determine these points with the complexity $O(mn)$,
which is significantly less than the cost of other known methods. Finally, we
give several examples to demonstrate the effectiveness of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03043</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03043</id><created>2015-01-13</created><updated>2016-03-07</updated><authors><author><keyname>Ambroszkiewicz</keyname><forenames>Stanislaw</forenames></author></authors><title>Types and operations (version 3)</title><categories>math.LO cs.LO</categories><comments>In this version (March 7, 2016) only small error in Fig. 7 was
  removed. There is a companion paper (also on arXiv) on Continuum as a
  primitive type</comments><msc-class>03D</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A revision of the basic concepts of type, function (called here operation),
and relation is proposed. A simple generic method is presented for constructing
operations and types as concrete finite structures parameterized by natural
numbers. The method gives rise to build inductively so called Universe intended
to contain all what can be effectively constructed at least in the sense
assumed in the paper. It is argued that the Universe is not yet another formal
theory but may be considered as a grounding for some formal theories. The
paradigm that computations on higher order functionals can be done only
symbolically (by term rewriting) is challenged.
  Keywords: higher order objects, functionals, higher order recursion,
functional hardware description language, non-von Neumann programming language
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03044</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03044</id><created>2014-12-30</created><authors><author><keyname>Lu</keyname><forenames>Wei</forenames></author></authors><title>Effects of Data Resolution and Human Behavior on Large Scale Evacuation
  Simulations</title><categories>physics.soc-ph cs.CE</categories><comments>PhD dissertation. UT Knoxville. 130 pages, 37 figures, 8 tables.
  University of Tennessee, 2013. http://trace.tennessee.edu/utk_graddiss/2595</comments><msc-class>68-02</msc-class><acm-class>I.6.3</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Traffic Analysis Zones (TAZ) based macroscopic simulation studies are mostly
applied in evacuation planning and operation areas. The large size in TAZ and
aggregated information of macroscopic simulation underestimate the real
evacuation performance. To take advantage of the high resolution demographic
data LandScan USA (the zone size is much smaller than TAZ) and agent-based
microscopic traffic simulation models, many new problems appeared and novel
solutions are needed. A series of studies are conducted using LandScan USA
Population Cells (LPC) data for evacuation assignments with different network
configurations, travel demand models, and travelers compliance behavior.
  First, a new Multiple Source Nearest Destination Shortest Path (MSNDSP)
problem is defined for generating Origin Destination matrix in evacuation
assignments when using LandScan dataset. Second, a new agent-based traffic
assignment framework using LandScan and TRANSIMS modules is proposed for
evacuation planning and operation study. Impact analysis on traffic analysis
area resolutions (TAZ vs LPC), evacuation start times (daytime vs nighttime),
and departure time choice models (normal S shape model vs location based model)
are studied. Third, based on the proposed framework, multi-scale network
configurations (two levels of road networks and two scales of zone sizes) and
three routing schemes (shortest network distance, highway biased, and shortest
straight-line distance routes) are implemented for the evacuation performance
comparison studies. Fourth, to study the impact of human behavior under
evacuation operations, travelers compliance behavior with compliance levels
from total complied to total non-complied are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03049</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03049</id><created>2015-01-08</created><authors><author><keyname>Bramson</keyname><forenames>Aaron</forenames></author><author><keyname>Vandermarliere</keyname><forenames>Benjamin</forenames></author></authors><title>Dynamical Properties of Interaction Data</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><comments>29 pages, 15 figures</comments><doi>10.1093/comnet/cnv009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network dynamics are typically presented as a time series of network
properties captured at each period. The current approach examines the dynamical
properties of transmission via novel measures on an integrated, temporally
extended network representation of interaction data across time. Because it
encodes time and interactions as network connections, static network measures
can be applied to this &quot;temporal web&quot; to reveal features of the dynamics
themselves. Here we provide the technical details and apply it to agent-based
implementations of the well-known SEIR and SEIS epidemiological models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03056</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03056</id><created>2015-01-13</created><authors><author><keyname>Begelfor</keyname><forenames>Evgeni</forenames></author><author><keyname>Miller</keyname><forenames>Stephen D.</forenames></author><author><keyname>Venkatesan</keyname><forenames>Ramarathnam</forenames></author></authors><title>Non-Abelian Analogs of Lattice Rounding</title><categories>math.GR cs.CR math.CO math.NT</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice rounding in Euclidean space can be viewed as finding the nearest
point in the orbit of an action by a discrete group, relative to the norm
inherited from the ambient space. Using this point of view, we initiate the
study of non-abelian analogs of lattice rounding involving matrix groups. In
one direction, we give an algorithm for solving a normed word problem when the
inputs are random products over a basis set, and give theoretical justification
for its success. In another direction, we prove a general inapproximability
result which essentially rules out strong approximation algorithms (i.e., whose
approximation factors depend only on dimension) analogous to LLL in the general
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03058</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03058</id><created>2015-01-13</created><authors><author><keyname>Sidhu</keyname><forenames>Arbaaz Singh</forenames></author></authors><title>An Adaptive Neuro-Fuzzy Inference System Modeling for Grid-Adaptive
  Interpolation over Depth Images</title><categories>cs.CV</categories><comments>8 pages, 6 figures, International conference on Signal, Image
  Processing and Management (SPM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A suitable interpolation method is essential to keep the noise level minimum
along with the time-delay. In recent years, many different interpolation
filters have been developed for instance H.264-6 tap filter, and AVS- 4 tap
filter. The present work uses Adaptive Neuro-Fuzzy Inference System (ANFIS)
technique to model and investigate the effects of a four-tap low-pass tap
filter (Grid-adaptive filter) on a hole-filled depth image. The work
demonstrates the general form of uniform interpolations for both integer and
sub-pixel locations in terms of the sampling interval and filter length of
depth-images via diverse finite impulse response filtering schemes. The
demonstrated model combined modelling function of fuzzy inference with the
learning ability of artificial neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03063</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03063</id><created>2015-01-13</created><updated>2015-01-15</updated><authors><author><keyname>Tschannen</keyname><forenames>Julian</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Nordio</keyname><forenames>Martin</forenames></author><author><keyname>Polikarpova</keyname><forenames>Nadia</forenames></author></authors><title>AutoProof: Auto-active Functional Verification of Object-oriented
  Programs</title><categories>cs.LO</categories><journal-ref>Proceedings of the 21st International Conference on Tools and
  Algorithms for the Construction and Analysis of Systems (TACAS). Lecture
  Notes in Computer Science, 9035:566--580, Springer, April 2015</journal-ref><doi>10.1007/978-3-662-46681-0_53</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auto-active verifiers provide a level of automation intermediate between
fully automatic and interactive: users supply code with annotations as input
while benefiting from a high level of automation in the back-end. This paper
presents AutoProof, a state-of-the-art auto-active verifier for object-oriented
sequential programs with complex functional specifications. AutoProof fully
supports advanced object-oriented features and a powerful methodology for
framing and class invariants, which make it applicable in practice to idiomatic
object-oriented patterns. The paper focuses on describing AutoProof's
interface, design, and implementation features, and demonstrates AutoProof's
performance on a rich collection of benchmark problems. The results attest
AutoProof's competitiveness among tools in its league on cutting-edge
functional verification of object-oriented programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03064</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03064</id><created>2015-01-13</created><authors><author><keyname>Corbera</keyname><forenames>Francisco</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Asenjo</keyname><forenames>Rafael</forenames></author><author><keyname>Navarro</keyname><forenames>Angeles</forenames></author><author><keyname>Vilches</keyname><forenames>Antonio</forenames></author><author><keyname>Garzaran</keyname><forenames>Maria</forenames></author><author><keyname>Draa</keyname><forenames>Ismat Chaib</forenames></author><author><keyname>Tayeb</keyname><forenames>Jamel</forenames></author><author><keyname>Niar</keyname><forenames>Smail</forenames></author><author><keyname>Desertot</keyname><forenames>Mikael</forenames></author><author><keyname>Gregorek</keyname><forenames>Daniel</forenames></author><author><keyname>Schmidt</keyname><forenames>Robert</forenames></author><author><keyname>Garcia-Ortiz</keyname><forenames>Alberto</forenames></author><author><keyname>Lopez-Garcia</keyname><forenames>Pedro</forenames></author><author><keyname>Haemmerl&#xe9;</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Klemen</keyname><forenames>Maximiliano</forenames></author><author><keyname>Liqat</keyname><forenames>Umer</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author><author><keyname>Vav&#x159;&#xed;k</keyname><forenames>Radim</forenames></author><author><keyname>Sa&#xe0;-Garriga</keyname><forenames>Albert</forenames></author><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Carrabina</keyname><forenames>Jordi</forenames></author></authors><title>Proceedings of the Workshop on High Performance Energy Efficient
  Embedded Systems (HIP3ES) 2015</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Proceedings of the Workshop on High Performance Energy Efficient Embedded
Systems (HIP3ES) 2015. Amsterdam, January 21st. Collocated with HIPEAC 2015
Conference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03069</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03069</id><created>2015-01-13</created><updated>2015-02-06</updated><authors><author><keyname>Zhu</keyname><forenames>Xiatian</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Learning from Multiple Sources for Video Summarisation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many visual surveillance tasks, e.g.video summarisation, is conventionally
accomplished through analysing imagerybased features. Relying solely on visual
cues for public surveillance video understanding is unreliable, since visual
observations obtained from public space CCTV video data are often not
sufficiently trustworthy and events of interest can be subtle. On the other
hand, non-visual data sources such as weather reports and traffic sensory
signals are readily accessible but are not explored jointly to complement
visual data for video content analysis and summarisation. In this paper, we
present a novel unsupervised framework to learn jointly from both visual and
independently-drawn non-visual data sources for discovering meaningful latent
structure of surveillance video data. In particular, we investigate ways to
cope with discrepant dimension and representation whist associating these
heterogeneous data sources, and derive effective mechanism to tolerate with
missing and incomplete data from different sources. We show that the proposed
multi-source learning framework not only achieves better video content
clustering than state-of-the-art methods, but also is capable of accurately
inferring missing non-visual semantics from previously unseen videos. In
addition, a comprehensive user study is conducted to validate the quality of
video summarisation generated using the proposed multi-source model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03077</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03077</id><created>2015-01-13</created><authors><author><keyname>Everitt</keyname><forenames>Niklas</forenames></author><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>Variance Analysis of Linear SIMO Models with Spatially Correlated Noise</title><categories>cs.SY</categories><comments>Submitted to Automatica for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Substantial improvement in accuracy of identified linear time-invariant
single-input multi-output (SIMO) dynamical models is possible when the
disturbances affecting the output measurements are spatially correlated. Using
an orthogonal representation for the modules composing the SIMO structure, in
this paper we show that the variance of a parameter estimate of a module is
dependent on the model structure of the other modules, and the correlation
structure of the disturbances. In addition, we quantify the variance-error for
the parameter estimates for finite model orders, where the effect of noise
correlation structure, model structure and signal spectra are visible. From
these results, we derive the noise correlation structure under which the
mentioned model parameterization gives the lowest variance, when one module is
identified using less parameters than the other modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03084</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03084</id><created>2015-01-13</created><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author></authors><title>Deep Learning with Nonparametric Clustering</title><categories>cs.LG</categories><comments>14 pages, 6 figures</comments><msc-class>68T10</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Clustering is an essential problem in machine learning and data mining. One
vital factor that impacts clustering performance is how to learn or design the
data representation (or features). Fortunately, recent advances in deep
learning can learn unsupervised features effectively, and have yielded state of
the art performance in many classification problems, such as character
recognition, object recognition and document categorization. However, little
attention has been paid to the potential of deep learning for unsupervised
clustering problems. In this paper, we propose a deep belief network with
nonparametric clustering. As an unsupervised method, our model first leverages
the advantages of deep learning for feature representation and dimension
reduction. Then, it performs nonparametric clustering under a maximum margin
framework -- a discriminative clustering model and can be trained online
efficiently in the code space. Lastly model parameters are refined in the deep
belief network. Thus, this model can learn features for clustering and infer
model complexity in an unified framework. The experimental results show the
advantage of our approach over competitive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03093</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03093</id><created>2015-01-13</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Forejt</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Ku&#x10d;era</keyname><forenames>Anton&#xed;n</forenames></author></authors><title>MultiGain: A controller synthesis tool for MDPs with multiple
  mean-payoff objectives</title><categories>cs.AI cs.LO</categories><comments>Extended version for a TACAS 2015 tool demo paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present MultiGain, a tool to synthesize strategies for Markov decision
processes (MDPs) with multiple mean-payoff objectives. Our models are described
in PRISM, and our tool uses the existing interface and simulator of PRISM. Our
tool extends PRISM by adding novel algorithms for multiple mean-payoff
objectives, and also provides features such as (i)~generating strategies and
exploring them for simulation, and checking them with respect to other
properties; and (ii)~generating an approximate Pareto curve for two mean-payoff
objectives. In addition, we present a new practical algorithm for the analysis
of MDPs with multiple mean-payoff objectives under memoryless strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03100</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03100</id><created>2015-01-13</created><updated>2015-04-28</updated><authors><author><keyname>Pas</keyname><forenames>Andreas ten</forenames></author><author><keyname>Platt</keyname><forenames>Robert</forenames></author></authors><title>Using Geometry to Detect Grasps in 3D Point Clouds</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new approach to detecting grasp points on novel objects
presented in clutter. The input to our algorithm is a point cloud and the
geometric parameters of the robot hand. The output is a set of hand
configurations that are expected to be good grasps. Our key idea is to use
knowledge of the geometry of a good grasp to improve detection. First, we use a
geometrically necessary condition to sample a large set of high quality grasp
hypotheses. We were surprised to find that using simple geometric conditions
for detection can result in a relatively high grasp success rate. Second, we
use the notion of an antipodal grasp (a standard characterization of a good two
fingered grasp) to help us classify these grasp hypotheses. In particular, we
generate a large automatically labeled training set that gives us high
classification accuracy. Overall, our method achieves an average grasp success
rate of 88% when grasping novels objects presented in isolation and an average
success rate of 73% when grasping novel objects presented in dense clutter.
This system is available as a ROS package at http://wiki.ros.org/agile_grasp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03105</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03105</id><created>2015-01-13</created><authors><author><keyname>Zhu</keyname><forenames>Yao</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>A Parallel Min-Cut Algorithm using Iteratively Reweighted Least Squares</title><categories>cs.DC cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel algorithm for the undirected $s,t$-mincut problem with
floating-point valued weights. Our overarching algorithm uses an iteratively
reweighted least squares framework. This generates a sequence of Laplacian
linear systems, which we solve using parallel matrix algorithms. Our overall
implementation is up to 30-times faster than a serial solver when using 128
cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03116</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03116</id><created>2015-01-12</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Graphs with Eulerian unit spheres</title><categories>math.CO cs.DM</categories><comments>44 pages, 17 figures</comments><msc-class>05C15, 05C10, 57M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  d-spheres in graph theory are inductively defined as graphs for which all
unit spheres S(x) are (d-1)-spheres and that the removal of one vertex renders
the graph contractible. Eulerian d-spheres are geometric d-spheres which are
d+1 colorable. We prove here that G is an Eulerian sphere if and only if the
degrees of all the (d-2)-dimensional sub-simplices in G are even. This
generalizes a Kempe-Heawood result for d=2 and is work related to the
conjecture that all d-spheres have chromatic number d+1 or d+2 which is based
on the geometric conjecture that every d-sphere can be embedded in an Eulerian
(d+1)-sphere. For d=2, such an embedding into an Eulerian 3-sphere would lead
to a geometric proof of the 4 color theorem, allowing to see &quot;why 4 colors
suffice&quot;. To achieve the goal of coloring a d-sphere G with d+2 colors, we hope
to embed it into a (d+1)-sphere and refine or thin out the later using special
homotopy deformations without touching the embedded sphere. Once rendered
Eulerian and so (d+2)-colorable, it colors the embedded graph G. In order to
define the degree of a simplex, we introduce a notion of dual graph H' of a
subgraph H in a general finite simple graph G. This leads to a natural sphere
bundle over the simplex graph. We look at geometric graphs which admit a unique
geodesic flow: their unit spheres must be Eulerian. We define Platonic spheres
graph theoretically as d-spheres for which all unit spheres S(x) are graph
isomorphic Platonic (d-1)-spheres. Gauss-Bonnet allows a classification within
graph theory: all spheres are Platonic for d=1, the octahedron and icosahedron
are the Platonic 2-spheres, the sixteen and six-hundred cells are the Platonic
3-spheres. The cross polytop is the unique Platonic d-sphere for d&gt;3. It is
Eulerian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03124</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03124</id><created>2015-01-13</created><authors><author><keyname>Dubey</keyname><forenames>Amartansh</forenames></author><author><keyname>Bhurchandi</keyname><forenames>K. M.</forenames></author></authors><title>Robust and Real Time Detection of Curvy Lanes (Curves) with Desired
  Slopes for Driving Assistance and Autonomous Vehicles</title><categories>cs.CV</categories><comments>13 pages, 12 figures, published in International Conference on Signal
  and Image Processing (AIRCC Publishing Corporation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest reasons for road accidents is curvy lanes and blind turns.
Even one of the biggest hurdles for new autonomous vehicles is to detect curvy
lanes, multiple lanes and lanes with a lot of discontinuity and noise. This
paper presents very efficient and advanced algorithm for detecting curves
having desired slopes (especially for detecting curvy lanes in real time) and
detection of curves (lanes) with a lot of noise, discontinuity and
disturbances. Overall aim is to develop robust method for this task which is
applicable even in adverse conditions. Even in some of most famous and useful
libraries like OpenCV and Matlab, there is no function available for detecting
curves having desired slopes , shapes, discontinuities. Only few predefined
shapes like circle, ellipse, etc, can be detected using presently available
functions. Proposed algorithm can not only detect curves with discontinuity,
noise, desired slope but also it can perform shadow and illumination correction
and detect/ differentiate between different curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03139</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03139</id><created>2015-01-13</created><authors><author><keyname>Duarte</keyname><forenames>Eduardo</forenames></author><author><keyname>Pinheiro</keyname><forenames>Filipe</forenames></author><author><keyname>Z&#xfa;quete</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Gomes</keyname><forenames>H&#xe9;lder</forenames></author></authors><title>Secure and trustworthy file sharing over cloud storage using eID tokens</title><categories>cs.CR</categories><comments>12 pages, 1 figure, submitted and presented at OID conference 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a multi-platform, open-source application that aims to
protect data stored and shared in existing cloud storage services. The access
to the cryptographic material used to protect data is implemented using the
identification and authentication functionalities of national electronic
identity (eID) tokens. All peer to peer dialogs to exchange cryptographic
material is implemented using the cloud storage facilities. Furthermore, we
have included a set of mechanisms to prevent files from being permanently lost
or damaged due to concurrent modification, deletion and malicious tampering. We
have implemented a prototype in Java that is agnostic relatively to cloud
storage providers; it only manages local folders, one of them being the local
image of a cloud folder. We have successfully tested our prototype in Windows,
Mac OS X and Linux, with Dropbox, OneDrive, Google Drive and SugarSync.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03188</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03188</id><created>2015-01-13</created><authors><author><keyname>Hubers</keyname><forenames>Alexander</forenames></author><author><keyname>Andrulis</keyname><forenames>Emily</forenames></author><author><keyname>Scott</keyname><forenames>Levi</forenames></author><author><keyname>Stirrat</keyname><forenames>Tanner</forenames></author><author><keyname>Tran</keyname><forenames>Duc</forenames></author><author><keyname>Zhang</keyname><forenames>Ruonan</forenames></author><author><keyname>Sowell</keyname><forenames>Ross</forenames></author><author><keyname>Grimm</keyname><forenames>Cindy</forenames></author><author><keyname>Smart</keyname><forenames>William D.</forenames></author></authors><title>Video Manipulation Techniques for the Protection of Privacy in Remote
  Presence Systems</title><categories>cs.RO cs.CR cs.CY</categories><comments>14 pages, 8 figures</comments><acm-class>H.5.2; I.2.9; I.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems that give control of a mobile robot to a remote user raise privacy
concerns about what the remote user can see and do through the robot. We aim to
preserve some of that privacy by manipulating the video data that the remote
user sees. Through two user studies, we explore the effectiveness of different
video manipulation techniques at providing different types of privacy. We
simultaneously examine task performance in the presence of privacy protection.
In the first study, participants were asked to watch a video captured by a
robot exploring an office environment and to complete a series of observational
tasks under differing video manipulation conditions. Our results show that
using manipulations of the video stream can lead to fewer privacy violations
for different privacy types. Through a second user study, it was demonstrated
that these privacy-protecting techniques were effective without diminishing the
task performance of the remote user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03191</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03191</id><created>2015-01-13</created><authors><author><keyname>Mericli</keyname><forenames>Benjamin S.</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author></authors><title>Annotating Cognates and Etymological Origin in Turkic Languages</title><categories>cs.CL</categories><comments>5 pages, 8 tables; appeared in Proceedings of the First Workshop on
  Language Resources and Technologies for Turkic Languages at the Eighth
  International Conference on Language Resources and Evaluation (LREC'12),
  pages 47-51, Istanbul, Turkey, May 2012. European Language Resources
  Association</comments><acm-class>I.2.7</acm-class><journal-ref>In Proceedings of the First Workshop on Language Resources and
  Technologies for Turkic Languages at LREC'12, pages 47-51, Istanbul, Turkey,
  May 2012. European Language Resources Association</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turkic languages exhibit extensive and diverse etymological relationships
among lexical items. These relationships make the Turkic languages promising
for exploring automated translation lexicon induction by leveraging cognate and
other etymological information. However, due to the extent and diversity of the
types of relationships between words, it is not clear how to annotate such
information. In this paper, we present a methodology for annotating cognates
and etymological origin in Turkic languages. Our method strives to balance the
amount of research effort the annotator expends with the utility of the
annotations for supporting research on improving automated translation lexicon
induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03194</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03194</id><created>2015-01-13</created><updated>2015-11-25</updated><authors><author><keyname>Ramezanali</keyname><forenames>Mohammad</forenames></author><author><keyname>Mitra</keyname><forenames>Partha P.</forenames></author><author><keyname>Sengupta</keyname><forenames>Anirvan M.</forenames></author></authors><title>The cavity method for analysis of large-scale penalized regression</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>15 pages, 7 figures, new version focusing only on cavity method,
  title change</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Penalized regression methods aim to retrieve reliable predictors among a
large set of putative ones from a limited amount of measurements. In
particular, penalized regression with singular penalty functions is important
for sparse reconstruction algorithms. For large-scale problems, these
algorithms exhibit sharp phase transition boundaries where sparse retrieval
breaks down. Large optimization problems associated with sparse reconstruction
have been analyzed in the literature by setting up corresponding statistical
mechanical models at a finite temperature. Using replica method for mean field
approximation, and subsequently taking a zero temperature limit, this approach
reproduces the algorithmic phase transition boundaries. Unfortunately, the
replica trick and the non-trivial zero temperature limit obscure the underlying
reasons for the failure of a sparse reconstruction algorithm, and of penalized
regression methods, in general. In this paper, we employ the ``cavity method''
to give an alternative derivation of the mean field equations, working directly
in the zero-temperature limit. This derivation provides insight into the origin
of the different terms in the self-consistency conditions. The cavity method
naturally involves a quantity, the average local susceptibility, whose behavior
distinguishes different phases in this system. This susceptibility can be
generalized for analysis of a broader class of sparse reconstruction
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03196</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03196</id><created>2015-01-13</created><authors><author><keyname>Le</keyname><forenames>Tuan-Anh</forenames></author><author><keyname>Bui</keyname><forenames>Loc X.</forenames></author></authors><title>Forward Delay-based Packet Scheduling Algorithm for Multipath TCP</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipath TCP (MPTCP) is a transport layer protocol that allows network
devices to transfer data over multiple concurrent paths, and hence, utilizes
the network resources more effectively than does the traditional single-path
TCP. However, as a reliable protocol, MPTCP still needs to deliver data packets
(to the upper application) at the receiver in the same order they are
transmitted at the sender. The out-of-order packet problem becomes more severe
for MPTCP due to the heterogeneous nature of delay and bandwidth of each path.
In this paper, we propose the forward-delay-based packet scheduling (FDPS)
algorithm for MPTCP to address that problem. The main idea is that the sender
dispatches packets via concurrent paths according to their estimated forward
delay and throughput differences. Via simulations with various network
conditions, the results show that our algorithm significantly maintains
in-order arrival packets at the receiver compared with several previous
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03208</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03208</id><created>2015-01-13</created><updated>2015-06-08</updated><authors><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Compressive Sensing with Redundant Dictionaries and Structured
  Measurements</title><categories>cs.IT math.IT math.NA</categories><msc-class>94A12, 41A45, 42A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of recovering an unknown signal from undersampled
measurements, given the knowledge that the signal has a sparse representation
in a specified dictionary $D$. This problem is now understood to be well-posed
and efficiently solvable under suitable assumptions on the measurements and
dictionary, if the number of measurements scales roughly with the sparsity
level. One sufficient condition for such is the $D$-restricted isometry
property ($D$-RIP), which asks that the sampling matrix approximately preserve
the norm of all signals which are sufficiently sparse in $D$. While many
classes of random matrices are known to satisfy such conditions, such matrices
are not representative of the structural constraints imposed by practical
sensing systems. We close this gap in the theory by demonstrating that one can
subsample a fixed orthogonal matrix in such a way that the $D$-RIP will hold,
provided this basis is sufficiently incoherent with the sparsifying dictionary
$D$. We also extend this analysis to allow for weighted sparse expansions.
Consequently, we arrive at compressive sensing recovery guarantees for
structured measurements and redundant dictionaries, opening the door to a wide
array of practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03209</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03209</id><created>2015-01-13</created><authors><author><keyname>Kharratzadeh</keyname><forenames>Milad</forenames></author><author><keyname>Shultz</keyname><forenames>Thomas R.</forenames></author></authors><title>Neural Implementation of Probabilistic Models of Cognition</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian models of cognition hypothesize that human brains make sense of data
by representing probability distributions and applying Bayes' rule to find the
best explanation for any given data. Understanding the neural mechanisms
underlying probabilistic models remains important because Bayesian models
essentially provide a computational framework, rather than specifying processes
at the algorithmic level. Here, we propose a constructive neural-network model
which estimates and represents probability distributions from observable events
--- a phenomenon related to the concept of probability matching. We use a form
of operant learning, where the underlying probabilities are learned from
positive and negative reinforcements of inputs. Our model is psychologically
plausible because, similar to humans, it learns to represent probabilities
without receiving any representation of them from the external world, but
rather by experiencing individual events. Moreover, we show that our neural
implementation of probability matching can be paired with a neural module
applying Bayes' rule, forming a comprehensive neural scheme to simulate human
Bayesian learning and inference. Our model also provides novel explanations of
several deviations from Bayes, including base-rate neglect and overweighting of
rare events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03210</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03210</id><created>2015-01-13</created><authors><author><keyname>Bansal</keyname><forenames>Piyush</forenames></author><author><keyname>Bansal</keyname><forenames>Romil</forenames></author><author><keyname>Varma</keyname><forenames>Vasudeva</forenames></author></authors><title>Towards Deep Semantic Analysis Of Hashtags</title><categories>cs.IR cs.CL</categories><comments>To Appear in 37th European Conference on Information Retrieval</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hashtags are semantico-syntactic constructs used across various social
networking and microblogging platforms to enable users to start a topic
specific discussion or classify a post into a desired category. Segmenting and
linking the entities present within the hashtags could therefore help in better
understanding and extraction of information shared across the social media.
However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden),
the segmentation of hashtags into constituent entities (&quot;NSA&quot; and &quot;Edward
Snowden&quot; in this case) is not a trivial task. Most of the current
state-of-the-art social media analytics systems like Sentiment Analysis and
Entity Linking tend to either ignore hashtags, or treat them as a single word.
In this paper, we present a context aware approach to segment and link entities
in the hashtags to a knowledge base (KB) entry, based on the context within the
tweet. Our approach segments and links the entities in hashtags such that the
coherence between hashtag semantics and the tweet is maximized. To the best of
our knowledge, no existing study addresses the issue of linking entities in
hashtags for extracting semantic information. We evaluate our method on two
different datasets, and demonstrate the effectiveness of our technique in
improving the overall entity linking in tweets via additional semantic
information provided by segmenting and linking entities in a hashtag.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03214</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03214</id><created>2015-01-13</created><authors><author><keyname>Bilisoly</keyname><forenames>Roger</forenames></author></authors><title>Quantifying Prosodic Variability in Middle English Alliterative Poetry</title><categories>stat.AP cs.CL</categories><comments>12 pages, 8 figures. Based on a presentation given at the Joint
  Statistical Meetings, Section on Statistical Learning and Data Mining, which
  took place August, 2014, in Boston, Massachusetts, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interest in the mathematical structure of poetry dates back to at least the
19th century: after retiring from his mathematics position, J. J. Sylvester
wrote a book on prosody called $\textit{The Laws of Verse}$. Today there is
interest in the computer analysis of poems, and this paper discusses how a
statistical approach can be applied to this task. Starting with the definition
of what Middle English alliteration is, $\textit{Sir Gawain and the Green
Knight}$ and William Langland's $\textit{Piers Plowman}$ are used to illustrate
the methodology. Theory first developed for analyzing data from a Riemannian
manifold turns out to be applicable to strings allowing one to compute a
generalized mean and variance for textual data, which is applied to the poems
above. The ratio of these two variances produces the analogue of the F test,
and resampling allows p-values to be estimated. Consequently, this methodology
provides a way to compare prosodic variability between two texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03218</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03218</id><created>2015-01-13</created><authors><author><keyname>Ates</keyname><forenames>Halim Cagri</forenames></author><author><keyname>Apostolopoulos</keyname><forenames>Ilias</forenames></author><author><keyname>Folmer</keyname><forenames>Eelke</forenames></author></authors><title>Expanding the Vocabulary of Multitouch Input using Magnetic Fingerprints</title><categories>cs.HC</categories><comments>8 pages, 8 figures</comments><acm-class>H.5.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present magnetic fingerprints; an input technique for mobile touchscreen
devices that uses a small magnet attached to a user's fingernail in order to
differentiate between a normal touch and a magnetic touch. The polarity of the
magnet can be used to create different magnetic fingerprints where this
technique takes advantage of the rich vocabulary offered by the use of
multitouch input. User studies investigate the accuracy of magnetic fingerprint
recognition in relation to magnet size, number of magnetic fingerprints used;
and size of the touchscreen. Studies found our technique to be limited to using
up to two fingerprints non-simultaneously, while achieving a high
classification accuracy (95%) but it nearly triples the number of
distinguishable multi touch events. Potential useful applications of this
technique are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03227</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03227</id><created>2015-01-13</created><updated>2015-02-24</updated><authors><author><keyname>Kalunga</keyname><forenames>Emmanuel K.</forenames></author><author><keyname>Chevallier</keyname><forenames>Sylvain</forenames></author><author><keyname>Barthelemy</keyname><forenames>Quentin</forenames></author></authors><title>Using Riemannian geometry for SSVEP-based Brain Computer Interface</title><categories>cs.LG stat.ML</categories><comments>29 pages, 6 figures, 1 table, research report. Update on the overall
  text, most of the figure are modified, the algorithm is explained more
  clearly, updated comparisons with state of the art methods</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Riemannian geometry has been applied to Brain Computer Interface (BCI) for
brain signals classification yielding promising results. Studying
electroencephalographic (EEG) signals from their associated covariance matrices
allows a mitigation of common sources of variability (electronic, electrical,
biological) by constructing a representation which is invariant to these
perturbations. While working in Euclidean space with covariance matrices is
known to be error-prone, one might take advantage of algorithmic advances in
information geometry and matrix manifold to implement methods for Symmetric
Positive-Definite (SPD) matrices. This paper proposes a comprehensive review of
the actual tools of information geometry and how they could be applied on
covariance matrices of EEG. In practice, covariance matrices should be
estimated, thus a thorough study of all estimators is conducted on real EEG
dataset. As a main contribution, this paper proposes an online implementation
of a classifier in the Riemannian space and its subsequent assessment in
Steady-State Visually Evoked Potential (SSVEP) experimentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03235</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03235</id><created>2015-01-13</created><authors><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Successive Cancellation Decoding of Polar Codes using Stochastic
  Computing</title><categories>cs.IT math.IT</categories><comments>accepted by International Symposium on Circuits and Systems (ISCAS)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes have emerged as the most favorable channel codes for their unique
capacity-achieving property. To date, numerous works have been reported for
efficient design of polar codes decoder. However, these prior efforts focused
on design of polar decoders via deterministic computation, while the behavior
of stochastic polar decoder, which can have potential advantages such as low
complexity and strong error-resilience, has not been studied in existing
literatures. This paper, for the first time, investigates polar decoding using
stochastic logic. Specifically, the commonly-used successive cancellation (SC)
algorithm is reformulated into the stochastic form. Several methods that can
potentially improve decoding performance are discussed and analyzed. Simulation
results show that a stochastic SC decoder can achieve similar error-correcting
performance as its deterministic counterpart. This work can pave the way for
future hardware design of stochastic polar codes decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03241</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03241</id><created>2015-01-13</created><authors><author><keyname>Awadi</keyname><forenames>Aymen Hasan Rashid Al</forenames></author><author><keyname>Belaton</keyname><forenames>Bahari</forenames></author></authors><title>Multi-phase IRC Botnet and Botnet Behavior Detection Model</title><categories>cs.CR</categories><comments>10 pages, Journal paper</comments><journal-ref>International Journal of Computer Applications 66(15):41-51, March
  2013</journal-ref><doi>10.5120/11164-6289</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Botnets are considered one of the most dangerous and serious security threats
facing the networks and the Internet. Comparing with the other security
threats, botnet members have the ability to be directed and controlled via C&amp;C
messages from the botmaster over common protocols such as IRC and HTTP, or even
over covert and unknown applications. As for IRC botnets, general security
instances like firewalls and IDSes do not provide by themselves a viable
solution to prevent them completely. These devices could not differentiate well
between the legitimate and malicious traffic of the IRC protocol. So, this
paper is proposing an IDS-based and multi-phase IRC botnet and botnet behavior
detection model based on C&amp;C responses messages and malicious behaviors of the
IRC bots inside the network environment. The proposed model has been evaluated
on five network traffic traces from two different network environments (Virtual
network and DARPA 2000 Windows NT Attack Data Set). The results show that the
proposed model could detect all the infected IRC botnet member(s), state their
current status of attack, filter their malicious IRC messages, pass the other
normal IRC messages and detect the botnet behavior regardless of the botnet
communication protocol with very low false positive rate. The proposed model
has been compared with some of the existing and well-known approaches,
including BotHunter, BotSniffer and Rishi regarding botnet characteristics
taken in each approach. The comparison showed that the proposed model has made
a progress on the comparative models by not to rely on a certain time window or
specific bot signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03246</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03246</id><created>2015-01-13</created><authors><author><keyname>Bus</keyname><forenames>Norbert</forenames></author><author><keyname>Garg</keyname><forenames>Shashwat</forenames></author><author><keyname>Mustafa</keyname><forenames>Nabil H.</forenames></author><author><keyname>Ray</keyname><forenames>Saurabh</forenames></author></authors><title>Tighter Estimates for epsilon-nets for Disks</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The geometric hitting set problem is one of the basic geometric combinatorial
optimization problems: given a set $P$ of points, and a set $\mathcal{D}$ of
geometric objects in the plane, the goal is to compute a small-sized subset of
$P$ that hits all objects in $\mathcal{D}$. In 1994, Bronniman and Goodrich
made an important connection of this problem to the size of fundamental
combinatorial structures called $\epsilon$-nets, showing that small-sized
$\epsilon$-nets imply approximation algorithms with correspondingly small
approximation ratios. Very recently, Agarwal and Pan showed that their scheme
can be implemented in near-linear time for disks in the plane. Altogether this
gives $O(1)$-factor approximation algorithms in $\tilde{O}(n)$ time for hitting
sets for disks in the plane.
  This constant factor depends on the sizes of $\epsilon$-nets for disks;
unfortunately, the current state-of-the-art bounds are large -- at least
$24/\epsilon$ and most likely larger than $40/\epsilon$. Thus the approximation
factor of the Agarwal and Pan algorithm ends up being more than $40$. The best
lower-bound is $2/\epsilon$, which follows from the Pach-Woeginger construction
for halfspaces in two dimensions. Thus there is a large gap between the
best-known upper and lower bounds. Besides being of independent interest,
finding precise bounds is important since this immediately implies an improved
linear-time algorithm for the hitting-set problem.
  The main goal of this paper is to improve the upper-bound to $13.4/\epsilon$
for disks in the plane. The proof is constructive, giving a simple algorithm
that uses only Delaunay triangulations. We have implemented the algorithm,
which is available as a public open-source module. Experimental results show
that the sizes of $\epsilon$-nets for a variety of data-sets is lower, around
$9/\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03268</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03268</id><created>2015-01-14</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author></authors><title>Progress, Fairness and Justness in Process Algebra</title><categories>cs.LO</categories><acm-class>F.1.2; F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To prove liveness properties of concurrent systems, it is often necessary to
postulate progress, fairness and justness properties. This paper investigates
how the necessary progress, fairness and justness assumptions can be added to
or incorporated in a standard process-algebraic specification formalism. We
propose a formalisation that can be applied to a wide range of process
algebras. The presented formalism is used to reason about route discovery and
packet delivery in the setting of wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03271</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03271</id><created>2015-01-14</created><authors><author><keyname>Paul</keyname><forenames>Joseph Suresh</forenames></author><author><keyname>Pillai</keyname><forenames>Uma Krishna Swamy</forenames></author></authors><title>Higher dimensional homodyne filtering for suppression of incidental
  phase artifacts in multichannel MRI</title><categories>cs.CV physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to introduce procedural steps for extension of the
1D homodyne phase correction for k-space truncation in all gradient encoding
directions. Compared to the existing method applied to 2D partial k-space,
signal losses introduced by the phase correction filter is observed to be
minimal for the extended approach. In addition, the modified form of phase
correction mitigates Incidental Phase Artifacts (IPA) due to truncation. For
parallel imaging with undersampling along phase encode direction, the extended
homodyne filtering is shown to be effective for minimizing these artifacts when
each of the channel k-spaces are truncated along both phase and frequency
encode directions. This is illustrated with 2D partial k-space for flow
compensated multichannel Susceptibility Weighted Imaging (SWI). Extension of
our method to 3D partial k-space shows improved reconstruction of flow
information in phase contrast angiography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03273</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03273</id><created>2015-01-14</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Livni</keyname><forenames>Roi</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Classification with Low Rank and Missing Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider classification and regression tasks where we have missing data
and assume that the (clean) data resides in a low rank subspace. Finding a
hidden subspace is known to be computationally hard. Nevertheless, using a
non-proper formulation we give an efficient agnostic algorithm that classifies
as good as the best linear classifier coupled with the best low-dimensional
subspace in which the data resides. A direct implication is that our algorithm
can linearly (and non-linearly through kernels) classify provably as well as
the best classifier that has access to the full data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03293</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03293</id><created>2015-01-14</created><updated>2015-04-17</updated><authors><author><keyname>Clouston</keyname><forenames>Ranald</forenames></author><author><keyname>Gor&#xe9;</keyname><forenames>Rajeev</forenames></author></authors><title>Sequent Calculus in the Topos of Trees</title><categories>cs.LO</categories><comments>Extended version, with full proof details, of a paper accepted to
  FoSSaCS 2015 (this version edited to fix some minor typos)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nakano's &quot;later&quot; modality, inspired by G\&quot;{o}del-L\&quot;{o}b provability logic,
has been applied in type systems and program logics to capture guarded
recursion. Birkedal et al modelled this modality via the internal logic of the
topos of trees. We show that the semantics of the propositional fragment of
this logic can be given by linear converse-well-founded intuitionistic Kripke
frames, so this logic is a marriage of the intuitionistic modal logic KM and
the intermediate logic LC. We therefore call this logic
$\mathrm{KM}_{\mathrm{lin}}$. We give a sound and cut-free complete sequent
calculus for $\mathrm{KM}_{\mathrm{lin}}$ via a strategy that decomposes
implication into its static and irreflexive components. Our calculus provides
deterministic and terminating backward proof-search, yields decidability of the
logic and the coNP-completeness of its validity problem. Our calculus and
decision procedure can be restricted to drop linearity and hence capture KM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03300</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03300</id><created>2015-01-14</created><authors><author><keyname>Martinelli</keyname><forenames>Agostino</forenames></author></authors><title>Complete analytic solution to Brownian unicycle dynamics</title><categories>cs.RO</categories><comments>22 pages, 6 figures, 2 tables</comments><doi>10.1088/1742-5468/2014/03/P03003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives a complete analytical solution for the probability
distribution of the configuration of a non-holonomic vehicle that moves in two
spatial dimensions by satisfying the unicycle kinematic constraints and in
presence of Brownian noises. In contrast to previous solutions, the one here
derived holds even in the case of arbitrary linear and angular speed. This
solution is obtained by deriving the analytical expression of any-order moment
of the probability distribution. To the best of our knowledge, an analytical
expression for any-order moment that holds even in the case of arbitrary linear
and angular speed, has never been derived before. To compute these moments, a
direct integration of the Langevin equation is carried out and each moment is
expressed as a multiple integral of the deterministic motion (i.e., the known
motion that would result in absence of noise). For the special case when the
ratio between the linear and angular speed is constant, the multiple integrals
can be easily solved and expressed as the real or the imaginary part of
suitable analytic functions. As an application of the derived analytical
results, the paper investigates the diffusivity of the considered Brownian
motion for constant and for arbitrary time-dependent linear and angular speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03302</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03302</id><created>2015-01-14</created><updated>2015-01-15</updated><authors><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>Hard to Cheat: A Turing Test based on Answering Questions about Images</title><categories>cs.AI cs.CL cs.CV cs.LG</categories><comments>Presented in AAAI-15 Workshop: Beyond the Turing Test</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Progress in language and image understanding by machines has sparkled the
interest of the research community in more open-ended, holistic tasks, and
refueled an old AI dream of building intelligent machines. We discuss a few
prominent challenges that characterize such holistic tasks and argue for
&quot;question answering about images&quot; as a particular appealing instance of such a
holistic task. In particular, we point out that it is a version of a Turing
Test that is likely to be more robust to over-interpretations and contrast it
with tasks like grounding and generation of descriptions. Finally, we discuss
tools to measure progress in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03307</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03307</id><created>2015-01-14</created><authors><author><keyname>Jones</keyname><forenames>Andrew L.</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author><author><keyname>Tassi</keyname><forenames>Andrea</forenames></author></authors><title>Binary Systematic Network Coding for Progressive Packet Decoding</title><categories>cs.IT cs.MM cs.PF math.IT</categories><comments>Proc. of IEEE ICC 2015 - Communication Theory Symposium, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider binary systematic network codes and investigate their capability
of decoding a source message either in full or in part. We carry out a
probability analysis, derive closed-form expressions for the decoding
probability and show that systematic network coding outperforms conventional
network coding. We also develop an algorithm based on Gaussian elimination that
allows progressive decoding of source packets. Simulation results show that the
proposed decoding algorithm can achieve the theoretical optimal performance.
Furthermore, we demonstrate that systematic network codes equipped with the
proposed algorithm are good candidates for progressive packet recovery owing to
their overall decoding delay characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03310</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03310</id><created>2015-01-14</created><authors><author><keyname>Carl&#xe0;</keyname><forenames>Lorenzo</forenames></author><author><keyname>Chiti</keyname><forenames>Francesco</forenames></author><author><keyname>Fantacci</keyname><forenames>Romano</forenames></author><author><keyname>Tassi</keyname><forenames>Andrea</forenames></author></authors><title>Sleep Period Optimization Model For Layered Video Service Delivery Over
  eMBMS Networks</title><categories>cs.NI cs.IT cs.MM cs.PF math.IT</categories><comments>Proc. of IEEE ICC 2015, Selected Areas in Communications Symposium -
  Green Communications Track, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long Term Evolution-Advanced (LTE-A) and the evolved Multimedia Broadcast
Multicast System (eMBMS) are the most promising technologies for the delivery
of highly bandwidth demanding applications. In this paper we propose a green
resource allocation strategy for the delivery of layered video streams to users
with different propagation conditions. The goal of the proposed model is to
minimize the user energy consumption. That goal is achieved by minimizing the
time required by each user to receive the broadcast data via an efficient power
transmission allocation model. A key point in our system model is that the
reliability of layered video communications is ensured by means of the Random
Linear Network Coding (RLNC) approach. Analytical results show that the
proposed resource allocation model ensures the desired quality of service
constraints, while the user energy footprint is significantly reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03311</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03311</id><created>2015-01-14</created><updated>2015-01-20</updated><authors><author><keyname>Tassi</keyname><forenames>Andrea</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author><author><keyname>Jones</keyname><forenames>Andrew L.</forenames></author></authors><title>Optimized Network-coded Scalable Video Multicasting over eMBMS Networks</title><categories>cs.IT cs.MM cs.NI cs.PF math.IT</categories><comments>Proc. of IEEE ICC 2015 - Mobile and Wireless Networking Symposium, to
  appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delivery of multicast video services over fourth generation (4G) networks
such as 3GPP Long Term Evolution-Advanced (LTE-A) is gaining momentum. In this
paper, we address the issue of efficiently multicasting layered video services
by defining a novel resource allocation framework that aims to maximize the
service coverage whilst keeping the radio resource footprint low. A key point
in the proposed system mode is that the reliability of multicast video services
is ensured by means of an Unequal Error Protection implementation of the
Network Coding (UEP-NC) scheme. In addition, both the communication parameters
and the UEP-NC scheme are jointly optimized by the proposed resource allocation
framework. Numerical results show that the proposed allocation framework can
significantly increase the service coverage when compared to a conventional
Multi-rate Transmission (MrT) strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03320</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03320</id><created>2015-01-14</created><authors><author><keyname>Akshara</keyname><forenames>P. K.</forenames></author><author><keyname>Paul</keyname><forenames>J. S.</forenames></author></authors><title>Image enhancement in intensity projected multichannel MRI using
  spatially adaptive directional anisotropic diffusion</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anisotropic Diffusion is widely used for noise reduction with simultaneous
preservation of vascular structures in maximum intensity projected (MIP)
angiograms. However, extension to minimum intensity projected (mIP) venograms
in Susceptibility Weighted Imaging (SWI) poses difficulties due to spatially
varying baseline. Here, we introduce a modified version of the directional
anisotropic diffusion which allows us to simultaneously reduce the noise and
enhance vascular structures reconstructed using both M/mIP angiograms. This
method is based on spatial adaptation of the diffusion function, separately in
the directions of the gradient, and along those of the minimum and maximum
curvatures. The existing approach of directional anisotropic diffusion uses
binary switched diffusion function to ensure diffusion along the direction of
maximum curvature stopped near the vessel borders. Here, the choice of a
threshold for detecting the upper limit of diffusion becomes difficult in the
presence of spatially varying baseline. Also, the approach of using vesselness
measure to steer the diffusion process results in structural discontinuities
due to junction suppression in mIP. The merits of the proposed method include
elimination of the need for an apriori choice of a threshold to detect the
vessel, and problems due to junction suppression. The proposed method is also
extended to multi-channel phase contrast angiogram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03326</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03326</id><created>2015-01-14</created><updated>2015-02-09</updated><authors><author><keyname>Strathmann</keyname><forenames>Heiko</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Girolami</keyname><forenames>Mark</forenames></author></authors><title>Unbiased Bayes for Big Data: Paths of Partial Posteriors</title><categories>stat.ML cs.LG stat.ME</categories><comments>18 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key quantity of interest in Bayesian inference are expectations of
functions with respect to a posterior distribution. Markov Chain Monte Carlo is
a fundamental tool to consistently compute these expectations via averaging
samples drawn from an approximate posterior. However, its feasibility is being
challenged in the era of so called Big Data as all data needs to be processed
in every iteration. Realising that such simulation is an unnecessarily hard
problem if the goal is estimation, we construct a computationally scalable
methodology that allows unbiased estimation of the required expectations --
without explicit simulation from the full posterior. The scheme's variance is
finite by construction and straightforward to control, leading to algorithms
that are provably unbiased and naturally arrive at a desired error tolerance.
This is achieved at an average computational complexity that is sub-linear in
the size of the dataset and its free parameters are easy to tune. We
demonstrate the utility and generality of the methodology on a range of common
statistical models applied to large-scale benchmark and real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03336</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03336</id><created>2015-01-14</created><authors><author><keyname>Corbera</keyname><forenames>Francisco</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Asenjo</keyname><forenames>Rafael</forenames></author><author><keyname>Navarro</keyname><forenames>Angeles</forenames></author><author><keyname>Vilches</keyname><forenames>Antonio</forenames></author><author><keyname>Garzar&#xe1;n</keyname><forenames>Mar&#xed;a J.</forenames></author></authors><title>Reducing overheads of dynamic scheduling on heterogeneous chips</title><categories>cs.DC</categories><comments>Presented at HIP3ES, 2015 (arXiv: 1501.03064)</comments><report-no>HIP3ES/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent processor development, we have witnessed the integration of GPU and
CPUs into a single chip. The result of this integration is a reduction of the
data communication overheads. This enables an efficient collaboration of both
devices in the execution of parallel workloads.
  In this work, we focus on the problem of efficiently scheduling chunks of
iterations of parallel loops among the computing devices on the chip (the GPU
and the CPU cores) in the context of irregular applications. In particular, we
analyze the sources of overhead that the host thread experiments when a chunk
of iterations is offloaded to the GPU while other threads are executing
concurrently other chunks on the CPU cores. We carefully study these overheads
on different processor architectures and operating systems using Barnes Hut as
a study case representative of irregular applications. We also propose a set of
optimizations to mitigate the overheads that arise in presence of
oversubscription and take advantage of the different features of the
heterogeneous architectures. Thanks to these optimizations we reduce
Energy-Delay Product (EDP) by 18% and 84% on Intel Ivy Bridge and Haswell
architectures, respectively, and by 57% on the Exynos big.LITTLE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03341</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03341</id><created>2015-01-14</created><authors><author><keyname>Hlupic</keyname><forenames>Nikica</forenames></author><author><keyname>Beros</keyname><forenames>Ivo</forenames></author></authors><title>Solving Polynomial Systems by Penetrating Gradient Algorithm Applying
  Deepest Descent Strategy</title><categories>math.OC cs.MS math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm and associated strategy for solving polynomial systems within
the optimization framework is presented. The algorithm and strategy are named,
respectively, the penetrating gradient algorithm and the deepest descent
strategy. The most prominent feature of penetrating gradient algorithm, after
which it was named, is its ability to see and penetrate through the obstacles
in error space along the line of search direction and to jump to the global
minimizer in a single step. The ability to find the deepest point in an
arbitrary direction, no matter how distant the point is and regardless of the
relief of error space between the current and the best point, motivates
movements in directions in which cost function can be maximally reduced, rather
than in directions that seem to be the best locally (like, for instance, the
steepest descent, i.e., negative gradient direction). Therefore, the strategy
is named the deepest descent, in contrast but alluding to the steepest descent.
Penetrating gradient algorithm is derived and its properties are proven
mathematically, while features of the deepest descent strategy are shown by
comparative simulations. Extensive benchmark tests confirm that the proposed
algorithm and strategy jointly form an effective solver of polynomial systems.
In addition, further theoretical considerations in Section 5 about solving
linear systems by the proposed method reveal a surprising and interesting
relation of proposed and Gauss-Seidel method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03342</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03342</id><created>2015-01-14</created><updated>2015-04-21</updated><authors><author><keyname>Peters</keyname><forenames>Isabella</forenames></author><author><keyname>Kraker</keyname><forenames>Peter</forenames></author><author><keyname>Lex</keyname><forenames>Elisabeth</forenames></author><author><keyname>Gumpenberger</keyname><forenames>Christian</forenames></author><author><keyname>Gorraiz</keyname><forenames>Juan</forenames></author></authors><title>Research Data Explored: Citations versus Altmetrics</title><categories>cs.DL</categories><comments>Accpeted for publication at the 15th International Conference on
  Scientometrics and Informetrics (ISSI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study explores the citedness of research data, its distribution over time
and how it is related to the availability of a DOI (Digital Object Identifier)
in Thomson Reuters' DCI (Data Citation Index). We investigate if cited research
data &quot;impact&quot; the (social) web, reflected by altmetrics scores, and if there is
any relationship between the number of citations and the sum of altmetrics
scores from various social media-platforms. Three tools are used to collect and
compare altmetrics scores, i.e. PlumX, ImpactStory, and Altmetric.com. In terms
of coverage, PlumX is the most helpful altmetrics tool. While research data
remain mostly uncited (about 85%), there has been a growing trend in citing
data sets published since 2007. Surprisingly, the percentage of the number of
cited research data with a DOI in DCI has decreased in the last years. Only
nine repositories account for research data with DOIs and two or more
citations. The number of cited research data with altmetrics scores is even
lower (4 to 9%) but shows a higher coverage of research data from the last
decade. However, no correlation between the number of citations and the total
number of altmetrics scores is observable. Certain data types (i.e. survey,
aggregate data, and sequence data) are more often cited and receive higher
altmetrics scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03347</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03347</id><created>2015-01-14</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Bartcus</keyname><forenames>Marius</forenames></author><author><keyname>Glotin</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Dirichlet Process Parsimonious Mixtures for clustering</title><categories>stat.ML cs.LG stat.ME</categories><comments>35 pages, preprint submitted to Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parsimonious Gaussian mixture models, which exploit an eigenvalue
decomposition of the group covariance matrices of the Gaussian mixture, have
shown their success in particular in cluster analysis. Their estimation is in
general performed by maximum likelihood estimation and has also been considered
from a parametric Bayesian prospective. We propose new Dirichlet Process
Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric
formulation of these parsimonious Gaussian mixture models. The proposed DPPM
models are Bayesian nonparametric parsimonious mixture models that allow to
simultaneously infer the model parameters, the optimal number of mixture
components and the optimal parsimonious mixture structure from the data. We
develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of
the developed DPMM models and provide a Bayesian model selection framework by
using Bayes factors. We apply them to cluster simulated data and real data
sets, and compare them to the standard parsimonious mixture models. The
obtained results highlight the effectiveness of the proposed nonparametric
parsimonious mixture models as a good nonparametric alternative for the
parametric parsimonious models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03353</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03353</id><created>2015-01-14</created><authors><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Bendun</keyname><forenames>Fabian</forenames></author><author><keyname>Hoffmann</keyname><forenames>Joerg</forenames></author><author><keyname>Marnau</keyname><forenames>Ninja</forenames></author></authors><title>PriCL: Creating a Precedent A Framework for Reasoning about Privacy Case
  Law</title><categories>cs.CR cs.LO</categories><comments>Extended version</comments><acm-class>K.4.1; J.1; I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce PriCL: the first framework for expressing and automatically
reasoning about privacy case law by means of precedent. PriCL is parametric in
an underlying logic for expressing world properties, and provides support for
court decisions, their justification, the circumstances in which the
justification applies as well as court hierarchies. Moreover, the framework
offers a tight connection between privacy case law and the notion of norms that
underlies existing rule-based privacy research. In terms of automation, we
identify the major reasoning tasks for privacy cases such as deducing legal
permissions or extracting norms. For solving these tasks, we provide generic
algorithms that have particularly efficient realizations within an expressive
underlying logic. Finally, we derive a definition of deducibility based on
legal concepts and subsequently propose an equivalent characterization in terms
of logic satisfiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03354</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03354</id><created>2015-01-14</created><authors><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed</forenames></author><author><keyname>Garetto</keyname><forenames>Michele</forenames></author><author><keyname>Giaccone</keyname><forenames>Paolo</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author><author><keyname>Niccolini</keyname><forenames>Saverio</forenames></author></authors><title>Unravelling the Impact of Temporal and Geographical Locality in Content
  Caching Systems</title><categories>cs.NI</categories><comments>14 pages, 11 Figures, 2 Appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To assess the performance of caching systems, the definition of a proper
process describing the content requests generated by users is required.
Starting from the analysis of traces of YouTube video requests collected inside
operational networks, we identify the characteristics of real traffic that need
to be represented and those that instead can be safely neglected. Based on our
observations, we introduce a simple, parsimonious traffic model, named Shot
Noise Model (SNM), that allows us to capture temporal and geographical locality
of content popularity. The SNM is sufficiently simple to be effectively
employed in both analytical and scalable simulative studies of caching systems.
We demonstrate this by analytically characterizing the performance of the LRU
caching policy under the SNM, for both a single cache and a network of caches.
With respect to the standard Independent Reference Model (IRM), some
paradigmatic shifts, concerning the impact of various traffic characteristics
on cache performance, clearly emerge from our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03358</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03358</id><created>2015-01-01</created><updated>2015-09-25</updated><authors><author><keyname>Amritkar</keyname><forenames>Amit</forenames></author><author><keyname>de Sturler</keyname><forenames>Eric</forenames></author><author><keyname>&#x15a;wirydowicz</keyname><forenames>Katarzyna</forenames></author><author><keyname>Tafti</keyname><forenames>Danesh</forenames></author><author><keyname>Ahuja</keyname><forenames>Kapil</forenames></author></authors><title>Recycling Krylov subspaces for CFD applications and a new hybrid
  recycling solver</title><categories>cs.NA math.NA physics.comp-ph physics.flu-dyn</categories><comments>26 pages, 7 figures</comments><doi>10.1016/j.jcp.2015.09.040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on robust and efficient iterative solvers for the pressure Poisson
equation in incompressible Navier-Stokes problems. Preconditioned Krylov
subspace methods are popular for these problems, with BiCGStab and GMRES(m)
most frequently used for nonsymmetric systems. BiCGStab is popular because it
has cheap iterations, but it may fail for stiff problems, especially early on
as the initial guess is far from the solution. Restarted GMRES is better, more
robust, in this phase, but restarting may lead to very slow convergence.
Therefore, we evaluate the rGCROT method for these systems. This method
recycles a selected subspace of the search space (called recycle space) after a
restart. This generally improves the convergence drastically compared with
GMRES(m). Recycling subspaces is also advantageous for subsequent linear
systems, if the matrix changes slowly or is constant. However, rGCROT
iterations are still expensive in memory and computation time compared with
those of BiCGStab. Hence, we propose a new, hybrid approach that combines the
cheap iterations of BiCGStab with the robustness of rGCROT. For the first few
time steps the algorithm uses rGCROT and builds an effective recycle space, and
then it recycles that space in the rBiCGStab solver. We evaluate rGCROT on a
turbulent channel flow problem, and we evaluate both rGCROT and the new, hybrid
combination of rGCROT and rBiCGStab on a porous medium flow problem. We see
substantial performance gains on both problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03371</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03371</id><created>2015-01-14</created><authors><author><keyname>Ermann</keyname><forenames>Leonardo</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Google matrix analysis of the multiproduct world trade network</title><categories>q-fin.ST cs.SI physics.soc-ph</categories><comments>19 pages, 25 figures</comments><journal-ref>Eur. Phys. J. B (2015) 88, 84</journal-ref><doi>10.1140/epjb/e2015-60047-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the United Nations COMTRADE database \cite{comtrade} we construct the
Google matrix $G$ of multiproduct world trade between the UN countries and
analyze the properties of trade flows on this network for years 1962 - 2010.
This construction, based on Markov chains, treats all countries on equal
democratic grounds independently of their richness and at the same time it
considers the contributions of trade products proportionally to their trade
volume. We consider the trade with 61 products for up to 227 countries. The
obtained results show that the trade contribution of products is asymmetric:
some of them are export oriented while others are import oriented even if the
ranking by their trade volume is symmetric in respect to export and import
after averaging over all world countries. The construction of the Google matrix
allows to investigate the sensitivity of trade balance in respect to price
variations of products, e.g. petroleum and gas, taking into account the world
connectivity of trade links. The trade balance based on PageRank and CheiRank
probabilities highlights the leading role of China and other BRICS countries in
the world trade in recent years. We also show that the eigenstates of $G$ with
large eigenvalues select specific trade communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03378</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03378</id><created>2015-01-14</created><authors><author><keyname>Sanatinia</keyname><forenames>Amirali</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author></authors><title>OnionBots: Subverting Privacy Infrastructure for Cyber Attacks</title><categories>cs.CR</categories><comments>12 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade botnets survived by adopting a sequence of increasingly
sophisticated strategies to evade detection and take overs, and to monetize
their infrastructure. At the same time, the success of privacy infrastructures
such as Tor opened the door to illegal activities, including botnets,
ransomware, and a marketplace for drugs and contraband. We contend that the
next waves of botnets will extensively subvert privacy infrastructure and
cryptographic mechanisms. In this work we propose to preemptively investigate
the design and mitigation of such botnets. We first, introduce OnionBots, what
we believe will be the next generation of resilient, stealthy botnets.
OnionBots use privacy infrastructures for cyber attacks by completely
decoupling their operation from the infected host IP address and by carrying
traffic that does not leak information about its source, destination, and
nature. Such bots live symbiotically within the privacy infrastructures to
evade detection, measurement, scale estimation, observation, and in general all
IP-based current mitigation techniques. Furthermore, we show that with an
adequate self-healing network maintenance scheme, that is simple to implement,
OnionBots achieve a low diameter and a low degree and are robust to
partitioning under node deletions. We developed a mitigation technique, called
SOAP, that neutralizes the nodes of the basic OnionBots. We also outline and
discuss a set of techniques that can enable subsequent waves of Super
OnionBots. In light of the potential of such botnets, we believe that the
research community should proactively develop detection and mitigation methods
to thwart OnionBots, potentially making adjustments to privacy infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03380</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03380</id><created>2015-01-13</created><updated>2015-12-17</updated><authors><author><keyname>Li</keyname><forenames>Yitong</forenames></author><author><keyname>Dai</keyname><forenames>Lin</forenames></author></authors><title>Maximum Sum Rate of Slotted Aloha with Capture</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The sum rate performance of random-access networks crucially depends on the
access protocol and receiver structure. Despite extensive studies, how to
characterize the maximum sum rate of the simplest version of random access,
Aloha, remains an open question. In this paper, a comprehensive study of the
sum rate performance of slotted Aloha networks is presented. By extending the
unified analytical framework proposed in [20], [21] from the classical
collision model to the capture model, the network steady-state point in
saturated conditions is derived as a function of the
signal-to-interference-plus-noise ratio (SINR) threshold which determines a
fundamental tradeoff between the information encoding rate and the network
throughput. To maximize the sum rate, both the SINR threshold and backoff
parameters of nodes should be properly selected. Explicit expressions of the
maximum sum rate and the optimal setting are obtained, which show that similar
to the sum capacity of the multiple access channel, the maximum sum rate of
slotted Aloha also logarithmically increases with the mean received
signal-to-noise ratio (SNR), but the high-SNR slope is only $e^{-1}$. Effects
of backoff and power control on the sum rate performance of slotted Aloha
networks are further discussed, which shed important light on the practical
network design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03383</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03383</id><created>2015-01-10</created><authors><author><keyname>Schauerte</keyname><forenames>Boris</forenames></author><author><keyname>Stiefelhagen</keyname><forenames>Rainer</forenames></author></authors><title>On the Distribution of Salient Objects in Web Images and its Influence
  on Salient Object Detection</title><categories>cs.CV</categories><journal-ref>PLoS ONE 10 (2015)</journal-ref><doi>10.1371/journal.pone.0130316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has become apparent that a Gaussian center bias can serve as an important
prior for visual saliency detection, which has been demonstrated for predicting
human eye fixations and salient object detection. Tseng et al. have shown that
the photographer's tendency to place interesting objects in the center is a
likely cause for the center bias of eye fixations. We investigate the influence
of the photographer's center bias on salient object detection, extending our
previous work. We show that the centroid locations of salient objects in
photographs of Achanta and Liu's data set in fact correlate strongly with a
Gaussian model. This is an important insight, because it provides an empirical
motivation and justification for the integration of such a center bias in
salient object detection algorithms and helps to understand why Gaussian models
are so effective. To assess the influence of the center bias on salient object
detection, we integrate an explicit Gaussian center bias model into two
state-of-the-art salient object detection algorithms. This way, first, we
quantify the influence of the Gaussian center bias on pixel- and segment-based
salient object detection. Second, we improve the performance in terms of F1
score, Fb score, area under the recall-precision curve, area under the receiver
operating characteristic curve, and hit-rate on the well-known data set by
Achanta and Liu. Third, by debiasing Cheng et al.'s region contrast model, we
exemplarily demonstrate that implicit center biases are partially responsible
for the outstanding performance of state-of-the-art algorithms. Last but not
least, as a result of debiasing Cheng et al.'s algorithm, we introduce a
non-biased salient object detection method, which is of interest for
applications in which the image data is not likely to have a photographer's
center bias (e.g., image data of surveillance cameras or autonomous robots).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03389</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03389</id><created>2015-01-14</created><updated>2015-01-26</updated><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>All-to-all Broadcast for Vehicular Networks Based on Coded Slotted ALOHA</title><categories>cs.IT math.IT</categories><comments>v2: small typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an uncoordinated all-to-all broadcast protocol for periodic
messages in vehicular networks based on coded slotted ALOHA (CSA). Unlike
classical CSA, each user acts as both transmitter and receiver in a half-duplex
mode. As in CSA, each user transmits its packet several times. The half-duplex
mode gives rise to an interesting design trade-off: the more the user repeats
its packet, the higher the probability that this packet is decoded by other
users, but the lower the probability for this user to decode packets from
others. We compare the proposed protocol with carrier sense multiple access
with collision avoidance, currently adopted as a multiple access protocol for
vehicular networks. The results show that the proposed protocol greatly
increases the number of users in the network that reliably communicate with
each other. We also provide analytical tools to predict the performance of the
proposed protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03396</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03396</id><created>2015-01-11</created><updated>2015-02-04</updated><authors><author><keyname>Bruno</keyname><forenames>Luca</forenames></author><author><keyname>Fransos</keyname><forenames>Davide</forenames></author></authors><title>Sand transverse dune aerodynamics: 3D Coherent Flow Structures from a
  computational study</title><categories>physics.flu-dyn cs.CE physics.ao-ph physics.geo-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The engineering interest about dune fields is dictated by the their
interaction with a number of human infrastructures in arid environments. Sand
dunes dynamics is dictated by wind and its ability to induce sand erosion,
transport and deposition. A deep understanding of dune aerodynamics serves then
to ground effective strategies for the protection of human infrastructures from
sand, the so-called sand mitigation. Because of their simple geometry and their
frequent occurrence in desert area, transverse sand dunes are usually adopted
in literature as a benchmark to investigate dune aerodynamics by means of both
computational or experimental approaches, usually in nominally 2D setups. The
present study aims at evaluating 3D flow features in the wake of a idealised
transverse dune, if any, under different nominally 2D setup conditions by means
of computational simulations and to compare the obtained results with
experimental measurements available in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03407</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03407</id><created>2015-01-14</created><authors><author><keyname>Xu</keyname><forenames>Yi</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author></authors><title>User Association in Massive MIMO HetNets</title><categories>cs.IT math.IT</categories><doi>10.1109/JSYST.2015.2475702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO and small cell are both recognized as the key technologies for
the future 5G wireless systems. In this paper, we investigate the problem of
user association in a heterogeneous network (HetNet) with massive MIMO and
small cells, where the macro base station (BS) is equipped with a massive MIMO
and the picocell BS's are equipped with regular MIMOs. We first develop
centralized user association algorithms with proven optimality, considering
various objectives such as rate maximization, proportional fairness, and joint
user association and resource allocation. We then model the massive MIMO HetNet
as a repeated game, which leads to distributed user association algorithms with
proven convergence to the Nash Equilibrium (NE). We demonstrate the efficacy of
these optimal schemes by comparison with several greedy algorithms through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03424</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03424</id><created>2015-01-14</created><authors><author><keyname>Jamiy</keyname><forenames>Fatima El</forenames></author><author><keyname>Daif</keyname><forenames>Abderrahmane</forenames></author><author><keyname>Azouazi</keyname><forenames>Mohamed</forenames></author><author><keyname>Marzak</keyname><forenames>Abdelaziz</forenames></author></authors><title>The potential and challenges of Big data - Recommendation systems next
  level application</title><categories>cs.CY</categories><comments>appears in IJCSI International Journal of Computer Science Issues,
  Vol. 11, Issue 5, No 2, September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continuous increase of data generated provides enormous possibilities of
both public and private companies. The management of this mass of data or big
data will play a crucial role in the society of the future, as it finds
applications in different fields. There are so much potential and extremely
useful insights hidden in the huge volume of data. The advanced analysis
techniques available including predictive analytics, text mining, semantic
analysis are needed to enable organizations to create a competitive advantage
through data analyzed with different levels of sophistication, speed and
accuracy previously unavailable. Therefore, is it still possible to have that
level of sophistication with the ubiquitous numeric ocean that accompanies use
every day via connected devices that invade our lives? However, development of
big data requires a good understanding of the issues associated with it. And
this is the purpose of this paper, which focuses on giving a close-up view of
big data analysis, opportunities and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03435</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03435</id><created>2015-01-14</created><authors><author><keyname>Easwarakumar</keyname><forenames>K. S.</forenames></author><author><keyname>Hema</keyname><forenames>T.</forenames></author></authors><title>BITS-Tree-An Efficient Data Structure for Segment Storage and Query
  Processing</title><categories>cs.CG cs.DS</categories><comments>11 pages, 5 figures</comments><acm-class>I.3.5</acm-class><journal-ref>International Journal of Computers and
  Technology,11(10):3108-3116, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new and novel data structure is proposed to dynamically
insert and delete segments. Unlike the standard segment trees[3], the proposed
data structure permits insertion of a segment with interval range beyond the
interval range of the existing tree, which is the interval between minimum and
maximum values of the end points of all the segments. Moreover, the number of
nodes in the proposed tree is lesser as compared to the dynamic version of the
standard segment trees, and is able to answer both stabbing and range queries
practically much faster compared to the standard segment trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03444</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03444</id><created>2015-01-14</created><authors><author><keyname>Granin</keyname><forenames>Sergey</forenames></author><author><keyname>Maximov</keyname><forenames>Yura</forenames></author></authors><title>Average case complexity of DNFs and Shannon semi-effect for narrow
  subclasses of boolean functions</title><categories>math.CO cs.CC</categories><comments>8 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we establish some bounds on the complexity of disjunctive
normal forms of boolean function from narrow subclasses (e.g. functions takes
value 0 in a limited number of points). The bounds are obtained by reduction
the initial problem to a simple set covering problem. The nature of the
complexity bounds provided is tightly connected with Shannon effect and
semi-effect for this classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03446</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03446</id><created>2015-01-14</created><updated>2015-03-18</updated><authors><author><keyname>Domingues</keyname><forenames>Guilherme</forenames></author><author><keyname>Silva</keyname><forenames>Edmundo de Souza e</forenames></author><author><keyname>Leao</keyname><forenames>Rosa M. M.</forenames></author><author><keyname>Menasche</keyname><forenames>Daniel S.</forenames></author></authors><title>Flexible Content Placement in Cache Networks using Reinforced Counters</title><categories>cs.NI</categories><comments>Submitted to 33rd Brazilian Symposium on Computer Networks and
  Distributed Systems, SBRC' 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of content placement in a cache network.
We consider a network where routing of requests is based on random walks.
Content placement is done using a novel mechanism referred to as reinforced
counters. To each content we associate a counter, which is incremented every
time the content is requested, and which is decremented at a fixed rate. We
model and analyze this mechanism, tuning its parameters so as to achieve
desired performance goals for a single cache or for a cache network. We also
show that the optimal static content placement, without reinforced counters, is
NP hard under different design goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03458</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03458</id><created>2015-01-11</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cubero</keyname><forenames>Juan-Carlos</forenames></author></authors><title>The ModelCC Model-Based Parser Generator</title><categories>cs.FL cs.PL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1111.3970,
  arXiv:1501.02038</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal languages let us define the textual representation of data with
precision. Formal grammars, typically in the form of BNF-like productions,
describe the language syntax, which is then annotated for syntax-directed
translation and completed with semantic actions. When, apart from the textual
representation of data, an explicit representation of the corresponding data
structure is required, the language designer has to devise the mapping between
the suitable data model and its proper language specification, and then develop
the conversion procedure from the parse tree to the data model instance.
Unfortunately, whenever the format of the textual representation has to be
modified, changes have to propagated throughout the entire language processor
tool chain. These updates are time-consuming, tedious, and error-prone.
Besides, in case different applications use the same language, several copies
of the same language specification have to be maintained. In this paper, we
introduce ModelCC, a model-based parser generator that decouples language
specification from language processing, hence avoiding many of the problems
caused by grammar-driven parsers and parser generators. ModelCC incorporates
reference resolution within the parsing process. Therefore, instead of
returning mere abstract syntax trees, ModelCC is able to obtain abstract syntax
graphs from input strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03461</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03461</id><created>2015-01-14</created><authors><author><keyname>Azad</keyname><forenames>Ariful</forenames></author></authors><title>An Algorithmic Pipeline for Analyzing Multi-parametric Flow Cytometry
  Data</title><categories>q-bio.QM cs.CE cs.DS</categories><comments>PhD dissertation, May 2014, Purdue University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow cytometry (FC) is a single-cell profiling platform for measuring the
phenotypes of individual cells from millions of cells in biological samples. FC
employs high-throughput technologies and generates high-dimensional data, and
hence algorithms for analyzing the data represent a bottleneck. This
dissertation addresses several computational challenges arising in modern
cytometry while mining information from high-dimensional and high-content
biological data. A collection of combinatorial and statistical algorithms for
locating, matching, prototyping, and classifying cellular populations from
multi-parametric FC data is developed.
  The algorithmic pipeline, flowMatch, developed in this dissertation consists
of five well-defined algorithmic modules to (1) transform data to stabilize
within-population variance, (2) identify cell populations by robust clustering
algorithms, (3) register cell populations across samples, (4) encapsulate a
class of samples with templates, and (5) classify samples based on their
similarity with the templates. Components of flowMatch can work independently
or collaborate with each other to perform the complete data analysis. flowMatch
is made available as an open-source R package in Bioconductor.
  We have employed flowMatch for classifying leukemia samples, evaluating the
phosphorylation effects on T cells, classifying healthy immune profiles, and
classifying the vaccination status of HIV patients. In these analyses, the
pipeline is able to reach biologically meaningful conclusions quickly and
efficiently with the automated algorithms. The algorithms included in flowMatch
can also be applied to problems outside of flow cytometry such as in microarray
data analysis and image recognition. Therefore, this dissertation contributes
to the solution of fundamental problems in computational cytometry and related
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03467</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03467</id><created>2015-01-07</created><authors><author><keyname>Fujii</keyname><forenames>Kazuyuki</forenames><affiliation>YCU</affiliation></author><author><keyname>Oike</keyname><forenames>Hiroshi</forenames></author></authors><title>A Superintroduction to Google Matrices for Undergraduates</title><categories>cs.SI physics.soc-ph</categories><comments>Latex; 12 pages; 2 figures</comments><journal-ref>East Journal of Mathematical Education, 14 (2015), 55-68</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider so-called Google matrices and show that all
eigenvalues ($\lambda$) of them have a fundamental property $|\lambda|\leq 1$.
The stochastic eigenvector corresponding to $\lambda=1$ called the PageRank
vector plays a central role in the Google's software. We study it in detail and
present some important problems.
  The purpose of the paper is to make {\bf the heart of Google} clearer for
undergraduates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03471</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03471</id><created>2015-01-14</created><authors><author><keyname>Ciampaglia</keyname><forenames>Giovanni Luca</forenames></author><author><keyname>Shiralkar</keyname><forenames>Prashant</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Computational fact checking from knowledge networks</title><categories>cs.CY cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0141938</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional fact checking by expert journalists cannot keep up with the
enormous volume of information that is now generated online. Computational fact
checking may significantly enhance our ability to evaluate the veracity of
dubious information. Here we show that the complexities of human fact checking
can be approximated quite well by finding the shortest path between concept
nodes under properly defined semantic proximity metrics on knowledge graphs.
Framed as a network problem this approach is feasible with efficient
computational techniques. We evaluate this approach by examining tens of
thousands of claims related to history, entertainment, geography, and
biographical information using a public knowledge graph extracted from
Wikipedia. Statements independently known to be true consistently receive
higher support via our method than do false ones. These findings represent a
significant step toward scalable computational fact-checking methods that may
one day mitigate the spread of harmful misinformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03474</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03474</id><created>2015-01-14</created><updated>2015-12-05</updated><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Stability of Markov regenerative switched linear systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a necessary and sufficient condition for mean
stability of switched linear systems having a Markov regenerative process as
its switching signal. This class of switched linear systems, which we call
Markov regenerative switched linear systems, contains Markov jump linear
systems and semi-Markov jump linear systems as special cases. We show that a
Markov regenerative switched linear system is $m$th mean stable if and only if
a particular matrix is Schur stable, under the assumption that either $m$ is
even or the system is positive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03481</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03481</id><created>2015-01-14</created><authors><author><keyname>Georgakoudis</keyname><forenames>Giorgis</forenames></author><author><keyname>Gillan</keyname><forenames>Charles J.</forenames></author><author><keyname>Sayed</keyname><forenames>Ahmed</forenames></author><author><keyname>Spence</keyname><forenames>Ivor</forenames></author><author><keyname>Faloon</keyname><forenames>Richard</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Dimitrios S.</forenames></author></authors><title>Iso-Quality of Service: Fairly Ranking Servers for Real-Time Data
  Analytics</title><categories>cs.DC</categories><comments>12 pages, 5 figures, 8 tables, 6 equations. arXiv admin note: text
  overlap with arXiv:1501.00048</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mathematically rigorous Quality-of-Service (QoS) metric which
relates the achievable quality of service metric (QoS) for a real-time
analytics service to the server energy cost of offering the service. Using a
new iso-QoS evaluation methodology, we scale server resources to meet QoS
targets and directly rank the servers in terms of their energy-efficiency and
by extension cost of ownership. Our metric and method are platform-independent
and enable fair comparison of datacenter compute servers with significant
architectural diversity, including micro-servers. We deploy our metric and
methodology to compare three servers running financial option pricing workloads
on real-life market data. We find that server ranking is sensitive to data
inputs and desired QoS level and that although scale-out micro-servers can be
up to two times more energy-efficient than conventional heavyweight servers for
the same target QoS, they are still six times less energy efficient than
high-performance computational accelerators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03529</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03529</id><created>2015-01-14</created><authors><author><keyname>Gholami</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dwivedi</keyname><forenames>Satyam</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Ranging without time stamps exchanging</title><categories>stat.AP cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the range estimate between two wireless nodes without time
stamps exchanging. Considering practical aspects of oscillator clocks, we
propose a new model for ranging in which the measurement errors include the sum
of two distributions, namely, uniform and Gaussian. We then derive an
approximate maximum likelihood estimator (AMLE), which poses a difficult global
optimization problem. To avoid the difficulty in solving the complex AMLE, we
propose a simple estimator based on the method of moments. Numerical results
show a promising performance for the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03542</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03542</id><created>2015-01-14</created><updated>2015-01-23</updated><authors><author><keyname>Castiglione</keyname><forenames>Jason</forenames></author><author><keyname>Kavcic</keyname><forenames>Aleksandar</forenames></author></authors><title>Secrecy Through Synchronization Errors</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, submitted to ISIT 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose a transmission scheme that achieves information
theoretic security, without making assumptions on the eavesdropper's channel.
This is achieved by a transmitter that deliberately introduces synchronization
errors (insertions and/or deletions) based on a shared source of randomness.
The intended receiver, having access to the same shared source of randomness as
the transmitter, can resynchronize the received sequence. On the other hand,
the eavesdropper's channel remains a synchronization error channel. We prove a
secrecy capacity theorem, provide a lower bound on the secrecy capacity, and
propose numerical methods to evaluate it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03545</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03545</id><created>2015-01-14</created><updated>2015-04-23</updated><authors><author><keyname>von Looz</keyname><forenames>Moritz</forenames></author><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author></authors><title>Fast generation of complex networks with underlying hyperbolic geometry</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks have become increasingly popular for modeling various
real-world phenomena. Realistic generative network models are important in this
context as they avoid privacy concerns of real data and simplify complex
network research regarding data sharing, reproducibility, and scalability
studies. \emph{Random hyperbolic graphs} are a well-analyzed family of
geometric graphs. Previous work provided empirical and theoretical evidence
that this generative graph model creates networks with non-vanishing clustering
and other realistic features. However, the investigated networks in previous
applied work were small, possibly due to the quadratic running time of a
previous generator.
  In this work we provide the first generation algorithm for these networks
with subquadratic running time. We prove a time complexity of $O((n^{3/2}+m)
\log n)$ with high probability for the generation process. This running time is
confirmed by experimental data with our implementation. The acceleration stems
primarily from the reduction of pairwise distance computations through a polar
quadtree, which we adapt to hyperbolic space for this purpose. In practice we
improve the running time of a previous implementation by at least two orders of
magnitude this way. Networks with billions of edges can now be generated in a
few minutes.
  Finally, we evaluate the largest networks of this model published so far. Our
empirical analysis shows that important features are retained over different
graph densities and degree distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03547</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03547</id><created>2015-01-14</created><authors><author><keyname>Abdelwahab</keyname><forenames>Sherif</forenames></author><author><keyname>Hamdaoui</keyname><forenames>Bechir</forenames></author><author><keyname>Guizani</keyname><forenames>Mohsen</forenames></author></authors><title>Cloud-Assisted Remote Sensor Network Virtualization for Distributed
  Consensus Estimation</title><categories>cs.NI</categories><comments>11 pages, double column, pre-submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop cloud-assisted remote sensing techniques for enabling distributed
consensus estimation of unknown parameters in a given geographic area. We first
propose a distributed sensor network virtualization algorithm that searches
for, selects, and coordinates Internet-accessible sensors to perform a sensing
task in a specific region. The algorithm converges in linearithmic time for
large-scale networks, and requires exchanging a number of messages that is at
most linear in the number of sensors. Second, we design an uncoordinated,
distributed algorithm that relies on the selected sensors to estimate a set of
parameters without requiring synchronization among the sensors. Our simulation
results show that the proposed algorithm, when compared to conventional ADMM
(Alternating Direction Method of Multipliers), reduces communication overhead
significantly without compromising the estimation error. In addition, the
convergence time, though increases slightly, is still linear as in the case of
conventional ADMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03549</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03549</id><created>2015-01-14</created><authors><author><keyname>Borcea</keyname><forenames>Ciprian S.</forenames></author><author><keyname>Streinu</keyname><forenames>Ileana</forenames></author></authors><title>Liftings and stresses for planar periodic frameworks</title><categories>math.MG cs.CG</categories><comments>An extended abstract of this paper has appeared in Proc. 30th annual
  Symposium on Computational Geometry (SOCG'14), Kyoto, Japan, June 2014</comments><msc-class>52C25, 74N10</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and prove a periodic analog of Maxwell's theorem relating
stressed planar frameworks and their liftings to polyhedral surfaces with
spherical topology. We use our lifting theorem to prove deformation and
rigidity-theoretic properties for planar periodic pseudo-triangulations,
generalizing features known for their finite counterparts. These properties are
then applied to questions originating in mathematical crystallography and
materials science, concerning planar periodic auxetic structures and ultrarigid
periodic frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03550</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03550</id><created>2015-01-14</created><updated>2015-11-22</updated><authors><author><keyname>Borcea</keyname><forenames>Ciprian S.</forenames></author><author><keyname>Streinu</keyname><forenames>Ileana</forenames></author></authors><title>Geometric auxetics</title><categories>math.MG cs.CG</categories><msc-class>52C25, 74N10</msc-class><acm-class>I.3.5</acm-class><doi>10.1098/rspa.2015.0033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate a mathematical theory of auxetic behavior based on one-parameter
deformations of periodic frameworks. Our approach is purely geometric, relies
on the evolution of the periodicity lattice and works in any dimension. We
demonstrate its usefulness by predicting or recognizing, without experiment,
computer simulations or numerical approximations, the auxetic capabilities of
several well-known structures available in the literature. We propose new
principles of auxetic design and rely on the stronger notion of expansive
behavior to provide an infinite supply of planar auxetic mechanisms and several
new three-dimensional structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03551</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03551</id><created>2015-01-14</created><authors><author><keyname>Borcea</keyname><forenames>Ciprian S.</forenames></author><author><keyname>Streinu</keyname><forenames>Ileana</forenames></author></authors><title>Deforming Diamond</title><categories>math.MG cs.CG</categories><msc-class>52C25, 74N10</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For materials science, diamond crystals are almost unrivaled for hardness and
a range of other properties. Yet, when simply abstracting the carbon bonding
structure as a geometric bar-and-joint periodic framework, it is far from
rigid. We study the geometric deformations of this type of framework in
arbitrary dimension d, with particular regard to the volume variation of a unit
cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03566</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03566</id><created>2015-01-14</created><authors><author><keyname>Ge</keyname><forenames>Gennian</forenames></author><author><keyname>Shangguan</keyname><forenames>Chong</forenames></author></authors><title>On a Conjecture of Erd{\H{o}}s, Frankl and F{\&quot;u}redi</title><categories>cs.IT math.CO math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{X}$ be an $n$-element set. Assume $\mathscr{F}$ is a collection
of subsets of $\mathcal{X}$. We call $\mathscr{F}$ an $r$-cover-free family if
$F_0\nsubseteq F_1\cup\cdots\cup F_r$ holds for all distinct
$F_0,F_1,...,F_r\in\mathscr{F}$. Given $r$, denote $n(r)$ the minimal $n$ such
that there exits an $r$-cover-free family on an $n$-element set with
cardinality larger than $n$. Thirty years ago, Erd\H{o}s, Frankl and F{\&quot;u}redi
\cite{CFF} proved that $\binom{r+2}{2}\leq n(r)&lt; r^2+o(r^2)$. They also
conjectured $\lim_{r\rightarrow\infty} n(r)/r^2=1$ and claimed that
$n(r)&gt;(1+o(1))\frac{5}{6}r^2$, without proof. In this paper, it is proved that
$\lim_{r\rightarrow\infty} n(r)/r^2\geq(15+\sqrt{33})/24$, which is a quantity
in $[6/7,7/8]$. In particular, their conjecture is proved to be true for all
r-cover-free families with uniform $(r+1)$-subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03569</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03569</id><created>2015-01-14</created><updated>2015-04-21</updated><authors><author><keyname>Truong</keyname><forenames>Lan V.</forenames></author><author><keyname>Yamamoto</keyname><forenames>Hirosuke</forenames></author></authors><title>On the Capacity of Symmetric Gaussian Interference Channels with
  Feedback</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. of IEEE International Symposium on Information
  Theory (ISIT), Hong Kong, June 14-19, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new coding scheme for symmetric Gaussian
interference channels with feedback based on the ideas of time-varying coding
schemes. The proposed scheme improves the Suh-Tse and Kramer inner bounds of
the channel capacity for the cases of weak and not very strong interference.
This improvement is more significant when the signal-to-noise ratio (SNR) is
not very high. It is shown theoretically and numerically that our coding scheme
can outperform the Kramer code. In addition, the generalized degrees-of-freedom
of our proposed coding scheme is equal to the Suh-Tse scheme in the strong
interference case. The numerical results show that our coding scheme can attain
better performance than the Suh-Tse coding scheme for all channel parameters.
Furthermore, the simplicity of the encoding/decoding algorithms is another
strong point of our proposed coding scheme compared with the Suh-Tse coding
scheme. More importantly, our results show that an optimal coding scheme for
the symmetric Gaussian interference channels with feedback can be achieved by
using only marginal posterior distributions under a better cooperation strategy
between transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03577</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03577</id><created>2015-01-15</created><authors><author><keyname>Zhu</keyname><forenames>Xuzhen</forenames></author><author><keyname>Tian</keyname><forenames>Hui</forenames></author><author><keyname>Hu</keyname><forenames>Zheng</forenames></author><author><keyname>Zhang</keyname><forenames>Ping</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Consistence beats causality in recommender systems</title><categories>cs.IR physics.data-an</categories><comments>16 pages, 4 tables, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth of information challenges people's capability in finding
out items fitting to their own interests. Recommender systems provide an
efficient solution by automatically push possibly relevant items to users
according to their past preferences. Recommendation algorithms usually embody
the causality from what having been collected to what should be recommended. In
this article, we argue that in many cases, a user's interests are stable, and
thus the previous and future preferences are highly consistent. The temporal
order of collections then does not necessarily imply a causality relationship.
We further propose a consistence-based algorithm that outperforms the
state-of-the-art recommendation algorithms in disparate real data sets,
including \textit{Netflix}, \textit{MovieLens}, \textit{Amazon} and
\textit{Rate Your Music}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03593</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03593</id><created>2015-01-15</created><authors><author><keyname>Ta</keyname><forenames>Vinh-Thong</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon, CITI</affiliation></author><author><keyname>Antignac</keyname><forenames>Thibaud</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon, CITI</affiliation></author></authors><title>Privacy by Design: On the Conformance Between Protocols and
  Architectures</title><categories>cs.CR cs.LO</categories><comments>FPS - 7th International Symposium on Foundations \&amp; Practice of
  Security, Nov 2014, Montreal, Canada. Springer</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In systems design, we generally distinguish the architecture and the protocol
levels. In the context of privacy by design, in the first case, we talk about
privacy architectures, which define the privacy goals and the main features of
the system at high level. In the latter case, we consider the underlying
concrete protocols and privacy enhancing technologies that implement the
architectures. In this paper, we address the question that whether a given
protocol conforms to a privacy architecture and provide the answer based on
formal methods. We propose a process algebra variant to define protocols and
reason about privacy properties, as well as a mapping procedure from protocols
to architectures that are defined in a high-level architecture language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03601</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03601</id><created>2015-01-15</created><authors><author><keyname>Zhong</keyname><forenames>Xiaoxiong</forenames></author><author><keyname>Qin</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author></authors><title>Capacity Analysis in Multi-Radio Multi-Channel Cognitive Radio Networks:
  A Small World Perspective</title><categories>cs.NI</categories><comments>Wireless Pers Commun(2014)79:2209-2225</comments><doi>10.1007/s11277-014-1981-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio (CR) has emerged as a promising technology to improve
spectrum utilization. Capacity analysis is very useful in investigating the
ultimate performance limits for wireless networks. Meanwhile, with increasing
potential future applications for the CR systems, it is necessary to explore
the limitations on their capacity in a dynamic spectrum access environment.
However, due to spectrum sharing in cognitive radio networks (CRNs), the
capacity of the secondary network (SRN) is much more difficult to analyze than
that of traditional wireless networks. To overcome this difficulty, in this
paper we introduce a novel solution based on small world model to analyze the
capacity of SRN. First, we propose a new method of shortcut creation for CRNs,
which is based on connectivity ratio. Also, a new channel assignment algorithm
is proposed, which jointly considers the available time and transmission time
of the channels. And then, we derive the capacity of SRN based on the small
world model over multi-radio multi-channel (MRMC) environment. The simulation
results show that our proposed scheme can obtain a higher capacity and smaller
latency compared with traditional schemes in MRMC CRNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03602</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03602</id><created>2015-01-15</created><authors><author><keyname>Hadravov&#xe1;</keyname><forenames>Jana</forenames></author><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author></authors><title>Equation $x^iy^jx^k=u^iv^ju^k$ in words</title><categories>cs.FL</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will prove that the word $a^ib^ja^k$ is periodicity forcing if $j \geq 3$
and $i+k \geq 3$, where $i$ and $k$ are positive integers. Also we will give
examples showing that both bounds are optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03605</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03605</id><created>2015-01-15</created><authors><author><keyname>Lawonn</keyname><forenames>Kai</forenames></author><author><keyname>Preim</keyname><forenames>Bernhard</forenames></author></authors><title>Feature Lines for Illustrating Medical Surface Models: Mathematical
  Background and Survey</title><categories>cs.GR</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a tutorial and survey for a specific kind of illustrative
visualization technique: feature lines. We examine different feature line
methods. For this, we provide the differential geometry behind these concepts
and adapt this mathematical field to the discrete differential geometry. All
discrete differential geometry terms are explained for triangulated surface
meshes. These utilities serve as basis for the feature line methods. We provide
the reader with all knowledge to re-implement every feature line method.
Furthermore, we summarize the methods and suggest a guideline for which kind of
surface which feature line algorithm is best suited. Our work is motivated by,
but not restricted to, medical and biological surface models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03609</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03609</id><created>2015-01-15</created><updated>2015-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Li-Chun</forenames></author></authors><title>Random Cell Association and Void Probability in Poisson-Distributed
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, conference (Figures are updated in this version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studied the fundamental modeling defect existing in
Poisson-distributed cellular networks in which all base stations form a
homogeneous Poisson point process (PPP) of intensity $\lambda_B$ and all users
form another independent PPP of intensity $\lambda_U$. The modeling defect,
hardly discovered in prior works, is the void cell issue that stems from the
independence between the distributions of users and BSs and &quot;user-centric&quot; cell
association, and it could give rise to very inaccurate analytical results. We
showed that the void probability of a cell under generalized random cell
association is always bounded above zero and its theoretical lower bound is
$\exp(-\frac{\lambda_U}{\lambda_B})$ that can be achieved by large association
weighting. An accurate expression of the void probability of a cell was derived
and simulation results validated its correctness. We also showed that the
associated BSs are essentially no longer a PPP such that modeling them as a PPP
to facilitate the analysis of interference-related performance metrics may
detach from reality if the BS intensity is not significantly large if compared
with the user intensity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03610</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03610</id><created>2015-01-15</created><updated>2015-04-23</updated><authors><author><keyname>Fu</keyname><forenames>Tom Z. J.</forenames></author><author><keyname>Ding</keyname><forenames>Jianbing</forenames></author><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Winslett</keyname><forenames>Marianne</forenames></author><author><keyname>Yang</keyname><forenames>Yin</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenjie</forenames></author></authors><title>DRS: Dynamic Resource Scheduling for Real-Time Analytics over Fast
  Streams</title><categories>cs.DC</categories><comments>This is the our latest version with certain modification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a data stream management system (DSMS), users register continuous queries,
and receive result updates as data arrive and expire. We focus on applications
with real-time constraints, in which the user must receive each result update
within a given period after the update occurs. To handle fast data, the DSMS is
commonly placed on top of a cloud infrastructure. Because stream properties
such as arrival rates can fluctuate unpredictably, cloud resources must be
dynamically provisioned and scheduled accordingly to ensure real-time response.
It is quite essential, for the existing systems or future developments, to
possess the ability of scheduling resources dynamically according to the
current workload, in order to avoid wasting resources, or failing in delivering
correct results on time. Motivated by this, we propose DRS, a novel dynamic
resource scheduler for cloud-based DSMSs. DRS overcomes three fundamental
challenges: (a) how to model the relationship between the provisioned resources
and query response time (b) where to best place resources; and (c) how to
measure system load with minimal overhead. In particular, DRS includes an
accurate performance model based on the theory of \emph{Jackson open queueing
networks} and is capable of handling \emph{arbitrary} operator topologies,
possibly with loops, splits and joins. Extensive experiments with real data
confirm that DRS achieves real-time response with close to optimal resource
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03613</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03613</id><created>2015-01-15</created><authors><author><keyname>Carl&#xe0;</keyname><forenames>Lorenzo</forenames></author><author><keyname>Fantacci</keyname><forenames>Romano</forenames></author><author><keyname>Gei</keyname><forenames>Francesco</forenames></author><author><keyname>Marabissi</keyname><forenames>Dania</forenames></author><author><keyname>Micciullo</keyname><forenames>Luigia</forenames></author></authors><title>LTE enhancements for Public Safety and Security communications to
  support Group Multimedia Communications</title><categories>cs.NI cs.MM</categories><comments>IEEE Network Magazine, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently Public Safety and Security communication systems rely on reliable
and secure Professional Mobile Radio (PMR) Networks that are mainly devoted to
provide voice services. However, the evolution trend for PMR networks is
towards the provision of new value-added multimedia services such as video
streaming, in order to improve the situational awareness and enhance the
life-saving operations. The challenge here is to exploit the future commercial
broadband networks to deliver voice and multimedia services satisfying the PMR
service requirements. In particular, a viable solution till now seems that of
adapting the new Long Term Evolution technology to provide IP-based broadband
services with the security and reliability typical of PMR networks. This paper
outlines different alternatives to achieve this goal and, in particular,
proposes a proper solution for providing multimedia services with PMR standards
over commercial LTE networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03616</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03616</id><created>2015-01-15</created><updated>2015-10-26</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On the Renyi Divergence, Joint Range of Relative Entropies, and a
  Channel Coding Theorem</title><categories>cs.IT math.IT math.PR</categories><comments>Submitted to IEEE Trans. on Information Theory in March 2015; revised
  in October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper starts by considering the minimization of the Renyi divergence
subject to a constraint on the total variation distance. Based on the solution
of this optimization problem, the exact locus of the points $\bigl( D(Q\|P_1),
D(Q\|P_2) \bigr)$ is determined when $P_1, P_2, Q$ are arbitrary probability
measures which are mutually absolutely continuous, and the total variation
distance between $P_1$ and $P_2$ is not below a given value. It is further
shown that all the points of this convex region are attained by probability
measures which are defined on a binary alphabet. This characterization yields a
geometric interpretation of the minimal Chernoff information subject to a
constraint on the total variation distance. This paper also derives an
exponential upper bound on the performance of binary linear block codes (or
code ensembles) under maximum-likelihood decoding. Its derivation relies on the
Gallager bounding technique, and it reproduces the Shulman-Feder bound as a
special case. The bound is expressed in terms of the Renyi divergence from the
normalized distance spectrum of the code (or the average distance spectrum of
the ensemble) to the binomially distributed distance spectrum of the
capacity-achieving ensemble of random block codes. This exponential bound
provides a quantitative measure of the degradation in performance of binary
linear block codes (or code ensembles) as a function of the deviation of their
distance spectra from the binomial distribution. An efficient use of this bound
is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03617</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03617</id><created>2015-01-15</created><authors><author><keyname>Mohamed</keyname><forenames>Marghny H.</forenames></author><author><keyname>Mahdy</keyname><forenames>Yousef B.</forenames></author><author><keyname>Shaban</keyname><forenames>Wafaa Abd El-Wahed</forenames></author></authors><title>Confidential Algorithm for Golden Cryptography Using Haar Wavelet</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the most important consideration techniques when one want to solve the
protecting of digital signal is the golden matrix. The golden matrices can be
used for creation of a new kind of cryptography called the golden cryptography.
Many research papers have proved that the method is very fast and simple for
technical realization and can be used for cryptographic protection of digital
signals. In this paper, we introduce a technique of encryption based on
combination of haar wavelet and golden matrix. These combinations carry out
after compression data by adaptive Huffman code to reduce data size and remove
redundant data. This process will provide multisecurity services. In addition
Message Authentication Code (MAC) technique can be used to provide
authentication and the integrity of this scheme. The proposed scheme is
accomplished through five stages, the compression data, key generation,
encryption stage, the decryption stage and decompression at communication ends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03619</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03619</id><created>2015-01-15</created><updated>2015-04-26</updated><authors><author><keyname>Ding</keyname><forenames>Jianbing</forenames></author><author><keyname>Fu</keyname><forenames>Tom Z. J.</forenames></author><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Winslett</keyname><forenames>Marianne</forenames></author><author><keyname>Yang</keyname><forenames>Yin</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenjie</forenames></author><author><keyname>Chao</keyname><forenames>Hongyang</forenames></author></authors><title>Optimal Operator State Migration for Elastic Data Stream Processing</title><categories>cs.DC</categories><comments>The latest version with a few modifications</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A cloud-based data stream management system (DSMS) handles fast data by
utilizing the massively parallel processing capabilities of the underlying
platform. An important property of such a DSMS is elasticity, meaning that
nodes can be dynamically added to or removed from an application to match the
latter's workload, which may fluctuate in an unpredictable manner. For an
application involving stateful operations such as aggregates, the addition /
removal of nodes necessitates the migration of operator states. Although the
importance of migration has been recognized in existing systems, two key
problems remain largely neglected, namely how to migrate and what to migrate,
i.e., the migration mechanism that reduces synchronization overhead and result
delay during migration, and the selection of the optimal task assignment that
minimizes migration costs. Consequently, migration in current systems typically
incurs a high spike in result delay caused by expensive synchronization
barriers and suboptimal task assignments. Motivated by this, we present the
first comprehensive study on efficient operator states migration, and propose
designs and algorithms that enable live, progressive, and optimized migrations.
Extensive experiments using real data justify our performance claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03641</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03641</id><created>2015-01-15</created><updated>2015-03-26</updated><authors><author><keyname>Franek</keyname><forenames>Peter</forenames></author><author><keyname>Krcal</keyname><forenames>Marek</forenames></author></authors><title>On Computability and Triviality of Well Groups</title><categories>math.AT cs.CG</categories><comments>20 pages main paper including bibliography, followed by 22 pages of
  Appendix</comments><msc-class>65H10, 68U05, 55S35, 55Q55</msc-class><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of well group in a special but important case captures
homological properties of the zero set of a continuous map $f:K\to R^n$ on a
compact space K that are invariant with respect to perturbations of f. The
perturbations are arbitrary continuous maps within $L_\infty$ distance r from f
for a given r&gt;0. The main drawback of the approach is that the computability of
well groups was shown only when dim K=n or n=1.
  Our contribution to the theory of well groups is twofold: on the one hand we
improve on the computability issue, but on the other hand we present a range of
examples where the well groups are incomplete invariants, that is, fail to
capture certain important robust properties of the zero set.
  For the first part, we identify a computable subgroup of the well group that
is obtained by cap product with the pullback of the orientation of R^n by f. In
other words, well groups can be algorithmically approximated from below. When f
is smooth and dim K&lt;2n-2, our approximation of the (dim K-n)th well group is
exact.
  For the second part, we find examples of maps $f,f': K\to R^n$ with all well
groups isomorphic but whose perturbations have different zero sets. We discuss
on a possible replacement of the well groups of vector valued maps by an
invariant of a better descriptive power and computability status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03643</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03643</id><created>2015-01-15</created><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author></authors><title>On robust width property for Lasso and Dantzig selector</title><categories>cs.IT math.IT math.OC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Jameson Cahill and Dustin G. Mixon completely characterize the
sensing operators in many compressed sensing instances with a robust width
property. The introduced property allows uniformly stable and robust
reconstruction via convex optimization. However, their theory does not cover
the Lasso and the Dantzig selector models, both of which are popular
alternatives in statistics community. In this note, we discover that the robust
width property can be perfectly applied to these two models as well. Our main
results definitely solve the open problem left by Jameson Cahill and Dustin G.
Mixon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03654</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03654</id><created>2015-01-15</created><updated>2015-09-25</updated><authors><author><keyname>Muppirisetty</keyname><forenames>L. Srikar</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author></authors><title>Spatial Wireless Channel Prediction under Location Uncertainty</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial wireless channel prediction is important for future wireless
networks, and in particular for proactive resource allocation at different
layers of the protocol stack. Various sources of uncertainty must be accounted
for during modeling and to provide robust predictions. We investigate two
channel prediction frameworks, classical Gaussian processes (cGP) and uncertain
Gaussian processes (uGP), and analyze the impact of location uncertainty during
learning/training and prediction/testing, for scenarios where measurements
uncertainty are dominated by large-scale fading. We observe that cGP generally
fails both in terms of learning the channel parameters and in predicting the
channel in the presence of location uncertainties.\textcolor{blue}{{} }In
contrast, uGP explicitly considers the location uncertainty. Using simulated
data, we show that uGP is able to learn and predict the wireless channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03669</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03669</id><created>2015-01-15</created><updated>2015-12-14</updated><authors><author><keyname>Chierchia</keyname><forenames>G.</forenames></author><author><keyname>Pustelnik</keyname><forenames>Nelly</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Pesquet-Popescu</keyname><forenames>B.</forenames></author></authors><title>A Proximal Approach for Sparse Multiclass SVM</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparsity-inducing penalties are useful tools to design multiclass support
vector machines (SVMs). In this paper, we propose a convex optimization
approach for efficiently and exactly solving the multiclass SVM learning
problem involving a sparse regularization and the multiclass hinge loss
formulated by Crammer and Singer. We provide two algorithms: the first one
dealing with the hinge loss as a penalty term, and the other one addressing the
case when the hinge loss is enforced through a constraint. The related convex
optimization problems can be efficiently solved thanks to the flexibility
offered by recent primal-dual proximal algorithms and epigraphical splitting
techniques. Experiments carried out on several datasets demonstrate the
interest of considering the exact expression of the hinge loss rather than a
smooth approximation. The efficiency of the proposed algorithms w.r.t. several
state-of-the-art methods is also assessed through comparisons of execution
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03685</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03685</id><created>2015-01-15</created><updated>2015-09-23</updated><authors><author><keyname>Larrousse</keyname><forenames>Benjamin</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu</forenames></author></authors><title>Coordination in distributed networks via coded actions with application
  to power control</title><categories>cs.IT math.IT math.OC</categories><comments>Submitted to ieee transactions on information theory. Some results
  partly published in references [1-3]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of coordinating several agents through
their actions. Although the methodology applies to general scenarios, the
present work focuses on a situation with an asymmetric observation structure
that only involves two agents. More precisely, one of the agents knows the
past, present, and future realizations of a state (the system state) that
affects the common payoff function of the agents; in contrast, the second agent
is assumed either to know the past realizations of the system state or to have
no knowledge of it. In both cases, the second agent has access to some strictly
causal observations of the first agent's actions, which enables the two agents
to coordinate. These scenarios are applied to the problem of distributed power
control; the key idea is that a transmitter may embed information about the
wireless channel state into its transmit power levels so that an observation of
these levels, e.g. the signal-to-interference plus noise ratio, allows the
other transmitter to coordinate its power levels. The main contributions of
this paper are twofold. First, we provide a characterization of the set of
feasible average payoffs when the agents repeatedly take long sequences of
actions and the realizations of the system state are \acs{iid}. Second, we
exploit these results in the context of distributed power control and introduce
the concept of coded power control. We carry out an extensive numerical
analysis of the benefits of coded power control over alternative power control
policies, and highlight a simple yet non-trivial example of a power control
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03686</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03686</id><created>2015-01-15</created><authors><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Packing Plane Perfect Matchings into a Point Set</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ points in the plane, where $n$ is even, we consider
the following question: How many plane perfect matchings can be packed into
$P$? We prove that at least $\lceil\log_2{n}\rceil-2$ plane perfect matchings
can be packed into any point set $P$. For some special configurations of point
sets, we give the exact answer. We also consider some extensions of this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03691</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03691</id><created>2015-01-15</created><updated>2015-06-30</updated><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>Integral D-Finite Functions</title><categories>cs.SC math.RA</categories><acm-class>I.1.2</acm-class><journal-ref>Proceedings of the International Symposium on Symbolic and
  Algebraic Computation (ISSAC 2015), pages 251-258, 2015. ACM, New York, USA,
  ISBN 978-1-4503-3435-8</journal-ref><doi>10.1145/2755996.2756658</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a differential analog of the notion of integral closure of
algebraic function fields. We present an algorithm for computing the integral
closure of the algebra defined by a linear differential operator. Our algorithm
is a direct analog of van Hoeij's algorithm for computing integral bases of
algebraic function fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03704</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03704</id><created>2015-01-15</created><authors><author><keyname>Zheng</keyname><forenames>Le</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Long</keyname><forenames>Teng</forenames></author></authors><title>Does $\ell_p$-minimization outperform $\ell_1$-minimization?</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many application areas we are faced with the following question: Can we
recover a sparse vector $x_o \in \mathbb{R}^N$ from its undersampled set of
noisy observations $y \in \mathbb{R}^n$, $y=A x_o+w$. The last decade has
witnessed a surge of algorithms and theoretical results addressing this
question. One of the most popular algorithms is the $\ell_p$-penalized least
squares (LPLS) given by the following formulation: \[ \hat{x}(\lambda,p )\in
\arg\min_x \frac{1}{2}\|y - Ax\|_2^2+\lambda\|x\|_p^p, \] where $p \in [0,1]$.
Despite the non-convexity of these problems for $p&lt;1$, they are still appealing
because of the following folklores in compressed sensing: (i)
$\hat{x}(\lambda,p )$ is closer to $x_o$ than $\hat{x}(\lambda,1)$. (ii) If we
employ iterative methods that aim to converge to a local minima of LPLS, then
under good initialization these algorithms converge to a solution that is
closer to $x_o$ than $\hat{x}(\lambda,1)$. In spite of the existence of plenty
of empirical results that support these folklore theorems, the theoretical
progress to establish them has been very limited.
  This paper aims to study the above folklore theorems and establish their
scope of validity. Starting with approximate message passing algorithm as a
heuristic method for solving LPLS, we study the impact of initialization on the
performance of AMP. Then, we employ the replica analysis to show the connection
between the solution of $AMP$ and $\hat{x}(\lambda, p)$ in the asymptotic
settings. This enables us to compare the accuracy of $\hat{x}(\lambda,p)$ for
$p \in [0,1]$. In particular, we will characterize the phase transition and
noise sensitivity of LPLS for every $0\leq p\leq 1$ accurately. Our results in
the noiseless setting confirm that LPLS exhibits the same phase transition for
every $0\leq p &lt;1$ and this phase transition is much higher than that of LASSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03711</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03711</id><created>2015-01-15</created><authors><author><keyname>Rubio</keyname><forenames>Javier</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Olga</forenames></author><author><keyname>Pascual-Iserte</keyname><forenames>Antonio</forenames></author></authors><title>A Stochastic Approach for Resource Allocation with Backhaul and Energy
  Harvesting Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel stochastic radio resource allocation strategy that
achieves long-term fairness considering backhaul and air-interface capacity
limitations. The base station is considered to be only powered with a finite
battery that is recharged by an energy harvesting source. Such energy
harvesting is also taken into account in the proposed resource allocation
strategy. This technical scenario can be found in remote rural areas where the
backhaul connection is very limited and the base stations are fed with solar
panels of reduced size. Our results show that the proposed scheme achieves
higher fairness among the users and, in some cases, a higher sum-rate compared
with the well-known proportional fair scheduler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03715</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03715</id><created>2015-01-15</created><authors><author><keyname>Tixier</keyname><forenames>Audrey</forenames></author></authors><title>Blind identification of an unknown interleaved convolutional code</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give here an efficient method to reconstruct the block interleaver and
recover the convolutional code when several noisy interleaved codewords are
given. We reconstruct the block interleaver without assumption on its
structure. By running some experimental tests we show the efficiency of this
method even with moderate noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03719</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03719</id><created>2015-01-15</created><authors><author><keyname>Levi</keyname><forenames>Gil</forenames></author><author><keyname>Hassner</keyname><forenames>Tal</forenames></author></authors><title>LATCH: Learned Arrangements of Three Patch Codes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel means of describing local image appearances using binary
strings. Binary descriptors have drawn increasing interest in recent years due
to their speed and low memory footprint. A known shortcoming of these
representations is their inferior performance compared to larger, histogram
based descriptors such as the SIFT. Our goal is to close this performance gap
while maintaining the benefits attributed to binary representations. To this
end we propose the Learned Arrangements of Three Patch Codes descriptors, or
LATCH. Our key observation is that existing binary descriptors are at an
increased risk from noise and local appearance variations. This, as they
compare the values of pixel pairs; changes to either of the pixels can easily
lead to changes in descriptor values, hence damaging its performance. In order
to provide more robustness, we instead propose a novel means of comparing pixel
patches. This ostensibly small change, requires a substantial redesign of the
descriptors themselves and how they are produced. Our resulting LATCH
representation is rigorously compared to state-of-the-art binary descriptors
and shown to provide far better performance for similar computation and space
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03724</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03724</id><created>2015-01-15</created><authors><author><keyname>Avraham</keyname><forenames>Rinat Ben</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>A faster algorithm for the discrete Fr\'echet distance under translation</title><categories>cs.CG</categories><acm-class>F.2.2; I.3.5; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discrete Fr\'echet distance is a useful similarity measure for comparing
two sequences of points $P=(p_1,\ldots, p_m)$ and $Q=(q_1,\ldots,q_n)$. In many
applications, the quality of the matching can be improved if we let $Q$ undergo
some transformation relative to $P$. In this paper we consider the problem of
finding a translation of $Q$ that brings the discrete Fr\'echet distance
between $P$ and $Q$ to a minimum. We devise an algorithm that computes the
minimum discrete Fr\'echet distance under translation in $\mathbb{R}^2$, and
runs in $O(m^3n^2(1+\log(n/m))\log(m+n))$ time, assuming $m\leq n$. This
improves a previous algorithm of Jiang et al.~\cite{JXZ08}, which runs in
$O(m^3n^3 \log(m + n))$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03726</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03726</id><created>2015-01-12</created><updated>2015-04-10</updated><authors><author><keyname>Danezis</keyname><forenames>George</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Hansen</keyname><forenames>Marit</forenames></author><author><keyname>Hoepman</keyname><forenames>Jaap-Henk</forenames></author><author><keyname>Metayer</keyname><forenames>Daniel Le</forenames></author><author><keyname>Tirtea</keyname><forenames>Rodica</forenames></author><author><keyname>Schiffner</keyname><forenames>Stefan</forenames></author></authors><title>Privacy and Data Protection by Design - from policy to engineering</title><categories>cs.CR</categories><comments>79 pages in European Union Agency for Network and Information
  Security (ENISA) report, December 2014, ISBN 978-92-9204-108-3</comments><msc-class>94A60</msc-class><acm-class>K.4.1; D.4.6; H.2.0</acm-class><doi>10.2824/38623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy and data protection constitute core values of individuals and of
democratic societies. There have been decades of debate on how those values
-and legal obligations- can be embedded into systems, preferably from the very
beginning of the design process.
  One important element in this endeavour are technical mechanisms, known as
privacy-enhancing technologies (PETs). Their effectiveness has been
demonstrated by researchers and in pilot implementations. However, apart from a
few exceptions, e.g., encryption became widely used, PETs have not become a
standard and widely used component in system design. Furthermore, for unfolding
their full benefit for privacy and data protection, PETs need to be rooted in a
data governance strategy to be applied in practice.
  This report contributes to bridging the gap between the legal framework and
the available technological implementation measures by providing an inventory
of existing approaches, privacy design strategies, and technical building
blocks of various degrees of maturity from research and development. Starting
from the privacy principles of the legislation, important elements are
presented as a first step towards a design process for privacy-friendly systems
and services. The report sketches a method to map legal obligations to design
strategies, which allow the system designer to select appropriate techniques
for implementing the identified privacy requirements. Furthermore, the report
reflects limitations of the approach. It concludes with recommendations on how
to overcome and mitigate these limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03736</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03736</id><created>2015-01-15</created><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Gauthier-Umana</keyname><forenames>Val&#xe9;rie</forenames></author></authors><title>A Polynomial-Time Attack on the BBCRS Scheme</title><categories>cs.CR cs.IT math.IT</categories><comments>Accepted to the conference Public Key Cryptography (PKC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The BBCRS scheme is a variant of the McEliece public-key encryption scheme
where the hiding phase is performed by taking the inverse of a matrix which is
of the form $\mathbf{T} +\mathbf{R}$ where $\mathbf{T}$ is a sparse matrix with
average row/column weight equal to a very small quantity $m$, usually $m &lt; 2$,
and $\mathbf{R}$ is a matrix of small rank $z\geqslant 1$. The rationale of
this new transformation is the reintroduction of families of codes, like
generalized Reed-Solomon codes, that are famously known for representing
insecure choices. We present a key-recovery attack when $z = 1$ and $m$ is
chosen between $1$ and $1 + R + O( \frac{1}{\sqrt{n}} )$ where $R$ denotes the
code rate. This attack has complexity $O(n^6)$ and breaks all the parameters
suggested in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03737</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03737</id><created>2015-01-15</created><authors><author><keyname>Hirche</keyname><forenames>Christoph</forenames></author></authors><title>Polar codes in quantum information theory</title><categories>quant-ph cs.IT math.IT</categories><comments>Master's thesis, Leibniz Universit\&quot;at Hannover, 67 pages, 8 figures,
  chapter 3 partly based on arXiv:1409.7246</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are the first capacity achieving and efficiently implementable
codes for classical communication. Recently they have also been generalized to
communication over classical-quantum and quantum channels. In this work we
present our recent results for polar coding in quantum information theory,
including applications to classical-quantum multiple access channels,
interference channels and compound communication settings, including the first
proof of channel coding achieving the Han-Kobayashi rate region of the
interference channel without the need of a simultaneous decoder. Moreover we
add to the existing framework by extending polar codes to achieve the
asymmetric capacity and improving the block error probability for
classical-quantum channels. In addition we use polar codes to prove a new
achievable rate region for the classical-quantum broadcast channel. We also
discuss polar codes for quantum communication over quantum channels and state
results towards codes for compound quantum channels in this setting. We
conclude by stating a list of interesting open questions to invite further
research on the topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03755</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03755</id><created>2015-01-15</created><updated>2015-02-19</updated><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Screen Content Image Segmentation Using Least Absolute Deviation Fitting</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm for separating the foreground (mainly text and line
graphics) from the smoothly varying background in screen content images. The
proposed method is designed based on the assumption that the background part of
the image is smoothly varying and can be represented by a linear combination of
a few smoothly varying basis functions, while the foreground text and graphics
create sharp discontinuity and cannot be modeled by this smooth representation.
The algorithm separates the background and foreground using a least absolute
deviation method to fit the smooth model to the image pixels. This algorithm
has been tested on several images from HEVC standard test sequences for screen
content coding, and is shown to have superior performance over other popular
methods, such as k-means clustering based segmentation in DjVu and shape
primitive extraction and coding (SPEC) algorithm. Such background/foreground
segmentation are important pre-processing steps for text extraction and
separate coding of background and foreground for compression of screen content
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03757</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03757</id><created>2015-01-15</created><updated>2016-01-28</updated><authors><author><keyname>Osipov</keyname><forenames>Evgeny</forenames></author><author><keyname>Kleyko</keyname><forenames>Denis</forenames></author><author><keyname>Shapin</keyname><forenames>Alexey</forenames></author></authors><title>An Approach for Self-Adaptive Path Loss Modeling for Accurate
  Positioning in Underground Environments</title><categories>cs.NI</categories><comments>9 pages, 9 figures</comments><doi>10.1155/2016/3424768</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a real-time self-adaptive approach for accurate path loss
estimation in underground mines or tunnels based on signal strength
measurements from heterogeneous radio communication technologies. The proposed
model features simplicity of implementation. The methodology was validated in
simulations as well as was verified by measurements taken in real environments.
The proposed method leverages accuracy of positioning matching to the existing
approaches while requiring smaller engineering efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03771</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03771</id><created>2015-01-15</created><authors><author><keyname>Osokin</keyname><forenames>Anton</forenames></author><author><keyname>Vetrov</keyname><forenames>Dmitry</forenames></author></authors><title>Submodular relaxation for inference in Markov random fields</title><categories>cs.CV math.OC stat.ML</categories><comments>This paper is accepted for publication in IEEE Transactions on
  Pattern Analysis and Machine Intelligence</comments><doi>10.1109/TPAMI.2014.2369046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of finding the most probable state of a
discrete Markov random field (MRF), also known as the MRF energy minimization
problem. The task is known to be NP-hard in general and its practical
importance motivates numerous approximate algorithms. We propose a submodular
relaxation approach (SMR) based on a Lagrangian relaxation of the initial
problem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMR
does not decompose the graph structure of the initial problem but constructs a
submodular energy that is minimized within the Lagrangian relaxation. Our
approach is applicable to both pairwise and high-order MRFs and allows to take
into account global potentials of certain types. We study theoretical
properties of the proposed approach and evaluate it experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03779</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03779</id><created>2015-01-15</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Hampshire</keyname><forenames>Thomas E.</forenames></author><author><keyname>Helbren</keyname><forenames>Emma</forenames></author><author><keyname>Hu</keyname><forenames>Mingxing</forenames></author><author><keyname>Vega</keyname><forenames>Roser</forenames></author><author><keyname>Halligan</keyname><forenames>Steve</forenames></author><author><keyname>Hawkes</keyname><forenames>David J.</forenames></author></authors><title>Computer-assisted polyp matching between optical colonoscopy and CT
  colonography: a phantom study</title><categories>cs.CV</categories><comments>This paper was presented at the SPIE Medical Imaging 2014 conference</comments><journal-ref>Proc. SPIE 9036, Medical Imaging 2014: Image-Guided Procedures,
  Robotic Interventions, and Modeling, 903609 (March 12, 2014)</journal-ref><doi>10.1117/12.2042860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Potentially precancerous polyps detected with CT colonography (CTC) need to
be removed subsequently, using an optical colonoscope (OC). Due to large
colonic deformations induced by the colonoscope, even very experienced
colonoscopists find it difficult to pinpoint the exact location of the
colonoscope tip in relation to polyps reported on CTC. This can cause unduly
prolonged OC examinations that are stressful for the patient, colonoscopist and
supporting staff.
  We developed a method, based on monocular 3D reconstruction from OC images,
that automatically matches polyps observed in OC with polyps reported on prior
CTC. A matching cost is computed, using rigid point-based registration between
surface point clouds extracted from both modalities. A 3D printed and painted
phantom of a 25 cm long transverse colon segment was used to validate the
method on two medium sized polyps. Results indicate that the matching cost is
smaller at the correct corresponding polyp between OC and CTC: the value is 3.9
times higher at the incorrect polyp, comparing the correct match between polyps
to the incorrect match. Furthermore, we evaluate the matching of the
reconstructed polyp from OC with other colonic endoluminal surface structures
such as haustral folds and show that there is a minimum at the correct polyp
from CTC.
  Automated matching between polyps observed at OC and prior CTC would
facilitate the biopsy or removal of true-positive pathology or exclusion of
false-positive CTC findings, and would reduce colonoscopy false-negative
(missed) polyps. Ultimately, such a method might reduce healthcare costs,
patient inconvenience and discomfort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03783</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03783</id><created>2015-01-13</created><updated>2015-03-24</updated><authors><author><keyname>Kenny</keyname><forenames>Robert</forenames><affiliation>The University of Western Australia, Perth, Australia</affiliation></author></authors><title>Effective zero-dimensionality for computable metric spaces</title><categories>math.LO cs.LO</categories><comments>25 pages. To appear in Logical Methods in Computer Science. Results
  in Section 4 have been presented at CCA 2013</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 25,
  2015) lmcs:1023</journal-ref><doi>10.2168/LMCS-11(1:11)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin to study classical dimension theory from the computable analysis
(TTE) point of view. For computable metric spaces, several effectivisations of
zero-dimensionality are shown to be equivalent. The part of this
characterisation that concerns covering dimension extends to higher dimensions
and to closed shrinkings of finite open covers. To deal with zero-dimensional
subspaces uniformly, four operations (relative to the space and a class of
subspaces) are defined; these correspond to definitions of inductive and
covering dimensions and a countable basis condition. Finally, an effective
retract characterisation of zero-dimensionality is proven under an effective
compactness condition. In one direction this uses a version of the construction
of bilocated sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03784</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03784</id><created>2015-01-15</created><authors><author><keyname>Kleyko</keyname><forenames>Denis</forenames></author><author><keyname>Osipov</keyname><forenames>Evgeny</forenames></author><author><keyname>Senior</keyname><forenames>Alexander</forenames></author><author><keyname>Khan</keyname><forenames>Asad I.</forenames></author><author><keyname>&#x15e;ekercio&#x11f;lu</keyname><forenames>Y. Ahmet</forenames></author></authors><title>Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern
  Processing</title><categories>cs.AI</categories><comments>9 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes the use of Vector Symbolic Architectures for
implementing Hierarchical Graph Neuron, an architecture for memorizing patterns
of generic sensor stimuli. The adoption of a Vector Symbolic representation
ensures a one-layered design for the approach, while maintaining the previously
reported properties and performance characteristics of Hierarchical Graph
Neuron, and also improving the noise resistance of the architecture. The
proposed architecture enables a linear (with respect to the number of stored
entries) time search for an arbitrary sub-pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03786</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03786</id><created>2015-01-15</created><updated>2015-01-16</updated><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author></authors><title>Multi-view learning for multivariate performance measures optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the problem of optimizing multivariate performance
measures from multi-view data, and an effective method to solve it. This
problem has two features: the data points are presented by multiple views, and
the target of learning is to optimize complex multivariate performance
measures. We propose to learn a linear discriminant functions for each view,
and combine them to construct a overall multivariate mapping function for
mult-view data. To learn the parameters of the linear dis- criminant functions
of different views to optimize multivariate performance measures, we formulate
a optimization problem. In this problem, we propose to minimize the complexity
of the linear discriminant functions of each view, encourage the consistences
of the responses of different views over the same data points, and minimize the
upper boundary of a given multivariate performance measure. To optimize this
problem, we employ the cutting-plane method in an iterative algorithm. In each
iteration, we update a set of constrains, and optimize the mapping function
parameter of each view one by one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03796</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03796</id><created>2015-01-15</created><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>The Fast Convergence of Incremental PCA</title><categories>cs.LG stat.ML</categories><comments>NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a situation in which we see samples in $\mathbb{R}^d$ drawn
i.i.d. from some distribution with mean zero and unknown covariance A. We wish
to compute the top eigenvector of A in an incremental fashion - with an
algorithm that maintains an estimate of the top eigenvector in O(d) space, and
incrementally adjusts the estimate with each new data point that arrives. Two
classical such schemes are due to Krasulina (1969) and Oja (1983). We give
finite-sample convergence rates for both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03810</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03810</id><created>2015-01-15</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Fischer</keyname><forenames>Nicholas</forenames></author><author><keyname>Obuz</keyname><forenames>Serhat</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Time-Varying Input and State Delay Compensation for Uncertain Nonlinear
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust controller is developed for uncertain, second-order nonlinear
systems subject to simultaneous unknown, time-varying state delays and known,
time-varying input delays in addition to additive, sufficiently smooth
disturbances. An integral term composed of previous control values facilitates
a delay-free open-loop error system and the development of the feedback control
structure. A stability analysis based on Lyapunov-Krasovskii (LK) functionals
guarantees uniformly ultimately bounded tracking under the assumption that the
delays are bounded and slowly varying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03834</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03834</id><created>2015-01-15</created><authors><author><keyname>Famutimi</keyname><forenames>Rantiola</forenames></author><author><keyname>Emuoyibofarhe</keyname><forenames>Ozichi</forenames></author><author><keyname>Akinpelu</keyname><forenames>Abiodun</forenames></author><author><keyname>Gambo</keyname><forenames>Ishaya</forenames></author><author><keyname>Odeleye</keyname><forenames>Damilola</forenames></author></authors><title>Development of a multifactor authentication result checker system
  through GSM</title><categories>cs.CY</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is an implementation of a multifactor authentication SMS based
result checking system. The objectives of this work were to improve on the
available authentication methods and apply it on examination result checking
system. The work takes care of only course codes with their grades, the current
GPA and the overall CGPA. It employs the Pull SMS service, built on an
independent service and a modem. Examination results consist of sensitive
information, hence the need to further enhance the ones already in place so as
to ensure further privacy and integrity, In the course of this project, the
following assumptions were made: That a system that does the computation of
students' result, calculate of GPA and CGPA is already in place. The
implemented system was connected to the database of the existing system. A
database that contains the bio-data of each student admitted exists. That SMTP
(Simple Mail Transport Protocol) modem exists and should have been used but to
reduce cost, a modem that can act like a SIM browser is used with a standard
SIM card inserted in it and connected via cable to the application server. The
system showed that further security and privacy could be achieved when
multifactor authentication is employed. For further work, the system could be
developed and built as a dependent service which involves having the
application server connected to the service provider's SMS Center (SMSC).
KEYWORDS Multifactor Authentication, GSM, Telecommunication, Communication,
Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03837</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03837</id><created>2015-01-15</created><authors><author><keyname>Abdelkader</keyname><forenames>Ahmed</forenames></author><author><keyname>Acharya</keyname><forenames>Aditya</forenames></author><author><keyname>Dasler</keyname><forenames>Philip</forenames></author></authors><title>On the Complexity of Slide-and-Merge Games</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of a particular class of board games, which we call
`slide and merge' games. Namely, we consider 2048 and Threes, which are among
the most popular games of their type. In both games, the player is required to
slide all rows or columns of the board in one direction to create a high value
tile by merging pairs of equal tiles into one with the sum of their values.
This combines features from both block pushing and tile matching puzzles, like
Push and Bejeweled, respectively. We define a number of natural decision
problems on a suitable generalization of these games and prove NP-hardness for
2048 by reducing from 3SAT. Finally, we discuss the adaptation of our reduction
to Threes and conjecture a similar result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03838</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03838</id><created>2015-01-15</created><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>PAC-Bayes with Minimax for Confidence-Rated Transduction</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider using an ensemble of binary classifiers for transductive
prediction, when unlabeled test data are known in advance. We derive minimax
optimal rules for confidence-rated prediction in this setting. By using
PAC-Bayes analysis on these rules, we obtain data-dependent performance
guarantees without distributional assumptions on the data. Our analysis
techniques are readily extended to a setting in which the predictor is allowed
to abstain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03839</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03839</id><created>2015-01-15</created><authors><author><keyname>Klimov</keyname><forenames>Arkady</forenames></author></authors><title>Yet Another Way of Building Exact Polyhedral Model for Weakly Dynamic
  Affine Programs</title><categories>cs.PL</categories><comments>8 pages, 11 figures, was submitted to IMPACT-2015 (however was not
  accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact polyhedral model (PM) can be built in the general case if the only
control structures are {\tt do}-loops and structured {\tt if}s, and if loop
counter bounds, array subscripts and {\tt if}-conditions are affine expressions
of enclosing loop counters and possibly some integer constants. In more general
dynamic control programs, where arbitrary {\tt if}s and {\tt while}s are
allowed, in the general case the usual dataflow analysis can be only fuzzy.
This is not a problem when PM is used just for guiding the parallelizing
transformations, but is insufficient for transforming source programs to other
computation models (CM) relying on the PM, such as our version of dataflow CM
or the well-known KPN.
  The paper presents a novel way of building the exact polyhedral model and an
extension of the concept of the exact PM, which allowed us to add in a natural
way all the processing related to the data dependent conditions. Currently, in
our system, only arbirary {\tt if}s (not {\tt while}s) are allowed in input
programs. The resulting polyhedral model can be easily put out as an equivalent
program with the dataflow computation semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03844</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03844</id><created>2015-01-15</created><updated>2015-10-28</updated><authors><author><keyname>Zhang</keyname><forenames>Pan</forenames></author></authors><title>Evaluating accuracy of community detection using the relative normalized
  mutual information</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI stat.ML</categories><comments>comments are welcome</comments><doi>10.1088/1742-5468/2015/11/P11006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Normalized Mutual Information (NMI) has been widely used to evaluate the
accuracy of community detection algorithms. However in this article we show
that the NMI is seriously affected by systematic errors due to finite size of
networks, and may give a wrong estimate of performance of algorithms in some
cases. We give a simple theory to the finite-size effect of NMI and test our
theory numerically. Then we propose a new metric for the accuracy of community
detection, namely the relative Normalized Mutual Information (rNMI), which
considers statistical significance of the NMI by comparing it with the expected
NMI of random partitions. Our numerical experiments show that the rNMI
overcomes the finite-size effect of the NMI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03849</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03849</id><created>2015-01-15</created><authors><author><keyname>Fiedor</keyname><forenames>Tomas</forenames></author><author><keyname>Holik</keyname><forenames>Lukas</forenames></author><author><keyname>Lengal</keyname><forenames>Ondrej</forenames></author><author><keyname>Vojnar</keyname><forenames>Tomas</forenames></author></authors><title>Nested Antichains for WS1S</title><categories>cs.LO</categories><comments>Accepted to TACAS'15</comments><report-no>FIT-TR-2014-06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for coping with alternating quantification as the
main source of nonelementary complexity of deciding WS1S formulae. Our approach
is applicable within the state-of-the-art automata-based WS1S decision
procedure implemented, e.g. in MONA. The way in which the standard decision
procedure processes quantifiers involves determinization, with its worst case
exponential complexity, for every quantifier alternation in the prefix of a
formula. Our algorithm avoids building the deterministic automata---instead, it
constructs only those of their states needed for (dis)proving validity of the
formula. It uses a symbolic representation of the states, which have a deeply
nested structure stemming from the repeated implicit subset construction, and
prunes the search space by a nested subsumption relation, a generalization of
the one used by the so-called antichain algorithms for handling
nondeterministic automata. We have obtained encouraging experimental results,
in some cases outperforming MONA by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03854</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03854</id><created>2015-01-15</created><updated>2015-01-28</updated><authors><author><keyname>Vu</keyname><forenames>Kevin</forenames></author><author><keyname>Snyder</keyname><forenames>John</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Rupp</keyname><forenames>Matthias</forenames></author><author><keyname>Chen</keyname><forenames>Brandon F.</forenames></author><author><keyname>Khelif</keyname><forenames>Tarek</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Burke</keyname><forenames>Kieron</forenames></author></authors><title>Understanding Kernel Ridge Regression: Common behaviors from simple
  functions to density functionals</title><categories>physics.comp-ph cs.LG stat.ML</categories><comments>15 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate approximations to density functionals have recently been obtained
via machine learning (ML). By applying ML to a simple function of one variable
without any random sampling, we extract the qualitative dependence of errors on
hyperparameters. We find universal features of the behavior in extreme limits,
including both very small and very large length scales, and the noise-free
limit. We show how such features arise in ML models of density functionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03868</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03868</id><created>2015-01-15</created><updated>2015-02-17</updated><authors><author><keyname>Mauw</keyname><forenames>Sjouke</forenames></author><author><keyname>Radomirovic</keyname><forenames>Sasa</forenames></author></authors><title>Generalizing Multi-party Contract Signing</title><categories>cs.CR</categories><comments>Extended version of POST 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-party contract signing (MPCS) protocols allow a group of signers to
exchange signatures on a predefined contract. Previous approaches considered
either completely linear protocols or fully parallel broadcasting protocols. We
introduce the new class of DAG MPCS protocols which combines parallel and
linear execution and allows for parallelism even within a signer role. This
generalization is useful in practical applications where the set of signers has
a hierarchical structure, such as chaining of service level agreements and
subcontracting.
  Our novel DAG MPCS protocols are represented by directed acyclic graphs and
equipped with a labeled transition system semantics. We define the notion of
abort-chaining sequences and prove that a DAG MPCS protocol satisfies fairness
if and only if it does not have an abort-chaining sequence. We exhibit several
examples of optimistic fair DAG MPCS protocols. The fairness of these protocols
follows from our theory and has additionally been verified with our automated
tool.
  We define two complexity measures for DAG MPCS protocols, related to
execution time and total number of messages exchanged. We prove lower bounds
for fair DAG MPCS protocols in terms of these measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03872</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03872</id><created>2015-01-15</created><updated>2015-10-06</updated><authors><author><keyname>Barbosa</keyname><forenames>Andr&#xe9; Luiz</forenames></author></authors><title>The Dead Cryptographers Society Problem</title><categories>cs.CC cs.CR</categories><comments>5 pages and some new ideas on Cryptography!</comments><msc-class>Primary 94A60, Secondary 94A62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines The Dead Cryptographers Society Problem - DCS (where
several great cryptographers created many polynomial-time Deterministic Turing
Machines (DTMs) of a specific type, ran them on their proper descriptions
concatenated with some arbitrary strings, deleted them and leaved only the
results from those running, after they dyed: if those DTMs only permute the
bits on input, is it possible to decide the language formed by such resulting
strings within polynomial time?), proves some facts about its computational
complexity, and discusses some possible uses on Cryptography, such as into
distance keys distribution, online reverse auction and secure communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03879</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03879</id><created>2015-01-16</created><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. R.</forenames></author></authors><title>A new ADMM algorithm for the Euclidean median and its application to
  robust patch regression</title><categories>cs.CV</categories><comments>5 pages, 3 figures, 1 table. To appear in Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing, April 19-24, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Euclidean Median (EM) of a set of points $\Omega$ in an Euclidean space
is the point x minimizing the (weighted) sum of the Euclidean distances of x to
the points in $\Omega$. While there exits no closed-form expression for the EM,
it can nevertheless be computed using iterative methods such as the Wieszfeld
algorithm. The EM has classically been used as a robust estimator of centrality
for multivariate data. It was recently demonstrated that the EM can be used to
perform robust patch-based denoising of images by generalizing the popular
Non-Local Means algorithm. In this paper, we propose a novel algorithm for
computing the EM (and its box-constrained counterpart) using variable splitting
and the method of augmented Lagrangian. The attractive feature of this approach
is that the subproblems involved in the ADMM-based optimization of the
augmented Lagrangian can be resolved using simple closed-form projections. The
proposed ADMM solver is used for robust patch-based image denoising and is
shown to exhibit faster convergence compared to an existing solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03895</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03895</id><created>2015-01-16</created><authors><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Ouali-Sebti</keyname><forenames>Nadia</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>Root-Weighted Tree Automata and their Applications to Tree Kernels</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a new kind of weighted tree automata where the
weights are only supported by final states. We show that these automata are
sequentializable and we study their closures under classical regular and
algebraic operations. We then use these automata to compute the subtree kernel
of two finite tree languages in an efficient way. Finally, we present some
perspectives involving the root-weighted tree automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03915</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03915</id><created>2015-01-16</created><authors><author><keyname>Tangaro</keyname><forenames>Sabina</forenames></author><author><keyname>Amoroso</keyname><forenames>Nicola</forenames></author><author><keyname>Brescia</keyname><forenames>Massimo</forenames></author><author><keyname>Cavuoti</keyname><forenames>Stefano</forenames></author><author><keyname>Chincarini</keyname><forenames>Andrea</forenames></author><author><keyname>Errico</keyname><forenames>Rosangela</forenames></author><author><keyname>Inglese</keyname><forenames>Paolo</forenames></author><author><keyname>Longo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Maglietta</keyname><forenames>Rosalia</forenames></author><author><keyname>Tateo</keyname><forenames>Andrea</forenames></author><author><keyname>Riccio</keyname><forenames>Giuseppe</forenames></author><author><keyname>Bellotti</keyname><forenames>Roberto</forenames></author></authors><title>Feature Selection based on Machine Learning in MRIs for Hippocampal
  Segmentation</title><categories>physics.med-ph cs.CV cs.LG</categories><comments>To appear on &quot;Computational and Mathematical Methods in Medicine&quot;,
  Hindawi Publishing Corporation. 19 pages, 7 figures</comments><journal-ref>Computational and Mathematical Methods in Medicine Volume 2015,
  Article ID 814104, 10 pages, Hindawi Publishing Corporation</journal-ref><doi>10.1155/2015/814104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neurodegenerative diseases are frequently associated with structural changes
in the brain. Magnetic Resonance Imaging (MRI) scans can show these variations
and therefore be used as a supportive feature for a number of neurodegenerative
diseases. The hippocampus has been known to be a biomarker for Alzheimer
disease and other neurological and psychiatric diseases. However, it requires
accurate, robust and reproducible delineation of hippocampal structures. Fully
automatic methods are usually the voxel based approach, for each voxel a number
of local features were calculated. In this paper we compared four different
techniques for feature selection from a set of 315 features extracted for each
voxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrapper
methods, respectively, (ii) Sequential Forward Selection and (iii) Sequential
Backward Elimination; and (iv) embedded method based on the Random Forest
Classifier on a set of 10 T1-weighted brain MRIs and tested on an independent
set of 25 subjects. The resulting segmentations were compared with manual
reference labelling. By using only 23 features for each voxel (sequential
backward elimination) we obtained comparable state of-the-art performances with
respect to the standard tool FreeSurfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03924</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03924</id><created>2015-01-16</created><updated>2015-04-26</updated><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author><author><keyname>Xiao</keyname><forenames>Ling</forenames></author><author><keyname>Bandi</keyname><forenames>Rama Krishna</forenames></author></authors><title>On cyclic codes over $\mathbb{Z}_q+u\mathbb{Z}_q$</title><categories>cs.IT math.IT</categories><comments>11. arXiv admin note: text overlap with arXiv:1309.1623</comments><msc-class>94B05, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R=\mathbb{Z}_q+u\mathbb{Z}_q$, where $q=p^s$ and $u^2=0$. In this paper,
some structural properties of cyclic codes over the ring $R$ are considered. A
necessary and sufficient condition for cyclic codes over the ring $R$ to be
free is obtained and a BCH-type bound on the minimum Hamming distance for them
is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03931</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03931</id><created>2015-01-16</created><updated>2015-01-24</updated><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Wieseke</keyname><forenames>Nicolas</forenames></author></authors><title>On Symbolic Ultrametrics, Cotree Representations, and Cograph Edge
  Decompositions and Partitions</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic ultrametrics define edge-colored complete graphs K_n and yield a
simple tree representation of K_n. We discuss, under which conditions this idea
can be generalized to find a symbolic ultrametric that, in addition,
distinguishes between edges and non-edges of arbitrary graphs G=(V,E) and thus,
yielding a simple tree representation of G. We prove that such a symbolic
ultrametric can only be defined for G if and only if G is a so-called cograph.
A cograph is uniquely determined by a so-called cotree. As not all graphs are
cographs, we ask, furthermore, what is the minimum number of cotrees needed to
represent the topology of G. The latter problem is equivalent to find an
optimal cograph edge k-decomposition {E_1,...,E_k} of E so that each subgraph
(V,E_i) of G is a cograph. An upper bound for the integer k is derived and it
is shown that determining whether a graph has a cograph 2-decomposition, resp.,
2-partition is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03933</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03933</id><created>2015-01-16</created><updated>2015-07-17</updated><authors><author><keyname>Bosch</keyname><forenames>Thomas</forenames></author><author><keyname>Nolle</keyname><forenames>Andreas</forenames></author><author><keyname>Acar</keyname><forenames>Erman</forenames></author><author><keyname>Eckert</keyname><forenames>Kai</forenames></author></authors><title>RDF Validation Requirements - Evaluation and Logical Underpinning</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1504.04479</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many case studies for which the formulation of RDF constraints and
the validation of RDF data conforming to these constraint is very important. As
a part of the collaboration with the W3C and the DCMI working groups on RDF
validation, we identified major RDF validation requirements and initiated an
RDF validation requirements database which is available to contribute at
http://purl.org/net/rdf-validation. The purpose of this database is to
collaboratively collect case studies, use cases, requirements, and solutions
regarding RDF validation. Although, there are multiple constraint languages
which can be used to formulate RDF constraints (associated with these
requirements), there is no standard way to formulate them. This paper serves to
evaluate to which extend each requirement is satisfied by each of these
constraint languages. We take reasoning into account as an important
pre-validation step and therefore map constraints to DL in order to show that
each constraint can be mapped to an ontology describing RDF constraints
generically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03935</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03935</id><created>2015-01-16</created><updated>2015-05-13</updated><authors><author><keyname>Chernogorov</keyname><forenames>Fedor</forenames></author><author><keyname>Chernov</keyname><forenames>Sergey</forenames></author><author><keyname>Brigatti</keyname><forenames>Kimmo</forenames></author><author><keyname>Ristaniemi</keyname><forenames>Tapani</forenames></author></authors><title>Sequence-based Detection of Sleeping Cell Failures in Mobile Networks</title><categories>cs.NI cs.DB</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an automatic malfunction detection framework based on
data mining approach to analysis of network event sequences. The considered
environment is Long Term Evolution (LTE) for Universal Mobile Telecommunication
System (UMTS) with sleeping cell caused by random access channel failure.
Sleeping cell problem means unavailability of network service without triggered
alarm. The proposed detection framework uses N-gram analysis for identification
of abnormal behavior in sequences of network events. These events are collected
with Minimization of Drive Tests (MDT) functionality standardized in LTE.
Further processing applies dimensionality reduction, anomaly detection with
k-nearest neighbor, cross-validation, post-processing techniques and efficiency
evaluation. Different anomaly detection approaches proposed in this paper are
compared against each other with both classic data mining metrics, such as
F-score and receiver operating characteristic curves, and a newly proposed
heuristic approach. Achieved results demonstrate that the suggested method can
be used in modern performance monitoring systems for reliable, timely and
automatic detection of random access channel sleeping cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03952</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03952</id><created>2015-01-16</created><authors><author><keyname>Raj</keyname><forenames>Anant</forenames></author><author><keyname>Namboodiri</keyname><forenames>Vinay P.</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Mind the Gap: Subspace based Hierarchical Domain Adaptation</title><categories>cs.CV</categories><comments>4 pages in Second Workshop on Transfer and Multi-Task Learning:
  Theory meets Practice in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation techniques aim at adapting a classifier learnt on a source
domain to work on the target domain. Exploiting the subspaces spanned by
features of the source and target domains respectively is one approach that has
been investigated towards solving this problem. These techniques normally
assume the existence of a single subspace for the entire source / target
domain. In this work, we consider the hierarchical organization of the data and
consider multiple subspaces for the source and target domain based on the
hierarchy. We evaluate different subspace based domain adaptation techniques
under this setting and observe that using different subspaces based on the
hierarchy yields consistent improvement over a non-hierarchical baseline
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03959</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03959</id><created>2015-01-16</created><authors><author><keyname>Ciosek</keyname><forenames>Kamil</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Value Iteration with Options and State Aggregation</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a way of solving Markov Decision Processes that combines
state abstraction and temporal abstraction. Specifically, we combine state
aggregation with the options framework and demonstrate that they work well
together and indeed it is only after one combines the two that the full benefit
of each is realized. We introduce a hierarchical value iteration algorithm
where we first coarsely solve subgoals and then use these approximate solutions
to exactly solve the MDP. This algorithm solved several problems faster than
vanilla value iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03969</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03969</id><created>2015-01-16</created><authors><author><keyname>Janakiraman</keyname><forenames>Vijay Manikandan</forenames></author><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author><author><keyname>Assanis</keyname><forenames>Dennis</forenames></author></authors><title>Nonlinear Model Predictive Control of A Gasoline HCCI Engine Using
  Extreme Learning Machines</title><categories>cs.SY cs.NE</categories><comments>This paper was written as an extract from my PhD thesis (July 2013)
  and so references may not be to date as of this submission (Jan 2015). The
  article is in review and contains 10 figures, 35 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homogeneous charge compression ignition (HCCI) is a futuristic combustion
technology that operates with a high fuel efficiency and reduced emissions.
HCCI combustion is characterized by complex nonlinear dynamics which
necessitates a model based control approach for automotive application. HCCI
engine control is a nonlinear, multi-input multi-output problem with state and
actuator constraints which makes controller design a challenging task. Typical
HCCI controllers make use of a first principles based model which involves a
long development time and cost associated with expert labor and calibration. In
this paper, an alternative approach based on machine learning is presented
using extreme learning machines (ELM) and nonlinear model predictive control
(MPC). A recurrent ELM is used to learn the nonlinear dynamics of HCCI engine
using experimental data and is shown to accurately predict the engine behavior
several steps ahead in time, suitable for predictive control. Using the ELM
engine models, an MPC based control algorithm with a simplified quadratic
program update is derived for real time implementation. The working and
effectiveness of the MPC approach has been analyzed on a nonlinear HCCI engine
model for tracking multiple reference quantities along with constraints defined
by HCCI states, actuators and operational limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03975</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03975</id><created>2015-01-16</created><authors><author><keyname>Janakiraman</keyname><forenames>Vijay Manikandan</forenames></author><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author><author><keyname>Assanis</keyname><forenames>Dennis</forenames></author></authors><title>Stochastic Gradient Based Extreme Learning Machines For Online Learning
  of Advanced Combustion Engines</title><categories>cs.NE cs.LG cs.SY</categories><comments>This paper was written as an extract from my PhD thesis (July 2013)
  and so references may not be to date as of this submission (Jan 2015). The
  article is in review and contains 10 figures, 35 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a stochastic gradient based online learning algorithm for
Extreme Learning Machines (ELM) is developed (SG-ELM). A stability criterion
based on Lyapunov approach is used to prove both asymptotic stability of
estimation error and stability in the estimated parameters suitable for
identification of nonlinear dynamic systems. The developed algorithm not only
guarantees stability, but also reduces the computational demand compared to the
OS-ELM approach based on recursive least squares. In order to demonstrate the
effectiveness of the algorithm on a real-world scenario, an advanced combustion
engine identification problem is considered. The algorithm is applied to two
case studies: An online regression learning for system identification of a
Homogeneous Charge Compression Ignition (HCCI) Engine and an online
classification learning (with class imbalance) for identifying the dynamic
operating envelope of the HCCI Engine. The results indicate that the accuracy
of the proposed SG-ELM is comparable to that of the state-of-the-art but adds
stability and a reduction in computational effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03982</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03982</id><created>2015-01-16</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Timotheou</keyname><forenames>Stelios</forenames></author></authors><title>Exploring Green Interference Power for Wireless Information and Energy
  Transfer in the MISO Downlink</title><categories>cs.IT math.IT</categories><comments>5 pages, ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a power-efficient transfer of information and
energy, where we exploit the constructive part of wireless interference as a
source of green useful signal power. Rather than suppressing interference as in
conventional schemes, we take advantage of constructive interference among
users, inherent in the downlink, as a source of both useful information and
wireless energy. Specifically, we propose a new precoding design that minimizes
the transmit power while guaranteeing the quality of service (QoS) and energy
harvesting constraints for generic phase shift keying modulated signals. The
QoS constraints are modified to accommodate constructive interference. We
derive a sub-optimal solution and a local optimum solution to the precoding
optimization problem. The proposed precoding reduces the transmit power
compared to conventional schemes, by adapting the constraints to accommodate
constructive interference as a source of useful signal power. Our simulation
results show significant power savings with the proposed data-aided precoding
compared to the conventional precoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03983</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03983</id><created>2015-01-16</created><updated>2015-01-26</updated><authors><author><keyname>Prakash</keyname><forenames>N.</forenames></author><author><keyname>Krishnan</keyname><forenames>M. Nikhil</forenames></author></authors><title>The Storage-Repair-Bandwidth Trade-off of Exact Repair Linear
  Regenerating Codes for the Case $d = k = n-1$</title><categories>cs.IT math.IT</categories><comments>Corrected typos, minor editing for better readability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the setting of exact repair linear regenerating
codes. Under this setting, we derive a new outer bound on the
storage-repair-bandwidth trade-off for the case when $d = k = n -1$, where $(n,
k, d)$ are parameters of the regenerating code, with their usual meaning. Taken
together with the achievability result of Tian et. al. [1], we show that the
new outer bound derived here completely characterizes the trade-off for the
case of exact repair linear regenerating codes, when $d = k = n -1$. The new
outer bound is derived by analyzing the dual code of the linear regenerating
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03988</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03988</id><created>2015-01-16</created><updated>2015-01-19</updated><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>A One-Dimensional Physically Universal Cellular Automaton</title><categories>cs.FL math.DS</categories><comments>17 pages, 6 figures. Corrected an error in a figure</comments><msc-class>37B15</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical universality of a cellular automaton was defined by Janzing in 2010
as the ability to implement an arbitrary transformation of spatial patterns. In
2014, Schaeffer gave a construction of a two-dimensional physically universal
cellular automaton. We construct a one-dimensional version of the automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03992</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03992</id><created>2015-01-16</created><authors><author><keyname>Goles</keyname><forenames>Eric</forenames></author><author><keyname>Montealegre</keyname><forenames>Pedro</forenames></author><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>PSPACE-Completeness of Majority Automata Networks</title><categories>cs.DM</categories><comments>14 pages, 8 figures</comments><msc-class>68R10</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamics of majority automata networks when the vertices are
updated according to a block sequential updating scheme. In particular, we show
that the complexity of the problem of predicting an eventual state change in
some vertex, given an initial configuration, is PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03994</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03994</id><created>2015-01-16</created><authors><author><keyname>Gui</keyname><forenames>Yilin</forenames></author><author><keyname>Bui</keyname><forenames>Ha H.</forenames></author><author><keyname>Kodikara</keyname><forenames>Jayantha</forenames></author></authors><title>Numerical modelling of sandstone uniaxial compression test using a
  mix-mode cohesive fracture model</title><categories>cs.CE physics.geo-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mix-mode cohesive fracture model considering tension, compression and shear
material behaviour is presented, which has wide applications to geotechnical
problems. The model considers both elastic and inelastic displacements.
Inelastic displacement comprises fracture and plastic displacements. The norm
of inelastic displacement is used to control the fracture behaviour. Meantime,
a failure function describing the fracture strength is proposed. Using the
internal programming FISH, the cohesive fracture model is programmed into a
hybrid distinct element algorithm as encoded in Universal Distinct Element Code
(UDEC). The model is verified through uniaxial tension and direct shear tests.
The developed model is then applied to model the behaviour of a uniaxial
compression test on Gosford sandstone. The modelling results indicate that the
proposed cohesive fracture model is capable of simulating combined failure
behaviour applicable to rock.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03996</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03996</id><created>2015-01-16</created><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Capacity of electron-based communication over bacterial cables: the
  full-CSI case</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Journal on Selected Areas in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent discoveries of microbial communities that transfer
electrons across centimeter-length scales, this paper studies the information
capacity of bacterial cables via electron transfer, which coexists with
molecular communications, under the assumption of full causal channel state
information (CSI). The bacterial cable is modeled as an electron queue that
transfers electrons from the encoder at the electron donor source, which
controls the desired input electron intensity, to the decoder at the electron
acceptor sink. Clogging due to local ATP saturation along the cable is modeled.
A discrete-time scheme is investigated, enabling the computation of an
achievable rate. The regime of asymptotically small time-slot duration is
analyzed, and the optimality of binary input distributions is proved, i.e., the
encoder transmits at either maximum or minimum intensity, as dictated by the
physical constraints of the cable. A dynamic programming formulation of the
capacity is proposed, and the optimal binary signaling is determined via policy
iteration. It is proved that the optimal signaling has smaller intensity than
that given by the myopic policy, which greedily maximizes the instantaneous
information rate but neglects its effect on the steady-state cable
distribution. In contrast, the optimal scheme balances the tension between
achieving high instantaneous information rate, and inducing a favorable
steady-state distribution, such that those states characterized by high
information rates are visited more frequently, thus revealing the importance of
CSI. This work represents a first contribution towards the design of electron
signaling schemes in complex microbial structures, e.g., bacterial cables and
biofilms, where the tension between maximizing the transfer of information and
guaranteeing the well-being of the overall bacterial community arises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.03997</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.03997</id><created>2014-12-30</created><authors><author><keyname>Jang</keyname><forenames>Hwanchol</forenames></author><author><keyname>Yoon</keyname><forenames>Changhyeong</forenames></author><author><keyname>Chung</keyname><forenames>Euiheon</forenames></author><author><keyname>Choi</keyname><forenames>Wonshik</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author></authors><title>Holistic random encoding for imaging through multimode fibers</title><categories>physics.optics cs.CV</categories><comments>under review for possible publication in Optics express</comments><doi>10.1364/OE.23.006705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The input numerical aperture (NA) of multimode fiber (MMF) can be effectively
increased by placing turbid media at the input end of the MMF. This provides
the potential for high-resolution imaging through the MMF. While the input NA
is increased, the number of propagation modes in the MMF and hence the output
NA remains the same. This makes the image reconstruction process
underdetermined and may limit the quality of the image reconstruction. In this
paper, we aim to improve the signal to noise ratio (SNR) of the image
reconstruction in imaging through MMF. We notice that turbid media placed in
the input of the MMF transforms the incoming waves into a better format for
information transmission and information extraction. We call this
transformation as holistic random (HR) encoding of turbid media. By exploiting
the HR encoding, we make a considerable improvement on the SNR of the image
reconstruction. For efficient utilization of the HR encoding, we employ sparse
representation (SR), a relatively new signal reconstruction framework when it
is provided with a HR encoded signal. This study shows for the first time to
our knowledge the benefit of utilizing the HR encoding of turbid media for
recovery in the optically underdetermined systems where the output NA of it is
smaller than the input NA for imaging through MMF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04000</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04000</id><created>2015-01-16</created><authors><author><keyname>Bui</keyname><forenames>H. H.</forenames></author><author><keyname>Kodikara</keyname><forenames>J. A.</forenames></author><author><keyname>Pathegama</keyname><forenames>R.</forenames></author><author><keyname>Bouazza</keyname><forenames>A.</forenames></author><author><keyname>Haque</keyname><forenames>A.</forenames></author></authors><title>Large deformation and post-failure simulations of segmental retaining
  walls using mesh-free method (SPH)</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical methods are extremely useful in gaining insights into the behaviour
of reinforced soil retaining walls. However, traditional numerical approaches
such as limit equilibrium or finite element methods are unable to simulate
large deformation and post-failure behaviour of soils and retaining wall blocks
in the reinforced soil retaining walls system. To overcome this limitation, a
novel numerical approach is developed aiming to predict accurately the large
deformation and post-failure behaviour of soil and segmental wall blocks.
Herein, soil is modelled using an elasto-plastic constitutive model, while
segmental wall blocks are assumed rigid with full degrees of freedom. A soft
contact model is proposed to simulate the interaction between soil-block and
block-block. A two dimensional experiment of reinforced soil retaining walls
collapse was conducted to verify the numerical results. It is shown that the
proposed method can simulate satisfactory post-failure behaviour of segmental
wall blocks in reinforced soil retaining wall systems. The comparison showed
that the proposed method can provide satisfactory agreement with experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04001</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04001</id><created>2015-01-16</created><authors><author><keyname>Faro</keyname><forenames>Simone</forenames></author><author><keyname>K&#xfc;lekci</keyname><forenames>O&#x11f;uzhan</forenames></author></authors><title>Efficient Algorithms for the Order Preserving Pattern Matching Problem</title><categories>cs.DS</categories><comments>16 pages, 3 figures, submitted to SEA 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pattern x of length m and a text y of length n, both over an ordered
alphabet, the order-preserving pattern matching problem consists in finding all
substrings of the text with the same relative order as the pattern. It is an
approximate variant of the well known exact pattern matching problem which has
gained attention in recent years. This interesting problem finds applications
in a lot of fields as time series analysis, like share prices on stock markets,
weather data analysis or to musical melody matching. In this paper we present
two new filtering approaches which turn out to be much more effective in
practice than the previously presented methods. From our experimental results
it turns out that our proposed solutions are up to 2 times faster than the
previous solutions reducing the number of false positives up to 99%
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04002</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04002</id><created>2015-01-16</created><updated>2015-01-26</updated><authors><author><keyname>Engwirda</keyname><forenames>Darren</forenames></author><author><keyname>Ivers</keyname><forenames>David</forenames></author></authors><title>Size-optimal Steiner points for Delaunay-refinement on curved surfaces</title><categories>cs.CG cs.NA math.NA</categories><comments>Submitted to Computer-Aided Design (23rd International Meshing
  Roundtable special issue). A short version appears in the proceedings of the
  23rd International Meshing Roundtable. (v2 - revisions to description of
  point-placement scheme, figures.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extension of the restricted Delaunay-refinement algorithm for surface mesh
generation is described, where a new point-placement scheme is introduced to
improve element quality in the presence of mesh size constraints. Specifically,
it is shown that the use of off-centre Steiner points, positioned on the faces
of the associated Voronoi diagram, typically leads to significant improvements
in the shape- and size-quality of the resulting surface tessellations. The new
algorithm can be viewed as a Frontal-Delaunay approach -- a hybridisation of
conventional Delaunay-refinement and advancing-front techniques in which new
vertices are positioned to satisfy both element size and shape constraints. The
performance of the new scheme is investigated experimentally via a series of
comparative studies that contrast its performance with that of a typical
Delaunay-refinement technique. It is shown that the new method inherits many of
the best features of classical Delaunay-refinement and advancing-front type
methods, leading to the construction of smooth, high quality surface
triangulations with bounded radius-edge ratios and convergence guarantees.
Experiments are conducted using a range of complex benchmarks, verifying the
robustness and practical performance of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04006</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04006</id><created>2015-01-16</created><authors><author><keyname>Rajeev</keyname><forenames>P.</forenames></author><author><keyname>Bui</keyname><forenames>Ha H.</forenames></author><author><keyname>Sivakugan</keyname><forenames>N.</forenames></author></authors><title>Seismic Earth Pressure Development in Sheet Pile Retaining Walls: A
  Numerical Study</title><categories>cs.CE physics.geo-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of retaining walls requires the complete knowledge of the earth
pressure distribution behind the wall. Due to the complex soil-structure
effect, the estimation of earth pressure is not an easy task; even in the
static case. The problem becomes even more complex for the dynamic (i.e.,
seismic) analysis and design of retaining walls. Several earth pressure models
have been developed over the years to integrate the dynamic earth pressure with
the static earth pressure and to improve the design of retaining wall in
seismic regions. Among all the models, MononobeOkabe (M-O) method is commonly
used to estimate the magnitude of seismic earth pressures in retaining walls
and is adopted in design practices around the world (e.g., EuroCode and
Australian Standards). However, the M-O method has several drawbacks and does
not provide reliable estimate of the earth pressure in many instances. This
study investigates the accuracy of the M-O method to predict the dynamic earth
pressure in sheet pile wall. A 2D plane strain finite element model of the
wall-soil system was developed in DIANA. The backfill soil was modelled with
Mohr-Coulomb failure criterion while the wall was assumed behave elastically.
The numerically predicted dynamic earth pressure was compared with the M-O
model prediction. Further, the point of application of total dynamic force was
determined and compared with the static case. Finally, the applicability of M-O
methods to compute the seismic earth pressure was discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04009</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04009</id><created>2015-01-15</created><authors><author><keyname>Preim</keyname><forenames>Bernhard</forenames></author><author><keyname>Klemm</keyname><forenames>Paul</forenames></author><author><keyname>Hauser</keyname><forenames>Helwig</forenames></author><author><keyname>Hegenscheid</keyname><forenames>Katrin</forenames></author><author><keyname>Oeltze</keyname><forenames>Steffen</forenames></author><author><keyname>Toennies</keyname><forenames>Klaus</forenames></author><author><keyname>V&#xf6;lzke</keyname><forenames>Henry</forenames></author></authors><title>Visual Analytics of Image-Centric Cohort Studies in Epidemiology</title><categories>cs.CV cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemiology characterizes the influence of causes to disease and health
conditions of defined populations. Cohort studies are population-based studies
involving usually large numbers of randomly selected individuals and comprising
numerous attributes, ranging from self-reported interview data to results from
various medical examinations, e.g., blood and urine samples. Since recently,
medical imaging has been used as an additional instrument to assess risk
factors and potential prognostic information. In this chapter, we discuss such
studies and how the evaluation may benefit from visual analytics. Cluster
analysis to define groups, reliable image analysis of organs in medical imaging
data and shape space exploration to characterize anatomical shapes are among
the visual analytics tools that may enable epidemiologists to fully exploit the
potential of their huge and complex data. To gain acceptance, visual analytics
tools need to complement more classical epidemiologic tools, primarily
hypothesis-driven statistical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04010</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04010</id><created>2015-01-16</created><authors><author><keyname>Richter</keyname><forenames>Hendrik</forenames></author></authors><title>Coevolutionary intransitivity in games: A landscape analysis</title><categories>cs.NE q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intransitivity is supposed to be a main reason for deficits in coevolutionary
progress and inheritable superiority. Besides, coevolutionary dynamics is
characterized by interactions yielding subjective fitness, but aiming at
solutions that are superior with respect to an objective measurement. Such an
approximation of objective fitness may be, for instance, generalization
performance. In the paper a link between rating-- and ranking--based measures
of intransitivity and fitness landscapes that can address the dichotomy between
subjective and objective fitness is explored. The approach is illustrated by
numerical experiments involving a simple random game with continuously tunable
degree of randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04036</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04036</id><created>2015-01-16</created><authors><author><keyname>Koo</keyname><forenames>Namhun</forenames></author><author><keyname>Cho</keyname><forenames>Gook Hwa</forenames></author><author><keyname>Byeonghwan</keyname></author><author><keyname>Kwon</keyname><forenames>Soonhak</forenames></author></authors><title>An Improvement of the Cipolla-Lehmer Type Algorithms</title><categories>cs.CR</categories><msc-class>11T06, 11Y16, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let F_q be a finite field with q elements with prime power q and let r&gt;1 be
an integer with $q\equiv 1 \pmod{r}$. In this paper, we present a refinement of
the Cipolla-Lehmer type algorithm given by H. C. Williams, and subsequently
improved by K. S. Williams and K. Hardy. For a given r-th power residue c in
F_q where r is an odd prime, the algorithm of H. C. Williams determines a
solution of X^r=c in $O(r^3\log q)$ multiplications in F_q, and the algorithm
of K. S. Williams and K. Hardy finds a solution in $O(r^4+r^2\log q)$
multiplications in F_q. Our refinement finds a solution in $O(r^3+r^2\log q)$
multiplications in F_q. Therefore our new method is better than the previously
proposed algorithms independent of the size of r, and the implementation result
via SAGE shows a substantial speed-up compared with the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04038</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04038</id><created>2014-12-16</created><authors><author><keyname>McCamish</keyname><forenames>Ben</forenames></author><author><keyname>Meier</keyname><forenames>Rich</forenames></author><author><keyname>Landford</keyname><forenames>Jordan</forenames></author><author><keyname>Bass</keyname><forenames>Robert</forenames></author><author><keyname>Cotilla-Sanchez</keyname><forenames>Eduardo</forenames></author><author><keyname>Chiu</keyname><forenames>David</forenames></author></authors><title>A Data Driven Framework for Real Time Power System Event Detection and
  Visualization</title><categories>cs.DB physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increased adoption and deployment of phasor measurement units (PMU) has
provided valuable fine-grained data over the grid. Analysis over these data can
provide real-time insight into the health of the grid, thereby improving
control over operations. Realizing this data-driven control, however, requires
validating, processing and storing massive amounts of PMU data. This paper
describes a PMU data management system that supports input from multiple PMU
data streams, features an event-detection algorithm, and provides an efficient
method for retrieving archival data. The event-detection algorithm rapidly
correlates multiple PMU data streams, providing details on events occurring
within the power system in real-time. The event-detection algorithm feeds into
a visualization component, allowing operators to recognize events as they
occur. The indexing and data retrieval mechanism facilitates fast access to
archived PMU data. Using this method, we achieved over 30x speedup for queries
with high selectivity. With the development of these two components, we have
developed a system that allows efficient analysis of multiple time-aligned PMU
data streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04041</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04041</id><created>2015-01-16</created><authors><author><keyname>Verma</keyname><forenames>Kshitiz</forenames></author><author><keyname>Zaks</keyname><forenames>Shmuel</forenames></author><author><keyname>Garcia-Martinez</keyname><forenames>Alberto</forenames></author></authors><title>Designing Low Cost and Energy Efficient Access Network for the
  Developing World</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet is growing rapidly in the developing world now. Our survey of four
networks in India, all having at least one thousand users, suggest that both
installation cost and recurring cost due to power consumption pose a challenge
in its deployment in developing countries. In this paper, we first model the
access design problem by dividing the users in two types 1) those that may
access the network anytime and 2) those who need it only during office hours on
working days. The problem is formulated as a binary integer linear program
which turns out to be NP-hard. We then give a distributed heuristic for network
design. We evaluate our model and heuristic using real data collected from IIT
Kanpur LAN for more than 50 days. Results show that even in a tree topology --
which is a common characteristic of all networks who participated in our study,
our design can reduce the energy consumption of the network by up to 11% in
residential-cum-office environments and up to 22% in office-only environments
in comparison with current methods without giving up on the performance. The
extra cost incurred due to our design can be compensated in less than an year
by saving in electricity bill of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04044</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04044</id><created>2015-01-07</created><authors><author><keyname>Wen</keyname><forenames>Miles H. F.</forenames></author><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author><author><keyname>von Meier</keyname><forenames>Alexandra</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Phase Identification in Distribution Networks with Micro-Synchrophasors</title><categories>cs.SY math.GN physics.data-an</categories><comments>5 Pages, PESGM2015, Denver, CO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel phase identification method for distribution
networks where phases can be severely unbalanced and insufficiently labeled.
The analysis approach draws on data from high-precision phasor measurement
units (micro-synchrophasors or uPMUs) for distribution systems. A key fact is
that time-series voltage phasors taken from a distribution network show
specific patterns regarding connected phases at measurement points. The
algorithm is based on analyzing crosscorrelations over voltage magnitudes along
with phase angle differences on two candidate phases to be matched. If two
measurement points are on the same phase, large positive voltage magnitude
correlations and small voltage angle differences should be observed. The
algorithm is initially validated using the IEEE 13-bus model, and subsequently
with actual uPMU measurements on a 12-kV feeder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04053</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04053</id><created>2015-01-16</created><updated>2015-05-05</updated><authors><author><keyname>Hristopulos</keyname><forenames>Dionissios T.</forenames></author></authors><title>Stochastic Local Interaction (SLI) Model: Interfacing Machine Learning
  and Geostatistics</title><categories>cs.LG stat.ML</categories><comments>31 pages, 7 figures</comments><doi>10.1016/j.cageo.2015.05.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning and geostatistics are powerful mathematical frameworks for
modeling spatial data. Both approaches, however, suffer from poor scaling of
the required computational resources for large data applications. We present
the Stochastic Local Interaction (SLI) model, which employs a local
representation to improve computational efficiency. SLI combines geostatistics
and machine learning with ideas from statistical physics and computational
geometry. It is based on a joint probability density function defined by an
energy functional which involves local interactions implemented by means of
kernel functions with adaptive local kernel bandwidths. SLI is expressed in
terms of an explicit, typically sparse, precision (inverse covariance) matrix.
This representation leads to a semi-analytical expression for interpolation
(prediction), which is valid in any number of dimensions and avoids the
computationally costly covariance matrix inversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04056</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04056</id><created>2015-01-16</created><authors><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author></authors><title>NLP Solutions as Asymptotic Values of ODE Trajectories</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, it is shown that the solutions of general differentiable
constrained optimization problems can be viewed as asymptotic solutions to sets
of Ordinary Differential Equations (ODEs). The construction of the ODE
associated to the optimization problem is based on an exact penalty formulation
in which the weighting parameter dynamics is coordinated with that of the
decision variable so that there is no need to solve a sequence of optimization
problems, instead, a single ODE has to be solved using available efficient
methods. Examples are given in order to illustrate the results. This includes a
novel systematic approach to solve combinatoric optimization problems as well
as fast computation of a class of optimization problems using analogic circuits
leading to fast, parallel and highly scalable solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04058</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04058</id><created>2015-01-16</created><authors><author><keyname>Ahmad</keyname><forenames>Syed Amaar</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz A.</forenames></author></authors><title>Power Control and Soft Topology Adaptations in Multihop Cellular
  Networks with Multi-Point Connectivity</title><categories>cs.IT math.IT</categories><comments>14 pages, 12 figures, To appear in IEEE Transactions on
  Communications 2015, Index Terms: Resource allocation, hetereogenous
  networks, coordinated multipoint transmission, dual connectivity</comments><journal-ref>IEEE Transactions on Communications, Vol. 63, No. 3, PP. 683-694,
  March 2015</journal-ref><doi>10.1109/TCOMM.2015.2391188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The LTE standards account for the use of relays to enhance coverage near the
cell edge. In a traditional topology, a mobile can either establish a direct
link to the base station (BS) or a link to the relay, but not both. In this
paper, we consider the benefit of multipoint connectivity in allowing User
Equipment (UEs) to split their transmit power over simultaneous links to the BS
and the relay, in effect transmitting two parallel flows. We model decisions by
the UEs as to: (i) which point of access to attach to (either a relay or a
relay and the BS or only the BS); and (ii) how to allocate transmit power over
these links so as to maximize their total rate. We show that this flexibility
in the selection of points of access leads to substantial network capacity
increase against when nodes operate in a fixed network topology. Individual
adaptations by UEs, in terms of both point of access and transmit power, are
interdependent due to interference and to the possibility of over-loading of
the backhaul links. We show that these decisions can converge without any
explicit cooperation and derive a closed-form expression for the transmit power
levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04060</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04060</id><created>2015-01-16</created><authors><author><keyname>Yu</keyname><forenames>Tina</forenames></author><author><keyname>Ben-Av</keyname><forenames>Radel</forenames></author></authors><title>Analysis of Quantum Particle Automata for Solving the Density
  Classification Problem</title><categories>quant-ph cs.FL nlin.CG</categories><doi>10.1007/s11128-015-0930-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To advance our understanding of Quantum Cellular Automata in problem solving
through parallel and distributed computing, this research quantized the density
classification problem and adopted the Quantum Particle Automata (QPA) to solve
the quantized problem. In order to solve this problem, the QPA needed a unitary
operator to carry out the QPA evolution and a boundary partition to make the
classification decisions. We designed a Genetic Algorithm (GA) to search for
the unitary operators and the boundary partitions to classify the density of
binary inputs with length 5. The GA was able to find more than one unitary
operator that can transform the QPA in ways such that when the particle was
measured, it was more likely to collapse to the basis states that were on the
correct side of the boundary partition for the QPA to decide if the binary
input had majority density 0 or majority density 1. We analyzed these solutions
and found that the QPA evolution dynamic was driven by a particular parameter
$\theta$ of the unitary operator: a small $\theta$ gave the particle small mass
hence fast evolution while large $\theta$ had the opposite effect. While these
results are encouraging, scaling these solutions for binary inputs of arbitrary
length of $n$ requires additional analysis, which we will investigate in our
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04067</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04067</id><created>2015-01-14</created><authors><author><keyname>Butler</keyname><forenames>Steve</forenames></author><author><keyname>Graham</keyname><forenames>Ron</forenames></author><author><keyname>Stong</keyname><forenames>Richard</forenames></author></authors><title>Partition and sum is fast</title><categories>math.HO cs.DM</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following &quot;partition and sum&quot; operation on a natural number:
Treating the number as a long string of digits insert several plus signs in
between some of the digits and carry out the indicated sum. This results in a
smaller number and repeated application can always reduce the number to a
single digit. We show that surprisingly few iterations of this operation are
needed to get down to a single digit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04091</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04091</id><created>2015-01-16</created><authors><author><keyname>Lang</keyname><forenames>John C.</forenames></author><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author></authors><title>A Hierarchy of Linear Threshold Models for the Spread of Political
  Revolutions on Social Networks</title><categories>physics.soc-ph cs.SI</categories><msc-class>91D10, 91D30, 70G60, 37M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a linear threshold agent-based model (ABM) for the spread of
political revolutions on social networks using empirical network data. We
propose new techniques for building a hierarchy of simplified ordinary
differential equation (ODE) based models that aim to capture essential features
of the ABM, including effects of the actual networks, and give insight in the
parameter regime transitions of the ABM. We relate the ABM and the hierarchy of
models to a population-level compartmental ODE model that we proposed
previously for the spread of political revolutions [1], which is shown to be
mathematically consistent with the proposed ABM and provides a way to analyze
the global behaviour of the ABM. This consistency with the linear threshold ABM
also provides further justification a posteriori for the compartmental model of
[1]. Extending concepts from epidemiological modelling, we define a basic
reproduction number $R_0$ for the linear threshold ABM and apply it to predict
ABM behaviour on empirical networks. In small-scale numerical tests we
investigate experimentally the differences in spreading behaviour that occur
under the linear threshold ABM model when applied to some empirical online and
offline social networks, searching for quantitative evidence that political
revolutions may be facilitated by the modern online social networks of social
media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04100</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04100</id><created>2015-01-16</created><authors><author><keyname>Albarghouthi</keyname><forenames>Aws</forenames></author><author><keyname>Berdine</keyname><forenames>Josh</forenames></author><author><keyname>Cook</keyname><forenames>Byron</forenames></author><author><keyname>Kincaid</keyname><forenames>Zachary</forenames></author></authors><title>Spatial Interpolants</title><categories>cs.LO cs.PL</categories><comments>Short version published in ESOP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Splinter, a new technique for proving properties of
heap-manipulating programs that marries (1) a new separation logic-based
analysis for heap reasoning with (2) an interpolation-based technique for
refining heap-shape invariants with data invariants. Splinter is property
directed, precise, and produces counterexample traces when a property does not
hold. Using the novel notion of spatial interpolants modulo theories, Splinter
can infer complex invariants over general recursive predicates, e.g., of the
form all elements in a linked list are even or a binary tree is sorted.
Furthermore, we treat interpolation as a black box, which gives us the freedom
to encode data manipulation in any suitable theory for a given program (e.g.,
bit vectors, arrays, or linear arithmetic), so that our technique immediately
benefits from any future advances in SMT solving and interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04131</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04131</id><created>2015-01-16</created><updated>2015-02-27</updated><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Structure Learning and Statistical Estimation in Distribution Networks -
  Part I</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally power distribution networks are either not observable or only
partially observable. This complicates development and implementation of new
smart grid technologies, such as those related to demand response, outage
detection and management, and improved load-monitoring. In this two part paper,
inspired by proliferation of metering technology, we discuss estimation
problems in structurally loopy but operationally radial distribution grids from
measurements, e.g. voltage data, which are either already available or can be
made available with a relatively minor investment. In Part I, the objective is
to learn the operational layout of the grid. Part II of this paper presents
algorithms that estimate load statistics or line parameters in addition to
learning the grid structure. Further, Part II discusses the problem of
structure estimation for systems with incomplete measurement sets. Our newly
suggested algorithms apply to a wide range of realistic scenarios. The
algorithms are also computationally efficient -- polynomial in time -- which is
proven theoretically and illustrated computationally on a number of test cases.
The technique developed can be applied to detect line failures in real time as
well as to understand the scope of possible adversarial attacks on the grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04132</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04132</id><created>2015-01-16</created><authors><author><keyname>Heule</keyname><forenames>Stefan</forenames></author><author><keyname>Stefan</keyname><forenames>Deian</forenames></author><author><keyname>Yang</keyname><forenames>Edward Z.</forenames></author><author><keyname>Mitchell</keyname><forenames>John C.</forenames></author><author><keyname>Russo</keyname><forenames>Alejandro</forenames></author></authors><title>IFC Inside: Retrofitting Languages with Dynamic Information Flow Control
  (Extended Version)</title><categories>cs.PL cs.CR</categories><comments>Extended version of POST'15 paper; 31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many important security problems in JavaScript, such as browser extension
security, untrusted JavaScript libraries and safe integration of mutually
distrustful websites (mash-ups), may be effectively addressed using an
efficient implementation of information flow control (IFC). Unfortunately
existing fine-grained approaches to JavaScript IFC require modifications to the
language semantics and its engine, a non-goal for browser applications. In this
work, we take the ideas of coarse-grained dynamic IFC and provide the
theoretical foundation for a language-based approach that can be applied to any
programming language for which external effects can be controlled. We then
apply this formalism to server- and client-side JavaScript, show how it
generalizes to the C programming language, and connect it to the Haskell LIO
system. Our methodology offers design principles for the construction of
information flow control systems when isolation can easily be achieved, as well
as compositional proofs for optimized concrete implementations of these
systems, by relating them to their isolated variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04138</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04138</id><created>2015-01-16</created><authors><author><keyname>Ni</keyname><forenames>Chien-Chun</forenames></author><author><keyname>Lin</keyname><forenames>Yu-Yao</forenames></author><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng David</forenames></author><author><keyname>Saucan</keyname><forenames>Emil</forenames></author></authors><title>Ricci Curvature of the Internet Topology</title><categories>cs.SI cs.CG cs.NI physics.soc-ph</categories><comments>9 pages, 16 figures. To be appear on INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of Internet topologies has shown that the Internet topology has
negative curvature, measured by Gromov's &quot;thin triangle condition&quot;, which is
tightly related to core congestion and route reliability. In this work we
analyze the discrete Ricci curvature of the Internet, defined by Ollivier, Lin,
etc. Ricci curvature measures whether local distances diverge or converge. It
is a more local measure which allows us to understand the distribution of
curvatures in the network. We show by various Internet data sets that the
distribution of Ricci cuvature is spread out, suggesting the network topology
to be non-homogenous. We also show that the Ricci curvature has interesting
connections to both local measures such as node degree and clustering
coefficient, global measures such as betweenness centrality and network
connectivity, as well as auxilary attributes such as geographical distances.
These observations add to the richness of geometric structures in complex
network theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04140</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04140</id><created>2015-01-16</created><authors><author><keyname>Naimi</keyname><forenames>H. Miar</forenames></author><author><keyname>Salarian</keyname><forenames>M.</forenames></author></authors><title>A Fast Fractal Image Compression Algorithm Using Predefined Values for
  Contrast Scaling</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new fractal image compression algorithm is proposed in which
the time of encoding process is considerably reduced. The algorithm exploits a
domain pool reduction approach, along with using innovative predefined values
for contrast scaling factor, S, instead of scanning the parameter space [0,1].
Within this approach only domain blocks with entropies greater than a threshold
are considered. As a novel point, it is assumed that in each step of the
encoding process, the domain block with small enough distance shall be found
only for the range blocks with low activity (equivalently low entropy). This
novel point is used to find reasonable estimations of S, and use them in the
encoding process as predefined values, mentioned above. The algorithm has been
examined for some well-known images. This result shows that our proposed
algorithm considerably reduces the encoding time producing images that are
approximately the same in quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04143</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04143</id><created>2015-01-16</created><authors><author><keyname>Osipov</keyname><forenames>Ilya V.</forenames></author><author><keyname>Prasikova</keyname><forenames>Anna Y.</forenames></author><author><keyname>Volinsky</keyname><forenames>Alex A.</forenames></author></authors><title>Communication and games in the online foreign language educational
  system. User behavior study</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes creation and development of the educational online
communication platform for teaching and learning foreign languages. The system
is based on the time bank principle, allowing users to teach others their
native tongue along with taking foreign language lessons. The system is based
on the WebRTC technology, allowing users to access synchronized teaching
materials along with seeing and hearing each other. The platform is free for
the users with implemented gamification mechanics to motivate them. It is based
on the freemium model, where the main functions are provided free of charged
with some premium features. The paper describes studies associated with user
involvement in the learning/teaching process. The hypothesis whether two
previously unfamiliar individuals could communicate with each other using a
foreign language, based on the developed system algorithms, was tested. System
virality, where new users are attracted by the existing users was also studied,
along with user motivation for viral behavior. Relationships between
monetization, virality and user involvement were also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04147</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04147</id><created>2015-01-16</created><authors><author><keyname>de Silva</keyname><forenames>Vin</forenames></author><author><keyname>Munch</keyname><forenames>Elizabeth</forenames></author><author><keyname>Patel</keyname><forenames>Amit</forenames></author></authors><title>Categorified Reeb Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reeb graph is a construction which originated in Morse theory to study a
real valued function defined on a topological space. More recently, it has been
used in various applications to study noisy data which creates a desire to
define a measure of similarity between these structures. Here, we exploit the
fact that the category of Reeb graphs is equivalent to the category of a
particular class of cosheaf. Using this equivalency, we can define an
`interleaving' distance between Reeb graphs which is stable under the
perturbation of a function. Along the way, we obtain a natural construction for
smoothing a Reeb graph to reduce its topological complexity. The smoothed Reeb
graph can be constructed in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04152</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04152</id><created>2015-01-16</created><authors><author><keyname>Lo</keyname><forenames>Chun</forenames></author><author><keyname>Bai</keyname><forenames>Yechao</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author><author><keyname>Lynch</keyname><forenames>Jerome P.</forenames></author></authors><title>Efficient Sensor Fault Detection Using Group Testing</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When faulty sensors are rare in a network, diagnosing sensors individually is
inefficient. This study introduces a novel use of concepts from group testing
and Kalman filtering in detecting these rare faulty sensors with significantly
fewer number of tests. By assigning sensors to groups and performing Kalman
filter-based fault detection over these groups, we obtain binary detection
outcomes, which can then be used to recover the fault state of all sensors. We
first present this method using combinatorial group testing. We then present a
novel adaptive group testing method based on Bayesian inference. This adaptive
method further reduces the number of required tests and is suitable for noisy
group test systems. Compared to non-group testing methods, our algorithm
achieves similar detection accuracy with fewer tests and thus lower
computational complexity. Compared to other adaptive group testing methods, the
proposed method achieves higher accuracy when test results are noisy. We
perform extensive numerical analysis using a set of real vibration data
collected from the New Carquinez Bridge in California using an 18-sensor
network mounted on the bridge. We also discuss how the features of the Kalman
filter-based group test can be exploited in forming groups and further
improving the detection accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04155</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04155</id><created>2015-01-16</created><authors><author><keyname>Osipov</keyname><forenames>Ilya V.</forenames></author><author><keyname>Prasikova</keyname><forenames>Anna Y.</forenames></author><author><keyname>Volinsky</keyname><forenames>Alex A.</forenames></author></authors><title>Real Time Collaborative Platform for Learning and Teaching Foreign
  Languages</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes a novel social network-based open educational resource
for learning foreign languages in real time from native speakers, based on the
predefined teaching materials. This virtual learning platform, named i2istudy,
eliminates misunderstanding by providing prepared and predefined scenarios,
enabling the participants to understand each other and, as a consequence, to
communicate freely. The system allows communication through the real time video
and audio feed. In addition to establishing the communication, it tracks the
student progress and allows rating the instructor, based on the learner's
experience. The system went live in April 2014, and had over six thousand
active daily users, with over 40,000 total registered users. Currently
monetization is being added to the system, and time will show how popular the
system will become in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04156</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04156</id><created>2015-01-16</created><authors><author><keyname>Osipov</keyname><forenames>Ilya V.</forenames></author><author><keyname>Prasikova</keyname><forenames>Anna Y.</forenames></author><author><keyname>Volinsky</keyname><forenames>Alex A.</forenames></author></authors><title>User involvement in the partner program of the educational social
  network</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes experiments to attract active online system users to the
partner program. The objective is to grow the number of users by involving
existing system users in viral mechanics. Several examples of user motivation
are given, along with the specific interface implementations and viral
mechanics. Viral K-factor was used as the metrics for the resulting system
growth assessment. Specific examples show both positive and negative outcomes.
Growth of the target system parameters is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04157</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04157</id><created>2015-01-17</created><authors><author><keyname>Osipov</keyname><forenames>Ilya V.</forenames></author><author><keyname>Prasikova</keyname><forenames>Anna Y.</forenames></author><author><keyname>Volinsky</keyname><forenames>Alex A.</forenames></author></authors><title>Monetization as a Motivator for the Freemium Educational Platform Growth</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes user behavior as a result of introducing monetization in
the freemium educational online platform. Monetization resulted in alternative
system growth mechanisms, causing viral increase in the number of users. Given
different options, users choose the most advantageous and simple ones for them.
System metrics in terms of the K-factor was utilized as an indicator of the
system user base growth. The weekly K-factor almost doubled as a result of
monetization introduction. Monetization and viral growth can be both competing
and complementary mechanisms for the system growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04158</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04158</id><created>2015-01-17</created><updated>2015-07-28</updated><authors><author><keyname>S&#xfc;nderhauf</keyname><forenames>Niko</forenames></author><author><keyname>Dayoub</keyname><forenames>Feras</forenames></author><author><keyname>Shirazi</keyname><forenames>Sareh</forenames></author><author><keyname>Upcroft</keyname><forenames>Ben</forenames></author><author><keyname>Milford</keyname><forenames>Michael</forenames></author></authors><title>On the Performance of ConvNet Features for Place Recognition</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After the incredible success of deep learning in the computer vision domain,
there has been much interest in applying Convolutional Network (ConvNet)
features in robotic fields such as visual navigation and SLAM. Unfortunately,
there are fundamental differences and challenges involved. Computer vision
datasets are very different in character to robotic camera data, real-time
performance is essential, and performance priorities can be different. This
paper comprehensively evaluates and compares the utility of three
state-of-the-art ConvNets on the problems of particular relevance to navigation
for robots; viewpoint-invariance and condition-invariance, and for the first
time enables real-time place recognition performance using ConvNets with large
maps by integrating a variety of existing (locality-sensitive hashing) and
novel (semantic search space partitioning) optimization techniques. We present
extensive experiments on four real world datasets cultivated to evaluate each
of the specific challenges in place recognition. The results demonstrate that
speed-ups of two orders of magnitude can be achieved with minimal accuracy
degradation, enabling real-time performance. We confirm that networks trained
for semantic place categorization also perform better at (specific) place
recognition when faced with severe appearance changes and provide a reference
for which networks and layers are optimal for different aspects of the place
recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04163</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04163</id><created>2015-01-17</created><authors><author><keyname>Xia</keyname><forenames>Gui-Song</forenames></author><author><keyname>Liu</keyname><forenames>Gang</forenames></author><author><keyname>Yang</keyname><forenames>Wen</forenames></author></authors><title>Meaningful Objects Segmentation from SAR Images via A Multi-Scale
  Non-Local Active Contour Model</title><categories>cs.CV</categories><journal-ref>IEEE Trans. Geoscience and Remote Sensing, Vol. 54, No.4, 2015</journal-ref><doi>10.1109/TGRS.2015.2490078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The segmentation of synthetic aperture radar (SAR) images is a longstanding
yet challenging task, not only because of the presence of speckle, but also due
to the variations of surface backscattering properties in the images.
Tremendous investigations have been made to eliminate the speckle effects for
the segmentation of SAR images, while few work devotes to dealing with the
variations of backscattering coefficients in the images. In order to overcome
both the two difficulties, this paper presents a novel SAR image segmentation
method by exploiting a multi-scale active contour model based on the non-local
processing principle. More precisely, we first formulize the SAR segmentation
problem with an active contour model by integrating the non-local interactions
between pairs of patches inside and outside the segmented regions. Secondly, a
multi-scale strategy is proposed to speed up the non-local active contour
segmentation procedure and to avoid falling into local minimum for achieving
more accurate segmentation results. Experimental results on simulated and real
SAR images demonstrate the efficiency and feasibility of the proposed method:
it can not only achieve precise segmentations for images with heavy speckles
and non-local intensity variations, but also can be used for SAR images from
different types of sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04167</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04167</id><created>2015-01-17</created><authors><author><keyname>Li</keyname><forenames>Xiaowei</forenames></author><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Kim</keyname><forenames>Seok-Tae</forenames></author><author><keyname>Lee</keyname><forenames>In-Kwon</forenames></author></authors><title>An optical image encryption scheme based on depth-conversion integral
  imaging and chaotic maps</title><categories>cs.CR</categories><comments>18 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integral imaging-based cryptographic algorithms provides a new way to design
secure and robust image encryption schemes. In this paper, we introduce a
performance-enhanced image encryption schemes based on depth-conversion
integral imaging and chaotic maps, aiming to meet the requirements of secure
image transmission. First, the input image is decomposed into an elemental
image array (EIA) by utilizing a pinhole array. Then, the obtained image are
encrypted by combining the use of cellular automata and chaotic logistic maps.
In the image reconstruction process, the conventional computational integral
imaging reconstruction (CIIR) technique is a pixel-superposition technique; the
resolution of the reconstructed image is dramatically degraded due to the large
magnification in the superposition process as the pickup distance increases.
The smart mapping technique is introduced to improve the problem of CIIR. A
novel property of the proposed scheme is its depth-conversion ability, which
converts original elemental images recorded at long distance to ones recorded
near the pinhole array and consequently reduce the magnification factor. The
results of numerical simulations demonstrate the effectiveness and security of
this proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04177</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04177</id><created>2015-01-17</created><authors><author><keyname>Ceschia</keyname><forenames>Sara</forenames></author><author><keyname>Dang</keyname><forenames>Nguyen Thi Thanh</forenames></author><author><keyname>De Causmaecker</keyname><forenames>Patrick</forenames></author><author><keyname>Haspeslagh</keyname><forenames>Stefaan</forenames></author><author><keyname>Schaerf</keyname><forenames>Andrea</forenames></author></authors><title>Second International Nurse Rostering Competition (INRC-II) --- Problem
  Description and Rules ---</title><categories>cs.AI</categories><msc-class>68T37</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide all information to participate to the Second
International Nurse Rostering Competition (INRC-II). First, we describe the
problem formulation, which, differently from INRC-I, is a multi-stage
procedure. Second, we illustrate all the necessary infrastructure do be used
together with the participant's solver, including the testbed, the file
formats, and the validation/simulation tools. Finally, we state the rules of
the competition. All update-to-date information about the competition is
available at http://mobiz.vives.be/inrc2/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04183</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04183</id><created>2015-01-17</created><updated>2015-04-24</updated><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author></authors><title>Holographic Transformation, Belief Propagation and Loop Calculus for
  Generalized Probabilistic Theories</title><categories>cs.IT cond-mat.stat-mech math.IT quant-ph</categories><comments>6 pages, to appear in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The holographic transformation, belief propagation and loop calculus are
generalized to problems in generalized probabilistic theories including quantum
mechanics. In this work, the partition function of classical factor graph is
represented by an inner product of two high-dimensional vectors both of which
can be decomposed to tensor products of low-dimensional vectors. On the
representation, the holographic transformation is clearly understood by using
adjoint linear maps. Furthermore, on the formulation using inner product, the
belief propagation is naturally defined from the derivation of the loop
calculus formula. As a consequence, the holographic transformation, the belief
propagation and the loop calculus are generalized to measurement problems in
quantum mechanics and generalized probabilistic theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04186</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04186</id><created>2015-01-17</created><authors><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Muralidhar</keyname><forenames>Krishnamurty</forenames></author></authors><title>New Directions in Anonymization: Permutation Paradigm, Verifiability by
  Subjects and Intruders, Transparency to Users</title><categories>cs.DB cs.CR</categories><comments>27 pages, 3 figures</comments><msc-class>94-XX, 47N30</msc-class><acm-class>H.2.8; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are currently two approaches to anonymization: &quot;utility first&quot; (use an
anonymization method with suitable utility features, then empirically evaluate
the disclosure risk and, if necessary, reduce the risk by possibly sacrificing
some utility) or &quot;privacy first&quot; (enforce a target privacy level via a privacy
model, e.g., k-anonymity or epsilon-differential privacy, without regard to
utility). To get formal privacy guarantees, the second approach must be
followed, but then data releases with no utility guarantees are obtained. Also,
in general it is unclear how verifiable is anonymization by the data subject
(how safely released is the record she has contributed?), what type of intruder
is being considered (what does he know and want?) and how transparent is
anonymization towards the data user (what is the user told about methods and
parameters used?).
  We show that, using a generally applicable reverse mapping transformation,
any anonymization for microdata can be viewed as a permutation plus (perhaps) a
small amount of noise; permutation is thus shown to be the essential principle
underlying any anonymization of microdata, which allows giving simple utility
and privacy metrics. From this permutation paradigm, a new privacy model
naturally follows, which we call (d,v)-permuted privacy. The privacy ensured by
this method can be verified by each subject contributing an original record
(subject-verifiability) and also at the data set level by the data protector.
We then proceed to define a maximum-knowledge intruder model, which we argue
should be the one considered in anonymization. Finally, we make the case for
anonymization transparent to the data user, that is, compliant with Kerckhoff's
assumption (only the randomness used, if any, must stay secret).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04192</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04192</id><created>2015-01-17</created><authors><author><keyname>Shahabuddin</keyname><forenames>Shahriar</forenames></author><author><keyname>Janhunen</keyname><forenames>Janne</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author></authors><title>Design of a Transport Triggered Architecture Processor for Flexible
  Iterative Turbo Decoder</title><categories>cs.AR</categories><comments>6 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to meet the requirement of high data rates for the next generation
wireless systems, the efficient implementation of receiver algorithms is
essential. On the other hand, the rapid development of technology motivates the
investigation of programmable implementations. This paper summarizes the design
of a programmable turbo decoder as an applicationspecific instruction-set
processor (ASIP) using Transport Triggered Architecture (TTA). The processor
architecture is designed in such manner that it can be programmed to support
other receiver algorithms, for example, decoding based on the Viterbi
algorithm. Different suboptimal maximum a posteriori (MAP) algorithms are used
and compared to one another for the softinput soft-output (SISO) component
decoders in a single TTA processor. The max-log-MAP algorithm outperforms the
other suboptimal algorithms in terms of latency. The design enables the
designer to change the suboptimal algorithms according to the bit error rate
(BER) performance requirement. Unlike many other programmable turbo decoder
implementations, quadratic polynomial permutation (QPP) interleaver is used in
this work for contention-free memory access and to make the processor 3GPP LTE
compliant. Several optimization techniques to enable real time processing on
programmable platforms are introduced. Using our method, with a single
iteration 31.32 Mbps throughput is achieved for the max-log-MAP algorithm for a
clock frequency of 200 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04199</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04199</id><created>2015-01-17</created><updated>2015-01-20</updated><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Distributed Resource Allocation in D2D-Enabled Multi-tier Cellular
  Networks: An Auction Approach</title><categories>cs.NI cs.GT</categories><comments>A part of this work appeared in proceedings of IEEE ICC 2015 - Mobile
  and Wireless Networking Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless networks are expected to be highly heterogeneous with the
co-existence of macrocells and small cells as well as provide support for
device-to-device (D2D) communication. In such muti-tier heterogeneous systems
centralized radio resource allocation and interference management schemes will
not be scalable. In this work, we propose an auction-based distributed solution
to allocate radio resources in a muti-tier heterogeneous network. We provide
the bound of achievable data rate and show that the complexity of the proposed
scheme is linear with number of transmitter nodes and the available resources.
The signaling issues (e.g., information exchange over control channels) for the
proposed distributed solution is also discussed. Numerical results show the
effectiveness of proposed solution in comparison with a centralized resource
allocation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04200</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04200</id><created>2015-01-17</created><authors><author><keyname>Athley</keyname><forenames>Fredrik</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Gustavsson</keyname><forenames>Ulf</forenames></author></authors><title>Analysis of Massive MIMO With Hardware Impairments and Different Channel
  Models</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive Multiple-Input Multiple-Output (MIMO) is foreseen to be one of the
main technology components in next generation cellular communications (5G). In
this paper, fundamental limits on the performance of downlink massive MIMO
systems are investigated by means of simulations and analytical analysis.
Signal-to-noise-and-interference ratio (SINR) and sum rate for a single-cell
scenario multi-user MIMO are analyzed for different array sizes, channel
models, and precoding schemes. The impact of hardware impairments on
performance is also investigated. Simple approximations are derived that show
explicitly how the number of antennas, number of served users, transmit power,
and magnitude of hardware impairments affect performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04204</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04204</id><created>2015-01-17</created><updated>2015-01-26</updated><authors><author><keyname>Torrellas</keyname><forenames>Marc</forenames></author><author><keyname>Agustin</keyname><forenames>Adrian</forenames></author><author><keyname>Vidal</keyname><forenames>Josep</forenames></author></authors><title>Retrospective Interference Alignment for the MIMO Interference Broadcast
  Channel</title><categories>cs.IT math.IT</categories><comments>1 copyright page + 5 paper pages + 3 appendix pages, Submitted to
  IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) of the multiple-input multiple-output (MIMO)
Interference Broadcast Channel (IBC) with 2 cells and 2 users per cell are
investigated when only delayed channel state information is available at the
transmitter side (delayed CSIT). Retrospective Interference Alignment has shown
the benefits in terms of DoF of exploiting delayed CSIT for interference,
broadcast and also for the IBC. However, previous works studying the IBC with
delayed CSIT do not exploit the fact that the users of each cell are served by
a common transmitter. This work presents a four-phase precoding strategy taking
this into consideration. Assuming that transmitters and receivers are equipped
with $M,N$ antennas, respectively, new DoF inner bounds are proposed,
outperforming the existing ones for $\rho = \frac{M}{N} &gt; 2.6413$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04212</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04212</id><created>2015-01-17</created><updated>2015-08-05</updated><authors><author><keyname>Maitra</keyname><forenames>Arpita</forenames></author><author><keyname>De</keyname><forenames>Sourya Joyee</forenames></author><author><keyname>Paul</keyname><forenames>Goutam</forenames></author><author><keyname>Pal</keyname><forenames>Asim K.</forenames></author></authors><title>Proposal for Quantum Rational Secret Sharing</title><categories>quant-ph cs.CR</categories><journal-ref>Physical Review A, article 022305, volume 92, issue 2, August 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rational secret sharing scheme is a game in which each party responsible
for reconstructing a secret tries to maximize his utility by obtaining the
secret alone. Quantum secret sharing schemes, either derived from quantum
teleportation or from quantum error correcting code, do not succeed when we
assume rational participants. This is because all existing quantum secret
sharing schemes consider that the secret is reconstructed by a party chosen by
the dealer. In this paper, for the first time, we propose a quantum secret
sharing scheme which is resistant to rational parties. The proposed scheme is
fair (everyone gets the secret), correct and achieves strict Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04223</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04223</id><created>2015-01-17</created><authors><author><keyname>Jia</keyname><forenames>Shuqiao</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Optimal Signaling of MISO Full-Duplex Two-Way Wireless Channel</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE ICC 2015, London, UK</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We model the self-interference in a multiple input single output (MISO)
full-duplex two-way channel and evaluate the achievable rate region. We
formulate the boundary of the achievable rate region termed as the Pareto
boundary by a family of coupled, non-convex optimization problems. Our main
contribution is decoupling and reformulating the original non-convex
optimization problems to a family of convex semidefinite programming problems.
For a MISO full-duplex two-way channel, we prove that beamforming is an optimal
transmission strategy which can achieve any point on the Pareto boundary.
Furthermore, we present a closed-form expression for the optimal beamforming
weights. In our numerical examples we quantify gains in the achievable rates of
the proposed beamforming over the zero-forcing beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04228</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04228</id><created>2015-01-17</created><updated>2015-08-20</updated><authors><author><keyname>Kennedy</keyname><forenames>Hugh L.</forenames></author></authors><title>Improved IIR Low-Pass Smoothers and Differentiators with Tunable Delay</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. International Conference on Digital Image
  Computing: Techniques and Applications (DICTA), Adelaide, 23rd-25th Nov. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression analysis using orthogonal polynomials in the time domain is used
to derive closed-form expressions for causal and non-causal filters with an
infinite impulse response (IIR) and a maximally-flat magnitude and delay
response. The phase response of the resulting low-order smoothers and
differentiators, with low-pass characteristics, may be tuned to yield the
desired delay in the pass band or for zero gain at the Nyquist frequency. The
filter response is improved when the shape of the exponential weighting
function is modified and discrete associated Laguerre polynomials are used in
the analysis. As an illustrative example, the derivative filters are used to
generate an optical-flow field and to detect moving ground targets, in real
video data collected from an airborne platform with an electro-optic sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04232</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04232</id><created>2015-01-17</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Hadiji</keyname><forenames>Fabian</forenames></author></authors><title>Maximum Entropy Models of Shortest Path and Outbreak Distributions in
  Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Properties of networks are often characterized in terms of features such as
node degree distributions, average path lengths, diameters, or clustering
coefficients. Here, we study shortest path length distributions. On the one
hand, average as well as maximum distances can be determined therefrom; on the
other hand, they are closely related to the dynamics of network spreading
processes. Because of the combinatorial nature of networks, we apply maximum
entropy arguments to derive a general, physically plausible model. In
particular, we establish the generalized Gamma distribution as a continuous
characterization of shortest path length histograms of networks or arbitrary
topology. Experimental evaluations corroborate our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04237</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04237</id><created>2015-01-17</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>Quantized linear systems on integer lattices: a frequency-based approach</title><categories>math.PR cs.DM math.DS math.NA</categories><comments>60 pages, 4 figures. This is a slightly edited version of two
  research reports: I.Vladimirov, Quantized linear systems on integer lattices:
  frequency-based approach, Parts I, II, CADSEM Reports 96-032, 96-033, October
  1996, Deakin University, Geelong, Victoria, Australia, issued while the
  author was with the Institute for Information Transmission Problems, the
  Russian Academy of Sciences, Moscow</comments><msc-class>65P20, 65G50, 60B10, 11K06, 11K70, 11L03, 37A05, 60D05, 60F17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The roundoff errors in computer simulations of continuous dynamical systems,
caused by finiteness of machine arithmetic, can lead to qualitative
discrepancies between phase portraits of the resulting spatially discretized
systems and the original systems. These effects can be modelled on a
multidimensional integer lattice by using a dynamical system obtained by
composing the transition operator of the original system with a quantizer. Such
models manifest pseudorandomness which can be studied using a rigorous
probability theoretic approach. To this end, the lattice $\mathbb{Z}^n$ is
endowed with a class of frequency measurable subsets and a spatial frequency
functional as a finitely additive probability measure on them. Using a
multivariate version of Weyl's equidistribution criterion, we introduce an
algebra of frequency measurable quasiperiodic subsets of the lattice. This
approach is applied to quantized linear systems with the transition operator $R
\circ L$, where $L$ is a nonsingular matrix of the original linear system in
$\mathbb{R}^n$, and the map $R$ commutes with the additive group of
translations of the lattice. For almost every $L$, the events associated with
the deviation of trajectories of the quantized and original systems are
frequency measurable quasiperiodic subsets of the lattice whose frequencies
involve geometric probabilities on finite-dimensional tori. Using the skew
products of measure preserving toral automorphisms, we prove mutual
independence and uniform distribution of the quantization errors and
investigate statistical properties of invertibility loss for the quantized
linear system, extending V.V.Voevodin's results. When $L$ is similar to an
orthogonal matrix, we establish a functional central limit theorem for the
deviations of trajectories of the quantized and original systems. These results
are demonstrated for rounded-off planar rotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04242</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04242</id><created>2015-01-17</created><updated>2015-12-24</updated><authors><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Tegn&#xe9;r</keyname><forenames>Jesper</forenames></author></authors><title>The Information-theoretic and Algorithmic Approach to Human, Animal and
  Artificial Cognition</title><categories>cs.AI</categories><comments>22 pages. Forthcoming in Gordana Dodig-Crnkovic and Raffaela
  Giovagnoli (eds). Representation and Reality: Humans, Animals and Machines,
  Springer Verlag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey concepts at the frontier of research connecting artificial, animal
and human cognition to computation and information processing---from the Turing
test to Searle's Chinese Room argument, from Integrated Information Theory to
computational and algorithmic complexity. We start by arguing that passing the
Turing test is a trivial computational problem and that its pragmatic
difficulty sheds light on the computational nature of the human mind more than
it does on the challenge of artificial intelligence. We then review our
proposed algorithmic information-theoretic measures for quantifying and
characterizing cognition in various forms. These are capable of accounting for
known biases in human behavior, thus vindicating a computational algorithmic
view of cognition as first suggested by Turing, but this time rooted in the
concept of algorithmic probability, which in turn is based on computational
universality while being independent of computational model, and which has the
virtue of being predictive and testable as a model theory of cognitive
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04244</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04244</id><created>2015-01-17</created><authors><author><keyname>Kursa</keyname><forenames>Miron B.</forenames></author></authors><title>Generalised Random Forest Space Overview</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming a view of the Random Forest as a special case of a nested ensemble
of interchangeable modules, we construct a generalisation space allowing one to
easily develop novel methods based on this algorithm. We discuss the role and
required properties of modules at each level, especially in context of some
already proposed RF generalisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04245</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04245</id><created>2015-01-17</created><updated>2015-03-23</updated><authors><author><keyname>Kopczynski</keyname><forenames>Eryk</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Complexity of Problems of Commutative Grammars</title><categories>cs.FL</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 25,
  2015) lmcs:875</journal-ref><doi>10.2168/LMCS-11(1:9)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider commutative regular and context-free grammars, or, in other
words, Parikh images of regular and context-free languages. By using linear
algebra and a branching analog of the classic Euler theorem, we show that,
under an assumption that the terminal alphabet is fixed, the membership problem
for regular grammars (given v in binary and a regular commutative grammar G,
does G generate v?) is P, and that the equivalence problem for context free
grammars (do G_1 and G_2 generate the same language?) is in $\mathrm{\Pi_2^P}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04254</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04254</id><created>2015-01-17</created><authors><author><keyname>Yan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Chen</keyname><forenames>Chang Wen</forenames></author></authors><title>Service Provisioning and Profit Maximization in Network-assisted
  Adaptive HTTP Streaming</title><categories>cs.NI cs.MM</categories><comments>ICIP 2015 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive HTTP streaming with centralized consideration of multiple streams
has gained increasing interest. It poses a special challenge that the interests
of both content provider and network operator need to be deliberately balanced.
More importantly, the adaptation strategy is required to be flexible enough to
be ported to various systems that work under different network environments,
QoE levels, and economic objectives. To address these challenges, we propose a
Markov Decision Process (MDP) based network-assisted adaptation framework,
wherein cost of buffering, significant playback variation, bandwidth management
and income of playback are jointly investigated. We then demonstrate its
promising service provisioning and maximal profit for a mobile network in which
fair or differentiated service is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04260</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04260</id><created>2015-01-17</created><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Disease spread over randomly switched large-scale networks</title><categories>cs.SY cs.SI q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study disease spread over a randomly switched network, which
is modeled by a stochastic switched differential equation based on the so
called $N$-intertwined model for disease spread over static networks. Assuming
that all the edges of the network are independently switched, we present
sufficient conditions for the convergence of infection probability to zero.
Though the stability theory for switched linear systems can naively derive a
necessary and sufficient condition for the convergence, the condition cannot be
used for large-scale networks because, for a network with $n$ agents, it
requires computing the maximum real eigenvalue of a matrix of size exponential
in $n$. On the other hand, our conditions that are based also on the spectral
theory of random matrices can be checked by computing the maximum real
eigenvalue of a matrix of size exactly $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04262</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04262</id><created>2015-01-17</created><updated>2015-09-07</updated><authors><author><keyname>White</keyname><forenames>Colin</forenames></author></authors><title>Lower Bounds in the Preprocessing and Query Phases of Routing Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, there has been a substantial amount of research in
finding routing algorithms designed specifically to run on real-world graphs.
In 2010, Abraham et al. showed upper bounds on the query time in terms of a
graph's highway dimension and diameter for the current fastest routing
algorithms, including contraction hierarchies, transit node routing, and hub
labeling. In this paper, we show corresponding lower bounds for the same three
algorithms. We also show how to improve a result by Milosavljevic which lower
bounds the number of shortcuts added in the preprocessing stage for contraction
hierarchies. We relax the assumption of an optimal contraction order (which is
NP-hard to compute), allowing the result to be applicable to real-world
instances. Finally, we give a proof that optimal preprocessing for hub labeling
is NP-hard. Hardness of optimal preprocessing is known for most routing
algorithms, and was suspected to be true for hub labeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04263</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04263</id><created>2015-01-17</created><updated>2015-06-14</updated><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Shitz</keyname><forenames>Shlomo Shamai</forenames></author></authors><title>On the Dirty Paper Channel with Fast Fading Dirt</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Costa`s &quot;writing on dirty paper&quot; result establishes that full state
pre-cancellation can be attained in the Gel`fand-Pinsker problem with additive
state and additive white Gaussian noise. This result holds under the
assumptions that full channel knowledge is available at both the transmitter
and the receiver. In this work we consider the scenario in which the state is
multiplied by an ergodic fading process which is not known at the encoder. We
study both the case in which the receiver has knowledge of the fading and the
case in which it does not: for both models we derive inner and outer bounds to
capacity and determine the distance between the two bounds when possible. For
the channel without fading knowledge at either the transmitter or the receiver,
the gap between inner and outer bounds is finite for a class of fading
distributions which includes a number of canonical fading models. In the
capacity approaching strategy for this class, the transmitter performs Costa`s
pre-coding against the mean value of the fading times the state while the
receiver treats the remaining signal as noise. For the case in which only the
receiver has knowledge of the fading, we determine a finite gap between inner
and outer bounds for two classes of discrete fading distribution. The first
class of distributions is the one in which there exists a probability mass
larger than one half while the second class is the one in which the fading is
uniformly distributed over values that are exponentially spaced apart.
Unfortunately, the capacity in the case of a continuous fading distribution
remains very hard to characterize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04264</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04264</id><created>2015-01-17</created><authors><author><keyname>Wang</keyname><forenames>Anyu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhifang</forenames></author></authors><title>Achieving Arbitrary Locality and Availability in Binary Codes</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $i$th coordinate of an $(n,k)$ code is said to have locality $r$ and
availability $t$ if there exist $t$ disjoint groups, each containing at most
$r$ other coordinates that can together recover the value of the $i$th
coordinate. This property is particularly useful for codes for distributed
storage systems because it permits local repair and parallel accesses of hot
data. In this paper, for any positive integers $r$ and $t$, we construct a
binary linear code of length $\binom{r+t}{t}$ which has locality $r$ and
availability $t$ for all coordinates. The information rate of this code attains
$\frac{r}{r+t}$, which is always higher than that of the direct product code,
the only known construction that can achieve arbitrary locality and
availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04265</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04265</id><created>2015-01-17</created><updated>2016-03-06</updated><authors><author><keyname>Situ</keyname><forenames>Haozhen</forenames></author></authors><title>Evolutionary Stable Strategies in Games with Fuzzy Payoffs</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionarily stable strategy (ESS) is a key concept in evolutionary game
theory. ESS provides an evolutionary stability criterion for biological, social
and economical behaviors. In this paper, we develop a new approach to evaluate
ESS in symmetric two player games with fuzzy payoffs. Particularly, every
strategy is assigned a fuzzy membership that describes to what degree it is an
ESS in presence of uncertainty. The fuzzy set of ESS characterize the nature of
ESS. The proposed approach avoids loss of any information that happens by the
defuzzification method in games and handles uncertainty of payoffs through all
steps of finding an ESS. We use the satisfaction function to compare fuzzy
payoffs, and adopts the fuzzy decision rule to obtain the membership function
of the fuzzy set of ESS. The theorem shows the relation between fuzzy ESS and
fuzzy Nash quilibrium. The numerical results illustrate the proposed method is
an appropriate generalization of ESS to fuzzy payoff games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04267</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04267</id><created>2015-01-18</created><updated>2015-01-19</updated><authors><author><keyname>Wang</keyname><forenames>Shuliang</forenames></author><author><keyname>Wang</keyname><forenames>Dakui</forenames></author><author><keyname>Li</keyname><forenames>Caoyuan</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author></authors><title>Comment on &quot;Clustering by fast search and find of density peaks&quot;</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1], a clustering algorithm was given to find the centers of clusters
quickly. However, the accuracy of this algorithm heavily depend on the
threshold value of d-c. Furthermore, [1] has not provided any efficient way to
select the threshold value of d-c, that is, one can have to estimate the value
of d_c depend on one's subjective experience. In this paper, based on the data
field [2], we propose a new way to automatically extract the threshold value of
d_c from the original data set by using the potential entropy of data field.
For any data set to be clustered, the most reasonable value of d_c can be
objectively calculated from the data set by using our proposed method. The same
experiments in [1] are redone with our proposed method on the same experimental
data set used in [1], the results of which shows that the problem to calculate
the threshold value of d_c in [1] has been solved by using our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04272</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04272</id><created>2015-01-18</created><updated>2016-02-14</updated><authors><author><keyname>Raviv</keyname><forenames>Netanel</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author></authors><title>Some Gabidulin Codes cannot be List Decoded Efficiently at any Radius</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gabidulin codes can be seen as the rank-metric equivalent of Reed-Solomon
codes. It was recently proven, using subspace polynomials, that Gabidulin codes
cannot be list decoded beyond the so-called Johnson radius. In another result,
cyclic subspace codes were constructed by inspecting the connection between
subspaces and their subspace polynomials. In this paper, these subspace codes
are used to prove two bounds on the list size in decoding certain Gabidulin
codes. The first bound is an existential one, showing that exponentially-sized
lists exist for codes with specific parameters. The second bound presents
exponentially-sized lists explicitly, for a different set of parameters. Both
bounds rule out the possibility of efficiently list decoding several families
of Gabidulin codes for any radius beyond half the minimum distance. Such a
result was known so far only for non-linear rank-metric codes, and not for
Gabidulin codes. Using a standard operation called lifting, identical results
also follow for an important class of constant dimension subspace codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04276</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04276</id><created>2015-01-18</created><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Correlation Adaptive Subspace Segmentation by Trace Lasso</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the subspace segmentation problem. Given a set of data
points drawn from a union of subspaces, the goal is to partition them into
their underlying subspaces they were drawn from. The spectral clustering method
is used as the framework. It requires to find an affinity matrix which is close
to block diagonal, with nonzero entries corresponding to the data point pairs
from the same subspace. In this work, we argue that both sparsity and the
grouping effect are important for subspace segmentation. A sparse affinity
matrix tends to be block diagonal, with less connections between data points
from different subspaces. The grouping effect ensures that the highly corrected
data which are usually from the same subspace can be grouped together. Sparse
Subspace Clustering (SSC), by using $\ell^1$-minimization, encourages sparsity
for data selection, but it lacks of the grouping effect. On the contrary,
Low-Rank Representation (LRR), by rank minimization, and Least Squares
Regression (LSR), by $\ell^2$-regularization, exhibit strong grouping effect,
but they are short in subset selection. Thus the obtained affinity matrix is
usually very sparse by SSC, yet very dense by LRR and LSR.
  In this work, we propose the Correlation Adaptive Subspace Segmentation
(CASS) method by using trace Lasso. CASS is a data correlation dependent method
which simultaneously performs automatic data selection and groups correlated
data together. It can be regarded as a method which adaptively balances SSC and
LSR. Both theoretical and experimental results show the effectiveness of CASS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04277</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04277</id><created>2015-01-18</created><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Lin</keyname><forenames>Min</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Correntropy Induced L2 Graph for Robust Subspace Clustering</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the robust subspace clustering problem, which aims to
cluster the given possibly noisy data points into their underlying subspaces. A
large pool of previous subspace clustering methods focus on the graph
construction by different regularization of the representation coefficient. We
instead focus on the robustness of the model to non-Gaussian noises. We propose
a new robust clustering method by using the correntropy induced metric, which
is robust for handling the non-Gaussian and impulsive noises. Also we further
extend the method for handling the data with outlier rows/features. The
multiplicative form of half-quadratic optimization is used to optimize the
non-convex correntropy objective function of the proposed models. Extensive
experiments on face datasets well demonstrate that the proposed methods are
more robust to corruptions and occlusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04281</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04281</id><created>2015-01-18</created><authors><author><keyname>Pansari</keyname><forenames>Pankaj</forenames></author><author><keyname>Rajagopalan</keyname><forenames>C.</forenames></author><author><keyname>Sundararajan</keyname><forenames>Ramasubramanian</forenames></author></authors><title>Grouping Entities in a Fleet by Community Detection in Network of
  Regression Models</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with grouping of entities in a fleet based on their
behavior. The behavior of each entity is characterized by its historical
dataset, which comprises a dependent variable, typically a performance measure,
and multiple independent variables, typically operating conditions. A
regression model built using this dataset is used as a proxy for the behavior
of an entity. The validation error of the model of one unit with respect to the
dataset of another unit is used as a measure of the difference in behavior
between two units. Grouping entities based on their behavior is posed as a
graph clustering problem with nodes representing regression models and edge
weights given by the validation errors. Specifically, we find communities in
this graph, having dense edge connections within and sparse connections
outside. A way to assess the goodness of grouping and finding the optimum
number of divisions is proposed. The algorithm and measures proposed are
illustrated with application to synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04282</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04282</id><created>2015-01-18</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Wang</keyname><forenames>Yunji</forenames></author><author><keyname>Jing</keyname><forenames>Bing-Yi</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>Regularized maximum correntropy machine</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the usage of regularized correntropy framework
for learning of classifiers from noisy labels. The class label predictors
learned by minimizing transitional loss functions are sensitive to the noisy
and outlying labels of training samples, because the transitional loss
functions are equally applied to all the samples. To solve this problem, we
propose to learn the class label predictors by maximizing the correntropy
between the predicted labels and the true labels of the training samples, under
the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we
regularize the predictor parameter to control the complexity of the predictor.
The learning problem is formulated by an objective function considering the
parameter regularization and MCC simultaneously. By optimizing the objective
function alternately, we develop a novel predictor learning algorithm. The
experiments on two chal- lenging pattern classification tasks show that it
significantly outperforms the machines with transitional loss functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04284</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04284</id><created>2015-01-18</created><authors><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author></authors><title>Pairwise Constraint Propagation on Multi-View Data</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a graph-based learning approach to pairwise constraint
propagation on multi-view data. Although pairwise constraint propagation has
been studied extensively, pairwise constraints are usually defined over pairs
of data points from a single view, i.e., only intra-view constraint propagation
is considered for multi-view tasks. In fact, very little attention has been
paid to inter-view constraint propagation, which is more challenging since
pairwise constraints are now defined over pairs of data points from different
views. In this paper, we propose to decompose the challenging inter-view
constraint propagation problem into semi-supervised learning subproblems so
that they can be efficiently solved based on graph-based label propagation. To
the best of our knowledge, this is the first attempt to give an efficient
solution to inter-view constraint propagation from a semi-supervised learning
viewpoint. Moreover, since graph-based label propagation has been adopted for
basic optimization, we develop two constrained graph construction methods for
interview constraint propagation, which only differ in how the intra-view
pairwise constraints are exploited. The experimental results in cross-view
retrieval have shown the promising performance of our inter-view constraint
propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04292</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04292</id><created>2015-01-18</created><authors><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Wen</keyname><forenames>Ji-Rong</forenames></author></authors><title>Image classification by visual bag-of-words refinement and reduction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new framework for visual bag-of-words (BOW) refinement
and reduction to overcome the drawbacks associated with the visual BOW model
which has been widely used for image classification. Although very influential
in the literature, the traditional visual BOW model has two distinct drawbacks.
Firstly, for efficiency purposes, the visual vocabulary is commonly constructed
by directly clustering the low-level visual feature vectors extracted from
local keypoints, without considering the high-level semantics of images. That
is, the visual BOW model still suffers from the semantic gap, and thus may lead
to significant performance degradation in more challenging tasks (e.g. social
image classification). Secondly, typically thousands of visual words are
generated to obtain better performance on a relatively large image dataset. Due
to such large vocabulary size, the subsequent image classification may take
sheer amount of time. To overcome the first drawback, we develop a graph-based
method for visual BOW refinement by exploiting the tags (easy to access
although noisy) of social images. More notably, for efficient image
classification, we further reduce the refined visual BOW model to a much
smaller size through semantic spectral clustering. Extensive experimental
results show the promising performance of the proposed framework for visual BOW
refinement and reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04298</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04298</id><created>2015-01-18</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Ma</keyname><forenames>Yutao</forenames></author></authors><title>A Hybrid Approach to Web Service Recommendation Based on QoS-Aware
  Rating and Ranking</title><categories>cs.IR cs.SE</categories><comments>23 pages, 9 figures, and 2 tables</comments><msc-class>68N99</msc-class><acm-class>D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of Web services with the same or similar functions increases
steadily on the Internet, nowadays more and more service consumers pay great
attention to the non-functional properties of Web services, also known as
quality of service (QoS), when finding and selecting appropriate Web services.
For most of the QoS-aware Web service recommendation systems, the list of
recommended Web services is generally obtained based on a rating-oriented
prediction approach, aiming at predicting the potential ratings that an active
user may assign to the unrated services as accurately as possible. However, in
some application scenarios, high accuracy of rating prediction may not
necessarily lead to a satisfactory recommendation result. In this paper, we
propose a ranking-oriented hybrid approach by combining the item-based
collaborative filtering and latent factor models to address the problem of Web
services ranking. In particular, the similarity between two Web services is
measured in terms of the correlation coefficient between their rankings instead
of between the traditional QoS ratings. Besides, we also improve the measure
NDCG (Normalized Discounted Cumulative Gain) for evaluating the accuracy of the
top K recommendations returned in ranked order. Comprehensive experiments on
the QoS data set composed of real-world Web services are conducted to test our
approach, and the experimental results demonstrate that our approach
outperforms other competing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04301</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04301</id><created>2015-01-18</created><updated>2015-05-18</updated><authors><author><keyname>Abdelnasser</keyname><forenames>Heba</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author><author><keyname>Harras</keyname><forenames>Khaled A.</forenames></author></authors><title>WiGest: A Ubiquitous WiFi-based Gesture Recognition System</title><categories>cs.HC</categories><comments>Accepted for publication in INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present WiGest: a system that leverages changes in WiFi signal strength to
sense in-air hand gestures around the user's mobile device. Compared to related
work, WiGest is unique in using standard WiFi equipment, with no
modi-fications, and no training for gesture recognition. The system identifies
different signal change primitives, from which we construct mutually
independent gesture families. These families can be mapped to distinguishable
application actions. We address various challenges including cleaning the noisy
signals, gesture type and attributes detection, reducing false positives due to
interfering humans, and adapting to changing signal polarity. We implement a
proof-of-concept prototype using off-the-shelf laptops and extensively evaluate
the system in both an office environment and a typical apartment with standard
WiFi access points. Our results show that WiGest detects the basic primitives
with an accuracy of 87.5% using a single AP only, including through-the-wall
non-line-of-sight scenarios. This accuracy in-creases to 96% using three
overheard APs. In addition, when evaluating the system using a multi-media
player application, we achieve a classification accuracy of 96%. This accuracy
is robust to the presence of other interfering humans, highlighting WiGest's
ability to enable future ubiquitous hands-free gesture-based interaction with
mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04305</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04305</id><created>2015-01-18</created><authors><author><keyname>Sandbichler</keyname><forenames>Michael</forenames></author><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Berer</keyname><forenames>Thomas</forenames></author><author><keyname>Burgholzer</keyname><forenames>Peter</forenames></author><author><keyname>Haltmeier</keyname><forenames>Markus</forenames></author></authors><title>A Novel Compressed Sensing Scheme for Photoacoustic Tomography</title><categories>math.NA cs.IT math.IT</categories><msc-class>45Q05, 94A08, 92C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speeding up the data acquisition is one of the central aims to advance
tomographic imaging. On the one hand, this reduces motion artifacts due to
undesired movements, and on the other hand this decreases the examination time
for the patient. In this article, we propose a new scheme for speeding up the
data collection process in photoacoustic tomography. Our proposal is based on
compressed sensing and reduces acquisition time and system costs while
maintaining image quality. As measurement data we use random combinations of
pressure values that we use to recover a complete set of pressure data prior to
the actual image reconstruction. We obtain theoretical recovery guarantees for
our compressed sensing scheme and support the theory by reconstruction results
on simulated data as well as on experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04309</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04309</id><created>2015-01-18</created><authors><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author></authors><title>Information Theory and its Relation to Machine Learning</title><categories>cs.IT cs.LG math.IT</categories><comments>10 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper, I first describe a new perspective on machine
learning (ML) by four basic problems (or levels), namely, &quot;What to learn?&quot;,
&quot;How to learn?&quot;, &quot;What to evaluate?&quot;, and &quot;What to adjust?&quot;. The paper stresses
more on the first level of &quot;What to learn?&quot;, or &quot;Learning Target Selection&quot;.
Towards this primary problem within the four levels, I briefly review the
existing studies about the connection between information theoretical learning
(ITL [1]) and machine learning. A theorem is given on the relation between the
empirically-defined similarity measure and information measures. Finally, a
conjecture is proposed for pursuing a unified mathematical interpretation to
learning target selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04310</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04310</id><created>2015-01-18</created><authors><author><keyname>Nagalapur</keyname><forenames>Keerthi Kumar</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author></authors><title>On Channel Estimation for 802.11p in Highly Time-Varying Vehicular
  Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 11 figures, conference</comments><journal-ref>IEEE International Conference on Communications (ICC), 10-14 June
  2014, pp.5659--5664</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular wireless channels are highly time-varying and the pilot pattern in
the 802.11p orthogonal frequency-division multiplexing frame has been shown to
be ill suited for long data packets. The high frame error rate in off-the-shelf
chipsets with noniterative receiver configurations is mostly due to the use of
outdated channel estimates for equalization. This paper deals with improving
the channel estimation in 802.11p systems using a cross layered approach, where
known data bits are inserted in the higher layers and a modified receiver makes
use of these bits as training data for improved channel estimation. We also
describe a noniterative receiver configuration for utilizing the additional
training bits and show through simulations that frame error rates close to the
case with perfect channel knowledge can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04312</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04312</id><created>2015-01-18</created><updated>2015-05-05</updated><authors><author><keyname>Xu</keyname><forenames>Zhinan</forenames></author><author><keyname>Gan</keyname><forenames>Mingming</forenames></author><author><keyname>Zemen</keyname><forenames>Thomas</forenames></author></authors><title>On the Degrees of Freedom for Opportunistic Interference Alignment with
  1-Bit Feedback: The 3 Cell Case</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic interference alignment (OIA) exploits channel randomness and
multiuser diversity by user selection. For OIA the transmitter needs channel
state information (CSI), which is usually measured on the receiver side and
sent to the transmitter side via a feedback channel. Lee and Choi show that $d$
degrees of freedom (DoF) per transmitter are achievable in a 3-cell MIMO
interference channel assuming perfect real-valued feedback. However, the
feedback of a real-valued variable still requires infinite rate. In this paper,
we investigate 1-bit quantization for opportunistic interference alignment
(OIA) in 3-cell interference channels. We prove that 1-bit feedback is
sufficient to achieve the optimal DoF $d$ in 3-cell MIMO interference channels
if the number of users per cell is scaled as ${\rm SNR}^{d^2}$. Importantly,
the required number of users for OIA with 1-bit feedback remains the same as
with real-valued feedback. For a given system configuration, we provide an
optimal choice of the 1-bit quantizer, which captures most of the capacity
provided by a system with real-valued feedback. Using our new 1-bit feedback
scheme for OIA, we compare OIA with IA and show that OIA has a much lower
complexity and provides a better rate in the practical operation region of a
cellular communication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04313</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04313</id><created>2015-01-18</created><authors><author><keyname>Elder</keyname><forenames>Murray</forenames></author><author><keyname>Taback</keyname><forenames>Jennifer</forenames></author></authors><title>Thompson's group F is 1-counter graph automatic</title><categories>math.GR cs.FL</categories><msc-class>20F65, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is not known whether Thompson's group F is automatic. With the recent
extensions of the notion of an automatic group to graph automatic by
Kharlampovich, Khoussainov and Miasnikov and then to C-graph automatic by the
authors, a compelling question is whether F is graph automatic or C-graph
automatic for an appropriate language class C. The extended definitions allow
the use of a symbol alphabet for the normal form language, replacing the
dependence on generating set. In this paper we construct a 1-counter graph
automatic structure for F based on the standard infinite normal form for group
elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04317</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04317</id><created>2015-01-18</created><authors><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author></authors><title>Semi-Active Control of the Sway Dynamics for Elevator Ropes</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the problem of rope sway dynamics control for elevator
systems. We choose to actuate the system with a semi-active damper mounted on
the top of the elevator car. We propose nonlinear controllers based on Lyapunov
theory, to actuate the semi-active damper and stabilize the rope sway dynamics.
We study the stability of the proposed controllers, and test their performances
on a numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04318</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04318</id><created>2015-01-18</created><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>A Generalized Affinity Propagation Clustering Algorithm for Nonspherical
  Cluster Discovery</title><categories>cs.LG cs.CV stat.ML</categories><comments>G-AP: a new (7th) member of the in-tree clustering family. 11 pages:
  1-7, texts and figures; 8-11, supplementary materials</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Clustering analysis aims to discover the underlying clusters in the data
points according to their similarities. It has wide applications ranging from
bioinformatics to astronomy. Here, we proposed a Generalized Affinity
Propagation (G-AP) clustering algorithm. Data points are first organized in a
sparsely connected in-tree (IT) structure by a physically inspired strategy.
Then, additional edges are added to the IT structure for those reachable nodes.
This expanded structure is subsequently trimmed by affinity propagation method.
Consequently, the underlying cluster structure, with separate clusters,
emerges. In contrast to other IT-based methods, G-AP is fully automatic and
takes as input the pairs of similarities between data points only. Unlike
affinity propagation, G-AP is capable of discovering nonspherical clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04321</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04321</id><created>2015-01-18</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Malisoff</keyname><forenames>Michael</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Ergodic Theorem for Stabilization of a Hyperbolic PDE Inspired by
  Age-Structured Chemostat</title><categories>math.OC cs.SY</categories><comments>34 pages, 3 figures</comments><msc-class>34K20 (34K05 35L04 35L60), 93D15 (92D40 93C15)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a feedback stabilization problem for a first-order hyperbolic
partial differential equation. The problem is inspired by the stabilization of
equilibrium age profiles for an age-structured chemostat, using the dilution
rate as the control. Two distinguishing features of the problem are that (a)
the PDE has a multiplicative (instead of an additive) input and (b) the state
is fed back to the inlet boundary. We provide a sampled-data feedback that
ensures stabilization under arbitrarily sparse sampling and that satisfies
input constraints. Our chemostat feedback does not require measurement of the
age profile, nor does it require exact knowledge of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04324</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04324</id><created>2015-01-18</created><authors><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Chen</keyname><forenames>Geliang</forenames></author></authors><title>Phrase Based Language Model For Statistical Machine Translation</title><categories>cs.CL</categories><comments>5 pages. This version of the paper was submitted for review to EMNLP
  2013. The title, the idea and the content of this paper was presented by the
  first author in the machine translation group meeting at the MSRA-NLC lab
  (Microsoft Research Asia, Natural Language Computing) on July 16, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider phrase based Language Models (LM), which generalize the commonly
used word level models. Similar concept on phrase based LMs appears in speech
recognition, which is rather specialized and thus less suitable for machine
translation (MT). In contrast to the dependency LM, we first introduce the
exhaustive phrase-based LMs tailored for MT use. Preliminary experimental
results show that our approach outperform word based LMs with the respect to
perplexity and translation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04325</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04325</id><created>2015-01-18</created><authors><author><keyname>Maaloe</keyname><forenames>Lars</forenames></author><author><keyname>Arngren</keyname><forenames>Morten</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Deep Belief Nets for Topic Modeling</title><categories>cs.CL cs.LG stat.ML</categories><comments>Accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning
  for Text Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying traditional collaborative filtering to digital publishing is
challenging because user data is very sparse due to the high volume of
documents relative to the number of users. Content based approaches, on the
other hand, is attractive because textual content is often very informative. In
this paper we describe large-scale content based collaborative filtering for
digital publishing. To solve the digital publishing recommender problem we
compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets
(DBN) that both find low-dimensional latent representations for documents.
Efficient retrieval can be carried out in the latent representation. We work
both on public benchmarks and digital media content provided by Issuu, an
online publishing platform. This article also comes with a newly developed deep
belief nets toolbox for topic modeling tailored towards performance evaluation
of the DBN model and comparisons to the LDA model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04328</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04328</id><created>2015-01-18</created><authors><author><keyname>Bronzino</keyname><forenames>Francesco</forenames></author><author><keyname>Stojadinovic</keyname><forenames>Dragoslav</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author><author><keyname>Raychaudhuri</keyname><forenames>Dipankar</forenames></author></authors><title>Exploiting Network Awareness to Enhance DASH Over Wireless</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of Dynamic Adaptive Streaming over HTTP (DASH) helped reduce
the consumption of resource in video delivery, but its client-based rate
adaptation is unable to optimally use the available end-to-end network
bandwidth. We consider the problem of optimizing the delivery of video content
to mobile clients while meeting the constraints imposed by the available
network resources. Observing the bandwidth available in the network's two main
components, core network, transferring the video from the servers to edge nodes
close to the client, and the edge network, which is in charge of transferring
the content to the user, via wireless links, we aim to find an optimal solution
by exploiting the predictability of future user requests of sequential video
segments, as well as the knowledge of available infrastructural resources at
the core and edge wireless networks in a given future time window. Instead of
regarding the bottleneck of the end-to-end connection as our throughput, we
distribute the traffic load over time and use intermediate nodes between the
server and the client for buffering video content to achieve higher throughput,
and ultimately significantly improve the Quality of Experience for the end user
in comparison with current solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04330</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04330</id><created>2015-01-18</created><updated>2015-08-03</updated><authors><author><keyname>Astolfi</keyname><forenames>Daniele</forenames></author><author><keyname>Marconi</keyname><forenames>Lorenzo</forenames></author></authors><title>A High-Gain Nonlinear Observer with Limited Gain Power</title><categories>cs.SY</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we deal with a new observer for nonlinear systems of dimension n
in canonical observability form. We follow the standard high-gain paradigm, but
instead of having an observer of dimension n with a gain that grows up to power
n, we design an observer of dimension 2n-2 with a gain that grows up only to
power 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04343</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04343</id><created>2015-01-18</created><updated>2015-09-01</updated><authors><author><keyname>Wu</keyname><forenames>Xiaohu</forenames></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames></author></authors><title>Algorithms for Scheduling Deadline-Sensitive Malleable Tasks</title><categories>cs.DC cs.DS</categories><comments>Appeared at Allerton'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the ubiquitous batch data processing in cloud computing, the
fundamental model of scheduling malleable batch tasks and its extensions have
received significant attention recently. In this model, a set of n tasks is to
be scheduled on C identical machines and each task is specified by a value, a
workload, a deadline and a parallelism bound. Within the parallelism bound, the
number of the machines allocated to a task can vary over time and its workload
will not change accordingly. In this paper, the two core results of this paper
are to quantitatively characterize a sufficient and necessary condition such
that a set of malleable batch tasks with deadlines can be feasibly scheduled on
C machines, and to propose a polynomial time algorithm to produce such a
feasible schedule. The core results provide a conceptual tool and an optimal
scheduling algorithm to enable proposing new analysis and design of algorithms
or improving existing algorithms for extensive scheduling objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04344</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04344</id><created>2015-01-18</created><updated>2016-02-13</updated><authors><author><keyname>Zakablukov</keyname><forenames>Dmitry V.</forenames></author></authors><title>Asymptotic bounds of depth for a reversible circuit consisting of NOT,
  CNOT and 2-CNOT gates</title><categories>cs.ET</categories><comments>In Russian, 14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses the asymptotic depth of a reversible circuit consisting
of NOT, CNOT and 2-CNOT gates. Reversible circuit depth function $D(n, q)$ for
a circuit implementing a transformation $f\colon \mathbb Z_2^n \to \mathbb
Z_2^n$ is introduced as a function of $n$ and the number of additional inputs
$q$. It is proved that for the case of implementing a permutation from
$A(\mathbb Z_2^n)$ with a reversible circuit having no additional inputs the
depth is bounded as $D(n, 0) \gtrsim 2^n / (3\log_2 n)$. It is proved that for
the case of implementing a transformation $f\colon \mathbb Z_2^n \to \mathbb
Z_2^n$ with a reversible circuit having $q_0 \sim 2^n$ additional inputs the
depth is bounded as $D(n, q_0) \lesssim 3n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04346</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04346</id><created>2015-01-18</created><authors><author><keyname>Lan</keyname><forenames>Andrew S.</forenames></author><author><keyname>Vats</keyname><forenames>Divyanshu</forenames></author><author><keyname>Waters</keyname><forenames>Andrew E.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Mathematical Language Processing: Automatic Grading and Feedback for
  Open Response Mathematical Questions</title><categories>stat.ML cs.AI cs.CL cs.LG</categories><comments>ACM Conference on Learning at Scale, March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While computer and communication technologies have provided effective means
to scale up many aspects of education, the submission and grading of
assessments such as homework assignments and tests remains a weak link. In this
paper, we study the problem of automatically grading the kinds of open response
mathematical questions that figure prominently in STEM (science, technology,
engineering, and mathematics) courses. Our data-driven framework for
mathematical language processing (MLP) leverages solution data from a large
number of learners to evaluate the correctness of their solutions, assign
partial-credit scores, and provide feedback to each learner on the likely
locations of any errors. MLP takes inspiration from the success of natural
language processing for text data and comprises three main steps. First, we
convert each solution to an open response mathematical question into a series
of numerical features. Second, we cluster the features from several solutions
to uncover the structures of correct, partially correct, and incorrect
solutions. We develop two different clustering approaches, one that leverages
generic clustering algorithms and one based on Bayesian nonparametrics. Third,
we automatically grade the remaining (potentially large number of) solutions
based on their assigned cluster and one instructor-provided grade per cluster.
As a bonus, we can track the cluster assignment of each step of a multistep
solution and determine when it departs from a cluster of correct solutions,
which enables us to indicate the likely locations of errors to learners. We
test and validate MLP on real-world MOOC data to demonstrate how it can
substantially reduce the human effort required in large-scale educational
platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04348</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04348</id><created>2015-01-18</created><updated>2015-10-01</updated><authors><author><keyname>Podobnik</keyname><forenames>Boris</forenames></author><author><keyname>Horvatic</keyname><forenames>Davor</forenames></author><author><keyname>Lipic</keyname><forenames>Tomislav</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author><author><keyname>Buldu</keyname><forenames>Javier M.</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>The cost of attack in competing networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 two-column pages, 6 figures, supplementary material; accepted for
  publication in Journal of the Royal Society Interface</comments><journal-ref>J. R. Soc. Interface 12 (2015) 20150770</journal-ref><doi>10.1098/rsif.2015.0770</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world attacks can be interpreted as the result of competitive
interactions between networks, ranging from predator-prey networks to networks
of countries under economic sanctions. Although the purpose of an attack is to
damage a target network, it also curtails the ability of the attacker, which
must choose the duration and magnitude of an attack to avoid negative impacts
on its own functioning. Nevertheless, despite the large number of studies on
interconnected networks, the consequences of initiating an attack have never
been studied. Here, we address this issue by introducing a model of network
competition where a resilient network is willing to partially weaken its own
resilience in order to more severely damage a less resilient competitor. The
attacking network can take over the competitor nodes after their long
inactivity. However, due to a feedback mechanism the takeovers weaken the
resilience of the attacking network. We define a conservation law that relates
the feedback mechanism to the resilience dynamics for two competing networks.
Within this formalism, we determine the cost and optimal duration of an attack,
allowing a network to evaluate the risk of initiating hostilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04354</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04354</id><created>2015-01-18</created><updated>2015-08-16</updated><authors><author><keyname>Czajka</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Coinductive Techniques in Infinitary Lambda-Calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main aim of this paper is to promote a certain style of doing coinductive
proofs, similar to inductive proofs as commonly done by mathematicians. For
this purpose we provide a reasonably direct justification for coinductive
proofs written in this style, i.e., converting a coinductive proof into a
non-coinductive argument is purely a matter of routine. Our main interest is in
applying this coinductive style of arguments in infinitary lambda-calculus.
  In the second part of the paper we present a new coinductive proof of
confluence of B\&quot;ohm reduction in infinitary lambda-calculus. The proof is
simpler than previous proofs of this result. The technique of the proof is new,
i.e., it is not merely a coinductive reformulation of any earlier proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04358</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04358</id><created>2015-01-18</created><updated>2015-02-18</updated><authors><author><keyname>Smaldino</keyname><forenames>Paul E.</forenames></author></authors><title>Does Learning Imply a Decrease in the Entropy of Behavior?</title><categories>cs.RO cs.AI</categories><comments>14 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon's information entropy measures of the uncertainty of an event's
outcome. If learning about a system reflects a decrease in uncertainty, then a
plausible intuition is that learning should be accompanied by a decrease in the
entropy of the organism's actions and/or perceptual states. To address whether
this intuition is valid, I examined an artificial organism -- a simple robot --
that learned to navigate in an arena and analyzed the entropy of the outcome
variables action, state, and reward. Entropy did indeed decrease in the initial
stages of learning, but two factors complicated the scenario: (1) the
introduction of new options discovered during the learning process and (2) the
shifting patterns of perceptual and environmental states resulting from changes
to the robot's learned movement strategies. These factors lead to a subsequent
increase in entropy as the agent learned. I end with a discussion of the
utility of information-based characterizations of learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04359</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04359</id><created>2015-01-18</created><authors><author><keyname>Pelevina</keyname><forenames>Maria</forenames></author></authors><title>Realization and Extension of Abstract Operation Contracts for Program
  Logic</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For engineering software with formal correctness proofs it is crucial that
proofs can be efficiently reused in case the software or its specification is
changed. Unfortunately, in reality even slight changes in the code or its
specification often result in disproportionate waste of verification effort:
For instance, whenever a method's specification is modified and as a
consequence the proof of its correctness breaks, all other proofs based on this
specification break too. Abstract method calls is a recently proposed
verification rule for method calls that allows for efficient systematic reuse
of proofs. In this thesis, we implement, extend and evaluate this approach
within the KeY verification system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04366</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04366</id><created>2015-01-18</created><authors><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Source Compression with a Quantum Helper</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 2 figures. Version submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study classical source coding with quantum side-information where the
quantum side-information is observed by a helper and sent to the decoder via a
classical channel. We derive a single-letter characterization of the achievable
rate region for this problem. The direct part of our result is proved via the
measurement compression theory by Winter. Our result reveals that a helper's
scheme that separately conducts a measurement and a compression is suboptimal,
and the measurement compression is fundamentally needed to achieve the optimal
rate region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04367</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04367</id><created>2015-01-18</created><authors><author><keyname>Kulkarni</keyname><forenames>Kuldeep</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author></authors><title>Reconstruction-free action inference from compressive imagers</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent surveillance from camera networks, such as at parking lots, UAVs,
etc., often results in large amounts of video data, resulting in significant
challenges for inference in terms of storage, communication and computation.
Compressive cameras have emerged as a potential solution to deal with the data
deluge issues in such applications. However, inference tasks such as action
recognition require high quality features which implies reconstructing the
original video data. Much work in compressive sensing (CS) theory is geared
towards solving the reconstruction problem, where state-of-the-art methods are
computationally intensive and provide low-quality results at high compression
rates. Thus, reconstruction-free methods for inference are much desired. In
this paper, we propose reconstruction-free methods for action recognition from
compressive cameras at high compression ratios of 100 and above. Recognizing
actions directly from CS measurements requires features which are mostly
nonlinear and thus not easily applicable. This leads us to search for such
properties that are preserved in compressive measurements. To this end, we
propose the use of spatio-temporal smashed filters, which are compressive
domain versions of pixel-domain matched filters. We conduct experiments on
publicly available databases and show that one can obtain recognition rates
that are comparable to the oracle method in uncompressed setup, even for high
compression ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04370</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04370</id><created>2015-01-18</created><authors><author><keyname>He</keyname><forenames>Ru</forenames></author><author><keyname>Tian</keyname><forenames>Jin</forenames></author><author><keyname>Wu</keyname><forenames>Huaiqing</forenames></author></authors><title>Structure Learning in Bayesian Networks of Moderate Size by Efficient
  Sampling</title><categories>cs.AI cs.LG stat.ML</categories><comments>51 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Bayesian model averaging approach to learning Bayesian network
structures (DAGs) from data. We develop new algorithms including the first
algorithm that is able to efficiently sample DAGs according to the exact
structure posterior. The DAG samples can then be used to construct estimators
for the posterior of any feature. We theoretically prove good properties of our
estimators and empirically show that our estimators considerably outperform the
estimators from the previous state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04376</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04376</id><created>2015-01-18</created><authors><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Wang</keyname><forenames>Xiumin</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author></authors><title>Optimal Power Allocation for Secure Communications in Large-Scale MIMO
  Relaying Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, ICC 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we address the problem of optimal power allocation at the
relay in two-hop secure communications. In order to solve the challenging issue
of short-distance interception in secure communications, the benefit of
large-scale MIMO (LS-MIMO) relaying techniques is exploited to improve the
secrecy performance significantly, even in the case without eavesdropper
channel state information (CSI). The focus of this paper is on the analysis and
design of optimal power allocation for the relay, so as to maximize the secrecy
outage capacity. We reveal the condition that the secrecy outage capacity is
positive, prove that there is one and only one optimal power, and present an
optimal power allocation scheme. Moreover, the asymptotic characteristics of
the secrecy outage capacity is carried out to provide some clear insights for
secrecy performance optimization. Finally, simulation results validate the
effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04378</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04378</id><created>2015-01-18</created><updated>2015-07-25</updated><authors><author><keyname>Liu</keyname><forenames>Jinwu</forenames></author><author><keyname>Lu</keyname><forenames>Yao</forenames></author><author><keyname>Zhou</keyname><forenames>Tianfei</forenames></author></authors><title>Instance Significance Guided Multiple Instance Boosting for Robust
  Visual Tracking</title><categories>cs.CV</categories><comments>A version of this manuscript will be submitted to Multimedia
  Modelling 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Instance Learning (MIL) recently provides an appealing way to
alleviate the drifting problem in visual tracking. Following the
tracking-by-detection framework, an online MILBoost approach is developed that
sequentially chooses weak classifiers by maximizing the bag likelihood. In this
paper, we extend this idea towards incorporating the instance significance
estimation into the online MILBoost framework. First, instead of treating all
instances equally, with each instance we associate a significance-coefficient
that represents its contribution to the bag likelihood. The coefficients are
estimated by a simple Bayesian formula that jointly considers the predictions
from several standard MILBoost classifiers. Next, we follow the online boosting
framework, and propose a new criterion for the selection of weak classifiers.
Experiments with challenging public datasets show that the proposed method
outperforms both existing MIL based and boosting based trackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04388</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04388</id><created>2015-01-19</created><authors><author><keyname>Brimkov</keyname><forenames>Boris</forenames></author><author><keyname>Hicks</keyname><forenames>Illya V.</forenames></author></authors><title>Chromatic and flow polynomials of generalized vertex join graphs and
  outerplanar graphs</title><categories>cs.DM math.CO</categories><comments>14 pages</comments><msc-class>05C15</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized vertex join of a graph is obtained by joining an arbitrary
multiset of its vertices to a new vertex. We present a low-order polynomial
time algorithm for finding the chromatic polynomials of generalized vertex
joins of trees, and by duality we find the flow polynomials of arbitrary
outerplanar graphs. We also present closed formulas for the chromatic and flow
polynomials of vertex joins of cliques and cycles, otherwise known as
&quot;generalized wheel&quot; graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04394</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04394</id><created>2015-01-19</created><authors><author><keyname>Mori</keyname><forenames>Hiroki</forenames></author><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Band Splitting Permutations for Spatially Coupled LDPC Codes Enhancing
  Burst Erasure Immunity</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that spatially coupled (SC) codes with erasure-BP decoding
have powerful error correcting capability over memoryless erasure channels.
However, the decoding performance of SC-codes significantly degrades when they
are used over burst erasure channels. In this paper, we propose band splitting
permutations (BSP) suitable for $(l,r,L)$ SC-codes. The BSP splits a diagonal
band in a base matrix into multiple bands in order to enhance the span of the
stopping sets in the base matrix. As theoretical performance guarantees, lower
and upper bounds on the maximal burst correctable length of the permuted
$(l,r,L)$ SC-codes are presented. Those bounds indicate that the maximal
correctable burst ratio of the permuted SC-codes converges to 1/k where k=r/l.
This implies the asymptotic optimality of the permuted SC-codes in terms of
burst erasure correction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04395</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04395</id><created>2015-01-19</created><authors><author><keyname>Alem</keyname><forenames>Yibeltal F.</forenames></author><author><keyname>Khalid</keyname><forenames>Zubair</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>Spherical Harmonic Expansion of Fisher-Bingham Distribution and 3D
  Spatial Fading Correlation for Multiple-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>9 pages, 8 figures, IEEE Transactions on Vehicular Technology
  Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the 3D spatial fading correlation (SFC) resulting from
an angle-of-arrival (AoA) distribution that can be modelled by a mixture of
Fisher-Bingham distributions on the sphere. By deriving a closed-form
expression for the spherical harmonic transform for the component
Fisher-Bingham distributions, with arbitrary parameter values, we obtain a
closed-form expression of the 3D-SFC for the mixture case. The 3D-SFC
expression is general and can be used in arbitrary multi-antenna array
geometries and is demonstrated for the cases of a 2D uniform circular array in
the horizontal plane and a 3D regular dodecahedral array. In computational
aspects, we use recursions to compute the spherical harmonic coefficients and
give pragmatic guidelines on the truncation size in the series representations
to yield machine precision accuracy results. The results are further
corroborated through numerical experiments to demonstrate that the closed-form
expressions yield the same results as significantly more computationally
expensive numerical integration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04402</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04402</id><created>2015-01-19</created><authors><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author><author><keyname>Izumi</keyname><forenames>Taizuke</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author></authors><title>Subgraph Domatic Problem and Writing Capacity of Memory Devises with
  Restricted State Transitions</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code design problem for memory devises with restricted state transitions is
formulated as a combinatorial optimization problem that is called a subgraph
domatic partition (subDP) problem. If any neighbor set of a given state
transition graph contains all the colors, then the coloring is said to be
valid. The goal of a subDP problem is to find a valid coloring with the largest
number of colors for a subgraph of a given directed graph. The number of colors
in an optimal valid coloring gives the writing capacity of a given state
transition graph. The subDP problems are computationally hard; it is proved to
be NP-complete in this paper. One of our main contributions in this paper is to
show the asymptotic behavior of the writing capacity $C(G)$ for sequences of
dense bidirectional graphs, that is given by C(G)=Omega(n/ln n) where n is the
number of nodes. A probabilistic method called Lovasz local lemma (LLL) plays
an essential role to derive the asymptotic expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04412</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04412</id><created>2015-01-19</created><authors><author><keyname>A</keyname><forenames>Krishna Chaitanya</forenames></author><author><keyname>Muherji</keyname><forenames>Utpal</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Power Allocation Games on Interference Channels with Complete and
  Partial Information</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1409.7551</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless channel shared by multiple transmitter-receiver pairs.
Their transmissions interfere with each other. Each transmitter-receiver pair
aims to maximize its long-term average transmission rate subject to an average
power constraint. This scenario is modeled as a stochastic game under different
assumptions. We first assume that each transmitter and receiver has knowledge
of all direct and cross link channel gains. We later relax the assumption to
the knowledge of incident channel gains and then further relax to the knowledge
of the direct link channel gains only. In all the cases, we formulate the
problem of finding the Nash equilibrium as a variational inequality (VI)
problem and present an algorithm to solve the VI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04413</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04413</id><created>2015-01-19</created><authors><author><keyname>Ohzeki</keyname><forenames>Masayuki</forenames></author></authors><title>Statistical-mechanical analysis of pre-training and fine tuning in deep
  learning</title><categories>stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG</categories><comments>13 pages and 2 figures, to appear in JPSJ</comments><doi>10.7566/JPSJ.84.034003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a statistical-mechanical analysis of deep learning.
We elucidate some of the essential components of deep learning---pre-training
by unsupervised learning and fine tuning by supervised learning. We formulate
the extraction of features from the training data as a margin criterion in a
high-dimensional feature-vector space. The self-organized classifier is then
supplied with small amounts of labelled data, as in deep learning. Although we
employ a simple single-layer perceptron model, rather than directly analyzing a
multi-layer neural network, we find a nontrivial phase transition that is
dependent on the number of unlabelled data in the generalization error of the
resultant classifier. In this sense, we evaluate the efficacy of the
unsupervised learning component of deep learning. The analysis is performed by
the replica method, which is a sophisticated tool in statistical mechanics. We
validate our result in the manner of deep learning, using a simple iterative
algorithm to learn the weight vector on the basis of belief propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04431</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04431</id><created>2015-01-19</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author></authors><title>Field-normalized citation impact indicators and the choice of an
  appropriate counting method</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bibliometric studies often rely on field-normalized citation impact
indicators in order to make comparisons between scientific fields. We discuss
the connection between field normalization and the choice of a counting method
for handling publications with multiple co-authors. Our focus is on the choice
between full counting and fractional counting. Based on an extensive
theoretical and empirical analysis, we argue that properly field-normalized
results cannot be obtained when full counting is used. Fractional counting does
provide results that are properly field normalized. We therefore recommend the
use of fractional counting in bibliometric studies that require field
normalization, especially in studies at the level of countries and research
organizations. We also compare different variants of fractional counting. In
general, it seems best to use either the author-level or the address-level
variant of fractional counting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04434</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04434</id><created>2015-01-19</created><authors><author><keyname>Krol</keyname><forenames>Kat</forenames></author><author><keyname>Philippou</keyname><forenames>Eleni</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Sasse</keyname><forenames>M. Angela</forenames></author></authors><title>&quot;`They brought in the horrible key ring thing!&quot; Analysing the Usability
  of Two-Factor Authentication in UK Online Banking</title><categories>cs.CR cs.HC</categories><comments>To appear in NDSS Workshop on Usable Security (USEC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To prevent password breaches and guessing attacks, banks increasingly turn to
two-factor authentication (2FA), requiring users to present at least one more
factor, such as a one-time password generated by a hardware token or received
via SMS, besides a password. We can expect some solutions -- especially those
adding a token -- to create extra work for users, but little research has
investigated usability, user acceptance, and perceived security of deployed
2FA.
  This paper presents an in-depth study of 2FA usability with 21 UK online
banking customers, 16 of whom had accounts with more than one bank. We
collected a rich set of qualitative and quantitative data through two rounds of
semi-structured interviews, and an authentication diary over an average of 11
days. Our participants reported a wide range of usability issues, especially
with the use of hardware tokens, showing that the mental and physical workload
involved shapes how they use online banking. Key targets for improvements are
(i) the reduction in the number of authentication steps, and (ii) removing
features that do not add any security but negatively affect the user
experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04451</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04451</id><created>2015-01-19</created><authors><author><keyname>Kang</keyname><forenames>Jaewook</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Bayesian Hypothesis Test using Nonparametric Belief Propagation for
  Noisy Sparse Recovery</title><categories>cs.IT math.IT</categories><comments>14 page, 11 figures, Digitally Published in IEEE Transaction on
  Signal Processing at Dec. 2014; IEEE Trans. Signal Process., Dec. 2014</comments><doi>10.1109/TSP.2014.2385659</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a low-computational Bayesian algorithm for noisy sparse
recovery (NSR), called BHT-BP. In this framework, we consider an LDPC-like
measurement matrices which has a tree-structured property, and additive white
Gaussian noise. BHT-BP has a joint detection-and-estimation structure
consisting of a sparse support detector and a nonzero estimator. The support
detector is designed under the criterion of the minimum detection error
probability using a nonparametric belief propagation (nBP) and composite binary
hypothesis tests. The nonzeros are estimated in the sense of linear MMSE, where
the support detection result is utilized. BHT-BP has its strength in noise
robust support detection, effectively removing quantization errors caused by
the uniform sampling-based nBP. Therefore, in the NSR problems, BHT-BP has
advantages over CS-BP which is an existing nBP algorithm, being comparable to
other recent CS solvers, in several aspects. In addition, we examine impact of
the minimum nonzero value of sparse signals via BHT-BP, on the basis of the
results of the recent literature. Our empirical result shows that variation of
x_min is reflected to recovery performance in the form of SNR shift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04457</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04457</id><created>2015-01-19</created><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Mechanism Design with Strategic Mediators</title><categories>cs.GT</categories><comments>46 pages, 1 figure, an extended abstract of this work appeared in
  ITCS 2015</comments><msc-class>68R05, 68W20, 68W25, 91A40, 91A46</msc-class><acm-class>F.2.2; G.2.1; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing mechanisms that interact with strategic
agents through strategic intermediaries (or mediators), and investigate the
cost to society due to the mediators' strategic behavior. Selfish agents with
private information are each associated with exactly one strategic mediator,
and can interact with the mechanism exclusively through that mediator. Each
mediator aims to optimize the combined utility of his agents, while the
mechanism aims to optimize the combined utility of all agents. We focus on the
problem of facility location on a metric induced by a publicly known tree. With
non-strategic mediators, there is a dominant strategy mechanism that is
optimal. We show that when both agents and mediators act strategically, there
is no dominant strategy mechanism that achieves any approximation. We, thus,
slightly relax the incentive constraints, and define the notion of a two-sided
incentive compatible mechanism. We show that the $3$-competitive deterministic
mechanism suggested by Procaccia and Tennenholtz (2013) and Dekel et al. (2010)
for lines extends naturally to trees, and is still $3$-competitive as well as
two-sided incentive compatible. This is essentially the best possible. We then
show that by allowing randomization one can construct a $2$-competitive
randomized mechanism that is two-sided incentive compatible, and this is also
essentially tight. This result also closes a gap left in the work of Procaccia
and Tennenholtz (2013) and Lu et al. (2009) for the simpler problem of
designing strategy-proof mechanisms for weighted agents with no mediators on a
line, while extending to the more general model of trees. We also investigate a
further generalization of the above setting where there are multiple levels of
mediators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04466</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04466</id><created>2015-01-19</created><updated>2015-05-07</updated><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author></authors><title>Improving the use of equational constraints in cylindrical algebraic
  decomposition</title><categories>cs.SC</categories><msc-class>68W30, 03C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>Proceedings of the 40th International Symposium on Symbolic and
  Algebraic Computation (ISSAC '15), pp. 165--172. ACM, 2015</journal-ref><doi>10.1145/2755996.2756678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When building a cylindrical algebraic decomposition (CAD) savings can be made
in the presence of an equational constraint (EC): an equation logically implied
by a formula.
  The present paper is concerned with how to use multiple ECs, propagating
those in the input throughout the projection set. We improve on the approach of
McCallum in ISSAC 2001 by using the reduced projection theory to make savings
in the lifting phase (both to the polynomials we lift with and the cells lifted
over). We demonstrate the benefits with worked examples and a complexity
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04473</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04473</id><created>2015-01-19</created><authors><author><keyname>Sethi</keyname><forenames>Shuchi</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author></authors><title>Seeking Black Lining In Cloud</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is focused on attacks on confidentiality that require time
synchronization. This manuscript proposes a detection framework for covert
channel perspective in cloud security. This problem is interpreted as a binary
classification problem and the algorithm proposed is based on certain features
that emerged after data analysis of Google cluster trace that forms base for
analyzing attack free data. This approach can be generalized to study the flow
of other systems and fault detection. The detection framework proposed does not
make assumptions pertaining to data distribution as a whole making it suitable
to meet cloud dynamism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04478</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04478</id><created>2015-01-19</created><updated>2015-01-23</updated><authors><author><keyname>Balmahoon</keyname><forenames>R</forenames></author><author><keyname>Cheng</keyname><forenames>L</forenames></author></authors><title>Information Leakage of Heterogeneous Encoded Correlated Sequences over
  Eavesdropped Channel</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.8805</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlated sources are present in communication systems where protocols
ensure that there is some predetermined information for sources. Here
correlated sources across an eavesdropped channel that incorporate a
heterogeneous encoding scheme and their effect on the information leakage when
some channel information and a source have been wiretapped is investigated. The
information leakage bounds for the Slepian-Wolf scenario are provided.
Thereafter, the Shannon cipher system approach is presented. Further, an
implementation method using a matrix partition approach is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04490</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04490</id><created>2015-01-19</created><updated>2016-01-25</updated><authors><author><keyname>Mansour</keyname><forenames>Ahmed S.</forenames></author><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>On The Capacity of Broadcast Channels With Degraded Message Sets and
  Message Cognition Under Different Secrecy Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a three-receiver broadcast channel with degraded message
sets and message cognition. The model consists of a common message for all
three receivers, a private common message for only two receivers and two
additional private messages for these two receivers, such that each receiver is
only interested in one message, while being fully cognizant of the other one.
First, this model is investigated without any secrecy constraints, where the
capacity region is established, showing that the straightforward extension of
the K\&quot;orner and Marton inner bound to the investigated scenario is optimal. In
particular, this agrees with Nair and Wang's result, which states that the idea
of indirect decoding - introduced to improve the K\&quot;orner and Marton inner
bound - does not provide a better region for this scenario. Further, some
secrecy constraints are introduced by letting the private messages to be
confidential ones. Two different secrecy criteria are considered: joint secrecy
and individual secrecy. For both criteria, a general achievable rate region is
provided. Moreover, the joint and individual secrecy capacity regions are
established, if the two legitimate receivers are more capable than the
eavesdropper. The established capacity regions indicate that the individual
secrecy criterion can provide a larger capacity region as compared to the joint
one, because each cognizant message can be used as a secret key for the other
individual message. Further, the joint secrecy capacity is established for a
more general class of more capable channels, where only one of the two
legitimate receivers is more capable than the eavesdropper. This was done by
showing that principle of indirect decoding introduced by Nair and El Gamal is
optimal for this class of channels. This result is in contrast with the
nonsecrecy case, where the indirect decoding does not provide any gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04495</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04495</id><created>2015-01-19</created><authors><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author><author><keyname>Hansson</keyname><forenames>Anders</forenames></author></authors><title>N2SID: Nuclear Norm Subsystem Identification</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data equation in subspace identification relates Hankel matrices
constructed from input and output data. The matrices in this equation, which
are derived from the system matrices of the unknown system, contain important
structural information. Such information is the low rank property of the
product between the extended observability matrix and the state sequence, and
the Toeplitz structure of the matrix of Markov parameters of the system in
innovation form. We will show that nuclear norm subspace identification makes
it possible to formulate these structural properties as constraints in a single
convex multi-criteria optimization problem. This problem seeks a trade-off
between a nuclear norm minimization problem to retrieve the subspace of
interest and a prediction error criterium to find an optimal match between the
measured and predicted output. The advantage of our method over existing
unconstrained and nuclear norm constrained subspace identification methods is
that the structural constraints are in general improving the estimates when
dealing with short data sets, i.e. where the number of measurements is a small
multiple of the system order. This advantage is demonstrated in a validation
study making use of data sets from the DaSIy library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04504</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04504</id><created>2015-01-19</created><updated>2015-09-23</updated><authors><author><keyname>Yu</keyname><forenames>Xiangyao</forenames></author><author><keyname>Devadas</keyname><forenames>Srinivas</forenames></author></authors><title>TARDIS: Timestamp based Coherence Algorithm for Distributed Shared
  Memory</title><categories>cs.DC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new memory coherence protocol, Tardis, is proposed. Tardis uses timestamp
counters representing logical time as well as physical time to order memory
operations and enforce sequential consistency in any type of shared memory
system. Tardis is unique in that as compared to the widely-adopted directory
coherence protocol, and its variants, it completely avoids multicasting and
only requires O(log N) storage per cache block for an N-core system rather than
O(N) sharer information. Tardis is simpler and easier to reason about, yet
achieves similar performance to directory protocols on a wide range of
benchmarks run on 16, 64 and 256 cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04505</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04505</id><created>2015-01-19</created><updated>2015-08-24</updated><authors><author><keyname>Zhang</keyname><forenames>Kaihua</forenames></author><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Wu</keyname><forenames>Yi</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author></authors><title>Robust Visual Tracking via Convolutional Networks</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Deep networks have been successfully applied to visual tracking by learning a
generic representation offline from numerous training images. However the
offline training is time-consuming and the learned generic representation may
be less discriminative for tracking specific objects. In this paper we present
that, even without offline training with a large amount of auxiliary data,
simple two-layer convolutional networks can be powerful enough to develop a
robust representation for visual tracking. In the first frame, we employ the
k-means algorithm to extract a set of normalized patches from the target region
as fixed filters, which integrate a series of adaptive contextual filters
surrounding the target to define a set of feature maps in the subsequent
frames. These maps measure similarities between each filter and the useful
local intensity patterns across the target, thereby encoding its local
structural information. Furthermore, all the maps form together a global
representation, which is built on mid-level features, thereby remaining close
to image-level information, and hence the inner geometric layout of the target
is also well preserved. A simple soft shrinkage method with an adaptive
threshold is employed to de-noise the global representation, resulting in a
robust sparse representation. The representation is updated via a simple and
effective online strategy, allowing it to robustly adapt to target appearance
variations. Our convolution networks have surprisingly lightweight structure,
yet perform favorably against several state-of-the-art methods on the CVPR2013
tracking benchmark dataset with 50 challenging videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04509</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04509</id><created>2015-01-19</created><authors><author><keyname>Chowdary</keyname><forenames>C Ravindranath</forenames></author><author><keyname>Singh</keyname><forenames>Anil Kumar</forenames></author><author><keyname>Nelakanti</keyname><forenames>Anil</forenames></author></authors><title>Responding to Retrieval: A Proposal to Use Retrieval Information for
  Better Presentation of Website Content</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Retrieval and content management are assumed to be mutually exclusive. In
this paper we suggest that they need not be so. In the usual information
retrieval scenario, some information about queries leading to a website (due to
`hits' or `visits') is available to the server administrator of the concerned
website. This information can used to better present the content on the
website. Further, we suggest that some more information can be shared by the
retrieval system with the content provider. This will enable the content
provider (any website) to have a more dynamic presentation of the content that
is in tune with the query trends, without violating the privacy of the querying
user. The result will be a better synchronization between retrieval systems and
content providers, with the purpose of improving the user's web search
experience. This will also give the content provider a say in this process,
given that the content provider is the one who knows much more about the
content than the retrieval system. It also means that the content presentation
may change in response to a query. In the end, the user will be able to find
the relevant content more easily and quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04511</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04511</id><created>2015-01-19</created><authors><author><keyname>Cotton-Barratt</keyname><forenames>Conrad</forenames></author><author><keyname>Hopkins</keyname><forenames>David</forenames></author><author><keyname>Murawski</keyname><forenames>Andrzej S.</forenames></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>Fragments of ML Decidable by Nested Data Class Memory Automata</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The call-by-value language RML may be viewed as a canonical restriction of
Standard ML to ground-type references, augmented by a &quot;bad variable&quot; construct
in the sense of Reynolds. We consider the fragment of (finitary) RML terms of
order at most 1 with free variables of order at most 2, and identify two
subfragments of this for which we show observational equivalence to be
decidable. The first subfragment consists of those terms in which the
P-pointers in the game semantic representation are determined by the underlying
sequence of moves. The second subfragment consists of terms in which the
O-pointers of moves corresponding to free variables in the game semantic
representation are determined by the underlying moves. These results are shown
using a reduction to a form of automata over data words in which the data
values have a tree-structure, reflecting the tree-structure of the threads in
the game semantic plays. In addition we show that observational equivalence is
undecidable at every third- or higher-order type, every second-order type which
takes at least two first-order arguments, and every second-order type (of arity
greater than one) that has a first-order argument which is not the final
argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04527</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04527</id><created>2015-01-19</created><updated>2015-01-22</updated><authors><author><keyname>D&#xfc;nker</keyname><forenames>Daniel</forenames></author><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Social Networking by Proxy: A Case Study of Catster, Dogster and
  Hamsterster</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages; small corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of online social networks in the last decade has not
stopped short of pets, and many different online platforms now exist catering
to owners of various pets such as cats and dogs. These online pet social
networks provide a unique opportunity to study an online social network in
which a single user manages multiple user profiles, i.e. one for each pet they
own. These types of multi-profile networks allow us to investigate two
questions: (1) What is the relationship between the pet-level and human-level
network, and (2) what is the relationship between friendship links and family
ties? Concretely, we study the online social pet networks Catster, Dogster and
Hamsterster, the first two of which are the two largest online pet networks in
existence. We show how the networks on the two levels interact, and perform
experiments to find out whether knowledge about friendships on a profile-level
alone can be used to predict which users are behind which profile. In order to
do so, we introduce the concept of multi-profile social network, extend a
previously defined spectral test of diagonality to multi-profile networks,
define two new homophily measures for multi-profile social networks, perform a
two-level social network analysis, and present an algorithm for predicting
whether two profiles were created by the same user. As a result, we are able to
predict with very high precision whether two profiles were created by a same
user. Our work is thus relevant for the analysis of other online communities in
which users may use multiple profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04537</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04537</id><created>2015-01-19</created><updated>2016-02-09</updated><authors><author><keyname>Baig</keyname><forenames>Mohammad Haris</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author></authors><title>Coupled Depth Learning</title><categories>cs.CV</categories><comments>10 pages, 3 Figures, 4 Tables with quantitative evaluations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method for estimating depth from a single image
using a coarse to fine approach. We argue that modeling the fine depth details
is easier after a coarse depth map has been computed. We express a global
(coarse) depth map of an image as a linear combination of a depth basis learned
from training examples. The depth basis captures spatial and statistical
regularities and reduces the problem of global depth estimation to the task of
predicting the input-specific coefficients in the linear combination. This is
formulated as a regression problem from a holistic representation of the image.
Crucially, the depth basis and the regression function are {\bf coupled} and
jointly optimized by our learning scheme. We demonstrate that this results in a
significant improvement in accuracy compared to direct regression of depth
pixel values or approaches learning the depth basis disjointly from the
regression function. The global depth estimate is then used as a guidance by a
local refinement method that introduces depth details that were not captured at
the global level. Experiments on the NYUv2 and KITTI datasets show that our
method outperforms the existing state-of-the-art at a considerably lower
computational cost for both training and testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04538</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04538</id><created>2015-01-19</created><authors><author><keyname>Kim</keyname><forenames>Kwang-Ki K.</forenames></author></authors><title>Mathematical Programs for Belief Propagation and Consensus</title><categories>cs.SY</categories><comments>25 pages, 2 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops methods of distributed Bayesian hypothesis tests for
fault detection and diagnosis that are based on belief propagation and
optimization in graphical models. The main challenges in developing distributed
statistical estimation algorithms are i) difficulties in ensuring convergence
and consensus for solutions of distributed inference problems, ii) increasing
computational costs due to lack of scalability, and iii) communication
constraints for networked multi-agent systems. To cope with those challenges,
this manuscript considers i) belief propagation and optimization in graphical
models of complex distributed systems, ii) decomposition methods of
optimization for parallel and iterative computations, and iii) distributed
decision-making protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04543</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04543</id><created>2015-01-19</created><authors><author><keyname>Keicher</keyname><forenames>Simon</forenames></author><author><keyname>Kremer</keyname><forenames>Thomas</forenames></author></authors><title>A test for monomial containment</title><categories>math.AC cs.SC math.AG</categories><comments>15 pages</comments><msc-class>13P05, 13P10, 13P15, 14Q99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm to decide whether a given ideal in the polynomial
ring contains a monomial without using Gr\&quot;obner bases, factorization or
sub-resultant computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04552</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04552</id><created>2015-01-19</created><authors><author><keyname>Aseeri</keyname><forenames>S.</forenames></author><author><keyname>Batra&#x161;ev</keyname><forenames>O.</forenames></author><author><keyname>Icardi</keyname><forenames>M.</forenames></author><author><keyname>Leu</keyname><forenames>B.</forenames></author><author><keyname>Liu</keyname><forenames>A.</forenames></author><author><keyname>Li</keyname><forenames>N.</forenames></author><author><keyname>Muite</keyname><forenames>B. K.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>E.</forenames></author><author><keyname>Palen</keyname><forenames>B.</forenames></author><author><keyname>Quell</keyname><forenames>M.</forenames></author><author><keyname>Servat</keyname><forenames>H.</forenames></author><author><keyname>Sheth</keyname><forenames>P.</forenames></author><author><keyname>Speck</keyname><forenames>R.</forenames></author><author><keyname>Van Moer</keyname><forenames>M.</forenames></author><author><keyname>Vienne</keyname><forenames>J.</forenames></author></authors><title>Solving the Klein-Gordon equation using Fourier spectral methods: A
  benchmark test for computer performance</title><categories>cs.PF cs.DC math.NA</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cubic Klein-Gordon equation is a simple but non-trivial partial
differential equation whose numerical solution has the main building blocks
required for the solution of many other partial differential equations. In this
study, the library 2DECOMP&amp;FFT is used in a Fourier spectral scheme to solve
the Klein-Gordon equation and strong scaling of the code is examined on
thirteen different machines for a problem size of 512^3. The results are useful
in assessing likely performance of other parallel fast Fourier transform based
programs for solving partial differential equations. The problem is chosen to
be large enough to solve on a workstation, yet also of interest to solve
quickly on a supercomputer, in particular for parametric studies. Unlike other
high performance computing benchmarks, for this problem size, the time to
solution will not be improved by simply building a bigger supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04553</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04553</id><created>2015-01-19</created><updated>2015-01-23</updated><authors><author><keyname>Lal</keyname><forenames>Nidhi</forenames></author><author><keyname>Singh</keyname><forenames>Anurag Prakash</forenames></author><author><keyname>Kumar</keyname><forenames>Shishupal</forenames></author><author><keyname>Mittal</keyname><forenames>Shikha</forenames></author><author><keyname>Singh</keyname><forenames>Meenakshi</forenames></author></authors><title>A Heuristic EDF Uplink Scheduler for Real Time Application in WiMAX
  Communication</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiMAX, Worldwide Interoperability for Microwave Access, is a developing
wireless communication scheme that can provide broadband access to large-scale
coverage. WiMAX belongs to the family of standards of IEEE-802.16. To satisfy
user demands and support a new set of real time services and applications, a
realistic and dynamic resource allocation algorithm is mandatory. One of the
most efficient algorithm is EDF (earliest deadline first). But the problem is
that when the difference between deadlines is large enough, then lower priority
queues have to starve. So in this paper, we present a heuristic earliest
deadline first (H-EDF) approach of the uplink scheduler of the WiMAX real time
system. This H-EDF presents a way for efficient allocation of the bandwidth for
uplink, so that bandwidth utilization is proper and appropriate fairness is
provided to the system. We use Opnet simulator for implementing the WiMAX
network, which uses this H-EDF scheduling algorithm. We will analysis the
performance of the H-EDF algorithm in consideration with throughput as well as
involvement of delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04557</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04557</id><created>2015-01-19</created><updated>2015-01-26</updated><authors><author><keyname>Vav&#x159;&#xed;k</keyname><forenames>Radim</forenames></author><author><keyname>Portero</keyname><forenames>Antoni</forenames></author><author><keyname>Kucha&#x159;</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>Golasowski</keyname><forenames>Martin</forenames></author><author><keyname>Libutti</keyname><forenames>Simone</forenames></author><author><keyname>Massari</keyname><forenames>Giuseppe</forenames></author><author><keyname>Fornaciari</keyname><forenames>William</forenames></author><author><keyname>Vondr&#xe1;k</keyname><forenames>V&#xed;t</forenames></author></authors><title>Precision-Aware application execution for Energy-optimization in HPC
  node system</title><categories>cs.DC</categories><comments>Presented at HIP3ES, 2015 (arXiv: 1501.03064)</comments><report-no>HIP3ES/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power consumption is a critical consideration in high performance computing
systems and it is becoming the limiting factor to build and operate Petascale
and Exascale systems. When studying the power consumption of existing systems
running HPC workloads, we find that power, energy and performance are closely
related which leads to the possibility to optimize energy consumption without
sacrificing (much or at all) the performance. In this paper, we propose a HPC
system running with a GNU/Linux OS and a Real Time Resource Manager (RTRM) that
is aware and monitors the healthy of the platform. On the system, an
application for disaster management runs. The application can run with
different QoS depending on the situation. We defined two main situations.
Normal execution, when there is no risk of a disaster, even though we still
have to run the system to look ahead in the near future if the situation
changes suddenly. In the second scenario, the possibilities for a disaster are
very high. Then the allocation of more resources for improving the precision
and the human decision has to be taken into account. The paper shows that at
design time, it is possible to describe different optimal points that are going
to be used at runtime by the RTOS with the application. This environment helps
to the system that must run 24/7 in saving energy with the trade-off of losing
precision. The paper shows a model execution which can improve the precision of
results by 65% in average by increasing the number of iterations from 1e3 to
1e4. This also produces one order of magnitude longer execution time which
leads to the need to use a multi-node solution. The optimal trade-off between
precision vs. execution time is computed by the RTOS with the time overhead
less than 10% against a native execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04558</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04558</id><created>2015-01-19</created><updated>2015-05-11</updated><authors><author><keyname>Carvalho</keyname><forenames>Catarina</forenames></author><author><keyname>Madelaine</keyname><forenames>Florent</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>From complexity to algebra and back: digraph classes, collapsibility and
  the PGP</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by computational complexity results for the quantified constraint
satisfaction problem, we study the clones of idempotent polymorphisms of
certain digraph classes. Our first results are two algebraic dichotomy, even
&quot;gap&quot;, theorems. Building on and extending [Martin CP'11], we prove that
partially reflexive paths bequeath a set of idempotent polymorphisms whose
associated clone algebra has: either the polynomially generated powers property
(PGP); or the exponentially generated powers property (EGP). Similarly, we
build on [DaMM ICALP'14] to prove that semicomplete digraphs have the same
property.
  These gap theorems are further motivated by new evidence that PGP could be
the algebraic explanation that a QCSP is in NP even for unbounded alternation.
Along the way we also effect a study of a concrete form of PGP known as
collapsibility, tying together the algebraic and structural threads from [Chen
Sicomp'08], and show that collapsibility is equivalent to its
$\Pi_2$-restriction. We also give a decision procedure for $k$-collapsibility
from a singleton source of a finite structure (a form of collapsibility which
covers all known examples of PGP for finite structures).
  Finally, we present a new QCSP trichotomy result, for partially reflexive
paths with constants. Without constants it is known these QCSPs are either in
NL or Pspace-complete [Martin CP'11], but we prove that with constants they
attain the three complexities NL, NP-complete and Pspace-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04560</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04560</id><created>2015-01-19</created><updated>2015-03-02</updated><authors><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy M.</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Transductive Multi-view Zero-Shot Learning</title><categories>cs.CV cs.DS cs.MM</categories><comments>accepted by IEEE TPAMI, more info and longer report will be available
  in :http://www.eecs.qmul.ac.uk/~yf300/embedding/index.html</comments><doi>10.1109/TPAMI.2015.2408354</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing zero-shot learning approaches exploit transfer learning via an
intermediate-level semantic representation shared between an annotated
auxiliary dataset and a target dataset with different classes and no
annotation. A projection from a low-level feature space to the semantic
representation space is learned from the auxiliary dataset and is applied
without adaptation to the target dataset. In this paper we identify two
inherent limitations with these approaches. First, due to having disjoint and
potentially unrelated classes, the projection functions learned from the
auxiliary dataset/domain are biased when applied directly to the target
dataset/domain. We call this problem the projection domain shift problem and
propose a novel framework, transductive multi-view embedding, to solve it. The
second limitation is the prototype sparsity problem which refers to the fact
that for each target class, only a single prototype is available for zero-shot
learning given a semantic representation. To overcome this problem, a novel
heterogeneous multi-view hypergraph label propagation method is formulated for
zero-shot learning in the transductive embedding space. It effectively exploits
the complementary information offered by different semantic representations and
takes advantage of the manifold structures of multiple representation spaces in
a coherent manner. We demonstrate through extensive experiments that the
proposed approach (1) rectifies the projection shift between the auxiliary and
target domains, (2) exploits the complementarity of multiple semantic
representations, (3) significantly outperforms existing methods for both
zero-shot and N-shot recognition on three image and video benchmark datasets,
and (4) enables novel cross-view annotation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04564</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04564</id><created>2015-01-19</created><authors><author><keyname>Banani</keyname><forenames>S. Alireza</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj S.</forenames></author></authors><title>Analyzing the Impact of Inter Cooperation Region Interference in
  Coordinated Multi-Point Uplink Networks</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE Trans. on Communications (TCOM), Jan. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the uplink of coordinated multi-point (CoMP) networks in which
cooperation can be amongst 2 or 3 base stations (BSs). We consider a 2D network
of BSs on a regular hexagonal lattice wherein the cooperation tessellates the
2D plane into cooperation regions (CRs); specifically, we analyze the impact of
the interference between the CRs in the network. Our model accounts realistic
propagation conditions, particularly including shadowing. We obtain accurate,
closed-form, approximations for the user capacity coverage probability (CCP)
and the ergodic capacity at each point within the CR. To provide a
network-level analysis, we focus on the locations within each CR with the
minimum CCP, the worstcase point(s). The worst-case CCP and or ergodic capacity
can be used in parametric studies for network design. Here, the analysis is
applied to obtain the relationship between cell size and CCP and, thereby, the
required density of BSs to achieve a chosen target capacity coverage. The
analysis also allows for a comparison between different orders of BS
cooperation, quantifying the reduced required BS density from higher orders of
cooperation. Comprehensive simulations are used to illustrate the accuracy of
our analysis, including the approximations used for analytic tractability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04572</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04572</id><created>2015-01-19</created><authors><author><keyname>He</keyname><forenames>Yuguo</forenames></author></authors><title>k variables are needed to define k-Clique in first-order logic</title><categories>cs.LO</categories><comments>148 pages, 15 figures</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an early paper, Immerman raised a proposal on developing model-theoretic
techniques to prove lower bounds on ordered structures, which represents a
long-standing challenge in finite model theory. An iconic question standing for
such a challenge is how many variables are needed to define $k$-Clique in
first-order logic on the class of finite ordered graphs? If $k$ variables are
necessary, as widely believed, it would imply that the bounded (or finite)
variable hierarchy in first-order logic is strict on the class of finite
ordered graphs. In 2008, Rossman made a breakthrough by establishing an optimal
average-case lower bound on the size of constant-depth unbounded fan-in
circuits computing $k$-Clique. In terms of logic, this means that it needs
greater than $\lfloor\frac{k} {4}\rfloor$ variables to describe the $k$-Clique
problem in first-order logic on the class of finite ordered graphs, even in the
presence of arbitrary arithmetic predicates. It follows, with an unpublished
result of Immerman, that the bounded variable hierarchy in first-order logic is
indeed strict. However, Rossman's methods come from circuit complexity and a
novel notion of sensitivity by himself. And the challenge before finite model
theory remains there. In this paper, we give an alternative proof for the
strictness of bounded variable hierarchy in $\fo$ using pure model-theoretic
toolkit, and anwser the question completely for first-order logic, i.e.
$k$-variables are indeed needed to describe $k$-Clique in this logic. In
contrast to Rossman's proof, our proof is purely constructive. Then we embed
the main structures into a pure arithmetic structure to show a similar result
where arbitrary arithmetic predicates are presented. Finally, we discuss its
application in circuit complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04579</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04579</id><created>2015-01-19</created><updated>2015-04-15</updated><authors><author><keyname>He</keyname><forenames>Xinran</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author></authors><title>Stability of Influence Maximization</title><categories>cs.SI</categories><comments>Erratum of Paper &quot;Stability of Influence Maximization&quot; which was
  presented and published in the KDD14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present article serves as an erratum to our paper of the same title,
which was presented and published in the KDD 2014 conference. In that article,
we claimed falsely that the objective function defined in Section 1.4 is
non-monotone submodular. We are deeply indebted to Debmalya Mandal, Jean
Pouget-Abadie and Yaron Singer for bringing to our attention a counter-example
to that claim.
  Subsequent to becoming aware of the counter-example, we have shown that the
objective function is in fact NP-hard to approximate to within a factor of
$O(n^{1-\epsilon})$ for any $\epsilon &gt; 0$.
  In an attempt to fix the record, the present article combines the problem
motivation, models, and experimental results sections from the original
incorrect article with the new hardness result. We would like readers to only
cite and use this version (which will remain an unpublished note) instead of
the incorrect conference version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04587</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04587</id><created>2015-01-19</created><updated>2015-04-23</updated><authors><author><keyname>Wang</keyname><forenames>Naiyan</forenames></author><author><keyname>Li</keyname><forenames>Siyi</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author></authors><title>Transferring Rich Feature Hierarchies for Robust Visual Tracking</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural network (CNN) models have demonstrated great success in
various computer vision tasks including image classification and object
detection. However, some equally important tasks such as visual tracking remain
relatively unexplored. We believe that a major hurdle that hinders the
application of CNN to visual tracking is the lack of properly labeled training
data. While existing applications that liberate the power of CNN often need an
enormous amount of training data in the order of millions, visual tracking
applications typically have only one labeled example in the first frame of each
video. We address this research issue here by pre-training a CNN offline and
then transferring the rich feature hierarchies learned to online tracking. The
CNN is also fine-tuned during online tracking to adapt to the appearance of the
tracked target specified in the first video frame. To fit the characteristics
of object tracking, we first pre-train the CNN to recognize what is an object,
and then propose to generate a probability map instead of producing a simple
class label. Using two challenging open benchmarks for performance evaluation,
our proposed tracker has demonstrated substantial improvement over other
state-of-the-art trackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04621</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04621</id><created>2015-01-19</created><authors><author><keyname>Saha</keyname><forenames>Sajib</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank</forenames></author><author><keyname>Nesterets</keyname><forenames>Ya. I.</forenames></author><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Tahtali</keyname><forenames>M.</forenames></author><author><keyname>Gureyev</keyname><forenames>T. E.</forenames></author></authors><title>Sparse Bayesian Learning for EEG Source Localization</title><categories>q-bio.QM cs.LG q-bio.NC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1406.2434</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: Localizing the sources of electrical activity from
electroencephalographic (EEG) data has gained considerable attention over the
last few years. In this paper, we propose an innovative source localization
method for EEG, based on Sparse Bayesian Learning (SBL). Methods: To better
specify the sparsity profile and to ensure efficient source localization, the
proposed approach considers grouping of the electrical current dipoles inside
human brain. SBL is used to solve the localization problem in addition with
imposed constraint that the electric current dipoles associated with the brain
activity are isotropic. Results: Numerical experiments are conducted on a
realistic head model that is obtained by segmentation of MRI images of the head
and includes four major components, namely the scalp, the skull, the
cerebrospinal fluid (CSF) and the brain, with appropriate relative conductivity
values. The results demonstrate that the isotropy constraint significantly
improves the performance of SBL. In a noiseless environment, the proposed
method was 1 found to accurately (with accuracy of &gt;75%) locate up to 6
simultaneously active sources, whereas for SBL without the isotropy constraint,
the accuracy of finding just 3 simultaneously active sources was &lt;75%.
Conclusions: Compared to the state-of-the-art algorithms, the proposed method
is potentially more consistent in specifying the sparsity profile of human
brain activity and is able to produce better source localization for EEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04656</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04656</id><created>2015-01-19</created><updated>2015-01-27</updated><authors><author><keyname>Punjani</keyname><forenames>Ali</forenames></author><author><keyname>Brubaker</keyname><forenames>Marcus A.</forenames></author></authors><title>Microscopic Advances with Large-Scale Learning: Stochastic Optimization
  for Cryo-EM</title><categories>stat.ML cs.CV cs.LG q-bio.QM</categories><comments>Presented at NIPS 2014 Workshop on Machine Learning in Computational
  Biology http://mlcb.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the 3D structures of biological molecules is a key problem for
both biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising
technique for structure estimation which relies heavily on computational
methods to reconstruct 3D structures from 2D images. This paper introduces the
challenging Cryo-EM density estimation problem as a novel application for
stochastic optimization techniques. Structure discovery is formulated as MAP
estimation in a probabilistic latent-variable model, resulting in an
optimization problem to which an array of seven stochastic optimization methods
are applied. The methods are tested on both real and synthetic data, with some
methods recovering reasonable structures in less than one epoch from a random
initialization. Complex quasi-Newton methods are found to converge more slowly
than simple gradient-based methods, but all stochastic methods are found to
converge to similar optima. This method represents a major improvement over
existing methods as it is significantly faster and is able to converge from a
random initialization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04659</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04659</id><created>2015-01-19</created><updated>2015-01-21</updated><authors><author><keyname>Possemato</keyname><forenames>Francesca</forenames></author><author><keyname>Paschero</keyname><forenames>Maurizio</forenames></author><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author></authors><title>On the impact of topological properties of smart grids in power losses
  optimization problems</title><categories>cs.CE cs.NE</categories><comments>35 pages, 38 references</comments><doi>10.1016/j.ijepes.2015.12.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power losses reduction is one of the main targets for any electrical energy
distribution company. In this paper, we face the problem of joint optimization
of both topology and network parameters in a real smart grid. We consider a
portion of the Italian electric distribution network managed by the ACEA
Distribuzione S.p.A. located in Rome. We perform both the power factor
correction (PFC) for tuning the generators and the distributed feeder
reconfiguration (DFR) to set the state of the breakers. This joint optimization
problem is faced considering a suitable objective function and by adopting
genetic algorithms as global optimization strategy. We analyze admissible
network configurations, showing that some of these violate constraints on
current and voltage at branches and nodes. Such violations depend only on pure
topological properties of the configurations. We perform tests by feeding the
simulation environment with real data concerning hourly samples of dissipated
and generated active and reactive power values of the ACEA smart grid. Results
show that removing the configurations violating the electrical constraints from
the solution space leads to interesting improvements in terms of power loss
reduction. To conclude, we provide also an electrical interpretation of the
phenomenon using graph-based pattern analysis techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04668</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04668</id><created>2015-01-19</created><updated>2015-06-10</updated><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Huang</keyname><forenames>Hui</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author></authors><title>A Modified Abramov-Petkovsek Reduction and Creative Telescoping for
  Hypergeometric Terms</title><categories>cs.SC math.RA</categories><comments>8 pages, ISSAC 2015 submission</comments><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Abramov-Petkovsek reduction computes an additive decomposition of a
hypergeometric term, which extends the functionality of the Gosper algorithm
for indefinite hypergeometric summation. We modify the Abramov-Petkovsek
reduction so as to decompose a hypergeometric term as the sum of a summable
term and a non-summable one. The outputs of the Abramov-Petkovsek reduction and
our modified version share the same required properties. The modified reduction
does not solve any auxiliary linear difference equation explicitly. It is also
more efficient than the original reduction according to computational
experiments. Based on this reduction, we design a new algorithm to compute
minimal telescopers for bivariate hypergeometric terms. The new algorithm can
avoid the costly computation of certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04675</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04675</id><created>2015-01-19</created><authors><author><keyname>Liu</keyname><forenames>Zhi</forenames></author><author><keyname>Huang</keyname><forenames>Yan</forenames></author></authors><title>Community Detection from Location-Tagged Networks</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.2.8</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Many real world systems or web services can be represented as a network such
as social networks and transportation networks. In the past decade, many
algorithms have been developed to detect the communities in a network using
connections between nodes. However in many real world networks, the locations
of nodes have great influence on the community structure. For example, in a
social network, more connections are established between geographically
proximate users. The impact of locations on community has not been fully
investigated by the research literature. In this paper, we propose a community
detection method which takes locations of nodes into consideration. The goal is
to detect communities with both geographic proximity and network closeness. We
analyze the distribution of the distances between connected and unconnected
nodes to measure the influence of location on the network structure on two real
location-tagged social networks. We propose a method to determine if a
location-based community detection method is suitable for a given network. We
propose a new community detection algorithm that pushes the location
information into the community detection. We test our proposed method on both
synthetic data and real world network datasets. The results show that the
communities detected by our method distribute in a smaller area compared with
the traditional methods and have the similar or higher tightness on network
connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04682</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04682</id><created>2015-01-19</created><updated>2015-02-04</updated><authors><author><keyname>Holopainen</keyname><forenames>Markus</forenames></author><author><keyname>Sarlin</keyname><forenames>Peter</forenames></author></authors><title>Toward robust early-warning models: A horse race, ensembles and model
  uncertainty</title><categories>q-fin.ST cs.CE q-fin.CP q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents first steps toward robust early-warning models. We
conduct a horse race of conventional statistical methods and more recent
machine learning methods. As early-warning models based upon one approach are
oftentimes built in isolation of other methods, the exercise is of high
relevance for assessing the relative performance of a wide variety of methods.
Further, we test various ensemble approaches to aggregating the information
products of the built early-warning models, providing a more robust basis for
measuring country-level vulnerabilities. Finally, we provide approaches to
estimating model uncertainty in early-warning exercises, particularly model
performance uncertainty and model output uncertainty. The approaches put
forward in this paper are shown with Europe as a playground.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04684</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04684</id><created>2015-01-19</created><authors><author><keyname>Ranca</keyname><forenames>Razvan</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Slice Sampling for Probabilistic Programming</title><categories>cs.AI cs.PL</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the first, general purpose, slice sampling inference engine for
probabilistic programs. This engine is released as part of StocPy, a new
Turing-Complete probabilistic programming language, available as a Python
library. We present a transdimensional generalisation of slice sampling which
is necessary for the inference engine to work on traces with different numbers
of random variables. We show that StocPy compares favourably to other PPLs in
terms of flexibility and usability, and that slice sampling can outperform
previously introduced inference methods. Our experiments include a logistic
regression, HMM, and Bayesian Neural Net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04686</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04686</id><created>2015-01-19</created><authors><author><keyname>Wang</keyname><forenames>Pichao</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author><author><keyname>Gao</keyname><forenames>Zhimin</forenames></author><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Tang</keyname><forenames>Chang</forenames></author><author><keyname>Ogunbona</keyname><forenames>Philip</forenames></author></authors><title>Deep Convolutional Neural Networks for Action Recognition Using Depth
  Map Sequences</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, deep learning approach has achieved promising results in various
fields of computer vision. In this paper, a new framework called Hierarchical
Depth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks
(3ConvNets) is proposed for human action recognition using depth map sequences.
Firstly, we rotate the original depth data in 3D pointclouds to mimic the
rotation of cameras, so that our algorithms can handle view variant cases.
Secondly, in order to effectively extract the body shape and motion
information, we generate weighted depth motion maps (DMM) at several temporal
scales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, three
channels of ConvNets are trained on the HDMMs from three projected orthogonal
planes separately. The proposed algorithms are evaluated on MSRAction3D,
MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively.
We also combine the last three datasets into a larger one (called Combined
Dataset) and test the proposed method on it. The results show that our approach
can achieve state-of-the-art results on the individual datasets and without
dramatical performance degradation on the Combined Dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04690</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04690</id><created>2015-01-19</created><authors><author><keyname>Zhou</keyname><forenames>Erjin</forenames></author><author><keyname>Cao</keyname><forenames>Zhimin</forenames></author><author><keyname>Yin</keyname><forenames>Qi</forenames></author></authors><title>Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition performance improves rapidly with the recent deep learning
technique developing and underlying large training dataset accumulating. In
this paper, we report our observations on how big data impacts the recognition
performance. According to these observations, we build our Megvii Face
Recognition System, which achieves 99.50% accuracy on the LFW benchmark,
outperforming the previous state-of-the-art. Furthermore, we report the
performance in a real-world security certification scenario. There still exists
a clear gap between machine recognition and human performance. We summarize our
experiments and present three challenges lying ahead in recent face
recognition. And we indicate several possible solutions towards these
challenges. We hope our work will stimulate the community's discussion of the
difference between research benchmark and real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04691</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04691</id><created>2015-01-19</created><updated>2015-01-31</updated><authors><author><keyname>Eppel</keyname><forenames>Sagi</forenames></author></authors><title>Tracing the boundaries of materials in transparent vessels using
  computer vision</title><categories>cs.CV</categories><comments>Code and documentation for the method described is freely available
  at:
  http://www.mathworks.com/matlabcentral/fileexchange/49076-find-the-boundaries-of-materials-in-transparent-vessels-using-computer-vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual recognition of material boundaries in transparent vessels is valuable
for numerous applications. Such recognition is essential for estimation of
fill-level, volume and phase-boundaries as well as for tracking of such
chemical processes as precipitation, crystallization, condensation, evaporation
and phase-separation. The problem of material boundary recognition in images is
particularly complex for materials with non-flat surfaces, i.e., solids,
powders and viscous fluids, in which the material interfaces have unpredictable
shapes. This work demonstrates a general method for finding the boundaries of
materials inside transparent containers in images. The method uses an image of
the transparent vessel containing the material and the boundary of the vessel
in this image. The recognition is based on the assumption that the material
boundary appears in the image in the form of a curve (with various constraints)
whose endpoints are both positioned on the vessel contour. The probability that
a curve matches the material boundary in the image is evaluated using a cost
function based on some image properties along this curve. Several image
properties were examined as indicators for the material boundary. The optimal
boundary curve was found using Dijkstra's algorithm. The method was
successfully examined for recognition of various types of phase-boundaries,
including liquid-air, solid-air and solid-liquid interfaces, as well as for
various types of glassware containers from everyday life and the chemistry
laboratory (i.e., bottles, beakers, flasks, jars, columns, vials and
separation-funnels). In addition, the method can be easily extended to
materials carried on top of carrier vessels (i.e., plates, spoons, spatulas).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04692</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04692</id><created>2015-01-19</created><authors><author><keyname>Nakanishi</keyname><forenames>Kensuke</forenames></author><author><keyname>Hara</keyname><forenames>Shinsuke</forenames></author><author><keyname>Matsuda</keyname><forenames>Takahiro</forenames></author><author><keyname>Takizawa</keyname><forenames>Kenichi</forenames></author><author><keyname>Ono</keyname><forenames>Fumie</forenames></author><author><keyname>Miura</keyname><forenames>Ryu</forenames></author></authors><title>Reflective Network Tomography Based on Compressed Sensing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network tomography means to estimate internal link states from end-to-end
path measurements. In conventional network tomography, to make packets
transmissively penetrate a network, a cooperation between transmitter and
receiver nodes is required, which are located at different places in the
network. In this paper, we propose a reflective network tomography, which can
totally avoid such a cooperation, since a single transceiver node transmits
packets and receives them after traversing back from the network. Furthermore,
we are interested in identification of a limited number of bottleneck links, so
we naturally introduce compressed sensing technique into it. Allowing two kinds
of paths such as (fully) loopy path and folded path, we propose a
computationally-efficient algorithm for constructing reflective paths for a
given network. In the performance evaluation by computer simulation, we confirm
the effectiveness of the proposed reflective network tomography scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04703</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04703</id><created>2015-01-19</created><authors><author><keyname>Liu</keyname><forenames>Jingchu</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author></authors><title>Graph-based Framework for Flexible Baseband Function Splitting and
  Placement in C-RAN</title><categories>cs.IT cs.NI math.IT</categories><comments>6 Pages, 6 Figures, Accepted by ICC'15</comments><doi>10.1109/ICC.2015.7248612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The baseband-up centralization architecture of radio access networks (C-RAN)
has recently been proposed to support efficient cooperative communications and
reduce deployment and operational costs. However, the massive fronthaul
bandwidth required to aggregate baseband samples from remote radio heads (RRHs)
to the central office incurs huge fronthauling cost, and existing baseband
compression algorithms can hardly solve this issue. In this paper, we propose a
graphbased framework to effectively reduce fronthauling cost through properly
splitting and placing baseband processing functions in the network. Baseband
transceiver structures are represented with directed graphs, in which nodes
correspond to baseband functions, and edges to the information flows between
functions. By mapping graph weighs to computational and fronthauling costs, we
transform the problem of finding the optimum location to place some baseband
functions into the problem of finding the optimum clustering scheme for graph
nodes. We then solve this problem using a genetic algorithm with customized
fitness function and mutation module. Simulation results show that proper
splitting and placement schemes can significantly reduce fronthauling cost at
the expense of increased computational cost. We also find that cooperative
processing structures and stringent delay requirements will increase the
possibility of centralized placement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04704</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04704</id><created>2015-01-19</created><authors><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author></authors><title>Extracting a shape function for a signal with intra-wave frequency
  modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider signals with intra-wave frequency modulation. To
handle this kind of signals effectively, we generalize our data-driven
time-frequency analysis by using a shape function to describe the intra-wave
frequency modulation. The idea of using a shape function in time-frequency
analysis was first proposed by Wu. A shape function could be any periodic
function. Based on this model, we propose to solve an optimization problem to
extract the shape function. By exploring the fact that s is a periodic
function, we can identify certain low rank structure of the signal. This
structure enables us to extract the shape function from the signal. To test the
robustness of our method, we apply our method on several synthetic and real
signals. The results are very encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04705</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04705</id><created>2015-01-19</created><authors><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Symbol-Decision Successive Cancellation List Decoder for Polar Codes</title><categories>cs.IT math.IT</categories><comments>13 pages, 17 figures</comments><doi>10.1109/TSP.2015.2486750</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are of great interests because they provably achieve the capacity
of both discrete and continuous memoryless channels while having an explicit
construction. Most existing decoding algorithms of polar codes are based on
bit-wise hard or soft decisions. In this paper, we propose symbol-decision
successive cancellation (SC) and successive cancellation list (SCL) decoders
for polar codes, which use symbol-wise hard or soft decisions for higher
throughput or better error performance. First, we propose to use a recursive
channel combination to calculate symbol-wise channel transition probabilities,
which lead to symbol decisions. Our proposed recursive channel combination also
has a lower complexity than simply combining bit-wise channel transition
probabilities. The similarity between our proposed method and Arikan's channel
transformations also helps to share hardware resources between calculating bit-
and symbol-wise channel transition probabilities. Second, a two-stage list
pruning network is proposed to provide a trade-off between the error
performance and the complexity of the symbol-decision SCL decoder. Third, since
memory is a significant part of SCL decoders, we propose a pre-computation
memory-saving technique to reduce memory requirement of an SCL decoder.
Finally, to evaluate the throughput advantage of our symbol-decision decoders,
we design an architecture based on a semi-parallel successive cancellation list
decoder. In this architecture, different symbol sizes, sorting implementations,
and message scheduling schemes are considered. Our synthesis results show that
in terms of area efficiency, our symbol-decision SCL decoders outperform both
bit- and symbol-decision SCL decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04706</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04706</id><created>2015-01-19</created><authors><author><keyname>Zhang</keyname><forenames>Jiayin</forenames></author><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author><author><keyname>Zhao</keyname><forenames>Kunyang</forenames></author></authors><title>A Novel Implementation of QuickHull Algorithm on the GPU</title><categories>cs.CG cs.GR</categories><comments>This paper has been submitted to the 2015 International Conference on
  Computational Science (ICCS2015). 10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel GPU-accelerated implementation of the QuickHull algorihtm
for calculating convex hulls of planar point sets. We also describe a practical
solution to demonstrate how to efficiently implement a typical
Divide-and-Conquer algorithm on the GPU. We highly utilize the parallel
primitives provided by the library Thrust such as the parallel segmented scan
for better efficiency and simplicity. To evaluate the performance of our
implementation, we carry out four groups of experimental tests using two groups
of point sets in two modes on the GPU K20c. Experimental results indicate that:
our implementation can achieve the speedups of up to 10.98x over the
state-of-art CPU-based convex hull implementation Qhull [16]. In addition, our
implementation can find the convex hull of 20M points in about 0.2 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04707</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04707</id><created>2015-01-19</created><updated>2015-05-06</updated><authors><author><keyname>Liu</keyname><forenames>Chunguang</forenames></author><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author></authors><title>On the Uniqueness of Sparse Time-Frequency Representation of Multiscale
  Data</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the uniqueness of the sparse time frequency
decomposition and investigate the efficiency of the nonlinear matching pursuit
method. Under the assumption of scale separation, we show that the sparse time
frequency decomposition is unique up to an error that is determined by the
scale separation property of the signal. We further show that the unique
decomposition can be obtained approximately by the sparse time frequency
decomposition using nonlinear matching pursuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04709</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04709</id><created>2015-01-19</created><updated>2015-02-25</updated><authors><author><keyname>Gaiteri</keyname><forenames>Chris</forenames></author><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw</forenames></author><author><keyname>Kuzmin</keyname><forenames>Konstantin</forenames></author><author><keyname>Xie</keyname><forenames>Jierui</forenames></author><author><keyname>Lee</keyname><forenames>Changkyu</forenames></author><author><keyname>Blanche</keyname><forenames>Timothy</forenames></author><author><keyname>Neto</keyname><forenames>Elias Chaibub</forenames></author><author><keyname>Huang</keyname><forenames>Su-Chun</forenames></author><author><keyname>Grabowski</keyname><forenames>Thomas</forenames></author><author><keyname>Madhyastha</keyname><forenames>Tara</forenames></author><author><keyname>Komashko</keyname><forenames>Vitalina</forenames></author></authors><title>Identifying robust communities and multi-community nodes by combining
  top-down and bottom-up approaches to clustering</title><categories>cs.CE cs.SI physics.bio-ph q-bio.MN</categories><journal-ref>Scientific Reports 5, Article number: 16361 (2015)</journal-ref><doi>10.1038/srep16361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological functions are carried out by groups of interacting molecules,
cells or tissues, known as communities. Membership in these communities may
overlap when biological components are involved in multiple functions. However,
traditional clustering methods detect non-overlapping communities. These
detected communities may also be unstable and difficult to replicate, because
traditional methods are sensitive to noise and parameter settings. These
aspects of traditional clustering methods limit our ability to detect
biological communities, and therefore our ability to understand biological
functions.
  To address these limitations and detect robust overlapping biological
communities, we propose an unorthodox clustering method called SpeakEasy which
identifies communities using top-down and bottom-up approaches simultaneously.
Specifically, nodes join communities based on their local connections, as well
as global information about the network structure. This method can quantify the
stability of each community, automatically identify the number of communities,
and quickly cluster networks with hundreds of thousands of nodes.
  SpeakEasy shows top performance on synthetic clustering benchmarks and
accurately identifies meaningful biological communities in a range of datasets,
including: gene microarrays, protein interactions, sorted cell populations,
electrophysiology and fMRI brain imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04711</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04711</id><created>2015-01-19</created><authors><author><keyname>Lin</keyname><forenames>Jie</forenames></author><author><keyname>Morere</keyname><forenames>Olivier</forenames></author><author><keyname>Chandrasekhar</keyname><forenames>Vijay</forenames></author><author><keyname>Veillard</keyname><forenames>Antoine</forenames></author><author><keyname>Goh</keyname><forenames>Hanlin</forenames></author></authors><title>DeepHash: Getting Regularization, Depth and Fine-Tuning Right</title><categories>cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on representing very high-dimensional global image
descriptors using very compact 64-1024 bit binary hashes for instance
retrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to
making DeepHash work at extremely low bitrates are three important
considerations -- regularization, depth and fine-tuning -- each requiring
solutions specific to the hashing problem. In-depth evaluation shows that our
scheme consistently outperforms state-of-the-art methods across all data sets
for both Fisher Vectors and Deep Convolutional Neural Network features, by up
to 20 percent over other schemes. The retrieval performance with 256-bit hashes
is close to that of the uncompressed floating point features -- a remarkable
512 times compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04717</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04717</id><created>2015-01-20</created><authors><author><keyname>Zhang</keyname><forenames>Yuting</forenames></author><author><keyname>Jia</keyname><forenames>Kui</forenames></author><author><keyname>Wang</keyname><forenames>Yueming</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author><author><keyname>Chan</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>Robust Face Recognition by Constrained Part-based Alignment</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing a reliable and practical face recognition system is a
long-standing goal in computer vision research. Existing literature suggests
that pixel-wise face alignment is the key to achieve high-accuracy face
recognition. By assuming a human face as piece-wise planar surfaces, where each
surface corresponds to a facial part, we develop in this paper a Constrained
Part-based Alignment (CPA) algorithm for face recognition across pose and/or
expression. Our proposed algorithm is based on a trainable CPA model, which
learns appearance evidence of individual parts and a tree-structured shape
configuration among different parts. Given a probe face, CPA simultaneously
aligns all its parts by fitting them to the appearance evidence with
consideration of the constraint from the tree-structured shape configuration.
This objective is formulated as a norm minimization problem regularized by
graph likelihoods. CPA can be easily integrated with many existing classifiers
to perform part-based face recognition. Extensive experiments on benchmark face
datasets show that CPA outperforms or is on par with existing methods for
robust face recognition across pose, expression, and/or illumination changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04719</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04719</id><created>2015-01-20</created><authors><author><keyname>Caron</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Pham</keyname><forenames>Quang-Cuong</forenames></author><author><keyname>Nakamura</keyname><forenames>Yoshihiko</forenames></author></authors><title>Stability of Surface Contacts for Humanoid Robots: Closed-Form Formulae
  of the Contact Wrench Cone for Rectangular Support Areas</title><categories>cs.RO</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanoid robots locomote by making and breaking contacts with their
environment. A crucial problem is therefore to find precise criteria for a
given contact to remain stable or to break. For rigid surface contacts, the
most general criterion is the Contact Wrench Condition (CWC). To check whether
a motion satisfies the CWC, existing approaches take into account a large
number of individual contact forces (for instance, one at each vertex of the
support polygon), which is computationally costly and prevents the use of
efficient inverse-dynamics methods. Here we argue that the CWC can be
explicitly computed without reference to individual contact forces, and give
closed-form formulae in the case of rectangular surfaces -- which is of
practical importance. It turns out that these formulae simply and naturally
express three conditions: (i) Coulomb friction on the resultant force, (ii) ZMP
inside the support area, and (iii) bounds on the yaw torque. Conditions (i) and
(ii) are already known, but condition (iii) is, to the best of our knowledge,
novel. It is also of particular interest for biped locomotion, where undesired
foot yaw rotations are a known issue. We also show that our formulae yield
simpler and faster computations than existing approaches for humanoid motions
in single support, and demonstrate their consistency in the OpenHRP simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04721</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04721</id><created>2015-01-20</created><authors><author><keyname>Liu</keyname><forenames>An</forenames></author><author><keyname>Lau</keyname><forenames>Vincent</forenames></author></authors><title>Two-Stage Subspace Constrained Precoding in Massive MIMO Cellular
  Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, accepted by IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a subspace constrained precoding scheme that exploits the spatial
channel correlation structure in massive MIMO cellular systems to fully unleash
the tremendous gain provided by massive antenna array with reduced channel
state information (CSI) signaling overhead. The MIMO precoder at each base
station (BS) is partitioned into an inner precoder and a Transmit (Tx) subspace
control matrix. The inner precoder is adaptive to the local CSI at each BS for
spatial multiplexing gain. The Tx subspace control is adaptive to the channel
statistics for inter-cell interference mitigation and Quality of Service (QoS)
optimization. Specifically, the Tx subspace control is formulated as a QoS
optimization problem which involves an SINR chance constraint where the
probability of each user's SINR not satisfying a service requirement must not
exceed a given outage probability. Such chance constraint cannot be handled by
the existing methods due to the two stage precoding structure. To tackle this,
we propose a bi-convex approximation approach, which consists of three key
ingredients: random matrix theory, chance constrained optimization and
semidefinite relaxation. Then we propose an efficient algorithm to find the
optimal solution of the resulting bi-convex approximation problem. Simulations
show that the proposed design has significant gain over various baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04725</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04725</id><created>2015-01-20</created><authors><author><keyname>Krishna</keyname><forenames>Siddharth</forenames></author><author><keyname>Puhrsch</keyname><forenames>Christian</forenames></author><author><keyname>Wies</keyname><forenames>Thomas</forenames></author></authors><title>Learning Invariants using Decision Trees</title><categories>cs.PL cs.LG</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of inferring an inductive invariant for verifying program safety
can be formulated in terms of binary classification. This is a standard problem
in machine learning: given a sample of good and bad points, one is asked to
find a classifier that generalizes from the sample and separates the two sets.
Here, the good points are the reachable states of the program, and the bad
points are those that reach a safety property violation. Thus, a learned
classifier is a candidate invariant. In this paper, we propose a new algorithm
that uses decision trees to learn candidate invariants in the form of arbitrary
Boolean combinations of numerical inequalities. We have used our algorithm to
verify C programs taken from the literature. The algorithm is able to infer
safe invariants for a range of challenging benchmarks and compares favorably to
other ML-based invariant inference techniques. In particular, it scales well to
large sample sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04730</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04730</id><created>2015-01-20</created><updated>2015-04-03</updated><authors><author><keyname>Medicherla</keyname><forenames>Raveendra Kumar</forenames></author><author><keyname>Komondoor</keyname><forenames>Raghavan</forenames></author><author><keyname>Narendran</keyname><forenames>S.</forenames></author></authors><title>Static Analysis of File-Processing Programs using File Format
  Specifications</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs that process data that reside in files are widely used in varied
domains, such as banking, healthcare, and web-traffic analysis. Precise static
analysis of these programs in the context of software verification and
transformation tasks is a challenging problem. Our key insight is that static
analysis of file-processing programs can be made more useful if knowledge of
the input file formats of these programs is made available to the analysis. We
propose a generic framework that is able to perform any given underlying
abstract interpretation on the program, while restricting the attention of the
analysis to program paths that are potentially feasible when the program's
input conforms to the given file format specification. We describe an
implementation of our approach, and present empirical results using real and
realistic programs that show how our approach enables novel verification and
transformation tasks, and also improves the precision of standard analysis
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04731</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04731</id><created>2015-01-20</created><authors><author><keyname>Han</keyname><forenames>Xiao</forenames></author><author><keyname>Shen</keyname><forenames>Zhesi</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Di</keyname><forenames>Zengru</forenames></author></authors><title>Robust Reconstruction of Complex Networks from Sparse Data</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 2 figures, 2 tables</comments><journal-ref>Physical Review Letters, 114(2), 028701 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.028701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstructing complex networks from measurable data is a fundamental problem
for understanding and controlling collective dynamics of complex networked
systems. However, a significant challenge arises when we attempt to decode
structural information hidden in limited amounts of data accompanied by noise
and in the presence of inaccessible nodes. Here, we develop a general framework
for robust reconstruction of complex networks from sparse and noisy data.
Specifically, we decompose the task of reconstructing the whole network into
recovering local structures centered at each node. Thus, the natural sparsity
of complex networks ensures a conversion from the local structure
reconstruction into a sparse signal reconstruction problem that can be
addressed by using the lasso, a convex optimization method. We apply our method
to evolutionary games, transportation and communication processes taking place
in a variety of model and real complex networks, finding that universal high
reconstruction accuracy can be achieved from sparse data in spite of noise in
time series and missing data of partial nodes. Our approach opens new routes to
the network reconstruction problem and has potential applications in a wide
range of fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04737</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04737</id><created>2015-01-20</created><authors><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Howard</keyname><forenames>Heidi</forenames></author><author><keyname>Chaudhry</keyname><forenames>Amir</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author><author><keyname>Madhavapeddy</keyname><forenames>Anil</forenames></author><author><keyname>Mortier</keyname><forenames>Richard</forenames></author></authors><title>Personal Data: Thinking Inside the Box</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose there is a need for a technical platform enabling people to engage
with the collection, management and consumption of personal data; and that this
platform should itself be personal, under the direct control of the individual
whose data it holds. In what follows, we refer to this platform as the Databox,
a personal, networked service that collates personal data and can be used to
make those data available. While your Databox is likely to be a virtual
platform, in that it will involve multiple devices and services, at least one
instance of it will exist in physical form such as on a physical form-factor
computing device with associated storage and networking, such as a home hub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04741</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04741</id><created>2015-01-20</created><authors><author><keyname>&#x141;aniewski-Wo&#x142;&#x142;k</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Rokicki</keyname><forenames>Jacek</forenames></author></authors><title>Adjoint Lattice Boltzmann for Topology Optimization on multi-GPU
  architecture</title><categories>cs.CE math.NA math.OC</categories><comments>18 pages, 11 figures, 3 tables, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a topology optimization technique applicable to a
broad range of flow design problems. We propose also a discrete adjoint
formulation effective for a wide class of Lattice Boltzmann Methods (LBM). This
adjoint formulation is used to calculate sensitivity of the LBM solution to
several type of parameters, both global and local. The numerical scheme for
solving the adjoint problem has many properties of the original system,
including locality and explicit time-stepping. Thus it is possible to integrate
it with the standard LBM solver, allowing for straightforward and efficient
parallelization (overcoming limitations typical for discrete adjoint solvers).
This approach is successfully used for the channel flow to design a
free-topology mixer and a heat exchanger. Both resulting geometries being very
complex maximize their objective functions, while keeping viscous losses at
acceptable level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04748</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04748</id><created>2015-01-20</created><updated>2015-03-17</updated><authors><author><keyname>He</keyname><forenames>Chaodong</forenames></author><author><keyname>Huang</keyname><forenames>Mingzhang</forenames></author></authors><title>Branching Bisimilarity on Normed BPA Is EXPTIME-complete</title><categories>cs.LO</categories><comments>We correct many typing errors, add several remarks and an interesting
  toy example</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We put forward an exponential-time algorithm for deciding branching
bisimilarity on normed BPA (Bacis Process Algebra) systems. The decidability of
branching (or weak) bisimilarity on normed BPA was once a long standing open
problem which was closed by Yuxi Fu. The EXPTIME-hardness is an inference of a
slight modification of the reduction presented by Richard Mayr. Our result
claims that this problem is EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04754</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04754</id><created>2015-01-20</created><authors><author><keyname>Wan</keyname><forenames>Jiuqing</forenames></author><author><keyname>Nie</keyname><forenames>Yuting</forenames></author><author><keyname>Liu</keyname><forenames>Li</forenames></author></authors><title>Distributed Data Association in Smart Camera Networks via Dual
  Decomposition</title><categories>cs.CV</categories><comments>30 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental requirements for visual surveillance using smart
camera networks is the correct association of each persons observations
generated on different cameras. Recently, distributed data association that
involves only local information processing on each camera node and mutual
information exchanging between neighboring cameras has attracted many research
interests due to its superiority in large scale applications. In this paper, we
formulate the problem of data association in smart camera networks as an
Integer Programming problem by introducing a set of linking variables, and
propose two distributed algorithms, namely L-DD and Q-DD, to solve the Integer
Programming problem using dual decomposition technique. In our algorithms, the
original IP problem is decomposed into several sub-problems, which can be
solved locally and efficiently on each smart camera, and then different
sub-problems reach consensus on their solutions in a rigorous way by adjusting
their parameters based on projected sub-gradient optimization. The proposed
methods are simple and flexible, in that (i) we can incorporate any feature
extraction and matching technique into our framework to measure the similarity
between two observations, which is used to define the cost of each link, and
(ii) we can decompose the original problem in any way as long as the resulting
sub-problem can be solved independently on individual camera. We show the
competitiveness of our methods in both accuracy and speed by theoretical
analysis and experimental comparison with state of the art algorithms on two
real data sets collected by camera networks in our campus garden and office
building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04760</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04760</id><created>2015-01-20</created><authors><author><keyname>Agarwal</keyname><forenames>Gaurav Kumar</forenames></author><author><keyname>Sasidharan</keyname><forenames>Birenjith</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>An Alternate Construction of an Access-Optimal Regenerating Code with
  Optimal Sub-Packetization Level</title><categories>cs.IT math.IT</categories><comments>To appear in National Conference on Communications 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the scale of today's distributed storage systems, the failure of an
individual node is a common phenomenon. Various metrics have been proposed to
measure the efficacy of the repair of a failed node, such as the amount of data
download needed to repair (also known as the repair bandwidth), the amount of
data accessed at the helper nodes, and the number of helper nodes contacted.
Clearly, the amount of data accessed can never be smaller than the repair
bandwidth. In the case of a help-by-transfer code, the amount of data accessed
is equal to the repair bandwidth. It follows that a help-by-transfer code
possessing optimal repair bandwidth is access optimal. The focus of the present
paper is on help-by-transfer codes that employ minimum possible bandwidth to
repair the systematic nodes and are thus access optimal for the repair of a
systematic node.
  The zigzag construction by Tamo et al. in which both systematic and parity
nodes are repaired is access optimal. But the sub-packetization level required
is $r^k$ where $r$ is the number of parities and $k$ is the number of
systematic nodes. To date, the best known achievable sub-packetization level
for access-optimal codes is $r^{k/r}$ in a MISER-code-based construction by
Cadambe et al. in which only the systematic nodes are repaired and where the
location of symbols transmitted by a helper node depends only on the failed
node and is the same for all helper nodes. Under this set-up, it turns out that
this sub-packetization level cannot be improved upon. In the present paper, we
present an alternate construction under the same setup, of an access-optimal
code repairing systematic nodes, that is inspired by the zigzag code
construction and that also achieves a sub-packetization level of $r^{k/r}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04762</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04762</id><created>2015-01-20</created><authors><author><keyname>Li</keyname><forenames>Fuwei</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Duan</keyname><forenames>Huiping</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>Computationally Efficient Sparse Bayesian Learning via Generalized
  Approximate Message Passing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sparse Beyesian learning (also referred to as Bayesian compressed
sensing) algorithm is one of the most popular approaches for sparse signal
recovery, and has demonstrated superior performance in a series of experiments.
Nevertheless, the sparse Bayesian learning algorithm has computational
complexity that grows exponentially with the dimension of the signal, which
hinders its application to many practical problems even with moderately large
data sets. To address this issue, in this paper, we propose a computationally
efficient sparse Bayesian learning method via the generalized approximate
message passing (GAMP) technique. Specifically, the algorithm is developed
within an expectation-maximization (EM) framework, using GAMP to efficiently
compute an approximation of the posterior distribution of hidden variables. The
hyperparameters associated with the hierarchical Gaussian prior are learned by
iteratively maximizing the Q-function which is calculated based on the
posterior approximation obtained from the GAMP. Numerical results are provided
to illustrate the computational efficacy and the effectiveness of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04764</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04764</id><created>2015-01-20</created><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Optimized Uplink Transmission in Multi-Antenna C-RAN with Spatial
  Compression and Forward</title><categories>cs.IT math.IT</categories><comments>submitted for possible publication</comments><doi>10.1109/TSP.2015.2450199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO and C-RAN are two promising techniques for implementing future
wireless communication systems, where a large number of antennas are deployed
either being co-located at the base station (BS) or totally distributed at
separate sites called remote radio heads (RRHs). In this paper, we consider a
general antenna deployment design for wireless networks, termed multi-antenna
C-RAN, where a flexible number of antennas can be equipped at each RRH to more
effectively balance the performance and fronthaul complexity trade-off beyond
the conventional massive MIMO and single-antenna C-RAN. Under the uplink
communication setup, we propose a new &quot;spatial-compression-and-forward (SCF)&quot;
scheme, where each RRH first performs a linear spatial filtering to denoise and
maximally compress its received signals from multiple users to a reduced number
of dimensions, then conducts uniform scalar quantization over each of the
resulting dimensions in parallel, and finally sends the total quantized bits to
the baseband unit (BBU) via a finite-rate fronthaul link for joint information
decoding. Under this scheme, we maximize the minimum
signal-to-interference-plus-noise ratio (SINR) of all users at the BBU by a
joint resource allocation over the wireless transmission and fronthaul links.
Specifically, each RRH determines its own spatial filtering solution in a
distributed manner to reduce the signalling overhead with the BBU, while the
BBU jointly optimizes the users' transmit power, the RRHs' fronthaul bits
allocation, and the BBU's receive beamforming with fixed spatial filters at
individual RRHs. Through numerical results, it is shown that given a total
number of antennas to be deployed, multi-antenna C-RAN with the proposed SCF
and joint optimization significantly outperforms both massive MIMO and
single-antenna C-RAN under practical fronthaul capacity constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04775</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04775</id><created>2015-01-20</created><updated>2015-01-28</updated><authors><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Srinath</keyname><forenames>K. Pavan</forenames></author></authors><title>Interference Aligned Space-Time Transmission with Diversity for the $2
  \times 2$ X-Network</title><categories>cs.IT math.IT</categories><comments>Single Column, 32 pages, 4 figures; typos in the previous version
  fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum degrees of freedom (DoF) of the two-transmitter, two-receiver
multiple-input multiple-output (MIMO) X-Network ($2 \times 2$ MIMO X-Network)
with $M$ antennas at each node is known to be $\frac{4M}{3}$. Transmission
schemes which couple local channel-state-information-at-the-transmitter (CSIT)
based precoding with space-time block coding to achieve the sum-DoF of this
network are known specifically for $M=2,4$. These schemes have been proven to
guarantee a diversity gain of $M$ when a finite-sized input constellation is
employed. In this paper, an explicit transmission scheme that achieves the
$\frac{4M}{3}$ sum-DoF of the $2 \times 2$ X-Network for arbitrary $M$ is
presented. The proposed scheme needs only local CSIT unlike the Jafar-Shamai
scheme which requires the availability of global CSIT in order to achieve the
$\frac{4M}{3}$ sum-DoF. Further, it is shown analytically that the proposed
scheme guarantees a diversity gain of $M+1$ when finite-sized input
constellations are employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04777</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04777</id><created>2015-01-20</created><authors><author><keyname>Wang</keyname><forenames>ShouGuang</forenames></author><author><keyname>You</keyname><forenames>Dan</forenames></author><author><keyname>Zhou</keyname><forenames>MengChu</forenames></author><author><keyname>Seatsu</keyname><forenames>Carla</forenames></author></authors><title>Transformation From Legal-marking Set to Admissible-marking Set of Petri
  Nets With Uncontrollable Transitions</title><categories>cs.FL</categories><comments>13 pages,13 figures, and the second version of TAC</comments><msc-class>00-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear constraint transformation is an essential step to solve the forbidden
state problem in Petri nets that contain uncontrollable transitions. This work
studies the equivalent transformation from a legal-marking set to its
admissible-marking set given such a net. First, the concepts of an
escaping-marking set and a transforming marking set are defined. Based on them,
two algorithms are given to compute the admissible-marking set and the
transforming marking set, which establish the theoretical foundation for the
equivalent transformation of linear constraints. Second, the theory about the
equivalent transformation of a disjunction of linear constraints imposed to
Petri nets with uncontrollable transitions is established. Third, two rules are
given to decide the priority of transitions for transformation. Finally, the
transformation procedure from a given linear constraint to a logic expression
of linear constraints that can describe its entire admissible-marking set is
illustrated via two examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04782</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04782</id><created>2015-01-20</created><updated>2015-07-16</updated><authors><author><keyname>Marku&#x161;</keyname><forenames>Nenad</forenames></author><author><keyname>Pand&#x17e;i&#x107;</keyname><forenames>Igor S.</forenames></author><author><keyname>Ahlberg</keyname><forenames>J&#xf6;rgen</forenames></author></authors><title>Constructing Binary Descriptors with a Stochastic Hill Climbing Search</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary descriptors of image patches provide processing speed advantages and
require less storage than methods that encode the patch appearance with a
vector of real numbers. We provide evidence that, despite its simplicity, a
stochastic hill climbing bit selection procedure for descriptor construction
defeats recently proposed alternatives on a standard discriminative power
benchmark. The method is easy to implement and understand, has no free
parameters that need fine tuning, and runs fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04784</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04784</id><created>2015-01-20</created><authors><author><keyname>Ram&#xed;rez-Gil</keyname><forenames>Francisco Javier</forenames></author><author><keyname>Tsuzuki</keyname><forenames>Marcos de Sales Guerra</forenames></author><author><keyname>Montealegre-Rubio</keyname><forenames>Wilfredo</forenames></author></authors><title>Global finite element matrix construction based on a CPU-GPU
  implementation</title><categories>cs.NA cs.CE cs.DC cs.PF math.AP</categories><msc-class>35Q68, 65N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The finite element method (FEM) has several computational steps to
numerically solve a particular problem, to which many efforts have been
directed to accelerate the solution stage of the linear system of equations.
However, the finite element matrix construction, which is also time-consuming
for unstructured meshes, has been less investigated. The generation of the
global finite element matrix is performed in two steps, computing the local
matrices by numerical integration and assembling them into a global system,
which has traditionally been done in serial computing. This work presents a
fast technique to construct the global finite element matrix that arises by
solving the Poisson's equation in a three-dimensional domain. The proposed
methodology consists in computing the numerical integration, due to its
intrinsic parallel opportunities, in the graphics processing unit (GPU) and
computing the matrix assembly, due to its intrinsic serial operations, in the
central processing unit (CPU). In the numerical integration, only the lower
triangular part of each local stiffness matrix is computed thanks to its
symmetry, which saves GPU memory and computing time. As a result of symmetry,
the global sparse matrix also contains non-zero elements only in its lower
triangular part, which reduces the assembly operations and memory usage. This
methodology allows generating the global sparse matrix from any unstructured
finite element mesh size on GPUs with little memory capacity, only limited by
the CPU memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04786</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04786</id><created>2015-01-20</created><authors><author><keyname>Chebbah</keyname><forenames>Mouna</forenames><affiliation>IRISA</affiliation></author><author><keyname>Kharoune</keyname><forenames>Mouloud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Consid{\'e}rant la d{\'e}pendance dans la th{\'e}orie des fonctions de
  croyance</title><categories>cs.AI</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Revue des Nouvelles Technologies Informatiques (RNTI), 2014,
  Fouille de donn{\'e}es complexes, RNTI-E-27, pp.43-64</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to learn sources independence in order to choose
the appropriate type of combination rules when aggregating their beliefs. Some
combination rules are used with the assumption of their sources independence
whereas others combine beliefs of dependent sources. Therefore, the choice of
the combination rule depends on the independence of sources involved in the
combination. In this paper, we propose also a measure of independence, positive
and negative dependence to integrate in mass functions before the combinaision
with the independence assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04789</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04789</id><created>2015-01-20</created><updated>2015-05-01</updated><authors><author><keyname>Grellois</keyname><forenames>Charles</forenames></author><author><keyname>Melli&#xe8;s</keyname><forenames>Paul-Andr&#xe9;</forenames></author></authors><title>Relational semantics of linear logic and higher-order model-checking</title><categories>cs.LO cs.PL</categories><comments>24 pages. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we develop a new and somewhat unexpected connection between
higher-order model-checking and linear logic. Our starting point is the
observation that once embedded in the relational semantics of linear logic, the
Church encoding of any higher-order recursion scheme (HORS) comes together with
a dual Church encoding of an alternating tree automata (ATA) of the same
signature. Moreover, the interaction between the relational interpretations of
the HORS and of the ATA identifies the set of accepting states of the tree
automaton against the infinite tree generated by the recursion scheme. We show
how to extend this result to alternating parity automata (APT) by introducing a
parametric version of the exponential modality of linear logic, capturing the
formal properties of colors (or priorities) in higher-order model-checking. We
show in particular how to reunderstand in this way the type-theoretic approach
to higher-order model-checking developed by Kobayashi and Ong. We briefly
explain in the end of the paper how his analysis driven by linear logic results
in a new and purely semantic proof of decidability of the formulas of the
monadic second-order logic for higher-order recursion schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04792</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04792</id><created>2015-01-20</created><authors><author><keyname>Dubois</keyname><forenames>Jean-Christophe</forenames><affiliation>IRISA</affiliation></author><author><keyname>Gall</keyname><forenames>Yolande Le</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author></authors><title>Designing a Belief Function-Based Accessibility Indicator to Improve Web
  Browsing for Disabled People</title><categories>cs.HC cs.AI</categories><proxy>ccsd</proxy><journal-ref>Belief 2014, Sep 2014, Oxford, United Kingdom. Lecture Notes in
  Artificial Intelligence, Lecture Notes in Computer Science, Vol. 8764, pp.134
  - 142, Belief Functions: Theory and Applications</journal-ref><doi>10.1007/978-3-319-11191-9_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this study is to provide an accessibility measure of
web-pages, in order to draw disabled users to the pages that have been designed
to be ac-cessible to them. Our approach is based on the theory of belief
functions, using data which are supplied by reports produced by automatic web
content assessors that test the validity of criteria defined by the WCAG 2.0
guidelines proposed by the World Wide Web Consortium (W3C) organization. These
tools detect errors with gradual degrees of certainty and their results do not
always converge. For these reasons, to fuse information coming from the
reports, we choose to use an information fusion framework which can take into
account the uncertainty and imprecision of infor-mation as well as divergences
between sources. Our accessibility indicator covers four categories of
deficiencies. To validate the theoretical approach in this context, we propose
an evaluation completed on a corpus of 100 most visited French news websites,
and 2 evaluation tools. The results obtained illustrate the interest of our
accessibility indicator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04795</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04795</id><created>2015-01-20</created><authors><author><keyname>Dhaou</keyname><forenames>Salma Ben</forenames><affiliation>IRISA</affiliation></author><author><keyname>Kharoune</keyname><forenames>Mouloud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Belief Approach for Social Networks</title><categories>cs.AI cs.SI</categories><proxy>ccsd</proxy><journal-ref>Belief 2014, Sep 2014, Oxford, United Kingdom. Lecture Notes in
  Artificial Intelligence, Lecture Notes in Computer Science, Vol. 8764,
  pp.115-123, Belief Functions: Theory and Applications</journal-ref><doi>10.1007/978-3-319-11191-9_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, social networks became essential in information exchange between
individuals. Indeed, as users of these networks, we can send messages to other
people according to the links connecting us. Moreover, given the large volume
of exchanged messages, detecting the true nature of the received message
becomes a challenge. For this purpose, it is interesting to consider this new
tendency with reasoning under uncertainty by using the theory of belief
functions. In this paper, we tried to model a social network as being a network
of fusion of information and determine the true nature of the received message
in a well-defined node by proposing a new model: the belief social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04796</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04796</id><created>2015-01-20</created><authors><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>What do we learn about development from baby robots?</title><categories>cs.AI cs.CY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding infant development is one of the greatest scientific challenges
of contemporary science. A large source of difficulty comes from the fact that
the development of skills in infants results from the interactions of multiple
mechanisms at multiple spatio-temporal scales. The concepts of &quot;innate&quot; or
&quot;acquired&quot; are not any more adequate tools for explanations, which call for a
shift from reductionist to systemic accounts. To address this challenge,
building and experimenting with robots modeling the growing infant brain and
body is crucial. Systemic explanations of pattern formation in sensorimotor,
cognitive and social development, viewed as a complex dynamical system, require
the use of formal models based on mathematics, algorithms and robots.
Formulating hypothesis about development using such models, and exploring them
through experiments, allows us to consider in detail the interaction between
many mechanisms and parameters. This complements traditional experimental
methods in psychology and neuroscience where only a few variables can be
studied at the same time. Furthermore, the use of robots is of particular
importance. The laws of physics generate everywhere around us spontaneous
patterns in the inorganic world. They also strongly impact the living, and in
particular constrain and guide infant development through the properties of its
(changing) body in interaction with the physical environment. Being able to
consider the body as an experimental variable, something that can be
systematically changed in order to study the impact on skill formation, has
been a dream to many developmental scientists. This is today becoming possible
with developmental robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04797</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04797</id><created>2015-01-20</created><authors><author><keyname>Li</keyname><forenames>Wenhui</forenames></author><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>Sidorenko</keyname><forenames>Vladimir</forenames></author></authors><title>Solving Shift Register Problems over Skew Polynomial Rings using Module
  Minimisation</title><categories>cs.IT math.IT</categories><comments>10 pages, submitted to WCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many algebraic codes the main part of decoding can be reduced to a shift
register synthesis problem. In this paper we present an approach for solving
generalised shift register problems over skew polynomial rings which occur in
error and erasure decoding of $\ell$-Interleaved Gabidulin codes. The algorithm
is based on module minimisation and has time complexity $O(\ell \mu^2)$ where
$\mu$ measures the size of the input problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04817</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04817</id><created>2015-01-20</created><authors><author><keyname>Wang</keyname><forenames>Jian</forenames></author></authors><title>Support Recovery with Orthogonal Matching Pursuit in the Presence of
  Noise: A New Analysis</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><doi>10.1109/TSP.2015.2468676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support recovery of sparse signals from compressed linear measurements is a
fundamental problem in compressed sensing (CS). In this paper, we study the
orthogonal matching pursuit (OMP) algorithm for the recovery of support under
noise. We consider two signal-to-noise ratio (SNR) settings: i) the SNR depends
on the sparsity level $K$ of input signals, and ii) the SNR is an absolute
constant independent of $K$. For the first setting, we establish necessary and
sufficient conditions for the exact support recovery with OMP, expressed as
lower bounds on the SNR. Our results indicate that in order to ensure the exact
support recovery of all $K$-sparse signals with the OMP algorithm, the SNR must
at least scale linearly with the sparsity level $K$. In the second setting,
since the necessary condition on the SNR is not fulfilled, the exact support
recovery with OMP is impossible. However, our analysis shows that recovery with
an arbitrarily small but constant fraction of errors is possible with the OMP
algorithm. This result may be useful for some practical applications where
obtaining some large fraction of support positions is adequate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04822</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04822</id><created>2015-01-20</created><updated>2015-01-26</updated><authors><author><keyname>Becchetti</keyname><forenames>Luca</forenames></author><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author><author><keyname>Posta</keyname><forenames>Gustavo</forenames></author></authors><title>Self-Stabilizing Repeated Balls-into-Bins</title><categories>cs.DC</categories><comments>Corrected reference [12]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following synchronous process that we call \emph {repeated
balls-into-bins}. The process is started by assigning $n$ balls to $n$ bins in
an arbitrary way. Then, in every subsequent round, one ball is chosen according
to some fixed strategy (random, FIFO, etc) from each non-empty bin, and
re-assigned to one of the $n$ bins uniformly at random. This process
corresponds to a non-reversible Markov chain and our aim is to study its \emph
{self-stabilization} properties with respect to the \emph{maximum (bin) load}
and some related performance measures.
  We define a configuration (i.e., a state) \emph{legitimate} if its maximum
load is $O(\log n)$. We first prove that, starting from any legitimate
configuration, the process will only take on legitimate configurations over a
period of length bounded by \emph{any} polynomial in $n$, \emph{with high
probability} (w.h.p.). Further we prove that, starting from \emph{any}
configuration, the process converges to a legitimate configuration in linear
time, w.h.p. This implies that the process is self-stabilizing w.h.p. and,
moreover, that every ball traverses all bins in $O(n\log^2 n)$ rounds, w.h.p.
The latter result can also be interpreted as an almost tight bound on the
\emph{cover time} for the problem of \emph{parallel resource assignment} in the
complete graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04826</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04826</id><created>2015-01-20</created><updated>2015-05-18</updated><authors><author><keyname>Atserias</keyname><forenames>Albert</forenames></author><author><keyname>Balc&#xe1;zar</keyname><forenames>Jos&#xe9; L.</forenames></author></authors><title>Entailment Among Probabilistic Implications</title><categories>cs.LO cs.DB cs.LG</categories><comments>Accepted for LICS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a natural variant of the implicational fragment of propositional
logic. Its formulas are pairs of conjunctions of positive literals, related
together by an implicational-like connective; the semantics of this sort of
implication is defined in terms of a threshold on a conditional probability of
the consequent, given the antecedent: we are dealing with what the data
analysis community calls confidence of partial implications or association
rules. Existing studies of redundancy among these partial implications have
characterized so far only entailment from one premise and entailment from two
premises. By exploiting a previously noted alternative view of this entailment
in terms of linear programming duality, we characterize exactly the cases of
entailment from arbitrary numbers of premises. As a result, we obtain decision
algorithms of better complexity; additionally, for each potential case of
entailment, we identify a critical confidence threshold and show that it is,
actually, intrinsic to each set of premises and antecedent of the conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04832</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04832</id><created>2015-01-20</created><authors><author><keyname>Jeansoulin</keyname><forenames>Robert</forenames></author></authors><title>Big Data: How Geo-information Helped Shape the Future of Data
  Engineering</title><categories>cs.DB cs.AI</categories><comments>Conference &quot;AutoCarto 6&quot;, revisited 30 years later in a
  &quot;Retrospective book&quot;, edited by Barry Wellar, the same chair as the original
  conference. 12 pages, 6 figures. see: AutoCarto Six Retrospective, 2013.
  ISBN: 978-0-9921435-0-3, pages 190-201</comments><msc-class>68U01</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Very large data sets are the common rule in automated mapping, GIS, remote
sensing, and what we can name geo-information. Indeed, in 1983 Landsat was
already delivering gigabytes of data, and other sensors were in orbit or ready
for launch, and a tantamount of cartographic data was being digitized. The
retrospective paper revisits several issues that geo-information sciences had
to face from the early stages on, including: structure ( to bring some
structure to the data registered from a sampled signal, metadata); processing
(huge amounts of data for big computers and fast algorithms); uncertainty (the
kinds of errors, their quantification); consistency (when merging different
sources of data is logically allowed, and meaningful); ontologies (clear and
agreed shared definitions, if any kind of decision should be based upon them).
All these issues are the background of Internet queries, and the underlying
technology has been shaped during those years when geo-information engineering
emerged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04835</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04835</id><created>2015-01-20</created><authors><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames></author></authors><title>Regularity Preserving but not Reflecting Encodings</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encodings, that is, injective functions from words to words, have been
studied extensively in several settings. In computability theory the notion of
encoding is crucial for defining computability on arbitrary domains, as well as
for comparing the power of models of computation. In language theory much
attention has been devoted to regularity preserving functions.
  A natural question arising in these contexts is: Is there a bijective
encoding such that its image function preserves regularity of languages, but
its pre-image function does not? Our main result answers this question in the
affirmative: For every countable class C of languages there exists a bijective
encoding f such that for every language L in C its image f[L] is regular.
  Our construction of such encodings has several noteworthy consequences.
Firstly, anomalies arise when models of computation are compared with respect
to a known concept of implementation that is based on encodings which are not
required to be computable: Every countable decision model can be implemented,
in this sense, by finite-state automata, even via bijective encodings. Hence
deterministic finite-state automata would be equally powerful as Turing machine
deciders.
  A second consequence concerns the recognizability of sets of natural numbers
via number representations and finite automata. A set of numbers is said to be
recognizable with respect to a representation if an automaton accepts the
language of representations. Our result entails that there is one number
representation with respect to which every recursive set is recognizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04836</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04836</id><created>2015-01-20</created><authors><author><keyname>Sturm</keyname><forenames>Thomas</forenames></author></authors><title>Subtropical Real Root Finding</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new incomplete but terminating method for real root finding for
large multivariate polynomials. We take an abstract view of the polynomial as
the set of exponent vectors associated with sign information on the
coefficients. Then we employ linear programming to heuristically find roots.
There is a specialized variant for roots with exclusively positive coordinates,
which is of considerable interest for applications in chemistry and systems
biology. An implementation of our method combining the computer algebra system
Reduce with the linear programming solver Gurobi has been successfully applied
to input data originating from established mathematical models used in these
areas. We have solved several hundred problems with up to more than 800000
monomials in up to 10 variables with degrees up to 12. Our method has failed
due to its incompleteness in less than 8 percent of the cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04843</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04843</id><created>2015-01-20</created><authors><author><keyname>Banik</keyname><forenames>Aritra</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Discrete Voronoi Games and $\epsilon$-Nets, in Two and Three Dimensions</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The one-round discrete Voronoi game, with respect to a $n$-point user set
$U$, consists of two players Player 1 ($\mathcal{P}_1$) and Player 2
($\mathcal{P}_2$). At first, $\mathcal{P}_1$ chooses a set of facilities $F_1$
following which $\mathcal{P}_2$ chooses another set of facilities $F_2$,
disjoint from $F_1$. The payoff of $\mathcal{P}_2$ is defined as the
cardinality of the set of points in $U$ which are closer to a facility in $F_2$
than to every facility in $F_1$, and the payoff of $\mathcal{P}_1$ is the
difference between the number of users in $U$ and the payoff of
$\mathcal{P}_2$. The objective of both the players in the game is to maximize
their respective payoffs. In this paper we study the one-round discrete Voronoi
game where $\mathcal{P}_1$ places $k$ facilities and $\mathcal{P}_2$ places one
facility and we have denoted this game as $VG(k,1)$. Although the optimal
solution of this game can be found in polynomial time, the polynomial has a
very high degree. In this paper, we focus on achieving approximate solutions to
$VG(k,1)$ with significantly better running times. We provide a constant-factor
approximate solution to the optimal strategy of $\mathcal{P}_1$ in $VG(k,1)$ by
establishing a connection between $VG(k,1)$ and weak $\epsilon$-nets. To the
best of our knowledge, this is the first time that Voronoi games are studied
from the point of view of $\epsilon$-nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04850</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04850</id><created>2015-01-20</created><authors><author><keyname>Yassine</keyname><forenames>Abdulsalam</forenames></author></authors><title>AAPPeC: Agent-based Architecture for Privacy Payoff in eCommerce</title><categories>cs.SE cs.CY</categories><comments>Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of applications in open distributed environments
such as eCommerce, privacy of information is becoming a critical issue. Today,
many online companies are gathering information and have assembled
sophisticated databases that know a great deal about many people, generally
without the knowledge of those people. Such information changes hands or
ownership as a normal part of eCommerce transactions, or through strategic
decisions that often includes the sale of users' information to other firms.
The key commercial value of users' personal information derives from the
ability of firms to identify consumers and charge them personalized prices for
goods and services they have previously used or may wish to use in the future.
A look at present-day practices reveals that consumers' profile data is now
considered as one of the most valuable assets owned by online businesses. In
this thesis, we argue the following: if consumers' private data is such a
valuable asset, should they not be entitled to commercially benefit from their
asset as well? The scope of this thesis is on developing architecture for
privacy payoff as a means of rewarding consumers for sharing their personal
information with online businesses. The architecture is a multi-agent system in
which several agents employ various requirements for personal information
valuation and interaction capabilities that most users cannot do on their own.
The agents in the system bear the responsibility of working on behalf of
consumers to categorize their personal data objects, report to consumers on
online businesses' trustworthiness and reputation, determine the value of their
compensation using risk-based financial models, and, finally, negotiate for a
payoff value in return for the dissemination of users' information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04854</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04854</id><created>2015-01-20</created><authors><author><keyname>Zhang</keyname><forenames>Yanfeng</forenames></author><author><keyname>Chen</keyname><forenames>Shimin</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author><author><keyname>Yu</keyname><forenames>Ge</forenames></author></authors><title>i2MapReduce: Incremental MapReduce for Mining Evolving Big Data</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As new data and updates are constantly arriving, the results of data mining
applications become stale and obsolete over time. Incremental processing is a
promising approach to refreshing mining results. It utilizes previously saved
states to avoid the expense of re-computation from scratch.
  In this paper, we propose i2MapReduce, a novel incremental processing
extension to MapReduce, the most widely used framework for mining big data.
Compared with the state-of-the-art work on Incoop, i2MapReduce (i) performs
key-value pair level incremental processing rather than task level
re-computation, (ii) supports not only one-step computation but also more
sophisticated iterative computation, which is widely used in data mining
applications, and (iii) incorporates a set of novel techniques to reduce I/O
overhead for accessing preserved fine-grain computation states. We evaluate
i2MapReduce using a one-step algorithm and three iterative algorithms with
diverse computation characteristics. Experimental results on Amazon EC2 show
significant performance improvements of i2MapReduce compared to both plain and
iterative MapReduce performing re-computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04860</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04860</id><created>2015-01-17</created><authors><author><keyname>Shahabuddin</keyname><forenames>Shahriar</forenames></author><author><keyname>Janhunen</keyname><forenames>Janne</forenames></author><author><keyname>Ghazi</keyname><forenames>Amanullah</forenames></author><author><keyname>Khan</keyname><forenames>Zaheer</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author></authors><title>A Customized Lattice Reduction Multiprocessor for MIMO Detection</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice reduction (LR) is a preprocessing technique for multiple-input
multiple-output (MIMO) symbol detection to achieve better bit error-rate (BER)
performance. In this paper, we propose a customized homogeneous multiprocessor
for LR. The processor cores are based on transport triggered architecture
(TTA). We propose some modification of the popular LR algorithm,
Lenstra-Lenstra-Lovasz (LLL) for high throughput. The TTA cores are programmed
with high level language. Each TTA core consists of several special function
units to accelerate the program code. The multiprocessor takes 187 cycles to
reduce a single matrix for LR. The architecture is synthesized on 90 nm
technology and takes 405 kgates at 210 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04865</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04865</id><created>2015-01-17</created><authors><author><keyname>Singh</keyname><forenames>Vibhutesh Kumar</forenames></author><author><keyname>Baghoriya</keyname><forenames>Sanjeev</forenames></author><author><keyname>Bohara</keyname><forenames>Vivek Ashok</forenames></author></authors><title>Project Monitomation Version 1</title><categories>cs.NI</categories><comments>Selected For Demos &amp; Exhibit Session COMSNETS 2015, January 6-10th,
  Bangalore, India. 3 Figure, 1 Table, 2 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Project Monitomation is a unique implementation which focuses on justifying
the capability of smart wireless networks based on IEEE 802.15.4 standard, for
low power, short range Personal Area Network (PAN) communication. Through this
project wireless text messaging, device control &amp; network monitoring is
implemented to demonstrate the future of Internet of things .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04867</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04867</id><created>2015-01-20</created><updated>2016-01-25</updated><authors><author><keyname>Kaced</keyname><forenames>Tarik</forenames></author><author><keyname>Romashchenko</keyname><forenames>Andrei</forenames></author><author><keyname>Vereshchagin</keyname><forenames>Nikolay</forenames></author></authors><title>Conditional Information Inequalities and Combinatorial Applications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the inequality $H(A \mid B,X) + H(A \mid B,Y) \le H(A\mid B)$
for jointly distributed random variables $A,B,X,Y$, which does not hold in
general case, holds under some natural condition on the support of the
probability distribution of $A,B,X,Y$. This result generalizes a version of the
conditional Ingleton inequality: if for some distribution $I(X: Y \mid A) =
H(A\mid X,Y)=0$, then $I(A : B) \le I(A : B \mid X) + I(A: B \mid Y) + I(X :
Y)$.
  We present two applications of our result. The first one is the following
easy-to-formulate combinatorial theorem: assume that the edges of a bipartite
graph are partitioned into $K$ matchings such that for each pair (left vertex
$x$, right vertex $y$) there is at most one matching in the partition involving
both $x$ and $y$; assume further that the degree of each left vertex is at
least $L$ and the degree of each right vertex is at least $R$. Then $K\ge LR$.
The second application is a new method to prove lower bounds for biclique
coverings of bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04870</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04870</id><created>2015-01-20</created><authors><author><keyname>Read</keyname><forenames>J.</forenames></author><author><keyname>Martino</keyname><forenames>L.</forenames></author><author><keyname>Olmos</keyname><forenames>P.</forenames></author><author><keyname>Luengo</keyname><forenames>D.</forenames></author></authors><title>Scalable Multi-Output Label Prediction: From Classifier Chains to
  Classifier Trellises</title><categories>stat.ML cs.CV cs.DS cs.LG stat.CO</categories><comments>(accepted in Pattern Recognition)</comments><journal-ref>Pattern Recognition, Volume 48, Issue 6, 2015, Pages 2096-2109</journal-ref><doi>10.1016/j.patcog.2015.01.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-output inference tasks, such as multi-label classification, have become
increasingly important in recent years. A popular method for multi-label
classification is classifier chains, in which the predictions of individual
classifiers are cascaded along a chain, thus taking into account inter-label
dependencies and improving the overall performance. Several varieties of
classifier chain methods have been introduced, and many of them perform very
competitively across a wide range of benchmark datasets. However, scalability
limitations become apparent on larger datasets when modeling a fully-cascaded
chain. In particular, the methods' strategies for discovering and modeling a
good chain structure constitutes a mayor computational bottleneck. In this
paper, we present the classifier trellis (CT) method for scalable multi-label
classification. We compare CT with several recently proposed classifier chain
methods to show that it occupies an important niche: it is highly competitive
on standard multi-label problems, yet it can also scale up to thousands or even
tens of thousands of labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04877</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04877</id><created>2015-01-20</created><authors><author><keyname>Abshoff</keyname><forenames>Sebastian</forenames></author><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>Jung</keyname><forenames>Daniel</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author></authors><title>Towards Gathering Robots with Limited View in Linear Time: The Closed
  Chain Case</title><categories>cs.DC cs.MA</categories><acm-class>F.1.2; F.2.2; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the gathering problem, n autonomous robots have to meet on a single point.
We consider the gathering of a closed chain of point-shaped, anonymous robots
on a grid. The robots only have local knowledge about a constant number of
neighboring robots along the chain in both directions. Actions are performed in
the fully synchronous time model FSYNC. Every robot has a limited memory that
may contain one timestamp of the global clock, also visible to its direct
neighbors. In this synchronous time model, there is no limited view gathering
algorithm known to perform better than in quadratic runtime. The configurations
that show the quadratic lower bound are closed chains. In this paper, we
present the first sub-quadratic---in fact linear time---gathering algorithm for
closed chains on a grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04878</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04878</id><created>2015-01-20</created><updated>2015-01-30</updated><authors><author><keyname>Naik</keyname><forenames>Nikhil</forenames></author><author><keyname>Kadambi</keyname><forenames>Achuta</forenames></author><author><keyname>Rhemann</keyname><forenames>Christoph</forenames></author><author><keyname>Izadi</keyname><forenames>Shahram</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author><author><keyname>Kang</keyname><forenames>Sing Bing</forenames></author></authors><title>A Light Transport Model for Mitigating Multipath Interference in TOF
  Sensors</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the submitter as the submission was
  made due to a miscommunication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous-wave Time-of-flight (TOF) range imaging has become a commercially
viable technology with many applications in computer vision and graphics.
However, the depth images obtained from TOF cameras contain scene dependent
errors due to multipath interference (MPI). Specifically, MPI occurs when
multiple optical reflections return to a single spatial location on the imaging
sensor. Many prior approaches to rectifying MPI rely on sparsity in optical
reflections, which is an extreme simplification. In this paper, we correct MPI
by combining the standard measurements from a TOF camera with information from
direct and global light transport. We report results on both simulated
experiments and physical experiments (using the Kinect sensor). Our results,
evaluated against ground truth, demonstrate a quantitative improvement in depth
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04884</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04884</id><created>2015-01-20</created><authors><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios K.</forenames></author></authors><title>Impact of User Mobility on Optimal Linear Receivers in Cellular Networks</title><categories>cs.IT math.IT</categories><comments>3 figures, 6 pages, accepted in ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the uplink of non-cooperative multi-cellular systems deploying
multiple antenna elements at the base stations (BS), covering both the cases of
conventional and very large number of antennas. Given the inevitable pilot
contamination and an arbitrary path-loss for each link, we address the impact
of time variation of the channel due to the relative movement between users and
BS antennas, which limits system's performance even if the number antennas is
increased, as shown. In particular, we propose an optimal linear receiver (OLR)
maximizing the received signal-to-interference-plus-noise (SINR). Closed-form
lower and upper bounds are derived as well as the deterministic equivalent of
the OLR is obtained. Numerical results reveal the outperformance of the
proposed OLR against known linear receivers, mostly in environments with high
interference and certain user mobility, as well as that massive MIMO is
preferable even in time-varying channel conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04887</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04887</id><created>2015-01-20</created><authors><author><keyname>Burnashev</keyname><forenames>Marat V.</forenames></author><author><keyname>Yamamoto</keyname><forenames>Hirosuke</forenames></author></authors><title>On Using Feedback in a Gaussian Channel</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1208.2786</comments><journal-ref>Problems of Information Transmission, vol. 50, no. 3, pp. 19--34,
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For information transmission a discrete time channel with independent
additive Gaussian noise is used. There is also another channel with independent
additive Gaussian noise (the feedback channel), and the transmitter observes
without delay all outputs of the forward channel via that channel. Transmission
of nonexponential number of messages is considered (i.e. transmission rate
equals zero) and the achievable decoding error exponent for such a combination
of channels is investigated. The transmission method strengthens the method
used by authors earlier for BSC and Gaussian channels. In particular, for small
feedback noise, it allows to gain 33.3\% (instead of 23.6\% earlier in the
similar case of Gaussian channel).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04894</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04894</id><created>2015-01-20</created><authors><author><keyname>Pal</keyname><forenames>Arindam</forenames></author><author><keyname>Ruj</keyname><forenames>Sushmita</forenames></author></authors><title>CITEX: A new citation index to measure the relative importance of
  authors and papers in scientific publications</title><categories>cs.DL cs.DM cs.IR cs.SI physics.soc-ph</categories><comments>Accepted for publication in IEEE International Conference on
  Communications (ICC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating the performance of researchers and measuring the impact of papers
written by scientists is the main objective of citation analysis. Various
indices and metrics have been proposed for this. In this paper, we propose a
new citation index CITEX, which gives normalized scores to authors and papers
to determine their rankings. To the best of our knowledge, this is the first
citation index which simultaneously assigns scores to both authors and papers.
Using these scores, we can get an objective measure of the reputation of an
author and the impact of a paper.
  We model this problem as an iterative computation on a publication graph,
whose vertices are authors and papers, and whose edges indicate which author
has written which paper. We prove that this iterative computation converges in
the limit, by using a powerful theorem from linear algebra. We run this
algorithm on several examples, and find that the author and paper scores match
closely with what is suggested by our intuition. The algorithm is theoretically
sound and runs very fast in practice. We compare this index with several
existing metrics and find that CITEX gives far more accurate scores compared to
the traditional metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04895</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04895</id><created>2015-01-19</created><authors><author><keyname>Yang</keyname><forenames>Li</forenames></author><author><keyname>Liang</keyname><forenames>Min</forenames></author></authors><title>Quantum McEliece public-key encryption scheme</title><categories>quant-ph cs.CR</categories><comments>18pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a quantum version of McEliece public-key encryption
(PKE) scheme, and analyzes its security. As is well known, the security of
classical McEliece PKE is not stronger than the onewayness of related classical
one-way function. We prove the security of quantum McEliece PKE ranks between
them. Moreover, we propose the double-encryption technique to improve its
security, and the security of the improved scheme is proved to be between the
original scheme and the quantum one-time pad.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04896</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04896</id><created>2015-01-19</created><authors><author><keyname>Xiang</keyname><forenames>Chong</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author><author><keyname>Peng</keyname><forenames>Yong</forenames></author><author><keyname>Chen</keyname><forenames>Dongqing</forenames></author></authors><title>The Classification of Quantum Symmetric-Key Encryption Protocols</title><categories>quant-ph cs.CR</categories><comments>12pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classification of quantum symmetric-key encryption protocol is presented.
According to five elements of a quantum symmetric-key encryption protocol:
plaintext, ciphertext, key, encryption algorithm and decryption algorithm,
there are 32 different kinds of them. Among them, 5 kinds of protocols have
already been constructed and studied, and 21 kinds of them are proved to be
impossible to construct, the last 6 kinds of them are not yet presented
effectively. That means the research on quantum symmetric-key encryption
protocol only needs to consider with 5 kinds of them nowadays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04904</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04904</id><created>2015-01-20</created><updated>2015-05-10</updated><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Vladut</keyname><forenames>Serge</forenames></author></authors><title>Locally recoverable codes on algebraic curves</title><categories>cs.IT math.AG math.IT math.NT</categories><comments>Will appear at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code over a finite alphabet is called locally recoverable (LRC code) if
every symbol in the encoding is a function of a small number (at most r) other
symbols. A family of linear LRC codes that generalize the classic construction
of Reed-Solomon codes was constructed in a recent paper by I. Tamo and A. Barg.
In this paper we extend this construction to codes on algebraic curves. We give
a general construction of LRC codes on curves and compute some examples,
including asymptotically good families of codes derived from the Garcia-
Stichtenoth towers. The local recovery procedure is performed by polynomial
interpolation over r coordinates of the codevector. We also obtain a family of
Hermitian codes with two disjoint recovering sets for every symbol of the
codeword.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04905</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04905</id><created>2015-01-20</created><updated>2015-09-01</updated><authors><author><keyname>Gomez-Cuba</keyname><forenames>Felipe</forenames></author><author><keyname>Du</keyname><forenames>Jinfeng</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Unified Capacity Limit of Non-coherent Wideband Fading Channels</title><categories>cs.IT math.IT</categories><comments>A few new references and remarks are added. This work has been
  submitted to the IEEE for possible publication. Copyright may be transferred
  without notice</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In non-coherent wideband fading channels where energy rather than spectrum is
the limiting resource, peaky and non-peaky signaling schemes have long been
considered species apart, as the first approaches asymptotically the capacity
of a wideband AWGN channel with the same average SNR, whereas the second
reaches a peak rate at some finite critical bandwidth and then falls to zero as
bandwidth grows to infinity. In this paper it is shown that this distinction is
in fact an artifact of the limited attention paid in the past to the product
between the bandwidth and the fraction of time it is in use. This fundamental
quantity, that is termed bandwidth occupancy, measures average bandwidth usage
over time. As it turns out, a peaky signal that transmits in an infinite
bandwidth but only for an infinitesimal fraction of the time may only have a
small bandwidth occupancy, and so does a non-peaky scheme that limits itself to
the critical bandwidth even though more spectrum is available, so as to not
degrade rate. The two types of signaling in the literature are harmonized to
show that, for all signaling schemes with the same bandwidth occupancy, rates
converge to the wideband AWGN capacity with the same asymptotic behavior as the
bandwidth occupancy approaches its critical value, and decrease to zero as the
occupancy goes to infinity. This unified analysis not only recovers previous
results on capacity bounds for (non-)peaky signaling schemes, but also reveals
the fundamental tradeoff between precision and accuracy when characterizing the
maximal achievable rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04920</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04920</id><created>2015-01-20</created><authors><author><keyname>Sierra</keyname><forenames>Gerardo</forenames></author><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author><author><keyname>Molina</keyname><forenames>Alejandro</forenames></author></authors><title>Regroupement s\'emantique de d\'efinitions en espagnol</title><categories>cs.IR cs.CL</categories><comments>11 pages, in French, 5 figures. Workshop Evaluation des m\'ethodes
  d'Extraction de Connaissances dans les Donn\'ees EvalECD EGC'10, 2010 Tunis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article focuses on the description and evaluation of a new unsupervised
learning method of clustering of definitions in Spanish according to their
semantic. Textual Energy was used as a clustering measure, and we study an
adaptation of the Precision and Recall to evaluate our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04925</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04925</id><created>2015-01-18</created><authors><author><keyname>Huang</keyname><forenames>Zhenqi</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author><author><keyname>Dullerud</keyname><forenames>Geir</forenames></author></authors><title>Controller Synthesis for Linear Time-varying Systems with Adversaries</title><categories>cs.SY cs.CR</categories><comments>10 pages 4 figures; under submission for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a controller synthesis algorithm for a discrete time reach-avoid
problem in the presence of adversaries. Our model of the adversary captures
typical malicious attacks envisioned on cyber-physical systems such as sensor
spoofing, controller corruption, and actuator intrusion. After formulating the
problem in a general setting, we present a sound and complete algorithm for the
case with linear dynamics and an adversary with a budget on the total L2-norm
of its actions. The algorithm relies on a result from linear control theory
that enables us to decompose and precisely compute the reachable states of the
system in terms of a symbolic simulation of the adversary-free dynamics and the
total uncertainty induced by the adversary. With this decomposition, the
synthesis problem eliminates the universal quantifier on the adversary's
choices and the symbolic controller actions can be effectively solved using an
SMT solver. The constraints induced by the adversary are computed by solving
second-order cone programmings. The algorithm is later extended to synthesize
state-dependent controller and to generate attacks for the adversary. We
present preliminary experimental results that show the effectiveness of this
approach on several example problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04931</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04931</id><created>2015-01-20</created><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Siminelakis</keyname><forenames>Paris</forenames></author></authors><title>Navigability is a Robust Property</title><categories>cs.SI cs.DS</categories><acm-class>G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Small World phenomenon has inspired researchers across a number of
fields. A breakthrough in its understanding was made by Kleinberg who
introduced Rank Based Augmentation (RBA): add to each vertex independently an
arc to a random destination selected from a carefully crafted probability
distribution. Kleinberg proved that RBA makes many networks navigable, i.e., it
allows greedy routing to successfully deliver messages between any two vertices
in a polylogarithmic number of steps. We prove that navigability is an inherent
property of many random networks, arising without coordination, or even
independence assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04948</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04948</id><created>2015-01-20</created><updated>2016-02-11</updated><authors><author><keyname>Cis&#x142;ak</keyname><forenames>Aleksander</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>A practical index for approximate dictionary matching with few
  mismatches</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate dictionary matching is a classic string matching problem
(checking if a query string occurs in a collection of strings) with
applications in, e.g., spellchecking, online catalogs, geolocation, and web
searchers. We present a surprisingly simple solution called a split index,
which is based on the Dirichlet principle, for matching a keyword with few
mismatches, and experimentally show that it offers competitive space-time
tradeoffs. Our implementation in the C++ language is focused mostly on data
compaction, which is beneficial for the search speed (e.g., by being cache
friendly). We compare our solution with other algorithms and we show that it
performs better for the Hamming distance. Query times in the order of 1
microsecond were reported for one mismatch for the dictionary size of a few
megabytes on a medium-end PC. We also demonstrate that a basic compression
technique consisting in $q$-gram substitution can significantly reduce the
index size (up to 50% of the input text size for the DNA), while still keeping
the query time relatively low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04979</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04979</id><created>2015-01-15</created><updated>2016-01-20</updated><authors><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>FASTA: A Generalized Implementation of Forward-Backward Splitting</title><categories>cs.MS cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a user manual for the software package FASTA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04981</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04981</id><created>2015-01-19</created><authors><author><keyname>Moussallam</keyname><forenames>Manuel</forenames></author><author><keyname>Liutkus</keyname><forenames>Antoine</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author></authors><title>Listening to features</title><categories>cs.SD</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores nonparametric methods which aim at synthesizing audio from
low-dimensionnal acoustic features typically used in MIR frameworks. Several
issues prevent this task to be straightforwardly achieved. Such features are
designed for analysis and not for synthesis, thus favoring high-level
description over easily inverted acoustic representation. Whereas some previous
studies already considered the problem of synthesizing audio from features such
as Mel-Frequency Cepstral Coefficients, they mainly relied on the explicit
formula used to compute those features in order to inverse them. Here, we
instead adopt a simple blind approach, where arbitrary sets of features can be
used during synthesis and where reconstruction is exemplar-based. After testing
the approach on a speech synthesis from well known features problem, we apply
it to the more complex task of inverting songs from the Million Song Dataset.
What makes this task harder is twofold. First, that features are irregularly
spaced in the temporal domain according to an onset-based segmentation. Second
the exact method used to compute these features is unknown, although the
features for new audio can be computed using their API as a black-box. In this
paper, we detail these difficulties and present a framework to nonetheless
attempting such synthesis by concatenating audio samples from a training
dataset, whose features have been computed beforehand. Samples are selected at
the segment level, in the feature space with a simple nearest neighbor search.
Additionnal constraints can then be defined to enhance the synthesis
pertinence. Preliminary experiments are presented using RWC and GTZAN audio
datasets to synthesize tracks from the Million Song Dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04985</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04985</id><created>2015-01-20</created><authors><author><keyname>Czyzowicz</keyname><forenames>Jurek</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Narayanan</keyname><forenames>Lata</forenames></author><author><keyname>Opatrny</keyname><forenames>Jarda</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Evacuating Robots from a Disk Using Face-to-Face Communication</title><categories>cs.DS</categories><comments>16 pages, 8 figures. An extended abstract of this work is accepted
  for publication in the LNCS proceedings of the 9th International Conference
  on Algorithms and Complexity (CIAC15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume that two robots are located at the centre of a unit disk. Their goal
is to evacuate from the disk through an exit at an unknown location on the
boundary of the disk. At any time the robots can move anywhere they choose on
the disk, independently of each other, with maximum speed $1$. The robots can
cooperate by exchanging information whenever they meet. We study algorithms for
the two robots to minimize the evacuation time: the time when both robots reach
the exit.
  In [CGGKMP14] the authors gave an algorithm defining trajectories for the two
robots yielding evacuation time at most $5.740$ and also proved that any
algorithm has evacuation time at least $3+ \frac{\pi}{4} + \sqrt{2} \approx
5.199$. We improve both the upper and lower bounds on the evacuation time of a
unit disk. Namely, we present a new non-trivial algorithm whose evacuation time
is at most $5.628$ and show that any algorithm has evacuation time at least $3+
\frac{\pi}{6} + \sqrt{3} \approx 5.255$. To achieve the upper bound, we
designed an algorithm which non-intuitively proposes a forced meeting between
the two robots, even if the exit has not been found by either of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.04986</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.04986</id><created>2015-01-19</created><authors><author><keyname>Herman</keyname><forenames>Joseph L.</forenames></author><author><keyname>Szab&#xf3;</keyname><forenames>Adrienn</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Instv&#xe1;n</forenames></author><author><keyname>Hein</keyname><forenames>Jotun</forenames></author></authors><title>Approximate statistical alignment by iterative sampling of substitution
  matrices</title><categories>q-bio.QM cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline a procedure for jointly sampling substitution matrices and
multiple sequence alignments, according to an approximate posterior
distribution, using an MCMC-based algorithm. This procedure provides an
efficient and simple method by which to generate alternative alignments
according to their expected accuracy, and allows appropriate parameters for
substitution matrices to be selected in an automated fashion. In the cases
considered here, the sampled alignments with the highest likelihood have an
accuracy consistently higher than alignments generated using the standard
BLOSUM62 matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05005</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05005</id><created>2015-01-20</created><updated>2015-11-16</updated><authors><author><keyname>Ar&#x131;kan</keyname><forenames>Erdal</forenames></author></authors><title>Varentropy Decreases Under the Polar Transform</title><categories>cs.IT math.IT</categories><comments>Presented in part at ISIT 2014. Submitted to IEEE Trans. Inform.
  Theory, Aug. 28, 2014. Revised Nov. 16, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the evolution of variance of entropy (varentropy) in the course
of a polar transform operation on binary data elements (BDEs). A BDE is a pair
$(X,Y)$ consisting of a binary random variable $X$ and an arbitrary side
information random variable $Y$. The varentropy of $(X,Y)$ is defined as the
variance of the random variable $-\log p_{X|Y}(X|Y)$. A polar transform of
order two is a certain mapping that takes two independent BDEs and produces two
new BDEs that are correlated with each other. It is shown that the sum of the
varentropies at the output of the polar transform is less than or equal to the
sum of the varentropies at the input, with equality if and only if at least one
of the inputs has zero varentropy. This result is extended to polar transforms
of higher orders and it is shown that the varentropy decreases to zero
asymptotically when the BDEs at the input are independent and identially
distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05007</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05007</id><created>2015-01-20</created><authors><author><keyname>Kim</keyname><forenames>Kwan Suk</forenames></author><author><keyname>Llado</keyname><forenames>Travis</forenames></author><author><keyname>Sentis</keyname><forenames>Luis</forenames></author></authors><title>Full-Body Collision Detection and Reaction with Omnidirectional Mobile
  Platforms: A Step Towards Safe Human-Robot Interaction</title><categories>cs.RO</categories><comments>17 pages, 11 figures, submitted to Springer's Autonomous Robots</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop estimation and control methods for quickly reacting
to collisions between omnidirectional mobile platforms and their environment.
To enable the full-body detection of external forces, we use torque sensors
located in the robot's drivetrain. Using model based techniques we estimate,
with good precision, the location, direction, and magnitude of collision
forces, and we develop an admittance controller that achieves a low effective
mass in reaction to them. For experimental testing, we use a facility
containing a calibrated collision dummy and our holonomic mobile platform. We
subsequently explore collisions with the dummy colliding against a stationary
base and the base colliding against a stationary dummy. Overall, we accomplish
fast reaction times and a reduction of impact forces. A proof of concept
experiment presents various parts of the mobile platform, including the wheels,
colliding safely with humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05016</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05016</id><created>2015-01-20</created><authors><author><keyname>V&#xe1;k&#xe1;r</keyname><forenames>Matthijs</forenames></author></authors><title>A Categorical Semantics for Linear Logical Frameworks</title><categories>cs.LO</categories><comments>Based on the technical report arXiv:1405.0033 . To appear in the
  proceedings of FoSSaCS 2015, in the Advanced Research in Computing and
  Software Science (ARCoSS) subline of Springer's Lecture Notes in Computer
  Science series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A type theory is presented that combines (intuitionistic) linear types with
type dependency, thus properly generalising both intuitionistic dependent type
theory and full linear logic. A syntax and complete categorical semantics are
developed, the latter in terms of (strict) indexed symmetric monoidal
categories with comprehension. Various optional type formers are treated in a
modular way. In particular, we will see that the historically much-debated
multiplicative quantifiers and identity types arise naturally from categorical
considerations. These new multiplicative connectives are further characterised
by several identities relating them to the usual connectives from dependent
type theory and linear logic. Finally, one important class of models, given by
families with values in some symmetric monoidal category, is investigated in
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05020</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05020</id><created>2015-01-20</created><updated>2016-02-07</updated><authors><author><keyname>Dujmovi&#x107;</keyname><forenames>Vida</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Layouts of Expander Graphs</title><categories>math.CO cs.CG cs.DM</categories><journal-ref>Chicago J. Theoretical Computer Science 16:Article 1, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bourgain and Yehudayoff recently constructed $O(1)$-monotone bipartite
expanders. By combining this result with a generalisation of the unraveling
method of Kannan, we construct 3-monotone bipartite expanders, which is best
possible. We then show that the same graphs admit 3-page book embeddings,
2-queue layouts, 4-track layouts, and have simple thickness 2. All these
results are best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05021</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05021</id><created>2015-01-20</created><updated>2015-06-24</updated><authors><author><keyname>Chin</keyname><forenames>Peter</forenames></author><author><keyname>Rao</keyname><forenames>Anup</forenames></author><author><keyname>Vu</keyname><forenames>Van</forenames></author></authors><title>Stochastic Block Model and Community Detection in the Sparse Graphs: A
  spectral algorithm with optimal rate of recovery</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present and analyze a simple and robust spectral algorithm
for the stochastic block model with $k$ blocks, for any $k$ fixed. Our
algorithm works with graphs having constant edge density, under an optimal
condition on the gap between the density inside a block and the density between
the blocks. As a co-product, we settle an open question posed by Abbe et. al.
concerning censor block models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05031</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05031</id><created>2015-01-20</created><authors><author><keyname>Gulko</keyname><forenames>Brad</forenames></author><author><keyname>Leung</keyname><forenames>Samantha</forenames></author></authors><title>Maximin Safety: When Failing to Lose is Preferable to Trying to Win</title><categories>cs.AI cs.GT</categories><comments>14 pages</comments><journal-ref>ECSQARU 2013: 254-265</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new decision rule, \emph{maximin safety}, that seeks to maintain
a large margin from the worst outcome, in much the same way minimax regret
seeks to minimize distance from the best. We argue that maximin safety is
valuable both descriptively and normatively. Descriptively, maximin safety
explains the well-known \emph{decoy effect}, in which the introduction of a
dominated option changes preferences among the other options. Normatively, we
provide an axiomatization that characterizes preferences induced by maximin
safety, and show that maximin safety shares much of the same behavioral basis
with minimax regret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05039</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05039</id><created>2015-01-20</created><authors><author><keyname>Zhu</keyname><forenames>Yangyong</forenames></author><author><keyname>Xiong</keyname><forenames>Yun</forenames></author></authors><title>Defining Data Science</title><categories>cs.DB cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data science is gaining more and more and widespread attention, but no
consensus viewpoint on what data science is has emerged. As a new science, its
objects of study and scientific issues should not be covered by established
sciences. Data in cyberspace have formed what we call datanature. In the
present paper, data science is defined as the science of exploring datanature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05041</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05041</id><created>2015-01-20</created><authors><author><keyname>Luckow</keyname><forenames>Andre</forenames></author><author><keyname>Mantha</keyname><forenames>Pradeep</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>Pilot-Abstraction: A Valid Abstraction for Data-Intensive Applications
  on HPC, Hadoop and Cloud Infrastructures?</title><categories>cs.DC</categories><comments>Submitted to HPDC 2015, 12 pages, 9 figures</comments><acm-class>C.1.4; C.2.4; D.1.3; D.2.12</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HPC environments have traditionally been designed to meet the compute demand
of scientific applications and data has only been a second order concern. With
science moving toward data-driven discoveries relying more on correlations in
data to form scientific hypotheses, the limitations of HPC approaches become
apparent: Architectural paradigms such as the separation of storage and compute
are not optimal for I/O intensive workloads (e.g. for data preparation,
transformation and SQL). While there are many powerful computational and
analytical libraries available on HPC (e.g. for scalable linear algebra), they
generally lack the usability and variety of analytical libraries found in other
environments (e.g. the Apache Hadoop ecosystem). Further, there is a lack of
abstractions that unify access to increasingly heterogeneous infrastructure
(HPC, Hadoop, clouds) and allow reasoning about performance trade-offs in this
complex environment. At the same time, the Hadoop ecosystem is evolving rapidly
and has established itself as de-facto standard for data-intensive workloads in
industry and is increasingly used to tackle scientific problems. In this paper,
we explore paths to interoperability between Hadoop and HPC, examine the
differences and challenges, such as the different architectural paradigms and
abstractions, and investigate ways to address them. We propose the extension of
the Pilot-Abstraction to Hadoop to serve as interoperability layer for
allocating and managing resources across different infrastructures. Further,
in-memory capabilities have been deployed to enhance the performance of
large-scale data analytics (e.g. iterative algorithms) for which the ability to
re-use data across iterations is critical. As memory naturally fits in with the
Pilot concept of retaining resources for a set of tasks, we propose the
extension of the Pilot-Abstraction to in-memory resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05057</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05057</id><created>2015-01-21</created><authors><author><keyname>Hoang</keyname><forenames>Dinh Thai</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Hung</keyname><forenames>Nguyen Tai</forenames></author></authors><title>Optimal Energy Allocation Policy for Wireless Networks in the Sky</title><categories>cs.NI</categories><comments>6 pages, 6 figures, 11 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Google's Project Loon was launched in 2013 with the aim of providing Internet
access to rural and remote areas. In the Loon network, balloons travel around
the Earth and bring access points to the users who cannot connect directly to
the global wired Internet. The signals from the users will be transmitted
through the balloon network to the base stations connected to the Internet
service provider (ISP) on Earth. The process of transmitting and receiving data
consume a certain amount of energy from the balloon, while the energy on
balloons cannot be supplied by stable power source or by replacing batteries
frequently. Instead, the balloons can harvest energy from natural energy
sources, e.g., solar energy, or from radio frequency energy by equipping with
appropriate circuits. However, such kinds of energy sources are often dynamic
and thus how to use this energy efficiently is the main goal of this paper. In
this paper, we study the optimal energy allocation problem for the balloons
such that network performance is optimized and the revenue for service
providers is maximized. We first formulate the stochastic optimization problem
as a Markov decision process and then apply a learning algorithm based on
simulation-based method to obtain optimal policies for the balloons. Numerical
results obtained by extensive simulations clearly show the efficiency and
convergence of the proposed learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05060</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05060</id><created>2015-01-21</created><authors><author><keyname>Thomas</keyname><forenames>Anoop</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Error Correcting Index Codes and Matroids</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connection between index coding and matroid theory have been well studied
in the recent past. El Rouayheb et al. established a connection between
multilinear representation of matroids and wireless index coding. Muralidharan
and Rajan showed that a vector linear solution to an index coding problem
exists if and only if there exists a representable discrete polymatroid
satisfying certain conditions. Recently index coding with erroneous
transmission was considered by Dau et al.. Error correcting index codes in
which all receivers are able to correct a fixed number of errors was studied.
In this paper we consider a more general scenario in which each receiver is
able to correct a desired number of errors, calling such index codes
differential error correcting index codes. A link between differential error
correcting index codes and certain matroids is established. We define matroidal
differential error correcting index codes and we show that a scalar linear
differential error correcting index code exists if and only if it is matroidal
differential error correcting index code associated with a representable
matroid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05080</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05080</id><created>2015-01-21</created><authors><author><keyname>Patel</keyname><forenames>Pankesh</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Enabling High-Level Application Development for the Internet of Things</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>Journal of Systems and Software, Elsevier, 2015, pp.1 - 26</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application development in the Internet of Things (IoT) is challenging
because it involves dealing with a wide range of related issues such as lack of
separation of concerns, and lack of high-level of abstractions to address both
the large scale and heterogeneity. Moreover, stakeholders involved in the
application development have to address issues that can be attributed to
different life-cycles phases. when developing applications. First, the
application logic has to be analyzed and then separated into a set of
distributed tasks for an underlying network. Then, the tasks have to be
implemented for the specific hardware. Apart from handling these issues, they
have to deal with other aspects of life-cycle such as changes in application
requirements and deployed devices. Several approaches have been proposed in the
closely related fields of wireless sensor network, ubiquitous and pervasive
computing, and software engineering in general to address the above challenges.
However, existing approaches only cover limited subsets of the above mentioned
challenges when applied to the IoT. This paper proposes an integrated approach
for addressing the above mentioned challenges. The main contributions of this
paper are: (1) a development methodology that separates IoT application
development into different concerns and provides a conceptual framework to
develop an application, (2) a development framework that implements the
development methodology to support actions of stakeholders. The development
framework provides a set of modeling languages to specify each development
concern and abstracts the scale and heterogeneity related complexity. It
integrates code generation, task-mapping, and linking techniques to provide
automation. Code generation supports the application development phase by
producing a programming framework that allows stakeholders to focus on the
application logic, while our mapping and linking techniques together support
the deployment phase by producing device-specific code to result in a
distributed system collaboratively hosted by individual devices. Our evaluation
based on two realistic scenarios shows that the use of our approach improves
the productivity of stakeholders involved in the application development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05086</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05086</id><created>2015-01-21</created><authors><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Wu</keyname><forenames>Jie</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>Multi-resource Energy-efficient Routing in Cloud Data Centers with
  Networks-as-a-Service</title><categories>cs.NI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of software defined networking and network
function virtualization, researchers have proposed a new cloud networking model
called Network-as-a-Service (NaaS) which enables both in-network packet
processing and application-specific network control. In this paper, we revisit
the problem of achieving network energy efficiency in data centers and identify
some new optimization challenges under the NaaS model. Particularly, we extend
the energy-efficient routing optimization from single-resource to
multi-resource settings. We characterize the problem through a detailed model
and provide a formal problem definition. Due to the high complexity of direct
solutions, we propose a greedy routing scheme to approximate the optimum, where
flows are selected progressively to exhaust residual capacities of active
nodes, and routing paths are assigned based on the distributions of both node
residual capacities and flow demands. By leveraging the structural regularity
of data center networks, we also provide a fast topology-aware heuristic method
based on hierarchically solving a series of vector bin packing instances. Our
simulations show that the proposed routing scheme can achieve significant gain
on energy savings and the topology-aware heuristic can produce comparably good
results while reducing the computation time to a large extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05089</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05089</id><created>2015-01-21</created><authors><author><keyname>Naserasr</keyname><forenames>Reza</forenames></author><author><keyname>Sen</keyname><forenames>Sagnik</forenames></author><author><keyname>Sun</keyname><forenames>Qiang</forenames></author></authors><title>Walk-powers and homomorphism bound of planar graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an extension of the Four-Color Theorem it is conjectured that every planar
graph of odd-girth at least $2k+1$ admits a homomorphism to
$PC_{2k}=(\mathbb{Z}_2^{2k}, \{e_1, e_2, ...,e_{2k}, J\})$ where $e_i$'s are
standard basis and $J$ is all 1 vector. Noting that $PC_{2k}$ itself is of
odd-girth $2k+1$, in this work we show that if the conjecture is true, then
$PC_{2k}$ is an optimal such a graph both with respect to number of vertices
and number of edges. The result is obtained using the notion of walk-power of
graphs and their clique numbers.
  An analogous result is proved for bipartite signed planar graphs of
unbalanced-girth $2k$. The work is presented on a uniform frame work of planar
consistent signed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05097</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05097</id><created>2015-01-21</created><authors><author><keyname>Casta&#xf1;os</keyname><forenames>Fernando</forenames></author><author><keyname>Michalska</keyname><forenames>Hannah</forenames></author><author><keyname>Gromov</keyname><forenames>Dmitry</forenames></author><author><keyname>Hayward</keyname><forenames>Vincent</forenames></author></authors><title>Discrete-Time Models for Implicit Port-Hamiltonian Systems</title><categories>cs.SY cs.NA math.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implicit representations of finite-dimensional port-Hamiltonian systems are
studied from the perspective of their use in numerical simulation and control
design. Implicit representations arise when a system is modeled in Cartesian
coordinates and when the system constraints are applied in the form of
additional algebraic equations (the system model is in a DAE form). Such
representations lend themselves better to sample-data approximations. An
implicit representation of a port-Hamiltonian system is given and it is shown
how to construct a sampled-data model that preserves the port-Hamiltonian
structure under sample and hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05098</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05098</id><created>2015-01-21</created><authors><author><keyname>Kosta</keyname><forenames>Marek</forenames></author><author><keyname>Sturm</keyname><forenames>Thomas</forenames></author><author><keyname>Dolzmann</keyname><forenames>Andreas</forenames></author></authors><title>Better Answers to Real Questions</title><categories>cs.SC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider existential problems over the reals. Extended quantifier
elimination generalizes the concept of regular quantifier elimination by
providing in addition answers, which are descriptions of possible assignments
for the quantified variables. Implementations of extended quantifier
elimination via virtual substitution have been successfully applied to various
problems in science and engineering. So far, the answers produced by these
implementations included infinitesimal and infinite numbers, which are hard to
interpret in practice. We introduce here a post-processing procedure to
convert, for fixed parameters, all answers into standard real numbers. The
relevance of our procedure is demonstrated by application of our implementation
to various examples from the literature, where it significantly improves the
quality of the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05104</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05104</id><created>2015-01-21</created><updated>2015-02-04</updated><authors><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>LACL</affiliation></author><author><keyname>Bagnol</keyname><forenames>Marc</forenames><affiliation>I2M</affiliation></author><author><keyname>Seiller</keyname><forenames>Thomas</forenames><affiliation>IHES</affiliation></author></authors><title>Memoization for Unary Logic Programming: Characterizing PTIME</title><categories>cs.LO cs.CC math.LO</categories><comments>Soumis {\`a} LICS 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a characterization of deterministic polynomial time computation based
on an algebraic structure called the resolution semiring, whose elements can be
understood as logic programs or sets of rewriting rules over first-order terms.
More precisely, we study the restriction of this framework to terms (and logic
programs, rewriting rules) using only unary symbols. We prove it is complete
for polynomial time computation, using an encoding of pushdown automata. We
then introduce an algebraic counterpart of the memoization technique in order
to show its PTIME soundness. We finally relate our approach and complexity
results to complexity of logic programming. As an application of our
techniques, we show a PTIME-completeness result for a class of logic
programming queries which use only unary function symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05115</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05115</id><created>2015-01-21</created><updated>2015-07-31</updated><authors><author><keyname>Melli&#xe8;s</keyname><forenames>Paul-Andr&#xe9;</forenames></author><author><keyname>Zeilberger</keyname><forenames>Noam</forenames></author></authors><title>An Isbell Duality Theorem for Type Refinement Systems</title><categories>cs.LO math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any refinement system (= functor) has a fully faithful representation in the
refinement system of presheaves, by interpreting types as relative slice
categories, and refinement types as presheaves over those categories. Motivated
by an analogy between side effects in programming and *context effects* in
linear logic, we study logical aspects of this &quot;positive&quot; (covariant)
representation, as well as of an associated &quot;negative&quot; (contravariant)
representation. We establish several preservation properties for these
representations, including a generalization of Day's embedding theorem for
monoidal closed categories. Then we establish that the positive and negative
representations satisfy an Isbell-style duality. As corollaries, we derive two
different formulas for the positive representation of a pushforward (inspired
by the classical negative translations of proof theory), which express it
either as the dual of a pullback of a dual, or as the double dual of a
pushforward. Besides explaining how these constructions on refinement systems
generalize familiar category-theoretic ones (by viewing categories as special
refinement systems), our main running examples involve representations of Hoare
Logic and linear sequent calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05120</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05120</id><created>2015-01-21</created><authors><author><keyname>Akhtar</keyname><forenames>Nadeem</forenames></author></authors><title>Requirements, Formal Verification and Model transformations of an
  Agent-based System: A CASE STUDY</title><categories>cs.SE cs.MA</categories><comments>16 pages; Computer Engineering and Intelligent Systems
  http://www.iiste.org - ISSN 2222-1719 (Paper) ISSN 2222-2863 (Online) -
  Vol.5, No.3, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most challenging tasks in software specifications engineering for
a multi-agent system is to ensure correctness. As these systems have high
concurrency, often have dynamic environments, the formal specification and
verification of these systems along with step-wise refinement from abstract to
concrete concepts play major role in system correctness. Our objectives are the
formal specification, analysis with respect to functional as well as
non-functional properties by step-wise refinement from abstract to concrete
specifications and then formal verification of these specifications. A
multi-agent system is concurrent system with processes working in parallel with
synchronization between them. We have worked on Gaia multi-agent method along
with finite state process based finite automata techniques and as a result we
have defined the formal specifications of our system, checked the correctness
and verified all possible flow of concurrent executions of these
specifications. Our contribution consists in transforming requirement
specifications based on organizational abstractions into executable formal
verification specifications based on finite automata. We have considered a case
study of our multi-agent system to exemplify formal specifications and
verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05126</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05126</id><created>2015-01-21</created><updated>2015-12-16</updated><authors><author><keyname>Pastor</keyname><forenames>Giancarlo</forenames></author><author><keyname>Mora-Jim&#xe9;nez</keyname><forenames>Inmaculada</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Caama&#xf1;o</keyname><forenames>Antonio J.</forenames></author></authors><title>Mathematics of Sparsity and Entropy: Axioms, Core Functions and Sparse
  Recovery</title><categories>cs.IT math.IT</categories><comments>33 pages, 4 figures, 9 tables This very preliminary work has been
  withdrawn by the author due to crucial errors. No future version will be
  available</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparsity and entropy are pillar notions of modern theories in signal
processing and information theory. However, there is no clear consensus among
scientists on the characterization of these notions. Previous efforts have
contributed to understand individually sparsity or entropy from specific
research interests. This paper proposes a mathematical formalism, a joint
axiomatic characterization, which contributes to comprehend (the beauty of)
sparsity and entropy. The paper gathers and introduces inherent and first
principles criteria as axioms and attributes that jointly characterize sparsity
and entropy. The proposed set of axioms is constructive and allows to derive
simple or \emph{core functions} and further generalizations. Core sparsity
generalizes the Hoyer measure, Gini index and $pq$-means. Core entropy
generalizes the R\'{e}nyi entropy and Tsallis entropy, both of which generalize
Shannon entropy. Finally, core functions are successfully applied to compressed
sensing and to minimum entropy given sample moments. More importantly, the
(simplest) core sparsity adds theoretical support to the $\ell_1$-minimization
approach in compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05132</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05132</id><created>2015-01-21</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Martins</keyname><forenames>Bruno</forenames></author><author><keyname>Calado</keyname><forenames>P&#xe1;vel</forenames></author></authors><title>Learning to Rank Academic Experts in the DBLP Dataset</title><categories>cs.IR</categories><comments>Expert Systems, 2013. arXiv admin note: text overlap with
  arXiv:1302.0413</comments><doi>10.1111/exsy.12062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert finding is an information retrieval task that is concerned with the
search for the most knowledgeable people with respect to a specific topic, and
the search is based on documents that describe people's activities. The task
involves taking a user query as input and returning a list of people who are
sorted by their level of expertise with respect to the user query. Despite
recent interest in the area, the current state-of-the-art techniques lack in
principled approaches for optimally combining different sources of evidence.
This article proposes two frameworks for combining multiple estimators of
expertise. These estimators are derived from textual contents, from
graph-structure of the citation patterns for the community of experts, and from
profile information about the experts. More specifically, this article explores
the use of supervised learning to rank methods, as well as rank aggregation
approaches, for combing all of the estimators of expertise. Several supervised
learning algorithms, which are representative of the pointwise, pairwise and
listwise approaches, were tested, and various state-of-the-art data fusion
techniques were also explored for the rank aggregation framework. Experiments
that were performed on a dataset of academic publications from the Computer
Science domain attest the adequacy of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05136</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05136</id><created>2015-01-21</created><authors><author><keyname>de Oliveira</keyname><forenames>Silvana Roque</forenames></author><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Borbinha</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Garcia</keyname><forenames>Mar&#xed;a &#xc1;mgeles Zuleta</forenames></author></authors><title>Uma an\'alise bibliom\'etrica do Congresso Nacional de Bibliotec\'arios,
  Arquivistas e Documentalistas (1985-2012)</title><categories>cs.DL</categories><comments>in Portuguese</comments><journal-ref>Cadernos BAD, N. 1/2 (2012/2013), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is the first bibliometric analysis of the 708 lectures published
by The Librarians and Archivists National Congress between 1985 and 2012,
having been developed markers for production, productivity, institutional
origin and thematic analysis, in a quantitative, relational and diachronic
perspective. Its results show a dynamic congress, essentially national and
professional, with a strong majority of individual authorships, even with the
recent growth of the ratio of collaborations. In its thematic approach,
emphasis is given to public services of information, with the greatest focus
being on libraries, while still giving relevance to reflections on professional
and academic training in the area of Information Sciences, and also following
the most recent technological developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05138</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05138</id><created>2015-01-21</created><authors><author><keyname>de Oliveira</keyname><forenames>Silvana Roque</forenames></author><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Borbinha</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Garcia</keyname><forenames>Mar&#xed;a &#xc1;ngeles Zulueta</forenames></author></authors><title>Thematic Identification of 'Little Science': Trends in Portuguese IS&amp;LS
  Literature by Controlled Vocabulary and Co-Word Analysis</title><categories>cs.DL</categories><comments>In Proceedings of the 5th International Conference on Qualitative and
  Quantitative Methods in Libraries, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents an overview of IS&amp;LS thematic trends in Portugal between
2001 and 2012. The results were obtained by means of an analysis, using
expeditious qualitative and quantitative techniques, of the bibliographic
records of proceedings papers identified during this period. These records were
processed using two techniques: a manual subject classification and an
automated co-word analysis of the Author-Assigned Keywords. From this we
designed cluster and co-occurrence maps, using the VOSviewer and the Pajek
software packages. The results indicated an accentuated dynamism in the
thematic evolution of this documental corpus, apart from revealing a
significant difference among the themes transmitted in nationally and
internationally visible production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05139</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05139</id><created>2015-01-21</created><authors><author><keyname>Havemann</keyname><forenames>Frank</forenames></author><author><keyname>Gl&#xe4;ser</keyname><forenames>Jochen</forenames></author><author><keyname>Heinz</keyname><forenames>Michael</forenames></author></authors><title>Detecting Overlapping Link Communities by Finding Local Minima of a Cost
  Function with a Memetic Algorithm. Part 1: Problem and Method</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 2 figures, 2 appendixes. There is some overlap in Appendix
  A with our earlier preprint arXiv:1206.3992</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose an algorithm for detecting communities of links in networks which
uses local information, is based on a new evaluation function, and allows for
pervasive overlaps of communities. The complexity of the clustering task
requires the application of a memetic algorithm that combines probabilistic
evolutionary strategies with deterministic local searches. In Part 2 we will
present results of experiments with a large citation network of astrophysical
papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05140</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05140</id><created>2015-01-21</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Martins</keyname><forenames>Bruno</forenames></author><author><keyname>Calado</keyname><forenames>P&#xe1;vel</forenames></author></authors><title>Using Rank Aggregation for Expert Search in Academic Digital Libraries</title><categories>cs.IR</categories><comments>In Simp\'{o}sio de Inform\'{a}tica, INForum, Portugal, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of expert finding has been getting increasing attention in
information retrieval literature. However, the current state-of-the-art is
still lacking in principled approaches for combining different sources of
evidence. This paper explores the usage of unsupervised rank aggregation
methods as a principled approach for combining multiple estimators of
expertise, derived from the textual contents, from the graph-structure of the
citation patterns for the community of experts, and from profile information
about the experts. We specifically experimented two unsupervised rank
aggregation approaches well known in the information retrieval literature,
namely CombSUM and CombMNZ. Experiments made over a dataset of academic
publications for the area of Computer Science attest for the adequacy of these
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05141</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05141</id><created>2015-01-21</created><updated>2015-01-22</updated><authors><author><keyname>Giabbanelli</keyname><forenames>Philippe J.</forenames></author><author><keyname>Peters</keyname><forenames>Joseph G.</forenames></author></authors><title>An Algebra to Merge Heterogeneous Classifiers</title><categories>cs.DM cs.LG</categories><comments>19 pages, 8 figures</comments><msc-class>97R50, 08Axx</msc-class><acm-class>H.2.8; I.1.2; I.5.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In distributed classification, each learner observes its environment and
deduces a classifier. As a learner has only a local view of its environment,
classifiers can be exchanged among the learners and integrated, or merged, to
improve accuracy. However, the operation of merging is not defined for most
classifiers. Furthermore, the classifiers that have to be merged may be of
different types in settings such as ad-hoc networks in which several
generations of sensors may be creating classifiers. We introduce decision
spaces as a framework for merging possibly different classifiers. We formally
study the merging operation as an algebra, and prove that it satisfies a
desirable set of properties. The impact of time is discussed for the two main
data mining settings. Firstly, decision spaces can naturally be used with
non-stationary distributions, such as the data collected by sensor networks, as
the impact of a model decays over time. Secondly, we introduce an approach for
stationary distributions, such as homogeneous databases partitioned over
different learners, which ensures that all models have the same impact. We also
present a method that uses storage flexibly to achieve different types of decay
for non-stationary distributions. Finally, we show that the algebraic approach
developed for merging can also be used to analyze the behaviour of other
operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05145</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05145</id><created>2015-01-21</created><updated>2015-07-01</updated><authors><author><keyname>Sasidevan</keyname><forenames>V.</forenames></author><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author></authors><title>Symmetry warrants rational cooperation by co-action in Social Dilemmas</title><categories>physics.soc-ph cs.GT nlin.AO</categories><comments>9 pages, 2 figures</comments><journal-ref>Scientific Reports, 5, 13071 (2015)</journal-ref><doi>10.1038/srep13071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it rational for selfish individuals to cooperate? The conventional answer
based on analysis of games such as the Prisoners Dilemma (PD) is that it is
not, even though mutual cooperation results in a better outcome for all. This
incompatibility between individual rationality and collective benefit lies at
the heart of questions about the evolution of cooperation, as illustrated by PD
and similar games. Here, we argue that this apparent incompatibility is due to
an inconsistency in the standard Nash framework for analyzing non-cooperative
games and propose a new paradigm, that of the co-action equilibrium. As in the
Nash solution, agents know that others are just as rational as them and taking
this into account leads them to realize that others will independently adopt
the same strategy, in contrast to the idea of unilateral deviation central to
Nash equilibrium thinking. Co-action equilibrium results in better collective
outcomes for games representing social dilemmas, with relatively &quot;nicer&quot;
strategies being chosen by rational selfish individuals. In particular, the
dilemma of PD gets resolved within this framework, suggesting that cooperation
can evolve in nature as the rational outcome even for selfish agents, without
having to take recourse to additional mechanisms for promoting it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05147</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05147</id><created>2015-01-21</created><updated>2015-06-13</updated><authors><author><keyname>Furusawa</keyname><forenames>Hitoshi</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Taming Multirelations</title><categories>cs.LO</categories><comments>34 pages, 4 figures</comments><acm-class>F.1.2; F.3.1; F.3.2; F.4.1; I.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary multirelations generalise binary relations by associating elements of
a set to its subsets. We study the structure and algebra of multirelations
under the operations of union, intersection, sequential and parallel
composition, as well as finite and infinite iteration. Starting from a
set-theoretic investigation, we propose axiom systems for multirelations in
contexts ranging from bi-monoids to bi-quantales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05151</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05151</id><created>2015-01-21</created><authors><author><keyname>Kurz</keyname><forenames>Gerhard</forenames></author><author><keyname>Gilitschenski</keyname><forenames>Igor</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Recursive Bayesian Filtering in Circular State Spaces</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For recursive circular filtering based on circular statistics, we introduce a
general framework for estimation of a circular state based on different
circular distributions, specifically the wrapped normal distribution and the
von Mises distribution. We propose an estimation method for circular systems
with nonlinear system and measurement functions. This is achieved by relying on
efficient deterministic sampling techniques. Furthermore, we show how the
calculations can be simplified in a variety of important special cases, such as
systems with additive noise as well as identity system or measurement
functions. We introduce several novel key components, particularly a
distribution-free prediction algorithm, a new and superior formula for the
multiplication of wrapped normal densities, and the ability to deal with
non-additive system noise. All proposed methods are thoroughly evaluated and
compared to several state-of-the-art solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05152</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05152</id><created>2015-01-21</created><authors><author><keyname>Yang</keyname><forenames>Heng</forenames></author><author><keyname>Patras</keyname><forenames>Ioannis</forenames></author></authors><title>Mirror, mirror on the wall, tell me, is the error small?</title><categories>cs.CV</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do object part localization methods produce bilaterally symmetric results on
mirror images? Surprisingly not, even though state of the art methods augment
the training set with mirrored images. In this paper we take a closer look into
this issue. We first introduce the concept of mirrorability as the ability of a
model to produce symmetric results in mirrored images and introduce a
corresponding measure, namely the \textit{mirror error} that is defined as the
difference between the detection result on an image and the mirror of the
detection result on its mirror image. We evaluate the mirrorability of several
state of the art algorithms in two of the most intensively studied problems,
namely human pose estimation and face alignment. Our experiments lead to
several interesting findings: 1) Surprisingly, most of state of the art methods
struggle to preserve the mirror symmetry, despite the fact that they do have
very similar overall performance on the original and mirror images; 2) the low
mirrorability is not caused by training or testing sample bias - all algorithms
are trained on both the original images and their mirrored versions; 3) the
mirror error is strongly correlated to the localization/alignment error (with
correlation coefficients around 0.7). Since the mirror error is calculated
without knowledge of the ground truth, we show two interesting applications -
in the first it is used to guide the selection of difficult samples and in the
second to give feedback in a popular Cascaded Pose Regression method for face
alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05153</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05153</id><created>2015-01-21</created><authors><author><keyname>Akhtar</keyname><forenames>Nadeem</forenames></author><author><keyname>Guyadec</keyname><forenames>Yann Le</forenames></author><author><keyname>Oquendo</keyname><forenames>Flavio</forenames></author></authors><title>Formal requirement and architecture specifications of a multi-agent
  robotic system</title><categories>cs.SE cs.MA</categories><comments>6 pages</comments><journal-ref>Journal of Computing, eISSN 2151-9617. Volume 4, Issue 4, April
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most challenging tasks in specification engineering for a
multi-agent robotic system is to formally specify and architect the system,
especially as a multi-agent robotic system is concurrent having concurrent
processing, and often having dynamic environment. The formal requirement and
architecture specifications along with step-wise refinement from abstract to
concrete concepts play major role in formalizing the system. This paper
proposes the formal requirement and architecture specifications aspects of an
approach that supports analysis with respect to functional as well as
non-functional properties by step-wise refinement from abstract to concrete
specifications and formal architecture definition. These formal specifications
have been exemplified by a case study. As formal specification techniques are
getting more mature, our capability to build a correct complex multi-agent
robotic system also grows quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05177</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05177</id><created>2015-01-21</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Optimal Fractional Repetition Codes and Fractional Repetition Batch
  Codes</title><categories>cs.IT cs.DM math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.4734</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional repetition (FR) codes is a family of codes for distributed storage
systems (DSS) that allow uncoded exact repairs with minimum repair bandwidth.
In this work, we consider a bound on the maximum amount of data that can be
stored using an FR code. Optimal FR codes which attain this bound are
presented. The constructions of these FR codes are based on families of regular
graphs, such as Tur\'an graphs and graphs with large girth; and on
combinatorial designs, such as transversal designs and generalized polygons. In
addition, based on a connection between FR codes and batch codes, we propose a
new family of codes for DSS, called fractional repetition batch codes, which
allow uncoded efficient exact repairs and load balancing which can be performed
by several users in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05180</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05180</id><created>2015-01-21</created><authors><author><keyname>Adamek</keyname><forenames>Jiri</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Myers</keyname><forenames>Robert</forenames></author><author><keyname>Urbat</keyname><forenames>Henning</forenames></author></authors><title>Varieties of Languages in a Category</title><categories>cs.FL cs.LO math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eilenberg's variety theorem, a centerpiece of algebraic automata theory,
establishes a bijective correspondence between varieties of languages and
pseudovarieties of monoids. In the present paper this result is generalized to
an abstract pair of algebraic categories: we introduce varieties of languages
in a category C, and prove that they correspond to pseudovarieties of monoids
in a closed monoidal category D, provided that C and D are dual on the level of
finite objects. By suitable choices of these categories our result uniformly
covers Eilenberg's theorem and three variants due to Pin, Polak and Reutenauer,
respectively, and yields new Eilenberg-type correspondences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05186</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05186</id><created>2015-01-21</created><authors><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>McKay</keyname><forenames>Matthew R.</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Artificial-Noise-Aided Secure Multi-Antenna Transmission with Limited
  Feedback</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2015.2391261</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an optimized secure multi-antenna transmission approach based on
artificial-noise-aided beamforming, with limited feedback from a desired
single-antenna receiver. To deal with beamformer quantization errors as well as
unknown eavesdropper channel characteristics, our approach is aimed at
maximizing throughput under dual performance constraints - a connection outage
constraint on the desired communication channel and a secrecy outage constraint
to guard against eavesdropping. We propose an adaptive transmission strategy
that judiciously selects the wiretap coding parameters, as well as the power
allocation between the artificial noise and the information signal. This
optimized solution reveals several important differences with respect to
solutions designed previously under the assumption of perfect feedback. We also
investigate the problem of how to most efficiently utilize the feedback bits.
The simulation results indicate that a good design strategy is to use
approximately 20% of these bits to quantize the channel gain information, with
the remainder to quantize the channel direction, and this allocation is largely
insensitive to the secrecy outage constraint imposed. In addition, we find that
8 feedback bits per transmit antenna is sufficient to achieve approximately 90%
of the throughput attainable with perfect feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05192</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05192</id><created>2015-01-21</created><updated>2015-01-23</updated><authors><author><keyname>Aktas</keyname><forenames>Umit Rusen</forenames></author><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Leonardis</keyname><forenames>Ales</forenames></author><author><keyname>Wyatt</keyname><forenames>Jeremy L.</forenames></author></authors><title>A Graph Theoretic Approach for Object Shape Representation in
  Compositional Hierarchies Using a Hybrid Generative-Descriptive Model</title><categories>cs.CV</categories><comments>Paper : 17 pages. 13th European Conference on Computer Vision (ECCV
  2014), Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III, pp
  566-581. Supplementary material can be downloaded from
  http://link.springer.com/content/esm/chp:10.1007/978-3-319-10578-9_37/file/MediaObjects/978-3-319-10578-9_37_MOESM1_ESM.pdf</comments><doi>10.1007/978-3-319-10578-9_37</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph theoretic approach is proposed for object shape representation in a
hierarchical compositional architecture called Compositional Hierarchy of Parts
(CHOP). In the proposed approach, vocabulary learning is performed using a
hybrid generative-descriptive model. First, statistical relationships between
parts are learned using a Minimum Conditional Entropy Clustering algorithm.
Then, selection of descriptive parts is defined as a frequent subgraph
discovery problem, and solved using a Minimum Description Length (MDL)
principle. Finally, part compositions are constructed by compressing the
internal data representation with discovered substructures. Shape
representation and computational complexity properties of the proposed approach
and algorithms are examined using six benchmark two-dimensional shape image
datasets. Experiments show that CHOP can employ part shareability and indexing
mechanisms for fast inference of part compositions using learned shape
vocabularies. Additionally, CHOP provides better shape retrieval performance
than the state-of-the-art shape retrieval methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05194</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05194</id><created>2015-01-21</created><updated>2015-10-19</updated><authors><author><keyname>Marrelec</keyname><forenames>Guillaume</forenames></author><author><keyname>Mess&#xe9;</keyname><forenames>Arnaud</forenames></author><author><keyname>Bellec</keyname><forenames>Pierre</forenames></author></authors><title>A Bayesian alternative to mutual information for the hierarchical
  clustering of dependent random variables</title><categories>stat.ML cs.LG q-bio.QM</categories><journal-ref>G. Marrelec, A. Messe, P. Bellec (2015) A Bayesian alternative to
  mutual information for the hierarchical clustering of dependent random
  variables. PLoS ONE 10(9): e0137278</journal-ref><doi>10.1371/journal.pone.0137278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of mutual information as a similarity measure in agglomerative
hierarchical clustering (AHC) raises an important issue: some correction needs
to be applied for the dimensionality of variables. In this work, we formulate
the decision of merging dependent multivariate normal variables in an AHC
procedure as a Bayesian model comparison. We found that the Bayesian
formulation naturally shrinks the empirical covariance matrix towards a matrix
set a priori (e.g., the identity), provides an automated stopping rule, and
corrects for dimensionality using a term that scales up the measure as a
function of the dimensionality of the variables. Also, the resulting log Bayes
factor is asymptotically proportional to the plug-in estimate of mutual
information, with an additive correction for dimensionality in agreement with
the Bayesian information criterion. We investigated the behavior of these
Bayesian alternatives (in exact and asymptotic forms) to mutual information on
simulated and real data. An encouraging result was first derived on
simulations: the hierarchical clustering based on the log Bayes factor
outperformed off-the-shelf clustering techniques as well as raw and normalized
mutual information in terms of classification accuracy. On a toy example, we
found that the Bayesian approaches led to results that were similar to those of
mutual information clustering techniques, with the advantage of an automated
thresholding. On real functional magnetic resonance imaging (fMRI) datasets
measuring brain activity, it identified clusters consistent with the
established outcome of standard procedures. On this application, normalized
mutual information had a highly atypical behavior, in the sense that it
systematically favored very large clusters. These initial experiments suggest
that the proposed Bayesian alternatives to mutual information are a useful new
tool for hierarchical clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05197</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05197</id><created>2015-01-21</created><authors><author><keyname>Adar</keyname><forenames>Ron</forenames></author><author><keyname>Epstein</keyname><forenames>Leah</forenames></author></authors><title>The weighted 2-metric dimension of trees in the non-landmarks model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let T=(V,E) be a tree graph with non-negative weights defined on the
vertices. A vertex z is called a separating vertex for u and v if the distances
of z to u and v are not equal. A set of vertices L\subseteq V is a feasible
solution for the non-landmarks model (NL), if for every pair of distinct
vertices, u,v \in V\setminus L, there are at least two vertices of L separating
them. Such a feasible solution is called a &quot;landmark set&quot;. We analyze the
structure of landmark sets for trees and design a linear time algorithm for
finding a minimum cost landmark set for a given tree graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05198</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05198</id><created>2015-01-21</created><updated>2015-07-06</updated><authors><author><keyname>Colman</keyname><forenames>Ewan R.</forenames></author><author><keyname>Greetham</keyname><forenames>Danica Vukadinovi&#x107;</forenames></author></authors><title>Memory and burstiness in dynamic networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>13 pages, 7 figures</comments><journal-ref>Phys. Rev. E 92, 012817 (2015)</journal-ref><doi>10.1103/PhysRevE.92.012817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discrete-time random process is described which can generate bursty
sequences of events. A Bernoulli process, where the probability of an event
occurring at time $t$ is given by a fixed probability $x$, is modified to
include a memory effect where the event probability is increased proportionally
to the number of events which occurred within a given amount of time preceding
$t$. For small values of $x$ the inter-event time distribution follows a
power-law with exponent $-2-x$. We consider a dynamic network where each node
forms, and breaks connections according to this process. The value of $x$ for
each node depends on the fitness distribution, $\rho(x)$, from which it is
drawn; we find exact solutions for the expectation of the degree distribution
for a variety of possible fitness distributions, and for both cases where the
memory effect either is, or is not present. This work can potentially lead to
methods to uncover hidden fitness distributions from fast changing, temporal
network data such as online social communications and fMRI scans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05203</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05203</id><created>2015-01-21</created><updated>2015-02-18</updated><authors><author><keyname>Chen</keyname><forenames>Geliang</forenames></author></authors><title>Phrase Based Language Model for Statistical Machine Translation:
  Empirical Study</title><categories>cs.CL</categories><comments>supplementary material of http://arxiv.org/abs/1501.04324. This
  version is identical to the Bachelor thesis of Geliang Chen archived on the
  20th June 2013 in Peking University. Thesis advisor: Professor Jia Xu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reordering is a challenge to machine translation (MT) systems. In MT, the
widely used approach is to apply word based language model (LM) which considers
the constituent units of a sentence as words. In speech recognition (SR), some
phrase based LM have been proposed. However, those LMs are not necessarily
suitable or optimal for reordering. We propose two phrase based LMs which
considers the constituent units of a sentence as phrases. Experiments show that
our phrase based LMs outperform the word based LM with the respect of
perplexity and n-best list re-ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05212</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05212</id><created>2015-01-21</created><authors><author><keyname>Mirzamany</keyname><forenames>Esmat</forenames></author><author><keyname>Friderikos</keyname><forenames>Vasilis</forenames></author></authors><title>QoE-Centric Localized Mobility Management for Future Mobile Networks</title><categories>cs.NI</categories><comments>IEEE Globecom Conference 2014 (unpublished)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility support in future networks will be predominately based on micro
mobility protocols. Current proposed schemes such as Hierarchical Mobile IPv6
(HMIPv6) and more importantly Proxy Mobile IPv6 (PMIPv6) provide localized
mobility support by electing a node within the network (topologically close to
the Access Routers (AR)) to act as mobility anchor. Such schemes can
significantly improve handover latency, as well as the end-to-end signalling
overhead, but might entail scalability issues, which in some instances, do not
fit adequately with the current explosion of mobile Internet traffic, and the
evolutionary trend towards flat network architectures. The notion of using
Distributed Mobility Management (DMM) allows for decentralization by anchoring
nodes at their AR. The idea is that for sessions with duration less than the
cell residence time efficient mobility can be supported. However, DMM might be
highly suboptimal in instances where nodes perform multiple handovers during
the session lifetime. Hence, one approach cannot be effective for all types of
applications. In this paper a hybrid mobility management solution, integrated
with a new routing scheme, is proposed. The scheme selects the most suitable
mobility approach, centralized or distributed, by taking into account the
flows' requirement, in terms of Quality of Experience (QoE), and new routing
constraints. The results of optimization problem show that the proposed
approach can achieve efficient resource utilisation, ease network congestion,
and lead to a significant improvement in the QoE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05215</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05215</id><created>2015-01-21</created><authors><author><keyname>Thwaites</keyname><forenames>Peter A.</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author></authors><title>A Separation Theorem for Chain Event Graphs</title><categories>stat.ME cs.AI</categories><comments>39 pages, 10 figures. Submitted to Electronic Journal of Statistics</comments><msc-class>62F15 (Primary), 68T37 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Networks (BNs) are popular graphical models for the representation
of statistical problems embodying dependence relationships between a number of
variables. Much of this popularity is due to the d-separation theorem of Pearl
and Lauritzen, which allows an analyst to identify the conditional independence
statements that a model of the problem embodies using only the topology of the
graph. However for many problems the complete model dependence structure cannot
be depicted by a BN. The Chain Event Graph (CEG) was introduced for these types
of problem. In this paper we introduce a separation theorem for CEGs, analogous
to the d-separation theorem for BNs, which likewise allows an analyst to
identify the conditional independence structure of their model from the
topology of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05222</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05222</id><created>2015-01-21</created><authors><author><keyname>Curtin</keyname><forenames>Ryan R.</forenames></author><author><keyname>Lee</keyname><forenames>Dongryeol</forenames></author><author><keyname>March</keyname><forenames>William B.</forenames></author><author><keyname>Ram</keyname><forenames>Parikshit</forenames></author></authors><title>Plug-and-play dual-tree algorithm runtime analysis</title><categories>cs.DS cs.LG</categories><comments>Submitted to JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous machine learning algorithms contain pairwise statistical problems at
their core---that is, tasks that require computations over all pairs of input
points if implemented naively. Often, tree structures are used to solve these
problems efficiently. Dual-tree algorithms can efficiently solve or approximate
many of these problems. Using cover trees, rigorous worst-case runtime
guarantees have been proven for some of these algorithms. In this paper, we
present a problem-independent runtime guarantee for any dual-tree algorithm
using the cover tree, separating out the problem-dependent and the
problem-independent elements. This allows us to just plug in bounds for the
problem-dependent elements to get runtime guarantees for dual-tree algorithms
for any pairwise statistical problem without re-deriving the entire proof. We
demonstrate this plug-and-play procedure for nearest-neighbor search and
approximate kernel density estimation to get improved runtime guarantees. Under
mild assumptions, we also present the first linear runtime guarantee for
dual-tree based range search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05237</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05237</id><created>2015-01-21</created><authors><author><keyname>Koniaris</keyname><forenames>Marios</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author><author><keyname>Vassiliou</keyname><forenames>Yannis</forenames></author></authors><title>Network Analysis in the Legal Domain: A complex model for European Union
  legal sources</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.3.7; J.1; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Legislators, designers of legal information systems, as well as citizens face
often problems due to the interdependence of the laws and the growing number of
references needed to interpret them. Quantifying this complexity is not an easy
task. In this paper, we introduce the &quot;Legislation Network&quot; as a novel approach
to address related problems. We have collected an extensive data set of a more
than 60-year old legislation corpus, as published in the Official Journal of
the European Union, and we further analysed it as a complex network, thus
gaining insight into its topological structure. Among other issues, we have
performed a temporal analysis of the evolution of the Legislation Network, as
well as a robust resilience test to assess its vulnerability under specific
cases that may lead to possible breakdowns. Results are quite promising,
showing that our approach can lead towards an enhanced explanation in respect
to the structure and evolution of legislation properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05239</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05239</id><created>2015-01-21</created><updated>2015-08-20</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>MOAIS, ARIC</affiliation></author><author><keyname>Sultan</keyname><forenames>Ziad</forenames><affiliation>MOAIS, LJK</affiliation></author></authors><title>Computing the Rank Profile Matrix</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>ACM International Symposium on Symbolic and Algebraic
  Computations, pp.146--153, 2015, ISSAC 2015</journal-ref><doi>10.1145/2755996.2756682</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The row (resp. column) rank profile of a matrix describes the staircase shape
of its row (resp. column) echelon form. In an ISSAC'13 paper, we proposed a
recursive Gaussian elimination that can compute simultaneously the row and
column rank profiles of a matrix as well as those of all of its leading
sub-matrices, in the same time as state of the art Gaussian elimination
algorithms. Here we first study the conditions making a Gaus-sian elimination
algorithm reveal this information. Therefore, we propose the definition of a
new matrix invariant, the rank profile matrix, summarizing all information on
the row and column rank profiles of all the leading sub-matrices. We also
explore the conditions for a Gaussian elimination algorithm to compute all or
part of this invariant, through the corresponding PLUQ decomposition. As a
consequence, we show that the classical iterative CUP decomposition algorithm
can actually be adapted to compute the rank profile matrix. Used, in a Crout
variant, as a base-case to our ISSAC'13 implementation, it delivers a
significant improvement in efficiency. Second, the row (resp. column) echelon
form of a matrix are usually computed via different dedicated triangular
decompositions. We show here that, from some PLUQ decompositions, it is
possible to recover the row and column echelon forms of a matrix and of any of
its leading sub-matrices thanks to an elementary post-processing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05260</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05260</id><created>2015-01-16</created><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>An Algebra of Reversible Quantum Computing</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.2960,
  arXiv:1410.5131, arXiv:1312.0686, arXiv:1404.0665</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the axiomatization of reversible computing RACP, we generalize it to
quantum reversible computing which is called qRACP. By use of the framework of
quantum configuration, we show that structural reversibility and quantum state
reversibility must be satisfied simultaneously in quantum reversible
computation. RACP and qRACP has the same axiomatization modulo the so-called
quantum forward-reverse bisimularity, that is, classical reversible computing
and quantum reversible computing are unified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05265</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05265</id><created>2015-01-21</created><authors><author><keyname>Marcozzi</keyname><forenames>Micha&#xeb;l</forenames></author><author><keyname>Vanhoof</keyname><forenames>Wim</forenames></author><author><keyname>Hainaut</keyname><forenames>Jean-Luc</forenames></author></authors><title>A Direct Symbolic Execution of SQL Code for Testing of Data-Oriented
  Applications</title><categories>cs.SE cs.DB</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic execution is a technique which enables automatically generating test
inputs (and outputs) exercising a set of execution paths within a program to be
tested. If the paths cover a sufficient part of the code under test, the test
data offer a representative view of the program's actual behaviour, which
notably enables detecting errors and correcting faults. Relational databases
are ubiquitous in software, but symbolic execution of pieces of code that
manipulate them remains a non-trivial problem, particularly because of the
complex structure of such databases and the complex behaviour of SQL
statements. In this work, we define a direct symbolic execution for database
manipulation code and integrate it with a more traditional symbolic execution
of normal program code. The database tables are represented by relational
symbols and the SQL statements by relational constraints over these symbols and
the symbols representing the normal variables of the program. An algorithm
based on these principles is presented for the symbolic execution of Java
methods that implement business use cases by reading and writing in a
relational database, the latter subject to data integrity constraints. The
algorithm is integrated in a test generation tool and experimented over sample
code. The target language for the constraints produced by the tool is the
SMT-Lib standard and the used solver is Microsoft Z3. The results show that the
proposed approach enables generating meaningful test data, including valid
database content, in reasonable time. In particular, the Z3 solver is shown to
be more scalable than the Alloy solver, used in our previous work, for solving
relational constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05269</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05269</id><created>2015-01-21</created><authors><author><keyname>Louail</keyname><forenames>Thomas</forenames></author><author><keyname>Lenormand</keyname><forenames>Maxime</forenames></author><author><keyname>Picornell</keyname><forenames>Miguel</forenames></author><author><keyname>Cant&#xfa;</keyname><forenames>Oliva Garc&#xed;a</forenames></author><author><keyname>Herranz</keyname><forenames>Ricardo</forenames></author><author><keyname>Frias-Martinez</keyname><forenames>Enrique</forenames></author><author><keyname>Ramasco</keyname><forenames>Jos&#xe9; J.</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author></authors><title>Uncovering the spatial structure of mobility networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 5 figures +Supplementary information</comments><journal-ref>Nature Communications 6, 6007 (2015)</journal-ref><doi>10.1038/ncomms7007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extraction of a clear and simple footprint of the structure of large,
weighted and directed networks is a general problem that has many applications.
An important example is given by origin-destination matrices which contain the
complete information on commuting flows, but are difficult to analyze and
compare. We propose here a versatile method which extracts a coarse-grained
signature of mobility networks, under the form of a $2\times 2$ matrix that
separates the flows into four categories. We apply this method to
origin-destination matrices extracted from mobile phone data recorded in
thirty-one Spanish cities. We show that these cities essentially differ by
their proportion of two types of flows: integrated (between residential and
employment hotspots) and random flows, whose importance increases with city
size. Finally the method allows to determine categories of networks, and in the
mobility case to classify cities according to their commuting structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05272</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05272</id><created>2015-01-21</created><authors><author><keyname>Dlala</keyname><forenames>Imen Ouled</forenames><affiliation>IRISA</affiliation></author><author><keyname>Attiaoui</keyname><forenames>Dorra</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Trolls Identification within an Uncertain Framework</title><categories>cs.AI</categories><comments>International Conference on Tools with Artificial Intelligence -
  ICTAI , Nov 2014, Limassol, Cyprus</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web plays an important role in people's social lives since the emergence
of Web 2.0. It facilitates the interaction between users, gives them the
possibility to freely interact, share and collaborate through social networks,
online communities forums, blogs, wikis and other online collaborative media.
However, an other side of the web is negatively taken such as posting
inflammatory messages. Thus, when dealing with the online communities forums,
the managers seek to always enhance the performance of such platforms. In fact,
to keep the serenity and prohibit the disturbance of the normal atmosphere,
managers always try to novice users against these malicious persons by posting
such message (DO NOT FEED TROLLS). But, this kind of warning is not enough to
reduce this phenomenon. In this context we propose a new approach for detecting
malicious people also called 'Trolls' in order to allow community managers to
take their ability to post online. To be more realistic, our proposal is
defined within an uncertain framework. Based on the assumption consisting on
the trolls' integration in the successful discussion threads, we try to detect
the presence of such malicious users. Indeed, this method is based on a
conflict measure of the belief function theory applied between the different
messages of the thread. In order to show the feasibility and the result of our
approach, we test it in different simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05279</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05279</id><created>2015-01-21</created><authors><author><keyname>Czarnecki</keyname><forenames>Wojciech Marian</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Extreme Entropy Machines: Robust information theoretic classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the existing classification methods are aimed at minimization of
empirical risk (through some simple point-based error measured with loss
function) with added regularization. We propose to approach this problem in a
more information theoretic way by investigating applicability of entropy
measures as a classification model objective function. We focus on quadratic
Renyi's entropy and connected Cauchy-Schwarz Divergence which leads to the
construction of Extreme Entropy Machines (EEM).
  The main contribution of this paper is proposing a model based on the
information theoretic concepts which on the one hand shows new, entropic
perspective on known linear classifiers and on the other leads to a
construction of very robust method competetitive with the state of the art
non-information theoretic ones (including Support Vector Machines and Extreme
Learning Machines).
  Evaluation on numerous problems spanning from small, simple ones from UCI
repository to the large (hundreads of thousands of samples) extremely
unbalanced (up to 100:1 classes' ratios) datasets shows wide applicability of
the EEM in real life problems and that it scales well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05286</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05286</id><created>2015-01-17</created><authors><author><keyname>Mascolo</keyname><forenames>Luigi</forenames></author><author><keyname>Quartulli</keyname><forenames>Marco</forenames></author><author><keyname>Guccione</keyname><forenames>Pietro</forenames></author><author><keyname>Nico</keyname><forenames>Giovanni</forenames></author><author><keyname>Olaizola</keyname><forenames>Igor G.</forenames></author></authors><title>Distributed mining of large scale remote sensing image archives on
  public computing infrastructures</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Earth Observation (EO) mining aims at supporting efficient access and
exploration of petabyte-scale space- and airborne remote sensing archives that
are currently expanding at rates of terabytes per day. A significant challenge
is performing the analysis required by envisaged applications --- like for
instance process mapping for environmental risk management --- in reasonable
time. In this work, we address the problem of content-based image retrieval via
example-based queries from EO data archives. In particular, we focus on the
analysis of polarimetric SAR data, for which target decomposition theorems have
proved fundamental in discovering patterns in data and characterize the ground
scattering properties. To this end, we propose an interactive region-oriented
content-based image mining system in which 1) unsupervised ingestion processes
are distributed onto virtual machines in elastic, on-demand computing
infrastructures 2) archive-scale content hierarchical indexing is implemented
in terms of a &quot;big data&quot; analytics cluster-computing framework 3) query
processing amounts to traversing the generated binary tree index, computing
distances that correspond to descriptor-based similarity measures between image
groups and a query image tile. We describe in depth both the strategies and the
actual implementations for the ingestion and indexing components, and verify
the approach by experiments carried out on the NASA/JPL UAVSAR full
polarimetric data archive. We report the results of the tests performed on
computer clusters by using a public Infrastructure-as-a-Service and evaluating
the impact of cluster configuration on system performance. Results are
promising for data mapping and information retrieval applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05290</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05290</id><created>2015-01-21</created><updated>2015-02-12</updated><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bernardo</forenames></author></authors><title>Managing large-scale scientific hypotheses as uncertain and
  probabilistic data</title><categories>cs.DB cs.AI cs.CE</categories><comments>145 pages, 61 figures, 1 table. PhD thesis, National Laboratory for
  Scientific Computing (LNCC), Brazil, February 2015</comments><acm-class>H.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In view of the paradigm shift that makes science ever more data-driven, in
this thesis we propose a synthesis method for encoding and managing large-scale
deterministic scientific hypotheses as uncertain and probabilistic data.
  In the form of mathematical equations, hypotheses symmetrically relate
aspects of the studied phenomena. For computing predictions, however,
deterministic hypotheses can be abstracted as functions. We build upon Simon's
notion of structural equations in order to efficiently extract the (so-called)
causal ordering between variables, implicit in a hypothesis structure (set of
mathematical equations).
  We show how to process the hypothesis predictive structure effectively
through original algorithms for encoding it into a set of functional
dependencies (fd's) and then performing causal reasoning in terms of acyclic
pseudo-transitive reasoning over fd's. Such reasoning reveals important causal
dependencies implicit in the hypothesis predictive data and guide our synthesis
of a probabilistic database. Like in the field of graphical models in AI, such
a probabilistic database should be normalized so that the uncertainty arisen
from competing hypotheses is decomposed into factors and propagated properly
onto predictive data by recovering its joint probability distribution through a
lossless join. That is motivated as a design-theoretic principle for
data-driven hypothesis management and predictive analytics.
  The method is applicable to both quantitative and qualitative deterministic
hypotheses and demonstrated in realistic use cases from computational science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05296</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05296</id><created>2015-01-20</created><updated>2015-04-24</updated><authors><author><keyname>Arnold</keyname><forenames>Andrew</forenames></author><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author></authors><title>Output-sensitive algorithms for sumset and sparse polynomial
  multiplication</title><categories>cs.SC cs.CC cs.DS</categories><comments>Submitted to ISSAC 2015</comments><msc-class>68W20, 68W30, 68Q17, 12Y05</msc-class><acm-class>F.2.1; F.2.3; G.3; G.4; I.1.2</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present randomized algorithms to compute the sumset (Minkowski sum) of two
integer sets, and to multiply two univariate integer polynomials given by
sparse representations. Our algorithm for sumset has cost softly linear in the
combined size of the inputs and output. This is used as part of our sparse
multiplication algorithm, whose cost is softly linear in the combined size of
the inputs, output, and the sumset of the supports of the inputs. As a
subroutine, we present a new method for computing the coefficients of a sparse
polynomial, given a set containing its support. Our multiplication algorithm
extends to multivariate Laurent polynomials over finite fields and rational
numbers. Our techniques are based on sparse interpolation algorithms and
results from analytic number theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05297</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05297</id><created>2015-01-21</created><authors><author><keyname>Vanga</keyname><forenames>Sandeep</forenames></author><author><keyname>Jaganade</keyname><forenames>Sachin</forenames></author></authors><title>Multi Stage based Time Series Analysis of User Activity on Touch
  Sensitive Surfaces in Highly Noise Susceptible Environments</title><categories>cs.HC stat.AP</categories><comments>9 pages (including 9 figures and 3 tables); International Journal of
  Computer Applications (published)</comments><journal-ref>International Journal of Computer Applications 105(16):23-31,
  November 2014</journal-ref><doi>10.5120/18462-9822 10.5120/18462-9822 10.5120/18462-9822</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a multistage framework for time series analysis of user
activity on touch sensitive surfaces in noisy environments. Here multiple
methods are put together in multi stage framework; including moving average,
moving median, linear regression, kernel density estimation, partial
differential equations and Kalman filter. The proposed three stage filter
consisting of partial differential equation based denoising, Kalman filter and
moving average method provides ~25% better noise reduction than other methods
according to Mean Squared Error (MSE) criterion in highly noise susceptible
environments. Apart from synthetic data, we also obtained real world data like
hand writing, finger/stylus drags etc. on touch screens in the presence of high
noise such as unauthorized charger noise or display noise and validated our
algorithms. Furthermore, the proposed algorithm performs qualitatively better
than the existing solutions for touch panels of the high end hand held devices
available in the consumer electronics market qualitatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05338</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05338</id><created>2015-01-21</created><updated>2015-11-15</updated><authors><author><keyname>Ernst</keyname><forenames>Michael</forenames></author><author><keyname>Macedonio</keyname><forenames>Damiano</forenames></author><author><keyname>Merro</keyname><forenames>Massimo</forenames></author><author><keyname>Spoto</keyname><forenames>Fausto</forenames></author></authors><title>Semantics for Locking Specifications</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To prevent concurrency errors, programmers need to obey a locking discipline.
Annotations that specify that discipline, such as Java's @GuardedBy, are
already widely used. Unfortunately, their semantics is expressed informally and
is consequently ambiguous. This article highlights such ambiguities and
formalizes the semantics of @GuardedBy in two alternative ways, building on an
operational semantics for a small concurrent fragment of a Java-like language.
It also identifies when such annotations are actual guarantees against data
races. Our work aids in understanding the annotations and supports the
development of sound formal tools that verify or infer such annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05352</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05352</id><created>2015-01-21</created><updated>2016-02-04</updated><authors><author><keyname>Raziperchikolaei</keyname><forenames>Ramin</forenames></author><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author></authors><title>Optimizing affinity-based binary hashing using auxiliary coordinates</title><categories>cs.LG cs.CV math.OC stat.ML</categories><comments>22 pages, 12 figures; added new experiments and references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In supervised binary hashing, one wants to learn a function that maps a
high-dimensional feature vector to a vector of binary codes, for application to
fast image retrieval. This typically results in a difficult optimization
problem, nonconvex and nonsmooth, because of the discrete variables involved.
Much work has simply relaxed the problem during training, solving a continuous
optimization, and truncating the codes a posteriori. This gives reasonable
results but is quite suboptimal. Recent work has tried to optimize the
objective directly over the binary codes and achieved better results, but the
hash function was still learned a posteriori, which remains suboptimal. We
propose a general framework for learning hash functions using affinity-based
loss functions that uses auxiliary coordinates. This closes the loop and
optimizes jointly over the hash functions and the binary codes so that they
gradually match each other. The resulting algorithm can be seen as a corrected,
iterated version of the procedure of optimizing first over the codes and then
learning the hash function. Compared to this, our optimization is guaranteed to
obtain better hash functions while being not much slower, as demonstrated
experimentally in various supervised datasets. In addition, our framework
facilitates the design of optimization algorithms for arbitrary types of loss
and hash functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05354</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05354</id><created>2015-01-21</created><authors><author><keyname>Kramer</keyname><forenames>Raphael</forenames></author><author><keyname>Maculan</keyname><forenames>Nelson</forenames></author><author><keyname>Subramanian</keyname><forenames>Anand</forenames></author><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author></authors><title>A speed and departure time optimization algorithm for the
  Pollution-Routing Problem</title><categories>cs.DS</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new speed and departure time optimization algorithm for the
Pollution-Routing Problem (PRP) which runs in quadratic time. This algorithm is
embedded into an iterated local search-based metaheuristic to achieve a
combined speed, scheduling and routing optimization. Extensive computational
experiments are conducted on classic PRP benchmark instances. Allowing delayed
departure times from the depot significantly increases the search space, and
contributes to reduce CO 2 emissions by 8.31% on the considered instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05367</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05367</id><created>2015-01-21</created><authors><author><keyname>Quinn</keyname><forenames>Peter</forenames></author><author><keyname>Axelrod</keyname><forenames>Tim</forenames></author><author><keyname>Bird</keyname><forenames>Ian</forenames></author><author><keyname>Dodson</keyname><forenames>Richard</forenames></author><author><keyname>Szalay</keyname><forenames>Alex</forenames></author><author><keyname>Wicenec</keyname><forenames>Andreas</forenames></author></authors><title>Delivering SKA Science</title><categories>astro-ph.IM cs.DC</categories><comments>27 pages, 14 figures, Conference: Advancing Astrophysics with the
  Square Kilometre Array June 8-13, 2014 Giardini Naxos, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SKA will be capable of producing a stream of science data products that
are Exa-scale in terms of their storage and processing requirements. This
Google-scale enterprise is attracting considerable international interest and
excitement from within the industrial and academic communities. In this chapter
we examine the data flow, storage and processing requirements of a number of
key SKA survey science projects to be executed on the baseline SKA1
configuration. Based on a set of conservative assumptions about trends for HPC
and storage costs, and the data flow process within the SKA Observatory, it is
apparent that survey projects of the scale proposed will potentially drive
construction and operations costs beyond the current anticipated SKA1 budget.
This implies a sharing of the resources and costs to deliver SKA science
between the community and what is contained within the SKA Observatory. A
similar situation was apparent to the designers of the LHC more than 10 years
ago. We propose that it is time for the SKA project and community to consider
the effort and process needed to design and implement a distributed SKA science
data system that leans on the lessons of other projects and looks to recent
developments in Cloud technologies to ensure an affordable, effective and
global achievement of SKA science goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05368</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05368</id><created>2015-01-21</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Ye</keyname><forenames>Junliang</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author></authors><title>Spatial Spectrum and Energy Efficiency of Random Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>appears in IEEE Transactions on Communications, April, 2015</comments><doi>10.1109/TCOMM.2015.2394386</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is a great challenge to evaluate the network performance of cellular
mobile communication systems. In this paper, we propose new spatial spectrum
and energy efficiency models for Poisson-Voronoi tessellation (PVT) random
cellular networks. To evaluate the user access the network, a Markov chain
based wireless channel access model is first proposed for PVT random cellular
networks. On that basis, the outage probability and blocking probability of PVT
random cellular networks are derived, which can be computed numerically.
Furthermore, taking into account the call arrival rate, the path loss exponent
and the base station (BS) density in random cellular networks, spatial spectrum
and energy efficiency models are proposed and analyzed for PVT random cellular
networks. Numerical simulations are conducted to evaluate the network spectrum
and energy efficiency in PVT random cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05371</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05371</id><created>2015-01-21</created><updated>2016-01-12</updated><authors><author><keyname>Jeong</keyname><forenames>Seongah</forenames></author><author><keyname>Khalili</keyname><forenames>Shahrouz</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author></authors><title>Multistatic Cloud Radar Systems: Joint Sensing and Communication Design</title><categories>cs.IT math.IT</categories><comments>13 pages, 8 figures, Transactions on Emerging Telecommunications
  Technologies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multistatic cloud radar system, receive sensors measure signals sent by
a transmit element and reflected from a target and possibly clutter, in the
presence of interference and noise. The receive sensors communicate over
non-ideal backhaul links with a fusion center, or cloud processor, where the
presence or absence of the target is determined. The backhaul architecture can
be characterized either by an orthogonal-access channel or by a non-orthogonal
multiple-access channel. Two backhaul transmission strategies are considered,
namely compress-and-forward (CF), which is well suited for the
orthogonal-access backhaul, and amplify-and-forward (AF), which leverages the
superposition property of the non-orthogonal multiple-access channel. In this
paper, the joint optimization of the sensing and backhaul communication
functions of the cloud radar system is studied. Specifically, the transmitted
waveform is jointly optimized with backhaul quantization in the case of CF
backhaul transmission and with the amplifying gains of the sensors for the AF
backhaul strategy. In both cases, the information-theoretic criterion of the
Bhattacharyya distance is adopted as a metric for the detection performance.
Algorithmic solutions based on successive convex approximation are developed
under different assumptions on the available channel state information (CSI).
Numerical results demonstrate that the proposed schemes outperform conventional
solutions that perform separate optimizations of the waveform and backhaul
operation, as well as the standard distributed detection approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05376</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05376</id><created>2015-01-21</created><authors><author><keyname>Zhu</keyname><forenames>Guangxu</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author></authors><title>Wireless Information and Power Transfer in Relay Systems with Multiple
  Antennas and Interference</title><categories>cs.IT math.IT</categories><comments>accepted, to appear IEEE TCOM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an energy harvesting dual-hop relaying system without/with the
presence of co-channel interference (CCI) is investigated. Specifically, the
energy constrained multi-antenna relay node is powered by either the
information signal of the source or via the signal receiving from both the
source and interferer. In particular, we first study the outage probability and
ergodic capacity of an interference free system, and then extend the analysis
to an interfering environment. To exploit the benefit of multiple antennas,
three different linear processing schemes are investigated, namely, 1) Maximum
ratio combining/maximal ratio transmission (MRC/MRT), 2) Zero-forcing/MRT
(ZF/MRT) and 3) Minimum mean-square error/MRT (MMSE/MRT). For all schemes, both
the systems outage probability and ergodic capacity are studied, and the
achievable diversity order is also presented. In addition, the optimal power
splitting ratio minimizing the outage probability is characterized. Our results
show that the implementation of multiple antennas increases the energy
harvesting capability, hence, significantly improves the systems performance.
Moreover, it is demonstrated that the CCI could be potentially exploited to
substantially boost the performance, while the choice of a linear processing
scheme plays a critical role in determining how much gain could be extracted
from the CCI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05379</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05379</id><created>2015-01-21</created><authors><author><keyname>Chen</keyname><forenames>Kwang-Cheng</forenames></author><author><keyname>Huang</keyname><forenames>Shao-Lun</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Communication Theoretic Data Analytics</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Journal on Selected Areas in Communications, Jan.
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Widespread use of the Internet and social networks invokes the generation of
big data, which is proving to be useful in a number of applications. To deal
with explosively growing amounts of data, data analytics has emerged as a
critical technology related to computing, signal processing, and information
networking. In this paper, a formalism is considered in which data is modeled
as a generalized social network and communication theory and information theory
are thereby extended to data analytics. First, the creation of an equalizer to
optimize information transfer between two data variables is considered, and
financial data is used to demonstrate the advantages. Then, an information
coupling approach based on information geometry is applied for dimensionality
reduction, with a pattern recognition example to illustrate the effectiveness.
These initial trials suggest the potential of communication theoretic data
analytics for a wide range of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05382</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05382</id><created>2015-01-21</created><updated>2015-01-28</updated><authors><author><keyname>Gong</keyname><forenames>Wenjuan</forenames></author><author><keyname>Huang</keyname><forenames>Yongzhen</forenames></author><author><keyname>Gonzalez</keyname><forenames>Jordi</forenames></author><author><keyname>Wang</keyname><forenames>and Liang</forenames></author></authors><title>Enhanced Mixtures of Part Model for Human Pose Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture of parts model has been successfully applied to 2D human pose
estimation problem either as explicitly trained body part model or as latent
variables for the whole human body model. Mixture of parts model usually
utilize tree structure for representing relations between body parts. Tree
structures facilitate training and referencing of the model but could not deal
with double counting problems, which hinder its applications in 3D pose
estimation. While most of work targeted to solve these problems tend to modify
the tree models or the optimization target. We incorporate other cues from
input features. For example, in surveillance environments, human silhouettes
can be extracted relative easily although not flawlessly. In this condition, we
can combine extracted human blobs with histogram of gradient feature, which is
commonly used in mixture of parts model for training body part templates. The
method can be easily extend to other candidate features under our generalized
framework. We show 2D body part detection results on a public available
dataset: HumanEva dataset. Furthermore, a 2D to 3D pose estimator is trained
with Gaussian process regression model and 2D body part detections from the
proposed method is fed to the estimator, thus 3D poses are predictable given
new 2D body part detections. We also show results of 3D pose estimation on
HumanEva dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05385</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05385</id><created>2015-01-21</created><updated>2015-10-21</updated><authors><author><keyname>Pan</keyname><forenames>Victor</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>How Much Randomness Do We Need for Supporting Gaussian Elimination,
  Block Gaussian Elimination, and Low-rank Approximation?</title><categories>cs.SC</categories><comments>36 pages, 12 figures, the paper is in both areas of computer science
  (randomized algorithms) and numerical matrix computations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At first we study low-rank approximation by means of random sampling and
oversampling. We prove that with a probability close to 1 Gaussian random
sampling produces low-rank approximation universally, that is, to any matrix
having small numerical rank r. We define average such matrix M and deduce that
sampling with any multiplier having full numerical rank r+p, for a nonnegative
oversampling integer p, definitely produces approximation of rank r+p of M. In
particular this applies to any element of well-known and new classes of
structured unitary or well-conditioned multipliers, thus explaining their
empirical efficiency and suggesting simple directions to enhancing such
efficiency. E.g., one can apply concurrent or successive sampling until
success, not require universality and detecting unlikely failure at a low
computational cost. We extend readily our results and recipes to randomized
preprocessing of Gaussian elimination with no pivoting (GENP), which is a
subject studied much less, but also of fundamental importance because popular
GEPP is costly in context of modern computer technology. We also advance in the
study of universality of some natural structured preprocessors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05387</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05387</id><created>2015-01-21</created><updated>2016-02-22</updated><authors><author><keyname>Wang</keyname><forenames>Yangzihao</forenames></author><author><keyname>Davidson</keyname><forenames>Andrew</forenames></author><author><keyname>Pan</keyname><forenames>Yuechao</forenames></author><author><keyname>Wu</keyname><forenames>Yuduo</forenames></author><author><keyname>Riffel</keyname><forenames>Andy</forenames></author><author><keyname>Owens</keyname><forenames>John D.</forenames></author></authors><title>Gunrock: A High-Performance Graph Processing Library on the GPU</title><categories>cs.DC</categories><comments>14 pages, accepted by PPoPP'16 (removed the text repetition in the
  previous version v5)</comments><acm-class>D.1.3</acm-class><doi>10.1145/2851141.2851145</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For large-scale graph analytics on the GPU, the irregularity of data access
and control flow, and the complexity of programming GPUs have been two
significant challenges for developing a programmable high-performance graph
library. &quot;Gunrock&quot;, our graph-processing system designed specifically for the
GPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on
operations on a vertex or edge frontier. Gunrock achieves a balance between
performance and expressiveness by coupling high performance GPU computing
primitives and optimization strategies with a high-level programming model that
allows programmers to quickly develop new graph primitives with small code size
and minimal GPU programming knowledge. We evaluate Gunrock on five key graph
primitives and show that Gunrock has on average at least an order of magnitude
speedup over Boost and PowerGraph, comparable performance to the fastest GPU
hardwired primitives, and better performance than any other GPU high-level
graph library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05390</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05390</id><created>2015-01-21</created><updated>2015-08-01</updated><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>Real Polynomial Root-finding by Means of Matrix and Polynomial
  Iterations</title><categories>cs.SC</categories><comments>24 pages 12 tables. arXiv admin note: substantial text overlap with
  arXiv:1404.6817</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Univariate polynomial root-finding is a classical subject, still important
for modern computing. Frequently one seeks just the real roots of a polynomial
with real coefficients. They can be approximated at a low computational cost if
the polynomial has no nonreal roots, but for high degree polynomials, nonreal
roots are typically much more numerous than the real ones. The challenge is
known for a long time, and the subject has been intensively studied.
Nevertheless, we produce some novel ideas and techniques and obtain dramatic
acceleration of the known algorithms. In order to achieve our progress we
exploit the correlation between the computations with matrices and polynomials,
randomized matrix computations, and complex plane geometry, extend the
techniques of the matrix sign iterations, and use the structure of the
companion matrix of the input polynomial. The results of our extensive tests
with benchmark polynomials and random matrices are quite encouraging. In
particular in our tests the number of iterations required for convergence of
our algorithms grew very slowly (if at all) as we increased the degree of the
univariate input polynomials and the dimension of the input matrices from 64 to
1024.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05392</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05392</id><created>2015-01-21</created><updated>2015-08-01</updated><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author><author><keyname>Tsigaridas</keyname><forenames>Elias P.</forenames></author></authors><title>Accelerated Approximation of the Complex Roots and Factors of a
  Univariate Polynomial</title><categories>cs.SC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The known algorithms approximate the roots of a univariate polynomial in
nearly optimal arithmetic and Boolean time. They are, however, quite involved
and require a high precision of computing when the degree of the input
polynomial is large, which causes numerical stability problems. We observe that
these difficulties do not appear at the initial stages of the algorithms, and
in our present paper we extend one of these stages, analyze it, and avoid the
cited problems, still achieving the solution within a nearly optimal complexity
estimates, provided that some mild initial isolation of the roots of the input
polynomial has been ensured. The resulting algorithms promise to be of some
practical value for root-finding and can be extended to the problem of
polynomial factorization, which is of interest on its own right. We conclude
with outlining such an extension, which enables us to cover the cases of
isolated multiple roots and root clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05396</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05396</id><created>2015-01-22</created><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Marcheret</keyname><forenames>Etienne</forenames></author><author><keyname>Goel</keyname><forenames>Vaibhava</forenames></author></authors><title>Deep Multimodal Learning for Audio-Visual Speech Recognition</title><categories>cs.CL cs.LG</categories><comments>ICASSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present methods in deep multimodal learning for fusing
speech and visual modalities for Audio-Visual Automatic Speech Recognition
(AV-ASR). First, we study an approach where uni-modal deep networks are trained
separately and their final hidden layers fused to obtain a joint feature space
in which another deep network is built. While the audio network alone achieves
a phone error rate (PER) of $41\%$ under clean condition on the IBM large
vocabulary audio-visual studio dataset, this fusion model achieves a PER of
$35.83\%$ demonstrating the tremendous value of the visual channel in phone
classification even in audio with high signal to noise ratio. Second, we
present a new deep network architecture that uses a bilinear softmax layer to
account for class specific correlations between modalities. We show that
combining the posteriors from the bilinear networks with those from the fused
model mentioned above results in a further significant phone error rate
reduction, yielding a final PER of $34.03\%$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05405</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05405</id><created>2015-01-22</created><authors><author><keyname>Bujorianu</keyname><forenames>Manuela</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Wisniewski</keyname><forenames>Rafael</forenames><affiliation>Aalborg University</affiliation></author></authors><title>Proceedings 4th Workshop on Hybrid Autonomous Systems</title><categories>cs.SY cs.CC cs.GT cs.MA</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 174, 2015</journal-ref><doi>10.4204/EPTCS.174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest in autonomous systems is increasing both in industry and
academia. Such systems must operate with limited human intervention in a
changing environment and must be able to compensate for significant system
failures without external intervention. The most appropriate models of
autonomous systems can be found in the class of hybrid systems that interact
with their environment. This workshop brings together researchers interested in
all aspects of autonomy and resilience of hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05414</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05414</id><created>2015-01-22</created><authors><author><keyname>Li</keyname><forenames>Weidong</forenames></author><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Xuejie</forenames></author><author><keyname>Cai</keyname><forenames>Xiaobo</forenames></author></authors><title>A Task-Type-Based Algorithm for the Energy-Aware Profit Maximizing
  Scheduling Problem in Heterogeneous Computing Systems</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design an efficient algorithm for the energy-aware profit
maximizing scheduling problem, where the high performance computing system
administrator is to maximize the profit per unit time. The running time of the
proposed algorithm is depending on the number of task types, while the running
time of the previous algorithm is depending on the number of tasks. Moreover,
we prove that the worst-case performance ratio is close to 2, which maybe the
best result. Simulation experiments show that the proposed algorithm is more
accurate than the previous method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05421</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05421</id><created>2015-01-22</created><authors><author><keyname>Chen</keyname><forenames>Long</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author><author><keyname>Xu</keyname><forenames>Hongli</forenames></author><author><keyname>Hu</keyname><forenames>Jie</forenames></author></authors><title>Queueing Analysis for Preemptive Transmission in Underlay Cognitive
  Radio Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many cognitive radio applications, there are multiple types of message
queues. Existing queueing analysis works in underlay CR networks failed to
discuss packets heterogeneity. Therefore high priority packets with impatient
waiting time that have preemptive transmission opportunities over low class are
investigated. We model the system behavior as a M/M/1+GI queue which is
represented by a two dimensional state transition graph. The reneging
probability of high priority packets and the average waiting time in two-class
priority queues is analyzed. Simulation results demonstrate that the average
waiting time of high priority packets decreases with the growing interference
power threshold and the average waiting time of the low priority packet is
proportional to the arrival rate of the high priority packet. This work may lay
the foundation to design efficient MAC protocols and optimize long term system
performance by carefully choosing system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05425</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05425</id><created>2015-01-22</created><authors><author><keyname>Blanchette</keyname><forenames>Jasmin Christian</forenames></author><author><keyname>Popescu</keyname><forenames>Andrei</forenames></author><author><keyname>Traytel</keyname><forenames>Dmitriy</forenames></author></authors><title>Foundational Extensible Corecursion</title><categories>cs.PL</categories><acm-class>F.3.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formalized framework for defining corecursive functions
safely in a total setting, based on corecursion up-to and relational
parametricity. The end product is a general corecursor that allows corecursive
(and even recursive) calls under well-behaved operations, including
constructors. Corecursive functions that are well behaved can be registered as
such, thereby increasing the corecursor's expressiveness. The metatheory is
formalized in the Isabelle proof assistant and forms the core of a prototype
tool. The corecursor is derived from first principles, without requiring new
axioms or extensions of the logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05426</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05426</id><created>2015-01-22</created><authors><author><keyname>Jendoubi</keyname><forenames>Siwar</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Li&#xe9;tard</keyname><forenames>Ludovic</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Classification of Message Spreading in a Heterogeneous Social Network</title><categories>cs.SI cs.AI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>International Conference on Information Processing and Management
  of Uncertainty in Knowledge-Based Systems (IPMU), Jul 2014, Montpellier,
  France. pp.66 - 75</journal-ref><doi>10.1007/978-3-319-08855-6_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, social networks such as Twitter, Facebook and LinkedIn become
increasingly popular. In fact, they introduced new habits, new ways of
communication and they collect every day several information that have
different sources. Most existing research works fo-cus on the analysis of
homogeneous social networks, i.e. we have a single type of node and link in the
network. However, in the real world, social networks offer several types of
nodes and links. Hence, with a view to preserve as much information as
possible, it is important to consider so-cial networks as heterogeneous and
uncertain. The goal of our paper is to classify the social message based on its
spreading in the network and the theory of belief functions. The proposed
classifier interprets the spread of messages on the network, crossed paths and
types of links. We tested our classifier on a real word network that we
collected from Twitter, and our experiments show the performance of our belief
classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05432</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05432</id><created>2015-01-22</created><authors><author><keyname>Wu</keyname><forenames>Xingyu</forenames></author><author><keyname>Mao</keyname><forenames>Xia</forenames></author><author><keyname>Chen</keyname><forenames>Lijiang</forenames></author><author><keyname>Xue</keyname><forenames>Yuli</forenames></author><author><keyname>Compare</keyname><forenames>Angelo</forenames></author></authors><title>Point Context: An Effective Shape Descriptor for RST-invariant
  Trajectory Recognition</title><categories>cs.CV</categories><comments>11 pages, 10 figures</comments><msc-class>51A05</msc-class><acm-class>I.2.10; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion trajectory recognition is important for characterizing the moving
property of an object. The speed and accuracy of trajectory recognition rely on
a compact and discriminative feature representation, and the situations of
varying rotation, scaling and translation has to be specially considered. In
this paper we propose a novel feature extraction method for trajectories.
Firstly a trajectory is represented by a proposed point context, which is a
rotation-scale-translation (RST) invariant shape descriptor with a flexible
tradeoff between computational complexity and discrimination, yet we prove that
it is a complete shape descriptor. Secondly, the shape context is nonlinearly
mapped to a subspace by kernel nonparametric discriminant analysis (KNDA) to
get a compact feature representation, and thus a trajectory is projected to a
single point in a low-dimensional feature space. Experimental results show
that, the proposed trajectory feature shows encouraging improvement than
state-of-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05450</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05450</id><created>2015-01-22</created><authors><author><keyname>Dwivedi</keyname><forenames>Satyam</forenames></author><author><keyname>De Angelis</keyname><forenames>Alessio</forenames></author><author><keyname>Zachariah</keyname><forenames>Dave</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Joint Ranging and Clock Parameter Estimation by Wireless Round Trip Time
  Measurements</title><categories>cs.NI cs.SY</categories><comments>IEEE Journal on Selected Areas in Communications (Accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a new technique for estimating fine clock errors and
range between two nodes simultaneously by two-way time-of-arrival measurements
us- ing impulse-radio ultra-wideband signals. Estimators for clock parameters
and the range are proposed that are robust with respect to outliers. They are
analyzed numerically and by means of experimental measurement campaigns. The
technique and derived estimators achieve accuracies below 1Hz for frequency
estimation, below 1 ns for phase estimation and 20 cm for range estimation, at
4m distance using 100MHz clocks at both nodes. Therefore, we show that the
proposed joint approach is practical and can simultaneously provide clock
synchronization and positioning in an experimental system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05461</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05461</id><created>2015-01-22</created><authors><author><keyname>Krishnan</keyname><forenames>R.</forenames></author><author><keyname>Khanzadi</keyname><forenames>M. R.</forenames></author><author><keyname>Krishnan</keyname><forenames>N.</forenames></author><author><keyname>Wu</keyname><forenames>Y.</forenames></author><author><keyname>Amat</keyname><forenames>A. Graell i</forenames></author><author><keyname>Eriksson</keyname><forenames>T.</forenames></author><author><keyname>Schober</keyname><forenames>R.</forenames></author></authors><title>Linear Massive MIMO Precoders in the Presence of Phase Noise -- A
  Large-Scale Analysis</title><categories>cs.IT math.IT</categories><comments>Under Review IEEE Transactions on Vehicular Technology, Jan 2015. 13
  pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the impact of phase noise on the downlink performance of a
multi-user multiple-input multiple-output system, where the base station (BS)
employs a large number of transmit antennas $M$. We consider a setup where the
BS employs $M_{\mathrm{osc}}$ free-running oscillators, and
$M/M_{\mathrm{osc}}$ antennas are connected to each oscillator. For this
configuration, we analyze the impact of phase noise on the performance of the
regularized zero-forcing (RZF), when $M$ and the number of users $K$ are
asymptotically large, while the ratio $M/K=\beta$ is fixed. We analytically
show that the impact of phase noise on the signal-to-interference-plus-noise
ratio (SINR) can be quantified as an effective reduction in the quality of the
channel state information available at the BS when compared to a system without
phase noise. As a consequence, we observe that as $M_{\mathrm{osc}}$ increases,
the SINR performance of all considered precoders degrades. On the other hand,
the variance of the random phase variations caused by the BS oscillators
reduces with increasing $M_{\mathrm{osc}}$. Through Monte-Carlo simulations, we
verify our analytical results, and compare the performance of the precoders for
different phase noise and channel noise variances. For all considered
precoders, we show that when $\beta$ is small, the performance of the setup
where all BS antennas are connected to a single oscillator is superior to that
of the setup where each BS antenna has its own oscillator. However, the
opposite is true when $\beta$ is large and the signal-to-noise ratio at the
users is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05462</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05462</id><created>2015-01-22</created><updated>2015-04-07</updated><authors><author><keyname>Mingers</keyname><forenames>John</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>A Review of Theory and Practice in Scientometrics</title><categories>cs.DL</categories><comments>accepted for publication in the European Journal of Operational
  Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientometrics is the study of the quantitative aspects of the process of
science as a communication system. It is centrally, but not only, concerned
with the analysis of citations in the academic literature. In recent years it
has come to play a major role in the measurement and evaluation of research
performance. In this review we consider: the historical development of
scientometrics, sources of citation data, citation metrics and the &quot;laws&quot; of
scientometrics, normalisation, journal impact factors and other journal
metrics, visualising and mapping science, evaluation and policy, and future
developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05469</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05469</id><created>2015-01-22</created><authors><author><keyname>Wu</keyname><forenames>Junfeng</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>An Improved Stability Condition for Kalman Filtering with Bounded
  Markovian Packet Losses</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the peak-covariance stability of Kalman filtering
subject to packet losses. The length of consecutive packet losses is governed
by a time-homogeneous finite-state Markov chain. We establish a sufficient
condition for peak-covariance stability and show that this stability check can
be recast as a linear matrix inequality (LMI) feasibility problem. Comparing
with the literature, the stability condition given in this paper is invariant
with respect to similarity state transformations; moreover, our condition is
proved to be less conservative than the existing results. Numerical examples
are provided to demonstrate the effectiveness of our result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05472</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05472</id><created>2015-01-22</created><authors><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Sen</keyname><forenames>Bibhash</forenames></author><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author></authors><title>Handwritten Devanagari Script Segmentation: A non-linear Fuzzy Approach</title><categories>cs.CV</categories><comments>In Proceedings of IEEE Conference on AI Tools and Engineering
  (ICAITE-08), March 6-8, 2008, Pune</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper concentrates on improvement of segmentation accuracy by addressing
some of the key challenges of handwritten Devanagari word image segmentation
technique. In the present work, we have developed a new feature based approach
for identification of Matra pixels from a word image, design of a non-linear
fuzzy membership functions for headline estimation and finally design of a
non-linear fuzzy functions for identifying segmentation points on the Matra.
The segmentation accuracy achieved by the current technique is 94.8%. This
shows an improvement of performance by 1.8% over the previous technique [1] on
a 300-word dataset, used for the current experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05475</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05475</id><created>2015-01-22</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>L&#xe9;v&#xea;que</keyname><forenames>Benjamin</forenames></author></authors><title>Structure of Schnyder labelings on orientable surfaces</title><categories>cs.DM cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple generalization of Schnyder woods from the plane to maps
on orientable surfaces of higher genus. This is done in the language of angle
labelings. Generalizing results of De Fraysseix and Ossona de Mendez, and
Felsner, we establish a correspondence between these labelings and orientations
and characterize the set of orientations of a map that correspond to such a
Schnyder wood. Furthermore, we study the set of these orientations of a given
map and provide a natural partition into distributive lattices depending on the
surface homology. This generalizes earlier results of Felsner and Ossona de
Mendez. In the toroidal case, a new proof for the existence of Schnyder woods
is derived from this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05480</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05480</id><created>2015-01-22</created><updated>2015-07-01</updated><authors><author><keyname>Minguez</keyname><forenames>Roberto</forenames></author><author><keyname>Garcia-Bertrand</keyname><forenames>Raquel</forenames></author></authors><title>Robust Transmission Network Expansion Planning in Energy Systems:
  Improving Computational Performance</title><categories>cs.CE</categories><comments>32 pages and 2 figures in European Journal of Operational Research
  2014</comments><doi>10.1016/j.ejor.2015.06.068</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent advances in solving the problem of transmission network expansion
planning, the use of robust optimization techniques has been put forward, as an
alternative to stochastic mathematical programming methods, to make the problem
tractable in realistic systems. Different sources of uncertainty have been
considered, mainly related to the capacity and availability of generation
facilities and demand, and making use of adaptive robust optimization models.
The mathematical formulations for these models give rise to three-level
mixed-integer optimization problems, which are solved using different
strategies. Although it is true that these robust methods are more efficient
than their stochastic counterparts, it is also correct that solution times for
mixed-integer linear programming problems increase exponentially with respect
to the size of the problem. Because of this, practitioners and system operators
need to use computationally efficient methods when solving this type of
problem. In this paper the issue of improving computational performance by
taking different features from existing algorithms is addressed. In particular,
we replace the lower-level problem with a dual one, and solve the resulting
bi-level problem using a primal cutting plane algorithm within a decomposition
scheme. By using this alternative and simple approach, the computing time for
solving transmission expansion planning problems has been reduced drastically.
Numerical results in an illustrative example, the IEEE-24 and IEEE 118-bus test
systems demonstrate that the algorithm is superior in terms of computational
performance with respect to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05493</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05493</id><created>2015-01-22</created><updated>2015-09-15</updated><authors><author><keyname>Brunsch</keyname><forenames>Tobias</forenames></author><author><keyname>Cornelissen</keyname><forenames>Kamiel</forenames></author><author><keyname>Manthey</keyname><forenames>Bodo</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author><author><keyname>R&#xf6;sner</keyname><forenames>Clemens</forenames></author></authors><title>Smoothed Analysis of the Successive Shortest Path Algorithm</title><categories>cs.DS</categories><comments>A preliminary version has been presented at SODA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum-cost flow problem is a classic problem in combinatorial
optimization with various applications. Several pseudo-polynomial, polynomial,
and strongly polynomial algorithms have been developed in the past decades, and
it seems that both the problem and the algorithms are well understood. However,
some of the algorithms' running times observed in empirical studies contrast
the running times obtained by worst-case analysis not only in the order of
magnitude but also in the ranking when compared to each other. For example, the
Successive Shortest Path (SSP) algorithm, which has an exponential worst-case
running time, seems to outperform the strongly polynomial Minimum-Mean Cycle
Canceling algorithm.
  To explain this discrepancy, we study the SSP algorithm in the framework of
smoothed analysis and establish a bound of $O(mn\phi)$ for the number of
iterations, which implies a smoothed running time of $O(mn\phi (m + n\log n))$,
where $n$ and $m$ denote the number of nodes and edges, respectively, and
$\phi$ is a measure for the amount of random noise. This shows that worst-case
instances for the SSP algorithm are not robust and unlikely to be encountered
in practice. Furthermore, we prove a smoothed lower bound of $\Omega(m \phi
\min\{n, \phi\})$ for the number of iterations of the SSP algorithm, showing
that the upper bound cannot be improved for $\phi = \Omega(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05494</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05494</id><created>2015-01-22</created><authors><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Pramanik</keyname><forenames>Sandip</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Saha</keyname><forenames>Punam Kumar</forenames></author><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>Design of a novel convex hull based feature set for recognition of
  isolated handwritten Roman numerals</title><categories>cs.CV</categories><comments>In proceedings of UB NE ASEE 2009 conference, University of
  Bridgeport, USA. arXiv admin note: substantial text overlap with
  arXiv:1410.0478</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, convex hull based features are used for recognition of
isolated Roman numerals using a Multi Layer Perceptron (MLP) based classifier.
Experiments of convex hull based features for handwritten character recognition
are few in numbers. Convex hull of a pattern and the centroid of the convex
hull both are affine invariant attributes. In this work, 25 features are
extracted based on different bays attributes of the convex hull of the digit
patterns. Then these patterns are divided into four sub-images with respect to
the centroid of the convex hull boundary. From each such sub-image 25 bays
features are also calculated. In all 125 convex hull based features are
extracted for each numeric digit patterns under the current experiment. The
performance of the designed feature set is tested on the standard MNIST data
set, consisting of 60000 training and 10000 test images of handwritten Roman
using an MLP based classifier a maximum success rate of 97.44% is achieved on
the test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05495</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05495</id><created>2015-01-22</created><authors><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Saha</keyname><forenames>Punam Kumar</forenames></author><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author></authors><title>A GA Based approach for selection of local features for recognition of
  handwritten Bangla numerals</title><categories>cs.CV</categories><comments>In proceedings of UB NE ASEE 2009 conference, University of
  Bridgeport, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft computing approaches are mainly designed to address the real world
ill-defined, imprecisely formulated problems, combining different kind of novel
models of computation, such as neural networks, genetic algorithms (GAs.
Handwritten digit recognition is a typical example of one such problem. In the
current work we have developed a two-pass approach where the first pass
classifier performs a coarse classification, based on some global features of
the input pattern by restricting the possibility of classification decisions
within a group of classes, smaller than the number of classes considered
initially. In the second pass, the group specific classifiers concentrate on
the features extracted from the selected local regions, and refine the earlier
decision by combining the local and the global features for selecting the true
class of the input pattern from the group of candidate classes selected in the
first pass. To optimize the selection of local regions a GA based approach has
been developed here. The maximum recognition performance on Bangla digit
samples as achieved on the test set, during the first pass of the two pass
approach is 93.35%. After combining the results of the two stage classifiers,
an overall success rate of 95.25% is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05497</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05497</id><created>2015-01-22</created><authors><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak kumar</forenames></author></authors><title>An Improved Feature Descriptor for Recognition of Handwritten Bangla
  Alphabet</title><categories>cs.CV</categories><comments>In proceedings of ICSIP 2009, pp. 451 to 454, August 2009, Mysore,
  India. arXiv admin note: substantial text overlap with arXiv:1203.0882,
  arXiv:1002.4040, arXiv:1410.0478</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Appropriate feature set for representation of pattern classes is one of the
most important aspects of handwritten character recognition. The effectiveness
of features depends on the discriminating power of the features chosen to
represent patterns of different classes. However, discriminatory features are
not easily measurable. Investigative experimentation is necessary for
identifying discriminatory features. In the present work we have identified a
new variation of feature set which significantly outperforms on handwritten
Bangla alphabet from the previously used feature set. 132 number of features in
all viz. modified shadow features, octant and centroid features, distance based
features, quad tree based longest run features are used here. Using this
feature set the recognition performance increases sharply from the 75.05%
observed in our previous work [7], to 85.40% on 50 character classes with MLP
based classifier on the same dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05499</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05499</id><created>2015-01-22</created><updated>2016-01-26</updated><authors><author><keyname>T&#xfc;retken</keyname><forenames>Engin</forenames></author><author><keyname>Wang</keyname><forenames>Xinchao</forenames></author><author><keyname>Becker</keyname><forenames>Carlos</forenames></author><author><keyname>Haubold</keyname><forenames>Carsten</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Globally Optimal Cell Tracking using Integer Programming</title><categories>cs.CV</categories><comments>Engin T\&quot;uretken and Xinchao Wang contributed equally to this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach to automatically tracking cell populations in
time-lapse images. To account for cell occlusions and overlaps, we introduce a
robust method that generates an over-complete set of competing detection
hypotheses. We then perform detection and tracking simultaneously on these
hypotheses by solving to optimality an integer program with only one type of
flow variables. This eliminates the need for heuristics to handle missed
detections due to occlusions and complex morphology. We demonstrate the
effectiveness of our approach on a range of challenging sequences consisting of
clumped cells and show that it outperforms state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05501</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05501</id><created>2015-01-22</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author></authors><title>Correlation between Channel State and Information Source with Empirical
  Coordination Constraint</title><categories>cs.IT math.IT</categories><comments>Conference IEEE ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation between channel state and source symbol is under investigation
for a joint source-channel coding problem. We investigate simultaneously the
lossless transmission of information and the empirical coordination of channel
inputs with the symbols of source and states. Empirical coordination is
achievable if the sequences of source symbols, channel states, channel inputs
and channel outputs are jointly typical for a target joint probability
distribution. We characterize the joint distributions that are achievable under
lossless decoding constraint. The performance of the coordination is evaluated
by an objective function. For example, we determine the minimal distortion
between symbols of source and channel inputs for lossless decoding. We show
that the correlation source/channel state improves the feasibility of the
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05502</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05502</id><created>2015-01-22</created><authors><author><keyname>Tan</keyname><forenames>Mao</forenames></author><author><keyname>Duan</keyname><forenames>Bin</forenames></author><author><keyname>Su</keyname><forenames>Yongxin</forenames></author><author><keyname>He</keyname><forenames>Feng</forenames></author></authors><title>Optimal hot rolling production scheduling for economic load dispatch
  under time-of-use electricity pricing</title><categories>cs.SY</categories><comments>13 pages,8 figures,5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-of-Use (TOU) electricity pricing provides a new opportunity for
industrial user to reduce their power costs and improve the efficiency of power
energy. Although many methods for Economic Load Dispatch (ELD) under TOU
pricing in continuous process industry have been proposed, there is difficulty
in batch-type process industry because of that the power load units are not
fixed but closely related with production planning and scheduling. In this
paper, for hot rolling, a typical batch-type and energy intensive process in
steel industry, a multi-objective production scheduling optimization model for
ELD is proposed under TOU pricing, in which the objective is to minimize the
power costs on the premise of ensuring the product quality. A NSGA-II based
production scheduling algorithm is proposed to generate Pareto-optimal
solutions, and then the TOPSIS based multi-criteria decision-making is
performed to recommend an optimal solution to facilitate filed operation.
Experimental results on practical production data show that the proposed method
cut down the power costs by creating load units corresponding to electricity
price and shifting loads to avoid on-peak time periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05508</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05508</id><created>2015-01-22</created><authors><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author></authors><title>High performance computing aspects of a dimension independent
  semi-Lagrangian discontinuous Galerkin code</title><categories>physics.comp-ph cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently developed semi-Lagrangian discontinuous Galerkin approach is
used to discretize hyperbolic partial differential equations (usually first
order equations). Since these methods are conservative, local in space, and
able to limit numerical diffusion, they are considered a promising alternative
to more traditional semi-Lagrangian schemes (which are usually based on
polynomial or spline interpolation).
  In this paper, we consider a parallel implementation of a semi-Lagrangian
discontinuous Galerkin method for distributed memory systems (so-called
clusters). Both strong and weak scaling studies are performed on the Vienna
Scientific Cluster 2 (VSC-2). In the case of weak scaling, up to 8192 cores, we
observe a parallel efficiency above 0.89 for both two and four dimensional
problems. Strong scaling results show good scalability to at least 1024 cores
(we consider problems that can be run on a single processor in reasonable
time). In addition, we study the scaling of a two dimensional Vlasov--Poisson
solver that is implemented using the framework provided. All of the simulation
are conducted in the context of worst case communication overhead; i.e., in a
setting where the CFL number increases linearly with the problem size.
  The framework introduced in this paper facilitates a dimension independent
implementation (based on C++ templates) of scientific codes using both an MPI
and a hybrid approach to parallelization. We describe the essential ingredients
of our implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05519</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05519</id><created>2015-01-22</created><updated>2015-11-04</updated><authors><author><keyname>Himpe</keyname><forenames>Christian</forenames></author><author><keyname>Ohlberger</keyname><forenames>Mario</forenames></author></authors><title>A Note on the Cross Gramian for Non-Symmetric Systems</title><categories>math.OC cs.SY</categories><msc-class>93Bxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cross gramian matrix is a tool for model reduction and system
identification, but it is only computable for square control systems. For
symmetric systems the cross gramian possesses a useful relation to the system's
associated Hankel singular values. Yet, many real-life models are neither
square nor symmetric. In this work, concepts from decentralized control are
used to approximate a cross gramian for non-symmetric and non-square systems.
To illustrate this new non-symmetric cross gramian, it is applied in the
context of model order reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05528</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05528</id><created>2015-01-22</created><updated>2015-07-08</updated><authors><author><keyname>B&#xfc;rgisser</keyname><forenames>Peter</forenames></author><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author><author><keyname>H&#xfc;ttenhain</keyname><forenames>Jesko</forenames></author></authors><title>Permanent versus determinant: not via saturations</title><categories>cs.CC math.RT</categories><comments>12 pages; shortened title, corrected error in proof, added bound on
  stretching factor, provided explicit examples</comments><msc-class>68Q17, 14L24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let Det_n denote the closure of the GL_{n^2}(C)-orbit of the determinant
polynomial det_n with respect to linear substitution. The highest weights
(partitions) of irreducible GL_{n^2}(C)-representations occurring in the
coordinate ring of Det_n form a finitely generated monoid S(Det_n). We prove
that the saturation of S(Det_n) contains all partitions lambda with length at
most n and size divisible by n. This implies that representation theoretic
obstructions for the permanent versus determinant problem must be holes of the
monoid S(Det_n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05530</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05530</id><created>2015-01-22</created><authors><author><keyname>Jendoubi</keyname><forenames>Siwar</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author></authors><title>Belief Hidden Markov Model for speech recognition</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Modeling, Simulation and Applied
  Optimization (ICMSAO), Apr 2013, Hammamet, Tunisia. pp.1 - 6</journal-ref><doi>10.1109/ICMSAO.2013.6552563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech Recognition searches to predict the spoken words automatically. These
systems are known to be very expensive because of using several pre-recorded
hours of speech. Hence, building a model that minimizes the cost of the
recognizer will be very interesting. In this paper, we present a new approach
for recognizing speech based on belief HMMs instead of proba-bilistic HMMs.
Experiments shows that our belief recognizer is insensitive to the lack of the
data and it can be trained using only one exemplary of each acoustic unit and
it gives a good recognition rates. Consequently, using the belief HMM
recognizer can greatly minimize the cost of these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05542</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05542</id><created>2015-01-22</created><authors><author><keyname>Mespotine</keyname><forenames>Meo</forenames></author></authors><title>Mespotine-RLE-basic v0.9 - An overhead-reduced and improved
  Run-Length-Encoding Method</title><categories>cs.DS cs.IT math.IT</categories><comments>16 pages and algorithm-flowcharts</comments><msc-class>94A08, 94A24, 68P30</msc-class><acm-class>E.4; I.4.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Run Length Encoding(RLE) is one of the oldest algorithms for data-compression
available, a method used for compression of large data into smaller and
therefore more compact data. It compresses by looking at the data for
repetitions of the same character in a row and storing the amount(called run)
and the respective character(called run_value) as target-data. Unfortunately it
only compresses within strict and special cases. Outside of these cases, it
increases the data-size, even doubles the size in worst cases compared to the
original, unprocessed data. In this paper, we will discuss modifications to
RLE, with which we will only store the run for characters, that are actually
compressible, getting rid of a lot of useless data like the runs of the
characters, that are uncompressible in the first place. This will be achieved
by storing the character first and the run second. Additionally we create a
bit-list of 256 positions(one for every possible ASCII-character), in which we
will store, if a specific (ASCII-)character is compressible(1) or not(0). Using
this list, we can now say, if a character is compressible (store [the
character]+[it's run]) or if it is not compressible (store [the character] only
and the next character is NOT a run, but the following character instead).
Using this list, we can also successfully decode the data(if the character is
compressible, the next character is a run, if not compressible, the next
character is a normal character). With that, we store runs only for characters,
that are compressible in the first place. In fact, in the worst case scenario,
the encoded data will create always just an overhead of the size of the
bit-list itself. With an alphabet of 256 different characters(i.e. ASCII) it
would be only a maximum of 32 bytes, no matter how big the original data was.
[...]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05546</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05546</id><created>2015-01-22</created><authors><author><keyname>Conard</keyname><forenames>Ashley Mae</forenames></author><author><keyname>Dodson</keyname><forenames>Stephanie</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Ricke</keyname><forenames>Darrell</forenames></author></authors><title>Using a Big Data Database to Identify Pathogens in Protein Data Space</title><categories>cs.DB q-bio.QM</categories><comments>2 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current metagenomic analysis algorithms require significant computing
resources, can report excessive false positives (type I errors), may miss
organisms (type II errors / false negatives), or scale poorly on large
datasets. This paper explores using big data database technologies to
characterize very large metagenomic DNA sequences in protein space, with the
ultimate goal of rapid pathogen identification in patient samples. Our approach
uses the abilities of a big data databases to hold large sparse associative
array representations of genetic data to extract statistical patterns about the
data that can be used in a variety of ways to improve identification
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05547</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05547</id><created>2015-01-22</created><updated>2015-01-23</updated><authors><author><keyname>Cechlarova</keyname><forenames>Katarina</forenames></author><author><keyname>Fleiner</keyname><forenames>Tamas</forenames></author><author><keyname>Manlove</keyname><forenames>David F.</forenames></author><author><keyname>McBride</keyname><forenames>Iain</forenames></author></authors><title>Stable matchings of teachers to schools</title><categories>cs.DS</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several countries successfully use centralized matching schemes for school or
higher education assignment, or for entry-level labour markets. In this paper
we explore the computational aspects of a possible similar scheme for assigning
teachers to schools. Our model is motivated by a particular characteristic of
the education system in many countries where each teacher specializes in two
subjects. We seek stable matchings, which ensure that no teacher and school
have the incentive to deviate from their assignments. Indeed we propose two
stability definitions depending on the precise format of schools' preferences.
If the schools' ranking of applicants is independent of their subjects of
specialism, we show that the problem of deciding whether a stable matching
exists is NP-complete, even if there are only three subjects, unless there are
master lists of applicants or of schools. By contrast, if the schools may order
applicants differently in each of their specialization subjects, the problem of
deciding whether a stable matching exists is NP-complete even in the presence
of subject-specific master lists plus a master list of schools. Finally, we
prove a strong inapproximability result for the problem of finding a matching
with the minimum number of blocking pairs with respect to both stability
definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05552</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05552</id><created>2015-01-22</created><authors><author><keyname>Halimi</keyname><forenames>A.</forenames></author><author><keyname>Honeine</keyname><forenames>P.</forenames></author><author><keyname>Kharouf</keyname><forenames>M.</forenames></author><author><keyname>Richard</keyname><forenames>C.</forenames></author><author><keyname>Tourneret</keyname><forenames>J. -Y.</forenames></author></authors><title>Estimating the Intrinsic Dimension of Hyperspectral Images Using an
  Eigen-Gap Approach</title><categories>stat.AP cs.CV</categories><comments>21 pages, 4 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear mixture models are commonly used to represent hyperspectral datacube
as a linear combinations of endmember spectra. However, determining of the
number of endmembers for images embedded in noise is a crucial task. This paper
proposes a fully automatic approach for estimating the number of endmembers in
hyperspectral images. The estimation is based on recent results of random
matrix theory related to the so-called spiked population model. More precisely,
we study the gap between successive eigenvalues of the sample covariance matrix
constructed from high dimensional noisy samples. The resulting estimation
strategy is unsupervised and robust to correlated noise. This strategy is
validated on both synthetic and real images. The experimental results are very
promising and show the accuracy of this algorithm with respect to
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05561</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05561</id><created>2015-01-22</created><updated>2015-06-26</updated><authors><author><keyname>Forejt</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author></authors><title>On Frequency LTL in Probabilistic Systems</title><categories>cs.LO</categories><comments>A paper presented at CONCUR 2015, with appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study frequency linear-time temporal logic (fLTL) which extends the
linear-time temporal logic (LTL) with a path operator $G^p$ expressing that on
a path, certain formula holds with at least a given frequency p, thus relaxing
the semantics of the usual G operator of LTL. Such logic is particularly useful
in probabilistic systems, where some undesirable events such as random failures
may occur and are acceptable if they are rare enough.
  Frequency-related extensions of LTL have been previously studied by several
authors, where mostly the logic is equipped with an extended &quot;until&quot; and
&quot;globally&quot; operator, leading to undecidability of most interesting problems.
  For the variant we study, we are able to establish fundamental decidability
results. We show that for Markov chains, the problem of computing the
probability with which a given fLTL formula holds has the same complexity as
the analogous problem for LTL. We also show that for Markov decision processes
the problem becomes more delicate, but when restricting the frequency bound $p$
to be 1 and negations not to be outside any $G^p$ operator, we can compute the
maximum probability of satisfying the fLTL formula. This can be again performed
with the same time complexity as for the ordinary LTL formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05576</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05576</id><created>2015-01-22</created><authors><author><keyname>Amasha</keyname><forenames>Mohammed</forenames></author><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author></authors><title>The effect of using facebook markup language (fbml) for designing an
  e-learning model in higher education</title><categories>cs.CY</categories><comments>Mohammed Amasha, Salem Alkhalaf, &quot;The Effect of using Facebook Markup
  Language (FBML) for Designing an E-Learning Model in Higher Education&quot;.
  International Journal of Research in Computer Science, 4 (5): pp. 1-9,
  January 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines the use of Facebook Markup Language (FBML) to design an
e-learning model to facilitate teaching and learning in an academic setting.
The qualitative research study presents a case study on how, Facebook is used
to support collaborative activities in higher education. We used FBML to design
an e-learning model called processes for e-learning resources in the Specialist
Learning Resources Diploma (SLRD) program. Two groups drawn from the SLRD
program were used; First were the participants in the treatment group and
second in the control group. Statistical analysis in the form of a t-test was
used to compare the dependent variables between the two groups. The findings
show a difference in the mean score between the pre-test and the post-test for
the treatment group (achievement, the skill, trends). Our findings suggest that
the use of FBML can support collaborative knowledge creation and improved the
academic achievement of participatns. The findings are expected to provide
insights into promoting the use of Facebook in a learning management system
(LMS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05578</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05578</id><created>2015-01-22</created><authors><author><keyname>Amasha</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author></authors><title>A Model of an E-Learning Web Site for Teaching and Evaluating Online</title><categories>cs.CY</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), 4(12), 2013</journal-ref><doi>10.14569/IJACSA.2013.041222#</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research is endeavoring to design an e-learning web site on the internet
having the course name as &quot;Object Oriented Programming&quot; (OOP) for the students
of level four at Computer Science Department (CSD). This course is to be taught
online (through web) and then a programme is to be designed to evaluate
students performance electronically while introducing a comparison between
online teaching , e-evaluation and traditional methods of evaluation. The
research seeks to lay out a futuristic perception that how the future online
teaching and e-electronic evaluation should be the matter which highlights the
importance of this research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05579</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05579</id><created>2015-01-22</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Myasnikov</keyname><forenames>Alexei G.</forenames></author><author><keyname>Wei&#xdf;</keyname><forenames>Armin</forenames></author></authors><title>Amenability of Schreier graphs and strongly generic algorithms for the
  conjugacy problem</title><categories>math.GR cs.DM cs.DS</categories><msc-class>20F65, 05C81, 20E06</msc-class><acm-class>F.2.2; G.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various occasions the conjugacy problem in finitely generated amalgamated
products and HNN extensions can be decided efficiently for elements which
cannot be conjugated into the base groups. This observation asks for a bound on
how many such elements there are. Such bounds can be derived using the theory
of amenable graphs:
  In this work we examine Schreier graphs of amalgamated products and HNN
extensions. For an amalgamated product $G = H *_A K $ with $[H:A] \geq [K:A]
\geq 2$, the Schreier graph with respect to $H$ or $K$ turns out to be
non-amenable if and only if $[H:A] \geq 3$. Moreover, for an HNN extension of
the form $G = &lt;H,b | bab^{-1}=\phi(a), a \in A &gt;$, we show that the Schreier
graph of $G$ with respect to the subgroup $H$ is non-amenable if and only if $A
\neq H \neq \phi(A)$.
  As application of these characterizations we show that under certain
conditions the conjugacy problem in fundamental groups of finite graphs of
groups with free abelian vertex groups can be solved in polynomial time on a
strongly generic set. Furthermore, the conjugacy problem in groups with more
than one end can be solved with a strongly generic algorithm which has
essentially the same time complexity as the word problem. These are rather
striking results as the word problem might be easy, but the conjugacy problem
might be even undecidable. Finally, our results yield another proof that the
set where the conjugacy problem of the Baumslag group is decidable in
polynomial time is also strongly generic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05580</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05580</id><created>2015-01-22</created><authors><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Wang</keyname><forenames>Chang-Jen</forenames></author><author><keyname>Wu</keyname><forenames>Gang</forenames></author></authors><title>Joint Channel-and-Data Estimation for Large-MIMO Systems with
  Low-Precision ADCs</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of low precision (e.g., 1-3 bits) analog-to-digital convenors (ADCs)
in very large multiple-input multiple-output (MIMO) systems is a technique to
reduce cost and power consumption. In this context, nevertheless, it has been
shown that the training duration is required to be {\em very large} just to
obtain an acceptable channel state information (CSI) at the receiver. A
possible solution to the quantized MIMO systems is joint channel-and-data (JCD)
estimation. This paper first develops an analytical framework for studying the
quantized MIMO system using JCD estimation. In particular, we use the
Bayes-optimal inference for the JCD estimation and realize this estimator
utilizing a recent technique based on approximate message passing. Large-system
analysis based on the replica method is then adopted to derive the asymptotic
performances of the JCD estimator. Results from simulations confirm our
theoretical findings and reveal that the JCD estimator can provide a
significant gain over conventional pilot-only schemes in the quantized MIMO
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05581</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05581</id><created>2015-01-22</created><authors><author><keyname>Riboni</keyname><forenames>Daniele</forenames></author><author><keyname>Bettini</keyname><forenames>Claudio</forenames></author><author><keyname>Civitarese</keyname><forenames>Gabriele</forenames></author><author><keyname>Janjua</keyname><forenames>Zaffar Haider</forenames></author><author><keyname>Helaoui</keyname><forenames>Rim</forenames></author></authors><title>Extended Report: Fine-grained Recognition of Abnormal Behaviors for
  Early Detection of Mild Cognitive Impairment</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the World Health Organization, the rate of people aged 60 or
more is growing faster than any other age group in almost every country, and
this trend is not going to change in a near future. Since senior citizens are
at high risk of non communicable diseases requiring long-term care, this trend
will challenge the sustainability of the entire health system. Pervasive
computing can provide innovative methods and tools for early detecting the
onset of health issues. In this paper we propose a novel method to detect
abnormal behaviors of elderly people living at home. The method relies on
medical models, provided by cognitive neuroscience researchers, describing
abnormal activity routines that may indicate the onset of early symptoms of
mild cognitive impairment. A non-intrusive sensor-based infrastructure acquires
low-level data about the interaction of the individual with home appliances and
furniture, as well as data from environmental sensors. Based on those data, a
novel hybrid statistical-symbolical technique is used to detect the abnormal
behaviors of the patient, which are communicated to the medical center.
Differently from related works, our method can detect abnormal behaviors at a
fine-grained level, thus providing an important tool to support the medical
diagnosis. In order to evaluate our method we have developed a prototype of the
system and acquired a large dataset of abnormal behaviors carried out in an
instrumented smart home. Experimental results show that our technique is able
to detect most anomalies while generating a small number of false positives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05582</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05582</id><created>2015-01-22</created><updated>2015-06-15</updated><authors><author><keyname>Tavakoli</keyname><forenames>Armin</forenames></author><author><keyname>Herbauts</keyname><forenames>Isabelle</forenames></author><author><keyname>Zukowski</keyname><forenames>Marek</forenames></author><author><keyname>Bourennane</keyname><forenames>Mohamed</forenames></author></authors><title>Secret Sharing with a Single d-level Quantum System</title><categories>quant-ph cs.CR</categories><journal-ref>Phys. Rev. A 92, 030302(R) (2015)</journal-ref><doi>10.1103/PhysRevA.92.030302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an example of a wide class of problems for which quantum information
protocols based on multi-system entanglement can be mapped into much simpler
ones involving one system. Secret sharing is a cryptographic primitive which
plays a central role in various secure multiparty computation tasks and
management of keys in cryptography. In secret sharing protocols, a classical
message is divided into shares given to recipient parties in such a way that
some number of parties need to collaborate in order to reconstruct the message.
Quantum protocols for the task commonly rely on multi-partite GHZ entanglement.
We present a multiparty secret sharing protocol which requires only sequential
communication of a single quantum d-level system (for any prime d). It has huge
advantages in scalabilility and can be realized with the state of the art
technology. n be realized with the state of the art technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05588</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05588</id><created>2015-01-22</created><updated>2015-05-29</updated><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames><affiliation>University of Trieste</affiliation></author><author><keyname>Sanguinetti</keyname><forenames>Guido</forenames><affiliation>University of Edinmburgh</affiliation></author></authors><title>Learning and Designing Stochastic Processes from Logical Constraints</title><categories>cs.SY</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:3) 2015</journal-ref><doi>10.2168/LMCS-11(2:3)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic processes offer a flexible mathematical formalism to model and
reason about systems. Most analysis tools, however, start from the premises
that models are fully specified, so that any parameters controlling the
system's dynamics must be known exactly. As this is seldom the case, many
methods have been devised over the last decade to infer (learn) such parameters
from observations of the state of the system. In this paper, we depart from
this approach by assuming that our observations are {\it qualitative}
properties encoded as satisfaction of linear temporal logic formulae, as
opposed to quantitative observations of the state of the system. An important
feature of this approach is that it unifies naturally the system identification
and the system design problems, where the properties, instead of observations,
represent requirements to be satisfied. We develop a principled statistical
estimation procedure based on maximising the likelihood of the system's
parameters, using recent ideas from statistical machine learning. We
demonstrate the efficacy and broad applicability of our method on a range of
simple but non-trivial examples, including rumour spreading in social networks
and hybrid models of gene regulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05590</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05590</id><created>2015-01-22</created><authors><author><keyname>Traganitis</keyname><forenames>Panagiotis A.</forenames></author><author><keyname>Slavakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Sketch and Validate for Big Data Clustering</title><categories>stat.ML cs.LG</categories><comments>The present paper will appear on Signal Processing for Big Data
  special issue (June 2015) of the IEEE Journal of Selected Topics in Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In response to the need for learning tools tuned to big data analytics, the
present paper introduces a framework for efficient clustering of huge sets of
(possibly high-dimensional) data. Building on random sampling and consensus
(RANSAC) ideas pursued earlier in a different (computer vision) context for
robust regression, a suite of novel dimensionality and set-reduction algorithms
is developed. The advocated sketch-and-validate (SkeVa) family includes two
algorithms that rely on K-means clustering per iteration on reduced number of
dimensions and/or feature vectors: The first operates in a batch fashion, while
the second sequential one offers computational efficiency and suitability with
streaming modes of operation. For clustering even nonlinearly separable
vectors, the SkeVa family offers also a member based on user-selected kernel
functions. Further trading off performance for reduced complexity, a fourth
member of the SkeVa family is based on a divergence criterion for selecting
proper minimal subsets of feature variables and vectors, thus bypassing the
need for K-means clustering per iteration. Extensive numerical tests on
synthetic and real data sets highlight the potential of the proposed
algorithms, and demonstrate their competitive performance relative to
state-of-the-art random projection alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05595</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05595</id><created>2015-01-22</created><authors><author><keyname>Steiner</keyname><forenames>Fabian</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author></authors><title>Protograph-Based LDPC Code Design for Bit-Metric Decoding</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A protograph-based low-density parity-check (LDPC) code design technique for
bandwidth-efficient coded modulation is presented. The approach jointly
optimizes the LDPC code node degrees and the mapping of the coded bits to the
bit-interleaved coded modulation (BICM) bit-channels. For BICM with uniform
input and for BICM with probabilistic shaping, binary-input symmetric-output
surrogate channels are constructed and used for code design. The constructed
codes perform as good as multi-edge type codes of Zhang and Kschischang (2013).
For 64-ASK with probabilistic shaping, a blocklength 64800 code is constructed
that operates within 0.69 dB of 0.5log(1+SNR) at a spectral efficiency of 4.2
bits/channel use and a frame error rate of 1e-3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05606</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05606</id><created>2014-11-30</created><authors><author><keyname>Smith</keyname><forenames>Reginald D.</forenames></author></authors><title>A rapid algorithm to calculate joint probability matrices for joint
  entropies of arbitrary order</title><categories>cs.IT math.IT</categories><comments>4 pages 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is no closed form analytical equation or quick method to calculate
probabilities based only on the entropy of a signal or process. Except in the
cases where there are constraints on the state probabilities, one must
typically derive the underlying probabilities through search algorithms. These
become more computationally expensive as entropies of higher orders are
investigated. In this paper, a method to calculate a joint probability matrix
based on the entropy for any order is elaborated. With this method, only first
order entropies need to be successfully calculated while the others are derived
via multiplicative cascades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05611</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05611</id><created>2015-01-22</created><updated>2015-02-26</updated><authors><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Wagener</keyname><forenames>Nolan</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Learning Contact-Rich Manipulation Skills with Guided Policy Search</title><categories>cs.RO</categories><journal-ref>S. Levine, N. Wagener, P. Abbeel, &quot;Learning Contact-Rich
  Manipulation Skills with Guided Policy Search,&quot; in International Conference
  on Robotics and Automation (ICRA), 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous learning of object manipulation skills can enable robots to
acquire rich behavioral repertoires that scale to the variety of objects found
in the real world. However, current motion skill learning methods typically
restrict the behavior to a compact, low-dimensional representation, limiting
its expressiveness and generality. In this paper, we extend a recently
developed policy search method \cite{la-lnnpg-14} and use it to learn a range
of dynamic manipulation behaviors with highly general policy representations,
without using known models or example demonstrations. Our approach learns a set
of trajectories for the desired motion skill by using iteratively refitted
time-varying linear models, and then unifies these trajectories into a single
control policy that can generalize to new situations. To enable this method to
run on a real robot, we introduce several improvements that reduce the sample
count and automate parameter selection. We show that our method can acquire
fast, fluent behaviors after only minutes of interaction time, and can learn
robust controllers for complex tasks, including putting together a toy
airplane, stacking tight-fitting lego blocks, placing wooden rings onto
tight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle caps
onto bottles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05612</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05612</id><created>2015-01-22</created><authors><author><keyname>Fiche</keyname><forenames>Anthony</forenames><affiliation>IRISA</affiliation></author><author><keyname>Cexus</keyname><forenames>Jean-Christophe</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Khenchaf</keyname><forenames>Ali</forenames></author></authors><title>Features modeling with an $\alpha$-stable distribution: Application to
  pattern recognition based on continuous belief functions</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Information Fusion, Elsevier, 2013, 14, pp.504 - 520</journal-ref><doi>10.1016/j.inffus.2013.02.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to show the interest in fitting features with an
$\alpha$-stable distribution to classify imperfect data. The supervised pattern
recognition is thus based on the theory of continuous belief functions, which
is a way to consider imprecision and uncertainty of data. The distributions of
features are supposed to be unimodal and estimated by a single Gaussian and
$\alpha$-stable model. Experimental results are first obtained from synthetic
data by combining two features of one dimension and by considering a vector of
two features. Mass functions are calculated from plausibility functions by
using the generalized Bayes theorem. The same study is applied to the automatic
classification of three types of sea floor (rock, silt and sand) with features
acquired by a mono-beam echo-sounder. We evaluate the quality of the
$\alpha$-stable model and the Gaussian model by analyzing qualitative results,
using a Kolmogorov-Smirnov test (K-S test), and quantitative results with
classification rates. The performances of the belief classifier are compared
with a Bayesian approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05613</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05613</id><created>2015-01-22</created><authors><author><keyname>Park</keyname><forenames>Jungyeul</forenames><affiliation>IRISA</affiliation></author><author><keyname>Chebbah</keyname><forenames>Mouna</forenames><affiliation>IRISA</affiliation></author><author><keyname>Jendoubi</keyname><forenames>Siwar</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author></authors><title>Second-Order Belief Hidden Markov Models</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Belief 2014, Sep 2014, Oxford, United Kingdom. pp.284 - 293</journal-ref><doi>10.1007/978-3-319-11191-9_31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov Models (HMMs) are learning methods for pattern recognition. The
probabilistic HMMs have been one of the most used techniques based on the
Bayesian model. First-order probabilistic HMMs were adapted to the theory of
belief functions such that Bayesian probabilities were replaced with mass
functions. In this paper, we present a second-order Hidden Markov Model using
belief functions. Previous works in belief HMMs have been focused on the
first-order HMMs. We extend them to the second-order model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05614</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05614</id><created>2015-01-22</created><authors><author><keyname>Kharoune</keyname><forenames>Mouloud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author></authors><title>Int{\'e}gration d'une mesure d'ind{\'e}pendance pour la fusion
  d'informations</title><categories>cs.AI</categories><comments>in French, appears in Atelier Fouille de donn{\'e}es complexes,
  Extraction et Gestion des Connaissances (EGC), Jan 2013, Toulouse, France.
  arXiv admin note: substantial text overlap with arXiv:1501.04786</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many information sources are considered into data fusion in order to improve
the decision in terms of uncertainty and imprecision. For each technique used
for data fusion, the asumption on independance is usually made. We propose in
this article an approach to take into acount an independance measure befor to
make the combination of information in the context of the theory of belief
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05617</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05617</id><created>2015-01-22</created><authors><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Mhiri</keyname><forenames>Mohamed</forenames></author></authors><title>Unsupervised image segmentation by Global and local Criteria
  Optimization Based on Bayesian Networks</title><categories>cs.CV</categories><comments>appears in International journal of robotics and imaging; volume 15,
  issue 1, januray 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today Bayesian networks are more used in many areas of decision support and
image processing. In this way, our proposed approach uses Bayesian Network to
modelize the segmented image quality. This quality is calculated on a set of
attributes that represent local evaluation measures. The idea is to have these
local levels chosen in a way to be intersected into them to keep the overall
appearance of segmentation. The approach operates in two phases: the first
phase is to make an over-segmentation which gives superpixels card. In the
second phase, we model the superpixels by a Bayesian Network. To find the
segmented image with the best overall quality we used two approximate inference
methods, the first using ICM algorithm which is widely used in Markov Models
and a second is a recursive method called algorithm of model decomposition
based on max-product algorithm which is very popular in the recent works of
image segmentation. For our model, we have shown that the composition of these
two algorithms leads to good segmentation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05623</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05623</id><created>2015-01-22</created><authors><author><keyname>Brutz</keyname><forenames>Michael</forenames></author><author><keyname>Meyer</keyname><forenames>Francois G.</forenames></author></authors><title>A Modular Multiscale Approach to Overlapping Community Detection</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of detecting overlapping communities in
social networks. Because the word &quot;community&quot; is an ambiguous term, it is
necessary to quantify what it means to be a community within the context of a
particular type of problem. Our interpretation is that this quantification must
be done at a minimum of three scales. These scales are at the level of:
individual nodes, individual communities, and the network as a whole. Each of
these scales involves quantitative features of community structure that are not
accurately represented at the other scales, but are important for defining a
particular notion of community. Our work focuses on providing sensible ways to
quantify what is desired at each of these scales for a notion of community
applicable to social networks, and using these models to develop a community
detection algorithm. Appealing features of our approach is that it naturally
allows for nodes to belong to multiple communities, and is computationally
efficient for large networks with low overall edge density. The scaling of the
algorithm is $O(N~\overline{k^2} + \overline{N_{com}^2})$, where $N$ is the
number of nodes in the network, $\overline{N_{com}^2}$ is the average squared
community size, and $\overline{k^2}$ is the expected value of a node's degree
squared. Although our work focuses on developing a computationally efficient
algorithm for overlapping community detection in the context of social
networks, our primary contribution is developing a methodology that is highly
modular and can easily be adapted to target specific notions of community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05624</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05624</id><created>2015-01-22</created><authors><author><keyname>Gultekin</keyname><forenames>San</forenames></author><author><keyname>Paisley</keyname><forenames>John</forenames></author></authors><title>A Collaborative Kalman Filter for Time-Evolving Dyadic Processes</title><categories>stat.ML cs.LG</categories><comments>Appeared at 2014 IEEE International Conference on Data Mining (ICDM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the collaborative Kalman filter (CKF), a dynamic model for
collaborative filtering and related factorization models. Using the matrix
factorization approach to collaborative filtering, the CKF accounts for time
evolution by modeling each low-dimensional latent embedding as a
multidimensional Brownian motion. Each observation is a random variable whose
distribution is parameterized by the dot product of the relevant Brownian
motions at that moment in time. This is naturally interpreted as a Kalman
filter with multiple interacting state space vectors. We also present a method
for learning a dynamically evolving drift parameter for each location by
modeling it as a geometric Brownian motion. We handle posterior intractability
via a mean-field variational approximation, which also preserves tractability
for downstream calculations in a manner similar to the Kalman filter. We
evaluate the model on several large datasets, providing quantitative evaluation
on the 10 million Movielens and 100 million Netflix datasets and qualitative
evaluation on a set of 39 million stock returns divided across roughly 6,500
companies from the years 1962-2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05628</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05628</id><created>2015-01-22</created><authors><author><keyname>Uyan&#x131;k</keyname><forenames>&#x130;smail</forenames></author><author><keyname>Ankaral&#x131;</keyname><forenames>Mustafa Mert</forenames></author><author><keyname>Cowan</keyname><forenames>Noah J.</forenames></author><author><keyname>Morg&#xfc;l</keyname><forenames>&#xd6;mer</forenames></author><author><keyname>Saranl&#x131;</keyname><forenames>Ulu&#xe7;</forenames></author></authors><title>Identification of a Hybrid Spring Mass Damper via Harmonic Transfer
  Functions as a Step Towards Data-Driven Models for Legged Locomotion</title><categories>cs.RO cs.SY math.DS</categories><comments>Draft submitted to the 17th International Conference on Advanced
  Robotics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are limitations on the extent to which manually constructed
mathematical models can capture relevant aspects of legged locomotion. Even
simple models for basic behaviors such as running involve non-integrable
dynamics, requiring the use of possibly inaccurate approximations in the design
of model-based controllers. In this study, we show how data-driven frequency
domain system identification methods can be used to obtain input--output
characteristics for a class of dynamical systems around their limit cycles,
with hybrid structural properties similar to those observed in legged
locomotion systems. Under certain assumptions, we can approximate hybrid
dynamics of such systems around their limit cycle as a piecewise smooth linear
time periodic system (LTP), further approximated as a time-periodic, piecewise
LTI system to reduce parametric degrees of freedom in the identification
process. In this paper, we use a simple one-dimensional hybrid model in which a
limit-cycle is induced through the actions of a linear actuator to illustrate
the details of our method. We first derive theoretical harmonic transfer
functions of our example model. We then excite the model with small chirp
signals to introduce perturbations around its limit-cycle and present
systematic identification results to estimate the harmonic transfer functions
for this model. Comparison between the data-driven HTF model and its
theoretical prediction illustrates the potential effectiveness of such
empirical identification methods in legged locomotion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05636</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05636</id><created>2015-01-22</created><updated>2015-10-04</updated><authors><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Quantum Markov chains, sufficiency of quantum channels, and Renyi
  information measures</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v4: 26 pages, 1 figure; reorganized and one open question solved with
  Choi's inequality (at the suggestion of an anonymous referee)</comments><journal-ref>Journal of Physics A vol. 48, no. 50, page 505301, November 2015</journal-ref><doi>10.1088/1751-8113/48/50/505301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A short quantum Markov chain is a tripartite state $\rho_{ABC}$ such that
system $A$ can be recovered perfectly by acting on system $C$ of the reduced
state $\rho_{BC}$. Such states have conditional mutual information $I(A;B|C)$
equal to zero and are the only states with this property. A quantum channel
$\mathcal{N}$ is sufficient for two states $\rho$ and $\sigma$ if there exists
a recovery channel using which one can perfectly recover $\rho$ from
$\mathcal{N}(\rho)$ and $\sigma$ from $\mathcal{N}(\sigma)$. The relative
entropy difference
$D(\rho\Vert\sigma)-D(\mathcal{N}(\rho)\Vert\mathcal{N}(\sigma))$ is equal to
zero if and only if $\mathcal{N}$ is sufficient for $\rho$ and $\sigma$. In
this paper, we show that these properties extend to Renyi generalizations of
these information measures which were proposed in [Berta et al., J. Math. Phys.
56, 022205, (2015)] and [Seshadreesan et al., J. Phys. A 48, 395303, (2015)],
thus providing an alternate characterization of short quantum Markov chains and
sufficient quantum channels. These results give further support to these
quantities as being legitimate Renyi generalizations of the conditional mutual
information and the relative entropy difference. Along the way, we solve some
open questions of Ruskai and Zhang, regarding the trace of particular matrices
that arise in the study of monotonicity of relative entropy under quantum
operations and strong subadditivity of the von Neumann entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05673</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05673</id><created>2015-01-22</created><authors><author><keyname>Jia</keyname><forenames>Limin</forenames></author><author><keyname>Sen</keyname><forenames>Shayak</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>System M: A Program Logic for Code Sandboxing and Identification</title><categories>cs.CR cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security-sensitive applications that execute untrusted code often check the
code's integrity by comparing its syntax to a known good value or sandbox the
code to contain its effects. System M is a new program logic for reasoning
about such security-sensitive applications. System M extends Hoare Type Theory
(HTT) to trace safety properties and, additionally, contains two new reasoning
principles. First, its type system internalizes logical equality, facilitating
reasoning about applications that check code integrity. Second, a confinement
rule assigns an effect type to a computation based solely on knowledge of the
computation's sandbox. We prove the soundness of system M relative to a
step-indexed trace-based semantic model. We illustrate both new reasoning
principles of system M by verifying the main integrity property of the design
of Memoir, a previously proposed trusted computing system for ensuring state
continuity of isolated security-sensitive applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05677</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05677</id><created>2015-01-22</created><updated>2015-05-05</updated><authors><author><keyname>Tolpin</keyname><forenames>David</forenames></author><author><keyname>van de Meent</keyname><forenames>Jan Willem</forenames></author><author><keyname>Paige</keyname><forenames>Brooks</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs</title><categories>cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an adaptive output-sensitive Metropolis-Hastings algorithm for
probabilistic models expressed as programs, Adaptive Lightweight
Metropolis-Hastings (AdLMH). The algorithm extends Lightweight
Metropolis-Hastings (LMH) by adjusting the probabilities of proposing random
variables for modification to improve convergence of the program output. We
show that AdLMH converges to the correct equilibrium distribution and compare
convergence of AdLMH to that of LMH on several test problems to highlight
different aspects of the adaptation scheme. We observe consistent improvement
in convergence on the test problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05680</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05680</id><created>2015-01-22</created><authors><author><keyname>Niethammer</keyname><forenames>Marc</forenames></author><author><keyname>Pohl</keyname><forenames>Kilian M.</forenames></author><author><keyname>Janoos</keyname><forenames>Firdaus</forenames></author><author><keyname>Wells</keyname><forenames>William M.</forenames><suffix>III</suffix></author></authors><title>Active Mean Fields for Probabilistic Image Segmentation: Connections
  with Chan-Vese and Rudin-Osher-Fatemi Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is a fundamental task for extracting semantically
meaningful regions from an image. The goal is to assign object labels to each
image location. Due to image-noise, shortcomings of algorithms and other
ambiguities in the images, there is uncertainty in the assigned labels. In
multiple application domains, estimates of this uncertainty are important. For
example, object segmentation and uncertainty quantification is essential for
many medical application, including tumor segmentation for radiation treatment
planning. While a Bayesian characterization of the label posterior provides
estimates of segmentation uncertainty, Bayesian approaches can be
computationally prohibitive for practical applications. On the other hand,
typical optimization based algorithms are computationally very efficient, but
only provide maximum a-posteriori solutions and hence no estimates of label
uncertainty.
  In this paper, we propose Active Mean Fields (AMF), a Bayesian technique that
uses a mean-field approximation to derive an efficient segmentation and
uncertainty quantification algorithm. This model, which allows combining any
label-likelihood measure with a boundary length prior, yields a variational
formulation that is convex. A specific implementation of that model is the
Chan--Vese segmentation model (CV), which formulates the binary segmentation
problem through Gaussian likelihoods combined with a boundary-length
regularizer. Furthermore, the Euler--Lagrange equations derived from the AMF
model are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for
image de-noising. Solutions to the AMF model can thus be implemented by
directly utilizing highly-efficient ROF solvers on log-likelihood ratio fields.
We demonstrate the approach using synthetic data, as well as real medical
images (for heart and prostate segmentations), and on standard computer vision
test images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05683</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05683</id><created>2015-01-22</created><updated>2015-11-02</updated><authors><author><keyname>Liu</keyname><forenames>Ling</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Polar Lattices for Lossy Compression</title><categories>cs.IT math.IT</categories><comments>35 pages, 12 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar lattices, which are constructed from polar codes, have recently been
proved to be able to achieve the capacity of the additive white Gaussian noise
(AWGN) channel. In this work, we propose a new construction of polar lattices
to solve the dual problem, i.e., achieving the rate-distortion bound of a
memoryless Gaussian source, which means that polar lattices can also be good
for the lossy compression of continuous sources. The structure of the proposed
polar lattices enables us to integrate the post-entropy coding process into the
lattice quantizer, which simplifies the quantization process. The overall
complexity of encoding and decoding complexity is $O(N \log^2 N)$ for a
sub-exponentially decaying excess distortion. Moreover, the nesting structure
of polar lattices further provides solutions for some multi-terminal coding
problems. The Wyner-Ziv coding problem for a Gaussian source can be solved by
an AWGN capacity-achieving polar lattice nested in a rate-distortion bound
achieving one, and the Gelfand-Pinsker problem can be solved in a reversed
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05684</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05684</id><created>2015-01-22</created><authors><author><keyname>Honeine</keyname><forenames>Paul</forenames></author><author><keyname>Zhu</keyname><forenames>Fei</forenames></author></authors><title>Bi-Objective Nonnegative Matrix Factorization: Linear Versus
  Kernel-Based Models</title><categories>stat.ML cs.CV cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) is a powerful class of feature
extraction techniques that has been successfully applied in many fields, namely
in signal and image processing. Current NMF techniques have been limited to a
single-objective problem in either its linear or nonlinear kernel-based
formulation. In this paper, we propose to revisit the NMF as a multi-objective
problem, in particular a bi-objective one, where the objective functions
defined in both input and feature spaces are taken into account. By taking the
advantage of the sum-weighted method from the literature of multi-objective
optimization, the proposed bi-objective NMF determines a set of nondominated,
Pareto optimal, solutions instead of a single optimal decomposition. Moreover,
the corresponding Pareto front is studied and approximated. Experimental
results on unmixing real hyperspectral images confirm the efficiency of the
proposed bi-objective NMF compared with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05691</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05691</id><created>2015-01-22</created><authors><author><keyname>Harper</keyname><forenames>Robert</forenames></author><author><keyname>Hou</keyname><forenames>Kuen-Bang</forenames></author></authors><title>A Note on the Uniform Kan Condition in Nominal Cubical Sets</title><categories>math.LO cs.PL math.CT</categories><comments>25 pages, 7 figures</comments><msc-class>03B15, 18C50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bezem, Coquand, and Huber have recently given a constructively valid model of
higher type theory in a category of nominal cubical sets satisfying a novel
condition, called the uniform Kan condition (UKC), which generalizes the
standard cubical Kan condition (as considered by, for example, Williamson in
his survey of combinatorial homotopy theory) to admit phantom &quot;additional&quot;
dimensions in open boxes. This note, which represents the authors' attempts to
fill in the details of the UKC, is intended for newcomers to the field who may
appreciate a more explicit formulation and development of the main ideas. The
crux of the exposition is an analogue of the Yoneda Lemma for co-sieves that
relates geometric open boxes bijectively to their algebraic counterparts, much
as its progenitor for representables relates geometric cubes to their algebraic
counterparts in a cubical set. This characterization is used to give a
formulation of uniform Kan fibrations in which uniformity emerges as naturality
in the additional dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05693</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05693</id><created>2015-01-22</created><authors><author><keyname>Yuan</keyname><forenames>Fang</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author><author><keyname>Song</keyname><forenames>Yang</forenames></author><author><keyname>Chen</keyname><forenames>Lan</forenames></author><author><keyname>Kakishima</keyname><forenames>Yuichi</forenames></author><author><keyname>Jiang</keyname><forenames>Huiling</forenames></author></authors><title>Joint Channel Direction Information Quantization For Spatially
  Correlated 3D MIMO Channels</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a codebook for jointly quantizing channel direction
information (CDI) of spatially correlated three-dimensional (3D)
multi-input-multi-output (MIMO) channels. To reduce the dimension for
quantizing the CDI of large antenna arrays, we introduce a special structure to
the codewords by using Tucker decomposition to exploit the unique features of
3D MIMO channels. Specifically, the codeword consists of four parts each with
low dimension individually targeting at a different type of information:
statistical CDIs in horizontal direction and in vertical direction, statistical
power coupling, and instantaneous CDI. The proposed codebook avoids the
redundancy led by existing independent CDI quantization. Analytical results
provide a sufficient condition on 3D MIMO channels to show that the proposed
codebook can achieve the same quantization performance as the well-known
rotated codebook applied to the global channel CDI, but with significant
reduction in the required statistical channel information. Simulation results
validate our analysis and demonstrate that the proposed joint CDI quantization
provides substantial performance gain over independent CDI quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05695</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05695</id><created>2015-01-22</created><authors><author><keyname>Quitin</keyname><forenames>Francois</forenames></author><author><keyname>Irish</keyname><forenames>Andrew T.</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>A scalable architecture for distributed receive beamforming: analysis
  and experimental demonstration</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose, analyze and demonstrate an architecture for scalable cooperative
reception. In a cluster of N + 1 receive nodes, one node is designated as the
final receiver, and the N other nodes act as amplify-and-forward relays which
adapt their phases such that the relayed signals add up constructively at the
designated receiver. This yields received SNR scaling linearly with N, while
avoiding the linear increase in overhead incurred by a direct approach in which
received signals are separately quantized and transmitted for centralized
processing. By transforming the task of long-distance distributed receive
beamforming into one of local distributed transmit beamforming, we can leverage
a scalable one-bit feedback algorithm for phase synchronization. We show that
time division between the long-distance and local links eliminates the need for
explicit frequency synchronization. We provide an analytical framework, whose
results closely match Monte Carlo simulations, to evaluate the impact of phase
noise due to relaying delay on the performance of the one-bit feedback
algorithm. Experimental results from our prototype implementation on
software-defined radios demonstrate the expected gains in received signal
strength despite significant oscillator drift, and are consistent with results
from our analytical framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05696</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05696</id><created>2015-01-22</created><updated>2015-08-29</updated><authors><author><keyname>Sakkos</keyname><forenames>Panos</forenames></author><author><keyname>Kotsakos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Katakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Gunopulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Anima: Adaptive Personalized Software Keyboard</title><categories>cs.HC</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Software Keyboard for smart touchscreen devices that learns its
owner's unique dictionary in order to produce personalized typing predictions.
The learning process is accelerated by analysing user's past typed
communication. Moreover, personal temporal user behaviour is captured and
exploited in the prediction engine. Computational and storage issues are
addressed by dynamically forgetting words that the user no longer types. A
prototype implementation is available at Google Play Store.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05700</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05700</id><created>2015-01-22</created><authors><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Soundarajan</keyname><forenames>Sucheta</forenames></author><author><keyname>Cao</keyname><forenames>Xuezhi</forenames></author><author><keyname>Hopcroft</keyname><forenames>John</forenames></author><author><keyname>Huang</keyname><forenames>Menglong</forenames></author></authors><title>Revealing Multiple Layers of Hidden Community Structure in Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new conception of community structure, which we refer to as
hidden community structure. Hidden community structure refers to a specific
type of overlapping community structure, in which the detection of weak, but
meaningful, communities is hindered by the presence of stronger communities. We
present Hidden Community Detection HICODE, an algorithm template that
identifies both the strong, dominant community structure as well as the weaker,
hidden community structure in networks. HICODE begins by first applying an
existing community detection algorithm to a network, and then removing the
structure of the detected communities from the network. In this way, the
structure of the weaker communities becomes visible. Through application of
HICODE, we demonstrate that a wide variety of real networks from different
domains contain many communities that, though meaningful, are not detected by
any of the popular community detection algorithms that we consider.
Additionally, on both real and synthetic networks containing a hidden
ground-truth community structure, HICODE uncovers this structure better than
any baseline algorithms that we compared against. For example, on a real
network of undergraduate students that can be partitioned either by `Dorm'
(residence hall) or `Year', we see that HICODE uncovers the weaker `Year'
communities with a JCRecall score (a recall-based metric that we define in the
text) of over 0.7, while the baseline algorithms achieve scores below 0.2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05703</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05703</id><created>2015-01-22</created><updated>2015-01-30</updated><authors><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author><author><keyname>Taigman</keyname><forenames>Yaniv</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author></authors><title>Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the task of recognizing peoples' identities in photo albums in an
unconstrained setting. To facilitate this, we introduce the new People In Photo
Albums (PIPA) dataset, consisting of over 60000 instances of 2000 individuals
collected from public Flickr photo albums. With only about half of the person
images containing a frontal face, the recognition task is very challenging due
to the large variations in pose, clothing, camera viewpoint, image resolution
and illumination. We propose the Pose Invariant PErson Recognition (PIPER)
method, which accumulates the cues of poselet-level person recognizers trained
by deep convolutional networks to discount for the pose variations, combined
with a face recognizer and a global recognizer. Experiments on three different
settings confirm that in our unconstrained setup PIPER significantly improves
on the performance of DeepFace, which is one of the best face recognizers as
measured on the LFW dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05705</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05705</id><created>2015-01-22</created><authors><author><keyname>Deng</keyname><forenames>Yi</forenames><affiliation>Rensselaer Polytechnic Institute</affiliation></author><author><keyname>Julius</keyname><forenames>Agung</forenames><affiliation>Rensselaer Polytechnic Institute</affiliation></author></authors><title>Safe Neighborhood Computation for Hybrid System Verification</title><categories>cs.SY</categories><comments>In Proceedings HAS 2014, arXiv:1501.05405</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 174, 2015, pp. 1-12</journal-ref><doi>10.4204/EPTCS.174.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the design and implementation of engineering systems, performing
model-based analysis can disclose potential safety issues at an early stage.
The analysis of hybrid system models is in general difficult due to the
intrinsic complexity of hybrid dynamics. In this paper, a simulation-based
approach to formal verification of hybrid systems is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05709</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05709</id><created>2015-01-22</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Chaidez</keyname><forenames>Julian</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Jansen</keyname><forenames>Hayden</forenames></author></authors><title>Associative Arrays: Unified Mathematics for Spreadsheets, Databases,
  Matrices, and Graphs</title><categories>cs.DB cs.MS</categories><comments>4 pages, 6 figures; New England Database Summit 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data processing systems impose multiple views on data as it is processed by
the system. These views include spreadsheets, databases, matrices, and graphs.
The common theme amongst these views is the need to store and operate on data
as whole sets instead of as individual data elements. This work describes a
common mathematical representation of these data sets (associative arrays) that
applies across a wide range of applications and technologies. Associative
arrays unify and simplify these different approaches for representing and
manipulating data into common two-dimensional view of data. Specifically,
associative arrays (1) reduce the effort required to pass data between steps in
a data processing system, (2) allow steps to be interchanged with full
confidence that the results will be unchanged, and (3) make it possible to
recognize when steps can be simplified or eliminated. Most database system
naturally support associative arrays via their tabular interfaces. The D4M
implementation of associative arrays uses this feature to provide a common
interface across SQL, NoSQL, and NewSQL databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05710</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05710</id><created>2015-01-22</created><authors><author><keyname>Hanay</keyname><forenames>Y. Sinan</forenames></author><author><keyname>Arakawa</keyname><forenames>Shinichi</forenames></author><author><keyname>Murata</keyname><forenames>Masayuki</forenames></author></authors><title>A Highly Tunable Virtual Topology Controller</title><categories>cs.NI</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much research in the last two decades has focused on Virtual Topology
Reconfiguration (VTR) problem. However, most of the proposed methods either has
low controllability, or the analysis of a control parameter is limited to
empirical analysis. In this paper, we present a highly tunable Virtual Topology
(VT) controller. First, we analyze the controllability of two previously
proposed VTR algorithms: a heuristic method and a neural networks based method.
Then we present insights on how to transform these VTR methods to their tunable
versions. To benefit from the controllability, an optimality analysis of the
control parameter is needed. In the second part of the paper, through a
probabilistic analysis we find an optimal parameter for the neural network
based method. We validated our analysis through simulations. We propose this
highly tunable method as a new VTR algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05714</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05714</id><created>2015-01-23</created><authors><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Identification of influential nodes in network of networks</title><categories>cs.SI physics.soc-ph</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network of networks(NON) research is focused on studying the properties
of n interdependent networks which is ubiquitous in the real world. Identifying
the influential nodes in the network of networks is theoretical and practical
significance. However, it is hard to describe the structure property of the NON
based on traditional methods. In this paper, a new method is proposed to
identify the influential nodes in the network of networks base on the evidence
theory. The proposed method can fuse different kinds of relationship between
the network components to constructed a comprehensive similarity network. The
nodes which have a big value of similarity are the influential nodes in the
NON. The experiment results illustrate that the proposed method is reasonable
and significant
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05724</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05724</id><created>2015-01-23</created><authors><author><keyname>Essaid</keyname><forenames>Amira</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Smits</keyname><forenames>Gr&#xe9;gory</forenames></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Uncertainty in Ontology Matching: A Decision Rule-Based Approach</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Information Processing and Management
  of Uncertainty in Knowledge-Based Systems (IPMU), Jul 2014, Montpellier,
  France. pp.46 - 55</journal-ref><doi>10.1007/978-3-319-08795-5_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering the high heterogeneity of the ontologies pub-lished on the web,
ontology matching is a crucial issue whose aim is to establish links between an
entity of a source ontology and one or several entities from a target ontology.
Perfectible similarity measures, consid-ered as sources of information, are
combined to establish these links. The theory of belief functions is a powerful
mathematical tool for combining such uncertain information. In this paper, we
introduce a decision pro-cess based on a distance measure to identify the best
possible matching entities for a given source entity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05725</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05725</id><created>2015-01-23</created><authors><author><keyname>Abbas</keyname><forenames>Hosny A.</forenames></author></authors><title>Efficient Web-Based SCADA System</title><categories>cs.SY</categories><comments>Master thesis, 170 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer-based supervisory control and data acquisition (SCADA) systems have
evolved over the past four decades, from standalone, compartmentalized
operations into networked architectures that communicate across large
distances. There is an emerging trend comprising SCADA and conventional IT
units toward consolidating some overlapping activities. This trend is motivated
by cost savings achieved by consolidating disparate platforms, networks,
software, and maintenance tools. For reasons of efficiency, maintenance,
economics, data acquisition, control platforms have migrated from isolated
in-plant networks using proprietary hardware and software to PC-based systems
using standard software, network protocols, and the Internet. In this thesis,
we present an approach for web-based SCADA systems that adapt to the behavior
of the target application. In addition, we take into account the real time
constraints that imposed by the nature of the problem. We show that our
approach is more efficient than other approaches in terms of consuming as
little as possible of the available resources (computational power and network
bandwidth).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05738</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05738</id><created>2015-01-23</created><authors><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author></authors><title>Hybrid Millimeter-Wave Systems: A Novel Paradigm for HetNets</title><categories>cs.NI</categories><comments>12 pages, 5 Figures, IEEE Communication Magazine. In press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous Networks (HetNets) are known to enhance the bandwidth
efficiency and throughput of wireless networks by more effectively utilizing
the network resources. However, the higher density of users and access points
in HetNets introduces significant inter-user interference that needs to be
mitigated through complex and sophisticated interference cancellation schemes.
Moreover, due to significant channel attenuation and presence of hardware
impairments, e.g., phase noise and amplifier nonlinearities, the vast bandwidth
in the millimeter-wave band has not been fully utilized to date. In order to
enable the development of multi-Gigabit per second wireless networks, we
introduce a novel millimeter-wave HetNet paradigm, termed hybrid HetNet, which
exploits the vast bandwidth and propagation characteristics in the 60 GHz and
70-80 GHz bands to reduce the impact of interference in HetNets. Simulation
results are presented to illustrate the performance advantage of hybrid HetNets
with respect to traditional networks. Next, two specific transceiver structures
that enable hand-offs from the 60 GHz band, i.e., the V-band to the 70-80 GHz
band, i.e., the E-band, and vice versa are proposed. Finally, the practical and
regulatory challenges for establishing a hybrid HetNet are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05739</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05739</id><created>2015-01-23</created><authors><author><keyname>Bosco</keyname><forenames>Claudio</forenames></author><author><keyname>Sander</keyname><forenames>Graham</forenames></author></authors><title>Estimating the effects of water-induced shallow landslides on soil
  erosion</title><categories>cs.CE</categories><comments>14 pages, 4 figures, 1 table, published in IEEE Earthzine 2014 Vol. 7
  Issue 2, 910137+ 2nd quarter theme. Geospatial Semantic Array Programming.
  Available: http://www.earthzine.org/?p=910137</comments><journal-ref>IEEE Earthzine, vol. 7, no. 2, pp. 910137+, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rainfall induced landslides and soil erosion are part of a complex system of
multiple interacting processes, and both are capable of significantly affecting
sediment budgets. These sediment mass movements also have the potential to
significantly impact on a broad network of ecosystems health, functionality and
the services they provide. To support the integrated assessment of these
processes it is necessary to develop reliable modelling architectures. This
paper proposes a semi-quantitative integrated methodology for a robust
assessment of soil erosion rates in data poor regions affected by landslide
activity. It combines heuristic, empirical and probabilistic approaches. This
proposed methodology is based on the geospatial semantic array programming
paradigm and has been implemented on a catchment scale methodology using
Geographic Information Systems (GIS) spatial analysis tools and GNU Octave. The
integrated data-transformation model relies on a modular architecture, where
the information flow among modules is constrained by semantic checks. In order
to improve computational reproducibility, the geospatial data transformations
implemented in ESRI ArcGis are made available in the free software GRASS GIS.
The proposed modelling architecture is flexible enough for future
transdisciplinary scenario analysis to be more easily designed. In particular,
the architecture might contribute as a novel component to simplify future
integrated analyses of the potential impact of wildfires or vegetation types
and distributions, on sediment transport from water induced landslides and
erosion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05740</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05740</id><created>2015-01-23</created><authors><author><keyname>Sundin</keyname><forenames>Martin</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author></authors><title>Bayesian Learning for Low-Rank matrix reconstruction</title><categories>stat.ML cs.LG cs.NA</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop latent variable models for Bayesian learning based low-rank matrix
completion and reconstruction from linear measurements. For under-determined
systems, the developed methods are shown to reconstruct low-rank matrices when
neither the rank nor the noise power is known a-priori. We derive relations
between the latent variable models and several low-rank promoting penalty
functions. The relations justify the use of Kronecker structured covariance
matrices in a Gaussian based prior. In the methods, we use evidence
approximation and expectation-maximization to learn the model parameters. The
performance of the methods is evaluated through extensive numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05752</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05752</id><created>2015-01-23</created><authors><author><keyname>Dimitrov</keyname><forenames>Darko</forenames></author></authors><title>On structural properties of trees with minimal atom-bond connectivity
  index II</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em atom-bond connectivity (ABC) index} is a degree-based graph
topological index that found chemical applications. The problem of complete
characterization of trees with minimal $ABC$ index is still an open problem.
In~\cite{d-sptmabci-2014}, it was shown that trees with minimal ABC index do
not contain so-called {\em $B_k$-branches}, with $k \geq 5$, and that they do
not have more than four $B_4$-branches. Our main results here reveal that the
number of $B_1$ and $B_2$-branches are also bounded from above by small fixed
constants. Namely, we show that trees with minimal ABC index do not contain
more than four $B_1$-branches and more than eleven $B_2$-branches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05757</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05757</id><created>2015-01-23</created><updated>2015-12-08</updated><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>On the Geometric Ergodicity of Metropolis-Hastings Algorithms for
  Lattice Gaussian Sampling</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling from the lattice Gaussian distribution is emerging as an important
problem in coding and cryptography. In this paper, the classic
Metropolis-Hastings (MH) algorithm from Markov chain Monte Carlo (MCMC) methods
is adapted for lattice Gaussian sampling. Two MH-based algorithms are proposed,
which overcome the restriction suffered by the default Klein's algorithm. The
first one, referred to as the independent Metropolis-Hastings-Klein (MHK)
algorithm, tries to establish a Markov chain through an independent proposal
distribution. We show that the Markov chain arising from the independent MHK
algorithm is uniformly ergodic, namely, it converges to the stationary
distribution exponentially fast regardless of the initial state. Moreover, the
rate of convergence is explicitly calculated in terms of the theta series,
leading to a predictable mixing time. In order to further exploit the
convergence potential, a symmetric Metropolis-Klein (SMK) algorithm is
proposed. It is proven that the Markov chain induced by the SMK algorithm is
geometrically ergodic, where a reasonable selection of the initial state is
capable to enhance the convergence performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05759</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05759</id><created>2015-01-23</created><authors><author><keyname>Zhang</keyname><forenames>Shanshan</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Filtered Channel Features for Pedestrian Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper starts from the observation that multiple top performing
pedestrian detectors can be modelled by using an intermediate layer filtering
low-level features in combination with a boosted decision forest. Based on this
observation we propose a unifying framework and experimentally explore
different filter families. We report extensive results enabling a systematic
analysis.
  Using filtered channel features we obtain top performance on the challenging
Caltech and KITTI datasets, while using only HOG+LUV as low-level features.
When adding optical flow features we further improve detection quality and
report the best known results on the Caltech dataset, reaching 93% recall at 1
FPPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05773</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05773</id><created>2015-01-23</created><authors><author><keyname>Nobili</keyname><forenames>Paolo</forenames></author><author><keyname>Sassano</keyname><forenames>Antonio</forenames></author></authors><title>An ${\cal O}(m\log n)$ algorithm for the weighted stable set problem in
  claw-free graphs with $\alpha({G}) \le 3$</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to solve the \emph{Maximum Weight Stable Set
Problem} in a claw-free graph $G(V, E)$ with $\alpha(G) \le 3$ in time ${\cal
O}(|E|\log|V|)$. More precisely, in time ${\cal O}(|E|)$ we check whether
$\alpha(G) \le 3$ or produce a stable set with cardinality at least $4$;
moreover, if $\alpha(G) \le 3$ we produce in time ${\cal O}(|E|\log|V|)$ a
maximum stable set of $G$. This improves the bound of ${\cal O}(|E||V|)$ due to
Faenza et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05775</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05775</id><created>2015-01-23</created><authors><author><keyname>Nobili</keyname><forenames>Paolo</forenames></author><author><keyname>Sassano</keyname><forenames>Antonio</forenames></author></authors><title>An ${\cal O}(n^2 \log(n))$ algorithm for the weighted stable set problem
  in claw-free graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A claw-free graph $G(V, E)$ is said to be basic if there exists a matching $M
\subseteq E$ whose edges are strongly bisimplicial and such that each connected
component $C$ of $G - M$ is either a clique or a {claw, net}-free graph or
satisfies $\alpha(G[C \setminus V(M)]) \le 3$. The Maximum Weight Stable Set
(MWSS) Problem in a basic claw-free graph can be easily solved by first solving
at most four MWSS problems in each connected component of $G - M$ in ${\cal
O}(|V|^2 \log(|V|))$ time (\cite{NobiliSassano15a,NobiliSassano15b}) and then
solving the MWSS Problem on a suitable line graph constructed from $G$ in
${\cal O}(|V|^2 \log(|V|))$ time. In this paper we show that, by means of
lifting operations, every claw-free graph $G(V, E)$ can be transformed, in
${\cal O}(|V|^2)$ time, into a basic claw-free graph $\bar G(\bar V, \bar E)$
such that $|\bar V| = {\cal O}(|V|)$ and a MWSS of $G$ can be obtained from a
MWSS of $\bar G$. This shows that the complexity of solving the MWSS Problem in
a claw-free graph $G(V, E)$ is ${\cal O}(|V|^2 \log(|V|))$, the same as in line
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05779</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05779</id><created>2015-01-23</created><authors><author><keyname>Gkiolmas</keyname><forenames>Aristotelis</forenames></author><author><keyname>Chalkidis</keyname><forenames>Anthimos</forenames></author><author><keyname>Papaconstantinou</keyname><forenames>Maria</forenames></author><author><keyname>Iqbal</keyname><forenames>Zafar</forenames></author><author><keyname>Skordoulis</keyname><forenames>Constantine</forenames></author></authors><title>An alternative use of the NetLogo modeling environment, where the
  student thinks and acts like an Agent, in order to teach concepts of Ecology</title><categories>cs.CY</categories><comments>9th Pan-Hellenic Conference with International Participation: &quot;ICT's
  in Education&quot; (HCICTE 2014) 3rd-5th October 2014, University of Crete,
  Rethymno, Greece</comments><journal-ref>Proceedings of the 9th Pan-Hellenic Conference with International
  Participation: &quot;ICT's in Education&quot; (HCICTE 2014) 3rd-5th October 2014,
  University of Crete, Rethymno, Greece, pp. 379-386</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multi Agent Based programming, modeling and simulation environment of
NetLogo has been used extensively during the last fifteen years for educational
among other purposes. The learning subject, upon interacting with the Users
Interface of NetLogo, can easily study properties of the simulated natural
systems, as well as observe the latters response, when altering their
parameters. In this research, NetLogo was used under the perspective that the
learning subject (student or prospective teacher)interacts with the model in a
deeper way, obtaining the role of an agent. This is not achieved by obliging
the learner to program (write NetLogo code) but by interviewing them, together
with applying the choices that they make on the model. The scheme was carried
out, as part of a broader research, with interviews, and web page like
interface menu selections, in a sample of 17 University students in Athens
(prospective Primary School teachers) and the results were judged as
encouraging. At a further stage, the computers were set as a network, where all
the agents performed together. In this way the learners could watch onscreen
the overall outcome of their choices and actions on the modeled ecosystem. This
seems to open a new, small, area of research in NetLogo educational
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05789</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05789</id><created>2015-01-23</created><authors><author><keyname>Xu</keyname><forenames>Minxian</forenames></author><author><keyname>Tian</keyname><forenames>Wenhong</forenames></author><author><keyname>Wang</keyname><forenames>Xinyang</forenames></author><author><keyname>Xiong</keyname><forenames>Qin</forenames></author></authors><title>FlexCloud: A Flexible and Extendible Simulator for Performance
  Evaluation of Virtual Machine Allocation</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Data centers aim to provide reliable, sustainable and scalable services
for all kinds of applications. Resource scheduling is one of keys to cloud
services. To model and evaluate different scheduling policies and algorithms,
we propose FlexCloud, a flexible and scalable simulator that enables users to
simulate the process of initializing cloud data centers, allocating virtual
machine requests and providing performance evaluation for various scheduling
algorithms. FlexCloud can be run on a single computer with JVM to simulate
large scale cloud environments with focus on infrastructure as a service;
adopts agile design patterns to assure the flexibility and extensibility;
models virtual machine migrations which is lack in the existing tools; provides
user-friendly interfaces for customized configurations and replaying. Comparing
to existing simulators, FlexCloud has combining features for supporting public
cloud providers, load-balance and energy-efficiency scheduling. FlexCloud has
advantage in computing time and memory consumption to support large-scale
simulations. The detailed design of FlexCloud is introduced and performance
evaluation is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05790</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05790</id><created>2015-01-23</created><authors><author><keyname>Hosang</keyname><forenames>Jan</forenames></author><author><keyname>Omran</keyname><forenames>Mohamed</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Taking a Deeper Look at Pedestrians</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the use of convolutional neural networks (convnets)
for the task of pedestrian detection. Despite their recent diverse successes,
convnets historically underperform compared to other pedestrian detectors. We
deliberately omit explicitly modelling the problem into the network (e.g. parts
or occlusion modelling) and show that we can reach competitive performance
without bells and whistles. In a wide range of experiments we analyse small and
big convnets, their architectural choices, parameters, and the influence of
different training data, including pre-training on surrogate tasks.
  We present the best convnet detectors on the Caltech and KITTI dataset. On
Caltech our convnets reach top performance both for the Caltech1x and
Caltech10x training setup. Using additional data at training time our strongest
convnet model is competitive even to detectors that use additional data
(optical flow) at test time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05800</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05800</id><created>2015-01-23</created><authors><author><keyname>Feghali</keyname><forenames>Carl</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>A Reconfigurations Analogue of Brooks' Theorem and its Consequences</title><categories>cs.CC cs.DM cs.DS</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a simple undirected graph on $n$ vertices with maximum
degree~$\Delta$. Brooks' Theorem states that $G$ has a $\Delta$-colouring
unless~$G$ is a complete graph, or a cycle with an odd number of vertices. To
recolour $G$ is to obtain a new proper colouring by changing the colour of one
vertex. We show an analogue of Brooks' Theorem by proving that from any
$k$-colouring, $k&gt;\Delta$, a $\Delta$-colouring of $G$ can be obtained by a
sequence of $O(n^2)$ recolourings using only the original $k$ colours unless
$G$ is a complete graph or a cycle with an odd number of vertices, or
$k=\Delta+1$, $G$ is $\Delta$-regular and, for each vertex $v$ in $G$, no two
neighbours of $v$ are coloured alike.
  We use this result to study the reconfiguration graph $R_k(G)$ of the
$k$-colourings of $G$. The vertex set of $R_k(G)$ is the set of all possible
$k$-colourings of $G$ and two colourings are adjacent if they differ on exactly
one vertex. We prove that for $\Delta\geq 3$, $R_{\Delta+1}(G)$ consists of
isolated vertices and at most one further component which has diameter
$O(n^2)$. This result enables us to complete both a structural classification
and an algorithmic classification for reconfigurations of colourings of graphs
of bounded maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05801</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05801</id><created>2015-01-23</created><authors><author><keyname>Buchbinder</keyname><forenames>Niv</forenames></author><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Schwartz</keyname><forenames>Roy</forenames></author></authors><title>Online Submodular Maximization with Preemption</title><categories>cs.DS</categories><comments>32 pages, an extended abstract of this work appeared in SODA 2015</comments><msc-class>68W27, 68W40, 68R05</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular function maximization has been studied extensively in recent years
under various constraints and models. The problem plays a major role in various
disciplines. We study a natural online variant of this problem in which
elements arrive one-by-one and the algorithm has to maintain a solution obeying
certain constraints at all times. Upon arrival of an element, the algorithm has
to decide whether to accept the element into its solution and may preempt
previously chosen elements. The goal is to maximize a submodular function over
the set of elements in the solution.
  We study two special cases of this general problem and derive upper and lower
bounds on the competitive ratio. Specifically, we design a $1/e$-competitive
algorithm for the unconstrained case in which the algorithm may hold any subset
of the elements, and constant competitive ratio algorithms for the case where
the algorithm may hold at most $k$ elements in its solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05802</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05802</id><created>2015-01-23</created><authors><author><keyname>Patri</keyname><forenames>Ashutosh</forenames></author><author><keyname>Nimaje</keyname><forenames>Devidas S.</forenames></author></authors><title>Radio Frequency Propagation Model and Fading of Wireless Signal at 2.4
  GHz in Underground Coal Mine</title><categories>cs.IT math.IT</categories><comments>21 pages, 6 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment of wireless sensor networks and wireless communication systems
have become indispensable for better real-time data acquisition from ground
monitoring devices, gas sensors, and equipment used in underground mines as
well as in locating the miners, since conventional methods like use of wireline
communication are rendered ineffective in the event of mine hazards such as
roof-falls, fire hazard etc. Before implementation of any wireless system, the
variable path loss indices for different work place should be determined; this
helps in better signal reception and sensor-node localisation. This also
improves the method by which miner carrying the wireless device is tracked.
This paper proposes a novel method for parameter determination of a suitable
radio propagation model with the help of results of a practical experiment
carried out in an underground coal mine of Southern India. The path loss
indices along with other essential parameters for accurate localisation have
been determined using XBee module and ZigBee protocol at 2.4 GHz frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05808</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05808</id><created>2015-01-23</created><updated>2015-07-19</updated><authors><author><keyname>Granell</keyname><forenames>Clara</forenames></author><author><keyname>Darst</keyname><forenames>Richard K.</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author></authors><title>Benchmark model to assess community structure in evolving networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>11 pages, 7 figures, 3 tables</comments><journal-ref>Physical Review E 92 (2015) 012805</journal-ref><doi>10.1103/PhysRevE.92.012805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the time evolution of the community structure of networks is
crucial to identify major changes in the internal organization of many complex
systems, which may undergo important endogenous or exogenous events. This
analysis can be done in two ways: considering each snapshot as an independent
community detection problem or taking into account the whole evolution of the
network. In the first case, one can apply static methods on the temporal
snapshots, which correspond to configurations of the system in short time
windows, and match afterwards the communities across layers. Alternatively, one
can develop dedicated dynamic procedures, so that multiple snapshots are
simultaneously taken into account while detecting communities, which allows us
to keep memory of the flow. To check how well a method of any kind could
capture the evolution of communities, suitable benchmarks are needed. Here we
propose a model for generating simple dynamic benchmark graphs, based on
stochastic block models. In them, the time evolution consists of a periodic
oscillation of the system's structure between configurations with built-in
community structure. We also propose the extension of quality comparison
indices to the dynamic scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05809</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05809</id><created>2015-01-23</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>CRDSA, CRDSA++ and IRSA: Stability and Performance Evaluation</title><categories>cs.IT cs.NI math.IT</categories><comments>6th Advanced Satellite Multimedia Systems Conference (ASMS) and 12th
  Signal Processing for Space Communications Workshop (SPSC), 2012</comments><doi>10.1109/ASMS-SPSC.2012.6333080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent past, new enhancements based on the well established Aloha
technique (CRDSA, CRDSA++, IRSA) have demonstrated the capability to reach
higher throughput than traditional SA, in bursty traffic conditions and without
any need of coordination among terminals. In this paper, retransmissions and
related stability for these new techniques are discussed. A model is also
formulated in order to provide a basis for the analysis of the stability and
the performance both for finite and infinite users population. This model can
be used as a framework for the design of such a communication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05810</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05810</id><created>2015-01-23</created><authors><author><keyname>Preclik</keyname><forenames>Tobias</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Ultrascale Simulations of Non-smooth Granular Dynamics</title><categories>cs.CE physics.comp-ph</categories><msc-class>65Y05 (Primary), 70F35, 70F40, 70E55 (Secondary)</msc-class><acm-class>I.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents new algorithms for massively parallel granular dynamics
simulations on distributed memory architectures using a domain partitioning
approach. Collisions are modelled with hard contacts in order to hide their
micro-dynamics and thus to extend the time and length scales that can be
simulated. The multi-contact problem is solved using a non-linear block
Gauss-Seidel method that is conforming to the subdomain structure. The parallel
algorithms employ a sophisticated protocol between processors that delegate
algorithmic tasks such as contact treatment and position integration uniquely
and robustly to the processors. Communication overhead is minimized through
aggressive message aggregation, leading to excellent strong and weak scaling.
The robustness and scalability is assessed on three clusters including two
peta-scale supercomputers with up to 458752 processor cores. The simulations
can reach unprecedented resolution of up to ten billion non-spherical particles
and contacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05814</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05814</id><created>2015-01-23</created><authors><author><keyname>Guillon</keyname><forenames>Pierre</forenames><affiliation>I2M</affiliation></author><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Infinite Communication Complexity</title><categories>cs.CC cs.DM</categories><comments>First Version. Written from the Computer Science POV</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that Alice and Bob are given each an infinite string, and they want
to decide whether their two strings are in a given relation. How much
communication do they need? How can communication be even defined and measured
for infinite strings? In this article, we propose a formalism for a notion of
infinite communication complexity, prove that it satisfies some natural
properties and coincides, for relevant applications, with the classical notion
of amortized communication complexity. More-over, an application is given for
tackling some conjecture about tilings and multidimensional sofic shifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05819</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05819</id><created>2015-01-23</created><updated>2015-01-30</updated><authors><author><keyname>Gorcin</keyname><forenames>Ali</forenames></author><author><keyname>Arslan</keyname><forenames>Huseyin</forenames></author></authors><title>Signal identification for adaptive spectrum hyperspace access in
  wireless communications systems</title><categories>cs.NI cs.IT math.IT</categories><comments>Appears in IEEE Communications Magazine, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technologies which will lead to adaptive, intelligent, and aware wireless
communications systems are expected to offer solutions to the capacity,
interference, and reliability problems of wireless networks. The spectrum
sensing feature of cognitive radio (CR) systems is a step forward to better
recognize the problems and to achieve efficient spectrum allocation. On the
other hand, even though spectrum sensing can constitute a solid base to
accomplish the reconfigurability and awareness goals of next generation
networks, a new perspective is required to benefit from the whole dimensions of
the available electro (or spectrum) hyperspace, beyond frequency and time.
Therefore, spectrum sensing should evolve to a more general and comprehensive
awareness providing mechanism, not only as part of CR systems but also as a
communication environment awareness component of an adaptive spectrum
hyperspace access (ASHA) paradigm which can adapt sensing parameters
autonomously to ensure robust signal identification, parameter estimation, and
interference avoidance. Such an approach will lead to recognition of
communication opportunities in different dimensions of the spectrum hyperspace,
and provide necessary information about the air interfaces, access techniques
and waveforms that are deployed over the monitored spectrum to accomplish ASHA,
resource and interference management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05821</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05821</id><created>2015-01-23</created><authors><author><keyname>Marcozzi</keyname><forenames>Micha&#xeb;l</forenames></author><author><keyname>Vanhoof</keyname><forenames>Wim</forenames></author><author><keyname>Hainaut</keyname><forenames>Jean-Luc</forenames></author></authors><title>A Symbolic Execution Algorithm for Constraint-Based Testing of Database
  Programs</title><categories>cs.SE</categories><comments>12 pages - preliminary work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In so-called constraint-based testing, symbolic execution is a common
technique used as a part of the process to generate test data for imperative
programs. Databases are ubiquitous in software and testing of programs
manipulating databases is thus essential to enhance the reliability of
software. This work proposes and evaluates experimentally a symbolic ex-
ecution algorithm for constraint-based testing of database programs. First, we
describe SimpleDB, a formal language which offers a minimal and well-defined
syntax and seman- tics, to model common interaction scenarios between pro-
grams and databases. Secondly, we detail the proposed al- gorithm for symbolic
execution of SimpleDB models. This algorithm considers a SimpleDB program as a
sequence of operations over a set of relational variables, modeling both the
database tables and the program variables. By inte- grating this relational
model of the program with classical static symbolic execution, the algorithm
can generate a set of path constraints for any finite path to test in the
control- flow graph of the program. Solutions of these constraints are test
inputs for the program, including an initial content for the database. When the
program is executed with respect to these inputs, it is guaranteed to follow
the path with re- spect to which the constraints were generated. Finally, the
algorithm is evaluated experimentally using representative SimpleDB models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05826</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05826</id><created>2015-01-23</created><authors><author><keyname>Kosta</keyname><forenames>Marek</forenames></author><author><keyname>Sturm</keyname><forenames>Thomas</forenames></author></authors><title>A Generalized Framework for Virtual Substitution</title><categories>cs.SC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the framework of virtual substitution for real quantifier
elimination to arbitrary but bounded degrees. We make explicit the
representation of test points in elimination sets using roots of parametric
univariate polynomials described by Thom codes. Our approach follows an early
suggestion by Weispfenning, which has never been carried out explicitly.
Inspired by virtual substitution for linear formulas, we show how to
systematically construct elimination sets containing only test points
representing lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05828</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05828</id><created>2015-01-23</created><authors><author><keyname>Chakraborty</keyname><forenames>Diptarka</forenames></author><author><keyname>Tewari</keyname><forenames>Raghunath</forenames></author></authors><title>An $O(n^{\epsilon})$ Space and Polynomial Time Algorithm for
  Reachability in Directed Layered Planar Graphs</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$ and two vertices $s$ and $t$ in it, {\em graph
reachability} is the problem of checking whether there exists a path from $s$
to $t$ in $G$. We show that reachability in directed layered planar graphs can
be decided in polynomial time and $O(n^\epsilon)$ space, for any $\epsilon &gt;
0$. The previous best known space bound for this problem with polynomial time
was approximately $O(\sqrt{n})$ space \cite{INPVW13}.
  Deciding graph reachability in {\SC} is an important open question in
complexity theory and in this paper we make progress towards resolving this
question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05843</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05843</id><created>2015-01-23</created><authors><author><keyname>B&#xe9;al</keyname><forenames>Marie-Pierre</forenames></author><author><keyname>Dima</keyname><forenames>C&#x1ce;t&#x1ce;lin</forenames></author></authors><title>$\mathbb{N}$-algebraicity of zeta functions of sofic-Dyck shifts</title><categories>cs.FL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1305.7413</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the multivariate zeta function of a sofic-Dyck shift is the
commutative series of some visibly pushdown language. As a consequence the zeta
function of a sofic-Dyck shift is the generating function of a visibly pushdown
language and is thus an $\mathbb{N}$-algebraic series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05847</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05847</id><created>2015-01-23</created><authors><author><keyname>Ho</keyname><forenames>Jack</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author></authors><title>Robust Decentralized Detection and Social Learning in Tandem Networks</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2448525</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a tandem of agents who make decisions about an underlying binary
hypothesis, where the distribution of the agent observations under each
hypothesis comes from an uncertainty class. We investigate both decentralized
detection rules, where agents collaborate to minimize the error probability of
the final agent, and social learning rules, where each agent minimizes its own
local minimax error probability. We then extend our results to the infinite
tandem network, and derive necessary and sufficient conditions on the
uncertainty classes for the minimax error probability to converge to zero when
agents know their positions in the tandem. On the other hand, when agents do
not know their positions in the network, we study the cases where agents
collaborate to minimize the asymptotic minimax error probability, and where
agents seek to minimize their worst-case minimax error probability (over all
possible positions in the tandem). We show that asymptotic learning of the true
hypothesis is no longer possible in these cases, and derive characterizations
for the minimax error performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05851</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05851</id><created>2015-01-23</created><updated>2015-10-23</updated><authors><author><keyname>Nobili</keyname><forenames>Paolo</forenames></author><author><keyname>Sassano</keyname><forenames>Antonio</forenames></author></authors><title>An ${\cal O}(n\sqrt{m})$ algorithm for the weighted stable set problem
  in {claw, net}-free graphs with $\alpha(G) \ge 4$</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that a connected {claw, net}-free graph $G(V, E)$ with
$\alpha(G) \ge 4$ is the union of a strongly bisimplicial clique $Q$ and at
most two clique-strips. A clique is strongly bisimplicial if its neighborhood
is partitioned into two cliques which are mutually non-adjacent and a
clique-strip is a sequence of cliques $\{H_0, \dots, H_p\}$ with the property
that $H_i$ is adjacent only to $H_{i-1}$ and $H_{i+1}$. By exploiting such a
structure we show how to solve the Maximum Weight Stable Set Problem in such a
graph in time ${\cal O}(|V|\sqrt{|E|})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05854</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05854</id><created>2015-01-23</created><authors><author><keyname>Torres</keyname><forenames>Wuilian</forenames></author><author><keyname>Rueda-Toicen</keyname><forenames>Antonio</forenames></author></authors><title>Unsupervised Segmentation of Multispectral Images with Cellular Automata</title><categories>cs.CV</categories><comments>6 pages, 6 figures, conference: CIMENICS XII, 2014</comments><msc-class>cs.CV</msc-class><doi>10.13140/2.1.2250.7849</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Multispectral images acquired by satellites are used to study phenomena on
the Earth's surface. Unsupervised classification techniques analyze
multispectral image content without considering prior knowledge of the observed
terrain; this is done using techniques which group pixels that have similar
statistics of digital level distribution in the various image channels. In this
paper, we propose a methodology for unsupervised classification based on a
deterministic cellular automaton. The automaton is initialized in an
unsupervised manner by setting seed cells, selected according to two criteria:
to be representative of the spatial distribution of the dominant elements in
the image, and to take into account the diversity of spectral signatures in the
image. The automaton's evolution is based on an attack rule that is applied
simultaneously to all its cells. Among the noteworthy advantages of
deterministic cellular automata for multispectral processing of satellite
imagery is the consideration of topological information in the image via seed
positioning, and the ability to modify the scale of the study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05882</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05882</id><created>2015-01-23</created><updated>2015-11-30</updated><authors><author><keyname>Subramanian</keyname><forenames>Anand</forenames></author><author><keyname>Farias</keyname><forenames>Katyanne</forenames></author></authors><title>Efficient local search limitation strategy for single machine total
  weighted tardiness scheduling with sequence-dependent setup times</title><categories>cs.AI</categories><comments>32 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the single machine total weighted tardiness scheduling
with sequence-dependent setup times, usually referred as $1|s_{ij}|\sum
w_jT_j$. In this $\mathcal{NP}$-hard problem, each job has an associated
processing time, due date and a weight. For each pair of jobs $i$ and $j$,
there may be a setup time before starting to process $j$ in case this job is
scheduled immediately after $i$. The objective is to determine a schedule that
minimizes the total weighted tardiness, where the tardiness of a job is equal
to its completion time minus its due date, in case the job is completely
processed only after its due date, and is equal to zero otherwise. Due to its
complexity, this problem is most commonly solved by heuristics. The aim of this
work is to develop a simple yet effective limitation strategy that speeds up
the local search procedure without a significant loss in the solution quality.
Such strategy consists of a filtering mechanism that prevents unpromising moves
to be evaluated. The proposed strategy has been embedded in a local search
based metaheuristic from the literature and tested in classical benchmark
instances. Computational experiments revealed that the limitation strategy
enabled the metaheuristic to be extremely competitive when compared to other
algorithms from the literature, since it allowed the use of a large number of
neighborhood structures without a significant increase in the CPU time and,
consequently, high quality solutions could be achieved in a matter of seconds.
In addition, we analyzed the effectiveness of the proposed strategy in two
other well-known metaheuristics. Further experiments were also carried out on
benchmark instances of problem $1|s_{ij}|\sum T_j$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05887</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05887</id><created>2015-01-23</created><updated>2015-04-24</updated><authors><author><keyname>Yagi</keyname><forenames>Hideki</forenames></author><author><keyname>Han</keyname><forenames>Te Sun</forenames></author><author><keyname>Nomura</keyname><forenames>Ryo</forenames></author></authors><title>First- and Second-Order Coding Theorems for Mixed Memoryless Channels
  with General Mixture</title><categories>cs.IT math.IT</categories><comments>28 pages; submitted to IEEE Trans. on Information Theory, Jan. 2015.
  A conference version of this paper is presented at ISIT2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the first- and second-order maximum achievable rates
of codes with/without cost constraints for mixed channels whose channel law is
characterized by a general mixture of (at most) uncountably many stationary and
memoryless discrete channels. These channels are referred to as general mixed
memoryless channels and include the class of mixed memoryless channels of
finitely or countably memoryless channels as a special case. For general mixed
memoryless channels, the first-order coding theorem which gives a formula for
the $\varepsilon$-capacity is established, and then a direct part of the
second-order coding theorem is provided. A subclass of general mixed memoryless
channels whose component channels can be ordered according to their capacity is
introduced, and the first- and second-order coding theorems are established. It
is shown that the established formulas reduce to several known formulas for
restricted scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05892</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05892</id><created>2015-01-23</created><updated>2015-04-12</updated><authors><author><keyname>Rush</keyname><forenames>Cynthia</forenames></author><author><keyname>Greig</keyname><forenames>Adam</forenames></author><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author></authors><title>Capacity-achieving Sparse Superposition Codes via Approximate Message
  Passing Decoding</title><categories>cs.IT math.IT stat.ML</categories><comments>46 pages. Added discussion of faster decoder using a Hadamard design
  matrix and its empirical performance. A shorter version to appear in ISIT
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse superposition codes were recently introduced by Barron and Joseph for
reliable communication over the AWGN channel at rates approaching the channel
capacity. The codebook is defined in terms of a Gaussian design matrix, and
codewords are sparse linear combinations of columns of the matrix. In this
paper, we propose an approximate message passing decoder for sparse
superposition codes, whose decoding complexity scales linearly with the size of
the design matrix. The performance of the decoder is rigorously analyzed and it
is shown to asymptotically achieve the AWGN capacity with an appropriate power
allocation. Simulation results are provided to demonstrate the performance of
the decoder at finite blocklengths. We introduce a power allocation scheme to
improve the empirical performance, and demonstrate how the decoding complexity
can be significantly reduced by using Hadamard design matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05902</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05902</id><created>2015-01-23</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author><author><keyname>Kissling</keyname><forenames>Christian</forenames></author><author><keyname>Berioli</keyname><forenames>Matteo</forenames></author></authors><title>Sliding window-based Contention Resolution Diversity Slotted ALOHA</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Global Communications Conference (GLOBECOM), 2012</comments><doi>10.1109/GLOCOM.2012.6503624</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contention Resolution Diversity Slotted ALOHA (CRDSA) and its burst degree
optimizations (CRDSA++, IRSA) make use of MAC burst repetitions and
Interference Cancellation (IC) making possible to reach throughput values as
high as $T \simeq 0.8$ in practical implementations, whereas for the
traditional slotted ALOHA $T \simeq 0.37$. However, these new techniques
introduce a frame-based access to the channel that limits the performance in
terms of throughput and packet delivery delay. In this paper, a new technique
named Sliding Window CRDSA (SW-CRDSA) and its counterpart for irregular
repetitions (SW-IRSA) are introduced in order to exploit the advantages of MAC
burst repetition and Interference Cancellation (IC) with an unframed access
scheme. Numerical results are also provided in order to validate the statement
of better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05916</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05916</id><created>2014-11-19</created><authors><author><keyname>Qamar</keyname><forenames>Nafees</forenames></author><author><keyname>Yang</keyname><forenames>Yilong</forenames></author><author><keyname>Nadas</keyname><forenames>Andras</forenames></author><author><keyname>Liu</keyname><forenames>Zhiming</forenames></author><author><keyname>Sztipanovits</keyname><forenames>Janos</forenames></author></authors><title>Anonymously Analyzing Clinical Datasets</title><categories>cs.SE cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper takes on the problem of automatically identifying
clinically-relevant patterns in medical datasets without compromising patient
privacy. To achieve this goal, we treat datasets as a black box for both
internal and external users of data that lets us handle clinical data queries
directly and far more efficiently. The novelty of the approach lies in avoiding
the data de-identification process often used as a means of preserving patient
privacy. The implemented toolkit combines software engineering technologies
such as Java EE and RESTful web services, to allow exchanging medical data in
an unidentifiable XML format as well as restricting users to the need-to-know
principle. Our technique also inhibits retrospective processing of data, such
as attacks by an adversary on a medical dataset using advanced computational
methods to reveal Protected Health Information (PHI). The approach is validated
on an endoscopic reporting application based on openEHR and MST standards. From
the usability perspective, the approach can be used to query datasets by
clinical researchers, governmental or non-governmental organizations in
monitoring health care services to improve quality of care.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05917</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05917</id><created>2015-01-05</created><authors><author><keyname>Subbotin</keyname><forenames>Igor Yakov</forenames></author></authors><title>On Generalized Rectangular Fuzzy Model for Assessment</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1404.7279 by other authors</comments><msc-class>03B52, 03E72, 97M10, 97M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article is dedicated to the analysis of the existing models for
assessment based of the fuzzy logic centroid technique. A new Generalized
Rectangular Model were developed. Some generalizations of the existing models
are offered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05921</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05921</id><created>2015-01-23</created><authors><author><keyname>Deutschmann</keyname><forenames>Emanuel</forenames></author></authors><title>The Spatial Structure of Transnational Human Activity</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that the spatial structures of animal displacements
and local-scale human motion follow L\'{e}vy flights. Whether transnational
human activity (THA) also exhibits such a pattern has however not been
thoroughly examined as yet. To fill this gap, this article examines the
planet-scale spatial structure of THA (a) across eight types of mobility and
communication and (b) in its development over time. Combining data from various
sources, it is shown that the spatial structure of THA can indeed be
approximated by L\'{e}vy flights with heavy tails that obey power laws. Scaling
exponent and power-law fit differ by type of THA, being highest in
refuge-seeking and tourism and lowest in student exchange. Variance in the
availability of resources and opportunities for satisfying associated needs
appears to explain these differences. Over time, the L\'{e}vy-flight pattern
remains intact and remarkably stable, contradicting the popular idea that
socio-technological trends lead to a &quot;death of distance.&quot; Longitudinal change
occurs only in some types of THA and predominantly at short distances,
indicating regional shifts rather than globalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05924</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05924</id><created>2015-01-23</created><updated>2015-02-03</updated><authors><author><keyname>Harel</keyname><forenames>David</forenames></author><author><keyname>Katz</keyname><forenames>Guy</forenames></author><author><keyname>Marelly</keyname><forenames>Rami</forenames></author><author><keyname>Marron</keyname><forenames>Assaf</forenames></author></authors><title>Wise Computing: Towards Endowing System Development with True Wisdom</title><categories>cs.SE</categories><acm-class>D.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encouraged by significant advances in algorithms and tools for verification
and analysis, high level modeling and programming techniques, natural language
programming, etc., we feel it is time for a major change in the way complex
software and systems are developed. We present a vision that will shift the
power balance between human engineers and the development and runtime
environments. The idea is to endow the computer with human-like wisdom - not
general wisdom, and not AI in the standard sense of the term - but wisdom
geared towards classical system-building, which will be manifested, throughout
development, in creativity and proactivity, and deep insights into the system's
own structure and behavior, its overarching goals and rationale. Ideally, the
computer will join the development team as an equal partner - knowledgeable,
concerned, and responsibly active. We present a running demo of our initial
efforts on the topic, illustrating on a small example what we feel is the
feasibility of the ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05927</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05927</id><created>2015-01-23</created><updated>2015-01-26</updated><authors><author><keyname>Wang</keyname><forenames>Z.</forenames></author><author><keyname>Chini</keyname><forenames>A.</forenames></author><author><keyname>Kilani</keyname><forenames>M.</forenames></author><author><keyname>Zhou</keyname><forenames>J.</forenames></author></authors><title>Multiple-Symbol Interleaved RS Codes and Two-Pass Decoding Algorithm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For communication systems with heavy burst noise, an optimal Forward Error
Correction (FEC) scheme is expected to have a large burst error correction
capacity while simultaneously owning moderate random error correction
capability. This letter presents a new FEC scheme based on multiple-symbol
interleaved Reed-Solomon codes and an associated two-pass decoding algorithm.
It is shown that the proposed multi-symbol interleaved coding scheme can
achieve nearly twice as much as the burst error correction capability of
conventional symbol-interleaved Reed-Solomon codes with the same code length
and code rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05930</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05930</id><created>2015-01-23</created><updated>2016-03-02</updated><authors><author><keyname>Chandar</keyname><forenames>Venkat</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>How to Quickly Detect a Change while Sleeping Almost all the Time</title><categories>cs.IT math.IT stat.AP</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change-point detection procedures usually operate under full sampling; all
observations are available until when the change is declared. Is this
necessary? This paper addresses this question and investigates the minimum
sampling rate needed in order to detect a transient change as efficiently as
under full sampling.
  The problem is cast into a Bayesian setup where the change, assumed to be of
fixed known duration $n$, starts at a time $\nu$ that is uniformly distributed
within time interval $\{1,2,\ldots,A_n=2^{\alpha n}\}$ for some known parameter
$\alpha&gt;0$. It is assumed that observations are i.i.d. $P_0$ except during the
change period where it is i.i.d. $P_1$. It is shown that, for any fixed $\alpha
\in (0,D(P_1||P_0))$, where $D(P_1||P_0)$ denotes the relative entropy between
$P_1$ and $P_0$, as long as the sampling rate is of order $\omega(1/n)$ the
change can be detected with the same minimum, linear in $n$, delay as under
full sampling, in the limit of vanishing false-alarm probability as $n\to
\infty$. This is achieved by means of a new multi-zoom detection procedure with
the following freatures. It agressively rejects and skips samples generated
according to $P_0$ while it sharply increases the sampling rate when a change
occurs thereby minimizing detection delay. Conversely, if $\alpha&gt;D(P_1||P_0) $
or if the sampling rate is $o(1/n)$ the delay is $\Theta(A_n=2^{\alpha n})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05936</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05936</id><created>2015-01-23</created><authors><author><keyname>Malik</keyname><forenames>Avinash</forenames></author><author><keyname>Roop</keyname><forenames>Partha</forenames></author></authors><title>A unified framework for modeling and implementation of hybrid systems
  with synchronous controllers</title><categories>cs.SY cs.PL</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to including non-instantaneous discrete
control transitions in the linear hybrid automaton approach to simulation and
verification of hybrid control systems. In this paper we study the control of a
continuously evolving analog plant using a controller programmed in a
synchronous programming language. We provide extensions to the synchronous
subset of the SystemJ programming language for modeling, implementation, and
verification of such hybrid systems. We provide a sound rewrite semantics that
approximate the evolution of the continuous variables in the discrete domain
inspired from the classical supervisory control theory. The resultant discrete
time model can be verified using classical model-checking tools. Finally, we
show that systems designed using our approach have a higher fidelity than the
ones designed using the hybrid automaton approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05940</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05940</id><created>2015-01-22</created><authors><author><keyname>Rachad</keyname><forenames>T.</forenames></author><author><keyname>Boutahar</keyname><forenames>J.</forenames></author><author><keyname>ghazi</keyname><forenames>S. El</forenames></author></authors><title>A New Efficient Method for Calculating Similarity Between Web Services</title><categories>cs.AI cs.CL cs.IR cs.SE</categories><comments>7 pages, 4 figures, 8 tables, International Journal of Advanced
  Computer Science and Applications (IJACSA),Vol. 5, No. 8, 2014</comments><acm-class>D.1.3; D.2.12; D.3.1; H.3.5</acm-class><doi>10.14569/IJACSA.2014.050809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web services allow communication between heterogeneous systems in a
distributed environment. Their enormous success and their increased use led to
the fact that thousands of Web services are present on the Internet. This
significant number of Web services which not cease to increase has led to
problems of the difficulty in locating and classifying web services, these
problems are encountered mainly during the operations of web services discovery
and substitution. Traditional ways of search based on keywords are not
successful in this context, their results do not support the structure of Web
services and they consider in their search only the identifiers of the web
service description language (WSDL) interface elements. The methods based on
semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web
service with a semantic description allow raising partially this problem, but
their complexity and difficulty delays their adoption in real cases. Measuring
the similarity between the web services interfaces is the most suitable
solution for this kind of problems, it will classify available web services so
as to know those that best match the searched profile and those that do not
match. Thus, the main goal of this work is to study the degree of similarity
between any two web services by offering a new method that is more effective
than existing works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05943</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05943</id><created>2015-01-25</created><updated>2015-01-29</updated><authors><author><keyname>Wu</keyname><forenames>Chenmiao</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>Bit-oriented quantum public-key encryption</title><categories>quant-ph cs.CR</categories><comments>17 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a bit-oriented quantum public-key scheme which uses Boolean
function as private-key and randomly changed pairs of quantum state and
classical string as public-keys. Contrast to the typical classical public-key
scheme, one private-key in our scheme corresponds to an exponential number of
public-keys. The goal of our scheme is to achieve information-theoretic
security, and the security analysis is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05956</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05956</id><created>2015-01-23</created><updated>2015-12-01</updated><authors><author><keyname>Gleeson</keyname><forenames>James P.</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Kevin P.</forenames></author><author><keyname>Ba&#xf1;os</keyname><forenames>Raquel A.</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Determinants of Meme Popularity</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>Reformatted. 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social media have greatly affected the way in which we communicate
with each other. However, little is known about what are the fundamental
mechanisms driving dynamical information flow in online social systems. Here,
we introduce a generative model for online sharing behavior that is
analytically tractable and which can reproduce several characteristics of
empirical micro-blogging data on hashtag usage, such as (time-dependent)
heavy-tailed distributions of meme popularity. The presented framework
constitutes a null model for social spreading phenomena which, in contrast to
purely empirical studies or simulation-based models, clearly distinguishes the
roles of two distinct factors affecting meme popularity: the memory time of
users and the connectivity structure of the social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05963</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05963</id><created>2015-01-23</created><updated>2015-08-02</updated><authors><author><keyname>Yoon</keyname><forenames>Man-Ki</forenames></author><author><keyname>Mohan</keyname><forenames>Sibin</forenames></author><author><keyname>Choi</keyname><forenames>Jaesik</forenames></author><author><keyname>Christodorescu</keyname><forenames>Mihai</forenames></author><author><keyname>Sha</keyname><forenames>Lui</forenames></author></authors><title>Learning Execution Contexts from System Call Distributions for Intrusion
  Detection in Embedded Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing techniques used for intrusion detection do not fully utilize the
intrinsic properties of embedded systems. In this paper, we propose a
lightweight method for detecting anomalous executions using a distribution of
system call frequencies. We use a cluster analysis to learn the legitimate
execution contexts of embedded applications and then monitor them at run-time
to capture abnormal executions. We also present an architectural framework with
minor processor modifications to aid in this process. Our prototype shows that
the proposed method can effectively detect anomalous executions without relying
on sophisticated analyses or affecting the critical execution paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05964</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05964</id><created>2015-01-23</created><authors><author><keyname>Cheng</keyname><forenames>Guangchun</forenames></author><author><keyname>Wan</keyname><forenames>Yiwen</forenames></author><author><keyname>Saudagar</keyname><forenames>Abdullah N.</forenames></author><author><keyname>Namuduri</keyname><forenames>Kamesh</forenames></author><author><keyname>Buckles</keyname><forenames>Bill P.</forenames></author></authors><title>Advances in Human Action Recognition: A Survey</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Human action recognition has been an important topic in computer vision due
to its many applications such as video surveillance, human machine interaction
and video retrieval. One core problem behind these applications is
automatically recognizing low-level actions and high-level activities of
interest. The former is usually the basis for the latter. This survey gives an
overview of the most recent advances in human action recognition during the
past several years, following a well-formed taxonomy proposed by a previous
survey. From this state-of-the-art survey, researchers can view a panorama of
progress in this area for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05970</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05970</id><created>2015-01-23</created><authors><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Wang</keyname><forenames>Yin</forenames></author><author><keyname>Wang</keyname><forenames>Honggang</forenames></author><author><keyname>Hua</keyname><forenames>Kun</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Shen</keyname><forenames>Ju</forenames></author></authors><title>Automatic Objects Removal for Scene Completion</title><categories>cs.CV</categories><comments>6 pages, IEEE International Conference on Computer Communications
  (INFOCOM 14), Workshop on Security and Privacy in Big Data, Toronto, Canada,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of web-based cameras and mobile devices, billions
of photographs are uploaded to the internet. We can trivially collect a huge
number of photo streams for various goals, such as 3D scene reconstruction and
other big data applications. However, this is not an easy task due to the fact
the retrieved photos are neither aligned nor calibrated. Furthermore, with the
occlusion of unexpected foreground objects like people, vehicles, it is even
more challenging to find feature correspondences and reconstruct realistic
scenes. In this paper, we propose a structure based image completion algorithm
for object removal that produces visually plausible content with consistent
structure and scene texture. We use an edge matching technique to infer the
potential structure of the unknown region. Driven by the estimated structure,
texture synthesis is performed automatically along the estimated curves. We
evaluate the proposed method on different types of images: from highly
structured indoor environment to the natural scenes. Our experimental results
demonstrate satisfactory performance that can be potentially used for
subsequent big data processing: 3D scene reconstruction and location
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05971</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05971</id><created>2015-01-23</created><updated>2015-12-13</updated><authors><author><keyname>Rebollo-Neira</keyname><forenames>Laura</forenames></author></authors><title>Cooperative Greedy Pursuit Strategies for Sparse Signal Representation
  by Partitioning</title><categories>cs.DS</categories><comments>A library of routines for implementing the proposed methods, as well
  as scripts to reproduce the examples in the manuscript, is available on the
  website http://www.nonlinear-approx.info/examples/node01.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative Greedy Pursuit Strategies are considered for approximating a
signal partition subjected to a global constraint on sparsity. The approach
aims at producing a high quality sparse approximation of the whole signal,
using highly coherent redundant dictionaries. The cooperation takes place by
ranking the partition units for their sequential stepwise approximation, and is
realized by means of i)forward steps for the upgrading of an approximation
and/or ii) backward steps for the corresponding downgrading. The advantage of
the strategy is illustrated by producing high quality approximations of music
signals using redundant trigonometric dictionaries. In addition to rendering
stunning improvements in sparsity with respect to the concomitant trigonometric
basis, these dictionaries enable a fast implementation of the approach via the
Fast Fourier Transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05973</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05973</id><created>2015-01-23</created><updated>2015-01-27</updated><authors><author><keyname>Kapoor</keyname><forenames>Ashish</forenames></author><author><keyname>Frady</keyname><forenames>E. Paxon</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Kristan</keyname><forenames>William B.</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author></authors><title>Inferring and Learning from Neuronal Correspondences</title><categories>q-bio.NC cs.AI cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study methods for inferring and learning from
correspondences among neurons. The approach enables alignment of data from
distinct multiunit studies of nervous systems. We show that the methods for
inferring correspondences combine data effectively from cross-animal studies to
make joint inferences about behavioral decision making that are not possible
with the data from a single animal. We focus on data collection, machine
learning, and prediction in the representative and long-studied invertebrate
nervous system of the European medicinal leech. Acknowledging the computational
intractability of the general problem of identifying correspondences among
neurons, we introduce efficient computational procedures for matching neurons
across animals. The methods include techniques that adjust for missing cells or
additional cells in the different data sets that may reflect biological or
experimental variation. The methods highlight the value harnessing inference
and learning in new kinds of computational microscopes for multiunit
neurobiological studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05976</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05976</id><created>2015-01-23</created><authors><author><keyname>McAndrew</keyname><forenames>Thomas C.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author></authors><title>Robustness of Spatial Micronetworks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>15 pages, 8 figures</comments><journal-ref>Phys. Rev. E 91, 042813 2015</journal-ref><doi>10.1103/PhysRevE.91.042813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power lines, roadways, pipelines and other physical infrastructure are
critical to modern society. These structures may be viewed as spatial networks
where geographic distances play a role in the functionality and construction
cost of links. Traditionally, studies of network robustness have primarily
considered the connectedness of large, random networks. Yet for spatial
infrastructure physical distances must also play a role in network robustness.
Understanding the robustness of small spatial networks is particularly
important with the increasing interest in microgrids, small-area distributed
power grids that are well suited to using renewable energy resources. We study
the random failures of links in small networks where functionality depends on
both spatial distance and topological connectedness. By introducing a
percolation model where the failure of each link is proportional to its spatial
length, we find that, when failures depend on spatial distances, networks are
more fragile than expected. Accounting for spatial effects in both construction
and robustness is important for designing efficient microgrids and other
network infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05978</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05978</id><created>2015-01-23</created><authors><author><keyname>Randriambololona</keyname><forenames>Hugues</forenames></author></authors><title>Linear independence of rank 1 matrices and the dimension of *-products
  of codes</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that with high probability, random rank 1 matrices over a finite
field are in (linearly) general position, at least provided their shape k x l
is not excessively unbalanced. This translates into saying that the dimension
of the *-product of two [n, k] and [n, l] random codes is equal to min(n, kl),
as one would have expected. Our work is inspired by a similar result of
Cascudo-Cramer-Mirandola-Zemor dealing with *-squares of codes, which it
complements, especially regarding applications to the analysis of McEliece-type
cryptosystems. We also briefly mention the case of higher *-powers, which
require to take the Frobenius into account. We then conclude with some open
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05980</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05980</id><created>2015-01-23</created><authors><author><keyname>Semiari</keyname><forenames>Omid</forenames></author><author><keyname>Maham</keyname><forenames>Behrouz</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>On the Effect of I/Q Imbalance on Energy Detection and a Novel
  Four-Level Hypothesis Spectrum Sensing</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Vehicular Technology (TVT), vol. 63, no. 8, pp.
  4136 - 4141, Feb. 2014</comments><doi>10.1109/TVT.2014.2306684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct-conversion transceivers are in demand, due to low implementation cost
of analog front-ends. However, these transmitters or receivers introduce
imperfections such as in-phase and quadrature-phase (I/Q) imbalances. In this
paper, we first investigate the effect of I/Q imbalance on the performance of
primary system, and show that these impairments can severely degrade the
performance of cognitive radio system that are based on orthogonal frequency
division multiplexing (OFDM) multiple access scheme. Next, we design a new
four-level hypothesis blind detector for spectrum sensing in such cognitive
radio system, and show that the proposed detector is less vulnerable to I/Q
imbalance than conventional two-level detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05983</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05983</id><created>2015-01-23</created><authors><author><keyname>Boutahar</keyname><forenames>J.</forenames></author><author><keyname>Rachad</keyname><forenames>T.</forenames></author><author><keyname>ghazi</keyname><forenames>S. El</forenames></author></authors><title>A new efficient Matching method for web services substitution</title><categories>cs.IR cs.DC</categories><comments>9 pages, 11 figures, 2 tables</comments><acm-class>D.1.3; D.2.12; D.3.1; H.3.5</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 5, No 2,196-204, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The internet is considered as the most extensive market in the world. To keep
its gradual reputation, it must confront real problems that result from its
distribution and from the diversity of the protocols used to insure
communications. The Web service technology has diminished significantly the
effects of distribution and heterogeneity, but there are several problems that
weaken their performance (unavailability, load increase of use, high cost of
CPU time...). Faced with this situation, we are forced to move in the direction
of the substitution of web services. In this context, we propose an effective
technique of substitution based on a new method of matching that allows
detecting and expressing the matching between the web services pairwise by
considering that each of them is ontology. Also, our method performs a
discovery of the most similar web service to that to be replaced by using an
efficient method of similarity measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05990</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05990</id><created>2015-01-23</created><authors><author><keyname>Shakarian</keyname><forenames>Jana</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Ruef</keyname><forenames>Andrew</forenames></author></authors><title>Cyber Attacks and Public Embarrassment: A Survey of Some Notable Hacks</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We hear it all too often in the media: an organization is attacked, its data,
often containing personally identifying information, is made public, and a
hacking group emerges to claim credit. In this excerpt, we discuss how such
groups operate and describe the details of a few major cyber-attacks of this
sort in the wider context of how they occurred. We feel that understanding how
such groups have operated in the past will give organizations ideas of how to
defend against them in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05992</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05992</id><created>2015-01-23</created><authors><author><keyname>Ord</keyname><forenames>S. M.</forenames></author><author><keyname>Crosse</keyname><forenames>B.</forenames></author><author><keyname>Emrich</keyname><forenames>D.</forenames></author><author><keyname>Pallot</keyname><forenames>D.</forenames></author><author><keyname>Wayth</keyname><forenames>R. B.</forenames></author><author><keyname>Clark</keyname><forenames>M. A.</forenames></author><author><keyname>Tremblay</keyname><forenames>S. E.</forenames></author><author><keyname>Arcus</keyname><forenames>W.</forenames></author><author><keyname>Barnes</keyname><forenames>D.</forenames></author><author><keyname>Bell</keyname><forenames>M.</forenames></author><author><keyname>Bernardi</keyname><forenames>G.</forenames></author><author><keyname>Bhat</keyname><forenames>N. D. R.</forenames></author><author><keyname>Bowman</keyname><forenames>J. D.</forenames></author><author><keyname>Briggs</keyname><forenames>F.</forenames></author><author><keyname>Bunton</keyname><forenames>J. D.</forenames></author><author><keyname>Cappallo</keyname><forenames>R. J.</forenames></author><author><keyname>Corey</keyname><forenames>B. E.</forenames></author><author><keyname>Deshpande</keyname><forenames>A. A.</forenames></author><author><keyname>deSouza</keyname><forenames>L.</forenames></author><author><keyname>Ewell-Wice</keyname><forenames>A.</forenames></author><author><keyname>Feng</keyname><forenames>L.</forenames></author><author><keyname>Goeke</keyname><forenames>R.</forenames></author><author><keyname>Greenhill</keyname><forenames>L. J.</forenames></author><author><keyname>Hazelton</keyname><forenames>B. J.</forenames></author><author><keyname>Herne</keyname><forenames>D.</forenames></author><author><keyname>Hewitt</keyname><forenames>J. N.</forenames></author><author><keyname>Hindson</keyname><forenames>L.</forenames></author><author><keyname>Hurley-Walker</keyname><forenames>H.</forenames></author><author><keyname>Jacobs</keyname><forenames>D.</forenames></author><author><keyname>Johnston-Hollitt</keyname><forenames>M.</forenames></author><author><keyname>Kaplan</keyname><forenames>D. L.</forenames></author><author><keyname>Kasper</keyname><forenames>J. C.</forenames></author><author><keyname>Kincaid</keyname><forenames>B. B.</forenames></author><author><keyname>Koenig</keyname><forenames>R.</forenames></author><author><keyname>Kratzenberg</keyname><forenames>E.</forenames></author><author><keyname>Kudryavtseva</keyname><forenames>N.</forenames></author><author><keyname>Lenc</keyname><forenames>E.</forenames></author><author><keyname>Lonsdale</keyname><forenames>C. J.</forenames></author><author><keyname>Lynch</keyname><forenames>M. J.</forenames></author><author><keyname>McKinley</keyname><forenames>B.</forenames></author><author><keyname>McWhirter</keyname><forenames>S. R.</forenames></author><author><keyname>Mitchell</keyname><forenames>D. A.</forenames></author><author><keyname>Morales</keyname><forenames>M. F.</forenames></author><author><keyname>Morgan</keyname><forenames>E.</forenames></author><author><keyname>Oberoi</keyname><forenames>D.</forenames></author><author><keyname>Offringa</keyname><forenames>A.</forenames></author><author><keyname>Pathikulangara</keyname><forenames>J.</forenames></author><author><keyname>Pindor</keyname><forenames>B.</forenames></author><author><keyname>Prabu</keyname><forenames>T.</forenames></author><author><keyname>Procopio</keyname><forenames>P.</forenames></author><author><keyname>Remillard</keyname><forenames>R. A.</forenames></author><author><keyname>Riding</keyname><forenames>J.</forenames></author><author><keyname>Rogers</keyname><forenames>A. E. E.</forenames></author><author><keyname>Roshi</keyname><forenames>A.</forenames></author><author><keyname>Salah</keyname><forenames>J. E.</forenames></author><author><keyname>Sault</keyname><forenames>R. J.</forenames></author><author><keyname>Shankar</keyname><forenames>N. Udaya</forenames></author><author><keyname>Srivani</keyname><forenames>K. S.</forenames></author><author><keyname>Stevens</keyname><forenames>J.</forenames></author><author><keyname>Subrahmanyan</keyname><forenames>R.</forenames></author><author><keyname>Tingay</keyname><forenames>S. J.</forenames></author><author><keyname>Waterson</keyname><forenames>M.</forenames></author><author><keyname>Webster</keyname><forenames>R. L.</forenames></author><author><keyname>Whitney</keyname><forenames>A. R.</forenames></author><author><keyname>Williams</keyname><forenames>A.</forenames></author><author><keyname>Williams</keyname><forenames>C. L.</forenames></author><author><keyname>Wyithe</keyname><forenames>J. S. B.</forenames></author></authors><title>The Murchison Widefield Array Correlator</title><categories>astro-ph.IM cs.CE</categories><comments>17 pages, 9 figures. Accepted for publication in PASA. Some figures
  altered to meet astro-ph submission requirements</comments><doi>10.1017/pasa.2015.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Murchison Widefield Array (MWA) is a Square Kilometre Array (SKA)
Precursor. The telescope is located at the Murchison Radio--astronomy
Observatory (MRO) in Western Australia (WA). The MWA consists of 4096 dipoles
arranged into 128 dual polarisation aperture arrays forming a connected element
interferometer that cross-correlates signals from all 256 inputs. A hybrid
approach to the correlation task is employed, with some processing stages being
performed by bespoke hardware, based on Field Programmable Gate Arrays (FPGAs),
and others by Graphics Processing Units (GPUs) housed in general purpose rack
mounted servers. The correlation capability required is approximately 8 TFLOPS
(Tera FLoating point Operations Per Second). The MWA has commenced operations
and the correlator is generating 8.3 TB/day of correlation products, that are
subsequently transferred 700 km from the MRO to Perth (WA) in real-time for
storage and offline processing. In this paper we outline the correlator design,
signal path, and processing elements and present the data format for the
internal and external interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05994</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.05994</id><created>2015-01-23</created><updated>2015-02-25</updated><authors><author><keyname>Bustin</keyname><forenames>Ronit</forenames></author><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>On MMSE Properties of Codes for the Gaussian Broadcast and Wiretap
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns the behavior of &quot;good&quot; (capacity achieving) codes in
several multi-user settings in the Gaussian regime, in terms of their minimum
mean-square error (MMSE) behavior. The settings investigated in this context
include the Gaussian wiretap channel, the Gaussian broadcast channel (BC) and
the Gaussian BC with confidential messages (BCC). In particular this work
addresses the effects of transmitting such codes on unintended receivers, that
is, receivers that neither require reliable decoding of the transmitted
messages nor are they eavesdroppers that must be kept ignorant, to some extent,
of the transmitted message. This work also examines the effect on the capacity
region that occurs when we limit the allowed disturbance in terms of MMSE on
some unintended receiver. This trade-off between the capacity region and the
disturbance constraint is given explicitly for the Gaussian BC and the secrecy
capacity region of the Gaussian BCC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06003</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06003</id><created>2015-01-23</created><updated>2016-02-14</updated><authors><author><keyname>Ghasemi</keyname><forenames>Hooshang</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>Improved Lower Bounds for Coded Caching</title><categories>cs.IT math.IT</categories><comments>31 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content delivery networks often employ caching to reduce transmission rates
from the central server to the end users. Recently, the technique of coded
caching was introduced whereby coding in the caches and coded transmission
signals from the central server are considered. Prior results in this area
demonstrate that carefully designing the placement of content in the caches and
designing appropriate coded delivery signals from the server allow for a system
where the delivery rates can be significantly smaller than conventional
schemes. However, matching upper and lower bounds on the transmission rate have
not yet been obtained. In this work, we derive tighter lower bounds on the
coded caching rate than were known previously. We demonstrate that this problem
can equivalently be posed as a combinatorial problem of optimally labeling the
leaves of a directed tree. Our proposed labeling algorithm allows for
significantly improved lower bounds on the coded caching rate. Furthermore, we
study certain structural properties of our algorithm that allow us to
analytically quantify improvements on the rate lower bound for general values
of the problem parameters. This allows us to obtain a multiplicative gap of at
most four between the achievable rate and our lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06005</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06005</id><created>2015-01-24</created><authors><author><keyname>Akazaki</keyname><forenames>Takumi</forenames><affiliation>The University of Tokyo</affiliation></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames><affiliation>The University of Tokyo</affiliation></author><author><keyname>Suenaga</keyname><forenames>Kohei</forenames><affiliation>Kyoto University</affiliation></author></authors><title>Input Synthesis for Sampled Data Systems by Program Logic</title><categories>cs.SY</categories><comments>In Proceedings HAS 2014, arXiv:1501.05405</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 174, 2015, pp. 22-39</journal-ref><doi>10.4204/EPTCS.174.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a concrete industry problem we consider the input synthesis
problem for hybrid systems: given a hybrid system that is subject to input from
outside (also called disturbance or noise), find an input sequence that steers
the system to the desired postcondition. In this paper we focus on sampled data
systems--systems in which a digital controller interrupts a physical plant in a
periodic manner, a class commonly known in control theory--and furthermore
assume that a controller is given in the form of an imperative program. We
develop a structural approach to input synthesis that features forward and
backward reasoning in program logic for the purpose of reducing a search space.
Although the examples we cover are limited both in size and in structure,
experiments with a prototype implementation suggest potential of our program
logic based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06006</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06006</id><created>2015-01-24</created><authors><author><keyname>Leth</keyname><forenames>John</forenames></author><author><keyname>Wisniewski</keyname><forenames>Rafael</forenames></author><author><keyname>Rasmussen</keyname><forenames>Jakob</forenames></author><author><keyname>Schioler</keyname><forenames>Henrik</forenames></author></authors><title>Stochastic Analysis of Synchronization in a Supermarket Refrigeration
  System</title><categories>cs.SY</categories><comments>In Proceedings HAS 2014, arXiv:1501.05405</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 174, 2015, pp. 40-50</journal-ref><doi>10.4204/EPTCS.174.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Display cases in supermarket systems often exhibit synchronization, in which
the expansion valves in the display cases turn on and off at exactly the same
time. The study of the influence of switching noise on synchronization in
supermarket refrigeration systems is the subject matter of this work. For this
purpose, we model it as a hybrid system, for which synchronization corresponds
to a periodic trajectory. Subsequently, we investigate the influence of
switching noise. We develop a statistical method for computing an intensity
function, which measures how often the refrigeration system stays synchronized.
By analyzing the intensity, we conclude that the increase in measurement
uncertainty yields the decrease at the prevalence of synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06009</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06009</id><created>2015-01-24</created><updated>2015-02-06</updated><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>Are Effective Leaders Creative?</title><categories>cs.MA</categories><comments>5 pages in Psychology Today (online). (2010).
  https://www.psychologytoday.com/blog/mindbloggling/201102/are-effective-leaders-creative</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explains in layperson's terms how an agent-based model was used to
investigate the widely held belief that creativity is an important component of
effective leadership. Creative leadership was found to increase the mean
fitness of cultural outputs across an artificial society, but the more creative
the followers were, the greater the extent to which the beneficial effect of
creative leadership was washed out. Early in a run when the fitness of ideas
was low, a form of leadership that entails the highest possible degree of
creativity was best for the mean fitness of outputs across the society. As the
mean fitness of outputs increased a transition inevitably occurs after which
point a less creative style of leadership proved most effective. Implications
of these findings are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06026</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06026</id><created>2015-01-24</created><authors><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Energy Harvesting Wireless Communications: A Review of Recent Advances</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Journal of Selected Areas in Communications
  (Special Issue: Wireless Communications Powered by Energy Harvesting and
  Wireless Energy Transfer)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article summarizes recent contributions in the broad area of energy
harvesting wireless communications. In particular, we provide the current state
of the art for wireless networks composed of energy harvesting nodes, starting
from the information-theoretic performance limits to transmission scheduling
policies and resource allocation, medium access and networking issues. The
emerging related area of energy transfer for self-sustaining energy harvesting
wireless networks is considered in detail covering both energy cooperation
aspects and simultaneous energy and information transfer. Various potential
models with energy harvesting nodes at different network scales are reviewed as
well as models for energy consumption at the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06027</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06027</id><created>2015-01-24</created><updated>2015-05-14</updated><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Pascal</keyname><forenames>Frederic</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Optimal Design of the Adaptive Normalized Matched Filter Detector</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses improvements on the design of the adaptive normalized
matched filter (ANMF) for radar detection. It is well-acknowledged that the
estimation of the noise-clutter covariance matrix is a fundamental step in
adaptive radar detection. In this paper, we consider regularized estimation
methods which force by construction the eigenvalues of the scatter estimates to
be greater than a positive regularization parameter rho. This makes them more
suitable for high dimensional problems with a limited number of secondary data
samples than traditional sample covariance estimates. While an increase of rho
seems to improve the conditioning of the estimate, it might however cause it to
significantly deviate from the true covariance matrix. The setting of the
optimal regularization parameter is a difficult question for which no
convincing answers have thus far been provided. This constitutes the major
motivation behind our work. More specifically, we consider the design of the
ANMF detector for two kinds of regularized estimators, namely the regularized
sample covariance matrix (RSCM), appropriate when the clutter follows a
Gaussian distribution and the regularized Tyler estimator (RTE) for
non-Gaussian spherically invariant distributed clutters. Based on recent random
matrix theory results studying the asymptotic fluctuations of the statistics of
the ANMF detector when the number of samples and their dimension grow together
to infinity, we propose a design for the regularization parameter that
maximizes the detection probability under constant false alarm rates.
Simulation results which support the efficiency of the proposed method are
provided in order to illustrate the gain of the proposed optimal design over
conventional settings of the regularization parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06032</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06032</id><created>2015-01-24</created><authors><author><keyname>Cheng</keyname><forenames>Yi</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Gain-scheduled Leader-follower Tracking Control for Interconnected
  Parameter Varying Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the gain-scheduled leader-follower tracking control
problem for a parameter varying complex interconnected system with directed
communication topology and uncertain norm-bounded coupling between the agents.
A gain-scheduled consensus-type control protocol is proposed and a sufficient
condition is obtained which guarantees a suboptimal bound on the system
tracking performance under this protocol. An interpolation technique is used to
obtain a protocol schedule which is continuous in the scheduling parameter. The
effectiveness of the proposed method is demonstrated using a simulation
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06035</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06035</id><created>2015-01-24</created><authors><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author><author><keyname>Willms</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Barker sequences of odd length</title><categories>math.CO cs.IT math.IT</categories><comments>6 pages, this note supersedes the main result of arXiv:1409.1434</comments><msc-class>11B83, 05B10, 94A55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Barker sequence is a binary sequence for which all nontrivial aperiodic
autocorrelations are at most 1 in magnitude. An old conjecture due to Turyn
asserts that there is no Barker sequence of length greater than 13. In 1961,
Turyn and Storer gave an elementary, though somewhat complicated, proof that
this conjecture holds for odd lengths. We give a new and simpler proof of this
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06042</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06042</id><created>2015-01-24</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>Tsallis entropy of complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How complex of the complex networks has attracted many researchers to explore
it. The entropy is an useful method to describe the degree of the $complex$ of
the complex networks. In this paper, a new method which is based on the Tsallis
entropy is proposed to describe the $complex$ of the complex networks. The
results in this paper show that the complex of the complex networks not only
decided by the structure property of the complex networks, but also influenced
by the relationship between each nodes. In other word, which kinds of nodes are
chosen as the main part of the complex networks will influence the value of the
entropy of the complex networks. The value of q in the Tsallis entropy of the
complex networks is used to decided which kinds of nodes will be chosen as the
main part in the complex networks. The proposed Tsallis entropy of the complex
networks is a generalised method to describe the property of the complex
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06059</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06059</id><created>2015-01-24</created><updated>2015-04-10</updated><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Zamdzhiev</keyname><forenames>Vladimir</forenames><affiliation>University of Oxford</affiliation></author></authors><title>!-Graphs with Trivial Overlap are Context-Free</title><categories>cs.LO cs.FL math.CT</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 16-31</journal-ref><doi>10.4204/EPTCS.181.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String diagrams are a powerful tool for reasoning about composite structures
in symmetric monoidal categories. By representing string diagrams as graphs,
equational reasoning can be done automatically by double-pushout rewriting.
!-graphs give us the means of expressing and proving properties about whole
families of these graphs simultaneously. While !-graphs provide elegant proofs
of surprisingly powerful theorems, little is known about the formal properties
of the graph languages they define. This paper takes the first step in
characterising these languages by showing that an important subclass of
!-graphs--those whose repeated structures only overlap trivially--can be
encoded using a (context-free) vertex replacement grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06060</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06060</id><created>2015-01-24</created><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Consistency Analysis of Nearest Subspace Classifier</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Nearest subspace classifier (NSS) finds an estimation of the underlying
subspace within each class and assigns data points to the class that
corresponds to its nearest subspace. This paper mainly studies how well NSS can
be generalized to new samples. It is proved that NSS is strongly consistent
under certain assumptions. For completeness, NSS is evaluated through
experiments on various simulated and real data sets, in comparison with some
other linear model based classifiers. It is also shown that NSS can obtain
effective classification results and is very efficient, especially for large
scale data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06076</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06076</id><created>2015-01-24</created><updated>2015-07-30</updated><authors><author><keyname>Nasser</keyname><forenames>Rajai</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>Fourier Analysis of MAC Polarization</title><categories>cs.IT math.IT</categories><comments>25 pages, submitted to IEEE Trans. Inform. Theory and presented in
  part in ISIT2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of MAC polar codes which are based on MAC polarization is that they
do not always achieve the entire capacity region. The reason behind this
problem is that MAC polarization sometimes induces a loss in the capacity
region. This paper provides a single letter necessary and sufficient condition
which characterizes all the MACs that do not lose any part of their capacity
region by polarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06080</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06080</id><created>2015-01-24</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Kiani</keyname><forenames>Narsis A.</forenames></author><author><keyname>Tegn&#xe9;r</keyname><forenames>Jesper</forenames></author></authors><title>Numerical Investigation of Graph Spectra and Information
  Interpretability of Eigenvalues</title><categories>cs.IT math.DS math.IT math.SP</categories><comments>Forthcoming in 3rd International Work-Conference on Bioinformatics
  and Biomedical Engineering (IWBBIO), Lecture Notes in Bioinformatics, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We undertake an extensive numerical investigation of the graph spectra of
thousands regular graphs, a set of random Erd\&quot;os-R\'enyi graphs, the two most
popular types of complex networks and an evolving genetic network by using
novel conceptual and experimental tools. Our objective in so doing is to
contribute to an understanding of the meaning of the Eigenvalues of a graph
relative to its topological and information-theoretic properties. We introduce
a technique for identifying the most informative Eigenvalues of evolving
networks by comparing graph spectra behavior to their algorithmic complexity.
We suggest that extending techniques can be used to further investigate the
behavior of evolving biological networks. In the extended version of this paper
we apply these techniques to seven tissue specific regulatory networks as
static example and network of a na\&quot;ive pluripotent immune cell in the process
of differentiating towards a Th17 cell as evolving example, finding the most
and least informative Eigenvalues at every stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06087</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06087</id><created>2015-01-24</created><updated>2015-04-22</updated><authors><author><keyname>Bordenave</keyname><forenames>Charles</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Massouli&#xe9;</keyname><forenames>Laurent</forenames></author></authors><title>Non-backtracking spectrum of random graphs: community detection and
  non-regular Ramanujan graphs</title><categories>math.PR cs.SI</categories><comments>59 pages</comments><msc-class>05C80, 05C50, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-backtracking walk on a graph is a directed path such that no edge is
the inverse of its preceding edge. The non-backtracking matrix of a graph is
indexed by its directed edges and can be used to count non-backtracking walks
of a given length. It has been used recently in the context of community
detection and has appeared previously in connection with the Ihara zeta
function and in some generalizations of Ramanujan graphs. In this work, we
study the largest eigenvalues of the non-backtracking matrix of the Erdos-Renyi
random graph and of the Stochastic Block Model in the regime where the number
of edges is proportional to the number of vertices. Our results confirm the
&quot;spectral redemption&quot; conjecture that community detection can be made on the
basis of the leading eigenvectors above the feasibility threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06091</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06091</id><created>2015-01-24</created><updated>2015-07-16</updated><authors><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>Feygin</keyname><forenames>Gennady</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Relaxed Polar Codes</title><categories>cs.IT math.IT</categories><comments>Conference version,Relaxed Channel Polarization for Reduced
  Complexity Polar Coding, accepted for presentation at IEEE Wireless
  Communications and Networking Conference WCNC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are the latest breakthrough in coding theory, as they are the
first family of codes with explicit construction that provably achieve the
symmetric capacity of discrete memoryless channels. Ar{\i}kan's polar encoder
and successive cancellation decoder have complexities of $N \log N$, for code
length $N$. Although, the complexity bound of $N \log N$ is asymptotically
favorable, we report in this work methods to further reduce the encoding and
decoding complexities of polar coding. The crux is to relax the polarization of
certain bit-channels without performance degradation. We consider schemes for
relaxing the polarization of both \emph{very good} and \emph{very bad}
bit-channels, in the process of channel polarization. Relaxed polar codes are
proved to preserve the capacity achieving property of polar codes. Analytical
bounds on the asymptotic and finite-length complexity reduction attainable by
relaxed polarization are derived.
  For binary erasure channels, we show that the computation complexity can be
reduced by a factor of 6, while preserving the rate and error performance. We
also show that relaxed polar codes can be decoded with significantly reduced
latency. For AWGN channels with medium code lengths, we show that relaxed polar
codes can have lower error probabilities than conventional polar codes, while
having reduced encoding and decoding computation complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06095</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06095</id><created>2015-01-24</created><authors><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Between Pure and Approximate Differential Privacy</title><categories>cs.DS cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a new lower bound on the sample complexity of $(\varepsilon,
\delta)$-differentially private algorithms that accurately answer statistical
queries on high-dimensional databases. The novelty of our bound is that it
depends optimally on the parameter $\delta$, which loosely corresponds to the
probability that the algorithm fails to be private, and is the first to
smoothly interpolate between approximate differential privacy ($\delta &gt; 0$)
and pure differential privacy ($\delta = 0$).
  Specifically, we consider a database $D \in \{\pm1\}^{n \times d}$ and its
\emph{one-way marginals}, which are the $d$ queries of the form &quot;What fraction
of individual records have the $i$-th bit set to $+1$?&quot; We show that in order
to answer all of these queries to within error $\pm \alpha$ (on average) while
satisfying $(\varepsilon, \delta)$-differential privacy, it is necessary that
$$ n \geq \Omega\left( \frac{\sqrt{d \log(1/\delta)}}{\alpha \varepsilon}
\right), $$ which is optimal up to constant factors. To prove our lower bound,
we build on the connection between \emph{fingerprinting codes} and lower bounds
in differential privacy (Bun, Ullman, and Vadhan, STOC'14).
  In addition to our lower bound, we give new purely and approximately
differentially private algorithms for answering arbitrary statistical queries
that improve on the sample complexity of the standard Laplace and Gaussian
mechanisms for achieving worst-case accuracy guarantees by a logarithmic
factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06099</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06099</id><created>2015-01-24</created><authors><author><keyname>Davaslioglu</keyname><forenames>Kemal</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Quantifying Potential Energy Efficiency Gain in Green Cellular Wireless
  Networks</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1210.8433</comments><journal-ref>IEEE Communications Surveys and Tutorials, Vol. 16, pp. 2065-2091,
  Fourth Quarter 2014</journal-ref><doi>10.1109/COMST.2014.2322951</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional cellular wireless networks were designed with the purpose of
providing high throughput for the user and high capacity for the service
provider, without any provisions of energy efficiency. As a result, these
networks have an enormous Carbon footprint. In this paper, we describe the
sources of the inefficiencies in such networks. First we present results of the
studies on how much Carbon footprint such networks generate. We also discuss
how much more mobile traffic is expected to increase so that this Carbon
footprint will even increase tremendously more. We then discuss specific
sources of inefficiency and potential sources of improvement at the physical
layer as well as at higher layers of the communication protocol hierarchy. In
particular, considering that most of the energy inefficiency in cellular
wireless networks is at the base stations, we discuss multi-tier networks and
point to the potential of exploiting mobility patterns in order to use base
station energy judiciously. We then investigate potential methods to reduce
this inefficiency and quantify their individual contributions. By a
consideration of the combination of all potential gains, we conclude that an
improvement in energy consumption in cellular wireless networks by two orders
of magnitude, or even more, is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06102</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06102</id><created>2015-01-24</created><authors><author><keyname>Adams</keyname><forenames>Terrence</forenames></author></authors><title>Development of a Big Data Framework for Connectomic Research</title><categories>cs.DC cs.CE</categories><comments>6 pages, 9 figures</comments><acm-class>H.3.4; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines research and development of a new Hadoop-based
architecture for distributed processing and analysis of electron microscopy of
brains. We show development of a new C++ library for implementation of 3D image
analysis techniques, and deployment in a distributed map/reduce framework. We
demonstrate our new framework on a subset of the Kasthuri11 dataset from the
Open Connectome Project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06114</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06114</id><created>2015-01-24</created><updated>2015-02-15</updated><authors><author><keyname>Salarian</keyname><forenames>Mahdi</forenames></author></authors><title>Accurate automatic segmentation of retina layers with emphasis on first
  layer</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantification of intra-retinal boundaries in optical coherence tomography
(OCT) is a crucial task for studying and diagnosing neurological and ocular
diseases. Since manual segmentation of layers is usually a time consuming task
and relay on user, a lot of attempts done to do it automatically and without
interference of user. Although for extracting all layers usually same procedure
is applied but finding the first layer is usually more difficult due to
vanishing it in some region specially close to Fobia. To have a general
software, beside using common methods like applying shortest path algorithm on
global gradient of image, some extra steps are used here to confine search area
for Dijstra algorithm especially for the second layer. Results demonstrates
high accuracy in segmenting all present layers, especially the first one that
is important for diagnosing issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06115</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06115</id><created>2015-01-25</created><updated>2015-02-04</updated><authors><author><keyname>Zhu</keyname><forenames>Wentao</forenames></author><author><keyname>Miao</keyname><forenames>Jun</forenames></author><author><keyname>Qing</keyname><forenames>Laiyun</forenames></author></authors><title>Constrained Extreme Learning Machines: A Study on Classification Cases</title><categories>cs.LG cs.CV cs.NE</categories><comments>14 pages, 6 figure, journel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme learning machine (ELM) is an extremely fast learning method and has a
powerful performance for pattern recognition tasks proven by enormous
researches and engineers. However, its good generalization ability is built on
large numbers of hidden neurons, which is not beneficial to real time response
in the test process. In this paper, we proposed new ways, named &quot;constrained
extreme learning machines&quot; (CELMs), to randomly select hidden neurons based on
sample distribution. Compared to completely random selection of hidden nodes in
ELM, the CELMs randomly select hidden nodes from the constrained vector space
containing some basic combinations of original sample vectors. The experimental
results show that the CELMs have better generalization ability than traditional
ELM, SVM and some other related methods. Additionally, the CELMs have a similar
fast learning speed as ELM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06120</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06120</id><created>2015-01-25</created><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>A Unified Framework for Identifiability Analysis in Bilinear Inverse
  Problems with Applications to Subspace and Sparsity Models</title><categories>cs.IT math.IT</categories><comments>40 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilinear inverse problems (BIPs), the resolution of two vectors given their
image under a bilinear mapping, arise in many applications. Without further
constraints, BIPs are usually ill-posed. In practice, properties of natural
signals are exploited to solve BIPs. For example, subspace constraints or
sparsity constraints are imposed to reduce the search space. These approaches
have shown some success in practice. However, there are few results on
uniqueness in BIPs. For most BIPs, the fundamental question of under what
condition the problem admits a unique solution, is yet to be answered. For
example, blind gain and phase calibration (BGPC) is a structured bilinear
inverse problem, which arises in many applications, including inverse rendering
in computational relighting (albedo estimation with unknown lighting), blind
phase and gain calibration in sensor array processing, and multichannel blind
deconvolution (MBD). It is interesting to study the uniqueness of such
problems.
  In this paper, we define identifiability of a BIP up to a group of
transformations. We derive necessary and sufficient conditions for such
identifiability, i.e., the conditions under which the solutions can be uniquely
determined up to the transformation group. Applying these results to BGPC, we
derive sufficient conditions for unique recovery under several scenarios,
including subspace, joint sparsity, and sparsity models. For BGPC with joint
sparsity or sparsity constraints, we develop a procedure to compute the
relevant transformation groups. We also give necessary conditions in the form
of tight lower bounds on sample complexities, and demonstrate the tightness of
these bounds by numerical experiments. The results for BGPC not only
demonstrate the application of the proposed general framework for
identifiability analysis, but are also of interest in their own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06123</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06123</id><created>2015-01-25</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>On Interference Alignment with Imperfect CSI: Characterizations of
  Outage Probability, Ergodic Rate and SER</title><categories>cs.IT math.IT</categories><comments>12 pages, 9 figures, IEEE Transactions on Vehicular Technology, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we give a unified performance analysis of interference
alignment (IA) over MIMO interference channels. Rather than the asymptotic
characterization, i.e. degree of freedom (DOF) at high signal-to-noise ratio
(SNR), we focus on the other practical performance metrics, namely outage
probability, ergodic rate and symbol error rate (SER). In particular, we
consider imperfect IA due to the fact that the transmitters usually have only
imperfect channel state information (CSI) in practical scenario. By
characterizing the impact of imperfect CSI, we derive the exact closed-form
expressions of outage probability, ergodic rate and SER in terms of CSI
accuracy, transmit SNR, channel condition, number of antennas, and the number
of data streams of each communication pair. Furthermore, we obtain some
important guidelines for performance optimization of IA under imperfect CSI by
minimizing the performance loss over IA with perfect CSI. Finally, our
theoretical claims are validated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06125</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06125</id><created>2015-01-25</created><updated>2015-06-15</updated><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames><affiliation>DEDUCTEAM</affiliation></author><author><keyname>Dowek</keyname><forenames>Gilles</forenames><affiliation>DEDUCTEAM</affiliation></author></authors><title>Simply Typed Lambda-Calculus Modulo Type Isomorphisms</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1306.5089</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a simply typed, non-deterministic lambda-calculus where isomorphic
types are equated. To this end, an equivalence relation is settled at the term
level. We then provide a proof of strong normalisation modulo equivalence. Such
a proof is a non-trivial adaptation of the reducibility method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06129</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06129</id><created>2015-01-25</created><authors><author><keyname>Garg</keyname><forenames>Sourav</forenames></author><author><keyname>Kumar</keyname><forenames>Swagat</forenames></author><author><keyname>Ratnakaram</keyname><forenames>Rajesh</forenames></author><author><keyname>Guha</keyname><forenames>Prithwijit</forenames></author></authors><title>An Occlusion Reasoning Scheme for Monocular Pedestrian Tracking in
  Dynamic Scenes</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks into the problem of pedestrian tracking using a monocular,
potentially moving, uncalibrated camera. The pedestrians are located in each
frame using a standard human detector, which are then tracked in subsequent
frames. This is a challenging problem as one has to deal with complex
situations like changing background, partial or full occlusion and camera
motion. In order to carry out successful tracking, it is necessary to resolve
associations between the detected windows in the current frame with those
obtained from the previous frame. Compared to methods that use temporal windows
incorporating past as well as future information, we attempt to make decision
on a frame-by-frame basis. An occlusion reasoning scheme is proposed to resolve
the association problem between a pair of consecutive frames by using an
affinity matrix that defines the closeness between a pair of windows and then,
uses a binary integer programming to obtain unique association between them. A
second stage of verification based on SURF matching is used to deal with those
cases where the above optimization scheme might yield wrong associations. The
efficacy of the approach is demonstrated through experiments on several
standard pedestrian datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06133</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06133</id><created>2015-01-25</created><authors><author><keyname>Shen</keyname><forenames>Zhesi</forenames></author><author><keyname>Cao</keyname><forenames>Shinan</forenames></author><author><keyname>Fan</keyname><forenames>Ying</forenames></author><author><keyname>Di</keyname><forenames>Zengru</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>Locating the source of spreading in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locating the sources that trigger a dynamical process is a fundamental but
challenging problem in complex networks, ranging from epidemic spreading in
society and on the Internet to cancer metastasis in the human body. An accurate
localization of the source is inherently limited by our ability to
simultaneously access the information of all nodes in a large-scale complex
network, such as the time at which each individual is infected in a large
population. This thus raises two critical questions: how do we locate the
source from incomplete information and can we achieve full localization of
sources at any possible location from a given set of observers. Here we develop
an efficient algorithm to locate the source of a diffusion-like process and
propose a general locatability condition. We test the algorithm by employing
epidemic spreading and consensus dynamics as typical dynamical processes and
apply it to the H1N1 pandemic in China. We find that the sources can be
precisely located in arbitrary networks insofar as the locatability condition
is assured. Our tools greatly improve our ability to locate the source of
diffusion in complex networks based on limited accessibility of nodal
information. Moreover they have implications for controlling a variety of
dynamical processes taking place on complex networks, such as inhibiting
epidemics, slowing the spread of rumors, and eliminating cancer seed cells in
the human body.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06137</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06137</id><created>2015-01-25</created><authors><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Building Bridges into the Unknown: Personalizing Connections to
  Little-known Countries</title><categories>cs.SI cs.CY cs.HC</categories><acm-class>H.3.3; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How are you related to Malawi? Do recent events on the Comoros effect you in
any subtle way? Who in your extended social network is in Croatia? We seldom
ask ourselves these questions, yet a &quot;long tail&quot; of content beyond our everyday
knowledge is waiting to be explored. In this work we propose a recommendation
task of creating interest in little-known content by building personalized
&quot;bridges&quot; to users. We consider an example task of interesting users in
little-known countries, and propose a system which aggregates a user's Twitter
profile, network, and tweets to create an interest model, which is then matched
to a library of knowledge about the countries. We perform a user study of 69
participants and conduct 11 in-depth interviews in order to evaluate the
efficacy of the proposed approach and gather qualitative insight into the
effect of multi-faceted use of Twitter on the perception of the bridges. We
find the increase in interest concerning little-known content to greatly depend
on the pre-existing disposition to it. Additionally, we discover a set of vital
properties good bridges must possess, including recency, novelty, emotiveness,
and a proper selection of language. Using the proposed approach we aim to
harvest the &quot;invisible connections&quot; to make explicit the idea of a &quot;small
world&quot; where even a faraway country is more closely connected to you than you
might have imagined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06140</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06140</id><created>2015-01-25</created><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author><author><keyname>Patt-Shamir</keyname><forenames>Boaz</forenames></author></authors><title>Better Online Deterministic Packet Routing on Grids</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following fundamental routing problem. An adversary inputs
packets arbitrarily at sources, each packet with an arbitrary destination.
Traffic is constrained by link capacities and buffer sizes, and packets may be
dropped at any time. The goal of the routing algorithm is to maximize
throughput, i.e., route as many packets as possible to their destination. Our
main result is an $O\left(\log n\right)$-competitive deterministic algorithm
for an $n$-node line network (i.e., $1$-dimensional grid), requiring only that
buffers can store at least $5$ packets, and that links can deliver at least $5$
packets per step. We note that $O(\log n)$ is the best ratio known, even for
randomized algorithms, even when allowed large buffers and wide links. The best
previous deterministic algorithm for this problem with constant-size buffers
and constant-capacity links was $O(\log^5 n)$-competitive. Our algorithm works
like admission-control algorithms in the sense that if a packet is not dropped
immediately upon arrival, then it is &quot;accepted&quot; and guaranteed to be delivered.
We also show how to extend our algorithm to a polylog-competitive algorithm for
any constant-dimension grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06148</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06148</id><created>2015-01-25</created><authors><author><keyname>Corneil</keyname><forenames>Derek G.</forenames></author><author><keyname>Dusart</keyname><forenames>Jeremie</forenames></author><author><keyname>Habib</keyname><forenames>Michel</forenames></author><author><keyname>de Montgolfier</keyname><forenames>Fabien</forenames></author></authors><title>A tie-break model for graph search</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of the recognition of various kinds of
orderings produced by graph searches. To this aim, we introduce a new
framework, the Tie-Breaking Label Search (TBLS), in order to handle a broad
variety of searches. This new model is based on partial orders defined on the
label set and it unifies the General Label Search (GLS) formalism of Krueger,
Simonet and Berry (2011), and the &quot;pattern-conditions&quot; formalism of Corneil and
Krueger (2008). It allows us to derive some general properties including new
pattern-conditions (yielding memory-efficient certificates) for many usual
searches, including BFS, DFS, LBFS and LDFS. Furthermore, the new model allows
easy expression of multi-sweep uses of searches that depend on previous
(search) orderings of the graph's vertex set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06158</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06158</id><created>2015-01-25</created><authors><author><keyname>Azar</keyname><forenames>Yossi</forenames></author><author><keyname>Vardi</keyname><forenames>Adi</forenames></author></authors><title>TSP with Time Windows and Service Time</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.0251</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider TSP with time windows and service time. In this problem we
receive a sequence of requests for a service at nodes in a metric space and a
time window for each request. The goal of the online algorithm is to maximize
the number of requests served during their time window. The time to traverse an
edge is the distance between the incident nodes of that edge. Serving a request
requires unit time. We characterize the competitive ratio for each metric space
separately. The competitive ratio depends on the relation between the minimum
laxity (the minimum length of a time window) and the diameter of the metric
space. Specifically, there is a constant competitive algorithm depending
whether the laxity is larger or smaller than the diameter. In addition, we
characterize the rate of convergence of the competitive ratio to $1$ as the
laxity increases. Specifically, we provide a matching lower and upper bounds
depending on the ratio between the laxity and the TSP of the metric space (the
minimum distance to traverse all nodes). An application of our result improves
the lower bound for colored packets with transition cost and matches the upper
bound. In proving our lower bounds we use an interesting non-standard embedding
with some special properties. This embedding may be interesting by its own.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06160</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06160</id><created>2015-01-25</created><authors><author><keyname>Richardson</keyname><forenames>Robert R.</forenames></author><author><keyname>Howey</keyname><forenames>David A.</forenames></author></authors><title>Sensorless Battery Internal Temperature Estimation using a Kalman Filter
  with Impedance Measurement</title><categories>cs.SY</categories><comments>10 pages, 9 figures, accepted for publication in IEEE Transactions on
  Sustainable Energy, 2015</comments><msc-class>93</msc-class><doi>10.1109/TSTE.2015.2420375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a method of estimating battery cell core and surface
temperature using a thermal model coupled with electrical impedance
measurement, rather than using direct surface temperature measurements. This is
advantageous over previous methods of estimating temperature from impedance,
which only estimate the average internal temperature. The performance of the
method is demonstrated experimentally on a 2.3 Ah lithium-ion iron phosphate
cell fitted with surface and core thermocouples for validation. An extended
Kalman filter, consisting of a reduced order thermal model coupled with
current, voltage and impedance measurements, is shown to accurately predict
core and surface temperatures for a current excitation profile based on a
vehicle drive cycle. A dual extended Kalman filter (DEKF) based on the same
thermal model and impedance measurement input is capable of estimating the
convection coefficient at the cell surface when the latter is unknown. The
performance of the DEKF using impedance as the measurement input is comparable
to an equivalent dual Kalman filter using a conventional surface temperature
sensor as measurement input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06166</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06166</id><created>2015-01-25</created><updated>2015-03-30</updated><authors><author><keyname>Lee</keyname><forenames>Wonju</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>HARQ Buffer Management: An Information-Theoretic View</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE International Symposium on Information Theory
  (ISIT) 2015. 29 pages, 12 figures, submitted to journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key practical constraint on the design of Hybrid automatic repeat request
(HARQ) schemes is the size of the on-chip buffer that is available at the
receiver to store previously received packets. In fact, in modern wireless
standards such as LTE and LTE-A, the HARQ buffer size is one of the main
drivers of the modem area and power consumption. This has recently highlighted
the importance of HARQ buffer management, that is, of the use of buffer-aware
transmission schemes and of advanced compression policies for the storage of
received data. This work investigates HARQ buffer management by leveraging
information-theoretic achievability arguments based on random coding.
Specifically, standard HARQ schemes, namely Type-I, Chase Combining and
Incremental Redundancy, are first studied under the assumption of a
finite-capacity HARQ buffer by considering both coded modulation, via Gaussian
signaling, and Bit Interleaved Coded Modulation (BICM). The analysis sheds
light on the impact of different compression strategies, namely the
conventional compression log-likelihood ratios and the direct digitization of
baseband signals, on the throughput. Then, coding strategies based on layered
modulation and optimized coding blocklength are investigated, highlighting the
benefits of HARQ buffer-aware transmission schemes. The optimization of
baseband compression for multiple-antenna links is also studied, demonstrating
the optimality of a transform coding approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06170</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06170</id><created>2015-01-25</created><updated>2015-05-04</updated><authors><author><keyname>Cho</keyname><forenames>Minsu</forenames></author><author><keyname>Kwak</keyname><forenames>Suha</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author><author><keyname>Ponce</keyname><forenames>Jean</forenames></author></authors><title>Unsupervised Object Discovery and Localization in the Wild: Part-based
  Matching with Bottom-up Region Proposals</title><categories>cs.CV</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses unsupervised discovery and localization of dominant
objects from a noisy image collection with multiple object classes. The setting
of this problem is fully unsupervised, without even image-level annotations or
any assumption of a single dominant class. This is far more general than
typical colocalization, cosegmentation, or weakly-supervised localization
tasks. We tackle the discovery and localization problem using a part-based
region matching approach: We use off-the-shelf region proposals to form a set
of candidate bounding boxes for objects and object parts. These regions are
efficiently matched across images using a probabilistic Hough transform that
evaluates the confidence for each candidate correspondence considering both
appearance and spatial consistency. Dominant objects are discovered and
localized by comparing the scores of candidate regions and selecting those that
stand out over other regions containing them. Extensive experimental
evaluations on standard benchmarks demonstrate that the proposed approach
significantly outperforms the current state of the art in colocalization, and
achieves robust object discovery in challenging mixed-class datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06180</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06180</id><created>2015-01-25</created><authors><author><keyname>Zhang</keyname><forenames>Shanshan</forenames></author><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author><author><keyname>Klein</keyname><forenames>Dominik A.</forenames></author><author><keyname>Cremers</keyname><forenames>Armin B.</forenames></author></authors><title>Exploring Human Vision Driven Features for Pedestrian Detection</title><categories>cs.CV</categories><comments>Accepted for publication in IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the center-surround mechanism in the human visual attention
system, we propose to use average contrast maps for the challenge of pedestrian
detection in street scenes due to the observation that pedestrians indeed
exhibit discriminative contrast texture. Our main contributions are first to
design a local, statistical multi-channel descriptorin order to incorporate
both color and gradient information. Second, we introduce a multi-direction and
multi-scale contrast scheme based on grid-cells in order to integrate
expressive local variations. Contributing to the issue of selecting most
discriminative features for assessing and classification, we perform extensive
comparisons w.r.t. statistical descriptors, contrast measurements, and scale
structures. This way, we obtain reasonable results under various
configurations. Empirical findings from applying our optimized detector on the
INRIA and Caltech pedestrian datasets show that our features yield
state-of-the-art performance in pedestrian detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06194</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06194</id><created>2015-01-25</created><authors><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Do Read Errors Matter for Genome Assembly?</title><categories>cs.IT math.IT q-bio.GN</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While most current high-throughput DNA sequencing technologies generate short
reads with low error rates, emerging sequencing technologies generate long
reads with high error rates. A basic question of interest is the tradeoff
between read length and error rate in terms of the information needed for the
perfect assembly of the genome. Using an adversarial erasure error model, we
make progress on this problem by establishing a critical read length, as a
function of the genome and the error rate, above which perfect assembly is
guaranteed. For several real genomes, including those from the GAGE dataset, we
verify that this critical read length is not significantly greater than the
read length required for perfect assembly from reads without errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06195</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06195</id><created>2015-01-25</created><authors><author><keyname>Yang</keyname><forenames>Yun</forenames></author><author><keyname>Pilanci</keyname><forenames>Mert</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Randomized sketches for kernels: Fast and optimal non-parametric
  regression</title><categories>stat.ML cs.DS cs.LG stat.CO</categories><comments>27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel ridge regression (KRR) is a standard method for performing
non-parametric regression over reproducing kernel Hilbert spaces. Given $n$
samples, the time and space complexity of computing the KRR estimate scale as
$\mathcal{O}(n^3)$ and $\mathcal{O}(n^2)$ respectively, and so is prohibitive
in many cases. We propose approximations of KRR based on $m$-dimensional
randomized sketches of the kernel matrix, and study how small the projection
dimension $m$ can be chosen while still preserving minimax optimality of the
approximate KRR estimate. For various classes of randomized sketches, including
those based on Gaussian and randomized Hadamard matrices, we prove that it
suffices to choose the sketch dimension $m$ proportional to the statistical
dimension (modulo logarithmic factors). Thus, we obtain fast and minimax
optimal approximations to the KRR estimate for non-parametric regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06202</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06202</id><created>2015-01-25</created><updated>2015-07-27</updated><authors><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy M.</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Xiong</keyname><forenames>Jiechao</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author><author><keyname>Wang</keyname><forenames>Yizhou</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author></authors><title>Robust Subjective Visual Property Prediction from Crowdsourced Pairwise
  Labels</title><categories>cs.CV cs.LG cs.MM cs.SI math.ST stat.TH</categories><comments>14 pages, accepted by IEEE TPAMI</comments><doi>10.1109/TPAMI.2015.2456887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating subjective visual properties from image and video
has attracted increasing interest. A subjective visual property is useful
either on its own (e.g. image and video interestingness) or as an intermediate
representation for visual recognition (e.g. a relative attribute). Due to its
ambiguous nature, annotating the value of a subjective visual property for
learning a prediction model is challenging. To make the annotation more
reliable, recent studies employ crowdsourcing tools to collect pairwise
comparison labels because human annotators are much better at ranking two
images/videos (e.g. which one is more interesting) than giving an absolute
value to each of them separately. However, using crowdsourced data also
introduces outliers. Existing methods rely on majority voting to prune the
annotation outliers/errors. They thus require large amount of pairwise labels
to be collected. More importantly as a local outlier detection method, majority
voting is ineffective in identifying outliers that can cause global ranking
inconsistencies. In this paper, we propose a more principled way to identify
annotation outliers by formulating the subjective visual property prediction
task as a unified robust learning to rank problem, tackling both the outlier
detection and learning to rank jointly. Differing from existing methods, the
proposed method integrates local pairwise comparison labels together to
minimise a cost that corresponds to global inconsistency of ranking order. This
not only leads to better detection of annotation outliers but also enables
learning with extremely sparse annotations. Extensive experiments on various
benchmark datasets demonstrate that our new approach significantly outperforms
state-of-the-arts alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06206</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06206</id><created>2015-01-25</created><updated>2015-01-27</updated><authors><author><keyname>Delhibabu</keyname><forenames>Radhakrishnan</forenames></author></authors><title>Dynamics of Belief: Abduction, Horn Knowledge Base And Database Updates</title><categories>cs.LO cs.AI cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.2499,
  arXiv:1405.2642, arXiv:1407.3512, arXiv:1301.5154</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
In order to apply the rationality result of belief dynamics theory to various
practical problems, it should be generalized in two respects: first it should
allow a certain part of belief to be declared as immutable; and second, the
belief state need not be deductively closed. Such a generalization of belief
dynamics, referred to as base dynamics, is presented in this paper, along with
the concept of a generalized revision algorithm for knowledge bases (Horn or
Horn logic with stratified negation). We show that knowledge base dynamics has
an interesting connection with kernel change via hitting set and abduction. In
this paper, we show how techniques from disjunctive logic programming can be
used for efficient (deductive) database updates. The key idea is to transform
the given database together with the update request into a disjunctive
(datalog) logic program and apply disjunctive techniques (such as minimal model
reasoning) to solve the original update problem. The approach extends and
integrates standard techniques for efficient query answering and integrity
checking. The generation of a hitting set is carried out through a hyper
tableaux calculus and magic set that is focused on the goal of minimality. The
present paper provides a comparative study of view update algorithms in
rational approach. For, understand the basic concepts with abduction, we
provide an abductive framework for knowledge base dynamics. Finally, we
demonstrate how belief base dynamics can provide an axiomatic characterization
for insertion a view atom to the database. We give a quick overview of the main
operators for belief change, in particular, belief update versus database
update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06209</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06209</id><created>2015-01-25</created><updated>2015-07-17</updated><authors><author><keyname>Uecker</keyname><forenames>Martin</forenames></author></authors><title>Parallel Magnetic Resonance Imaging</title><categories>cs.NA cs.CV math.NA math.OC physics.med-ph</categories><comments>22 pages, 9 Figures, 76 References. Copyright: Martin Uecker. Draft
  for a book chapter. To appear in: A Majumdar and RK Ward (eds.), MRI:
  Physics, Image Reconstruction, and Analysis, CRC Press 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scan
times and, in consequence, its sensitivity to motion. Exploiting the
complementary information from multiple receive coils, parallel imaging is able
to recover images from under-sampled k-space data and to accelerate the
measurement. Because parallel magnetic resonance imaging can be used to
accelerate basically any imaging sequence it has many important applications.
Parallel imaging brought a fundamental shift in image reconstruction: Image
reconstruction changed from a simple direct Fourier transform to the solution
of an ill-conditioned inverse problem. This work gives an overview of image
reconstruction from the perspective of inverse problems. After introducing
basic concepts such as regularization, discretization, and iterative
reconstruction, advanced topics are discussed including algorithms for
auto-calibration, the connection to approximation theory, and the combination
with compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06216</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06216</id><created>2015-01-25</created><authors><author><keyname>&#xc7;akmak</keyname><forenames>Burak</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard H.</forenames></author></authors><title>S-AMP for Non-linear Observation Models</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently we extended Approximate message passing (AMP) algorithm to be able
to handle general invariant matrix ensembles. In this contribution we extend
our S-AMP approach to non-linear observation models. We obtain generalized AMP
(GAMP) algorithm as the special case when the measurement matrix has zero-mean
iid Gaussian entries. Our derivation is based upon 1) deriving expectation
propagation (EP) like algorithms from the stationary-points equations of the
Gibbs free energy under first- and second-moment constraints and 2) applying
additive free convolution in free probability theory to get low-complexity
updates for the second moment quantities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06218</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06218</id><created>2015-01-25</created><updated>2015-12-30</updated><authors><author><keyname>Zhou</keyname><forenames>Mingyuan</forenames></author></authors><title>Infinite Edge Partition Models for Overlapping Community Detection and
  Link Prediction</title><categories>stat.ML cs.SI</categories><comments>Appeared in Artificial Intelligence and Statistics (AISTATS), May
  2015. 9 pages + 2 page appendix. The paper is updated to fix a typo in the
  equation right after (25) in the Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hierarchical gamma process infinite edge partition model is proposed to
factorize the binary adjacency matrix of an unweighted undirected relational
network under a Bernoulli-Poisson link. The model describes both homophily and
stochastic equivalence, and is scalable to big sparse networks by focusing its
computation on pairs of linked nodes. It can not only discover overlapping
communities and inter-community interactions, but also predict missing edges. A
simplified version omitting inter-community interactions is also provided and
we reveal its interesting connections to existing models. The number of
communities is automatically inferred in a nonparametric Bayesian manner, and
efficient inference via Gibbs sampling is derived using novel data augmentation
techniques. Experimental results on four real networks demonstrate the models'
scalability and state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06223</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06223</id><created>2015-01-25</created><authors><author><keyname>Spear</keyname><forenames>Wyatt</forenames></author><author><keyname>Norris</keyname><forenames>Boyana</forenames></author></authors><title>A Roofline Visualization Framework</title><categories>cs.PF</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Roofline Model and its derivatives provide an intuitive representation of
the best achievable performance on a given architecture. The Roofline Toolkit
project is a collaboration among researchers at Argonne National Laboratory,
Lawrence Berkeley National Laboratory, and the University of Oregon and
consists of three main parts: hardware characterization, software
characterization, and data manipulation and visualization interface. These
components address the different aspects of performance data acquisition and
manipulation required for performance analysis, modeling and optimization of
codes on existing and emerging architectures. In this paper we introduce an
initial implementation of the third component, a system for visualizing
roofline charts and managing roofline performance analysis data. We discuss the
implementation and rationale for the integration of the roofline visualization
system into the Eclipse IDE. An overview of our continuing efforts and goals in
the development of this project is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06225</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06225</id><created>2015-01-25</created><authors><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Online Optimization : Competing with Dynamic Comparators</title><categories>cs.LG math.OC stat.ML</categories><comments>23 pages, To appear in International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent literature on online learning has focused on developing adaptive
algorithms that take advantage of a regularity of the sequence of observations,
yet retain worst-case performance guarantees. A complementary direction is to
develop prediction methods that perform well against complex benchmarks. In
this paper, we address these two directions together. We present a fully
adaptive method that competes with dynamic benchmarks in which regret guarantee
scales with regularity of the sequence of cost functions and comparators.
Notably, the regret bound adapts to the smaller complexity measure in the
problem environment. Finally, we apply our results to drifting zero-sum,
two-player games where both players achieve no regret guarantees against best
sequences of actions in hindsight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06237</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06237</id><created>2015-01-25</created><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author></authors><title>Deep Transductive Semi-supervised Maximum Margin Clustering</title><categories>cs.LG</categories><comments>14</comments><msc-class>68T10</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Semi-supervised clustering is an very important topic in machine learning and
computer vision. The key challenge of this problem is how to learn a metric,
such that the instances sharing the same label are more likely close to each
other on the embedded space. However, little attention has been paid to learn
better representations when the data lie on non-linear manifold. Fortunately,
deep learning has led to great success on feature learning recently. Inspired
by the advances of deep learning, we propose a deep transductive
semi-supervised maximum margin clustering approach. More specifically, given
pairwise constraints, we exploit both labeled and unlabeled data to learn a
non-linear mapping under maximum margin framework for clustering analysis.
Thus, our model unifies transductive learning, feature learning and maximum
margin techniques in the semi-supervised clustering framework. We pretrain the
deep network structure with restricted Boltzmann machines (RBMs) layer by layer
greedily, and optimize our objective function with gradient descent. By
checking the most violated constraints, our approach updates the model
parameters through error backpropagation, in which deep features are learned
automatically. The experimental results shows that our model is significantly
better than the state of the art on semi-supervised clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06238</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06238</id><created>2015-01-25</created><updated>2015-10-04</updated><authors><author><keyname>Chen</keyname><forenames>Houwu</forenames></author><author><keyname>Shu</keyname><forenames>Jiwu</forenames></author></authors><title>Sky: Opinion Dynamics Based Consensus for P2P Network with Trust
  Relationships</title><categories>cs.DC</categories><comments>The 15th International Conference on Algorithms and Architectures for
  Parallel Processing (ICA3PP 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Byzantine consensus does not work in P2P network due to Sybil
attack while the most prevalent Sybil-proof consensus at present still can't
resist adversary with dominant compute power. This paper proposed opinion
dynamics based consensus for P2P network with trust relationships, consisting
of the sky framework and the sky model. With the sky framework, opinion
dynamics can be applied in P2P network for consensus which is Sybil-proof
through trust relationships and emerges from local interactions of each node
with its direct contacts without topology, global information or even sample of
the network involved. The sky model has better performance of convergence than
existing models including MR, voter and Sznajd, and its lower bound of fault
tolerance performance is also analyzed and proved. Simulations show that our
approach can tolerant failures by at least 13% random nodes or 2% top
influential nodes while over 96% correct nodes still make correct decision
within 70 seconds on the SNAP Wikipedia who-votes-on-whom network for initial
configuration of convergence&gt;0.5 with reasonable latencies. Comparing to
compute power based consensus, our approach can resist any faulty or malicious
nodes by unfollowing them. To the best of our knowledge, it's the first work to
bring opinion dynamics to P2P network for consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06239</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06239</id><created>2015-01-25</created><authors><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Proactive Push with Energy Harvesting Based Small Cells in Heterogeneous
  Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, accepted by ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent development of energy harvesting communications, and
the trend of multimedia contents caching and push at the access edge and user
terminals, this paper considers how to design an effective push mechanism of
energy harvesting powered small-cell base stations (SBSs) in heterogeneous
networks. The problem is formulated as a Markov decision process by optimizing
the push policy based on the battery energy, user request and content
popularity state to maximize the service capability of SBSs. We extensively
analyze the problem and propose an effective policy iteration algorithm to find
the optimal policy. According to the numerical results, we find that the
optimal policy reveals a state dependent threshold based structure. Besides,
more than 50% performance gain is achieved by the optimal push policy compared
with the non-push policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06241</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06241</id><created>2015-01-25</created><updated>2015-03-16</updated><authors><author><keyname>Song</keyname><forenames>Ruiyang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>Sequential Sensing with Model Mismatch</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Submitted to IEEE for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the performance of sequential information guided sensing,
Info-Greedy Sensing, when there is a mismatch between the true signal model and
the assumed model, which may be a sample estimate. In particular, we consider a
setup where the signal is low-rank Gaussian and the measurements are taken in
the directions of eigenvectors of the covariance matrix in a decreasing order
of eigenvalues. We establish a set of performance bounds when a mismatched
covariance matrix is used, in terms of the gap of signal posterior entropy, as
well as the additional amount of power required to achieve the same signal
recovery precision. Based on this, we further study how to choose an
initialization for Info-Greedy Sensing using the sample covariance matrix, or
using an efficient covariance sketching scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06243</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06243</id><created>2015-01-25</created><updated>2015-03-25</updated><authors><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Poisson Matrix Completion</title><categories>stat.ML cs.LG</categories><comments>Submitted to IEEE for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the theory of matrix completion to the case where we make Poisson
observations for a subset of entries of a low-rank matrix. We consider the
(now) usual matrix recovery formulation through maximum likelihood with proper
constraints on the matrix $M$, and establish theoretical upper and lower bounds
on the recovery error. Our bounds are nearly optimal up to a factor on the
order of $\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by adapting
the arguments used for one-bit matrix completion \cite{davenport20121}
(although these two problems are different in nature) and the adaptation
requires new techniques exploiting properties of the Poisson likelihood
function and tackling the difficulties posed by the locally sub-Gaussian
characteristic of the Poisson distribution. Our results highlight a few
important distinctions of Poisson matrix completion compared to the prior work
in matrix completion including having to impose a minimum signal-to-noise
requirement on each observed entry. We also develop an efficient iterative
algorithm and demonstrate its good performance in recovering solar flare
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06247</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06247</id><created>2015-01-25</created><updated>2015-01-27</updated><authors><author><keyname>Xia</keyname><forenames>Peng</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Sun</keyname><forenames>Yizhou</forenames></author><author><keyname>Chen</keyname><forenames>Cindy</forenames></author></authors><title>Reciprocal Recommendation System for Online Dating</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online dating sites have become popular platforms for people to look for
potential romantic partners. Different from traditional user-item
recommendations where the goal is to match items (e.g., books, videos, etc)
with a user's interests, a recommendation system for online dating aims to
match people who are mutually interested in and likely to communicate with each
other. We introduce similarity measures that capture the unique features and
characteristics of the online dating network, for example, the interest
similarity between two users if they send messages to same users, and
attractiveness similarity if they receive messages from same users. A
reciprocal score that measures the compatibility between a user and each
potential dating candidate is computed and the recommendation list is generated
to include users with top scores. The performance of our proposed
recommendation system is evaluated on a real-world dataset from a major online
dating site in China. The results show that our recommendation algorithms
significantly outperform previously proposed approaches, and the collaborative
filtering-based algorithms achieve much better performance than content-based
algorithms in both precision and recall. Our results also reveal interesting
behavioral difference between male and female users when it comes to looking
for potential dates. In particular, males tend to be focused on their own
interest and oblivious towards their attractiveness to potential dates, while
females are more conscientious to their own attractiveness to the other side of
the line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06259</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06259</id><created>2015-01-26</created><authors><author><keyname>&#x130;leri</keyname><forenames>Atalay Mert</forenames></author><author><keyname>K&#xfc;lekci</keyname><forenames>M. O&#x11f;uzhan</forenames></author><author><keyname>Xu</keyname><forenames>Bojian</forenames></author></authors><title>On Longest Repeat Queries</title><categories>cs.DS</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repeat finding in strings has important applications in subfields such as
computational biology. Surprisingly, all prior work on repeat finding did not
consider the constraint on the locality of repeats. In this paper, we propose
and study the problem of finding longest repetitive substrings covering
particular string positions. We propose an $O(n)$ time and space algorithm for
finding the longest repeat covering every position of a string of size $n$. Our
work is optimal since the reading and the storage of an input string of size
$n$ takes $O(n)$ time and space. Because any substring of a repeat is also a
repeat, our solution to longest repeat queries effectively provides a
&quot;stabbing&quot; tool for practitioners for finding most of the repeats that cover
particular string positions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06262</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06262</id><created>2015-01-26</created><updated>2015-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Keze</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author></authors><title>3D Human Activity Recognition with Reconfigurable Convolutional Neural
  Networks</title><categories>cs.CV</categories><comments>This manuscript has 10 pages with 9 figures, and a preliminary
  version was published in ACM MM'14 conference</comments><msc-class>68U01</msc-class><acm-class>I.4</acm-class><doi>10.1145/2647868.2654912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human activity understanding with 3D/depth sensors has received increasing
attention in multimedia processing and interactions. This work targets on
developing a novel deep model for automatic activity recognition from RGB-D
videos. We represent each human activity as an ensemble of cubic-like video
segments, and learn to discover the temporal structures for a category of
activities, i.e. how the activities to be decomposed in terms of
classification. Our model can be regarded as a structured deep architecture, as
it extends the convolutional neural networks (CNNs) by incorporating structure
alternatives. Specifically, we build the network consisting of 3D convolutions
and max-pooling operators over the video segments, and introduce the latent
variables in each convolutional layer manipulating the activation of neurons.
Our model thus advances existing approaches in two aspects: (i) it acts
directly on the raw inputs (grayscale-depth data) to conduct recognition
instead of relying on hand-crafted features, and (ii) the model structure can
be dynamically adjusted accounting for the temporal variations of human
activities, i.e. the network configuration is allowed to be partially activated
during inference. For model training, we propose an EM-type optimization method
that iteratively (i) discovers the latent structure by determining the
decomposed actions for each training example, and (ii) learns the network
parameters by using the back-propagation algorithm. Our approach is validated
in challenging scenarios, and outperforms state-of-the-art methods. A large
human activity database of RGB-D videos is presented in addition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06267</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06267</id><created>2015-01-26</created><updated>2016-02-23</updated><authors><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>The Stability Region of the Two-User Broadcast Channel</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE International Conference on Communications (ICC) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the stability region of the two-user broadcast
channel. First, we obtain the stability region in the general case. Second, we
consider the particular case where each receiver treats the interfering signal
as noise, as well as the case in which the packets are transmitted using
superposition coding and successive decoding is employed at the strong
receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06272</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06272</id><created>2015-01-26</created><updated>2015-04-19</updated><authors><author><keyname>Zhao</keyname><forenames>Fang</forenames></author><author><keyname>Huang</keyname><forenames>Yongzhen</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Tan</keyname><forenames>Tieniu</forenames></author></authors><title>Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval</title><categories>cs.CV cs.LG</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of web images, hashing has received increasing
interests in large scale image retrieval. Research efforts have been devoted to
learning compact binary codes that preserve semantic similarity based on
labels. However, most of these hashing methods are designed to handle simple
binary similarity. The complex multilevel semantic structure of images
associated with multiple labels have not yet been well explored. Here we
propose a deep semantic ranking based method for learning hash functions that
preserve multilevel semantic similarity between multi-label images. In our
approach, deep convolutional neural network is incorporated into hash functions
to jointly learn feature representations and mappings from them to hash codes,
which avoids the limitation of semantic representation power of hand-crafted
features. Meanwhile, a ranking list that encodes the multilevel similarity
information is employed to guide the learning of such deep hash functions. An
effective scheme based on surrogate loss is used to solve the intractable
optimization problem of nonsmooth and multivariate ranking measures involved in
the learning procedure. Experimental results show the superiority of our
proposed approach over several state-of-the-art hashing methods in term of
ranking evaluation metrics when tested on multi-label image datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06279</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06279</id><created>2015-01-26</created><authors><author><keyname>Wahls</keyname><forenames>Sander</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Fast Inverse Nonlinear Fourier Transform For Generating Multi-Solitons
  In Optical Fiber</title><categories>cs.IT math.IT nlin.PS nlin.SI physics.optics</categories><comments>Submitted to IEEE ISIT 2015 (fixed a few typos)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The achievable data rates of current fiber-optic
wavelength-division-multiplexing (WDM) systems are limited by nonlinear
interactions between different subchannels. Recently, it was thus proposed to
replace the conventional Fourier transform in WDM systems with an appropriately
defined nonlinear Fourier transform (NFT). The computational complexity of NFTs
is a topic of current research. In this paper, a fast inverse NFT algorithm for
the important special case of multi-solitonic signals is presented. The
algorithm requires only $\mathcal{O}(D\log^{2}D)$ floating point operations to
compute $D$ samples of a multi-soliton. To the best of our knowledge, this is
the first algorithm for this problem with $\log^{2}$-linear complexity. The
paper also includes a many samples analysis of the generated nonlinear Fourier
spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06281</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06281</id><created>2015-01-26</created><updated>2015-06-23</updated><authors><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Replica Symmetric Bound for Restricted Isometry Constant</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech cs.CC math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a method for evaluating restricted isometry constants (RICs). This
evaluation is reduced to the identification of the zero-points of entropy,
which is defined for submatrices that are composed of columns selected from a
given measurement matrix. Using the replica method developed in statistical
mechanics, we assess RICs for Gaussian random matrices under the replica
symmetric (RS) assumption. In order to numerically validate the adequacy of our
analysis, we employ the exchange Monte Carlo (EMC) method, which has been
empirically demonstrated to achieve much higher numerical accuracy than naive
Monte Carlo methods. The EMC method suggests that our theoretical estimation of
an RIC corresponds to an upper bound that is tighter than in preceding studies.
Physical consideration indicates that our assessment of the RIC could be
improved by taking into account the replica symmetry breaking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06283</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06283</id><created>2015-01-26</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>Random access congestion control in DVB-RCS2 interactive satellite
  terminals</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE International Symposium on Broadband Multimedia Systems and
  Broadcasting (BMSB), 2013. arXiv admin note: text overlap with
  arXiv:1501.05809</comments><doi>10.1109/BMSB.2013.6621777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The next generation of interactive satellite terminals is going to play a
crucial role in the future of DVB standards. As a matter of fact in the current
standard, satellite terminals are expected to be interactive thus offering
apart from the possibility of logon signalling and control signalling also data
transmission in the return channel with satisfying quality. Considering the
nature of the traffic from terminals that is by nature bursty and with big
periods of inactivity, the use of a Random Access technique could be preferred.
In this paper Random Access congestion control in DVB-RCS2 is considered with
particular regard to the recently introduced Contention Resolution Diversity
Slotted Aloha technique, able to boost the performance compared to Slotted
Aloha. The paper analyzes the stability of such a channel with particular
emphasis on the design and on limit control procedures that can be applied in
order to ensure stability of the channel even in presence of possible
instability due to statistical fluctuations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06284</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06284</id><created>2015-01-26</created><authors><author><keyname>Baisero</keyname><forenames>Andrea</forenames></author><author><keyname>Pokorny</keyname><forenames>Florian T.</forenames></author><author><keyname>Ek</keyname><forenames>Carl Henrik</forenames></author></authors><title>On a Family of Decomposable Kernels on Sequences</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications data is naturally presented in terms of orderings of
some basic elements or symbols. Reasoning about such data requires a notion of
similarity capable of handling sequences of different lengths. In this paper we
describe a family of Mercer kernel functions for such sequentially structured
data. The family is characterized by a decomposable structure in terms of
symbol-level and structure-level similarities, representing a specific
combination of kernels which allows for efficient computation. We provide an
experimental evaluation on sequential classification tasks comparing kernels
from our family of kernels to a state of the art sequence kernel called the
Global Alignment kernel which has been shown to outperform Dynamic Time Warping
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06285</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06285</id><created>2015-01-26</created><updated>2015-05-19</updated><authors><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author></authors><title>Analyzing data citation practices using the Data Citation Index</title><categories>cs.DL</categories><comments>Paper accepted for publication in the Journal of the Association for
  Information Science and Technology. v2 revises style on text, title and
  abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analysis of data citation practices based on the Data Citation
Index from Thomson Reuters. This database launched in 2012 aims to link data
sets and data studies with citations received from the other citation indexes.
The DCI harvests citations to research data from papers indexed in the Web of
Science. It relies on the information provided by the data repository as data
citation practices are inconsistent or inexistent in many cases. The findings
of this study show that data citation practices are far from common in most
research fields. Some differences have been reported on the way researchers
cite data: while in the areas of Science and Engineering and Technology data
sets were the most cited, in Social Sciences and Arts and Humanities data
studies play a greater role. A total of 88.1 percent of the records have
received no citation, but some repositories show very low uncitedness rates.
Although data citation practices are rare in most fields, they have expanded in
disciplines such as crystallography and genomics. We conclude by emphasizing
the role that the DCI could play in encouraging the consistent, standardized
citation of research data; a role that would enhance their value as a means of
following the research process from data collection to publication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06287</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06287</id><created>2015-01-26</created><updated>2015-07-29</updated><authors><author><keyname>Parizi</keyname><forenames>Mani Bastani</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>On the Secrecy Exponent of the Wire-tap Channel</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures; to be presented at 2015 IEEE Information Theory
  Workshop (ITW'2015)</comments><doi>10.1109/ITWF.2015.7360781</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive an exponentially decaying upper-bound on the unnormalized amount of
information leaked to the wire-tapper in Wyner's wire-tap channel setting. We
characterize the exponent of the bound as a function of the randomness used by
the encoder. This exponent matches that of the recent work of Hayashi (2011)
which is, to the best of our knowledge, the best exponent that exists in the
literature. Our proof---like those of Han et al. (2014) and Hayashi (2015)---is
exclusively based on an i.i.d. random coding construction while that of Hayashi
(2011), in addition, requires the use of random universal hash functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06297</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06297</id><created>2015-01-26</created><updated>2015-11-19</updated><authors><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Boscaini</keyname><forenames>Davide</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Geodesic convolutional neural networks on Riemannian manifolds</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature descriptors play a crucial role in a wide range of geometry analysis
and processing applications, including shape correspondence, retrieval, and
segmentation. In this paper, we introduce Geodesic Convolutional Neural
Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm
to non-Euclidean manifolds. Our construction is based on a local geodesic
system of polar coordinates to extract &quot;patches&quot;, which are then passed through
a cascade of filters and linear and non-linear operators. The coefficients of
the filters and linear combination weights are optimization variables that are
learned to minimize a task-specific cost function. We use GCNN to learn
invariant shape features, allowing to achieve state-of-the-art performance in
problems such as shape description, retrieval, and correspondence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06301</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06301</id><created>2015-01-26</created><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Efthymiou</keyname><forenames>Charilaos</forenames></author><author><keyname>Jaafari</keyname><forenames>Nor</forenames></author></authors><title>Local convergence of random graph colorings</title><categories>math.CO cs.DM math.PR</categories><msc-class>05C80, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=G(n,m)$ be a random graph whose average degree $d=2m/n$ is below the
$k$-colorability threshold. If we sample a $k$-coloring $\sigma$ of $G$
uniformly at random, what can we say about the correlations between the colors
assigned to vertices that are far apart? According to a prediction from
statistical physics, for average degrees below the so-called {\em condensation
threshold} $d_c(k)$, the colors assigned to far away vertices are
asymptotically independent [Krzakala et al.: Proc. National Academy of Sciences
2007]. We prove this conjecture for $k$ exceeding a certain constant $k_0$.
More generally, we investigate the joint distribution of the $k$-colorings that
$\sigma$ induces locally on the bounded-depth neighborhoods of any fixed number
of vertices. In addition, we point out an implication on the reconstruction
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06307</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06307</id><created>2015-01-26</created><updated>2015-03-23</updated><authors><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Jadidi</keyname><forenames>Mohsen</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>It's a Man's Wikipedia? Assessing Gender Inequality in an Online
  Encyclopedia</title><categories>cs.CY cs.SI</categories><comments>in The International AAAI Conference on Web and Social Media
  (ICWSM2015), Oxford, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia is a community-created encyclopedia that contains information about
notable people from different countries, epochs and disciplines and aims to
document the world's knowledge from a neutral point of view. However, the
narrow diversity of the Wikipedia editor community has the potential to
introduce systemic biases such as gender biases into the content of Wikipedia.
In this paper we aim to tackle a sub problem of this larger challenge by
presenting and applying a computational method for assessing gender bias on
Wikipedia along multiple dimensions. We find that while women on Wikipedia are
covered and featured well in many Wikipedia language editions, the way women
are portrayed starkly differs from the way men are portrayed. We hope our work
contributes to increasing awareness about gender biases online, and in
particular to raising attention to the different levels in which gender biases
can manifest themselves on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06313</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06313</id><created>2015-01-26</created><authors><author><keyname>Lykourentzou</keyname><forenames>Ioanna</forenames></author><author><keyname>Antoniou</keyname><forenames>Angeliki</forenames></author><author><keyname>Naudet</keyname><forenames>Yannick</forenames></author></authors><title>Matching or Crashing? Personality-based Team Formation in Crowdsourcing
  Environments</title><categories>cs.HC cs.CY cs.SI</categories><comments>11 pages</comments><acm-class>H.1.2; H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Does placing workers together based on their personality give better
performance results in cooperative crowdsourcing settings, compared to
non-personality based crowd team formation?&quot; In this work we examine the impact
of personality compatibility on the effectiveness of crowdsourced team work.
Using a personality-based group dynamics approach, we examine two main types of
personality combinations (matching and crashing) on two main types of tasks
(collaborative and competitive). Our experimental results show that personality
compatibility significantly affects the quality of the team's final outcome,
the quality of interactions and the emotions experienced by the team members.
The present study is the first to examine the effect of personality over team
result in crowdsourcing settings, and it has practical implications for the
better design of crowdsourced team work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06318</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06318</id><created>2015-01-26</created><updated>2015-05-09</updated><authors><author><keyname>Kuleshov</keyname><forenames>Volodymyr</forenames></author><author><keyname>Chaganty</keyname><forenames>Arun Tesjavi</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Simultaneous diagonalization: the asymmetric, low-rank, and noisy
  settings</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous matrix diagonalization is used as a subroutine in many machine
learning problems, including blind source separation and paramater estimation
in latent variable models. Here, we extend algorithms for performing joint
diagonalization to low-rank and asymmetric matrices, and we also provide
extensions to the perturbation analysis of these methods. Our results allow
joint diagonalization to be applied in several new settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06323</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06323</id><created>2015-01-26</created><authors><author><keyname>Nishiyama</keyname><forenames>Hiroshi</forenames></author><author><keyname>Kobayashi</keyname><forenames>Yusuke</forenames></author><author><keyname>Yamauchi</keyname><forenames>Yukiko</forenames></author><author><keyname>Kijima</keyname><forenames>Shuji</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>The Parity Hamiltonian Cycle Problem</title><categories>cs.CC</categories><comments>23 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a variant of the Hamiltonian Cycle (HC) problem,
named the Parity Hamiltonian Cycle (PHC) problem: The problem is to find a
closed walk visiting each vertex odd number of times, instead of exactly once.
We show that the PHC problem is in P even when a closed walk is allowed to use
an edge at most z=4 times, by considering a T-join, which is a generalization
of matching. On the other hand, the PHC problem is NP-complete when z=3. In the
case of z=3 however, the problem is in P when an input graph is four-edge
connected, but it still remains NP-complete even when it is two-edge connected.
Thus, we are concerned with the hard case in detail, and give a simple
necessary and sufficient condition that a two-edge connected C&gt;=5-free (or
P6-free) graph has a PHC. Note that the HC problem is known to be NP-complete
for those graph classes. This subject is motivated by a new approach to
connecting a hard problem HC and an easy problem T-join, by relaxing a
constraint of HC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06326</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06326</id><created>2015-01-26</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author></authors><title>The GPU vs Phi Debate: Risk Analytics Using Many-Core Computing</title><categories>cs.DC cs.CE</categories><comments>A modified version of this article is accepted to the Computers and
  Electrical Engineering Journal under the title - &quot;The Hardware Accelerator
  Debate: A Financial Risk Case Study Using Many-Core Computing&quot;; Blesson
  Varghese, &quot;The Hardware Accelerator Debate: A Financial Risk Case Study Using
  Many-Core Computing,&quot; Computers and Electrical Engineering, 2015</comments><doi>10.1016/j.compeleceng.2015.01.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The risk of reinsurance portfolios covering globally occurring natural
catastrophes, such as earthquakes and hurricanes, is quantified by employing
simulations. These simulations are computationally intensive and require large
amounts of data to be processed. The use of many-core hardware accelerators,
such as the Intel Xeon Phi and the NVIDIA Graphics Processing Unit (GPU), are
desirable for achieving high-performance risk analytics. In this paper, we set
out to investigate how accelerators can be employed in risk analytics, focusing
on developing parallel algorithms for Aggregate Risk Analysis, a simulation
which computes the Probable Maximum Loss of a portfolio taking both primary and
secondary uncertainties into account. The key result is that both hardware
accelerators are useful in different contexts; without taking data transfer
times into account the Phi had lowest execution times when used independently
and the GPU along with a host in a hybrid platform yielded best performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06328</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06328</id><created>2015-01-26</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A weakly universal cellular automaton with 2 states on the tiling {11,3}</title><categories>nlin.CG cs.FL</categories><comments>31 pages, 22 figures, 19 tables</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct a weakly universal cellular automaton with two
states only on the tiling {11,3}. The cellular automaton is rotation invariant
and it is a true planar one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06329</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06329</id><created>2015-01-26</created><authors><author><keyname>Steiner</keyname><forenames>Thomas</forenames></author><author><keyname>Verborgh</keyname><forenames>Ruben</forenames></author></authors><title>Disaster Monitoring with Wikipedia and Online Social Networking Sites:
  Structured Data and Linked Data Fragments to the Rescue?</title><categories>cs.SI</categories><comments>Accepted for publication at the AAAI Spring Symposium 2015:
  Structured Data for Humanitarian Technologies: Perfect fit or Overkill?
  #SD4HumTech15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the first results of our ongoing early-stage
research on a realtime disaster detection and monitoring tool. Based on
Wikipedia, it is language-agnostic and leverages user-generated multimedia
content shared on online social networking sites to help disaster responders
prioritize their efforts. We make the tool and its source code publicly
available as we make progress on it. Furthermore, we strive to publish detected
disasters and accompanying multimedia content following the Linked Data
principles to facilitate its wide consumption, redistribution, and evaluation
of its usefulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06339</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06339</id><created>2015-01-26</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>Average power limitations in Sliding Window Contention Resolution
  Diversity Slotted Aloha</title><categories>cs.IT cs.NI math.IT</categories><comments>9th International Wireless Communications and Mobile Computing
  Conference (IWCMC), 2013; IEEE proceedings 2013</comments><doi>10.1109/IWCMC.2013.6583526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently a new Random Access technique based on Aloha and using Interference
Cancellation (IC) named Sliding Window Contention Resolution Diversity Slotted
Aloha (SW-CRDSA) has been introduced. Differently from classic CRDSA that
operates grouping slots in frames, this technique operates in an unframed
manner yielding to better throughput results and smaller average packet delay
with respect to frame-based CRDSA. However as classic CRDSA also SW-CRDSA
relies on multiple transmission of the same packet. While this can be
acceptable in systems where the only limit resides in the peak transmission
power, it could represent a problem when constraints on the average power (e.g.
at the transponder of a satellite system) are present. In this paper, a
comparison in terms of normalized efficiency is carried out between Slotted
Aloha and the two CRDSA techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06350</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06350</id><created>2015-01-26</created><updated>2015-05-06</updated><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author><author><keyname>Huynh</keyname><forenames>The Dang</forenames></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames></author></authors><title>D-Iteration: diffusion approach for solving PageRank</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new method that can accelerate the computation of
the PageRank importance vector. Our method, called D-Iteration (DI), is based
on the decomposition of the matrix-vector product that can be seen as a fluid
diffusion model and is potentially adapted to asynchronous implementation. We
give theoretical results about the convergence of our algorithm and we show
through experimentations on a real Web graph that DI can improve the
computation efficiency compared to other classical algorithm like Power
Iteration, Gauss-Seidel or OPIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06361</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06361</id><created>2015-01-26</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>Random Access in DVB-RCS2: Design and Dynamic Control for Congestion
  Avoidance</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication: IEEE Transactions on Broadcasting; IEEE
  Transactions on Broadcasting, 2014</comments><doi>10.1109/TBC.2013.2293920</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current DVB generation, satellite terminals are expected to be
interactive and capable of transmission in the return channel with satisfying
quality. Considering the bursty nature of their traffic and the long
propagation delay, the use of a random access technique is a viable solution
for such a Medium Access Control (MAC) scenario. In this paper, random access
communication design in DVB-RCS2 is considered with particular regard to the
recently introduced Contention Resolution Diversity Slotted Aloha (CRDSA)
technique. This paper presents a model for design and tackles some issues on
performance evaluation of the system by giving intuitive and effective tools.
Moreover, dynamic control procedures that are able to avoid congestion at the
gateway are introduced. Results show the advantages brought by CRDSA to
DVB-RCS2 with regard to the previous state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06363</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06363</id><created>2015-01-26</created><authors><author><keyname>Plaga</keyname><forenames>Rainer</forenames></author><author><keyname>Merli</keyname><forenames>Dominik</forenames></author></authors><title>A new Definition and Classification of Physical Unclonable Functions</title><categories>cs.CR</categories><comments>6 pages, 3 figures; Proceedings &quot;CS2 '15 Proceedings of the Second
  Workshop on Cryptography and Security in Computing Systems&quot;, Amsterdam, 2015,
  ACM Digital Library</comments><acm-class>K.6.5; H.3.0</acm-class><doi>10.1145/2694805.2694807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new definition of &quot;Physical Unclonable Functions&quot; (PUFs), the first one
that fully captures its intuitive idea among experts, is presented. A PUF is an
information-storage system with a security mechanism that is
  1. meant to impede the duplication of a precisely described
storage-functionality in another, separate system and
  2. remains effective against an attacker with temporary access to the whole
original system.
  A novel classification scheme of the security objectives and mechanisms of
PUFs is proposed and its usefulness to aid future research and security
evaluation is demonstrated. One class of PUF security mechanisms that prevents
an attacker to apply all addresses at which secrets are stored in the
information-storage system, is shown to be closely analogous to cryptographic
encryption. Its development marks the dawn of a new fundamental primitive of
hardware-security engineering: cryptostorage. These results firmly establish
PUFs as a fundamental concept of hardware security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06364</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06364</id><created>2015-01-26</created><authors><author><keyname>Schlei</keyname><forenames>B. R.</forenames></author></authors><title>GPU Programming - Speeding Up the 3D Surface Generator VESTA</title><categories>cs.GR cs.CG cs.DC</categories><comments>1 page, 1 figure, submitted contribution to the GSI Scientific Report
  2014</comments><report-no>GSI SCIENTIFIC REPORT 2014</report-no><doi>10.15120/GR-2015-1-FG-GENERAL-42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The novel &quot;Volume-Enclosing Surface exTraction Algorithm&quot; (VESTA) generates
triangular isosurfaces from computed tomography volumetric images and/or
three-dimensional (3D) simulation data. Here, we present various benchmarks for
GPU-based code implementations of both VESTA and the current state-of-the-art
Marching Cubes Algorithm (MCA). One major result of this study is that VESTA
runs significantly faster than the MCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06370</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06370</id><created>2015-01-26</created><authors><author><keyname>Hasler</keyname><forenames>Laura</forenames></author><author><keyname>Halvey</keyname><forenames>Martin</forenames></author><author><keyname>Villa</keyname><forenames>Robert</forenames></author></authors><title>Augmented Test Collections: A Step in the Right Direction</title><categories>cs.IR</categories><comments>SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper we argue that certain aspects of relevance assessment
in the evaluation of IR systems are oversimplified and that human assessments
represented by qrels should be augmented to take account of contextual factors
and the subjectivity of the task at hand. We propose enhancing test collections
used in evaluation with information related to human assessors and their
interpretation of the task. Such augmented collections would provide a more
realistic and user-focused evaluation, enabling us to better understand the
evaluation process, the performance of systems and user interactions. A first
step is to conduct user studies to examine in more detail what people actually
do when we ask them to judge the relevance of a document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06380</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06380</id><created>2015-01-26</created><authors><author><keyname>Moll&#xe1;</keyname><forenames>Diego</forenames></author><author><keyname>Amini</keyname><forenames>Iman</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Document Distance for the Automated Expansion of Relevance Judgements
  for Information Retrieval Evaluation</title><categories>cs.IR</categories><comments>SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance</comments><acm-class>H.2.4; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports the use of a document distance-based approach to
automatically expand the number of available relevance judgements when these
are limited and reduced to only positive judgements. This may happen, for
example, when the only available judgements are extracted from a list of
references in a published review paper. We compare the results on two document
sets: OHSUMED, based on medical research publications, and TREC-8, based on
news feeds. We show that evaluations based on these expanded relevance
judgements are more reliable than those using only the initially available
judgements, especially when the number of available judgements is very limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06396</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06396</id><created>2015-01-26</created><authors><author><keyname>Teng</keyname><forenames>Ben</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Liu</keyname><forenames>Jiming</forenames></author><author><keyname>Cai</keyname><forenames>Zhipeng</forenames></author><author><keyname>Wan</keyname><forenames>Xiang</forenames></author></authors><title>Exploring the genetic patterns of complex diseases via the integrative
  genome-wide approach</title><categories>cs.CE q-bio.QM stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Genome-wide association studies (GWASs), which assay more than a
million single nucleotide polymorphisms (SNPs) in thousands of individuals,
have been widely used to identify genetic risk variants for complex diseases.
However, most of the variants that have been identified contribute relatively
small increments of risk and only explain a small portion of the genetic
variation in complex diseases. This is the so-called missing heritability
problem. Evidence has indicated that many complex diseases are genetically
related, meaning these diseases share common genetic risk variants. Therefore,
exploring the genetic correlations across multiple related studies could be a
promising strategy for removing spurious associations and identifying
underlying genetic risk variants, and thereby uncovering the mystery of missing
heritability in complex diseases. Results: We present a general and robust
method to identify genetic patterns from multiple large-scale genomic datasets.
We treat the summary statistics as a matrix and demonstrate that genetic
patterns will form a low-rank matrix plus a sparse component. Hence, we
formulate the problem as a matrix recovering problem, where we aim to discover
risk variants shared by multiple diseases/traits and those for each individual
disease/trait. We propose a convex formulation for matrix recovery and an
efficient algorithm to solve the problem. We demonstrate the advantages of our
method using both synthesized datasets and real datasets. The experimental
results show that our method can successfully reconstruct both the shared and
the individual genetic patterns from summary statistics and achieve better
performance compared with alternative methods under a wide range of scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06398</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06398</id><created>2015-01-26</created><authors><author><keyname>Ma&#xdf;berg</keyname><forenames>Jens</forenames></author></authors><title>Solitaire Chess is NP-complete</title><categories>cs.CC math.CO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Solitaire Chess&quot; is a logic puzzle published by Thinkfun, that can be seen
as a single person version of traditional chess. Given a chess board with some
chess pieces of the same color placed on it, the task is to capture all pieces
but one using only moves that are allowed in chess. Moreover, in each move one
piece has to be captured. We prove that deciding if a given instance of
Solitaire Chess is solvable is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06407</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06407</id><created>2015-01-26</created><authors><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Wang</keyname><forenames>Gongpu</forenames></author><author><keyname>Yao</keyname><forenames>Yu-Dong</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>On Secrecy Performance of Antenna Selection Aided MIMO Systems Against
  Eavesdropping</title><categories>cs.IT math.IT</categories><comments>11 pages; IEEE Transactions on Vehicular Technology, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multiple-input multiple-output (MIMO) system
consisting of one source, one destination and one eavesdropper, where each node
is equipped with an arbitrary number of antennas. To improve the security of
source-destination transmissions, we investigate the antenna selection at the
source and propose the optimal antenna selection (OAS) and suboptimal antenna
selection (SAS) schemes, depending on whether the source node has the global
channel state information (CSI) of both the main link (from source to
destination) and wiretap link (from source to eavesdropper). Also, the
traditional space-time transmission (STT) is studied as a benchmark. We
evaluate the secrecy performance of STT, SAS, and OAS schemes in terms of the
probability of zero secrecy capacity. Furthermore, we examine the generalized
secrecy diversity of STT, SAS, and OAS schemes through an asymptotic analysis
of the probability of zero secrecy capacity, as the ratio between the average
gains of the main and wiretap channels tends to infinity. This is different
from the conventional secrecy diversity which assumes an infinite
signal-to-noise ratio (SNR) received at the destination under the condition
that the eavesdropper has a finite received SNR. It is shown that the
generalized secrecy diversity orders of STT, SAS, and OAS schemes are the
product of the number of antennas at source and destination. Additionally,
numerical results show that the proposed OAS scheme strictly outperforms both
the STT and SAS schemes in terms of the probability of zero secrecy capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06412</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06412</id><created>2015-01-26</created><authors><author><keyname>Chuklin</keyname><forenames>Aleksandr</forenames></author><author><keyname>de Rijke</keyname><forenames>Maarten</forenames></author></authors><title>The Anatomy of Relevance: Topical, Snippet and Perceived Relevance in
  Search Result Evaluation</title><categories>cs.IR</categories><comments>SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, the quality of a search engine is often determined using so-called
topical relevance, i.e., the match between the user intent (expressed as a
query) and the content of the document. In this work we want to draw attention
to two aspects of retrieval system performance affected by the presentation of
results: result attractiveness (&quot;perceived relevance&quot;) and immediate usefulness
of the snippets (&quot;snippet relevance&quot;). Perceived relevance may influence
discoverability of good topical documents and seemingly better rankings may in
fact be less useful to the user if good-looking snippets lead to irrelevant
documents or vice-versa. And result items on a search engine result page (SERP)
with high snippet relevance may add towards the total utility gained by the
user even without the need to click those items.
  We start by motivating the need to collect different aspects of relevance
(topical, perceived and snippet relevances) and how these aspects can improve
evaluation measures. We then discuss possible ways to collect these relevance
aspects using crowdsourcing and the challenges arising from that.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06419</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06419</id><created>2015-01-26</created><updated>2015-08-20</updated><authors><author><keyname>Mirandola</keyname><forenames>Diego</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>Critical pairs for the Product Singleton Bound</title><categories>cs.IT math.IT</categories><journal-ref>Information Theory, IEEE Transactions on (Volume:61 , Issue: 9 ),
  2015</journal-ref><doi>10.1109/TIT.2015.2450207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize Product-MDS pairs of linear codes, i.e.\ pairs of codes $C,D$
whose product under coordinatewise multiplication has maximum possible minimum
distance as a function of the code length and the dimensions $\dim C, \dim D$.
We prove in particular, for $C=D$, that if the square of the code $C$ has
minimum distance at least $2$, and $(C,C)$ is a Product-MDS pair, then either
$C$ is a generalized Reed-Solomon code, or $C$ is a direct sum of self-dual
codes. In passing we establish coding-theory analogues of classical theorems of
additive combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06440</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06440</id><created>2015-01-26</created><updated>2015-04-23</updated><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Maric</keyname><forenames>Ivana</forenames></author><author><keyname>Hui</keyname><forenames>Dennis</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>On the Achievable Rates of Multihop Virtual Full-Duplex Relay Channels</title><categories>cs.IT math.IT</categories><comments>To be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a multihop &quot;virtual&quot; full-duplex relay channel as a special case of
a general multiple multicast relay network. For such channel,
quantize-map-and-forward (QMF) (or noisy network coding (NNC)) achieves the
cut-set upper bound within a constant gap where the gap grows {\em linearly}
with the number of relay stages $K$. However, this gap may not be negligible
for the systems with multihop transmissions (i.e., a wireless backhaul
operating at higher frequencies). We have recently attained an improved result
to the capacity scaling where the gap grows {\em logarithmically} as $\log{K}$,
by using an optimal quantization at relays and by exploiting relays' messages
(decoded in the previous time slot) as side-information. In this paper, we
further improve the performance of this network by presenting a mixed scheme
where each relay can perform either decode-and-forward (DF) or QMF with
possibly rate-splitting. We derive the achievable rate and show that the
proposed scheme outperforms the QMF-optimized scheme. Furthermore, we
demonstrate that this performance improvement increases with $K$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06446</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06446</id><created>2015-01-26</created><authors><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Guo</keyname><forenames>Xueying</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Index Policies for Optimal Mean-Variance Trade-Off of Inter-delivery
  Times in Real-Time Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of much current practical interest is the replacement of the wiring
infrastructure connecting approximately 200 sensor and actuator nodes in
automobiles by an access point. This is motivated by the considerable savings
in automobile weight, simplification of manufacturability, and future
upgradability.
  A key issue is how to schedule the nodes on the shared access point so as to
provide regular packet delivery. In this and other similar applications, the
mean of the inter-delivery times of packets, i.e., throughput, is not
sufficient to guarantee service-regularity. The time-averaged variance of the
inter-delivery times of packets is also an important metric.
  So motivated, we consider a wireless network where an Access Point schedules
real-time generated packets to nodes over a fading wireless channel. We are
interested in designing simple policies which achieve optimal mean-variance
tradeoff in interdelivery times of packets by minimizing the sum of
time-averaged means and variances over all clients. Our goal is to explore the
full range of the Pareto frontier of all weighted linear combinations of mean
and variance so that one can fully exploit the design possibilities. We
transform this problem into a Markov decision process and show that the problem
of choosing which node's packet to transmit in each slot can be formulated as a
bandit problem. We establish that this problem is indexable and explicitly
derive the Whittle indices. The resulting Index policy is optimal in certain
cases. We also provide upper and lower bounds on the cost for any policy.
Extensive simulations show that Index policies perform better than previously
proposed policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06450</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06450</id><created>2015-01-26</created><updated>2015-03-18</updated><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>IT-map: an Effective Nonlinear Dimensionality Reduction Method for
  Interactive Clustering</title><categories>stat.ML cs.CV cs.LG</categories><comments>13 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Scientists in many fields have the common and basic need of dimensionality
reduction: visualizing the underlying structure of the massive multivariate
data in a low-dimensional space. However, many dimensionality reduction methods
confront the so-called &quot;crowding problem&quot; that clusters tend to overlap with
each other in the embedding. Previously, researchers expect to avoid that
problem and seek to make clusters maximally separated in the embedding.
However, the proposed in-tree (IT) based method, called IT-map, allows clusters
in the embedding to be locally overlapped, while seeking to make them
distinguishable by some small yet key parts. IT-map provides a simple,
effective and novel solution to cluster-preserving mapping, which makes it
possible to cluster the original data points interactively and thus should be
of general meaning in science and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06451</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06451</id><created>2015-01-26</created><authors><author><keyname>Mu&#xf1;oz</keyname><forenames>Cristina</forenames></author><author><keyname>Leone</keyname><forenames>Pierre</forenames></author></authors><title>Design of a Novel Network Architecture for Distributed Event-Based
  Systems Using Directional Random Walks in an Ubiquitous Sensing Scenario</title><categories>cs.DC cs.NI</categories><comments>10 pages. arXiv admin note: text overlap with arXiv:1408.3033</comments><journal-ref>International Journal on Advances in Networks and Services, 2014
  vol 7 nr 3&amp;4</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Ubiquitous sensing devices frequently disseminate data among them. The use of
a distributed event-based system that decouples publishers from subscribers
arises as an ideal candidate to implement the dissemination process. In this
paper, we present a network architecture that merges the network and overlay
layers of typical structured event-based systems. Directional random walks are
used for the construction of this merged layer. Our strategy avoids using a
specific network protocol that provides point-to-point communication. This
implies that the topology of the network is not maintained, so that nodes not
involved in the system are able to save energy and computing resources. We
evaluate the performance of the overlay layer using directional random walks
and pure random walks for its construction. Our results show that directional
random walks are more efficient because: (1) they use less nodes of the network
for the establishment of the active path of the overlay layer and (2) they have
a more reliable performance. Furthermore, as the number of nodes in the network
increases, so do the number of nodes in the active path of the overlay layer
for the same number of publishers and subscribers. Finally, we discard any
correlation between the number of nodes that form the overlay layer and the
maximum Euclidean distance traversed by the walkers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06456</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06456</id><created>2015-01-26</created><authors><author><keyname>Dey</keyname><forenames>Ratul Dey Sanjay Chakraborty Lopamudra</forenames></author></authors><title>Weather forecasting using Convex hull &amp; K-Means Techniques An Approach</title><categories>cs.DB</categories><comments>1st International Science &amp; Technology Congress(IEMCON-2015) Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is a popular concept of mined necessary data from a large set of
data. Data mining using clustering is a powerful way to analyze data and gives
prediction. In this paper non structural time series data is used to forecast
daily average temperature, humidity and overall weather conditions of Kolkata
city. The air pollution data have been taken from West Bengal Pollution Control
Board to build the original dataset on which the prediction approach of this
paper is studied and applied. This paper describes a new technique to predict
the weather conditions using convex hull which gives structural data and then
apply incremental K-means to define the appropriate clusters. It splits the
total database into four separate databases with respect to different weather
conditions. In the final step, the result will be calculated on the basis of
priority based protocol which is defined based on some mathematical deduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06461</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06461</id><created>2015-01-26</created><updated>2015-01-27</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>National Research Center for Mathematics and Computer Science in the Netherlands</affiliation></author></authors><title>On The Average-Case Complexity of Shellsort</title><categories>cs.DS cs.CC</categories><comments>11 pages LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a lower bound expressed in the increment sequence on the
average-case complexity (number of inversions which is proportional to the
running time) of Shellsort. This lower bound is sharp in every case where it
could be checked. We obtain new results e.g. determining the average-case
complexity precisely in the Yao-Janson-Knuth 3-pass case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06469</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06469</id><created>2015-01-26</created><updated>2015-02-18</updated><authors><author><keyname>Peng</keyname><forenames>Ching-Ting</forenames></author><author><keyname>Wang</keyname><forenames>Li-Chun</forenames></author><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author></authors><title>Optimal Base Station Deployment for Small Cell Networks with
  Energy-Efficient Power Control</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, how to optimally deploy base station density in a small cell
network with energy-efficient power control was investigated. Base stations
(BSs) and users form two independent Poisson point processes (PPPs) in the
network. Since user-centric cell association may lead to void cells that do not
have any users, the power of each BS is controlled in either all-on or on-off
mode depending on whether its cell is void or not. The average cell rates for
each power control mode are first found and their corresponding energy
efficiency is also characterized. The optimal BS density that maximizes the
energy efficiency under a given user density is theoretically proved to exist
and its value can be found numerically. Both analytical and simulated results
indicate that on-off power control is significantly superior to all-on power
control in terms of energy efficiency if BSs are deployed based on their
optimal energy-efficient density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06470</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06470</id><created>2015-01-26</created><updated>2015-02-04</updated><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>Interference Calculation in Asynchronous Random Access Protocols using
  Diversity</title><categories>cs.IT cs.NI math.IT</categories><comments>This paper has been accepted for publication in the Springer's
  Telecommunication Systems journal. The final publication will be made
  available at Springer. Please refer to that version when citing this paper;
  Springer Telecommunication Systems, 2015</comments><doi>10.1007/s11235-015-9970-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of Aloha-based Random Access protocols is interesting when channel
sensing is either not possible or not convenient and the traffic from terminals
is unpredictable and sporadic. In this paper an analytic model for packet
interference calculation in asynchronous Random Access protocols using
diversity is presented. The aim is to provide a tool that avoids time-consuming
simulations to evaluate packet loss and throughput in case decodability is
still possible when a certain interference threshold is not exceeded. Moreover
the same model represents the groundbase for further studies in which iterative
Interference Cancellation is applied to received frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06473</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06473</id><created>2015-01-26</created><authors><author><keyname>Misra</keyname><forenames>Prasant</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author><author><keyname>Yang</keyname><forenames>Mingrui</forenames></author><author><keyname>Duarte</keyname><forenames>Marco</forenames></author><author><keyname>Jha</keyname><forenames>Sanjay</forenames></author></authors><title>Sparsity based Efficient Cross-Correlation Techniques in Sensor Networks</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-correlation is a popular signal processing technique used in numerous
localization and tracking systems for obtaining reliable range information.
However, a practical efficient implementation has not yet been achieved on
resource constrained wireless sensor network platforms. In this paper, we
propose: SparseXcorr: cross-correlation via sparse representation, a new
framework for ranging based on L1-minimization. The key idea is to compress the
signal samples on the mote platform by efficient random projections and
transfer them to a central device, where a convex optimization process
estimates the range by exploiting its sparsity in our proposed correlation
domain. Through sparse representation theory validation, extensive empirical
studies and experiments on an end-to-end acoustic ranging system implemented on
resource limited off-the-shelf sensor nodes, we show that the proposed
framework, together with the proposed correlation domain achieved up to two
orders of magnitude better performance compared to naive approaches such as
working on DCT domain and downsampling. Compared to standard cross-correlation,
SparseXcorr uses only 30-40% measurements to obtain precise range estimates
with an additional bias of only 2-6 cm for applications with high accuracy
requirement; whereas for applications with less constrained accuracy levels,
only 5% measurements are adequate to achieve approximately 100cm ranging
precision. We also present StructSparseXcorr: cross-correlation via structured
sparse representation, an addendum to the proposed computing framework to
overcome shortcoming due to dictionary coherence. For cases of high compression
factor and low signal-to-noise ratio, we show that StructSparseXcorr improves
the performance of SparseXcorr by approximately 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06478</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06478</id><created>2015-01-26</created><updated>2015-02-02</updated><authors><author><keyname>Xu</keyname><forenames>Zhixiang</forenames></author><author><keyname>Gardner</keyname><forenames>Jacob R.</forenames></author><author><keyname>Tyree</keyname><forenames>Stephen</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author></authors><title>Compressed Support Vector Machines</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support vector machines (SVM) can classify data sets along highly non-linear
decision boundaries because of the kernel-trick. This expressiveness comes at a
price: During test-time, the SVM classifier needs to compute the kernel
inner-product between a test sample and all support vectors. With large
training data sets, the time required for this computation can be substantial.
In this paper, we introduce a post-processing algorithm, which compresses the
learned SVM model by reducing and optimizing support vectors. We evaluate our
algorithm on several medium-scaled real-world data sets, demonstrating that it
maintains high test accuracy while reducing the test-time evaluation cost by
several orders of magnitude---in some cases from hours to seconds. It is fair
to say that most of the work in this paper was previously been invented by
Burges and Sch\&quot;olkopf almost 20 years ago. For most of the time during which
we conducted this research, we were unaware of this prior work. However, in the
past two decades, computing power has increased drastically, and we can
therefore provide empirical insights that were not possible in their original
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06479</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06479</id><created>2015-01-26</created><authors><author><keyname>Sridhar</keyname><forenames>Sabarish</forenames></author><author><keyname>Misra</keyname><forenames>Prasant</forenames></author><author><keyname>Warrior</keyname><forenames>Jay</forenames></author></authors><title>CheepSync: A Time Synchronization Service for Resource Constrained
  Bluetooth Low Energy Advertisers</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clock synchronization is highly desirable in distributed systems, including
many applications in the Internet of Things and Humans (IoTH). It improves the
efficiency, modularity and scalability of the system; and optimizes use of
event triggers. For IoTH, Bluetooth Low Energy (BLE) - a subset of the recent
Bluetooth v4:0 stack - provides a low-power and loosely coupled mechanism for
sensor data collection with ubiquitous units (e.g., smartphones and tablets)
carried by humans. This fundamental design paradigm of BLE is enabled by a
range of broadcast advertising modes. While its operational benefits are
numerous, the lack of a common time reference in the broadcast mode of BLE has
been a fundamental limitation. This paper presents and describes CheepSync: a
time synchronization service for BLE advertisers, especially tailored for
applications requiring high time precision on resource constrained BLE
platforms. Designed on top of the existing Bluetooth v4:0 standard, the
CheepSync framework utilizes low-level timestamping and comprehensive error
compensation mechanisms for overcoming uncertainties in message transmission,
clock drift and other system specific constraints. CheepSync was implemented on
custom designed nRF24Cheep beacon platforms (as broadcasters) and commercial
off-the-shelf Android ported smartphones (as passive listeners). We demonstrate
the efficacy of CheepSync by numerous empirical evaluations in a variety of
experimental setups; and show that its average (single-hop) time
synchronization accuracy is in the 10u?s range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06484</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06484</id><created>2015-01-26</created><authors><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author><author><keyname>Varghese</keyname><forenames>Thomas</forenames></author></authors><title>Symmetric Strategy Improvement</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetry is inherent in the definition of most of the two-player zero-sum
games, including parity, mean-payoff, and discounted-payoff games. It is
therefore quite surprising that no symmetric analysis techniques for these
games exist. We develop a novel symmetric strategy improvement algorithm where,
in each iteration, the strategies of both players are improved simultaneously.
We show that symmetric strategy improvement defies Friedmann's traps, which
shook the belief in the potential of classic strategy improvement to be
polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06487</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06487</id><created>2015-01-20</created><authors><author><keyname>Brissaud</keyname><forenames>Florent</forenames></author><author><keyname>Luiz</keyname><forenames>Fernando</forenames></author></authors><title>Average probability of a dangerous failure on demand: Different
  modelling methods, similar results</title><categories>cs.SE math.PR</categories><proxy>ccsd</proxy><journal-ref>11th International Probabilistic Safety Assessment and Management
  Conference and the Annual European Safety and Reliability Conference, Jun
  2012, Helsinki, Finland. 8, pp.6073-6082</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the IEC 61508 functional safety standard, it is required to
estimate the achieved safety integrity of the system due to random hardware
failures. For a safety function operating in a low demand mode, this measure is
the average probability of a dangerous failure on demand (PFDavg). In the
present paper, four techniques have been applied to various configurations of a
case study: fault tree analyses supported by GRIF/Tree, multi-phase Markov
models supported by GRIF/Markov, stochastic Petri nets with predicates
supported by GRIF/Petri, and approximate equations (developed by DNV and
different from those given in IEC 61508) supported by OrbitSIL. It is shown
that all these methods yield very similar results for PFDavg, taking the
characteristics required by the standard into account. The choice of a method
should therefore not be determined by dogmatic assumptions, but should result
of a balance between modelling effort and objectives, given the system
properties. For this task, a discussion about pros and cons of each method is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06493</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06493</id><created>2015-01-26</created><updated>2015-05-07</updated><authors><author><keyname>Larrousse</keyname><forenames>Benjamin</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Coordination in State-Dependent Distributed Networks: The Two-Agent Case</title><categories>cs.IT math.IT</categories><comments>Published in 2015 IEEE International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a coordination problem between two agents (Agents $1$
and $2$) in the presence of a noisy communication channel which depends on an
external system state $\{x_{0,t}\}$. The channel takes as inputs both agents'
actions, $\{x_{1,t}\}$ and $\{x_{2,t}\}$ and produces outputs that are observed
strictly causally at Agent $2$ but not at Agent $1$. The system state is
available either causally or non-causally at Agent $1$ but unknown at Agent
$2$. Necessary and sufficient conditions on a joint distribution
$\bar{Q}(x_0,x_1,x_2)$ to be implementable asymptotically (i.e, when the number
of taken actions grows large) are provided for both causal and non-causal state
information at Agent $1$.
  Since the coordination degree between the agents' actions, $x_{1,t}$ and
$x_{2,t}$, and the system state $x_{0,t}$ is measured in terms of an average
payoff function, feasible payoffs are fully characterized by implementable
joint distributions. In this sense, our results allow us to derive the
performance of optimal power control policies on an interference channel and to
assess the gain provided by non-causal knowledge of the system state at Agent
$1$.
  The derived proofs readily yield new results also for the problem of
state-amplification under a causality constraint at the decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06508</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06508</id><created>2015-01-26</created><updated>2015-09-15</updated><authors><author><keyname>Bajaj</keyname><forenames>Sumeet</forenames></author><author><keyname>Chakraborti</keyname><forenames>Anrin</forenames></author><author><keyname>Sion</keyname><forenames>Radu</forenames></author></authors><title>Practical Foundations of History Independence</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The way data structures organize data is often a function of the sequence of
past operations. The organization of data is referred to as the data
structure's state, and the sequence of past operations constitutes the data
structure's history. A data structure state can therefore be used as an oracle
to derive information about its history. As a result, for history-sensitive
applications, such as privacy in e-voting, incremental signature schemes, and
regulatory compliant data retention; it is imperative to conceal historical
information contained within data structure states.
  Data structure history can be hidden by making data structures history
independent. In this paper, we explore how to achieve history independence.
  We observe that current history independence notions are significantly
limited in number and scope. There are two existing notions of history
independence -- weak history independence (WHI) and strong history independence
(SHI). WHI does not protect against insider adversaries and SHI mandates
canonical representations, resulting in inefficiency.
  We postulate the need for a broad, encompassing notion of history
independence, which can capture WHI, SHI, and a broad spectrum of new history
independence notions. To this end, we introduce $\Delta$history independence
($\Delta$HI), a generic game-based framework that is malleable enough to
accommodate existing and new history independence notions.
  As an essential step towards formalizing $\Delta$HI, we explore the concepts
of abstract data types, data structures, machine models, memory representations
and history independence. Finally, to bridge the gap between theory and
practice, we outline a general recipe for building end-to-end, history
independent systems and demonstrate the use of the recipe in designing two
history independent file systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06515</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06515</id><created>2015-01-26</created><authors><author><keyname>Vidyarthi</keyname><forenames>Shalabh</forenames></author><author><keyname>Shukla</keyname><forenames>Kaushal K</forenames></author></authors><title>Approximation Algorithms for P2P Orienteering and Stochastic Vehicle
  Routing Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the P2P orienteering problem on general metrics and present a
(2+{\epsilon}) approximation algorithm. In the stochastic P2P orienteering
problem we are given a metric and each node has a fixed reward and random size.
The goal is to devise a strategy for visiting the nodes so as to maximize the
expected value of the reward without violating the budget constraints. We
present an approximation algorithm for the non-adaptive variant of the P2P
Stochastic orienteering. As an implication of the approximation to the
stochastic P2P orienteering problem, we define a stochastic vehicle routing
problem with time-windows and present a constant factor approximation solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06518</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06518</id><created>2015-01-26</created><authors><author><keyname>Kardos</keyname><forenames>Frantisek</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Liebenau</keyname><forenames>Anita</forenames></author><author><keyname>Mach</keyname><forenames>Lukas</forenames></author></authors><title>First order convergence of matroids</title><categories>math.CO cs.DM math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model theory based notion of the first order convergence unifies the
notions of the left-convergence for dense structures and the Benjamini-Schramm
convergence for sparse structures. It is known that every first order
convergent sequence of graphs with bounded tree-depth has an analytic limit
object called a limit modeling. We establish the matroid counterpart of this
result: every first order convergent sequence of matroids with bounded
branch-depth representable over a fixed finite field has a limit modeling,
i.e., there exists an infinite matroid with the elements forming a probability
space that has asymptotically the same first order properties. We show that
neither of the bounded branch-depth assumption nor the representability
assumption can be removed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06521</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06521</id><created>2015-01-26</created><updated>2016-02-18</updated><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author></authors><title>Noisy Tensor Completion via the Sum-of-Squares Hierarchy</title><categories>cs.LG cs.DS stat.ML</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the noisy tensor completion problem we observe $m$ entries (whose location
is chosen uniformly at random) from an unknown $n_1 \times n_2 \times n_3$
tensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goal
is to fill in its missing entries using as few observations as possible. Let $n
= \max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is a
polynomial time algorithm based on the sixth level of the sum-of-squares
hierarchy for completing it. Our estimate agrees with almost all of $T$'s
entries almost exactly and works even when our observations are corrupted by
noise. This is also the first algorithm for tensor completion that works in the
overcomplete case when $r &gt; n$, and in fact it works all the way up to $r =
n^{3/2-\epsilon}$.
  Our proofs are short and simple and are based on establishing a new
connection between noisy tensor completion (through the language of Rademacher
complexity) and the task of refuting random constant satisfaction problems.
This connection seems to have gone unnoticed even in the context of matrix
completion. Furthermore, we use this connection to show matching lower bounds.
Our main technical result is in characterizing the Rademacher complexity of the
sequence of norms that arise in the sum-of-squares relaxations to the tensor
nuclear norm. These results point to an interesting new direction: Can we
explore computational vs. sample complexity tradeoffs through the
sum-of-squares hierarchy?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06522</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06522</id><created>2015-01-26</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Models and termination of proof-reduction in the $\lambda$$\Pi$-calculus
  modulo theory</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a notion of model for the $\lambda \Pi$-calculus modulo theory, a
notion of super-consistent theory, and prove that proof-reduction terminates in
the $\lambda \Pi$-calculus modulo a super-consistent theory. We prove this way
the termination of proof-reduction in two theories in the $\lambda
\Pi$-calculus modulo theory, and their consistency: an embedding of Simple type
theory and an embedding of the Calculus of constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06523</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06523</id><created>2015-01-26</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Deduction modulo theory</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>All about proofs. Proofs for all., Jul 2014, Wien, Austria</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey on Deduction modulo theory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06528</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06528</id><created>2015-01-26</created><updated>2015-02-04</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Wigderson</keyname><forenames>Yuval</forenames></author></authors><title>High-Girth Matrices and Polarization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The girth of a matrix is the least number of linearly dependent columns, in
contrast to the rank which is the largest number of linearly independent
columns. This paper considers the construction of {\it high-girth} matrices,
whose probabilistic girth is close to its rank. Random matrices can be used to
show the existence of high-girth matrices with constant relative rank, but the
construction is non-explicit. This paper uses a polar-like construction to
obtain a deterministic and efficient construction of high-girth matrices for
arbitrary fields and relative ranks. Applications to coding and sparse recovery
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06543</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06543</id><created>2015-01-26</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author></authors><title>Construction of Quasi-Cyclic Product Codes</title><categories>cs.IT cs.CR cs.DM math.CO math.IT math.NT</categories><comments>10th International ITG Conference on Systems, Communications and
  Coding (SCC), Feb 2015, Hamburg, Germany</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear quasi-cyclic product codes over finite fields are investigated. Given
the generating set in the form of a reduced Gr{\&quot;o}bner basis of a quasi-cyclic
component code and the generator polynomial of a second cyclic component code,
an explicit expression of the basis of the generating set of the quasi-cyclic
product code is given. Furthermore, the reduced Gr{\&quot;o}bner basis of a
one-level quasi-cyclic product code is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06558</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06558</id><created>2015-01-26</created><authors><author><keyname>Nax</keyname><forenames>Heinrich H.</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Directional learning and the provisioning of public goods</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>7 two-column pages, 3 figures; accepted for publication in Scientific
  Reports</comments><journal-ref>Sci. Rep. 5 (2015) 8010</journal-ref><doi>10.1038/srep08010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an environment where players are involved in a public goods game
and must decide repeatedly whether to make an individual contribution or not.
However, players lack strategically relevant information about the game and
about the other players in the population. The resulting behavior of players is
completely uncoupled from such information, and the individual strategy
adjustment dynamics are driven only by reinforcement feedbacks from each
player's own past. We show that the resulting &quot;directional learning&quot; is
sufficient to explain cooperative deviations away from the Nash equilibrium. We
introduce the concept of k-strong equilibria, which nest both the Nash
equilibrium and the Aumann-strong equilibrium as two special cases, and we show
that, together with the parameters of the learning model, the maximal
k-strength of equilibrium determines the stationary distribution. The
provisioning of public goods can be secured even under adverse conditions, as
long as players are sufficiently responsive to the changes in their own payoffs
and adjust their actions accordingly. Substantial levels of public cooperation
can thus be explained without arguments involving selflessness or social
preferences, solely on the basis of uncoordinated directional (mis)learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06561</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06561</id><created>2015-01-26</created><authors><author><keyname>Desai</keyname><forenames>Amey</forenames></author><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Improved Practical Matrix Sketching with Guarantees</title><categories>cs.DS</categories><comments>27 pages</comments><msc-class>68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrices have become essential data representations for many large-scale
problems in data analytics, and hence matrix sketching is a critical task.
Although much research has focused on improving the error/size tradeoff under
various sketching paradigms, the many forms of error bounds make these
approaches hard to compare in theory and in practice. This paper attempts to
categorize and compare most known methods under row-wise streaming updates with
provable guarantees, and then to tweak some of these methods to gain practical
improvements while retaining guarantees.
  For instance, we observe that a simple heuristic iSVD, with no guarantees,
tends to outperform all known approaches in terms of size/error trade-off. We
modify the best performing method with guarantees FrequentDirections under the
size/error trade-off to match the performance of iSVD and retain its
guarantees. We also demonstrate some adversarial datasets where iSVD performs
quite poorly. In comparing techniques in the time/error trade-off, techniques
based on hashing or sampling tend to perform better. In this setting we modify
the most studied sampling regime to retain error guarantee but obtain dramatic
improvements in the time/error trade-off.
  Finally, we provide easy replication of our studies on APT, a new testbed
which makes available not only code and datasets, but also a computing platform
with fixed environmental settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06582</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06582</id><created>2015-01-26</created><authors><author><keyname>Farajtabar</keyname><forenames>Mehrdad</forenames></author><author><keyname>Gomez-Rodriguez</keyname><forenames>Manuel</forenames></author><author><keyname>Du</keyname><forenames>Nan</forenames></author><author><keyname>Zamani</keyname><forenames>Mohammad</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Back to the Past: Source Identification in Diffusion Networks from
  Partially Observed Cascades</title><categories>cs.SI physics.soc-ph</categories><comments>To appear in the 18th International Conference on Artificial
  Intelligence and Statistics (AISTATS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a piece of malicious information becomes rampant in an information
diffusion network, can we identify the source node that originally introduced
the piece into the network and infer the time when it initiated this? Being
able to do so is critical for curtailing the spread of malicious information,
and reducing the potential losses incurred. This is a very challenging problem
since typically only incomplete traces are observed and we need to unroll the
incomplete traces into the past in order to pinpoint the source. In this paper,
we tackle this problem by developing a two-stage framework, which first learns
a continuous-time diffusion network model based on historical diffusion traces
and then identifies the source of an incomplete diffusion trace by maximizing
the likelihood of the trace under the learned model. Experiments on both large
synthetic and real-world data show that our framework can effectively go back
to the past, and pinpoint the source node and its initiation time significantly
more accurately than previous state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06587</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06587</id><created>2015-01-26</created><authors><author><keyname>Zhu</keyname><forenames>Xiaodan</forenames></author><author><keyname>Turney</keyname><forenames>Peter</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author><author><keyname>Vellino</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Measuring academic influence: Not all citations are equal</title><categories>cs.DL cs.CL cs.LG</categories><journal-ref>Journal of the Association for Information Science and Technology,
  66: 408-427</journal-ref><doi>10.1002/asi.23179</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The importance of a research article is routinely measured by counting how
many times it has been cited. However, treating all citations with equal weight
ignores the wide variety of functions that citations perform. We want to
automatically identify the subset of references in a bibliography that have a
central academic influence on the citing paper. For this purpose, we examine
the effectiveness of a variety of features for determining the academic
influence of a citation. By asking authors to identify the key references in
their own work, we created a data set in which citations were labeled according
to their academic influence. Using automatic feature selection with supervised
machine learning, we found a model for predicting academic influence that
achieves good performance on this data set using only four features. The best
features, among those we evaluated, were those based on the number of times a
reference is mentioned in the body of a citing paper. The performance of these
features inspired us to design an influence-primed h-index (the hip-index).
Unlike the conventional h-index, it weights citations by how many times a
reference is mentioned. According to our experiments, the hip-index is a better
indicator of researcher performance than the conventional h-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06595</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06595</id><created>2015-01-26</created><updated>2015-02-23</updated><authors><author><keyname>Geyik</keyname><forenames>Sahin Cem</forenames></author><author><keyname>Dasdan</keyname><forenames>Ali</forenames></author><author><keyname>Lee</keyname><forenames>Kuang-Chih</forenames></author></authors><title>User Clustering in Online Advertising via Topic Models</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the domain of online advertising, our aim is to serve the best ad to a
user who visits a certain webpage, to maximize the chance of a desired action
to be performed by this user after seeing the ad. While it is possible to
generate a different prediction model for each user to tell if he/she will act
on a given ad, the prediction result typically will be quite unreliable with
huge variance, since the desired actions are extremely sparse, and the set of
users is huge (hundreds of millions) and extremely volatile, i.e., a lot of new
users are introduced everyday, or are no longer valid. In this paper we aim to
improve the accuracy in finding users who will perform the desired action, by
assigning each user to a cluster, where the number of clusters is much smaller
than the number of users (in the order of hundreds). Each user will fall into
the same cluster with another user if their event history are similar. For this
purpose, we modify the probabilistic latent semantic analysis (pLSA) model by
assuming the independence of the user and the cluster id, given the history of
events. This assumption helps us to identify a cluster of a new user without
re-clustering all the users. We present the details of the algorithm we
employed as well as the distributed implementation on Hadoop, and some initial
results on the clusters that were generated by the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06598</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06598</id><created>2015-01-26</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Online Nonparametric Regression with General Loss Functions</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1402.2594</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes minimax rates for online regression with arbitrary
classes of functions and general losses. We show that below a certain threshold
for the complexity of the function class, the minimax rates depend on both the
curvature of the loss function and the sequential complexities of the class.
Above this threshold, the curvature of the loss does not affect the rates.
Furthermore, for the case of square loss, our results point to the interesting
phenomenon: whenever sequential and i.i.d. empirical entropies match, the rates
for statistical and online learning are the same.
  In addition to the study of minimax regret, we derive a generic forecaster
that enjoys the established optimal rates. We also provide a recipe for
designing online prediction algorithms that can be computationally efficient
for certain problems. We illustrate the techniques by deriving existing and new
forecasters for the case of finite experts and for online linear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06613</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06613</id><created>2015-01-26</created><authors><author><keyname>M&#xed;nguez</keyname><forenames>Roberto</forenames></author><author><keyname>Garc&#xed;a-Bertrand</keyname><forenames>Raquel</forenames></author><author><keyname>Arroyo</keyname><forenames>Jos&#xe9; Manuel</forenames></author></authors><title>Adaptive Robust Transmission Network Expansion Planning using Structural
  Reliability and Decomposition Techniques</title><categories>cs.CE math.OC</categories><comments>32 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1501.05480</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural reliability and decomposition techniques have recently proved to
be appropriate tools for solving robust uncertain mixed-integer linear programs
using ellipsoidal uncertainty sets. In fact, its computational performance
makes this type of problem to be an alternative method in terms of tractability
with respect to robust problems based on cardinality constrained uncertainty
sets. This paper extends the use of these techniques for solving an adaptive
robust optimization (ARO) problem, i.e. the adaptive robust solution of the
transmission network expansion planning for energy systems. The formulation of
this type of problem materializes on a three-level mixed-integer optimization
formulation, which based on structural reliability methods, can be solved using
an ad-hoc decomposition technique. The method allows the use of the correlation
structure of the uncertain variables involved by means of their
variance-covariance matrix, and besides, it provides a new interpretation of
the robust problem based on quantile optimization. We also compare results with
respect to robust optimization methods that consider cardinality constrained
uncertainty sets. Numerical results on an illustrative example, the IEEE-24 and
IEEE 118-bus test systems demonstrate that the algorithm is comparable in terms
of computational performance with respect to existing robust methods with the
additional advantage that the correlation structure of the uncertain variables
involved can be incorporated straightforwardly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06614</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06614</id><created>2015-01-26</created><authors><author><keyname>Saha</keyname><forenames>Sudip</forenames></author><author><keyname>Adiga</keyname><forenames>Abhijin</forenames></author><author><keyname>Prakash</keyname><forenames>B. Aditya</forenames></author><author><keyname>Vullikanti</keyname><forenames>Anil Kumar S.</forenames></author></authors><title>Approximation Algorithms for Reducing the Spectral Radius to Control
  Epidemic Spread</title><categories>cs.SI</categories><comments>Part of it will be published in SDM,15 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The largest eigenvalue of the adjacency matrix of a network (referred to as
the spectral radius) is an important metric in its own right. Further, for
several models of epidemic spread on networks (e.g., the `flu-like' SIS model),
it has been shown that an epidemic dies out quickly if the spectral radius of
the graph is below a certain threshold that depends on the model parameters.
This motivates a strategy to control epidemic spread by reducing the spectral
radius of the underlying network.
  In this paper, we develop a suite of provable approximation algorithms for
reducing the spectral radius by removing the minimum cost set of edges
(modeling quarantining) or nodes (modeling vaccinations), with different time
and quality tradeoffs. Our main algorithm, \textsc{GreedyWalk}, is based on the
idea of hitting closed walks of a given length, and gives an
$O(\log^2{n})$-approximation, where $n$ denotes the number of nodes; it also
performs much better in practice compared to all prior heuristics proposed for
this problem. We further present a novel sparsification method to improve its
running time.
  In addition, we give a new primal-dual based algorithm with an even better
approximation guarantee ($O(\log n)$), albeit with slower running time. We also
give lower bounds on the worst-case performance of some of the popular
heuristics. Finally we demonstrate the applicability of our algorithms and the
properties of our solutions via extensive experiments on multiple synthetic and
real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06618</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06618</id><created>2015-01-26</created><authors><author><keyname>Tereshkov</keyname><forenames>Vasiliy M.</forenames></author></authors><title>A Simple Observer for Gyro and Accelerometer Biases in Land Navigation
  Systems</title><categories>cs.SY math.DS math.OC</categories><comments>14 pages, 5 figures in Journal of Navigation, 2015.
  http://journals.cambridge.org/article_S0373463315000016</comments><msc-class>93B30</msc-class><acm-class>I.2.9</acm-class><journal-ref>Journal of Navigation (2015), 68, pp. 635-645</journal-ref><doi>10.1017/S0373463315000016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various applications of land vehicle navigation and automatic guidance
systems, Global Navigation Satellite System/Inertial Measurement Unit
(GNSS/IMU) positioning performance crucially depends on the attitude
determination accuracy affected by gyro and accelerometer bias instabilities.
Traditional bias estimation approaches based on the Kalman filter suffer from
implementation complexity and require non-intuitive tuning procedures. In this
paper we propose, as an alternative, a simple observer that estimates inertial
sensor biases exclusively in terms of quantities with obvious geometrical
meaning. By this, any multidimensional vector-matrix operations are avoided and
observer tuning is substantially simplified. The observer has been successfully
tested in a farming vehicle navigation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06619</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06619</id><created>2015-01-26</created><authors><author><keyname>Nakashima</keyname><forenames>Yuto</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Constructing LZ78 Tries and Position Heaps in Linear Time for Large
  Alphabets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first worst-case linear-time algorithm to compute the
Lempel-Ziv 78 factorization of a given string over an integer alphabet. Our
algorithm is based on nearest marked ancestor queries on the suffix tree of the
given string. We also show that the same technique can be used to construct the
position heap of a set of strings in worst-case linear time, when the set of
strings is given as a trie.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="71000" completeListSize="102538">1122234|72001</resumptionToken>
</ListRecords>
</OAI-PMH>
