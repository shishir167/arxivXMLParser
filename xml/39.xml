<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:06:26Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|38001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2304</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2304</id><created>2012-11-10</created><authors><author><keyname>Acharya</keyname><forenames>Ayan</forenames></author><author><keyname>Hruschka</keyname><forenames>Eduardo R.</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author><author><keyname>Sarwar</keyname><forenames>Badrul</forenames></author><author><keyname>Ruvini</keyname><forenames>Jean-David</forenames></author></authors><title>Probabilistic Combination of Classifier and Cluster Ensembles for
  Non-transductive Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised models can provide supplementary soft constraints to help
classify new target data under the assumption that similar objects in the
target set are more likely to share the same class label. Such models can also
help detect possible differences between training and target distributions,
which is useful in applications where concept drift may take place. This paper
describes a Bayesian framework that takes as input class labels from existing
classifiers (designed based on labeled data from the source domain), as well as
cluster labels from a cluster ensemble operating solely on the target data to
be classified, and yields a consensus labeling of the target data. This
framework is particularly useful when the statistics of the target data drift
or change from those of the training data. We also show that the proposed
framework is privacy-aware and allows performing distributed learning when
data/models have sharing restrictions. Experiments show that our framework can
yield superior results to those provided by applying classifier ensembles only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2313</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2313</id><created>2012-11-10</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author><author><keyname>Bishop</keyname><forenames>Steven</forenames></author><author><keyname>Lukowicz</keyname><forenames>Paul</forenames></author><author><keyname>Consortium</keyname><forenames>the FuturICT</forenames></author></authors><title>FuturICT</title><categories>nlin.AO cs.CY physics.soc-ph</categories><comments>For related information see http://www.futurict.eu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FuturlCT is a FET Flagship project using collective, participatory research,
integrated across ICT, the social sciences and complexity science, to design
socio-inspired technology and develop a science of global, socially interactive
systems. The project will bring together, on a global level, Big Data, new
modelling techniques and new forms of interaction, leading to a new
understanding of society and its coevolution with technology. It aims to
understand, explore and manage our complex, connected world in a more
sustainable and resilient way. FuturICT is motivated by the fact that
ubiquitous communication and sensing blur the boundaries between the physical
and digital worlds, creating unparalleled opportunities for understanding the
socio-economic fabric of our world, and for empowering humanity to make
informed, responsible decisions for its future. The intimate, complex and
dynamic relationship between global, networked ICT systems and human society
directly influences the complexity and manageability of both. This also opens
up the possibility to fundamentally change the way ICT will be designed, built
and operated, to reflect the need for socially interactive, ethically
sensitive, trustworthy, self-organised and reliable systems. FuturICT will
create a new public resource - value-oriented tools and models to aggregate,
access, query and understand vast amounts of data. Information from open
sources, real-time devices and mobile sensors will be integrated with
multi-scale models of the behaviour of social, technological, environmental and
economic systems, which can be interrogated by policy-makers, business people
and citizens alike. Together, these will build an eco-system that will lead to
new business models, scientific paradigm shifts and more rapid and effective
ways to create and disseminate new knowledge and social benefits - thereby
forming an innovation accelerator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2322</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2322</id><created>2012-11-10</created><updated>2012-11-13</updated><authors><author><keyname>Del Genio</keyname><forenames>Charo I.</forenames></author><author><keyname>Gross</keyname><forenames>Thilo</forenames></author></authors><title>Graph isomorphism and automorphism problems are polynomial</title><categories>cs.DS</categories><comments>After appearance of the manuscript on arXiv we received communication
  from Dr. Matthew Anderson with a counter example. Although we performed
  extensive tests prior to uploading, the counter example is correct and there
  seems to be an error in our procedure. We would like to beg the pardon of any
  colleagues who spent time reading the preprint and once again thank Dr.
  Anderson for his communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex questions in biology, physics, and mathematics can be mapped to
the graph isomorphism problem and the closely related graph automorphism
problem. In particular, these problems appear in the context of network
visualization, computational logic, structure recognition, and dynamics of
complex systems. Both problems have previously been suspected, but not proven,
to be NP-complete. In this paper we propose an algorithm that solves both graph
automorphism and isomorphism problems in polynomial time. The algorithm can be
easily implemented and thus opens up a wide range of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2333</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2333</id><created>2012-11-10</created><authors><author><keyname>Fioriti</keyname><forenames>Vincenzo</forenames></author><author><keyname>Chinnici</keyname><forenames>Marta</forenames></author></authors><title>Predicting the sources of an outbreak with a spectral technique</title><categories>math-ph cs.SI math.MP physics.soc-ph</categories><comments>5 tables, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The epidemic spreading of a disease can be described by a contact network
whose nodes are persons or centers of contagion and links heterogeneous
relations among them. We provide a procedure to identify multiple sources of an
outbreak or their closer neighbors. Our methodology is based on a simple
spectral technique requiring only the knowledge of the undirected contact
graph. The algorithm is tested on a variety of graphs collected from outbreaks
including fluency, H5N1, Tbc, in urban and rural areas. Results show that the
spectral technique is able to identify the source nodes if the graph
approximates a tree sufficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2338</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2338</id><created>2012-11-10</created><updated>2013-08-09</updated><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author></authors><title>An Efficient Encryption Algorithm for P2P Networks Robust Against
  Man-in-the-Middle Adversary</title><categories>cs.CR</categories><journal-ref>IJCSI, International Journal of Computer Science Issues, Volume 9,
  Issue 6, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer (P2P) networks have become popular as a new paradigm for
information exchange and are being used in many applications such as file
sharing, distributed computing, video conference, VoIP, radio and TV
broadcasting. This popularity comes with security implications and
vulnerabilities that need to be addressed. Especially duo to direct
communication between two end nodes in P2P networks, these networks are
potentially vulnerable to &quot;Man-in-the-Middle&quot; attacks. In this paper, we
propose a new public-key cryptosystem for P2P networks that is robust against
Man-in-the-Middle adversary. This cryptosystem is based on RSA and knapsack
problems. Our precoding-based algorithm uses knapsack problem for performing
permutation and padding random data to the message. We show that comparing to
other proposed cryptosystems, our algorithm is more efficient and it is fully
secure against an active adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2340</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2340</id><created>2012-11-10</created><updated>2015-04-11</updated><authors><author><keyname>Mackenthun</keyname><forenames>Kenneth M.</forenames><suffix>Jr</suffix></author></authors><title>Time domain and symmetry domain analysis of a controllable group system,
  group code, and group shift</title><categories>cs.IT math.IT</categories><comments>New material</comments><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What we term here a spectral domain approach to group systems has been
previously given in the literature by Forney and Trott. Here we consider a time
domain approach and symmetry domain approach. In some sense, the symmetry
domain approach is more fundamental than the time or spectral domain approach
since it does not depend on choice of basis. A symmetry in the symmetry domain
permutes shift vectors of essentially a shift register graph. Given a basis,
each shift vector corresponds to a unique generator vector of the group system
in either the time or spectral domain. Then a symmetry permutes generator
vectors in the time or spectral domain, and this gives information about the
structure of a group system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2354</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2354</id><created>2012-11-10</created><authors><author><keyname>Fard</keyname><forenames>Amin Milani</forenames></author></authors><title>Privacy Preserving Web Query Log Publishing: A Survey on Anonymization
  Techniques</title><categories>cs.DB cs.CR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Releasing Web query logs which contain valuable information for research or
marketing, can breach the privacy of search engine users. Therefore rendering
query logs to limit linking a query to an individual while preserving the data
usefulness for analysis, is an important research problem. This survey provides
an overview and discussion on the recent studies on this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2361</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2361</id><created>2012-11-10</created><updated>2013-03-22</updated><authors><author><keyname>Jahandideh</keyname><forenames>Hossein</forenames></author><author><keyname>Asef-Vaziri</keyname><forenames>Ardavan</forenames></author><author><keyname>Modarres</keyname><forenames>Mohammad</forenames></author></authors><title>Genetic Algorithm for Designing a Convenient Facility Layout for a
  Circular Flow Path</title><categories>cs.NE</categories><comments>Accepted to the 2013 IEEE Symposium Series on Computational
  Intelligence: Swarm Intelligence Symposium. This paper has been withdrawn by
  the author, by the request of the supervisor, to be updated, fixed, and
  combined with other papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a heuristic for designing facility layouts that are
convenient for designing a unidirectional loop for material handling. We use
genetic algorithm where the objective function and crossover and mutation
operators have all been designed specifically for this purpose. Our design is
made under flexible bay structure and comparisons are made with other layouts
from the literature that were designed under flexible bay structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2365</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2365</id><created>2012-11-10</created><authors><author><keyname>Eriksson-Bique</keyname><forenames>Sylvester</forenames><affiliation>University of Washington and University of Helsinki</affiliation></author><author><keyname>Kirkpatrick</keyname><forenames>David</forenames><affiliation>University of British Columbia</affiliation></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames><affiliation>University of Helsinki</affiliation></author></authors><title>Discrete Dubins Paths</title><categories>cs.DM cs.CG math.OC</categories><comments>26 pages</comments><msc-class>51D20, 52C10</msc-class><acm-class>G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Dubins path is a shortest path with bounded curvature. The seminal result
in non-holonomic motion planning is that (in the absence of obstacles) a Dubins
path consists either from a circular arc followed by a segment followed by
another arc, or from three circular arcs [Dubins, 1957]. Dubins original proof
uses advanced calculus; later, Dubins result was reproved using control theory
techniques [Reeds and Shepp, 1990], [Sussmann and Tang, 1991], [Boissonnat,
C\'er\'ezo, and Leblond, 1994].
  We introduce and study a discrete analogue of curvature-constrained motion.
We show that shortest &quot;bounded-curvature&quot; polygonal paths have the same
structure as Dubins paths. The properties of Dubins paths follow from our
results as a limiting case---this gives a new, &quot;discrete&quot; proof of Dubins
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2367</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2367</id><created>2012-11-10</created><authors><author><keyname>Fu</keyname><forenames>Ada Wai-Chee</forenames></author><author><keyname>Wu</keyname><forenames>Huanhuan</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Chu</keyname><forenames>Shumo</forenames></author><author><keyname>Wong</keyname><forenames>Raymond Chi-Wing</forenames></author></authors><title>IS-LABEL: an Independent-Set based Labeling Scheme for Point-to-Point
  Distance Querying on Large Graphs</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing shortest path or distance between two query
vertices in a graph, which has numerous important applications. Quite a number
of indexes have been proposed to answer such distance queries. However, all of
these indexes can only process graphs of size barely up to 1 million vertices,
which is rather small in view of many of the fast-growing real-world graphs
today such as social networks and Web graphs. We propose an efficient index,
which is a novel labeling scheme based on the independent set of a graph. We
show that our method can handle graphs of size three orders of magnitude larger
than those existing indexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2376</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2376</id><created>2012-11-11</created><updated>2013-03-05</updated><authors><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author><author><keyname>Srivastava</keyname><forenames>Piyush</forenames></author></authors><title>Lee-Yang theorems and the complexity of computing averages</title><categories>cs.CC cond-mat.stat-mech cs.DM</categories><comments>30 pages, 3 figures. This version adds definitions of
  complexity-theoretic terms</comments><acm-class>G.2.1; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of computing average quantities related to spin
systems, such as the mean magnetization and susceptibility in the ferromagnetic
Ising model, and the average dimer count (or average size of a matching) in the
monomer-dimer model. By establishing connections between the complexity of
computing these averages and the location of the complex zeros of the partition
function, we show that these averages are #P-hard to compute. In case of the
Ising model, our approach requires us to prove an extension of the famous
Lee-Yang Theorem from the 1950s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2378</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2378</id><created>2012-11-11</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE, CHD Castellucio</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author></authors><title>Hybrid methodology for hourly global radiation forecasting in
  Mediterranean area</title><categories>cs.NE cs.LG physics.ao-ph stat.AP</categories><proxy>ccsd</proxy><doi>10.1016/j.renene.2012.10.049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The renewable energies prediction and particularly global radiation
forecasting is a challenge studied by a growing number of research teams. This
paper proposes an original technique to model the insolation time series based
on combining Artificial Neural Network (ANN) and Auto-Regressive and Moving
Average (ARMA) model. While ANN by its non-linear nature is effective to
predict cloudy days, ARMA techniques are more dedicated to sunny days without
cloud occurrences. Thus, three hybrids models are suggested: the first proposes
simply to use ARMA for 6 months in spring and summer and to use an optimized
ANN for the other part of the year; the second model is equivalent to the first
but with a seasonal learning; the last model depends on the error occurred the
previous hour. These models were used to forecast the hourly global radiation
for five places in Mediterranean area. The forecasting performance was compared
among several models: the 3 above mentioned models, the best ANN and ARMA for
each location. In the best configuration, the coupling of ANN and ARMA allows
an improvement of more than 1%, with a maximum in autumn (3.4%) and a minimum
in winter (0.9%) where ANN alone is the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2379</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2379</id><created>2012-11-11</created><updated>2013-04-03</updated><authors><author><keyname>Gouillart</keyname><forenames>Emmanuelle</forenames><affiliation>SVI</affiliation></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames><affiliation>LPCT</affiliation></author><author><keyname>Mezard</keyname><forenames>Marc</forenames><affiliation>LPTMS</affiliation></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames><affiliation>IPHT</affiliation></author></authors><title>Belief Propagation Reconstruction for Discrete Tomography</title><categories>cs.NA cond-mat.stat-mech cs.IT math.IT</categories><proxy>ccsd</proxy><journal-ref>Inverse Problems 29, 3 (2013) 035003</journal-ref><doi>10.1088/0266-5611/29/3/035003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the reconstruction of a two-dimensional discrete image from a set
of tomographic measurements corresponding to the Radon projection. Assuming
that the image has a structure where neighbouring pixels have a larger
probability to take the same value, we follow a Bayesian approach and introduce
a fast message-passing reconstruction algorithm based on belief propagation.
For numerical results, we specialize to the case of binary tomography. We test
the algorithm on binary synthetic images with different length scales and
compare our results against a more usual convex optimization approach. We
investigate the reconstruction error as a function of the number of tomographic
measurements, corresponding to the number of projection angles. The belief
propagation algorithm turns out to be more efficient than the
convex-optimization algorithm, both in terms of recovery bounds for noise-free
projections, and in terms of reconstruction quality when moderate Gaussian
noise is added to the projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2384</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2384</id><created>2012-11-11</created><authors><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>Strong Bounds for Evolution in Undirected Graphs</title><categories>cs.DS math.PR</categories><comments>28 pages, 18 fugures</comments><acm-class>G.3; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the generalized Moran process, as introduced by Lieberman
et al. [Nature, 433:312-316, 2005], where the individuals of a population
reside on the vertices of an undirected connected graph. The initial population
has a single mutant of a fitness value $r$, residing at some vertex $v$, while
every other individual has initially fitness 1. The main quantity of interest
is the fixation probability, i.e. the probability that eventually the whole
graph is occupied by descendants of the mutant. In this work we concentrate on
the fixation probability when the mutant is initially on a specific vertex $v$.
We then aim at finding graphs that have many &quot;strong starts&quot; (or many &quot;weak
starts&quot;) for the mutant. Thus we introduce a parameterized notion of selective
amplifiers/suppressors, i.e. graphs with at least some $h(n)$ vertices
(starting points of the mutant), which fixate the graph with large (resp.
small) probability. We prove the existence of strong selective amplifiers and
of quite strong selective suppressors. Regarding the traditional notion of
fixation probability from a random start, we provide the first non-trivial
upper and lower bounds: first we demonstrate the non-existence of &quot;strong
universal&quot; amplifiers, i.e. we prove that for any graph the fixation
probability from a random start is at most $1-\frac{c(r)}{n^{3/4}}$. Then we
prove the &quot;Thermal Theorem&quot;, stating that for any graph, when the mutant starts
at vertex $v$, the fixation probability is at least $(r-1)/(r+\frac{deg
v}{deg_{min}})$. This implies the first nontrivial lower bound for the usual
notion of fixation probability, which is almost tight. This theorem extends the
&quot;Isothermal Theorem&quot; of Lieberman et al. for regular graphs. Our proof
techniques are original and are based on new domination arguments which may be
of general interest in Markov Processes that are of the general birth-death
type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2386</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2386</id><created>2012-11-11</created><authors><author><keyname>Borham</keyname><forenames>Mohamed Labib</forenames></author><author><keyname>Mostafa</keyname><forenames>Mostafa Sami</forenames></author><author><keyname>Shamardan</keyname><forenames>Hossam Eldeen Moustafa</forenames></author></authors><title>MDSA: Modified Distributed Storage Algorithm for Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>5 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:1201.0178, arXiv:1011.2795, arXiv:0902.1278, arXiv:0908.4419 by other
  authors</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA),Vol. 3, No. 10, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a Modified distributed storage algorithm for
wireless sensor networks (MDSA). Wireless Sensor Networks, as it is well known,
suffer of power limitation, small memory capacity,and limited processing
capabilities. Therefore, every node may disappear temporarily or permanently
from the network due to many different reasons such as battery failure or
physical damage. Since every node collects significant data about its region,
it is important to find a methodology to recover these data in case of failure
of the source node. Distributed storage algorithms provide reliable access to
data through the redundancy spread over individual unreliable nodes. The
proposed algorithm uses flooding to spread data over the network and unicasting
to provide controlled data redundancy through the network. We evaluate the
performance of the proposed algorithm through implementation and simulation. We
show the results and the performance evaluation of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2396</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2396</id><created>2012-11-11</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>Alhussain</keyname><forenames>Thamer</forenames></author></authors><title>A Conceptual Framework for the Promotion of Trusted Online Retailing
  Environment in Saudi Arabia</title><categories>cs.CY</categories><journal-ref>International Journal of Business and Management 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model conceptual framework that is aimed at promoting
trust in the online retailing environment in the Kingdom of Saudi Arabia (KSA).
Despite rapid Internet growth, the development of online retailing in Saudi
Arabia continues to progress very slowly compared to that of the developed and
leading developing countries. To determine the reason behind the sluggish
growth of online retailing in the KSA, a mixed methods study involving
retailers and customers was conducted in four stages. The outcomes of the study
point to distrust in the online retailing environment in Saudi Arabia as a key
inhibitory factor for growth. As such, a five-part model is proposed to promote
trust in the online shopping environment in the KSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2398</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2398</id><created>2012-11-11</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author></authors><title>Government Initiatives: The Missing Key for E-commerce Growth in KSA</title><categories>cs.CY</categories><journal-ref>World Academy of Science, Engineering and Technology 77 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the issues that influence online retailing in Saudi
Arabia. Retailers in Saudi Arabia have been reserved in their adoption of
electronically delivered aspects of their business. Despite the fact that Saudi
Arabia has the largest and fastest growth of ICT marketplaces in the Arab
region, e-commerce activities are not progressing at the same speed. Only very
few Saudi companies, mostly medium and large companies from the manufacturing
sector, are involved in e-commerce implementation. Based on qualitative data
collected by conducting interviews with 16 retailers and 16 potential customers
in Saudi Arabia, several factors influencing online retailing diffusion in
Saudi Arabia are identified. However, government support comes the highest and
most influencing factor for online retailing growth as identified by both
parties; retailers and potential customers in Saudi Arabia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2399</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2399</id><created>2012-11-11</created><authors><author><keyname>Tagiew</keyname><forenames>Rustam</forenames></author></authors><title>Mining Determinism in Human Strategic Behavior</title><categories>cs.GT cs.AI</categories><comments>8 pages, no figures, EEML 2012</comments><journal-ref>Experimental Economics and Machine Learning 2012, CEUR-WS Vol-870,
  urn:nbn:de:0074-870-0</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work lies in the fusion of experimental economics and data mining. It
continues author's previous work on mining behaviour rules of human subjects
from experimental data, where game-theoretic predictions partially fail to
work. Game-theoretic predictions aka equilibria only tend to success with
experienced subjects on specific games, what is rarely given. Apart from game
theory, contemporary experimental economics offers a number of alternative
models. In relevant literature, these models are always biased by psychological
and near-psychological theories and are claimed to be proven by the data. This
work introduces a data mining approach to the problem without using vast
psychological background. Apart from determinism, no other biases are regarded.
Two datasets from different human subject experiments are taken for evaluation.
The first one is a repeated mixed strategy zero sum game and the second -
repeated ultimatum game. As result, the way of mining deterministic
regularities in human strategic behaviour is described and evaluated. As future
work, the design of a new representation formalism is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2402</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2402</id><created>2012-11-11</created><authors><author><keyname>Bahaddad</keyname><forenames>Adel A.</forenames></author><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Houghton</keyname><forenames>Luke</forenames></author></authors><title>To What Extent Would E-mall Enable SMEs to Adopt E-Commerce?</title><categories>cs.CY</categories><journal-ref>International Journal of Business and Management vol 7, no 22,
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents findings from a study of e-commerce adoption by Small and
Medium Enterprises (SMEs) in Saudi Arabia. Only tiny number of Saudi commercial
organizations, mostly medium and large companies from the manufacturing sector,
in involved in e-commerce implementation. The latest report released in 2010 by
The Communications and Information Technology Commission (CITC) in Saudi Arabia
shows that only 8% of businesses sell online. Accordingly new research has been
conducted to explore to what extent electronic mall (e-mall) would enable SMEs
in Saudi Arabia to adopt and use online channels for their sales. A
quantitative analysis of responses obtained from a survey of 108 SMEs in Saudi
Arabia was conducted. The main results of the current analysis demonstrate the
significant of organizational factors, and technology and environmental
factors. Interestingly, traditional &amp; cultural factors have no significance in
this regard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2404</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2404</id><created>2012-11-11</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>Al-Ghaith</keyname><forenames>Waleed</forenames></author></authors><title>Factors unflinching e-commerce adoption by retailers in Saudi Arabia:
  Qual Analysis</title><categories>cs.CY</categories><journal-ref>The Electronic Journal on Information Systems in Developing
  Countries (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the preliminary findings of a study researching the
diffusion and the adoption of online retailing in Saudi Arabia. It reports new
research that identifies and explores the key issues that positively and
negatively influence retailers in Saudi Arabia regarding the adoption of
electronic commerce. Retailers in Saudi Arabia have been reserved in their
adoption of electronically delivered aspects of their business. Despite the
fact that Saudi Arabia has the largest and fastest growth of ICT marketplaces
in the Arab region, ecommerce activities are not progressing at the same speed.
Only a tiny number of Saudi commercial organizations, mostly medium and large
companies from the manufacturing sector, are involved in e-commerce
implementation. Based on qualitative data, collected by conducting interviews
with a sample population of retail sector decision makers in Saudi Arabia, both
positive and negative issues influencing retailer adoption of electronic
retailing systems in Saudi Arabia are identified. A number of impediments which
include cultural, business and technical issues were reported. Facilitating
factors include access to educational programs and awareness building of
e-commerce, government support and assistance for ecommerce, trustworthy and
secure online payment options, developing strong ICT infrastructure, and
provision of sample e-commerce software to trial. While literature reveals that
government promotion has had limited effects on the diffusion of e-commerce in
most countries, this study significantly indicates government promotion and
support as a key driver to online retailing in KSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2405</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2405</id><created>2012-11-11</created><authors><author><keyname>von Stengel</keyname><forenames>Bernhard</forenames></author></authors><title>Rank-1 Games With Exponentially Many Nash Equilibria</title><categories>cs.GT</categories><comments>4 pages</comments><msc-class>91A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank of a bimatrix game (A,B) is the rank of the matrix A+B. We give a
construction of rank-1 games with exponentially many equilibria, which answers
an open problem by Kannan and Theobald (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2406</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2406</id><created>2012-11-11</created><authors><author><keyname>Alfarraj</keyname><forenames>Osama</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author></authors><title>EGovernment Stage Model: Evaluating the Rate of Web Development Progress
  of Government Websites in Saudi Arabia</title><categories>cs.CY</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications, International Journal of Advanced Computer Science and
  Applications Vol. 2, No. 9, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contributes to the issue of eGovernment implementation in Saudi
Arabia by discussing the current situation of ministry websites. It evaluates
the rate of web development progress of vital government websites in Saudi
Arabia using the eGovernment stage model. In 2010, Saudi Arabia ranked 58th in
the world and 4th in the Gulf region in eGovernment readiness according to
United Nations reports. In particular, Saudi Arabia has ranked 75th worldwide
for its online service index and its components compared to the neighbouring
Gulf country of Bahrain, which was ranked 8th for the same index. While this is
still modest in relation to the Saudi government expectation concerning its
vision for eGovernment implementation for 2010, and the results achieved by the
neighbouring Gulf countries such as Bahrain and the United Arab Emirates on the
eGovernment index, the Saudi government has endeavoured to meet the public
needs concerning eGovernment and carry out the implementation of eGovernment
properly. Governments may heed the importance of actively launching official
government websites as the main portals for delivering their online services to
all the different categories of eGovernment (including G2C, G2B, and G2G).
However, certain Saudi ministries have not given due attention to this vital
issue. This is evidenced by the fact that some of their websites are not fully
developed or do not yet exist, which clearly impedes that particular ministry
from appropriately delivering eServices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2407</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2407</id><created>2012-11-11</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Nguyen</keyname><forenames>Ann</forenames></author><author><keyname>Nguyen</keyname><forenames>Jeremy</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author></authors><title>Factors influencing the decision of Saudi consumers to purchase form
  online retailers: Quantitative Analysis</title><categories>cs.CY</categories><journal-ref>IADIS International Conference e-Commerce 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the preliminary findings of a study researching the
diffusion and the adoption of online retailing in Saudi Arabia. It reports new
research that identifies and explores the key issues that positively and
negatively influence the decision of Saudi customers to buy from online
retailers in Saudi Arabia. Although Saudi Arabia has the largest and fastest
growth of ICT marketplaces in the Arab region, e-commerce activities are not
progressing at the same speed. While the overall research project involves
exploratory research using mixed methods, the focus of this paper is on a
quantitative analysis of responses obtained from a survey of Saudi customers,
with the design of the questionnaire instrument being based on the findings of
a qualitative analysis reported in a previous paper. The main findings of the
current analysis include a list of key factors that affect Saudi customers'
purchase from Saudi online retailers, and quantitative indications of the
relative strengths of the various relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2410</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2410</id><created>2012-11-11</created><authors><author><keyname>Alshehri</keyname><forenames>Mohammed</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>Alhussain</keyname><forenames>Thamer</forenames></author><author><keyname>Alghamdi</keyname><forenames>Rayed</forenames></author></authors><title>The Effects of Website Quality on Adoption of E-Government Service:
  AnEmpirical Study Applying UTAUT Model Using SEM</title><categories>cs.CY</categories><comments>23rd Australasian Conference On Information Systems 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today global age, e-government services have become the main channel for
online communication between the government and its citizens. They aim to
provide citizens with more accessible, accurate, real-time and high quality
services. Therefore, the quality of government websites which provide
e-services is an essential factor in the successful adoption of e-government
services by the public. This paper discusses an investigation of the effect of
the Website Quality (WQ) factor on the acceptance of using e-government
services (G2C) in the Kingdom of Saudi Arabia (KSA) by adopting the Unified
Theory of Acceptance and Use of Technology (UTAUT) Model. Survey Data collected
from 400 respondents were examined using the structural equation modelling
(SEM) technique and utilising AMOS tools. This study found that the factors
that significantly influenced the Use Behaviour of e-government services in KSA
(USE) include Performance Expectancy (PE), Effort expectancy (EE), Facilitating
Conditions (FC) and Website Quality (WQ), while the construct known Social
Influence (SI) did not. Moreover, the results confirm the importance of quality
government websites and support systems as one of the main significant and
influential factors of e-government services adoption. The results of this
study can be helpful to Saudi governmental sectors to adjust their corporate
strategies and plans to advance successful adoption and diffusion of
e-government services (G2C) in KSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2412</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2412</id><created>2012-11-11</created><authors><author><keyname>Shehri</keyname><forenames>Waleed Abdullah Al</forenames></author></authors><title>Work Integrated Learning (WIL) In Virtual Reality (VR)</title><categories>cs.HC</categories><comments>13 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 2, September 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this report is to initially discuss the concepts WIL and VR,
their main characteristics and current applications. Moreover, the pros and
cons of VWIL are also analyzed. Finally, the report presents some
recommendation including further researches into areas where VWIL has potential
to be successful in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2417</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2417</id><created>2012-11-11</created><updated>2013-01-24</updated><authors><author><keyname>Mori</keyname><forenames>Hiromu</forenames></author><author><keyname>Matsumoto</keyname><forenames>Yoshihiro</forenames></author><author><keyname>Mori</keyname><forenames>Koichi</forenames></author><author><keyname>Kryssanov</keyname><forenames>Victor</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Struzik</keyname><forenames>Zbigniew R.</forenames></author><author><keyname>Hori</keyname><forenames>Gen</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Multicommand Tactile Brain Computer Interface based on Fingertips or
  Head Stimulation</title><categories>q-bio.NC cs.HC</categories><comments>This paper has been withdrawn by the author due to extension of the
  research and submission to the other conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents results from a computational neuroscience study conducted
to test vibrotactile stimuli delivered to subject fingertips and head areas in
order to evoke the somatosensory brain responses utilized in a haptic brain
computer interface (hBCI) paradigm. We present the preliminary and very
encouraging results, with subjects conducting online hBCI interfacing
experiments, ranging from 40% to 90% with a very fast inter-stimulus-interval
(ISI) of 250ms. The presented results confirm our hypothesis that the hBCI
paradigm concept is valid and it allows for rapid stimuli presentation in order
to achieve a satisfactory information-transfer-rate of the novel BCI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2430</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2430</id><created>2012-11-11</created><updated>2012-11-13</updated><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Chyzak</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Feng</keyname><forenames>Ruyong</forenames></author><author><keyname>Fu</keyname><forenames>Guofeng</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author></authors><title>On the Existence of Telescopers for Mixed Hypergeometric Terms</title><categories>cs.SC math.CO</categories><msc-class>33F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a criterion for the existence of telescopers for mixed
hypergeometric terms, which is based on multiplicative and additive
decompositions. The criterion enables us to determine the termination of
Zeilberger's algorithms for mixed hypergeometric inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2441</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2441</id><created>2012-11-11</created><updated>2013-07-15</updated><authors><author><keyname>Wang</keyname><forenames>Lanhui</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Exact and Stable Recovery of Rotations for Robust Synchronization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synchronization problem over the special orthogonal group $SO(d)$
consists of estimating a set of unknown rotations $R_1,R_2,...,R_n$ from noisy
measurements of a subset of their pairwise ratios $R_{i}^{-1}R_{j}$. The
problem has found applications in computer vision, computer graphics, and
sensor network localization, among others. Its least squares solution can be
approximated by either spectral relaxation or semidefinite programming followed
by a rounding procedure, analogous to the approximation algorithms of
\textsc{Max-Cut}. The contribution of this paper is three-fold: First, we
introduce a robust penalty function involving the sum of unsquared deviations
and derive a relaxation that leads to a convex optimization problem; Second, we
apply the alternating direction method to minimize the penalty function;
Finally, under a specific model of the measurement noise and for both complete
and random measurement graphs, we prove that the rotations are exactly and
stably recovered, exhibiting a phase transition behavior in terms of the
proportion of noisy measurements. Numerical simulations confirm the phase
transition behavior for our method as well as its improved accuracy compared to
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2445</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2445</id><created>2012-11-11</created><authors><author><keyname>Khaled</keyname><forenames>Abdelilah</forenames></author><author><keyname>Idrissi</keyname><forenames>Mohammed Abdou Janati</forenames></author></authors><title>A Semi-Structured Tailoring-Driven Approach for ERP Selection</title><categories>cs.OH</categories><comments>10 pages, 7 figues; IJCSI International Journal of Computer Science
  Issues, Vol. 9, Issue 5, No 2, September 2012</comments><acm-class>H.3.4; C.4; D.2.1; D.2.2; D.2.8; D.2.9; D.2.10; G.1.1; G.1.3; G.1.6</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 2, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been widely reported that selecting an inappropriate system is a major
reason for ERP implementation failures. The selection of an ERP system is
therefore critical. While the number of papers related to ERP implementation is
substantial, ERP evaluation and selection approaches have received few
attention. Motivated by the adaptation concept of the ERP systems, we propose
in this paper a semi-structured approach for ERP system selection that differs
from existing models in that it has a more holistic focus by simultaneously 1)
considering the anticipated fitness of ERP solutions after the optimal
resolution, within limited resources, of a set of the identified mismatches and
2) evaluating candidate products according to both functional and
non-functional requirements. The approach consists of an iterative selection
process model and an evaluation methodology based on 0-1 linear programming and
MACBETH technique to elaborate multi-criteria performance expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2459</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2459</id><created>2012-11-11</created><updated>2014-09-01</updated><authors><author><keyname>Giraldo</keyname><forenames>Luis G. Sanchez</forenames></author><author><keyname>Rao</keyname><forenames>Murali</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>Measures of Entropy from Data Using Infinitely Divisible Kernels</title><categories>cs.LG cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory provides principled ways to analyze different inference
and learning problems such as hypothesis testing, clustering, dimensionality
reduction, classification, among others. However, the use of information
theoretic quantities as test statistics, that is, as quantities obtained from
empirical data, poses a challenging estimation problem that often leads to
strong simplifications such as Gaussian models, or the use of plug in density
estimators that are restricted to certain representation of the data. In this
paper, a framework to non-parametrically obtain measures of entropy directly
from data using operators in reproducing kernel Hilbert spaces defined by
infinitely divisible kernels is presented. The entropy functionals, which bear
resemblance with quantum entropies, are defined on positive definite matrices
and satisfy similar axioms to those of Renyi's definition of entropy.
Convergence of the proposed estimators follows from concentration results on
the difference between the ordered spectrum of the Gram matrices and the
integral operators associated to the population quantities. In this way,
capitalizing on both the axiomatic definition of entropy and on the
representation power of positive definite kernels, the proposed measure of
entropy avoids the estimation of the probability distribution underlying the
data. Moreover, estimators of kernel-based conditional entropy and mutual
information are also defined. Numerical experiments on independence tests
compare favourably with state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2476</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2476</id><created>2012-11-11</created><authors><author><keyname>Soufiani</keyname><forenames>Hossein Azari</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Random Utility Theory for Social Choice</title><categories>cs.MA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random utility theory models an agent's preferences on alternatives by
drawing a real-valued score on each alternative (typically independently) from
a parameterized distribution, and then ranking the alternatives according to
scores. A special case that has received significant attention is the
Plackett-Luce model, for which fast inference methods for maximum likelihood
estimators are available. This paper develops conditions on general random
utility models that enable fast inference within a Bayesian framework through
MC-EM, providing concave loglikelihood functions and bounded sets of global
maxima solutions. Results on both real-world and simulated data provide support
for the scalability of the approach and capability for model selection among
general random utility models including Plackett-Luce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2483</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2483</id><created>2012-11-11</created><authors><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Guarding and Searching Polyhedra</title><categories>cs.CG cs.CC</categories><comments>Ph.D. thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the Art Gallery Problem and the Searchlight Scheduling Problem in
3-dimensional polyhedral environments, putting special emphasis on edge guards
and orthogonal polyhedra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2487</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2487</id><created>2012-11-11</created><authors><author><keyname>Karamad</keyname><forenames>Ehsan</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author><author><keyname>Chow</keyname><forenames>Jerry</forenames></author></authors><title>Power Control and Interference Management in Dense Wireless Networks</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of interference management and power control in terms
of maximization of a general utility function. For the utility functions under
consideration, we propose a power control algorithm based on a fixed-point
iteration; further, we prove local convergence of the algorithm in the
neighborhood of the optimal power vector. Our algorithm has several benefits
over the previously studied works in the literature: first, the algorithm can
be applied to problems other than network utility maximization (NUM), e.g.,
power control in a relay network; second, for a network with $N$ wireless
transmitters, the computational complexity of the proposed algorithm is
$\mathcal{O}(N^2)$ calculations per iteration (significantly smaller than the
$\mathcal{O}(N^3) $ calculations for Newton's iterations or gradient descent
approaches). Furthermore, the algorithm converges very fast (usually in less
than 15 iterations), and in particular, if initialized close to the optimal
solution, the convergence speed is much faster. This suggests the potential of
tracking variations in slowly fading channels. Finally, when implemented in a
distributed fashion, the algorithm attains the optimal power vector with a
signaling/computational complexity of only $\mathcal{O}(N)$ at each node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2488</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2488</id><created>2012-11-11</created><authors><author><keyname>Chen</keyname><forenames>Congcong</forenames></author><author><keyname>Yu</keyname><forenames>Jiguo</forenames></author><author><keyname>Zhang</keyname><forenames>Xiujuan</forenames></author></authors><title>Edge Dominating Capability based Backbone Construction in Wireless
  Networks</title><categories>cs.NI</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  5, No 1, September 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Constructing a connected dominating set as the virtual backbone plays an
important role in wireless networks. In this paper, we propose two novel
approximate algorithms for dominating set and connected dominating set in
wireless networks, respectively. Both of the algorithms are based on edge
dominating capability which is a novel notion proposed in this paper.
Simulations show that each of proposed algorithm has good performance
especially in dense wireless networks .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2495</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2495</id><created>2012-11-11</created><authors><author><keyname>Firdhous</keyname><forenames>M. F. M.</forenames></author><author><keyname>Basnayake</keyname><forenames>D. L.</forenames></author><author><keyname>Kodithuwakku</keyname><forenames>K. H. L.</forenames></author><author><keyname>Haththalla</keyname><forenames>N. K.</forenames></author><author><keyname>Charlin</keyname><forenames>N. W.</forenames></author><author><keyname>Bandara</keyname><forenames>P. M. R. I. K.</forenames></author></authors><title>Route Planning Made Easy - An Automated System for Sri Lanka</title><categories>cs.CY</categories><comments>17 pages, 17 figures and 2 tables</comments><journal-ref>Route Planning Made Easy-An Automated System for Sri Lanka, Bhumi:
  the Planning Research Journal of the University of Moratuwa, Vol. 02 No. 02,
  2010, pp. 13-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commercial cities like Colombo constantly face to problem of traffic
congestion due to the large number of people visiting the city for various
reasons. Also these cities have a large number of roads with many roads
connecting any two selected locations. Finding the best path between two
locations in Colombo city is not a trivial task, due to the complexity of the
road network and other reasons such as heavy traffic, changes to the road
networks such as road closures and one-ways. This paper presents the results of
a study carried out to understand this problem and development of a system to
plan the travel way ahead of the planned day or time of the journey. This
system can compute the best route from between two locations taking multiple
factors such as traffic conditions, road closures or one-way declarations etc.,
into account. This system also has the capability to compute the best route
between any two locations on a future date based on the road conditions on that
date. The system comprises three main modules and two user interfaces one for
normal users and the other for administrators. The Administrative interface can
only be accessed via web browser running on a computer, while the other
interface can be accessed either via a web browser or a GPRS enabled mobile
phone. The system is powered mainly by the Geographic Information System (GIS)
technology and the other supporting technologies used are database management
system, ASP.Net technology and the GPRS technology. Finally the developed
system was evaluated for its functionality and user friendliness using a user
survey. The results of the survey are also presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2496</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2496</id><created>2012-11-11</created><updated>2014-01-19</updated><authors><author><keyname>Banaei</keyname><forenames>Armin</forenames></author><author><keyname>Cline</keyname><forenames>Daren B. H.</forenames></author><author><keyname>Georghiades</keyname><forenames>Costas N.</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>On Asymptotic Statistics for Geometric Routing Schemes in Wireless
  Ad-Hoc Networks</title><categories>cs.NI</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a methodology employing statistical analysis and
stochastic geometry to study geometric routing schemes in wireless ad-hoc
networks. In particular, we analyze the network layer performance of one such
scheme, the random $\frac{1}{2}$disk routing scheme, which is a localized
geometric routing scheme in which each node chooses the next relay randomly
among the nodes within its transmission range and in the general direction of
the destination. The techniques developed in this paper enable us to establish
the asymptotic connectivity and the convergence results for the mean and
variance of the routing path lengths generated by geometric routing schemes in
random wireless networks. In particular, we approximate the progress of the
routing path towards the destination by a Markov process and determine the
sufficient conditions that ensure the asymptotic connectivity for both dense
and large-scale ad-hoc networks deploying the random $\frac{1}{2}$disk routing
scheme. Furthermore, using this Markov characterization, we show that the
expected length (hop-count) of the path generated by the random
$\frac{1}{2}$disk routing scheme normalized by the length of the path generated
by the ideal direct-line routing, converges to $3\pi/4$ asymptotically.
Moreover, we show that the variance-to-mean ratio of the routing path length
converges to $9\pi^2/64-1$ asymptotically. Through simulation, we show that the
aforementioned asymptotic statistics are in fact quite accurate even for finite
granularity and size of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2497</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2497</id><created>2012-11-11</created><authors><author><keyname>Rahmati</keyname><forenames>Mojtaba</forenames></author><author><keyname>Duman</keyname><forenames>Tolga M.</forenames></author></authors><title>A Note on the Deletion Channel Capacity</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memoryless channels with deletion errors as defined by a stochastic channel
matrix allowing for bit drop outs are considered in which transmitted bits are
either independently deleted with probability $d$ or unchanged with probability
$1-d$. Such channels are information stable, hence their Shannon capacity
exists. However, computation of the channel capacity is formidable, and only
some upper and lower bounds on the capacity exist. In this paper, we first show
a simple result that the parallel concatenation of two different independent
deletion channels with deletion probabilities $d_1$ and $d_2$, in which every
input bit is either transmitted over the first channel with probability of
$\lambda$ or over the second one with probability of $1-\lambda$, is nothing
but another deletion channel with deletion probability of $d=\lambda
d_1+(1-\lambda)d_2$. We then provide an upper bound on the concatenated
deletion channel capacity $C(d)$ in terms of the weighted average of $C(d_1)$,
$C(d_2)$ and the parameters of the three channels. An interesting consequence
of this bound is that $C(\lambda d_1+(1-\lambda))\leq \lambda C(d_1)$ which
enables us to provide an improved upper bound on the capacity of the i.i.d.
deletion channels, i.e., $C(d)\leq 0.4143(1-d)$ for $d\geq 0.65$. This
generalizes the asymptotic result by Dalai as it remains valid for all $d\geq
0.65$. Using the same approach we are also able to improve upon existing upper
bounds on the capacity of the deletion/substitution channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2500</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2500</id><created>2012-11-11</created><authors><author><keyname>El-Sayed</keyname><forenames>Mohamed A.</forenames></author></authors><title>A New Algorithm Based Entropic Threshold for Edge Detection in Images</title><categories>cs.CV</categories><msc-class>94A17, 68U10, 94A08</msc-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 1, 2011, 71-78</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge detection is one of the most critical tasks in automatic image analysis.
There exists no universal edge detection method which works well under all
conditions. This paper shows the new approach based on the one of the most
efficient techniques for edge detection, which is entropy-based thresholding.
The main advantages of the proposed method are its robustness and its
flexibility. We present experimental results for this method, and compare
results of the algorithm against several leading edge detection methods, such
as Canny, LOG, and Sobel. Experimental results demonstrate that the proposed
method achieves better result than some classic methods and the quality of the
edge detector of the output images is robust and decrease the computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2501</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2501</id><created>2012-11-11</created><authors><author><keyname>Valtchev</keyname><forenames>Petko</forenames></author><author><keyname>Mounaouar</keyname><forenames>Omar</forenames></author><author><keyname>Cherkaoui</keyname><forenames>Omar</forenames></author><author><keyname>Dimitrov</keyname><forenames>Alexandar</forenames></author><author><keyname>Marchand</keyname><forenames>Laurent</forenames></author></authors><title>FlowME: Lattice-based Traffic Measurement</title><categories>cs.NI</categories><acm-class>C.4; C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow-based traffic measurement is a very challenging problem: Managing
counters for each individual traffic flow in hardware resources knowingly
struggle to scale with high-speed links. In this paper we propose a novel
lattice theory-based approach that improves flow-based measurement performances
and scales by keeping the number of the maintained hardware counters to a
minimum (result mathematically established in the paper). The crucial
contribution of the lattice is to map the computational semantics of the packet
processing to user requests for traffic measurement thus allowing for a
better-informed and focused counter assignment.
  An implementation over an Openflow switch, FlowME, was developed and
evaluated upon its memory usage, performance overhead, and processing effort to
generate the minimal solution. Experimental results indicate a significant
decrease in resource consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2502</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2502</id><created>2012-11-11</created><authors><author><keyname>El-Sayed</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Hafeez</keyname><forenames>Tarek Abd-El</forenames></author></authors><title>New Edge Detection Technique based on the Shannon Entropy in Gray Level
  Images</title><categories>cs.CV</categories><msc-class>94A17, 68U10, 94A08</msc-class><journal-ref>International Journal on Computer Science and Engineering (IJCSE),
  Vol. 3 No. 6, 2011, 2224-2232</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge detection is an important field in image processing. Edges characterize
object boundaries and are therefore useful for segmentation, registration,
feature extraction, and identification of objects in a scene. In this paper, an
approach utilizing an improvement of Baljit and Amar method which uses Shannon
entropy other than the evaluation of derivatives of the image in detecting
edges in gray level images has been proposed. The proposed method can reduce
the CPU time required for the edge detection process and the quality of the
edge detector of the output images is robust. A standard test images, the
real-world and synthetic images are used to compare the results of the proposed
edge detector with the Baljit and Amar edge detector method. In order to
validate the results, the run time of the proposed method and the pervious
method are presented. It has been observed that the proposed edge detector
works effectively for different gray scale digital images. The performance
evaluation of the proposed technique in terms of the measured CPU time and the
quality of edge detector method are presented. Experimental results demonstrate
that the proposed method achieve better result than the relevant classic
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2512</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2512</id><created>2012-11-12</created><updated>2013-06-02</updated><authors><author><keyname>Zhao</keyname><forenames>Hong</forenames></author><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Minimal cost feature selection of data with normal distribution
  measurement errors</title><categories>cs.AI cs.LG</categories><comments>This paper has been withdrawn by the author due to an error of the
  title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimal cost feature selection is devoted to obtain a trade-off between test
costs and misclassification costs. This issue has been addressed recently on
nominal data. In this paper, we consider numerical data with measurement errors
and study minimal cost feature selection in this model. First, we build a data
model with normal distribution measurement errors. Second, the neighborhood of
each data item is constructed through the confidence interval. Comparing with
discretized intervals, neighborhoods are more reasonable to maintain the
information of data. Third, we define a new minimal total cost feature
selection problem through considering the trade-off between test costs and
misclassification costs. Fourth, we proposed a backtracking algorithm with
three effective pruning techniques to deal with this problem. The algorithm is
tested on four UCI data sets. Experimental results indicate that the pruning
techniques are effective, and the algorithm is efficient for data sets with
nearly one thousand objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2517</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2517</id><created>2012-11-12</created><updated>2013-03-11</updated><authors><author><keyname>Cao</keyname><forenames>Yanchuang</forenames></author><author><keyname>Wen</keyname><forenames>Lihua</forenames></author><author><keyname>Rong</keyname><forenames>Junjie</forenames></author></authors><title>A SVD accelerated kernel-independent fast multipole method and its
  application to BEM</title><categories>cs.NA math.NA</categories><comments>19 pages, 4 figures</comments><journal-ref>Boundary Elements and Other Mesh Reduction Methods XXXVI. 431-443.
  2013</journal-ref><doi>10.2495/BEM360351</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The kernel-independent fast multipole method (KIFMM) proposed in [1] is of
almost linear complexity. In the original KIFMM the time-consuming M2L
translations are accelerated by FFT. However, when more equivalent points are
used to achieve higher accuracy, the efficiency of the FFT approach tends to be
lower because more auxiliary volume grid points have to be added. In this
paper, all the translations of the KIFMM are accelerated by using the singular
value decomposition (SVD) based on the low-rank property of the translating
matrices. The acceleration of M2L is realized by first transforming the
associated translating matrices into more compact form, and then using low-rank
approximations. By using the transform matrices for M2L, the orders of the
translating matrices in upward and downward passes are also reduced. The
improved KIFMM is then applied to accelerate BEM. The performance of the
proposed algorithms are demonstrated by three examples. Numerical results show
that, compared with the original KIFMM, the present method can reduce about 40%
of the iterating time and 25% of the memory requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2521</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2521</id><created>2012-11-12</created><authors><author><keyname>Stefanescu</keyname><forenames>Razvan</forenames></author><author><keyname>Navon</keyname><forenames>Ionel Michael</forenames></author></authors><title>POD/DEIM Nonlinear model order reduction of an ADI implicit shallow
  water equations model</title><categories>physics.comp-ph cs.NA math.NA</categories><comments>41 pages, 16 figures, 12 tables, one appendix</comments><doi>10.1016/j.jcp.2012.11.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we consider a 2-D shallow-water equations (SWE) model on
a $\beta$-plane solved using an alternating direction fully implicit (ADI)
finite-difference scheme on a rectangular domain. The scheme was shown to be
unconditionally stable for the linearized equations.
  The discretization yields a number of nonlinear systems of algebraic
equations. We then use a proper orthogonal decomposition (POD) to reduce the
dimension of the SWE model. Due to the model nonlinearities, the computational
complexity of the reduced model still depends on the number of variables of the
full shallow - water equations model. By employing the discrete empirical
interpolation method (DEIM) we reduce the computational complexity of the
reduced order model due to its depending on the nonlinear full dimension model
and regain the full model reduction expected from the POD model.
  To emphasize the CPU gain in performance due to use of POD/DEIM, we also
propose testing an explicit Euler finite difference scheme (EE) as an
alternative to the ADI implicit scheme for solving the swallow water equations
model.
  We then proceed to assess the efficiency of POD/DEIM as a function of number
of spatial discretization points, time steps, and POD basis functions. As was
expected, our numerical experiments showed that the CPU time performances of
POD/DEIM schemes are proportional to the number of mesh points. Once the number
of spatial discretization points exceeded 10000 and for 90 DEIM interpolation
points, the CPU time was decreased by a factor of 10 in case of POD/DEIM
implicit SWE scheme and by a factor of 15 for the POD/DEIM explicit SWE scheme
in comparison with the corresponding POD SWE schemes. Our numerical tests
revealed that if the number of points selected by DEIM algorithm reached 50,
the approximation errors due to POD/DEIM and POD reduced systems have the same
orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2532</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2532</id><created>2012-11-12</created><updated>2012-11-26</updated><authors><author><keyname>Guillot</keyname><forenames>Dominique</forenames></author><author><keyname>Rajaratnam</keyname><forenames>Bala</forenames></author><author><keyname>Rolfs</keyname><forenames>Benjamin T.</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Wong</keyname><forenames>Ian</forenames></author></authors><title>Iterative Thresholding Algorithm for Sparse Inverse Covariance
  Estimation</title><categories>stat.CO cs.LG stat.ML</categories><comments>25 pages, 1 figure, 4 tables. Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The L1-regularized maximum likelihood estimation problem has recently become
a topic of great interest within the machine learning, statistics, and
optimization communities as a method for producing sparse inverse covariance
estimators. In this paper, a proximal gradient method (G-ISTA) for performing
L1-regularized covariance matrix estimation is presented. Although numerous
algorithms have been proposed for solving this problem, this simple proximal
gradient method is found to have attractive theoretical and numerical
properties. G-ISTA has a linear rate of convergence, resulting in an O(log e)
iteration complexity to reach a tolerance of e. This paper gives eigenvalue
bounds for the G-ISTA iterates, providing a closed-form linear convergence
rate. The rate is shown to be closely related to the condition number of the
optimal point. Numerical convergence results and timing comparisons for the
proposed method are presented. G-ISTA is shown to perform very well, especially
when the optimal point is well-conditioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2547</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2547</id><created>2012-11-12</created><authors><author><keyname>Nayyer</keyname><forenames>Muhammad Ziad</forenames></author></authors><title>Analysis of AODV over increased density and mobility in Intelligent
  Transportation System</title><categories>cs.NI</categories><comments>10 pages,9 figures, 3 graphs, 2 tables, research paper; IJCSI
  International Journal of Computer Science Issues, Vol. 9, Issue 5, No 1, 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Currently the area of VANET lacks in having some better designed algorithms
to handle dynamic change and frequent disruption due to the high mobility of
the vehicles. There are many techniques to disseminate messages across the
moving vehicles but they are all highly dependent on some conditions involving
flow, density and speed. The two techniques that are commonly used are AODV (Ad
Hoc on Demand Distance Vector) and DSRC (Dedicated Short Range Communication).
This work presents a detailed analysis of AODV. This study is focused on the
use of AODV in Intelligent Transportation System. The limitations in the
working of AODV routing protocol has been identified and proved. These
limitations can be removed to some extent in order to increase the performance
of vehicular networks and make the driving more safe and easy for a normal user
as well as the implementation complications will be removed and an efficient
system implementation will be possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2555</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2555</id><created>2012-11-12</created><updated>2014-03-16</updated><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author><author><keyname>Hatano</keyname><forenames>Naomichi</forenames></author></authors><title>Viral spreading of daily information in online social networks</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 3 figures, accepted for publication in Physica A:
  Statistical Mechanics and its Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explain a possible mechanism of an information spreading on a network
which spreads extremely far from a seed node, namely the viral spreading. On
the basis of a model of the information spreading in an online social network,
in which the dynamics is expressed as a random multiplicative process of the
spreading rates, we will show that the correlation between the spreading rates
enhances the chance of the viral spreading, shifting the tipping point at which
the spreading goes viral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2556</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2556</id><created>2012-11-12</created><authors><author><keyname>Anifowose</keyname><forenames>Fatai Adesina</forenames></author></authors><title>A Comparative Study of Gaussian Mixture Model and Radial Basis Function
  for Voice Recognition</title><categories>cs.LG cs.CV stat.ML</categories><comments>9 pages, 10 figures; International Journal of Advanced Computer
  Science and Applications (IJACSA), Vol. 1, No.3, September 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comparative study of the application of Gaussian Mixture Model (GMM) and
Radial Basis Function (RBF) in biometric recognition of voice has been carried
out and presented. The application of machine learning techniques to biometric
authentication and recognition problems has gained a widespread acceptance. In
this research, a GMM model was trained, using Expectation Maximization (EM)
algorithm, on a dataset containing 10 classes of vowels and the model was used
to predict the appropriate classes using a validation dataset. For experimental
validity, the model was compared to the performance of two different versions
of RBF model using the same learning and validation datasets. The results
showed very close recognition accuracy between the GMM and the standard RBF
model, but with GMM performing better than the standard RBF by less than 1% and
the two models outperformed similar models reported in literature. The DTREG
version of RBF outperformed the other two models by producing 94.8% recognition
accuracy. In terms of recognition time, the standard RBF was found to be the
fastest among the three models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2569</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2569</id><created>2012-11-12</created><authors><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author><author><keyname>Lam</keyname><forenames>Ka Chun</forenames></author><author><keyname>Yau</keyname><forenames>Shing-Tung</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng</forenames></author></authors><title>Teichm\&quot;uller extremal mapping and its applications to landmark matching
  registration</title><categories>cs.CG cs.GR cs.MM math.DG</categories><comments>26 pages, 21 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Registration, which aims to find an optimal 1-1 correspondence between
shapes, is an important process in different research areas. Conformal mappings
have been widely used to obtain a diffeomorphism between shapes that minimizes
angular distortion. Conformal registrations are beneficial since it preserves
the local geometry well. However, when landmark constraints are enforced,
conformal mappings generally do not exist. This motivates us to look for a
unique landmark matching quasi-conformal registration, which minimizes the
conformality distortion. Under suitable condition on the landmark constraints,
a unique diffeomporphism, called the Teichm\&quot;uller extremal mapping between two
surfaces can be obtained, which minimizes the maximal conformality distortion.
In this paper, we propose an efficient iterative algorithm, called the
Quasi-conformal (QC) iterations, to compute the Teichm\&quot;uller mapping. The
basic idea is to represent the set of diffeomorphisms using Beltrami
coefficients (BCs), and look for an optimal BC associated to the desired
Teichm\&quot;uller mapping. The associated diffeomorphism can be efficiently
reconstructed from the optimal BC using the Linear Beltrami Solver(LBS). Using
BCs to represent diffeomorphisms guarantees the diffeomorphic property of the
registration. Using our proposed method, the Teichm\&quot;uller mapping can be
accurately and efficiently computed within 10 seconds. The obtained
registration is guaranteed to be bijective. The proposed algorithm can also be
extended to compute Teichm\&quot;uller mapping with soft landmark constraints. We
applied the proposed algorithm to real applications, such as brain landmark
matching registration, constrained texture mapping and human face registration.
Experimental results shows that our method is both effective and efficient in
computing a non-overlap landmark matching registration with least amount of
conformality distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2571</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2571</id><created>2012-11-12</created><updated>2013-01-21</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author><author><keyname>de Nooy</keyname><forenames>Wouter</forenames></author></authors><title>Field-normalized Impact Factors: A Comparison of Rescaling versus
  Fractionally Counted IFs</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (2013, in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two methods for comparing impact factors and citation rates across fields of
science are tested against each other using citations to the 3,705 journals in
the Science Citation Index 2010 (CD-Rom version of SCI) and the 13 field
categories used for the Science and Engineering Indicators of the US National
Science Board. We compare (i) normalization by counting citations in proportion
to the length of the reference list (1/N of references) with (ii) rescaling by
dividing citation scores by the arithmetic mean of the citation rate of the
cluster. Rescaling is analytical and therefore independent of the quality of
the attribution to the sets, whereas fractional counting provides an empirical
strategy for normalization among sets (by evaluating the between-group
variance). By the fairness test of Radicchi &amp; Castellano (2012a), rescaling
outperforms fractional counting of citations for reasons that we consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2573</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2573</id><created>2012-11-12</created><updated>2013-08-13</updated><authors><author><keyname>Ivanova</keyname><forenames>Inga A.</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Rotational Symmetry and the Transformation of Innovation Systems in a
  Triple Helix of University-Industry-Government Relations</title><categories>cs.CY</categories><comments>Technological Forecasting and Social Change (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a mathematical model, we show that a Triple Helix (TH) system contains
self-interaction, and therefore self-organization of innovations can be
expected in waves, whereas a Double Helix (DH) remains determined by its linear
constituents. (The mathematical model is fully elaborated in the Appendices.)
The ensuing innovation systems can be expected to have a fractal structure:
innovation systems at different scales can be considered as spanned in a
Cartesian space with the dimensions of (S)cience, (B)usiness, and (G)overnment.
A national system, for example, contains sectorial and regional systems, and is
a constituent part in technological and supra-national systems of innovation.
The mathematical modeling enables us to clarify the mechanisms, and provides
new possibilities for the prediction. Emerging technologies can be expected to
be more diversified and their life cycles will become shorter than before. In
terms of policy implications, the model suggests a shift from the production of
material objects to the production of innovative technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2575</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2575</id><created>2012-11-12</created><authors><author><keyname>Limam</keyname><forenames>Hela</forenames></author><author><keyname>Akaichi</keyname><forenames>Jalel</forenames></author></authors><title>A semantic cache for enhancing Web services communities activities:
  Health care case study</title><categories>cs.DL</categories><comments>6 pages,3 figures; (IJACSA) International Journal of Advanced
  Computer Science and Applications, Vol. 3, No. 10, 2012Vol. 3, No. 10, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective memories are strong support for enhancing the activities of
capitalization, management and dissemination inside a Web services community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2609</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2609</id><created>2012-11-12</created><updated>2013-05-27</updated><authors><author><keyname>Bartoletti</keyname><forenames>Massimo</forenames></author><author><keyname>Scalas</keyname><forenames>Alceste</forenames></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames></author><author><keyname>Zunino</keyname><forenames>Roberto</forenames></author></authors><title>Honesty by Typing</title><categories>cs.PL</categories><acm-class>D.2.4; D.3.1; D.3.2; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a type system for a calculus of contracting processes. Processes
may stipulate contracts, and then either behave honestly, by keeping the
promises made, or not. Type safety guarantees that a typeable process is honest
- that is, the process abides by the contract it has stipulated in all possible
contexts, even those containing dishonest adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2620</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2620</id><created>2012-11-12</created><updated>2013-12-04</updated><authors><author><keyname>Burnay</keyname><forenames>Corentin</forenames></author><author><keyname>Jureta</keyname><forenames>Ivan</forenames></author><author><keyname>Faulkner</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Context-Driven Elicitation of Default Requirements: an Empirical
  Validation</title><categories>cs.SE</categories><comments>Currently under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Requirements Engineering, requirements elicitation aims the acquisition of
information from the stakeholders of a system-to-be. An important task during
elicitation is to identify and render explicit the stakeholders' implicit
assumptions about the system-to-be and its environment. Purpose of doing so is
to identify omissions in, and conflicts between requirements. This paper o?ers
a conceptual framework for the identi?cation and documentation of default
requirements that stakeholders may be using. The framework is relevant for
practice, as it forms a check-list for types of questions to use during
elicitation. An empirical validation is described, and guidelines for
elicitation are drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2627</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2627</id><created>2012-11-12</created><updated>2013-07-14</updated><authors><author><keyname>Erd&#xe9;lyi</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author><author><keyname>Pfandler</keyname><forenames>Andreas</forenames></author></authors><title>Computational Aspects of Nearly Single-Peaked Electorates</title><categories>cs.CC</categories><comments>31 pages; A short version of this paper appeared in the proceedings
  of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2013).
  An earlier version of this paper appeared in the proceedings of the Fourth
  International Workshop on Computational Social Choice 2012 (COMSOC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manipulation, bribery, and control are well-studied ways of changing the
outcome of an election. Many voting systems are, in the general case,
computationally resistant to some of these manipulative actions. However when
restricted to single-peaked electorates, these systems suddenly become easy to
manipulate. Recently, Faliszewski, Hemaspaandra, and Hemaspaandra studied the
complexity of dishonest behavior in nearly single-peaked electorates. These are
electorates that are not single-peaked but close to it according to some
distance measure.
  In this paper we introduce several new distance measures regarding
single-peakedness. We prove that determining whether a given profile is nearly
single-peaked is NP-complete in many cases. For one case we present a
polynomial-time algorithm. Furthermore, we explore the relations between
several notions of nearly single-peakedness and study the complexity of
manipulation for nearly single-peaked veto elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2632</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2632</id><created>2012-11-12</created><authors><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Jahan</keyname><forenames>Ishrat</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andy</forenames></author></authors><title>Sequential Voronoi diagram calculations using simple chemical reactions</title><categories>nlin.PS cs.CG physics.chem-ph</categories><comments>22 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent paper [de Lacy Costello et al. 2010] we described the formation
of complex tessellations of the plane arising from the various reactions of
metal salts with potassium ferricyanide and ferrocyanide loaded gels. In
addition to producing colourful tessellations these reactions are naturally
computing generalised Voronoi diagrams of the plane. The reactions reported
previously were capable of the calculation of three distinct Voronoi diagrams
of the plane. As diffusion coupled with a chemical reaction is responsible for
the calculation then this is achieved in parallel. Thus an increase in the
complexity of the data input does not utilise additional computational
resource. Additional benefits of these chemical reactions is that a permanent
record of the Voronoi diagram calculation (in the form of precipitate free
bisectors) is achieved, so there is no requirement for further processing to
extract the calculation results. Previously it was assumed that the permanence
of the results was also a potential drawback which limited reusability. This
paper presents new data which shows that sequential Voronoi diagram
calculations can be performed on the same chemical substrate. This is dependent
on the reactivity of the original reagent and the cross reactivity of the
secondary reagent with the primary product. We present the results from a
number of binary combinations of metal salts on both potassium ferricyanide and
potassium ferrocyanide substrates. We observe three distinct mechanisms whereby
secondary sequential Voronoi diagrams can be calculated. In most cases the
result was two interpenetrating permanent Voronoi diagrams. This is interesting
from the perspective of mapping the capability of unconventional computing
substrates. But also in the study of natural pattern formation per se.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2636</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2636</id><created>2012-11-12</created><authors><author><keyname>Kulekci</keyname><forenames>M. Oguzhan</forenames></author></authors><title>A memory versus compression ratio trade-off in PPM via compressed
  context modeling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its introduction prediction by partial matching (PPM) has always been a
de facto gold standard in lossless text compression, where many variants
improving the compression ratio and speed have been proposed. However, reducing
the high space requirement of PPM schemes did not gain that much attention.
This study focuses on reducing the memory consumption of PPM via the recently
proposed compressed context modeling that uses the compressed representations
of contexts in the statistical model. Differently from the classical context
definition as the string of the preceding characters at a particular position,
CCM considers context as the amount of preceding information that is actually
the bit stream composed by compressing the previous symbols. We observe that by
using the CCM, the data structures, particularly the context trees, can be
implemented in smaller space, and present a trade-off between the compression
ratio and the space requirement. The experiments conducted showed that this
trade-off is especially beneficial in low orders with approximately 20 - 25
percent gain in memory by a sacrifice of up to nearly 7 percent loss in
compression ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2647</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2647</id><created>2012-11-12</created><updated>2013-03-22</updated><authors><author><keyname>Jahandideh</keyname><forenames>Hossein</forenames></author><author><keyname>Asef-Vaziri</keyname><forenames>Ardavan</forenames></author><author><keyname>Modarres</keyname><forenames>Mohammad</forenames></author></authors><title>Determining a Loop Material Flow Pattern for Automatic Guided Vehicle
  Systems on a Facility Layout</title><categories>cs.SY</categories><comments>Accepted to the 10th IEEE International Conference on Control and
  Automation 2013 This paper has been withdrawn to be fixed, updated, and
  combined with other papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a heuristic procedure for designing a loop material
flow pattern on a given facility layout with the aim of minimizing the total
material handling distances. We present an approximation of the total material
handling costs and greatly drop the required computational time by minimizing
the approximation instead of the original objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2651</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2651</id><created>2012-11-08</created><updated>2013-04-11</updated><authors><author><keyname>Lacasa</keyname><forenames>Lucas</forenames></author><author><keyname>Gomez-Garde&#xf1;es</keyname><forenames>Jesus</forenames></author></authors><title>Correlation dimension of complex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>New version with a supplementary material attached, accepted for
  publication in Physical Review Letters</comments><doi>10.1103/PhysRevLett.110.168703</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new measure to characterize the dimension of complex networks
based on the ergodic theory of dynamical systems. This measure is derived from
the correlation sum of a trajectory generated by a random walker navigating the
network, and extends the classical Grassberger-Procaccia algorithm to the
context of complex networks. The method is validated with reliable results for
both synthetic networks and real-world networks such as the world
air-transportation network or urban networks, and provides a computationally
fast way for estimating the dimensionality of networks which only relies on the
local information provided by the walkers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2662</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2662</id><created>2012-11-12</created><updated>2016-02-01</updated><authors><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author></authors><title>Recognizing Interval Bigraphs by Forbidden Patterns</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let H be a connected bipartite graph with n nodes and m edges. We give an
O(nm) time algorithm to decide whether H is an interval bigraph. The best known
algorithm has time complexity O(nm^6(m + n) \log n) and it was developed in
1997 [18]. Our approach is based on an ordering characterization of interval
bigraphs introduced by Hell and Huang [13]. We transform the problem of finding
the desired ordering to choosing strong components of a pair-digraph without
creating conflicts. We make use of the structure of the pair-digraph as well as
decomposition of bigraph H based on the special components of the pair-digraph.
This way we make explicit what the difficult cases are and gain efficiency by
isolating such situations. We believe our method can be used to find a desired
ordering for other classes of graphs and digraphs having ordering
characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2664</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2664</id><created>2012-11-12</created><updated>2015-01-16</updated><authors><author><keyname>Canonne</keyname><forenames>Clement</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Testing probability distributions using conditional samples</title><categories>cs.DS cs.CC math.PR math.ST stat.TH</categories><comments>Significant changes on Section 9 (detailing and expanding the proof
  of Theorem 16). Several clarifications and typos fixed in various places</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new framework for property testing of probability distributions,
by considering distribution testing algorithms that have access to a
conditional sampling oracle.* This is an oracle that takes as input a subset $S
\subseteq [N]$ of the domain $[N]$ of the unknown probability distribution $D$
and returns a draw from the conditional probability distribution $D$ restricted
to $S$. This new model allows considerable flexibility in the design of
distribution testing algorithms; in particular, testing algorithms in this
model can be adaptive.
  We study a wide range of natural distribution testing problems in this new
framework and some of its variants, giving both upper and lower bounds on query
complexity. These problems include testing whether $D$ is the uniform
distribution $\mathcal{U}$; testing whether $D = D^\ast$ for an explicitly
provided $D^\ast$; testing whether two unknown distributions $D_1$ and $D_2$
are equivalent; and estimating the variation distance between $D$ and the
uniform distribution. At a high level our main finding is that the new
&quot;conditional sampling&quot; framework we consider is a powerful one: while all the
problems mentioned above have $\Omega(\sqrt{N})$ sample complexity in the
standard model (and in some cases the complexity must be almost linear in $N$),
we give $\mathrm{poly}(\log N, 1/\varepsilon)$-query algorithms (and in some
cases $\mathrm{poly}(1/\varepsilon)$-query algorithms independent of $N$) for
all these problems in our conditional sampling setting.
  *Independently from our work, Chakraborty et al. also considered this
framework. We discuss their work in Subsection [1.4].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2670</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2670</id><created>2012-11-12</created><authors><author><keyname>Anagnostopoulos</keyname><forenames>Aris</forenames></author><author><keyname>Grandoni</keyname><forenames>Fabrizio</forenames></author><author><keyname>Leonardi</keyname><forenames>Stefano</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>A Mazing 2+eps Approximation for Unsplittable Flow on a Path</title><categories>cs.DS</categories><comments>23 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the unsplittable flow on a path problem (UFP) where we are given a
path with non-negative edge capacities and tasks, which are characterized by a
subpath, a demand, and a profit. The goal is to find the most profitable subset
of tasks whose total demand does not violate the edge capacities. This problem
naturally arises in many settings such as bandwidth allocation, resource
constrained scheduling, and interval packing.
  A natural task classification defines the size of a task i to be the ratio
delta between the demand of i and the minimum capacity of any edge used by i.
If all tasks have sufficiently small delta, the problem is already well
understood and there is a 1+eps approximation. For the complementary
setting---instances whose tasks all have large delta---much remains unknown,
and the best known polynomial-time procedure gives only (for any constant
delta&gt;0) an approximation ratio of 6+eps.
  In this paper we present a polynomial time 1+eps approximation for the latter
setting. Key to this result is a complex geometrically inspired dynamic
program. Here each task is represented as a segment underneath the capacity
curve, and we identify a proper maze-like structure so that each passage of the
maze is crossed by only O(1) tasks in the computed solution. In combination
with the known PTAS for delta-small tasks, our result implies a 2+eps
approximation for UFP, improving on the previous best 7+eps approximation
[Bonsma et al., FOCS 2011]. We remark that our improved approximation factor
matches the best known approximation ratio for the considerably easier special
case of uniform edge capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2687</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2687</id><created>2012-11-12</created><authors><author><keyname>Gupta</keyname><forenames>Varun</forenames></author><author><keyname>Radovanovic</keyname><forenames>Ana</forenames></author></authors><title>Online Stochastic Bin Packing</title><categories>cs.DS math.PR</categories><acm-class>F.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of packing Virtual Machines on physical servers in
the cloud, we study the problem of one-dimensional online stochastic bin
packing. Items with sizes i.i.d. from an unknown distribution with integral
support arrive as a stream and must be packed on arrival in bins of size B,
also an integer. The size of an item is known when it arrives and the goal is
to minimize the waste, defined to be the total unused space in non-empty bins.
While there are many heuristics for online stochastic bin packing, all such
heuristics are either optimal for only certain classes of item size
distributions, or rely on learning the distribution. The state-of-the-art Sum
of Squares heuristic (Csirik et al.) obtains sublinear (in number of items
seen) waste for distributions where the expected waste for the optimal offline
algorithm is sublinear, but has a constant factor larger waste for
distributions with linear waste under OPT. Csirik et al. solved this problem by
learning the distribution and solving an LP to inject phantom jobs in the
arrival stream.
  We present two distribution-agnostic bin packing heuristics that achieve
additive O(sqrt{n}) waste compared to OPT for all distributions. Our algorithms
are gradient descent on suitably defined Lagrangian relaxations of the bin
packing Linear Program. The first algorithm is very similar to the SS
algorithm, but conceptually packs the bins top-down instead of bottom-up. This
motivates our second heuristic that uses a different Lagrangian relaxation to
pack bins bottom-up.
  Next, we consider the more general problem of online stochastic bin packing
with item departures where the time requirement of an item is only revealed
when the item departs. Our algorithms extend as is to the case of item
departures. We also briefly revisit the Best Fit heuristic which has not been
studied in the scenario of item departures yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2696</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2696</id><created>2012-11-12</created><updated>2014-11-04</updated><authors><author><keyname>Ferraioli</keyname><forenames>Diodato</forenames></author><author><keyname>Ventre</keyname><forenames>Carmine</forenames></author></authors><title>Metastability of Asymptotically Well-Behaved Potential Games</title><categories>cs.GT cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main criticisms to game theory concerns the assumption of full
rationality. Logit dynamics is a decentralized algorithm in which a level of
irrationality (a.k.a. &quot;noise&quot;) is introduced in players' behavior. In this
context, the solution concept of interest becomes the logit equilibrium, as
opposed to Nash equilibria. Logit equilibria are distributions over strategy
profiles that possess several nice properties, including existence and
uniqueness. However, there are games in which their computation may take time
exponential in the number of players. We therefore look at an approximate
version of logit equilibria, called metastable distributions, introduced by
Auletta et al. [SODA 2012]. These are distributions that remain stable (i.e.,
players do not go too far from it) for a super-polynomial number of steps
(rather than forever, as for logit equilibria). The hope is that these
distributions exist and can be reached quickly by logit dynamics.
  We identify a class of potential games, called asymptotically well-behaved,
for which the behavior of the logit dynamics is not chaotic as the number of
players increases so to guarantee meaningful asymptotic results. We prove that
any such game admits distributions which are metastable no matter the level of
noise present in the system, and the starting profile of the dynamics. These
distributions can be quickly reached if the rationality level is not too big
when compared to the inverse of the maximum difference in potential. Our proofs
build on results which may be of independent interest, including some spectral
characterizations of the transition matrix defined by logit dynamics for
generic games and the relationship of several convergence measures for Markov
chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2699</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2699</id><created>2012-11-12</created><authors><author><keyname>Shahid</keyname><forenames>Abdur</forenames></author><author><keyname>Badsha</keyname><forenames>Shahriar</forenames></author><author><keyname>Kabeer</keyname><forenames>Md. Rethwan</forenames></author><author><keyname>Ahsan</keyname><forenames>Junaid</forenames></author><author><keyname>Mahmud</keyname><forenames>Mufti</forenames></author></authors><title>A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete
  Wavelet Transform Domain using Two Subbands</title><categories>cs.MM cs.CV</categories><comments>9 pages, 7 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 1, September 2012, page 101-109</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Digital watermarking is the process to hide digital pattern directly into a
digital content. Digital watermarking techniques are used to address digital
rights management, protect information and conceal secrets. An invisible
non-blind watermarking approach for gray scale images is proposed in this
paper. The host image is decomposed into 3-levels using Discrete Wavelet
Transform. Based on the parent-child relationship between the wavelet
coefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression
algorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the
significant coefficients. The most significant coefficients of LH2 and HL2
bands are selected to embed a binary watermark image. The selected significant
coefficients are modulated using Noise Visibility Function, which is considered
as the best strength to ensure better imperceptibility. The approach is tested
against various image processing attacks such as addition of noise, filtering,
cropping, JPEG compression, histogram equalization and contrast adjustment. The
experimental results reveal the high effectiveness of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2708</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2708</id><created>2012-11-12</created><authors><author><keyname>Bouaroudj</keyname><forenames>Kenza</forenames></author><author><keyname>Kitouni</keyname><forenames>Ilham</forenames></author><author><keyname>Hachichi</keyname><forenames>Hiba</forenames></author><author><keyname>Saidouni</keyname><forenames>Djamel-Eddine</forenames></author></authors><title>Extending Refusal Testing by Stochastic Refusals for Testing
  Non-deterministic Systems</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing is a validation activity used to check the system's correctness with
respect to the specification. In this context,test based on refusals is studied
in theory and tools are effectively constructed. This paper addresses,a formal
testing based on stochastic refusals graphs (SRG) in order to test stochastic
system represented by maximality-based labeled stochastic transition systems
(MLSTS). First, we propose a framework to generate SRGs from MLSTSs. Second,we
present a new technique to generate automatically a canonical tester from
stochastic refusal graph and conformance relation confSRG. Finally,
implementation is proposed and the application of our approach is shown by an
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2713</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2713</id><created>2012-11-12</created><updated>2013-04-04</updated><authors><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Iterative Row Sampling</title><categories>cs.DS</categories><comments>26 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant interest and progress recently in algorithms that
solve regression problems involving tall and thin matrices in input sparsity
time. These algorithms find shorter equivalent of a n*d matrix where n &gt;&gt; d,
which allows one to solve a poly(d) sized problem instead. In practice, the
best performances are often obtained by invoking these routines in an iterative
fashion. We show these iterative methods can be adapted to give theoretical
guarantees comparable and better than the current state of the art.
  Our approaches are based on computing the importances of the rows, known as
leverage scores, in an iterative manner. We show that alternating between
computing a short matrix estimate and finding more accurate approximate
leverage scores leads to a series of geometrically smaller instances. This
gives an algorithm that runs in $O(nnz(A) + d^{\omega + \theta} \epsilon^{-2})$
time for any $\theta &gt; 0$, where the $d^{\omega + \theta}$ term is comparable
to the cost of solving a regression problem on the small approximation. Our
results are built upon the close connection between randomized matrix
algorithms, iterative methods, and graph sparsification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2717</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2717</id><created>2012-11-12</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Proximal Stochastic Dual Coordinate Ascent</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a proximal version of dual coordinate ascent method. We
demonstrate how the derived algorithmic framework can be used for numerous
regularized loss minimization problems, including $\ell_1$ regularization and
structured output SVM. The convergence rates we obtain match, and sometimes
improve, state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2719</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2719</id><created>2012-11-12</created><updated>2012-11-13</updated><authors><author><keyname>B&#xe1;tfai</keyname><forenames>N.</forenames></author></authors><title>Quantum Consciousness Soccer Simulator</title><categories>cs.AI cs.MA</categories><comments>9 pages, grammatically improved</comments><msc-class>91B69</msc-class><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive sciences it is not uncommon to use various games effectively.
For example, in artificial intelligence, the RoboCup initiative was to set up
to catalyse research on the field of autonomous agent technology. In this
paper, we introduce a similar soccer simulation initiative to try to
investigate a model of human consciousness and a notion of reality in the form
of a cognitive problem. In addition, for example, the home pitch advantage and
the objective role of the supporters could be naturally described and discussed
in terms of this new soccer simulation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2723</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2723</id><created>2012-11-12</created><authors><author><keyname>Yazdi</keyname><forenames>S. M. Hossein Tabatabaei</forenames></author><author><keyname>Savari</keyname><forenames>Serap A.</forenames></author></authors><title>On the Relationships among Optimal Symmetric Fix-Free Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetric fix-free codes are prefix condition codes in which each codeword is
required to be a palindrome. Their study is motivated by the topic of joint
source-channel coding. Although they have been considered by a few communities
they are not well understood. In earlier work we used a collection of instances
of Boolean satisfiability problems as a tool in the generation of all optimal
binary symmetric fix-free codes with n codewords and observed that the number
of different optimal codelength sequences grows slowly compared with the
corresponding number for prefix condition codes. We demonstrate that all
optimal symmetric fix-free codes can alternatively be obtained by sequences of
codes generated by simple manipulations starting from one particular code. We
also discuss simplifications in the process of searching for this set of codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2734</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2734</id><created>2012-11-12</created><authors><author><keyname>Babu</keyname><forenames>Jasine</forenames></author><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Fixed-Orientation Equilateral Triangle Matching of Point Sets</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a point set $P$ and a class $\mathcal{C}$ of geometric objects,
$G_\mathcal{C}(P)$ is a geometric graph with vertex set $P$ such that any two
vertices $p$ and $q$ are adjacent if and only if there is some $C \in
\mathcal{C}$ containing both $p$ and $q$ but no other points from $P$. We study
$G_{\bigtriangledown}(P)$ graphs where $\bigtriangledown$ is the class of
downward equilateral triangles (ie. equilateral triangles with one of their
sides parallel to the x-axis and the corner opposite to this side below that
side). For point sets in general position, these graphs have been shown to be
equivalent to half-$\Theta_6$ graphs and TD-Delaunay graphs.
  The main result in our paper is that for point sets $P$ in general position,
$G_{\bigtriangledown}(P)$ always contains a matching of size at least
$\lceil\frac{n-2}{3}\rceil$ and this bound cannot be improved above
$\lceil\frac{n-1}{3}\rceil$.
  We also give some structural properties of $G_{\davidsstar}(P)$ graphs, where
$\davidsstar$ is the class which contains both upward and downward equilateral
triangles. We show that for point sets in general position, the block cut point
graph of $G_{\davidsstar}(P)$ is simply a path. Through the equivalence of
$G_{\davidsstar}(P)$ graphs with $\Theta_6$ graphs, we also derive that any
$\Theta_6$ graph can have at most $5n-11$ edges, for point sets in general
position.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2736</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2736</id><created>2012-11-12</created><authors><author><keyname>N.</keyname><forenames>Rajeswari P. V.</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>Hybrid Systems for Knowledge Representation in Artificial Intelligence</title><categories>cs.AI</categories><comments>6 pages</comments><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence, 1 (8), 2012, 31-36</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are few knowledge representation (KR) techniques available for
efficiently representing knowledge. However, with the increase in complexity,
better methods are needed. Some researchers came up with hybrid mechanisms by
combining two or more methods. In an effort to construct an intelligent
computer system, a primary consideration is to represent large amounts of
knowledge in a way that allows effective use and efficiently organizing
information to facilitate making the recommended inferences. There are merits
and demerits of combinations, and standardized method of KR is needed. In this
paper, various hybrid schemes of KR were explored at length and details
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2737</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2737</id><created>2012-11-12</created><authors><author><keyname>Rao</keyname><forenames>T. Kameswara</forenames></author><author><keyname>Lakshmi</keyname><forenames>M. Rajya</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>An Exploration on Brain Computer Interface and Its Recent Trends</title><categories>cs.HC cs.ET cs.SY</categories><comments>6 pages</comments><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence, 1(8), 2012, 17-22</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detailed exploration on Brain Computer Interface (BCI) and its recent trends
has been done in this paper. Work is being done to identify objects, images,
videos and their color compositions. Efforts are on the way in understanding
speech, words, emotions, feelings and moods. When humans watch the surrounding
environment, visual data is processed by the brain, and it is possible to
reconstruct the same on the screen with some appreciable accuracy by analyzing
the physiological data. This data is acquired by using one of the non-invasive
techniques like electroencephalography (EEG) in BCI. The acquired signal is to
be translated to produce the image on to the screen. This paper also lays
suitable directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2741</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2741</id><created>2012-11-12</created><authors><author><keyname>Sharma</keyname><forenames>Kamlesh</forenames></author><author><keyname>Prasad</keyname><forenames>S. V. A. V.</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>A Hindi Speech Actuated Computer Interface for Web Search</title><categories>cs.CL cs.HC cs.IR</categories><comments>7 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications 3(10), 2012, 147-152</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aiming at increasing system simplicity and flexibility, an audio evoked based
system was developed by integrating simplified headphone and user-friendly
software design. This paper describes a Hindi Speech Actuated Computer
Interface for Web search (HSACIWS), which accepts spoken queries in Hindi
language and provides the search result on the screen. This system recognizes
spoken queries by large vocabulary continuous speech recognition (LVCSR),
retrieves relevant document by text retrieval, and provides the search result
on the Web by the integration of the Web and the voice systems. The LVCSR in
this system showed enough performance levels for speech with acoustic and
language models derived from a query corpus with target contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2742</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2742</id><created>2012-11-12</created><authors><author><keyname>Vashisht</keyname><forenames>Vasudha</forenames></author><author><keyname>Choudhury</keyname><forenames>Tanupriya</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>Sketch Recognition using Domain Classification</title><categories>cs.CV cs.HC</categories><comments>9 pages; International Journal of Advanced Computer Science and
  Applications, Special Issue on Image Processing and Analysis 1, 2011, 1-9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceptualizing away the sketch processing details in a user interface will
enable general users and domain experts to create more complex sketches. There
are many domains for which sketch recognition systems are being developed. But
they entail image-processing skill if they are to handle the details of each
domain, and also they are lengthy to build. The implemented system goal is to
enable user interface designers and domain experts who may not have proficiency
in sketch recognition to be able to construct these sketch systems. This sketch
recognition system takes in rough sketches from user drawn with the help of
mouse as its input. It then recognizes the sketch using segmentation and domain
classification, the properties of the user drawn sketch and segments are
searched heuristically in the domains and each figures of each domain, and
finally it shows its domain, the figure name and properties. It also draws the
sketch smoothly. The work is resulted through extensive research and study of
many existing image processing and pattern matching algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2743</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2743</id><created>2012-11-12</created><authors><author><keyname>Rameshwari</keyname><forenames>Rashmi</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>Systematic and Integrative Analysis of Proteomic Data using
  Bioinformatics Tools</title><categories>cs.CE</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications, 2(5), 2011, 29-35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis and interpretation of relationships between biological molecules
is done with the help of networks. Networks are used ubiquitously throughout
biology to represent the relationships between genes and gene products. Network
models have facilitated a shift from the study of evolutionary conservation
between individual gene and gene products towards the study of conservation at
the level of pathways and complexes. Recent work has revealed much about
chemical reactions inside hundreds of organisms as well as universal
characteristics of metabolic networks, which shed light on the evolution of the
networks. However, characteristics of individual metabolites have been
neglected in this network. The current paper provides an overview of
bioinformatics software used in visualization of biological networks using
proteomic data, their main functions and limitations of the software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2751</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2751</id><created>2012-11-12</created><authors><author><keyname>Prabhu</keyname><forenames>Monisha</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Random Sequences from Primitive Pythagorean Triples</title><categories>cs.CR</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the six classes of PPTs can be put into two groups.
Autocorrelation and cross-correlation functions of the six classes derived from
the gaps between each class type have been computed. It is shown that Classes A
and D (in which the largest term is divisible by 5) are different from the
other four classes in their randomness properties if they are ordered by the
largest term. In the other two orderings each of the six random Baudhayana
sequences has excellent randomness properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2756</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2756</id><created>2012-11-12</created><authors><author><keyname>Nikolenko</keyname><forenames>Sergey I.</forenames></author><author><keyname>Korobeynikov</keyname><forenames>Anton I.</forenames></author><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author></authors><title>BayesHammer: Bayesian clustering for error correction in single-cell
  sequencing</title><categories>q-bio.QM cs.CE cs.DS q-bio.GN</categories><journal-ref>BMC Genomics 14(Suppl 1) (2013), pp. S7</journal-ref><doi>10.1186/1471-2164-14-S1-S7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error correction of sequenced reads remains a difficult task, especially in
single-cell sequencing projects with extremely non-uniform coverage. While
existing error correction tools designed for standard (multi-cell) sequencing
data usually come up short in single-cell sequencing projects, algorithms
actually used for single-cell error correction have been so far very
simplistic.
  We introduce several novel algorithms based on Hamming graphs and Bayesian
subclustering in our new error correction tool BayesHammer. While BayesHammer
was designed for single-cell sequencing, we demonstrate that it also improves
on existing error correction tools for multi-cell sequencing data while working
much faster on real-life datasets. We benchmark BayesHammer on both $k$-mer
counts and actual assembly results with the SPAdes genome assembler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2776</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2776</id><created>2012-11-12</created><authors><author><keyname>Delaval</keyname><forenames>Gwena&#xeb;l</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Girault</keyname><forenames>Alain</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Pouzet</keyname><forenames>Marc</forenames><affiliation>LRI</affiliation></author></authors><title>A Type System for the Automatic Distribution of Higher-order Synchronous
  Dataflow Programs</title><categories>cs.PL</categories><proxy>ccsd</proxy><journal-ref>LCTES - ACM International Conference on Languages, Compilers, and
  Tools for Embedded Systems (2008) 101-110</journal-ref><doi>10.1145/1375657.1375672</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the design of distributed systems with synchronous dataflow
programming languages. As modular design entails handling both architectural
and functional modularity, our first contribution is to extend an existing
synchronous dataflow programming language with primitives allowing the
description of a distributed architecture and the localization of some
expressions onto some processors. We also present a distributed semantics to
formalize the distributed execution of synchronous programs. Our second
contribution is to provide a type system, in order to infer the localization of
non-annotated values by means of type inference and to ensure, at compilation
time, the consistency of the distribution. Our third contribution is to provide
a type-directed projection operation to obtain automatically,from a centralized
typed program, the local program to be executed by each computing resource. The
type system as well as the automatic distribution mechanism has been fully
implemented in the compiler of an existing synchronous data-flow programming
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2799</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2799</id><created>2012-11-11</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Nguyen</keyname><forenames>Ann</forenames></author><author><keyname>Nguyen</keyname><forenames>Jeremy</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author></authors><title>Factors influencing E-commerce Adoption by Retailers in Saudi Arabia</title><categories>cs.CY</categories><comments>International Conference on Internet Studies. arXiv admin note: text
  overlap with arXiv:1211.2407, arXiv:1211.2396, arXiv:1211.2398</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents some findings from a study researching the diffusion and
adoption of online retailing in Saudi Arabia. Although the country has the
largest and fastest growing ICT marketplace in the Arab region, e-commerce
activities have not progressed at a similar speed. In general, Saudi retailers
have not responded actively to the global growth of online retailing.
Accordingly new research has been conducted to identify and explore key issues
that positively and negatively influence Saudi retailers in deciding whether to
adopt the online channel. While the overall research project uses mixed
methods, the focus of this paper is on a quantitative analysis of responses
obtained from a survey of retailers in Saudi Arabia, with the design of the
questionnaire instrument being based on the findings of a qualitative analysis
reported in a previous paper. The main findings of the current analysis include
a list of key factors that affect retailers decision to adopt e-commerce, and
quantitative indications of the relative strengths of the various
relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2838</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2838</id><created>2012-11-12</created><updated>2012-12-06</updated><authors><author><keyname>Sasaki</keyname><forenames>Tatsuya</forenames></author><author><keyname>Uchida</keyname><forenames>Satoshi</forenames></author></authors><title>The evolution of cooperation by social exclusion</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.PE</categories><comments>28 pages, 3 figures, supplementary material (materials and methods,
  and 6 supplementary figures)</comments><journal-ref>Proc. R. Soc. B. 2013 280 20122498</journal-ref><doi>10.1098/rspb.2012.2498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exclusion of freeriders from common privileges or public acceptance is
widely found in the real world. Current models on the evolution of cooperation
with incentives mostly assume peer sanctioning, whereby a punisher imposes
penalties on freeriders at a cost to itself. It is well known that such costly
punishment has two substantial difficulties. First, a rare punishing cooperator
barely subverts the asocial society of freeriders, and second, natural
selection often eliminates punishing cooperators in the presence of
non-punishing cooperators (namely, &quot;second-order&quot; freeriders). We present a
game-theoretical model of social exclusion in which a punishing cooperator can
exclude freeriders from benefit sharing. We show that such social exclusion can
overcome the above-mentioned difficulties even if it is costly and stochastic.
The results do not require a genetic relationship, repeated interaction,
reputation, or group selection. Instead, only a limited number of freeriders
are required to prevent the second-order freeriders from eroding the social
immune system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2853</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2853</id><created>2012-11-12</created><updated>2014-08-01</updated><authors><author><keyname>Nguyen</keyname><forenames>Philon</forenames></author></authors><title>Coding 35GB of Data in 35 Pages of Numbers</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error
  that would require a new paper to resolve</comments><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usual information theoretical results show a logarithmic coding factor of
value spaces to digital binary spaces using p-adic numbering systems. The
following paper discusses a less commonly used case. It applies the same
results to the difference space of bijective mappings of n-dimensional spaces
to the line. It discusses a method where the logarithmic coding factor is
provided over the Hamming radius of the code. An example is provided using the
35GB data dump of the Wikipedia website. This technique was initially developed
for the study and computation of large permutation matrices on small clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2854</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2854</id><created>2012-11-12</created><authors><author><keyname>Mhimdi</keyname><forenames>Wahiba Ben Abdessalem Karaa Nouha</forenames></author></authors><title>Using ontology for resume annotation</title><categories>cs.IR</categories><comments>9 pages</comments><journal-ref>International Journal of Metadata, Semantics and Ontologies
  (IJMSO), 2011 Vol. 6 No. 3/4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Employers collect a large number of resumes from job portals, or from the
company's own website. These documents are used for an automated selection of
candidates satisfying the requirements and therefore reducing recruitment
costs. Various approaches for process documents have already been developed for
recruitment. In this paper we present an approach based on semantic annotation
of resumes for e-recruitment process. The most important task consists on
modelling the semantic content of these documents using ontology. The ontology
is built taking into account the most significant components of resumes
inspired from the structure of EUROPASS CV. This ontology is thereafter used to
annotate automatically the resumes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2858</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2858</id><created>2012-11-12</created><authors><author><keyname>Fry</keyname><forenames>Zachary P.</forenames></author><author><keyname>Weimer</keyname><forenames>Westley</forenames></author></authors><title>Fault Localization Using Textual Similarities</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maintenance is a dominant component of software cost, and localizing reported
defects is a significant component of maintenance. We propose a scalable
approach that leverages the natural language present in both defect reports and
source code to identify files that are potentially related to the defect in
question. Our technique is language-independent and does not require test
cases. The approach represents reports and code as separate structured
documents and ranks source files based on a document similarity metric that
leverages inter-document relationships.
  We evaluate the fault-localization accuracy of our method against both
lightweight baseline techniques and also reported results from state-of-the-art
tools. In an empirical evaluation of 5345 historical defects from programs
totaling 6.5 million lines of code, our approach reduced the number of files
inspected per defect by over 91%. Additionally, we qualitatively and
quantitatively examine the utility of the textual and surface features used by
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2863</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2863</id><created>2012-11-12</created><authors><author><keyname>Schclar</keyname><forenames>Alon</forenames></author></authors><title>Multi-Sensor Fusion via Reduction of Dimensionality</title><categories>cs.CV</categories><comments>PhD Thesis, Tel Aviv Univ, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large high-dimensional datasets are becoming more and more popular in an
increasing number of research areas. Processing the high dimensional data
incurs a high computational cost and is inherently inefficient since many of
the values that describe a data object are redundant due to noise and inner
correlations. Consequently, the dimensionality, i.e. the number of values that
are used to describe a data object, needs to be reduced prior to any other
processing of the data. The dimensionality reduction removes, in most cases,
noise from the data and reduces substantially the computational cost of
algorithms that are applied to the data.
  In this thesis, a novel coherent integrated methodology is introduced
(theory, algorithm and applications) to reduce the dimensionality of
high-dimensional datasets. The method constructs a diffusion process among the
data coordinates via a random walk. The dimensionality reduction is obtained
based on the eigen-decomposition of the Markov matrix that is associated with
the random walk. The proposed method is utilized for: (a) segmentation and
detection of anomalies in hyper-spectral images; (b) segmentation of
multi-contrast MRI images; and (c) segmentation of video sequences.
  We also present algorithms for: (a) the characterization of materials using
their spectral signatures to enable their identification; (b) detection of
vehicles according to their acoustic signatures; and (c) classification of
vascular vessels recordings to detect hyper-tension and cardio-vascular
diseases.
  The proposed methodology and algorithms produce excellent results that
successfully compete with current state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2874</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2874</id><created>2012-11-12</created><updated>2013-09-23</updated><authors><author><keyname>Yan</keyname><forenames>Xiao-Yong</forenames></author><author><keyname>Han</keyname><forenames>Xiao-Pu</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Diversity of individual mobility patterns and emergence of aggregated
  scaling laws</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>4 figures, 5 pages</comments><journal-ref>Scientific Reports 3 (2013) 2678</journal-ref><doi>10.1038/srep02678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncovering human mobility patterns is of fundamental importance to the
understanding of epidemic spreading, urban transportation and other
socioeconomic dynamics embodying spatiality and human travel. According to the
direct travel diaries of volunteers, we show the absence of scaling properties
in the displacement distribution at the individual level,while the aggregated
displacement distribution follows a power law with an exponential cutoff. Given
the constraint on total travelling cost, this aggregated scaling law can be
analytically predicted by the mixture nature of human travel under the
principle of maximum entropy. A direct corollary of such theory is that the
displacement distribution of a single mode of transportation should follow an
exponential law, which also gets supportive evidences in known data. We thus
conclude that the travelling cost shapes the displacement distribution at the
aggregated level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2875</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2875</id><created>2012-11-12</created><authors><author><keyname>Ibrahim</keyname><forenames>Maged hamada</forenames></author></authors><title>A Novel Approach to Fully Private and Secure Auction: A Sealed Bid
  Knapsack Auction</title><categories>cs.CR cs.CY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an electronic auction protocol, the main participants are the seller, a
set of trusted auctioneer(s) and the set of bidders. In this paper we consider
the situation where there is a seller and a set of n bidders intending to come
to an agreement on the selling price of a certain good. Full private or
bidder-resolved auction means that this agreement is reached without the help
of trusted parties or auctioneers. Therefore, only the seller and the set of
bidders are involved, the role of the auctioneers becomes obsolete in this
case. property.We propose a new technique for the design of a full private
sealed-bid auction protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2881</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2881</id><created>2012-11-12</created><updated>2012-11-28</updated><authors><author><keyname>Chung</keyname><forenames>Junyoung</forenames></author><author><keyname>Lee</keyname><forenames>Donghoon</forenames></author><author><keyname>Seo</keyname><forenames>Youngjoo</forenames></author><author><keyname>Yoo</keyname><forenames>Chang D.</forenames></author></authors><title>Deep Attribute Networks</title><categories>cs.CV cs.LG stat.ML</categories><comments>This paper has been withdrawn by the author due to a crucial
  grammatical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining compact and discriminative features is one of the major challenges
in many of the real-world image classification tasks such as face verification
and object recognition. One possible approach is to represent input image on
the basis of high-level features that carry semantic meaning which humans can
understand. In this paper, a model coined deep attribute network (DAN) is
proposed to address this issue. For an input image, the model outputs the
attributes of the input image without performing any classification. The
efficacy of the proposed model is evaluated on unconstrained face verification
and real-world object recognition tasks using the LFW and the a-PASCAL
datasets. We demonstrate the potential of deep learning for attribute-based
classification by showing comparable results with existing state-of-the-art
results. Once properly trained, the DAN is fast and does away with calculating
low-level features which are maybe unreliable and computationally expensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2891</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2891</id><created>2012-11-13</created><authors><author><keyname>Bar</keyname><forenames>Ariel</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author><author><keyname>Shani</keyname><forenames>Guy</forenames></author><author><keyname>Shapira</keyname><forenames>Bracha</forenames></author><author><keyname>Schclar</keyname><forenames>Alon</forenames></author></authors><title>Boosting Simple Collaborative Filtering Models Using Ensemble Methods</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the effect of applying ensemble learning to the
performance of collaborative filtering methods. We present several systematic
approaches for generating an ensemble of collaborative filtering models based
on a single collaborative filtering algorithm (single-model or homogeneous
ensemble). We present an adaptation of several popular ensemble techniques in
machine learning for the collaborative filtering domain, including bagging,
boosting, fusion and randomness injection. We evaluate the proposed approach on
several types of collaborative filtering base models: k- NN, matrix
factorization and a neighborhood matrix factorization model. Empirical
evaluation shows a prediction improvement compared to all base CF algorithms.
In particular, we show that the performance of an ensemble of simple (weak) CF
models such as k-NN is competitive compared with a single strong CF model (such
as matrix factorization) while requiring an order of magnitude less
computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2897</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2897</id><created>2012-11-13</created><updated>2013-10-09</updated><authors><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Annapureddy</keyname><forenames>V. Sreekanth</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Interference Channels with Coordinated Multi-Point Transmission: Degrees
  of Freedom, Message Assignment, and Fractional Reuse</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated Multi-Point (CoMP) transmission is an infrastructural enhancement
under consideration for next generation wireless networks. In this work, the
capacity gain achieved through CoMP transmission is studied in various models
of wireless networks that have practical significance. The capacity gain is
analyzed through the degrees of freedom (DoF) criterion. The DoF available for
communication provides an analytically tractable way to characterize the
capacity of interference channels. The considered channel model has K
transmitter/receiver pairs, and each receiver is interested in one unique
message from a set of K independent messages. Each message can be available at
more than one transmitter. The maximum number of transmitters at which each
message can be available, is defined as the cooperation order M. For fully
connected interference channels, it is shown that the asymptotic per user DoF,
as K goes to infinity, remains at 1/2 as M is increased from 1 to 2.
Furthermore, the same negative result is shown to hold for all M &gt; 1 for any
message assignment that satisfies a local cooperation constraint. On the other
hand, when the assumption of full connectivity is relaxed to local
connectivity, and each transmitter is connected only to its own receiver as
well as L neighboring receivers, it is shown that local cooperation is optimal.
The asymptotic per user DoF is shown to be at least max {1/2,2M/(2M+L)} for
locally connected channels, and is shown to be 2M/(2M+1) for the special case
of Wyner's asymmetric model where L=1. An interesting feature of the proposed
achievability scheme is that it relies on simple zero-forcing transmit beams
and does not require symbol extensions. Also, to achieve the optimal per user
DoF for Wyner's model, messages are assigned to transmitters in an asymmetric
fashion unlike traditional assignments where message i has to be available at
transmitter i.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2926</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2926</id><created>2012-11-13</created><authors><author><keyname>Kulekci</keyname><forenames>M. Oguzhan</forenames></author></authors><title>Enumeration of sequences with large alphabets</title><categories>cs.DS cs.DM cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study focuses on efficient schemes for enumerative coding of
$\sigma$--ary sequences by mainly borrowing ideas from \&quot;Oktem &amp; Astola's
\cite{Oktem99} hierarchical enumerative coding and Schalkwijk's
\cite{Schalkwijk72} asymptotically optimal combinatorial code on binary
sequences. By observing that the number of distinct $\sigma$--dimensional
vectors having an inner sum of $n$, where the values in each dimension are in
range $[0...n]$ is $K(\sigma,n) = \sum_{i=0}^{\sigma-1} {{n-1} \choose
{\sigma-1-i}} {{\sigma} \choose {i}}$, we propose representing $C$ vector via
enumeration, and present necessary algorithms to perform this task. We prove
$\log K(\sigma,n)$ requires approximately $ (\sigma -1) \log (\sigma-1) $ less
bits than the naive $(\sigma-1)\lceil \log (n+1) \rceil$ representation for
relatively large $n$, and examine the results for varying alphabet sizes
experimentally. We extend the basic scheme for the enumerative coding of
$\sigma$--ary sequences by introducing a new method for large alphabets. We
experimentally show that the newly introduced technique is superior to the
basic scheme by providing experiments on DNA sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2945</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2945</id><created>2012-11-13</created><authors><author><keyname>Wan</keyname><forenames>Cen</forenames></author><author><keyname>Biktasheva</keyname><forenames>Irina V.</forenames></author><author><keyname>Lane</keyname><forenames>Steven</forenames></author></authors><title>The application of a perceptron model to classify an individual's
  response to a proposed loading dose regimen of Warfarin</title><categories>stat.AP cs.NE</categories><comments>12 pages, 5 figures, 1 table</comments><msc-class>68T05, 92C50</msc-class><acm-class>I.2.1; I.5.1; I.5.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dose regimen of Warfarin is separated into two phases. Firstly a loading
dose is given, which is designed to bring the International Normalisation Ratio
(INR) to within therapeutic range. Then a stable maintenance dose is given to
maintain the INR within therapeutic range. In the United Kingdom (UK) the
loading dose is usually given as three individual daily doses, the standard
loading dose being 10mg on days one and two and 5mgs on day three, which can be
varied at the discretion of the clinician. However, due to the large
inter-individual variation in the response to Warfarin therapy, it is difficult
to identify which patients will reach the narrow therapeutic window for target
INR, and which will be above or below the therapeutic window. The aim of this
research was to develop a methodology using a neural networks classification
algorithm and data mining techniques to predict for a given loading dose and
patient characteristics if the patient is more likely to achieve target INR or
more likely to be above or below therapeutic range.
  Multilayer perceptron (MLP) and 10-fold stratified cross validation
algorithms were used to determine an artificial neural network to classify
patients' response to their initial Warfarin loading dose. The resulting neural
network model correctly classifies an individual's response to their Warfarin
loading dose over 80% of the time. As well as taking into account the initial
loading dose, the final model also includes demographic, genetic and a number
of other potential confounding factors. With this model clinicians can
predetermine whether a given loading regimen, along with specific patient
characteristics will achieve a therapeutic response for a particular patient.
Thus tailoring the loading dose regimen to meet the individual needs of the
patient and reducing the risk of adverse drug reactions associated with
Warfarin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2946</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2946</id><created>2012-11-13</created><updated>2013-09-17</updated><authors><author><keyname>Bahaa-ElDin</keyname><forenames>Ayman M.</forenames></author><author><keyname>Halim</keyname><forenames>Islam Tharwat A.</forenames></author><author><keyname>Fahmy</keyname><forenames>Hossam M. A.</forenames></author></authors><title>ATDSR: Trusted On-Demand Routing Protocol based on Agents for Mobile
  Ad-hoc Networks</title><categories>cs.NI cs.CR</categories><comments>Submited by error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The routing performance in Mobile Ad-hoc Networks (MANETs) relies on the
co-operation of the individual nodes that constitute the network. The existence
of misbehaving nodes may paralyze the routing operation in MANETs. To overcome
this behavior, the trustworthiness of the network nodes should be considered in
the route selection process combined with the hop count. The trustworthiness is
achieved by measuring the trust value for each node in the network. In this
paper, a new protocol based on self monitoring (agent-based) and following the
dynamic source routing (DSR) algorithm is presented. This protocol is called
Agent-Based Trusted Dynamic Source Routing (ATDSR) Protocol for MANETs. The
objective of this protocol is to manage trust information locally with minimal
overhead in terms of extra messages and time delay. This objective is achieved
through installing in each participated node in the network a multi-agent
system (MAS). MAS consists of two types of agents: monitoring agent (MOA) and
routing agent (ROA). A new mathematical and more realistic objective model for
measuring the trust value is introduced. This model is weighted by both number
and size of routed packets to reflect the selective forwarding behavior of a
node. The performance evaluation via simulation shows that our protocol is
better than standard and trusted DSR. The simulation is done over a variety of
environmental conditions such as number of malicious nodes, host density and
movement rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2960</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2960</id><created>2012-11-13</created><authors><author><keyname>Allouch</keyname><forenames>Hamid</forenames></author><author><keyname>Chana</keyname><forenames>Idriss</forenames></author><author><keyname>Belkasmi</keyname><forenames>Mostafa</forenames></author></authors><title>Iterative decoding of Generalized Parallel Concatenated Block codes
  using cyclic permutations</title><categories>cs.IT cs.DS math.IT</categories><comments>8 pages, 14 figures, IJCSI journal link:
  http://www.ijcsi.org/articles/Iterative-decoding-of-generalized-parallel-concatenated-block-codes-using-cyclic-permutations.php</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 1, September 2012 (ISSN (Online): 1694-0814)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Iterative decoding techniques have gain popularity due to their performance
and their application in most communications systems. In this paper, we present
a new application of our iterative decoder on the GPCB (Generalized Parallel
Concatenated Block codes) which uses cyclic permutations. We introduce a new
variant of the component decoder. After extensive simulation; the obtained
result is very promising compared with several existing methods. We evaluate
the effects of various parameters component codes, interleaver size, block
size, and the number of iterations. Three interesting results are obtained; the
first one is that the performances in terms of BER (Bit Error Rate) of the new
constituent decoder are relatively similar to that of original one. Secondly
our turbo decoding outperforms another turbo decoder for some linear block
codes. Thirdly the proposed iterative decoding of GPCB-BCH (75, 51) is about
2.1dB from its Shannon limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2963</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2963</id><created>2012-11-13</created><updated>2013-01-28</updated><authors><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Borgdorff</keyname><forenames>Joris</forenames></author><author><keyname>Bona-Casas</keyname><forenames>Carles</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Nash</keyname><forenames>Rupert W.</forenames></author><author><keyname>Zasada</keyname><forenames>Stefan J.</forenames></author><author><keyname>Saverchenko</keyname><forenames>Ilya</forenames></author><author><keyname>Mamonski</keyname><forenames>Mariusz</forenames></author><author><keyname>Kurowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Bernabeu</keyname><forenames>Miguel O.</forenames></author><author><keyname>Hoekstra</keyname><forenames>Alfons G.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Flexible composition and execution of high performance, high fidelity
  multiscale biomedical simulations</title><categories>cs.DC cs.CE</categories><comments>accepted by Interface Focus. 17 pages, 2 figures, 4 tables</comments><journal-ref>Interface Focus April 6, 2013 3 2 20120087</journal-ref><doi>10.1098/rsfs.2012.0087</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiscale simulations are essential in the biomedical domain to accurately
model human physiology. We present a modular approach for designing,
constructing and executing multiscale simulations on a wide range of resources,
from desktops to petascale supercomputers, including combinations of these. Our
work features two multiscale applications, in-stent restenosis and
cerebrovascular bloodflow, which combine multiple existing single-scale
applications to create a multiscale simulation. These applications can be
efficiently coupled, deployed and executed on computers up to the largest
(peta) scale, incurring a coupling overhead of 1 to 10% of the total execution
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2972</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2972</id><created>2012-11-13</created><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Segregating event streams and noise with a Markov renewal process model</title><categories>cs.AI</categories><acm-class>I.5.1</acm-class><journal-ref>Journal of Machine Learning Research, 14(Aug):2213-2238, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe an inference task in which a set of timestamped event
observations must be clustered into an unknown number of temporal sequences
with independent and varying rates of observations. Various existing approaches
to multi-object tracking assume a fixed number of sources and/or a fixed
observation rate; we develop an approach to inferring structure in timestamped
data produced by a mixture of an unknown and varying number of similar Markov
renewal processes, plus independent clutter noise. The inference simultaneously
distinguishes signal from noise as well as clustering signal observations into
separate source streams. We illustrate the technique via a synthetic experiment
as well as an experiment to track a mixture of singing birds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2980</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2980</id><created>2012-11-13</created><authors><author><keyname>Moran</keyname><forenames>Shay</forenames></author></authors><title>Shattering-Extremal Systems</title><categories>math.CO cs.CG cs.DM cs.LG</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shatters relation and the VC dimension have been investigated since the
early seventies. These concepts have found numerous applications in statistics,
combinatorics, learning theory and computational geometry. Shattering extremal
systems are set-systems with a very rich structure and many different
characterizations. The goal of this thesis is to elaborate on the structure of
these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2985</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2985</id><created>2012-11-13</created><authors><author><keyname>Berbakov</keyname><forenames>Lazar</forenames></author><author><keyname>Ant&#xf3;n-Haro</keyname><forenames>Carles</forenames></author><author><keyname>Matamoros</keyname><forenames>Javier</forenames></author></authors><title>Optimal Transmission Policy for Cooperative Transmission with Energy
  Harvesting and Battery Operated Sensor Nodes</title><categories>cs.IT math.IT</categories><comments>Submitted to Elsevier Signal Processing Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a scenario where one energy harvesting and one
battery operated sensor cooperatively transmit a common message to a distant
base station. The goal is to find the jointly optimal transmission (power
allocation) policy which maximizes the total throughput for a given deadline.
First, we address the case in which the storage capacity of the energy
harvesting sensor is infinite. In this context, we identify the necessary
conditions for such optimal transmission policy. On their basis, we first show
that the problem is convex. Then we go one step beyond and prove that (i) the
optimal power allocation for the energy harvesting sensor can be computed
independently; and (ii) it unequivocally determines (and allows to compute)
that of the battery operated one. Finally, we generalize the analysis for the
case of finite storage capacity. Performance is assessed by means of computer
simulations. Particular attention is paid to the impact of finite storage
capacity and long-term battery degradation on the achievable throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2986</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2986</id><created>2012-11-13</created><updated>2012-11-30</updated><authors><author><keyname>Tasharrofi</keyname><forenames>Shahab</forenames></author></authors><title>A Strongly Grounded Stable Model Semantics for Full Propositional
  Language</title><categories>cs.LO</categories><comments>This paper has been withdrawn due to a mistake in the intended proof
  for Theorem 10 and, thus Corollary 4, does not work. Other theorems,
  propositions and corollaries are true and remain unchanged. Theorem 10 might
  still be true but the intended proof of using an irrelevant propositional
  atom does not work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer set programming is one of the most praised frameworks for declarative
programming in general and non-monotonic reasoning in particular. There has
been many efforts to extend stable model semantics so that answer set programs
can use a more extensive syntax. To such endeavor, the community of
non-monotonic reasoning has introduced extensions such as equilibrium models
and FLP semantics. However, both of these extensions suffer from two problems:
intended models according to such extensions (1) are not guaranteed to be
minimal, and (2) more importantly, may have self-justifications (i.e., the
justification for pertinence of an atom in an intended model may be its own
pertinence). Both of these properties directly violate the spirit of stable
model semantics. Therefore, we believe that we need a new extension of stable
model semantics that guarantees both minimality and being strongly grounded.
  This paper introduces one such extension using two different approaches:
firstly, by extending the goal-reachability interpretation of logic programs to
the full propositional language and, secondly, using derivability in
intuitionistic propositional logic. We show that both these approaches give the
same semantics, that we call the supported semantics. Moreover, using the first
approach, we also extend well-founded semantics to full propositional language.
Furthermore, we discuss how our supported models relate to other existing
semantics for non-monotonic reasoning including the equilibrium models. Last,
but not the least, we discuss the complexity of reasoning about supported
models and show that all interesting reasoning tasks (such as brave/cautious
reasoning) are PSPACE-complete. Therefore, supported model semantics is a much
more expressive semantics then the existing semantics such as equlibrium models
(that have reasoning procedures in $\Delta^P_3$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3000</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3000</id><created>2012-11-13</created><authors><author><keyname>Gerbner</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author><author><keyname>Wiener</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Search for the end of a path in the d-dimensional grid and in other
  graphs</title><categories>math.CO cs.DM</categories><msc-class>90B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following search problem. Given a graph $G$ with a vertex $s$
there is an unknown path starting from $s$. In one query we ask a vertex $v$
and the answer is the set of edges of the path incident to $v$. We want to
determine what is the minimal number of queries needed to find the other
endvertex of the path. We consider different variants of this problem and
examine their relations. We prove a strong connection between this problem and
the theory of graph separators. Finally, we consider the case when the graph
$G$ is a grid graph, in which case using the connection with separators, we
give asymptotically tight bounds (as a function of the size of the grid $n$, in
case the dimension of the grid $d$ is considered to be fixed) for variants of
the above search problem. For this we prove a separator theorem about grid
graphs. This search problem is a discrete variant of a problem of Hirsch,
Papadimitriou and Vavasis about finding Brouwer fixed points, in particular in
the discrete setting our results improve their results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3006</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3006</id><created>2012-11-13</created><updated>2013-01-23</updated><authors><author><keyname>Prabh</keyname><forenames>K. Shashi</forenames></author></authors><title>Near-Optimal Distributed Scheduling Algorithms for Regular Wireless
  Sensor Networks</title><categories>cs.NI cs.DC</categories><comments>Version 2: Added new evaluations and revised text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are normally characterized by resource challenged
nodes. Since communication costs the most in terms of energy in these networks,
minimizing this overhead is important. We consider minimum length node
scheduling in regular multi-hop wireless sensor networks. We present
collision-free decentralized scheduling algorithms based on TDMA with spatial
reuse that do not use message passing, this saving communication overhead. We
develop the algorithms using graph-based k-hop interference model and show that
the schedule complexity in regular networks is independent of the number of
nodes and varies quadratically with k which is typically a very small number.
We follow it by characterizing feasibility regions in the SINR parameter space
where the constant complexity continues to hold while simultaneously satisfying
the SINR criteria. Using simulation, we evaluate the efficiency of our solution
on random network deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3010</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3010</id><created>2012-11-13</created><authors><author><keyname>Veeramachaneni</keyname><forenames>Sriharsha</forenames></author></authors><title>Time-series Scenario Forecasting</title><categories>stat.ML cs.LG stat.AP</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require the ability to judge uncertainty of time-series
forecasts. Uncertainty is often specified as point-wise error bars around a
mean or median forecast. Due to temporal dependencies, such a method obscures
some information. We would ideally have a way to query the posterior
probability of the entire time-series given the predictive variables, or at a
minimum, be able to draw samples from this distribution. We use a Bayesian
dictionary learning algorithm to statistically generate an ensemble of
forecasts. We show that the algorithm performs as well as a physics-based
ensemble method for temperature forecasts for Houston. We conclude that the
method shows promise for scenario forecasting where physics-based methods are
absent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3016</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3016</id><created>2012-11-13</created><authors><author><keyname>Franconi</keyname><forenames>Enrico</forenames></author><author><keyname>Guagliardo</keyname><forenames>Paolo</forenames></author></authors><title>The View Update Problem Revisited</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the view update problem in a relational setting and
propose a framework based on the notion of determinacy under constraints.
Within such a framework, we characterise when a view mapping is invertible,
establishing that this is the case precisely when each database symbol has an
exact rewriting in terms of the view symbols under the given constraints, and
we provide a general effective criterion to understand whether the changes
introduced by a view update can be propagated to the underlying database
relations in a unique and unambiguous way.
  Afterwards, we show how determinacy under constraints can be checked, and
rewritings effectively found, in three different relevant scenarios in the
absence of view constraints. First, we settle the long-standing open issue of
how to solve the view update problem in a multi-relational database with views
that are projections of joins of relations, and we do so in a more general
setting where views are defined by arbitrary conjunctive queries and database
constraints are stratified embedded dependencies. Next, we study a setting
based on horizontal decompositions of a single database relation, where views
are defined by selections on possibly interpreted attributes (e.g., arithmetic
comparisons) in the presence of domain constraints over the database schema.
Lastly, we look into another multi-relational database setting, where views are
defined in an expressive &quot;Type&quot; Relational Algebra based on the n-ary
Description Logic DLR and database constraints are inclusions of expressions in
that algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3020</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3020</id><created>2012-11-13</created><authors><author><keyname>Fischer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Hekler</keyname><forenames>Achim</forenames></author><author><keyname>Dolgov</keyname><forenames>Maxim</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Optimal Sequence-Based LQG Control over TCP-like Networks Subject to
  Random Transmission Delays and Packet Losses</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of sequence-based controller design for
Networked Control Systems (NCS), where control inputs and measurements are
transmitted over TCP-like network connections that are subject to stochastic
packet losses and time-varying packet delays. At every time step, the
controller sends a sequence of predicted control inputs to the actuator in
addition to the current control input. In this sequence-based setup, we derive
an optimal solution to the Linear Quadratic Gaussian (LQG) control problem and
prove that the separation principle holds. Simulations demonstrate the improved
performance of this optimal controller compared to other sequence-based
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3043</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3043</id><created>2012-11-13</created><updated>2014-06-19</updated><authors><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author></authors><title>General Truthfulness Characterizations Via Convex Analysis</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model of truthful elicitation which generalizes and extends
mechanisms, scoring rules, and a number of related settings that do not quite
qualify as one or the other. Our main result is a characterization theorem,
yielding characterizations for all of these settings, including a new
characterization of scoring rules for non-convex sets of distributions. We
generalize this model to eliciting some property of the agent's private
information, and provide the first general characterization for this setting.
We also show how this yields a new proof of a result in mechanism design due to
Saks and Yu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3046</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3046</id><created>2012-11-13</created><updated>2014-02-21</updated><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>Recovering the Optimal Solution by Dual Random Projection</title><categories>cs.LG</categories><comments>The 26th Annual Conference on Learning Theory (COLT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random projection has been widely used in data classification. It maps
high-dimensional data into a low-dimensional subspace in order to reduce the
computational cost in solving the related optimization problem. While previous
studies are focused on analyzing the classification performance of using random
projection, in this work, we consider the recovery problem, i.e., how to
accurately recover the optimal solution to the original optimization problem in
the high-dimensional space based on the solution learned from the subspace
spanned by random projections. We present a simple algorithm, termed Dual
Random Projection, that uses the dual solution of the low-dimensional
optimization problem to recover the optimal solution to the original problem.
Our theoretical analysis shows that with a high probability, the proposed
algorithm is able to accurately recover the optimal solution to the original
problem, provided that the data matrix is of low rank or can be well
approximated by a low rank matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3049</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3049</id><created>2012-11-13</created><updated>2013-07-11</updated><authors><author><keyname>Bucci</keyname><forenames>Michelangelo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>Reversible Christoffel factorizations</title><categories>cs.DM cs.FL math.CO</categories><comments>12 pages, submitted. Previous draft presented at RuFiDiM 2011</comments><msc-class>68R15</msc-class><journal-ref>Theoretical Computer Science 495 (2013) 17-24</journal-ref><doi>10.1016/j.tcs.2013.05.042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a family of natural decompositions of Sturmian words in Christoffel
words, called *reversible Christoffel* (RC) factorizations. They arise from the
observation that two Sturmian words with the same language have (almost always)
arbitrarily long Abelian equivalent prefixes. Using the three gap theorem, we
prove that in each RC factorization, only 2 or 3 distinct Christoffel words may
occur. We begin the study of such factorizations, considered as infinite words
over 2 or 3 letters, and show that in the general case they are either Sturmian
words, or obtained by a three-interval exchange transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3056</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3056</id><created>2012-11-13</created><updated>2013-06-05</updated><authors><author><keyname>Fortin</keyname><forenames>Pierre</forenames><affiliation>LIP6</affiliation></author><author><keyname>Gouicem</keyname><forenames>Mourad</forenames><affiliation>LIP6</affiliation></author><author><keyname>Graillat</keyname><forenames>Stef</forenames><affiliation>LIP6</affiliation></author></authors><title>GPU-accelerated generation of correctly-rounded elementary functions</title><categories>cs.MS cs.DC cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 754-2008 standard recommends the correct rounding of some elementary
functions. This requires to solve the Table Maker's Dilemma which implies a
huge amount of CPU computation time. We consider in this paper accelerating
such computations, namely Lefe'vre algorithm on Graphics Processing Units
(GPUs) which are massively parallel architectures with a partial SIMD execution
(Single Instruction Multiple Data). We first propose an analysis of the
Lef\`evre hard-to-round argument search using the concept of continued
fractions. We then propose a new parallel search algorithm much more efficient
on GPU thanks to its more regular control flow. We also present an efficient
hybrid CPU-GPU deployment of the generation of the polynomial approximations
required in Lef\`evre algorithm. In the end, we manage to obtain overall
speedups up to 53.4x on one GPU over a sequential CPU execution, and up to 7.1x
over a multi-core CPU, which enable a much faster solving of the Table Maker's
Dilemma for the double precision format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3063</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3063</id><created>2012-11-13</created><authors><author><keyname>Carlone</keyname><forenames>Luca</forenames></author><author><keyname>Censi</keyname><forenames>Andrea</forenames></author></authors><title>From Angular Manifolds to the Integer Lattice: Guaranteed Orientation
  Estimation with Application to Pose Graph Optimization</title><categories>cs.RO math.OC</categories><comments>24 pages, 5 figures, 5 tables</comments><msc-class>68T40</msc-class><acm-class>I.2.9; G.1.6; G.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the orientations of nodes in a pose graph from relative angular
measurements is challenging because the variables live on a manifold product
with nontrivial topology and the maximum-likelihood objective function is
non-convex and has multiple local minima; these issues prevent iterative
solvers to be robust for large amounts of noise. This paper presents an
approach that allows working around the problem of multiple minima, and is
based on the insight that the original estimation problem on orientations is
equivalent to an unconstrained quadratic optimization problem on integer
vectors. This equivalence provides a viable way to compute the maximum
likelihood estimate and allows guaranteeing that such estimate is almost surely
unique. A deeper consequence of the derivation is that the maximum likelihood
solution does not necessarily lead to an estimate that is &quot;close&quot; to the actual
nodes orientations, hence it is not necessarily the best choice for the problem
at hand. To alleviate this issue, our algorithm computes a set of estimates,
for which we can derive precise probabilistic guarantees. Experiments show that
the method is able to tolerate extreme amounts of noise (e.g., {\sigma} =
30{\deg} on each measurement) that are above all noise levels of sensors
commonly used in mapping. For most range-finder-based scenarios, the
multi-hypothesis estimator returns only a single hypothesis, because the
problem is very well constrained. Finally, using the orientations estimate
provided by our method to bootstrap the initial guess of pose graph
optimization methods improves their robustness and makes them avoid local
minima even for high levels of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3085</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3085</id><created>2012-11-13</created><authors><author><keyname>Jonoska</keyname><forenames>Natasha</forenames></author><author><keyname>Karpenko</keyname><forenames>Daria</forenames></author></authors><title>Active Tile Self-assembly, Self-similar Structures and Recursion</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an active tile assembly model which extends Winfree's abstract
tile assembly model to tiles that are capable of transmitting and receiving
binding site activation signals. In addition, we introduce a mathematical
framework to address self-similarity and recursion within the model. The model
is applied to show a recursive assembly of an archetypal self-similar aperiodic
tiling known as the L-shape tiling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3089</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3089</id><created>2012-11-13</created><updated>2012-12-21</updated><authors><author><keyname>Hu</keyname><forenames>Yuheng</forenames></author><author><keyname>John</keyname><forenames>Ajita</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>ET-LDA: Joint Topic Modeling for Aligning Events and their Twitter
  Feedback</title><categories>cs.SI cs.AI cs.CY</categories><comments>reference error, delete for now</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During broadcast events such as the Superbowl, the U.S. Presidential and
Primary debates, etc., Twitter has become the de facto platform for crowds to
share perspectives and commentaries about them. Given an event and an
associated large-scale collection of tweets, there are two fundamental research
problems that have been receiving increasing attention in recent years. One is
to extract the topics covered by the event and the tweets; the other is to
segment the event. So far these problems have been viewed separately and
studied in isolation. In this work, we argue that these problems are in fact
inter-dependent and should be addressed together. We develop a joint Bayesian
model that performs topic modeling and event segmentation in one unified
framework. We evaluate the proposed model both quantitatively and qualitatively
on two large-scale tweet datasets associated with two events from different
domains to show that it improves significantly over baseline models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3093</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3093</id><created>2012-11-13</created><updated>2014-05-28</updated><authors><author><keyname>Cadek</keyname><forenames>Martin</forenames></author><author><keyname>Krcal</keyname><forenames>Marek</forenames></author><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author><author><keyname>Vokrinek</keyname><forenames>Lukas</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Polynomial-time computation of homotopy groups and Postnikov systems in
  fixed dimension</title><categories>cs.CG math.AT</categories><comments>53 pages. SIAM J. Comput., in press</comments><msc-class>68U05, 68W99, 55S45, 55S37</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several computational problems in homotopy theory, we obtain algorithms
with running time polynomial in the input size. In particular, for every fixed
k&gt;1, there is a polynomial-time algorithm that, for a 1-connected topological
space X given as a finite simplicial complex, or more generally, as a
simplicial set with polynomial-time homology, computes the k-th homotopy group
\pi_k(X), as well as the first k stages of a Postnikov system of X. Combined
with results of an earlier paper, this yields a polynomial-time computation of
[X,Y], i.e., all homotopy classes of continuous mappings X -&gt; Y, under the
assumption that Y is (k-1)-connected and dim X &lt; 2k-1. We also obtain a
polynomial-time solution of the extension problem, where the input consists of
finite simplicial complexes X,Y, where Y is (k-1)-connected and dim X &lt; 2k,
plus a subspace A\subseteq X and a (simplicial) map f:A -&gt; Y, and the question
is the extendability of f to all of X.
  The algorithms are based on the notion of a simplicial set with
polynomial-time homology, which is an enhancement of the notion of a simplicial
set with effective homology developed earlier by Sergeraert and his co-workers.
Our polynomial-time algorithms are obtained by showing that simplicial sets
with polynomial-time homology are closed under various operations, most
notably, Cartesian products, twisted Cartesian products, and classifying space.
One of the key components is also polynomial-time homology for the
Eilenberg--MacLane space K(Z,1), provided in another recent paper by Krcal,
Matousek, and Sergeraert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3128</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3128</id><created>2012-11-13</created><authors><author><keyname>Kulkarni</keyname><forenames>Ankur A.</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Non-asymptotic Upper Bounds for Deletion Correcting Codes</title><categories>cs.IT math.CO math.IT math.NT math.OC</categories><comments>18 pages, 4 figures</comments><msc-class>68P30, 94A15, 94B99, 94B75, 68R99, 90C27, 05C65</msc-class><acm-class>E.4; G.2.1; G.2.2; F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explicit non-asymptotic upper bounds on the sizes of multiple-deletion
correcting codes are presented. In particular, the largest single-deletion
correcting code for $q$-ary alphabet and string length $n$ is shown to be of
size at most $\frac{q^n-q}{(q-1)(n-1)}$. An improved bound on the asymptotic
rate function is obtained as a corollary. Upper bounds are also derived on
sizes of codes for a constrained source that does not necessarily comprise of
all strings of a particular length, and this idea is demonstrated by
application to sets of run-length limited strings.
  The problem of finding the largest deletion correcting code is modeled as a
matching problem on a hypergraph. This problem is formulated as an integer
linear program. The upper bound is obtained by the construction of a feasible
point for the dual of the linear programming relaxation of this integer linear
program.
  The non-asymptotic bounds derived imply the known asymptotic bounds of
Levenshtein and Tenengolts and improve on known non-asymptotic bounds.
Numerical results support the conjecture that in the binary case, the
Varshamov-Tenengolts codes are the largest single-deletion correcting codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3147</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3147</id><created>2012-11-13</created><updated>2013-02-22</updated><authors><author><keyname>Powers</keyname><forenames>James</forenames></author><author><keyname>Chen</keyname><forenames>Keke</forenames></author></authors><title>Secure Computation of Top-K Eigenvectors for Shared Matrices in the
  Cloud</title><categories>cs.CR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of sensor network, mobile computing, and web
applications, data are now collected from many distributed sources to form big
datasets. Such datasets can be hosted in the cloud to achieve economical
processing. However, these data might be highly sensitive requiring secure
storage and processing. We envision a cloud-based data storage and processing
framework that enables users to economically and securely share and handle big
datasets. Under this framework, we study the matrix-based data mining
algorithms with a focus on the secure top-k eigenvector algorithm. Our approach
uses an iterative processing model in which the authorized user interacts with
the cloud to achieve the result. In this process, both the source matrix and
the intermediate results keep confidential and the client-side incurs low
costs. The security of this approach is guaranteed by using Paillier Encryption
and a random perturbation technique. We carefully analyze its security under a
cloud-specific threat model. Our experimental results show that the proposed
method is scalable to big matrices while requiring low client-side costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3148</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3148</id><created>2012-11-11</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author></authors><title>Seven Key Drivers to Online Retailing Growth in KSA</title><categories>cs.CY</categories><comments>IADIS International Conference e-Society 2011. arXiv admin note:
  substantial text overlap with arXiv:1211.2398, arXiv:1211.2404,
  arXiv:1211.2799</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Retailers in Saudi Arabia have been reserved in their adoption of
electronically delivered aspects of their business. This paper reports research
that identifies and explores key issues to enhance the diffusion of online
retailing in Saudi Arabia. Despite the fact that Saudi Arabia has the largest
and fastest growth of ICT marketplaces in the Arab region, e-commerce
activities are not progressing at the same speed. Only very few Saudi
companies, mostly medium and large companies from the manufacturing sector, are
involved in e-commerce implementation. Based on qualitative data collected by
conducting interviews with 16 retailers and 16 potential customers in Saudi
Arabia, 7 key drivers to online retailing diffusion in Saudi Arabia are
identified. These key drivers are government support, providing trustworthy and
secure online payments options, provision of individual house mailboxes,
providing high speed Internet connection at low cost, providing educational
programs, the success of bricks-and-clicks model, and competitive prices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3169</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3169</id><created>2012-11-13</created><authors><author><keyname>Amblard</keyname><forenames>Pierre-Olivier</forenames></author><author><keyname>Michel</keyname><forenames>Olivier J. J.</forenames></author></authors><title>The relation between Granger causality and directed information theory:
  a review</title><categories>cs.IT math.IT</categories><doi>10.3390/e15010113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report reviews the conceptual and theoretical links between Granger
causality and directed information theory. We begin with a short historical
tour of Granger causality, concentrating on its closeness to information
theory. The definitions of Granger causality based on prediction are recalled,
and the importance of the observation set is discussed. We present the
definitions based on conditional independence. The notion of instantaneous
coupling is included in the definitions. The concept of Granger causality
graphs is discussed. We present directed information theory from the
perspective of studies of causal influences between stochastic processes.
Causal conditioning appears to be the cornerstone for the relation between
information theory and Granger causality. In the bivariate case, the
fundamental measure is the directed information, which decomposes as the sum of
the transfer entropies and a term quantifying instantaneous coupling. We show
the decomposition of the mutual information into the sums of the transfer
entropies and the instantaneous coupling measure, a relation known for the
linear Gaussian case. We study the multivariate case, showing that the useful
decomposition is blurred by instantaneous coupling. The links are further
developed by studying how measures based on directed information theory
naturally emerge from Granger causality inference frameworks as hypothesis
testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3174</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3174</id><created>2012-11-13</created><authors><author><keyname>Dikaliotis</keyname><forenames>Theodoros K.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author></authors><title>On the Delay Advantage of Coding in Packet Erasure Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. arXiv admin
  note: substantial text overlap with arXiv:0910.3975</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the delay of network coding compared to routing with
retransmissions in packet erasure networks with probabilistic erasures. We
investigate the sub-linear term in the block delay required for unicasting $n$
packets and show that there is an unbounded gap between network coding and
routing. In particular, we show that delay benefit of network coding scales at
least as $\sqrt{n}$. Our analysis of the delay function for the routing
strategy involves a major technical challenge of computing the expectation of
the maximum of two negative binomial random variables. This problem has been
studied previously and we derive the first exact characterization which may be
of independent interest. We also use a martingale bounded differences argument
to show that the actual coding delay is tightly concentrated around its
expectation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3189</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3189</id><created>2012-11-13</created><authors><author><keyname>Feng</keyname><forenames>Tao</forenames></author></authors><title>A characterization of two-weight projective cyclic codes</title><categories>cs.IT math.IT math.NT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give necessary conditions for a two-weight projective cyclic code to be
the direct sum of two one-weight irreducible cyclic subcodes of the same
dimension, following the work of Wolfmann and Vega. This confirms Vega's
conjecture that all the two-weight cyclic codes of this type are the known ones
in the projective case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3191</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3191</id><created>2012-11-13</created><updated>2012-12-17</updated><authors><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Zhou</keyname><forenames>Wenxuan</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>SWEET: Serving the Web by Exploiting Email Tunnels</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open communication over the Internet poses a serious threat to countries with
repressive regimes, leading them to develop and deploy censorship mechanisms
within their networks. Unfortunately, existing censorship circumvention systems
do not provide high availability guarantees to their users, as censors can
identify, hence disrupt, the traffic belonging to these systems using today's
advanced censorship technologies. In this paper we propose SWEET, a highly
available censorship-resistant infrastructure. SWEET works by encapsulating a
censored user's traffic to a proxy server inside email messages that are
carried over by public email service providers, like Gmail and Yahoo Mail. As
the operation of SWEET is not bound to specific email providers we argue that a
censor will need to block all email communications in order to disrupt SWEET,
which is infeasible as email constitutes an important part of today's Internet.
Through experiments with a prototype of our system we find that SWEET's
performance is sufficient for web traffic. In particular, regular websites are
downloaded within couple of seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3193</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3193</id><created>2012-11-13</created><updated>2013-06-26</updated><authors><author><keyname>Mori</keyname><forenames>Shintaro</forenames></author><author><keyname>Hisakado</keyname><forenames>Masato</forenames></author><author><keyname>Takahashi</keyname><forenames>Taiki</forenames></author></authors><title>Collective Adoption of Max-Min Strategy in an Information Cascade Voting
  Experiment</title><categories>physics.soc-ph cs.SI</categories><comments>25 pages,9 figures</comments><journal-ref>J. Phys. Soc. Jpn. 82, 084004 (2013)</journal-ref><doi>10.7566/JPSJ.82.084004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a situation where one has to choose an option with multiplier m.
The multiplier is inversely proportional to the number of people who have
chosen the option and is proportional to the return if it is correct. If one
does not know the correct option, we call him a herder, and then there is a
zero-sum game between the herder and other people who have set the multiplier.
The max-min strategy where one divides one's choice inversely proportional to m
is optimal from the viewpoint of the maximization of expected return. We call
the optimal herder an analog herder. The system of analog herders takes the
probability of correct choice to one for any value of the ratio of herders,
p&lt;1, in the thermodynamic limit if the accuracy of the choice of informed
person q is one. We study how herders choose by a voting experiment in which 50
to 60 subjects sequentially answer a two-choice quiz. We show that the
probability of selecting a choice by the herders is inversely proportional to m
for 4/3 &lt; m &lt; 4 and they collectively adopt the max-min strategy in that range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3200</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3200</id><created>2012-11-13</created><authors><author><keyname>Allahbakhsh</keyname><forenames>Mohammad</forenames></author><author><keyname>Ignjatovic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author><author><keyname>Beheshti</keyname><forenames>Seyed-Mehdi-Reza</forenames></author><author><keyname>Foo</keyname><forenames>Norman</forenames></author><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author></authors><title>An Analytic Approach to People Evaluation in Crowdsourcing Systems</title><categories>cs.IR cs.SI</categories><comments>23 pages</comments><report-no>UNSW-CSE-TR-201204</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Worker selection is a significant and challenging issue in crowdsourcing
systems. Such selection is usually based on an assessment of the reputation of
the individual workers participating in such systems. However, assessing the
credibility and adequacy of such calculated reputation is a real challenge. In
this paper, we propose an analytic model which leverages the values of the
tasks completed, the credibility of the evaluators of the results of the tasks
and time of evaluation of the results of these tasks in order to calculate an
accurate and credible reputation rank of participating workers and fairness
rank for evaluators. The model has been implemented and experimentally
validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3201</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3201</id><created>2012-11-13</created><updated>2013-09-10</updated><authors><author><keyname>Minooei</keyname><forenames>Hadi</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Truthful Mechanism Design for Multidimensional Covering Problems</title><categories>cs.GT cs.DS</categories><acm-class>F.2.2; G.2.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate {\em multidimensional covering mechanism-design} problems,
wherein there are $m$ items that need to be covered and $n$ agents who provide
covering objects, with each agent $i$ having a private cost for the covering
objects he provides. The goal is to select a set of covering objects of minimum
total cost that together cover all the items.
  We focus on two representative covering problems: uncapacitated facility
location (\ufl) and vertex cover (\vcp). For multidimensional \ufl, we give a
black-box method to transform any {\em Lagrangian-multiplier-preserving}
$\rho$-approximation algorithm for \ufl to a truthful-in-expectation,
$\rho$-approx. mechanism. This yields the first result for multidimensional
\ufl, namely a truthful-in-expectation 2-approximation mechanism.
  For multidimensional \vcp (\mvcp), we develop a {\em decomposition method}
that reduces the mechanism-design problem into the simpler task of constructing
{\em threshold mechanisms}, which are a restricted class of truthful
mechanisms, for simpler (in terms of graph structure or problem dimension)
instances of \mvcp. By suitably designing the decomposition and the threshold
mechanisms it uses as building blocks, we obtain truthful mechanisms with the
following approximation ratios ($n$ is the number of nodes): (1) $O(r^2\log n)$
for $r$-dimensional \vcp; and (2) $O(r\log n)$ for $r$-dimensional \vcp on any
proper minor-closed family of graphs (which improves to $O(\log n)$ if no two
neighbors of a node belong to the same player). These are the first truthful
mechanisms for \mvcp with non-trivial approximation guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3211</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3211</id><created>2012-11-14</created><authors><author><keyname>Sekihara</keyname><forenames>Kensuke</forenames></author><author><keyname>Attias</keyname><forenames>Hagai</forenames></author><author><keyname>Owen</keyname><forenames>Julia P.</forenames></author><author><keyname>Nagarajan</keyname><forenames>Srikantan S.</forenames></author></authors><title>Effectiveness of sparse Bayesian algorithm for MVAR coefficient
  estimation in MEG/EEG source-space causality analysis</title><categories>stat.AP cs.NA</categories><comments>Proceedings of the 8th Annual Conference of Non-invasive Functional
  Source Imaging held at Banff, May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the effectiveness of a sparse Bayesian algorithm to
estimate multivariate autoregressive coefficients when a large amount of
background interference exists. This paper employs computer experiments to
compare two methods in the source-space causality analysis: the conventional
least-squares method and a sparse Bayesian method. Results of our computer
experiments show that the interference affects the least-squares method in a
very severe manner. It produces large false-positive results, unless the
signal-to-interference ratio is very high. On the other hand, the sparse
Bayesian method is relatively insensitive to the existence of interference.
However, this robustness of the sparse Bayesian method is attained on the
scarifies of the detectability of true causal relationship. Our experiments
also show that the surrogate data bootstrapping method tends to give a
statistical threshold that are too low for the sparse method.
  The permutation-test-based method gives a higher (more conservative)
threshold and it should be used with the sparse Bayesian method whenever the
control period is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3212</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3212</id><created>2012-11-14</created><authors><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Liu</keyname><forenames>Zhenming</forenames></author><author><keyname>Radunovic</keyname><forenames>Bozidar</forenames></author></authors><title>Distributed Non-Stochastic Experts</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the online distributed non-stochastic experts problem, where the
distributed system consists of one coordinator node that is connected to $k$
sites, and the sites are required to communicate with each other via the
coordinator. At each time-step $t$, one of the $k$ site nodes has to pick an
expert from the set ${1, ..., n}$, and the same site receives information about
payoffs of all experts for that round. The goal of the distributed system is to
minimize regret at time horizon $T$, while simultaneously keeping communication
to a minimum.
  The two extreme solutions to this problem are: (i) Full communication: This
essentially simulates the non-distributed setting to obtain the optimal
$O(\sqrt{\log(n)T})$ regret bound at the cost of $T$ communication. (ii) No
communication: Each site runs an independent copy : the regret is
$O(\sqrt{log(n)kT})$ and the communication is 0. This paper shows the
difficulty of simultaneously achieving regret asymptotically better than
$\sqrt{kT}$ and communication better than $T$. We give a novel algorithm that
for an oblivious adversary achieves a non-trivial trade-off: regret
$O(\sqrt{k^{5(1+\epsilon)/6} T})$ and communication $O(T/k^{\epsilon})$, for
any value of $\epsilon \in (0, 1/5)$. We also consider a variant of the model,
where the coordinator picks the expert. In this model, we show that the
label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us
strategy that is near optimal in regret vs communication trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3218</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3218</id><created>2012-11-14</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Pop</keyname><forenames>Petrica C.</forenames></author><author><keyname>Hajdu-Macelaru</keyname><forenames>Mara</forenames></author></authors><title>Classical hybrid approaches on a transportation problem with gas
  emissions constraints</title><categories>cs.CY</categories><comments>4 pages, 1 figure</comments><journal-ref>Advances in Intelligent Systems and Computing, 188:449-458, 2013</journal-ref><doi>10.1007/978-3-642-32922-7_46</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to keep a green planet, in particular its important to limiting the
pollution with gas emissions. In a specific capacitated fixed-charge
transportation problem with fixed capacities for distribution centers and
customers with particular demands, the objective is to keep the pollution
factor in a given range while the total cost of the transportation is as low as
possible. In order to solve this problem, we developed several hybrid variants
of the nearest neighbor classical approach. The proposed models are analyzed on
a set of instances used in the literature. The preliminary results shows that
the newly approaches are attractive and appropriate for solving the described
transportation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3229</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3229</id><created>2012-11-14</created><authors><author><keyname>Hafiddi</keyname><forenames>Hatim</forenames></author><author><keyname>Baidouri</keyname><forenames>Hicham</forenames></author><author><keyname>Nassar</keyname><forenames>Mahmoud</forenames></author><author><keyname>Kriouile</keyname><forenames>Abdelaziz</forenames></author></authors><title>Context-Awareness for Service Oriented Systems</title><categories>cs.SE</categories><comments>10 pages, 12 figures, IJCSI (International Journal of Computer
  Science Issues); IJCSI International Journal of Computer Science Issues, Vol.
  9, Issue 5, No 2, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, service oriented systems need to be enhanced to sense and react to
users context in order to provide a better user experience. To meet this
requirement, Context-Aware Services (CAS) have emerged as an underling design
and development paradigm for the development of context-aware systems. The
fundamental challenges for such systems development are context-awareness
management and service adaptation to the users context. To cope with such
requirements, we propose a well designed architecture, named ACAS, to support
the development of Context-Aware Service Oriented Systems (CASOS). This
architecture relies on a set of context-awareness and CAS specifications and
metamodels to enhance a core service, in service oriented systems, to be
context-aware. This enhancement is fulfilled by the Aspect Adaptations Weaver
(A2W) which, based on the Aspect Paradigm (AP) concepts, considers the services
adaptations as aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3233</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3233</id><created>2012-11-14</created><updated>2013-10-07</updated><authors><author><keyname>Bahroun</keyname><forenames>R.</forenames></author><author><keyname>Michel</keyname><forenames>O.</forenames></author><author><keyname>Frassati</keyname><forenames>F.</forenames></author><author><keyname>Carmona</keyname><forenames>M.</forenames></author><author><keyname>Lacoume</keyname><forenames>J. L.</forenames></author></authors><title>New algorithm for footstep localization using seismic sensors in an
  indoor environment</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we consider the use of seismic sensors for footstep
localization in indoor environments. A popular strategy of localization is to
use the measured differences in arrival times of source signals at multiple
pairs of receivers. In the literature, most algorithms that are based on time
differences of arrival (TDOA) assume that the propagation velocity is a
constant as a function of the source position, which is valid for air
propagation or even for narrow band signals. However a bounded medium such as a
concrete slab (encountered in indoor environement) is usually dispersive and
damped. In this study, we demonstrate that under such conditions, the concrete
slab can be assimilated to a thin plate; considering a Kelvin-Voigt damping
model, we introduce the notion of {\em perceived propagation velocity}, which
decreases when the source-sensor distance increases. This peculiar behaviour
precludes any possibility to rely on existing localization methods in indoor
environment. Therefore, a new localization algorithm that is adapted to a
damped and dispersive medium is proposed, using only on the sign of the
measured TDOA (SO-TDOA). A simulation and some experimental results are
included, to define the performance of this SO-TDOA algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3234</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3234</id><created>2012-11-14</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Paix&#xe3;o</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Spreer</keyname><forenames>Jonathan</forenames></author></authors><title>Computational topology and normal surfaces: Theoretical and experimental
  complexity bounds</title><categories>math.GT cs.CG math.CO</categories><comments>A 10-page extended abstract of this work will appear in ALENEX 2013;
  this is the full version of the paper including details of proofs. 23 pages,
  10 figures</comments><msc-class>68Q17 (Primary) 68Q15, 68Q15, 57Q35, 57Q35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In three-dimensional computational topology, the theory of normal surfaces is
a tool of great theoretical and practical significance. Although this theory
typically leads to exponential time algorithms, very little is known about how
these algorithms perform in &quot;typical&quot; scenarios, or how far the best known
theoretical bounds are from the real worst-case scenarios. Here we study the
combinatorial and algebraic complexity of normal surfaces from both the
theoretical and experimental viewpoints. Theoretically, we obtain new
exponential lower bounds on the worst-case complexities in a variety of
settings that are important for practical computation. Experimentally, we study
the worst-case and average-case complexities over a comprehensive body of
roughly three billion input triangulations. Many of our lower bounds are the
first known exponential lower bounds in these settings, and experimental
evidence suggests that many of our theoretical lower bounds on worst-case
growth rates may indeed be asymptotically tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3238</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3238</id><created>2012-11-14</created><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Wu</keyname><forenames>Hongrun</forenames></author><author><keyname>Du</keyname><forenames>Wenhua</forenames></author><author><keyname>Shu</keyname><forenames>Wanneng</forenames></author><author><keyname>Qin</keyname><forenames>Jun</forenames></author></authors><title>The Robustness of Scale-free Networks Under Edge Attacks with the
  Quantitative Analysis</title><categories>cs.SI nlin.AO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies on the invulnerability of scale-free networks under edge
attacks supported the conclusion that scale-free networks would be fragile
under selective attacks. However, these studies are based on qualitative
methods with obscure definitions on the robustness. This paper therefore
employs a quantitative method to analyze the invulnerability of the scale-free
networks, and uses four scale-free networks as the experimental group and four
random networks as the control group. The experimental results show that some
scale-free networks are robust under selective edge attacks, different to
previous studies. Thus, this paper analyzes the difference between the
experimental results and previous studies, and suggests reasonable
explanations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3250</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3250</id><created>2012-11-14</created><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Jaffr&#xe8;s-Runser</keyname><forenames>Katia</forenames></author><author><keyname>Goursaud</keyname><forenames>Claire</forenames></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames></author></authors><title>Deriving Pareto-optimal performance bounds for 1 and 2-relay wireless
  networks</title><categories>cs.NI cs.PF</categories><comments>Shorter version submitted to ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the problem of deriving fundamental trade-off bounds for
a 1-relay and a 2-relay wireless network when multiple performance criteria are
of interest. It proposes a simple MultiObjective (MO) performance evaluation
framework composed of a broadcast and interference-limited network model;
capacity, delay and energy performance metrics and an associated MO
optimization problem. Pareto optimal performance bounds between end-to-end
delay and energy for a capacity-achieving network are given for 1-relay and
2-relay topologies and assessed through simulations. Moreover, we also show in
this paper that these bounds are tight since they can be reached by simple
practical coding strategies performed by the source and the relays. Two
different types of network coding strategies are investigated. Practical
performance bounds for both strategies are compared to the theoretical upper
bound. Results confirm that the proposed upper bound on delay and energy
performance is tight and can be reached with the proposed combined source and
network coding strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3257</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3257</id><created>2012-11-14</created><authors><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author><author><keyname>Oriol</keyname><forenames>Manuel</forenames></author><author><keyname>Tikhomirov</keyname><forenames>Andrey</forenames></author><author><keyname>Wei</keyname><forenames>Yi</forenames></author></authors><title>The Search for the Laws of Automatic Random Testing</title><categories>cs.SE</categories><comments>20 pages</comments><journal-ref>Software Verification and Testing 2013, A Track of the ACM
  Symposium on Applied Computing (ACM-SAC 2013), March 18--22, 2013, Coimbra,
  Portugal</journal-ref><doi>10.1145/2480362.2480590</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can one estimate the number of remaining faults in a software system? A
credible estimation technique would be immensely useful to project managers as
well as customers. It would also be of theoretical interest, as a general law
of software engineering. We investigate possible answers in the context of
automated random testing, a method that is increasingly accepted as an
effective way to discover faults. Our experimental results, derived from
best-fit analysis of a variety of mathematical functions, based on a large
number of automated tests of library code equipped with automated oracles in
the form of contracts, suggest a poly-logarithmic law. Although further
confirmation remains necessary on different code bases and testing techniques,
we argue that understanding the laws of testing may bring significant benefits
for estimating the number of detectable faults and comparing different projects
and practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3293</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3293</id><created>2012-11-14</created><authors><author><keyname>Rozen</keyname><forenames>Rakefet</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames></author></authors><title>Ex-Post Equilibrium and VCG Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an abstract social choice setting with incomplete information, where
the number of alternatives is large. Albeit natural, implementing VCG
mechanisms may not be feasible due to the prohibitive communication
constraints. However, if players restrict attention to a subset of the
alternatives, feasibility may be recovered.
  This paper characterizes the class of subsets which induce an ex-post
equilibrium in the original game. It turns out that a crucial condition for
such subsets to exist is the existence of a type-independent optimal social
alternative, for each player. We further analyze the welfare implications of
these restrictions.
  This work follows work by Holzman, Kfir-Dahav, Monderer and Tennenholtz
(2004) and Holzman and Monderer (2004) where similar analysis is done for
combinatorial auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3295</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3295</id><created>2012-11-14</created><updated>2013-09-27</updated><authors><author><keyname>Colombo</keyname><forenames>Diego</forenames></author><author><keyname>Maathuis</keyname><forenames>Marloes H.</forenames></author></authors><title>Order-independent constraint-based causal structure learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider constraint-based methods for causal structure learning, such as
the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),
Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first
step of all these algorithms consists of the PC-algorithm. This algorithm is
known to be order-dependent, in the sense that the output can depend on the
order in which the variables are given. This order-dependence is a minor issue
in low-dimensional settings. We show, however, that it can be very pronounced
in high-dimensional settings, where it can lead to highly variable results. We
propose several modifications of the PC-algorithm (and hence also of the other
algorithms) that remove part or all of this order-dependence. All proposed
modifications are consistent in high-dimensional settings under the same
conditions as their original counterparts. We compare the PC-, FCI-, and
RFCI-algorithms and their modifications in simulation studies and on a yeast
gene expression data set. We show that our modifications yield similar
performance in low-dimensional settings and improved performance in
high-dimensional settings. All software is implemented in the R-package pcalg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3297</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3297</id><created>2012-11-14</created><updated>2013-08-01</updated><authors><author><keyname>Yan</keyname><forenames>Dong-Ming</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author></authors><title>Gap Processing for Adaptive Maximal Poisson-Disk Sampling</title><categories>cs.GR</categories><comments>16 pages. ACM Transactions on Graphics, 2013</comments><acm-class>I.3.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the generation of maximal Poisson-disk sets with
varying radii. First, we present a geometric analysis of gaps in such disk
sets. This analysis is the basis for maximal and adaptive sampling in Euclidean
space and on manifolds. Second, we propose efficient algorithms and data
structures to detect gaps and update gaps when disks are inserted, deleted,
moved, or have their radius changed. We build on the concepts of the regular
triangulation and the power diagram. Third, we will show how our analysis can
make a contribution to the state-of-the-art in surface remeshing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3299</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3299</id><created>2012-11-14</created><authors><author><keyname>Brunsch</keyname><forenames>Tobias</forenames></author><author><keyname>Cornelissen</keyname><forenames>Kamiel</forenames></author><author><keyname>Manthey</keyname><forenames>Bodo</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author></authors><title>Smoothed Analysis of Belief Propagation for Minimum-Cost Flow and
  Matching</title><categories>cs.DS</categories><comments>To be presented at WALCOM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief propagation (BP) is a message-passing heuristic for statistical
inference in graphical models such as Bayesian networks and Markov random
fields. BP is used to compute marginal distributions or maximum likelihood
assignments and has applications in many areas, including machine learning,
image processing, and computer vision. However, the theoretical understanding
of the performance of BP is unsatisfactory.
  Recently, BP has been applied to combinatorial optimization problems. It has
been proved that BP can be used to compute maximum-weight matchings and
minimum-cost flows for instances with a unique optimum. The number of
iterations needed for this is pseudo-polynomial and hence BP is not efficient
in general.
  We study belief propagation in the framework of smoothed analysis and prove
that with high probability the number of iterations needed to compute
maximum-weight matchings and minimum-cost flows is bounded by a polynomial if
the weights/costs of the edges are randomly perturbed. To prove our upper
bounds, we use an isolation lemma by Beier and V\&quot;{o}cking (SIAM J. Comput.
2006) for matching and generalize an isolation lemma for min-cost flow by
Gamarnik, Shah, and Wei (Operations Research, 2012). We also prove almost
matching lower tail bounds for the number of iterations that BP needs to
converge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3302</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3302</id><created>2012-11-14</created><authors><author><keyname>Vinogradova</keyname><forenames>Galina</forenames></author><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>Rational Instability in the Natural Coalition Forming</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 12 Figures</comments><doi>10.1016/j.physa.2013.07.065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are investigating a paradigm of instability in coalition forming among
countries, which indeed is intrinsic to any collection of individual groups or
other social aggregations. Coalitions among countries are formed by the
respective attraction or repulsion caused by the historical bond propensities
between the countries, which produced an intricate circuit of bilateral bonds.
Contradictory associations into coalitions occur due to the independent
evolution of the bonds. Those coalitions tend to be unstable and break down
frequently. The model extends some features of the physical theory of Spin
Glasses. Within the frame of this model, the instability is viewed as a
consequence of decentralized maximization processes searching for the best
coalition allocations. In contrast to the existing literature, a rational
instability is found to result from forecast rationality of countries. Using a
general theoretical framework allowing to analyze the countries' decision
making in coalition forming, we feature a system where stability can eventually
be achieved as a result of the maximization processes. We provide a formal
implementation of the maximization principles and illustrate it in the
multi-thread simulation of the coalition forming. The results shed a new light
on the prospect of searches for the best coalition allocations in the networks
of social, political or economical entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3307</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3307</id><created>2012-11-14</created><authors><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Santucci</keyname><forenames>Fortunato</forenames></author></authors><title>Dynamic Optimization of Generalized Least Squares Handover Algorithms</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient handover algorithms are essential for highly performing mobile
wireless communications. These algorithms depend on numerous parameters, whose
settings must be appropriately optimized to offer a seamless connectivity.
Nevertheless, such an optimization is difficult in a time varying context,
unless adaptive strategies are used. In this paper, a new approach for the
handover optimization is proposed. First, a new modeling of the handover
process by a hybrid system that takes as input the handover parameters is
established. Then, this hybrid system is used to pose some dynamical
optimization approaches where the probability of outage and the probability of
handover are considered. Since it is shown that these probabilities are
difficult to compute, simple approximations of adequate accuracy are developed.
Based on these approximations, a new approach to the solution of the handover
optimizations is proposed by the use of a trellis diagram. A distributed
optimization algorithm is then developed to maximize handover performance. From
an extensive set of results obtained by numerical computations and simulations,
it is shown that the proposed algorithm allows to improve performance of the
handover considerably when compared to more traditional approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3322</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3322</id><created>2012-11-14</created><updated>2013-06-05</updated><authors><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author></authors><title>The Degrees of Freedom Region of Temporally-Correlated MIMO Networks
  with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>Revised to IEEE Trans. Inf. Theory. A new simple and unified
  framework is proposed, allowing to attain optimal DoF region for general
  antenna configurations and current CSIT qualities. A striking feature is
  that, every corner point in the DoF region can be achieved with one single
  scheme, and hence a new systematic way is proposed to prove the achievability
  instead of checking every corner point</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the temporally-correlated Multiple-Input Multiple-Output (MIMO)
broadcast channels (BC) and interference channels (IC) where the transmitter(s)
has/have (i) delayed channel state information (CSI) obtained from a
latency-prone feedback channel as well as (ii) imperfect current CSIT,
obtained, e.g., from prediction on the basis of these past channel samples
based on the temporal correlation. The degrees of freedom (DoF) regions for the
two-user broadcast and interference MIMO networks with general antenna
configuration under such conditions are fully characterized, as a function of
the prediction quality indicator. Specifically, a simple unified framework is
proposed, allowing to attain optimal DoF region for the general antenna
configurations and current CSIT qualities. Such a framework builds upon
block-Markov encoding with interference quantization, optimally combining the
use of both outdated and instantaneous CSIT. A striking feature of our work is
that, by varying the power allocation, every point in the DoF region can be
achieved with one single scheme. As a result, instead of checking the
achievability of every corner point of the outer bound region, as typically
done in the literature, we propose a new systematic way to prove the
achievability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3340</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3340</id><created>2012-11-14</created><authors><author><keyname>Texier</keyname><forenames>Jose</forenames></author><author><keyname>De Giusti</keyname><forenames>Marisa</forenames></author><author><keyname>Oviedo</keyname><forenames>Nestor</forenames></author><author><keyname>Villarreal</keyname><forenames>Gonzalo</forenames></author><author><keyname>Lira</keyname><forenames>Ariel</forenames></author></authors><title>The Benefits of Model-Driven Development in Institutional Repositories -
  Los Beneficios del Desarrollo Dirigido por Modelos en los Repositorios
  Institucionales</title><categories>cs.DL cs.SE</categories><comments>BIREDIAL 2012,
  http://eventos.uninorte.edu.co/index.php/biredial/biredial2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Institutional Repositories (IR) have been consolidated into the
institutions in scientific and academic areas, as shown by the directories
existing open access repositories and the deposits daily of articles made by
different ways, such as by self-archiving of registered users and the
cataloging by librarians. IR systems are based on various conceptual models, so
in this paper a bibliographic survey Model-Driven Development (MDD) in systems
and applications for RI in order to expose the benefits of applying MDD in IR.
The MDD is a paradigm for building software that assigns a central role models
and active under which derive models ranging from the most abstract to the
concrete, this is done through successive transformations. This paradigm
provides a framework that allows interested parties to share their views and
directly manipulate representations of the entities of this domain. Therefore,
the benefits are grouped by actors that are present, namely, developers,
business owners and domain experts. In conclusion, these benefits help make
more formal software implementations, resulting in a consolidation of such
systems, where the main beneficiaries are the end users through the services
are offered
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3371</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3371</id><created>2012-11-14</created><authors><author><keyname>Simons</keyname><forenames>C. L.</forenames></author><author><keyname>Smith</keyname><forenames>J. E.</forenames></author></authors><title>A Comparison of Meta-heuristic Search for Interactive Software Design</title><categories>cs.AI cs.NE</categories><comments>31 pages, 4 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in processing capacity, coupled with the desire to tackle problems
where a human subjective judgment plays an important role in determining the
value of a proposed solution, has led to a dramatic rise in the number of
applications of Interactive Artificial Intelligence. Of particular note is the
coupling of meta-heuristic search engines with user-provided evaluation and
rating of solutions, usually in the form of Interactive Evolutionary Algorithms
(IEAs). These have a well-documented history of successes, but arguably the
preponderance of IEAs stems from this history, rather than as a conscious
design choice of meta-heuristic based on the characteristics of the problem at
hand. This paper sets out to examine the basis for that assumption, taking as a
case study the domain of interactive software design. We consider a range of
factors that should affect the design choice including ease of use,
scalability, and of course, performance, i.e. that ability to generate good
solutions within the limited number of evaluations available in interactive
work before humans lose focus. We then evaluate three methods, namely greedy
local search, an evolutionary algorithm and ant colony optimization, with a
variety of representations for candidate solutions. Results show that after
suitable parameter tuning, ant colony optimization is highly effective within
interactive search and out-performs evolutionary algorithms with respect to
increasing numbers of attributes and methods in the software design problem.
However, when larger numbers of classes are present in the software design, an
evolutionary algorithm using a naive grouping integer-based representation
appears more scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3375</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3375</id><created>2012-11-14</created><updated>2012-11-29</updated><authors><author><keyname>Seufert</keyname><forenames>Stephan</forenames></author><author><keyname>Anand</keyname><forenames>Avishek</forenames></author><author><keyname>Bedathur</keyname><forenames>Srikanta</forenames></author><author><keyname>Weikum</keyname><forenames>Gerhard</forenames></author></authors><title>High-Performance Reachability Query Processing under Index Size
  Restrictions</title><categories>cs.DB cs.SI</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a scalable and highly efficient index structure for
the reachability problem over graphs. We build on the well-known node interval
labeling scheme where the set of vertices reachable from a particular node is
compactly encoded as a collection of node identifier ranges. We impose an
explicit bound on the size of the index and flexibly assign approximate
reachability ranges to nodes of the graph such that the number of index probes
to answer a query is minimized. The resulting tunable index structure generates
a better range labeling if the space budget is increased, thus providing a
direct control over the trade off between index size and the query processing
performance. By using a fast recursive querying method in conjunction with our
index structure, we show that in practice, reachability queries can be answered
in the order of microseconds on an off-the-shelf computer - even for the case
of massive-scale real world graphs. Our claims are supported by an extensive
set of experimental results using a multitude of benchmark and real-world
web-scale graph datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3376</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3376</id><created>2012-11-12</created><updated>2014-06-16</updated><authors><author><keyname>Foster</keyname><forenames>Erich L</forenames></author><author><keyname>Overfelt</keyname><forenames>James R</forenames></author></authors><title>Clipping of Arbitrary Polygons with Degeneracies</title><categories>cs.CG</categories><comments>The paper has been withdrawn due to not being able to truly handle
  all degenerate cases as claimed</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Polygon clipping is a frequent operation in Arbitrary Lagrangian-Eulerian
methods, Computer Graphics, GIS, and CAD. In fact, clipping algorithms are said
to be one of the most important operations in computer graphics. Thus,
efficient and general polygon clipping algorithms are of great importance.
Greiner et al. developed a time efficient algorithm which could clip arbitrary
polygons, including concave and self intersecting polygons. However, the
Greiner-Hormann algorithm does not properly handle degenerate cases, without
the undesirable need for perturbing vertices. We present an extension to the
Greiner-Hormann polygon clipping algorithm which properly deals with degenerate
cases. We combine the method proposed by Kim et al. and the method mentioned by
Liu et al. to remove or properly label degenerate cases. Additionally, the
algorithm presented avoids the need for calculating midpoints, doesn't require
additional entry/exit flags, and avoids changing the vertex data structure used
in the original Greiner-Hormann algorithm, which was required by the extension
presented by Kim et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3384</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3384</id><created>2012-11-14</created><updated>2012-11-23</updated><authors><author><keyname>Azouaoui</keyname><forenames>Ahmed</forenames></author><author><keyname>Berkani</keyname><forenames>Ahlam</forenames></author><author><keyname>Belkasmi</keyname><forenames>Mostafa</forenames></author></authors><title>An Efficient Soft Decoder of Block Codes Based on Compact Genetic
  Algorithm</title><categories>cs.IT math.IT</categories><comments>8 pages; IJCSI 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Soft-decision decoding is NP-hard problem of great interest to developers of
communication system. We present an efficient soft-decision decoding of linear
block codes based on compact genetic algorithm (cGA) and compare its
performance with various other decoding algorithms including Shakeel
algorithms. The proposed algorithm uses the dual code in contrast to Shakeel
algorithm that uses the code itself. Hence, this new approach reduces the
decoding complexity of high rates codes. The complexity and an optimized
version of this new algorithm is also presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3402</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3402</id><created>2012-11-14</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>Genetic Optimization of Keywords Subset in the Classification Analysis
  of Texts Authorship</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The genetic selection of keywords set, the text frequencies of which are
considered as attributes in text classification analysis, has been analyzed.
The genetic optimization was performed on a set of words, which is the fraction
of the frequency dictionary with given frequency limits. The frequency
dictionary was formed on the basis of analyzed text array of texts of English
fiction. As the fitness function which is minimized by the genetic algorithm,
the error of nearest k neighbors classifier was used. The obtained results show
high precision and recall of texts classification by authorship categories on
the basis of attributes of keywords set which were selected by the genetic
algorithm from the frequency dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3412</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3412</id><created>2012-11-13</created><authors><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author><author><keyname>Kompella</keyname><forenames>Ramana</forenames></author></authors><title>Network Sampling: From Static to Streaming Graphs</title><categories>cs.SI cs.DS cs.LG physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network sampling is integral to the analysis of social, information, and
biological networks. Since many real-world networks are massive in size,
continuously evolving, and/or distributed in nature, the network structure is
often sampled in order to facilitate study. For these reasons, a more thorough
and complete understanding of network sampling is critical to support the field
of network science. In this paper, we outline a framework for the general
problem of network sampling, by highlighting the different objectives,
population and units of interest, and classes of network sampling methods. In
addition, we propose a spectrum of computational models for network sampling
methods, ranging from the traditionally studied model based on the assumption
of a static domain to a more challenging model that is appropriate for
streaming domains. We design a family of sampling methods based on the concept
of graph induction that generalize across the full spectrum of computational
models (from static to streaming) while efficiently preserving many of the
topological properties of the input graphs. Furthermore, we demonstrate how
traditional static sampling algorithms can be modified for graph streams for
each of the three main classes of sampling methods: node, edge, and
topology-based sampling. Our experimental results indicate that our proposed
family of sampling methods more accurately preserves the underlying properties
of the graph for both static and streaming graphs. Finally, we study the impact
of network sampling algorithms on the parameter estimation and performance
evaluation of relational classification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3428</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3428</id><created>2012-11-14</created><authors><author><keyname>Ogden</keyname><forenames>R. D.</forenames><affiliation>Computer Science Dept. Texas State University</affiliation></author></authors><title>The Encoding of Natural Numbers as Nested Parentheses Strings with
  Associated Probability Distributions</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an efficient encoding of the natural numbers {0,1,2,3,...} as
strings of nested parentheses {(),(()),(()()),((())),...}, or considered
inversely, an efficient enumeration of such strings. The technique is based on
the recursive definition of the Catalan numbers. The probability distributions
arising from this encoding are explored. Applications of this encoding to
prefix-free data encoding and recursive function theory are briefly considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3439</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3439</id><created>2012-11-14</created><authors><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Desai</keyname><forenames>Devendra</forenames></author><author><keyname>Srinivasan</keyname><forenames>Srikanth</forenames></author></authors><title>Optimal Hitting Sets for Combinatorial Shapes</title><categories>cs.CC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing explicit Hitting sets for
Combinatorial Shapes, a class of statistical tests first studied by Gopalan,
Meka, Reingold, and Zuckerman (STOC 2011). These generalize many well-studied
classes of tests, including symmetric functions and combinatorial rectangles.
Generalizing results of Linial, Luby, Saks, and Zuckerman (Combinatorica 1997)
and Rabani and Shpilka (SICOMP 2010), we construct hitting sets for
Combinatorial Shapes of size polynomial in the alphabet, dimension, and the
inverse of the error parameter. This is optimal up to polynomial factors. The
best previous hitting sets came from the Pseudorandom Generator construction of
Gopalan et al., and in particular had size that was quasipolynomial in the
inverse of the error parameter.
  Our construction builds on natural variants of the constructions of Linial et
al. and Rabani and Shpilka. In the process, we construct fractional perfect
hash families and hitting sets for combinatorial rectangles with stronger
guarantees. These might be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3444</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3444</id><created>2012-11-14</created><authors><author><keyname>Cung</keyname><forenames>B.</forenames></author><author><keyname>Jin</keyname><forenames>T.</forenames></author><author><keyname>Ramirez</keyname><forenames>J.</forenames></author><author><keyname>Thompson</keyname><forenames>A.</forenames></author><author><keyname>Boutsidis</keyname><forenames>C.</forenames></author><author><keyname>Needell</keyname><forenames>D.</forenames></author></authors><title>Spectral Clustering: An empirical study of Approximation Algorithms and
  its Application to the Attrition Problem</title><categories>cs.LG math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is the problem of separating a set of objects into groups (called
clusters) so that objects within the same cluster are more similar to each
other than to those in different clusters. Spectral clustering is a now
well-known method for clustering which utilizes the spectrum of the data
similarity matrix to perform this separation. Since the method relies on
solving an eigenvector problem, it is computationally expensive for large
datasets. To overcome this constraint, approximation methods have been
developed which aim to reduce running time while maintaining accurate
classification. In this article, we summarize and experimentally evaluate
several approximation methods for spectral clustering. From an applications
standpoint, we employ spectral clustering to solve the so-called attrition
problem, where one aims to identify from a set of employees those who are
likely to voluntarily leave the company from those who are not. Our study sheds
light on the empirical performance of existing approximate spectral clustering
methods and shows the applicability of these methods in an important business
optimization related problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3451</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3451</id><created>2012-11-14</created><authors><author><keyname>Stowe</keyname><forenames>Matt</forenames></author></authors><title>Memory Capacity of a Random Neural Network</title><categories>cs.NE</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3466</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3466</id><created>2012-11-14</created><authors><author><keyname>Dimovski</keyname><forenames>Tome</forenames></author><author><keyname>Mitrevski</keyname><forenames>Pece</forenames></author></authors><title>On the Performance Potential of Connection Fault-Tolerant Commit
  Processing in Mobile Environment</title><categories>cs.NI</categories><acm-class>C.2.4; C.2.1; C.4</acm-class><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 5, October 2012</journal-ref><doi>10.5121/ijwmn.2012.4503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile inventory, mobile commerce, banking and/or commercial applications are
some distinctive examples that increasingly use distributed transactions. It is
inevitably harder to design efficient commit protocols, due to some intrinsic
mobile environment limitations. A handful of protocols for transaction
processing have been offered, but the majority considers only a limited number
of communication models. We introduce an improved Connection Fault-Tolerant
model and evaluate its performance potential by comparing results in several
deferent scenarios, as well as its contribution to the overall mobile
transaction commit rate. Our performance analysis, conducted using
general-purpose discrete-event simulation programming language, evaluates the
effect of (i) ad-hoc communication between mobile hosts and (ii) the employment
of an appropriate decision algorithm for mobile host agents. Conjointly, they
substantially improve commit rate. In respect of the stochastic nature of
random network disconnections, we determine connection timeout values that
contribute the most to the highest perceived ad-hoc communication impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3476</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3476</id><created>2012-11-14</created><authors><author><keyname>Ciobanu</keyname><forenames>Gabriel</forenames><affiliation>Romanian Academy, Institute of Computer Science</affiliation></author></authors><title>Proceedings 6th Workshop on Membrane Computing and Biologically Inspired
  Process Calculi</title><categories>cs.PL cs.ET cs.LO</categories><proxy>EPTCS</proxy><acm-class>F.1.2; F.3.2; G.3; J.3</acm-class><journal-ref>EPTCS 100, 2012</journal-ref><doi>10.4204/EPTCS.100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the 6th Membrane Computing and
Biologically Inspired Process Calculi (MeCBIC 2012), a satellite workshop of
the 23rd International Conference on Concurrency Theory (CONCUR) held on 8th
September 2012 in Newcastle upon Tyne, UK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3480</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3480</id><created>2012-11-14</created><authors><author><keyname>Alves</keyname><forenames>Sandra</forenames><affiliation>University of Porto</affiliation></author><author><keyname>Mackie</keyname><forenames>Ian</forenames><affiliation>&#xc9;cole Polytechnique</affiliation></author></authors><title>Proceedings 2nd International Workshop on Linearity</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 101, 2012</journal-ref><doi>10.4204/EPTCS.101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains a selection of the papers presented at the 2nd
International Workshop on Linearity (LINEARITY'2012), which took place 1 April
2012 in Tallinn, Estonia. The workshop was a one-day satellite event of ETAPS
2012, the 15th European Joint Conference on Theory and Practice of Software.
  The aim of this workshop was to bring together researchers who are currently
developing theory and applications of linear calculi, in order to foster their
interaction, to provide a forum for presenting new ideas and work in progress,
and to enable newcomers to learn about current activities in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3484</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3484</id><created>2012-11-14</created><authors><author><keyname>Ruan</keyname><forenames>Liangzhong</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>The Feasibility Conditions for Interference Alignment in MIMO Networks</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE Trans. Signal Process</comments><doi>10.1109/TSP.2013.2241056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) has attracted great attention in the last few
years for its breakthrough performance in interference networks. However,
despite the numerous works dedicated to IA, the feasibility conditions of IA
remains unclear for most network topologies. The IA feasibility analysis is
challenging as the IA constraints are sets of high-degree polynomials, for
which no systematic tool to analyze the solvability conditions exists. In this
work, by developing a new mathematical framework that maps the solvability of
sets of polynomial equations to the linear independence of their first-order
terms, we propose a sufficient condition that applies to MIMO interference
networks with general configurations. We have further proved that this
sufficient condition matches with the necessary conditions under a wide range
of configurations. These results further consolidate the theoretical basis of
IA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3487</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3487</id><created>2012-11-14</created><authors><author><keyname>Conlon</keyname><forenames>David</forenames></author><author><keyname>Fox</keyname><forenames>Jacob</forenames></author></authors><title>Graph removal lemmas</title><categories>math.CO cs.DM</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph removal lemma states that any graph on n vertices with o(n^{v(H)})
copies of a fixed graph H may be made H-free by removing o(n^2) edges. Despite
its innocent appearance, this lemma and its extensions have several important
consequences in number theory, discrete geometry, graph theory and computer
science. In this survey we discuss these lemmas, focusing in particular on
recent improvements to their quantitative aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3492</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3492</id><created>2012-11-15</created><authors><author><keyname>Malinina</keyname><forenames>Natalia L.</forenames></author></authors><title>On the principal impossibility to prove P=NP</title><categories>cs.CC</categories><comments>20 pages, 21 figures. arXiv admin note: substantial text overlap with
  arXiv:1210.6088, arXiv:1007.1059</comments><msc-class>05C10, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The material of the article is devoted to the most complicated and
interesting problem -- a problem of P = NP?. This research was presented to
mathematical community in Hyderabad during International Congress of
Mathematicians. But there it was published in a very brief form, so this
article is an attempt to give those, who are interested in the problem, my
reasoning on the theme. It is not a proof in full, because it is very difficult
to prove something, which is not provable, but it seems that these reasoning
will help us to understand the problem of the combinatorial explosion more
deeply and to realize in full all the problems to which we are going because of
the combinatorial explosion. Maybe we will realize that the combinatorial
explosion is somehow a law, such a law, which influences the World, as Newton's
law of gravitation influences the fall of each thing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3497</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3497</id><created>2012-11-15</created><authors><author><keyname>Abeysiriwardana</keyname><forenames>Prabath Chaminda</forenames></author><author><keyname>Kodituwakku</keyname><forenames>Saluka R</forenames></author></authors><title>Ontology Based Information Extraction for Disease Intelligence</title><categories>cs.AI cs.DL cs.IR</categories><comments>Disease Intelligence, Disease Ontology, Information Extraction,
  Semantic Web</comments><journal-ref>International Journal of Research in Computer Science, 2 (6): pp.
  7-19, November 2012. doi:10.7815/ijorcs.26.2012.051</journal-ref><doi>10.7815/ijorcs.26.2012.051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disease Intelligence (DI) is based on the acquisition and aggregation of
fragmented knowledge of diseases at multiple sources all over the world to
provide valuable information to doctors, researchers and information seeking
community. Some diseases have their own characteristics changed rapidly at
different places of the world and are reported on documents as unrelated and
heterogeneous information which may be going unnoticed and may not be quickly
available. This research presents an Ontology based theoretical framework in
the context of medical intelligence and country/region. Ontology is designed
for storing information about rapidly spreading and changing diseases with
incorporating existing disease taxonomies to genetic information of both humans
and infectious organisms. It further maps disease symptoms to diseases and drug
effects to disease symptoms. The machine understandable disease ontology
represented as a website thus allows the drug effects to be evaluated on
disease symptoms and exposes genetic involvements in the human diseases.
Infectious agents which have no known place in an existing classification but
have data on genetics would still be identified as organisms through the
intelligence of this system. It will further facilitate researchers on the
subject to try out different solutions for curing diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3500</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3500</id><created>2012-11-15</created><updated>2013-06-24</updated><authors><author><keyname>Zhou</keyname><forenames>Guoxu</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author></authors><title>Accelerated Canonical Polyadic Decomposition by Using Mode Reduction</title><categories>cs.NA cs.LG math.NA</categories><comments>12 pages. Accepted by TNNLS</comments><doi>10.1109/TNNLS.2013.2271507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widely
applied to analyze high order tensors. Existing CPD methods use alternating
least square (ALS) iterations and hence need to unfold tensors to each of the
$N$ modes frequently, which is one major bottleneck of efficiency for
large-scale data and especially when $N$ is large. To overcome this problem, in
this paper we proposed a new CPD method which converts the original $N$th
($N&gt;3$) order tensor to a 3rd-order tensor first. Then the full CPD is realized
by decomposing this mode reduced tensor followed by a Khatri-Rao product
projection procedure. This way is quite efficient as unfolding to each of the
$N$ modes are avoided, and dimensionality reduction can also be easily
incorporated to further improve the efficiency. We show that, under mild
conditions, any $N$th-order CPD can be converted into a 3rd-order case but
without destroying the essential uniqueness, and theoretically gives the same
results as direct $N$-way CPD methods. Simulations show that, compared with
state-of-the-art CPD methods, the proposed method is more efficient and escape
from local solutions more easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3502</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3502</id><created>2012-11-15</created><authors><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author></authors><title>Efficient Group Key Management Schemes for Multicast Dynamic
  Communication Systems</title><categories>cs.CR cs.NI</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key management in multicast dynamic groups, where users can leave or join at
their ease is one of the most crucial and essential part of secure
communication. Various efficient management strategies have been proposed
during last decade that aim to decrease encryption costs and transmission
overheads. In this report, two different types of key management schemes are
proposed. First proposed scheme is based on One-way function tree (OFT). The
proposed scheme fulfills the security gaps that have been pointed out in recent
years. Second proposed scheme is based on logical key hierarchy (LKH). This
proposed scheme provides better performance for, rather inflexible and
expensive, LKH scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3503</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3503</id><created>2012-11-15</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Spectral Efficiency in Large-Scale MIMO-OFDM Systems with Per-Antenna
  Power Cost</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures. It's accepted and presented in Asilomar 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, resource allocation for multiple-input multiple-output
orthogonal frequency division multiplexing (MIMO-OFDM) downlink networks with
large numbers of base station antennas is studied. Assuming perfect channel
state information at the transmitter, the resource allocation algorithm design
is modeled as a non-convex optimization problem which takes into account the
joint power consumption of the power amplifiers, antenna unit, and signal
processing circuit unit. Subsequently, by exploiting the law of large numbers
and dual decomposition, an efficient suboptimal iterative resource allocation
algorithm is proposed for maximization of the system capacity (bit/s). In
particular, closed-form power allocation and antenna allocation policies are
derived in each iteration. Simulation results illustrate that the proposed
iterative resource allocation algorithm achieves a close-to-optimal performance
in a small number of iterations and unveil a trade-off between system capacity
and the number of activated antennas: Activating all antennas may not be a good
solution for system capacity maximization when a system with a per antenna
power cost is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3553</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3553</id><created>2012-11-15</created><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Xie</keyname><forenames>Tao</forenames></author></authors><title>Breaking a novel image encryption scheme based on improved hyperchaotic
  sequences</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a novel image encryption scheme based on improved hyperchaotic
sequences was proposed. A pseudo-random number sequence, generated by a
hyper-chaos system, is used to determine two involved encryption functions,
bitwise exclusive or (XOR) operation and modulo addition. It was reported that
the scheme can be broken with some pairs of chosen plain-images and the
corresponding cipher-images. This paper re-evaluates security of the encryption
scheme and finds that the encryption scheme can be broken with only one known
plain-image. The performance of the known-plaintext attack, in terms of success
probability and computation load, become even much better when two known
plain-images are available. In addition, security defects on insensitivity of
encryption result with respect to changes of secret key and plain-image, are
reported also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3567</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3567</id><created>2012-11-15</created><authors><author><keyname>Mirkov</keyname><forenames>Nikola</forenames></author><author><keyname>Rasuo</keyname><forenames>Bosko</forenames></author></authors><title>A Bernstein Polynomial Collocation Method for the Solution of Elliptic
  Boundary Value Problems</title><categories>math.NA cs.MS cs.NA physics.comp-ph</categories><comments>21 page, 12 figures, 5tables, Python code listings in the Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a formulation of a point-collocation method in which the
unknown function is approximated using global expansion in tensor product
Bernstein polynomial basis is presented. Bernstein polynomials used in this
study are defined over general interval [a,b]. Method incorporates several
ideas that enable higher numerical efficiency compared to Bernstein polynomial
methods that have been previously presented. The approach is illustrated by a
solution of Poisson, Helmholtz and Biharmonic equations with Dirichlet and
Neumann type boundary conditions. Comparisons with analytical solutions are
given to demonstrate the accuracy and convergence properties of the current
procedure. The method is implemented in an open-source code, and a library for
manipulation of Bernstein polynomials bernstein-poly, developed by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3603</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3603</id><created>2012-11-15</created><authors><author><keyname>Orieux</keyname><forenames>F.</forenames></author><author><keyname>Giovannelli</keyname><forenames>J. -F.</forenames></author><author><keyname>Rodet</keyname><forenames>T.</forenames></author><author><keyname>Abergel</keyname><forenames>A.</forenames></author></authors><title>Estimating hyperparameters and instrument parameters in regularized
  inversion. Illustration for SPIRE/Herschel map making</title><categories>astro-ph.IM cs.NA stat.AP stat.ME</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe regularized methods for image reconstruction and focus on the
question of hyperparameter and instrument parameter estimation, i.e.
unsupervised and myopic problems. We developed a Bayesian framework that is
based on the \post density for all unknown quantities, given the observations.
This density is explored by a Markov Chain Monte-Carlo sampling technique based
on a Gibbs loop and including a Metropolis-Hastings step. The numerical
evaluation relies on the SPIRE instrument of the Herschel observatory. Using
simulated and real observations, we show that the hyperparameters and
instrument parameters are correctly estimated, which opens up many perspectives
for imaging in astrophysics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3624</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3624</id><created>2012-11-15</created><updated>2013-05-30</updated><authors><author><keyname>Bartoletti</keyname><forenames>Massimo</forenames></author><author><keyname>Cimoli</keyname><forenames>Tiziana</forenames></author><author><keyname>Pinna</keyname><forenames>G. Michele</forenames></author></authors><title>Lending Petri nets and contracts</title><categories>cs.LO cs.MA cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreography-based approaches to service composition typically assume that,
after a set of services has been found which correctly play the roles
prescribed by the choreography, each service respects his role. Honest services
are not protected against adversaries. We propose a model for contracts based
on a extension of Petri nets, which allows services to protect themselves while
still realizing the choreography. We relate this model with Propositional
Contract Logic, by showing a translation of formulae into our Petri nets which
preserves the logical notion of agreement, and allows for compositional
verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3642</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3642</id><created>2012-11-15</created><updated>2013-01-18</updated><authors><author><keyname>Goto</keyname><forenames>Keisuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author></authors><title>Simpler and Faster Lempel Ziv Factorization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new, simple, and efficient approach for computing the Lempel-Ziv
(LZ77) factorization of a string in linear time, based on suffix arrays.
Computational experiments on various data sets show that our approach
constantly outperforms the currently fastest algorithm LZ OG (Ohlebusch and Gog
2011), and can be up to 2 to 3 times faster in the processing after obtaining
the suffix array, while requiring the same or a little more space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3643</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3643</id><created>2012-11-15</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author></authors><title>A Principled Approach to Grammars for Controlled Natural Languages and
  Predictive Editors</title><categories>cs.CL</categories><journal-ref>Journal of Logic, Language and Information, 22(1), 2013</journal-ref><doi>10.1007/s10849-012-9167-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlled natural languages (CNL) with a direct mapping to formal logic have
been proposed to improve the usability of knowledge representation systems,
query interfaces, and formal specifications. Predictive editors are a popular
approach to solve the problem that CNLs are easy to read but hard to write.
Such predictive editors need to be able to &quot;look ahead&quot; in order to show all
possible continuations of a given unfinished sentence. Such lookahead features,
however, are difficult to implement in a satisfying way with existing grammar
frameworks, especially if the CNL supports complex nonlocal structures such as
anaphoric references. Here, methods and algorithms are presented for a new
grammar notation called Codeco, which is specifically designed for controlled
natural languages and predictive editors. A parsing approach for Codeco based
on an extended chart parsing algorithm is presented. A large subset of Attempto
Controlled English (ACE) has been represented in Codeco. Evaluation of this
grammar and the parser implementation shows that the approach is practical,
adequate and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3659</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3659</id><created>2012-11-15</created><authors><author><keyname>Alben</keyname><forenames>Silas</forenames></author></authors><title>Color scales that are effective in both color and grayscale</title><categories>cs.GR</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a color scale which performs well when
converted to a grayscale. We assume that each color is converted to a shade of
gray with the same intensity as the color. We also assume that the color scales
have a linear variation of intensity and hue, and find scales which maximize
the average chroma (or &quot;colorfulness&quot;) of the colors. We find two classes of
solutions, which traverse the color wheel in opposite directions. The two
classes of scales start with hues near cyan and red. The average chroma of the
scales are 65-77% those of the pure colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3666</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3666</id><created>2012-11-15</created><authors><author><keyname>Li</keyname><forenames>Shuang</forenames></author><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Ekici</keyname><forenames>Eylem</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Maximizing System Throughput Using Cooperative Sensing in Multi-Channel
  Cognitive Radio Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cognitive Radio Networks (CRNs), unlicensed users are allowed to access
the licensed spectrum when it is not currently being used by primary users
(PUs). In this paper, we study the throughput maximization problem for a
multi-channel CRN where each SU can only sense a limited number of channels. We
show that this problem is strongly NP-hard, and propose an approximation
algorithm with a factor at least $1/2\mu$ where $\mu \in [1,2]$ is a system
parameter reflecting the sensing capability of SUs across channels and their
sensing budgets. This performance guarantee is achieved by exploiting a nice
structural property of the objective function and constructing a particular
matching. Our numerical results demonstrate the advantage of our algorithm
compared with both a random and a greedy sensing assignment algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3668</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3668</id><created>2012-11-15</created><updated>2013-05-23</updated><authors><author><keyname>Ley</keyname><forenames>Christophe</forenames></author><author><keyname>Swan</keyname><forenames>Yvik</forenames></author></authors><title>Local Pinsker inequalities via Stein's discrete density approach</title><categories>math.PR cs.IT math.IT</categories><comments>This is a revised version of our paper &quot;Discrete Stein
  characterizations and discrete information distances&quot; (arXiv reference :
  arXiv:1201.0143). Essential changes have been made. Certain elements of the
  previous version remain relevant to the literature and have not been included
  in the present version, therefore we upload this as a new arXiv submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pinsker's inequality states that the relative entropy $d_{\mathrm{KL}}(X, Y)$
between two random variables $X$ and $Y$ dominates the square of the total
variation distance $d_{\mathrm{TV}}(X,Y)$ between $X$ and $Y$. In this paper we
introduce generalized Fisher information distances $\mathcal{J}(X, Y)$ between
discrete distributions $X$ and $Y$ and prove that these also dominate the
square of the total variation distance. To this end we introduce a general
discrete Stein operator for which we prove a useful covariance identity. We
illustrate our approach with several examples. Whenever competitor inequalities
are available in the literature, the constants in ours are at least as good,
and, in several cases, better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3677</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3677</id><created>2012-11-15</created><authors><author><keyname>Canzian</keyname><forenames>Luca</forenames></author><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Pricing and Intervention in Slotted-Aloha: Technical Report</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many wireless communication networks a common channel is shared by
multiple users who must compete to gain access to it. The operation of the
network by self-interested and strategic users usually leads to the overuse of
the channel resources and to substantial inefficiencies. Hence, incentive
schemes are needed to overcome the inefficiencies of non-cooperative
equilibrium. In this work we consider a slotted-Aloha like random access
protocol and two incentive schemes: pricing and intervention. We provide some
criteria for the designer of the protocol to choose one scheme between them and
to design the best policy for the selected scheme, depending on the system
parameters. Our results show that intervention can achieve the maximum
efficiency in the perfect monitoring scenario. In the imperfect monitoring
scenario, instead, the performance of the system depends on the information
held by the different entities and, in some cases, there exists a threshold for
the number of users such that, for a number of users lower than the threshold,
intervention outperforms pricing, whereas, for a number of users higher than
the threshold pricing outperforms intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3682</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3682</id><created>2012-11-07</created><authors><author><keyname>Aswani</keyname><forenames>P. Naga</forenames></author><author><keyname>Shekar</keyname><forenames>K. Chandra</forenames></author></authors><title>Fuzzy Keyword Search over Encrypted Data using Symbol-Based
  Trie-traverse Search Scheme in Cloud Computing</title><categories>cs.CR</categories><comments>8 pages, 2012 CSC 2278-9200 published http://www.cschronicle.org</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We exploit edit distance to quantify keywords similarity and develop two
advanced techniques on constructing fuzzy keyword sets, which achieve optimized
storage and representation overheads. We further propose a brand new
symbol-based trie-traverse searching scheme, where a multi-way tree structure
is built up using symbols transformed from the resulted fuzzy keyword sets.
Through rigorous security analysis, we show that our proposed solution is
secure and privacy-preserving, while correctly realizing the goal of fuzzy
keyword search. Extensive experimental results demonstrate the efficiency of
the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3700</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3700</id><created>2012-11-15</created><updated>2012-11-15</updated><authors><author><keyname>Hirsch</keyname><forenames>Andrew K.</forenames></author><author><keyname>Clarkson</keyname><forenames>Michael R.</forenames></author></authors><title>Nexus Authorization Logic (NAL): Logical Results</title><categories>cs.CR cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nexus Authorization Logic (NAL) [Schneider et al. 2011] is a logic for
reasoning about authorization in distributed systems. A revised version of NAL
is given here, including revised syntax, a revised proof theory using localized
hypotheses, and a new Kripke semantics. The proof theory is proved sound with
respect to the semantics, and that proof is formalized in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3711</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3711</id><created>2012-11-14</created><authors><author><keyname>Graves</keyname><forenames>Alex</forenames></author></authors><title>Sequence Transduction with Recurrent Neural Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>First published in the International Conference of Machine Learning
  (ICML) 2012 Workshop on Representation Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning tasks can be expressed as the transformation---or
\emph{transduction}---of input sequences into output sequences: speech
recognition, machine translation, protein secondary structure prediction and
text-to-speech to name but a few. One of the key challenges in sequence
transduction is learning to represent both the input and output sequences in a
way that is invariant to sequential distortions such as shrinking, stretching
and translating. Recurrent neural networks (RNNs) are a powerful sequence
learning architecture that has proven capable of learning such representations.
However RNNs traditionally require a pre-defined alignment between the input
and output sequences to perform transduction. This is a severe limitation since
\emph{finding} the alignment is the most difficult aspect of many sequence
transduction problems. Indeed, even determining the length of the output
sequence is often challenging. This paper introduces an end-to-end,
probabilistic sequence transduction system, based entirely on RNNs, that is in
principle able to transform any input sequence into any finite, discrete output
sequence. Experimental results for phoneme recognition are provided on the
TIMIT speech corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3719</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3719</id><created>2012-11-15</created><updated>2013-07-21</updated><authors><author><keyname>Lioumpas</keyname><forenames>Athanasios S.</forenames></author><author><keyname>Bithas</keyname><forenames>Petros S.</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Partitioning of Distributed MIMO Systems based on Overhead
  Considerations</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed-Multiple Input Multiple Output (DMIMO) networks is a promising
enabler to address the challenges of high traffic demand in future wireless
networks. A limiting factor that is directly related to the performance of
these systems is the overhead signaling required for distributing data and
control information among the network elements. In this paper, the concept of
orthogonal partitioning is extended to D-MIMO networks employing joint
multi-user beamforming, aiming to maximize the effective sum-rate, i.e., the
actual transmitted information data. Furthermore, in order to comply with
practical requirements, the overhead subframe size is considered to be
constrained. In this context, a novel formulation of constrained orthogonal
partitioning is introduced as an elegant Knapsack optimization problem, which
allows the derivation of quick and accurate solutions. Several numerical
results give insight into the capabilities of D-MIMO networks and the actual
sum-rate scaling under overhead constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3722</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3722</id><created>2012-11-15</created><updated>2013-07-24</updated><authors><author><keyname>Johnson</keyname><forenames>J. Ian</forenames></author><author><keyname>Labich</keyname><forenames>Nicholas</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Optimizing Abstract Abstract Machines</title><categories>cs.PL</categories><comments>Proceedings of the International Conference on Functional Programming
  2013 (ICFP 2013). Boston, Massachusetts. September, 2013</comments><acm-class>F.3.2</acm-class><doi>10.1145/2500365.2500604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technique of abstracting abstract machines (AAM) provides a systematic
approach for deriving computable approximations of evaluators that are easily
proved sound. This article contributes a complementary step-by-step process for
subsequently going from a naive analyzer derived under the AAM approach, to an
efficient and correct implementation. The end result of the process is a two to
three order-of-magnitude improvement over the systematically derived analyzer,
making it competitive with hand-optimized implementations that compute
fundamentally less precise results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3729</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3729</id><created>2012-11-15</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Data-Efficient Quickest Change Detection in Minimax Settings</title><categories>math.ST cs.IT math.IT math.OC math.PR stat.TH</categories><comments>Submitted to IEEE Transactions on Information Theory 14-Nov-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical problem of quickest change detection is studied with an
additional constraint on the cost of observations used in the detection
process. The change point is modeled as an unknown constant, and minimax
formulations are proposed for the problem. The objective in these formulations
is to find a stopping time and an on-off observation control policy for the
observation sequence, to minimize a version of the worst possible average
delay, subject to constraints on the false alarm rate and the fraction of time
observations are taken before change. An algorithm called DE-CuSum is proposed
and is shown to be asymptotically optimal for the proposed formulations, as the
false alarm rate goes to zero. Numerical results are used to show that the
DE-CuSum algorithm has good trade-off curves and performs significantly better
than the approach of fractional sampling, in which the observations are skipped
using the outcome of a sequence of coin tosses, independent of the observation
process. This work is guided by the insights gained from an earlier study of a
Bayesian version of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3754</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3754</id><created>2012-11-15</created><updated>2014-08-19</updated><authors><author><keyname>Qiu</keyname><forenames>Chenlu</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author><author><keyname>Lois</keyname><forenames>Brian</forenames></author><author><keyname>Hogben</keyname><forenames>Leslie</forenames></author></authors><title>Recursive Robust PCA or Recursive Sparse Recovery in Large but
  Structured Noise</title><categories>cs.IT math.IT</categories><comments>This version was accepted for publication in IEEE Transactions on
  Information Theory, August 2014</comments><journal-ref>Information Theory, IEEE Transactions on , vol.60, no.8,
  pp.5007,5039, Aug. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the recursive robust principal components' analysis(PCA)
problem. Here, &quot;robust&quot; refers to robustness to both independent and correlated
sparse outliers. If the outlier is the signal-of-interest, this problem can be
interpreted as one of recursively recovering a time sequence of sparse vectors,
St, in the presence of large but structured noise, Lt. The structure that we
assume on Lt is that Lt is dense and lies in a low dimensional subspace that is
either fixed or changes &quot;slowly enough&quot;. A key application where this problem
occurs is in video surveillance where the goal is to separate a slowly changing
background (Lt) from moving foreground objects (St) on-the-fly. To solve the
above problem, we introduce a novel solution called Recursive Projected CS
(ReProCS). Under mild assumptions, we show that, with high probability
(w.h.p.), ReProCS can exactly recover the support set of St at all times; and
the reconstruction errors of both St and Lt are upper bounded by a
time-invariant and small value at all times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3776</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3776</id><created>2012-11-15</created><authors><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Komnakos</keyname><forenames>Dimitris I.</forenames></author><author><keyname>Vouyioukas</keyname><forenames>Demosthenes D.</forenames></author><author><keyname>Constantinou</keyname><forenames>Philip</forenames></author></authors><title>Radio Resource Allocation Algorithms for Multi-Service OFDMA Networks:
  The Uniform Power Loading Scenario</title><categories>cs.IT math.IT</categories><comments>accepted for publication at the Springer Telecommunication Systems
  Journal (TSMJ)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive Radio Resource Allocation is essential for guaranteeing high
bandwidth and power utilization as well as satisfying heterogeneous
Quality-of-Service requests regarding next generation broadband multicarrier
wireless access networks like LTE and Mobile WiMAX. A downlink OFDMA
single-cell scenario is considered where heterogeneous Constant-Bit-Rate and
Best-Effort QoS profiles coexist and the power is uniformly spread over the
system bandwidth utilizing a Uniform Power Loading (UPL) scenario. We express
this particular QoS provision scenario in mathematical terms, as a variation of
the well-known generalized assignment problem answered in the combinatorial
optimization field. Based on this concept, we propose two heuristic search
algorithms for dynamically allocating subchannels to the competing QoS classes
and users which are executed under polynomially-bounded cost. We also propose
an Integer Linear Programming model for optimally solving and acquiring a
performance upper bound for the same problem at reasonable yet high execution
times. Through extensive simulation results we show that the proposed
algorithms exhibit high close-to-optimal performance, thus comprising
attractive candidates for implementation in modern OFDMA-based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3796</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3796</id><created>2012-11-15</created><authors><author><keyname>Phan</keyname><forenames>Anh Huy</forenames></author><author><keyname>Tichavsky</keyname><forenames>Petr</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>CANDECOMP/PARAFAC Decomposition of High-order Tensors Through Tensor
  Reshaping</title><categories>math.NA cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, algorithms for order-3 CANDECOMP/-PARAFAC (CP), also coined
canonical polyadic decomposition (CPD), are easily to implement and can be
extended to higher order CPD. Unfortunately, the algorithms become
computationally demanding, and they are often not applicable to higher order
and relatively large scale tensors. In this paper, by exploiting the uniqueness
of CPD and the relation of a tensor in Kruskal form and its unfolded tensor, we
propose a fast approach to deal with this problem. Instead of directly
factorizing the high order data tensor, the method decomposes an unfolded
tensor with lower order, e.g., order-3 tensor. On basis of the order-3
estimated tensor, a structured Kruskal tensor of the same dimension as the data
tensor is then generated, and decomposed to find the final solution using fast
algorithms for the structured CPD. In addition, strategies to unfold tensors
are suggested and practically verified in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3812</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3812</id><created>2012-11-16</created><authors><author><keyname>Chen</keyname><forenames>Li M.</forenames></author></authors><title>Determining the Number of Holes of a 2D Digital Component is Easy</title><categories>cs.CG cs.DM math.CO</categories><comments>7 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The number of holes in a connected component in 2D images is a basic
invariant. In this note, a simple formula was proven using our previous results
in digital topology (Chen 2004, Chen and Rong (2010). The new is: $h =1+
(|C_4|-|C_2|)/4$, where h is the number of holes, and $C_i$ indicate the set of
corner points having $i$ direct adjacent points in the component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3821</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3821</id><created>2012-11-16</created><authors><author><keyname>Soriano</keyname><forenames>Enrique Nadal</forenames><affiliation>DIMM</affiliation></author><author><keyname>Estrada</keyname><forenames>Octavio Andr&#xe9;s Gonz&#xe1;lez</forenames><affiliation>IMAM</affiliation></author><author><keyname>Garc&#xed;a</keyname><forenames>Juan Jos&#xe9; R&#xf3;denas</forenames><affiliation>DIMM</affiliation></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Francisco Javier Fuenmayor</forenames><affiliation>DIMM</affiliation></author></authors><title>Report: Error estimation of recovered solution in FE analysis</title><categories>cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recovery type error estimators introduced by Zienkiewicz and Zhu use a
recovered stress field evaluated from the Finite Element (FE) solution. Their
accuracy depends on the quality of the recovered field. In this sense, accurate
results are obtained using recovery procedures based on the Superconvergent
Patch recovery technique (SPR). These error estimators can be easily
implemented and provide accurate estimates. Another important feature is that
the recovered solution is of a better quality than the FE solution and can
therefore be used as an enhanced solution. We have developed an SPR-type
recovery technique that considers equilibrium and displacements constraints to
obtain a very accurate recovered displacements field from which a recovered
stress field can also be evaluated. We propose the use of these recovered
fields as the standard output of the FE code instead of the raw FE solution.
Techniques to quantify the error of the recovered solution are therefore
needed. In this report we present an error estimation technique that accurately
evaluates the error of the recovered solution both at global and local levels
in the FEM and XFEM frameworks. We have also developed an h-adaptive mesh
refinement strategy based on the error of the recovered solution. As the
converge rate of the error of the recovered solution is higher than that of the
FE one, the computational cost required to obtain a solution with a prescribed
accuracy is smaller than for traditional h-adaptive processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3823</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3823</id><created>2012-11-16</created><authors><author><keyname>Latu</keyname><forenames>Guillaume</forenames><affiliation>IRFM, INRIA Nancy - Grand Est / IECN / LSIIT / IRMA</affiliation></author><author><keyname>Becoulet</keyname><forenames>Marina</forenames><affiliation>IRFM</affiliation></author><author><keyname>Dif-Pradalier</keyname><forenames>Guilhem</forenames><affiliation>IRFM</affiliation></author><author><keyname>Grandgirard</keyname><forenames>Virginie</forenames><affiliation>IRFM</affiliation></author><author><keyname>Hoelzl</keyname><forenames>Matthias</forenames><affiliation>IPP Garching</affiliation></author><author><keyname>Huysmans</keyname><forenames>G.</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Lacoste</keyname><forenames>Xavier</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Nardon</keyname><forenames>Eric</forenames><affiliation>IRFM</affiliation></author><author><keyname>Orain</keyname><forenames>Francois</forenames><affiliation>IRFM</affiliation></author><author><keyname>Passeron</keyname><forenames>Chantal</forenames><affiliation>IRFM</affiliation></author><author><keyname>Ramet</keyname><forenames>Pierre</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Ratnani</keyname><forenames>Ahmed</forenames><affiliation>IRFM</affiliation></author></authors><title>Non regression testing for the JOREK code</title><categories>cs.DC math.AP</categories><comments>No. RR-8134 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non Regression Testing (NRT) aims to check if software modifications result
in undesired behaviour. Suppose the behaviour of the application previously
known, this kind of test makes it possible to identify an eventual regression,
a bug. Improving and tuning a parallel code can be a time-consuming and
difficult task, especially whenever people from different scientific fields
interact closely. The JOREK code aims at investing Magnetohydrodynamic (MHD)
instabilities in a Tokamak plasma. This paper describes the NRT procedure that
has been tuned for this simulation code. Automation of the NRT is one keypoint
to keeping the code healthy in a source code repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3828</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3828</id><created>2012-11-16</created><authors><author><keyname>Park</keyname><forenames>Hosung</forenames></author><author><keyname>Hong</keyname><forenames>Seokbeom</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>Construction of High-Rate Regular Quasi-Cyclic LDPC Codes Based on
  Cyclic Difference Families</title><categories>cs.IT math.IT</categories><comments>14 pages, submitted to IEEE Transactions on Communications on
  November 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a high-rate case, it is difficult to randomly construct good low-density
parity-check (LDPC) codes of short and moderate lengths because their Tanner
graphs are prone to making short cycles. Also, the existing high-rate
quasi-cyclic (QC) LDPC codes can be constructed only for very restricted code
parameters. In this paper, a new construction method of high-rate regular QC
LDPC codes with parity-check matrices consisting of a single row of circulants
with the column-weight 3 or 4 is proposed based on special classes of cyclic
difference families. The proposed QC LDPC codes can be constructed for various
code rates and lengths including the minimum achievable length for a given
design rate, which cannot be achieved by the existing high-rate QC LDPC codes.
It is observed that the parity-check matrices of the proposed QC LDPC codes
have full rank. It is shown that the error correcting performance of the
proposed QC LDPC codes of short and moderate lengths is almost the same as that
of the existing ones through numerical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3831</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3831</id><created>2012-11-16</created><updated>2013-03-07</updated><authors><author><keyname>Akimoto</keyname><forenames>Youhei</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Ollivier</keyname><forenames>Yann</forenames><affiliation>LRI</affiliation></author></authors><title>Objective Improvement in Information-Geometric Optimization</title><categories>cs.LG cs.AI math.OC stat.ML</categories><proxy>ccsd</proxy><journal-ref>Foundations of Genetic Algorithms XII (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-Geometric Optimization (IGO) is a unified framework of stochastic
algorithms for optimization problems. Given a family of probability
distributions, IGO turns the original optimization problem into a new
maximization problem on the parameter space of the probability distributions.
IGO updates the parameter of the probability distribution along the natural
gradient, taken with respect to the Fisher metric on the parameter manifold,
aiming at maximizing an adaptive transform of the objective function. IGO
recovers several known algorithms as particular instances: for the family of
Bernoulli distributions IGO recovers PBIL, for the family of Gaussian
distributions the pure rank-mu CMA-ES update is recovered, and for exponential
families in expectation parametrization the cross-entropy/ML method is
recovered. This article provides a theoretical justification for the IGO
framework, by proving that any step size not greater than 1 guarantees monotone
improvement over the course of optimization, in terms of q-quantile values of
the objective function f. The range of admissible step sizes is independent of
f and its domain. We extend the result to cover the case of different step
sizes for blocks of the parameters in the IGO algorithm. Moreover, we prove
that expected fitness improves over time when fitness-proportional selection is
applied, in which case the RPP algorithm is recovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3836</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3836</id><created>2012-11-16</created><authors><author><keyname>Qian</keyname><forenames>Jie</forenames></author><author><keyname>Qamar</keyname><forenames>Nafees</forenames></author></authors><title>An experimental evaluation of de-identification tools for electronic
  health records</title><categories>cs.CR</categories><comments>6 pages</comments><acm-class>J.3; D.4.6; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robust development of Electronic Health Records (EHRs) causes a
significant growth in sharing EHRs for clinical research. However, such a
sharing makes it difficult to protect patient's privacy. A number of automated
de-identification tools have been developed to reduce the re-identification
risk of published data, while preserving its statistical meaning. In this
paper, we focus on the experimental evaluation of existing automated
de-identification tools, as applied to our EHR database, to assess which tool
performs better with each quasi-identifiers defined in our paper. Performance
of each tool is analyzed wrt. two aspects: individual disclosure risk and
information loss. Through this experiment, the generalization method has better
performance on reducing risk and lower degree of information loss than
suppression, which validates it as more appropriate de-identification technique
for EHR databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3845</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3845</id><created>2012-11-16</created><authors><author><keyname>Andras</keyname><forenames>Peter</forenames></author></authors><title>A Bayesian Interpretation of the Particle Swarm Optimization and Its
  Kernel Extension</title><categories>cs.NE</categories><comments>12 pages</comments><acm-class>I.2.8; I.2.11; F.2.2</acm-class><journal-ref>PLoS ONE 7(11): e48710 (2012)</journal-ref><doi>10.1371/journal.pone.0048710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle swarm optimization is a popular method for solving difficult
optimization problems. There have been attempts to formulate the method in
formal probabilistic or stochastic terms (e.g. bare bones particle swarm) with
the aim to achieve more generality and explain the practical behavior of the
method. Here we present a Bayesian interpretation of the particle swarm
optimization. This interpretation provides a formal framework for incorporation
of prior knowledge about the problem that is being solved. Furthermore, it also
allows to extend the particle optimization method through the use of kernel
functions that represent the intermediary transformation of the data into a
different space where the optimization problem is expected to be easier to be
resolved, such transformation can be seen as a form of prior knowledge about
the nature of the optimization problem. We derive from the general Bayesian
formulation the commonly used particle swarm methods as particular cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3869</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3869</id><created>2012-11-16</created><authors><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author><author><keyname>Visentini-Scarzanella</keyname><forenames>Marco</forenames></author><author><keyname>Dragotti</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Tubaro</keyname><forenames>Stefano</forenames></author></authors><title>Transform coder identification based on quantization footprints and
  lattice theory</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transform coding is routinely used for lossy compression of discrete sources
with memory. The input signal is divided into N-dimensional vectors, which are
transformed by means of a linear mapping. Then, transform coefficients are
quantized and entropy coded. In this paper we consider the problem of
identifying the transform matrix as well as the quantization step sizes. We
study the challenging case in which the only available information is a set of
P transform decoded vectors. We formulate the problem in terms of finding the
lattice with the largest determinant that contains all observed vectors. We
propose an algorithm that is able to find the optimal solution and we formally
study its convergence properties. Our analysis shows that it is possible to
identify successfully both the transform and the quantization step sizes when P
&gt;= N + d where d is a small integer, and the probability of failure decreases
exponentially to zero as P - N increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3871</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3871</id><created>2012-11-16</created><authors><author><keyname>Padhy</keyname><forenames>Neelamadhab</forenames></author><author><keyname>Panigrahi</keyname><forenames>Rasmita</forenames></author></authors><title>Multi Relational Data Mining Approaches: A Data Mining Technique</title><categories>cs.DB</categories><comments>10 pages, 1 Figure, 3 Tables &quot;Published with International Journal of
  Computer Applications (IJCA)&quot;</comments><doi>10.5120/9207-3742</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The multi relational data mining approach has developed as an alternative way
for handling the structured data such that RDBMS. This will provides the mining
in multiple tables directly. In MRDM the patterns are available in multiple
tables (relations) from a relational database. As the data are available over
the many tables which will affect the many problems in the practice of the data
mining. To deal with this problem, one either constructs a single table by
Propositionalisation, or uses a Multi-Relational Data Mining algorithm. MRDM
approaches have been successfully applied in the area of bioinformatics. Three
popular pattern finding techniques classification, clustering and association
are frequently used in MRDM. Multi relational approach has developed as an
alternative for analyzing the structured data such as relational database. MRDM
allowing applying directly in the data mining in multiple tables. To avoid the
expensive joining operations and semantic losses we used the MRDM technique.
This paper focuses some of the application areas of MRDM and feature directions
as well as the comparison of ILP, GM, SSDM and MRDM
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3881</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3881</id><created>2012-11-16</created><authors><author><keyname>Krivulin</keyname><forenames>N. K.</forenames></author></authors><title>Unbiased gradient estimation in queueing networks with
  parameter-dependent routing</title><categories>math.OC cs.NI</categories><comments>International Conference on Control and Information 1995, The
  Institute of Mathematical Sciences, The Chinese University of Hong Kong, Hong
  Kong, June 5-9, 1995</comments><msc-class>62F10 (Primary) 68M20, 90B15, 93C65, 93C73 (Secondary)</msc-class><journal-ref>Proc. Intern. Conf. on Control and Information 1995 (1995), pp.
  351-356</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic queueing network model with parameter-dependent service times
and routing mechanism, and its related performance measures are considered. An
estimate of performance measure gradient is proposed, and rather general
sufficient conditions for the estimate to be unbiased are given. A gradient
estimation algorithm is also presented, and its validity is briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3882</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3882</id><created>2012-11-16</created><updated>2012-11-20</updated><authors><author><keyname>Moore</keyname><forenames>Edward</forenames></author><author><keyname>Obst</keyname><forenames>Oliver</forenames></author><author><keyname>Prokopenko</keyname><forenames>Mikhail</forenames></author><author><keyname>Wang</keyname><forenames>Peter</forenames></author><author><keyname>Held</keyname><forenames>Jason</forenames></author></authors><title>Gliders2012: Development and Competition Results</title><categories>cs.AI cs.MA cs.RO</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The RoboCup 2D Simulation League incorporates several challenging features,
setting a benchmark for Artificial Intelligence (AI). In this paper we describe
some of the ideas and tools around the development of our team, Gliders2012. In
our description, we focus on the evaluation function as one of our central
mechanisms for action selection. We also point to a new framework for watching
log files in a web browser that we release for use and further development by
the RoboCup community. Finally, we also summarize results of the group and
final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out
of 19 teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3886</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3886</id><created>2012-11-16</created><authors><author><keyname>Molu</keyname><forenames>Mehdi</forenames></author><author><keyname>Goertz</keyname><forenames>Norbert</forenames></author></authors><title>Maximum Eigenmode Relaying with statistical Channel State Information at
  the Relay</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal precoding in the relay is investigated to maximize ergodic capacity
of a multiple antenna relay channel. The source and the relay nodes are
equipped with multiple antennas and the destination with a single antenna. It
is assumed that the channel covariance matrices of the relay's receive and
transmit channels are available to the relay, and optimal precoding at the
relay is investigated. It is shown that the optimal transmission from the relay
should be conducted in the direction of the eigenvectors of the
transmit-channel covariance matrix. Then, we derive the necessary and
sufficient conditions under which the relay transmission only from the
strongest eigenvector achieves capacity; this method is called Maximum
Eigenmode Relaying (MER).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3901</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3901</id><created>2012-11-16</created><authors><author><keyname>Akram</keyname><forenames>Saad</forenames></author><author><keyname>Beskow</keyname><forenames>Jonas</forenames></author><author><keyname>Kjellstrom</keyname><forenames>Hedvig</forenames></author></authors><title>Visual Recognition of Isolated Swedish Sign Language Signs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for recognition of isolated Swedish Sign Language signs.
The method will be used in a game intended to help children training signing at
home, as a complement to training with a teacher. The target group is not
primarily deaf children, but children with language disorders. Using sign
language as a support in conversation has been shown to greatly stimulate the
speech development of such children. The signer is captured with an RGB-D
(Kinect) sensor, which has three advantages over a regular RGB camera. Firstly,
it allows complex backgrounds to be removed easily. We segment the hands and
face based on skin color and depth information. Secondly, it helps with the
resolution of hand over face occlusion. Thirdly, signs take place in 3D; some
aspects of the signs are defined by hand motion vertically to the image plane.
This motion can be estimated if the depth is observable. The 3D motion of the
hands relative to the torso are used as a cue together with the hand shape, and
HMMs trained with this input are used for classification. To obtain higher
robustness towards differences across signers, Fisher Linear Discriminant
Analysis is used to find the combinations of features that are most descriptive
for each sign, regardless of signer. Experiments show that the system can
distinguish signs from a challenging 94 word vocabulary with a precision of up
to 94% in the signer dependent case and up to 47% in the signer independent
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3908</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3908</id><created>2012-11-16</created><authors><author><keyname>Ma</keyname><forenames>Zhendong</forenames></author><author><keyname>Smith</keyname><forenames>Paul</forenames></author><author><keyname>Skopik</keyname><forenames>Florian</forenames></author></authors><title>Towards a Layered Architectural View for Security Analysis in SCADA
  Systems</title><categories>cs.CR</categories><comments>7 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Supervisory Control and Data Acquisition (SCADA) systems support and control
the operation of many critical infrastructures that our society depend on, such
as power grids. Since SCADA systems become a target for cyber attacks and the
potential impact of a successful attack could lead to disastrous consequences
in the physical world, ensuring the security of these systems is of vital
importance. A fundamental prerequisite to securing a SCADA system is a clear
understanding and a consistent view of its architecture. However, because of
the complexity and scale of SCADA systems, this is challenging to acquire. In
this paper, we propose a layered architectural view for SCADA systems, which
aims at building a common ground among stakeholders and supporting the
implementation of security analysis. In order to manage the complexity and
scale, we define four interrelated architectural layers, and uses the concept
of viewpoints to focus on a subset of the system. We indicate the applicability
of our approach in the context of SCADA system security analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3929</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3929</id><created>2012-11-16</created><updated>2013-12-23</updated><authors><author><keyname>Munemasa</keyname><forenames>Akihiro</forenames></author><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author><author><keyname>Taniguchi</keyname><forenames>Tetsuji</forenames></author></authors><title>Fat Hoffman graphs with smallest eigenvalue greater than -3</title><categories>math.CO cs.DM</categories><comments>21+5 pages</comments><msc-class>05C50, 05C75</msc-class><journal-ref>Discrete Applied Mathematics 176 (2014) 78-88</journal-ref><doi>10.1016/j.dam.2014.01.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a combinatorial characterization of the special graphs
of fat Hoffman graphs containing $\mathfrak{K}_{1,2}$ with smallest eigenvalue
greater than -3, where $\mathfrak{K}_{1,2}$ is the Hoffman graph having one
slim vertex and two fat vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3934</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3934</id><created>2012-11-16</created><authors><author><keyname>Qin</keyname><forenames>Shao-Meng</forenames></author><author><keyname>Verkasalo</keyname><forenames>Hannu</forenames></author><author><keyname>Mohtaschemi</keyname><forenames>Mikael</forenames></author><author><keyname>Hartonen</keyname><forenames>Tuomo</forenames></author><author><keyname>Alava</keyname><forenames>Mikko</forenames></author></authors><title>Patterns, entropy, and predictability of human mobility and life</title><categories>physics.soc-ph cs.SI</categories><comments>5 figures, for data see smartphonedata.aalto.fi, accepted to PLoS One</comments><doi>10.1371/journal.pone.0051353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular phones are now offering an ubiquitous means for scientists to
observe life: how people act, move and respond to external influences. They can
be utilized as measurement devices of individual persons and for groups of
people of the social context and the related interactions. The picture of human
life that emerges shows complexity, which is manifested in such data in
properties of the spatiotemporal tracks of individuals. We extract from
smartphone-based data for a set of persons important locations such as &quot;home&quot;,
&quot;work&quot; and so forth over fixed length time-slots covering the days in the
data-set. This set of typical places is heavy-tailed, a power-law distribution
with an exponent close to -1.7. To analyze the regularities and stochastic
features present, the days are classified for each person into regular,
personal patterns. To this are superimposed fluctuations for each day. This
randomness is measured by &quot;life&quot; entropy, computed both before and after
finding the clustering so as to subtract the contribution of a number of
patterns. The main issue, that we then address, is how predictable individuals
are in their mobility. The patterns and entropy are reflected in the
predictability of the mobility of the life both individually and on average. We
explore the simple approaches to guess the location from the typical behavior,
and of exploiting the transition probabilities with time from location or
activity A to B. The patterns allow an enhanced predictability, at least up to
a few hours into the future from the current location. Such fixed habits are
most clearly visible in the working-day length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3951</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3951</id><created>2012-11-16</created><updated>2014-01-19</updated><authors><author><keyname>Joseph</keyname><forenames>Andreas</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author></authors><title>Composite Centrality: A Natural Scale for Complex Evolving Networks</title><categories>stat.ME cs.SI physics.data-an physics.soc-ph</categories><comments>11 pages, 5 figures, 4 tables</comments><journal-ref>Physica D, vol. 267, p. 58-67, 2014</journal-ref><doi>10.1016/j.physd.2013.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a composite centrality measure for general weighted and directed
complex networks, based on measure standardisation and invariant statistical
inheritance schemes. Different schemes generate different intermediate abstract
measures providing additional information, while the composite centrality
measure tends to the standard normal distribution. This offers a unified scale
to measure node and edge centralities for complex evolving networks under a
uniform framework. Considering two real-world cases of the world trade web and
the world migration web, both during a time span of 40 years, we propose a
standard set-up to demonstrate its remarkable normative power and accuracy. We
illustrate the applicability of the proposed framework for large and arbitrary
complex systems, as well as its limitations, through extensive numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3955</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3955</id><created>2012-11-16</created><authors><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author><author><keyname>Muralidharan</keyname><forenames>Omkar</forenames></author></authors><title>On Calibrated Predictions for Auction Selection Mechanisms</title><categories>cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calibration is a basic property for prediction systems, and algorithms for
achieving it are well-studied in both statistics and machine learning. In many
applications, however, the predictions are used to make decisions that select
which observations are made. This makes calibration difficult, as adjusting
predictions to achieve calibration changes future data. We focus on
click-through-rate (CTR) prediction for search ad auctions. Here, CTR
predictions are used by an auction that determines which ads are shown, and we
want to maximize the value generated by the auction.
  We show that certain natural notions of calibration can be impossible to
achieve, depending on the details of the auction. We also show that it can be
impossible to maximize auction efficiency while using calibrated predictions.
Finally, we give conditions under which calibration is achievable and
simultaneously maximizes auction efficiency: roughly speaking, bids and queries
must not contain information about CTRs that is not already captured by the
predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3959</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3959</id><created>2012-11-16</created><authors><author><keyname>Metelitsyn</keyname><forenames>Pavel</forenames></author></authors><title>How to compute the constant term of a power of a Laurent polynomial
  efficiently</title><categories>cs.SC math.AG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for efficient computation of the constant term of a
power of a multivariate Laurent polynomial. The algorithm is based on
univariate interpolation, does not require the storage of intermediate data and
can be easily parallelized. As an application we compute the power series
expansion of the principal period of some toric Calabi-Yau varieties and find
previously unknown differential operators of Calabi-Yau type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3966</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3966</id><created>2012-11-16</created><updated>2014-10-15</updated><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Lasso Screening Rules via Dual Polytope Projection</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Lasso is a widely used regression technique to find sparse representations.
When the dimension of the feature space and the number of samples are extremely
large, solving the Lasso problem remains challenging. To improve the efficiency
of solving large-scale Lasso problems, El Ghaoui and his colleagues have
proposed the SAFE rules which are able to quickly identify the inactive
predictors, i.e., predictors that have $0$ components in the solution vector.
Then, the inactive predictors or features can be removed from the optimization
problem to reduce its scale. By transforming the standard Lasso to its dual
form, it can be shown that the inactive predictors include the set of inactive
constraints on the optimal dual solution. In this paper, we propose an
efficient and effective screening rule via Dual Polytope Projections (DPP),
which is mainly based on the uniqueness and nonexpansiveness of the optimal
dual solution due to the fact that the feasible set in the dual space is a
convex and closed polytope. Moreover, we show that our screening rule can be
extended to identify inactive groups in group Lasso. To the best of our
knowledge, there is currently no &quot;exact&quot; screening rule for group Lasso. We
have evaluated our screening rule using synthetic and real data sets. Results
show that our rule is more effective in identifying inactive predictors than
existing state-of-the-art screening rules for Lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3979</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3979</id><created>2012-11-11</created><authors><author><keyname>Firdhous</keyname><forenames>Mohamed</forenames></author><author><keyname>Ghazali</keyname><forenames>Osman</forenames></author><author><keyname>Hassan</keyname><forenames>Suhaidi</forenames></author></authors><title>Trust Management in Cloud Computing: A Critical Review</title><categories>cs.DC cs.CR</categories><comments>13 pages, 1 figure, 1 table, 61 references</comments><journal-ref>Publication in the International Journal on Advances in ICT for
  Emerging Regions (ICTer), vol. 04, no. 02, 2011, pp. 24-36</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has been attracting the attention of several researchers both
in the academia and the industry as it provides many opportunities for
organizations by offering a range of computing services. For cloud computing to
become widely adopted by both the enterprises and individuals, several issues
have to be solved. A key issue that needs special attention is security of
clouds, and trust management is an important component of cloud security. In
this paper, the authors look at what trust is and how trust has been applied in
distributed computing. Trust models proposed for various distributed system has
then been summarized. The trust management systems proposed for cloud computing
have been investigated with special emphasis on their capability, applicability
in practical heterogonous cloud environment and implementabilty. Finally, the
proposed models/systems have been compared with each other based on a selected
set of cloud computing parameters in a table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.3981</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.3981</id><created>2012-11-16</created><authors><author><keyname>Borodin</keyname><forenames>Oleg V.</forenames></author><author><keyname>Kostochka</keyname><forenames>Alexandr V.</forenames></author><author><keyname>Lidick&#xfd;</keyname><forenames>Bernard</forenames></author><author><keyname>Yancey</keyname><forenames>Matthew</forenames></author></authors><title>Short proofs of coloring theorems on planar graphs</title><categories>math.CO cs.DM</categories><comments>13 pages, 4 figures</comments><msc-class>05C15, 05C10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent lower bound on the number of edges in a k-critical n-vertex graph by
Kostochka and Yancey yields a half-page proof of the celebrated Gr\&quot;otzsch
Theorem that every planar triangle-free graph is 3-colorable. In this paper we
use the same bound to give short proofs of other known theorems on 3-coloring
of planar graphs, among whose is the Gr\&quot;unbaum-Aksenov Theorem that every
planar with at most three triangles is 3-colorable. We also prove the new
result that every graph obtained from a triangle-free planar graph by adding a
vertex of degree at most four is 3-colorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4000</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4000</id><created>2012-11-16</created><authors><author><keyname>Szalkowski</keyname><forenames>Greg</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>The Performance of Betting Lines for Predicting the Outcome of NFL Games</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We investigated the performance of the collective intelligence of NFL fans
predicting the outcome of games as realized through the Vegas betting lines.
Using data from 2560 games (all post-expansion, regular- and post-season games
from 2002-2011), we investigated the opening and closing lines, and the margin
of victory. We found that the line difference (the difference between the
opening and closing line) could be used to retroactively predict divisional
winners with no less accuracy than 75% accuracy (i.e., &quot;straight up&quot;
predictions). We also found that although home teams only beat the spread 47%
of the time, a strategy of betting the home team underdogs (from 2002-2011)
would have produced a cumulative winning strategy of 53.5%, above the threshold
of 52.38% needed to break even.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4014</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4014</id><created>2012-11-16</created><authors><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Pulikkoonattu</keyname><forenames>Rethnakaran</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Intermediate Performance Analysis of Growth Codes</title><categories>cs.IT cs.MM cs.NI math.IT</categories><comments>submitted to Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Growth codes are a subclass of Rateless codes that have found interesting
applications in data dissemination problems. Compared to other Rateless and
conventional channel codes, Growth codes show improved intermediate performance
which is particularly useful in applications where performance increases with
the number of decoded data units. In this paper, we provide a generic
analytical framework for studying the asymptotic performance of Growth codes in
different settings. Our analysis based on Wormald method applies to any class
of Rateless codes that does not include a precoding step. We evaluate the
decoding probability model for short codeblocks and validate our findings by
experiments. We then exploit the decoding probability model in an illustrative
application of Growth codes to error resilient video transmission. The video
transmission problem is cast as a joint source and channel rate allocation
problem that is shown to be convex with respect to the channel rate. This
application permits to highlight the main advantage of Growth codes that is
improved performance (hence distortion in video) in the intermediate loss
region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4038</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4038</id><created>2012-11-16</created><authors><author><keyname>Shah</keyname><forenames>Shridhar K.</forenames></author><author><keyname>Tanner</keyname><forenames>Herbert G.</forenames></author><author><keyname>Pahlajani</keyname><forenames>Chetan D.</forenames></author></authors><title>Stochastic receding horizon control of nonlinear stochastic systems with
  probabilistic state constraints</title><categories>cs.SY cs.RO math.OC</categories><comments>Draft of submission to IEEE Transactions of Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes a receding horizon control design framework for
continuous-time stochastic nonlinear systems subject to probabilistic state
constraints. The intention is to derive solutions that are implementable in
real-time on currently available mobile processors. The approach consists of
decomposing the problem into designing receding horizon reference paths based
on the drift component of the system dynamics, and then implementing a
stochastic optimal controller to allow the system to stay close and follow the
reference path. In some cases, the stochastic optimal controller can be
obtained in closed form; in more general cases, pre-computed numerical
solutions can be implemented in real-time without the need for on-line
computation. The convergence of the closed loop system is established assuming
no constraints on control inputs, and simulation results are provided to
corroborate the theoretical predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4041</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4041</id><created>2012-11-16</created><updated>2013-06-22</updated><authors><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author></authors><title>Modeling, Analysis and Design for Carrier Aggregation in Heterogeneous
  Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>submitted to IEEE Transactions on Communications, Nov. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carrier aggregation (CA) and small cells are two distinct features of
next-generation cellular networks. Cellular networks with small cells take on a
very heterogeneous characteristic, and are often referred to as HetNets. In
this paper, we introduce a load-aware model for CA-enabled \textit{multi}-band
HetNets. Under this model, the impact of biasing can be more appropriately
characterized; for example, it is observed that with large enough biasing, the
spectral efficiency of small cells may increase while its counterpart in a
fully-loaded model always decreases. Further, our analysis reveals that the
peak data rate does not depend on the base station density and transmit powers;
this strongly motivates other approaches e.g. CA to increase the peak data
rate. Last but not least, different band deployment configurations are studied
and compared. We find that with large enough small cell density, spatial reuse
with small cells outperforms adding more spectrum for increasing user rate.
More generally, universal cochannel deployment typically yields the largest
rate; and thus a capacity loss exists in orthogonal deployment. This
performance gap can be reduced by appropriately tuning the HetNet coverage
distribution (e.g. by optimizing biasing factors).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4047</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4047</id><created>2012-11-16</created><updated>2013-04-25</updated><authors><author><keyname>Alnaes</keyname><forenames>Martin S.</forenames></author><author><keyname>Logg</keyname><forenames>Anders</forenames></author><author><keyname>Oelgaard</keyname><forenames>Kristian B.</forenames></author><author><keyname>Rognes</keyname><forenames>Marie E.</forenames></author><author><keyname>Wells</keyname><forenames>Garth N.</forenames></author></authors><title>Unified Form Language: A domain-specific language for weak formulations
  of partial differential equations</title><categories>cs.MS cs.NA cs.SC</categories><comments>To appear in ACM Transactions on Mathematical Software</comments><msc-class>97N80</msc-class><acm-class>G.4; G.1.8; G.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Unified Form Language (UFL), which is a domain-specific
language for representing weak formulations of partial differential equations
with a view to numerical approximation. Features of UFL include support for
variational forms and functionals, automatic differentiation of forms and
expressions, arbitrary function space hierarchies for multi-field problems,
general differential operators and flexible tensor algebra. With these
features, UFL has been used to effortlessly express finite element methods for
complex systems of partial differential equations in near-mathematical
notation, resulting in compact, intuitive and readable programs. We present in
this work the language and its construction. An implementation of UFL is freely
available as an open-source software library. The library generates abstract
syntax tree representations of variational problems, which are used by other
software libraries to generate concrete low-level implementations. Some
application examples are presented and libraries that support UFL are
highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4049</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4049</id><created>2012-11-16</created><authors><author><keyname>Alc&#xf3;n</keyname><forenames>Liliana</forenames></author><author><keyname>Gutierrez</keyname><forenames>Marisa</forenames></author><author><keyname>Hurlbert</keyname><forenames>Glenn</forenames></author></authors><title>Pebbling in Split Graphs</title><categories>math.CO cs.DM</categories><comments>33 pages, 3 figures</comments><msc-class>05C85 (primary), 68Q17, 90C35 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph pebbling is a network optimization model for transporting discrete
resources that are consumed in transit: the movement of two pebbles across an
edge consumes one of the pebbles. The pebbling number of a graph is the fewest
number of pebbles t so that, from any initial configuration of t pebbles on its
vertices, one can place a pebble on any given target vertex via such pebbling
steps. It is known that deciding if a given configuration on a particular graph
can reach a specified target is NP-complete, even for diameter two graphs, and
that deciding if the pebbling number has a prescribed upper bound is
\Pi_2^P-complete.
  On the other hand, for many families of graphs there are formulas or
polynomial algorithms for computing pebbling numbers; for example, complete
graphs, products of paths (including cubes), trees, cycles, diameter two
graphs, and more. Moreover, graphs having minimum pebbling number are called
Class 0, and many authors have studied which graphs are Class 0 and what graph
properties guarantee it, with no characterization in sight.
  In this paper we investigate an important family of diameter three chordal
graphs called split graphs; graphs whose vertex set can be partitioned into a
clique and an independent set. We provide a formula for the pebbling number of
a split graph, along with an algorithm for calculating it that runs in
O(n^\beta) time, where \beta=2\omega/(\omega+1)\cong 1.41 and \omega\cong 2.376
is the exponent of matrix multiplication. Furthermore we determine that all
split graphs with minimum degree at least 3 are Class 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4053</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4053</id><created>2012-11-16</created><updated>2013-06-17</updated><authors><author><keyname>Khalil</keyname><forenames>Karim</forenames></author><author><keyname>Ekici</keyname><forenames>Eylem</forenames></author></authors><title>Spectrum Access through Threats in Cognitive Radio Networks</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multiple access games in which primary users are interested in
maximizing their confidential data rate at the minimum possible transmission
power and secondary users employ eavesdropping as a leverage to maximize their
data rate to a common destination at minimum transmission energy. For the
non-cooperative static game, Nash equilibria in pure and mixed strategies are
derived and shown to be Pareto inefficient in general, when channel gains are
common knowledge. For the two-player Stackelberg game where the primary user is
the leader, it is shown that the secondary user is forced to play as the
follower where the Stackelberg equilibrium dominates the Nash equilibrium, even
if the eavesdropper channel is better than the primary channel. Here, the
utility achieved by the Stackelberg game Pareto-dominates the achieved Nash
utility. Moreover, we study the unknown eavesdropper channel case numerically
where the primary user has only statistical knowledge about the channel gain.
We compare the results to the first scenario and show that it is not always
beneficial for the cognitive user to hide the actual eavesdropper channel gain.
Finally, we extend the equilibrium analysis to a multiple SU game where the
primary system selects a subset of the secondary users to transmit such that
the performance of the primary users is maximized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4055</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4055</id><created>2012-11-16</created><authors><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>Brinkworth</keyname><forenames>Carolyn</forenames></author><author><keyname>Gelino</keyname><forenames>Dawn</forenames></author><author><keyname>Wittman</keyname><forenames>Dennis K.</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Juve</keyname><forenames>Gideon</forenames></author><author><keyname>Rynge</keyname><forenames>Mats</forenames></author><author><keyname>Kinney</keyname><forenames>Jamie</forenames></author></authors><title>A Tale Of 160 Scientists, Three Applications, A Workshop and A Cloud</title><categories>astro-ph.IM cs.DC</categories><comments>4 pages, 1 figure, 1 table. Submitted to Astronomical Data Analysis
  Software and Systems XXII</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NASA Exoplanet Science Institute (NExScI) hosts the annual Sagan
Workshops, thematic meetings aimed at introducing researchers to the latest
tools and methodologies in exoplanet research. The theme of the Summer 2012
workshop, held from July 23 to July 27 at Caltech, was to explore the use of
exoplanet light curves to study planetary system architectures and atmospheres.
A major part of the workshop was to use hands-on sessions to instruct attendees
in the use of three open source tools for the analysis of light curves,
especially from the Kepler mission. Each hands-on session involved the 160
attendees using their laptops to follow step-by-step tutorials given by
experts. We describe how we used the Amazon Elastic Cloud 2 to run these
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4056</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4056</id><created>2012-11-16</created><authors><author><keyname>Cullina</keyname><forenames>Daniel</forenames></author><author><keyname>Kulkarni</keyname><forenames>Ankur A.</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Two Approaches to the Construction of Deletion Correcting Codes: Weight
  Partitioning and Optimal Colorings</title><categories>cs.IT cs.DM math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing deletion correcting codes over a
binary alphabet and take a graph theoretic view. An $n$-bit $s$-deletion
correcting code is an independent set in a particular graph. We propose
constructing such a code by taking the union of many constant Hamming weight
codes. This results in codes that have additional structure. Searching for
codes in constant Hamming weight induced subgraphs is computationally easier
than searching the original graph. We prove a lower bound on size of a codebook
constructed this way for any number of deletions and show that it is only a
small factor below the corresponding lower bound on unrestricted codes. In the
single deletion case, we find optimal colorings of the constant Hamming weight
induced subgraphs. We show that the resulting code is asymptotically optimal.
We discuss the relationship between codes and colorings and observe that the VT
codes are optimal in a coloring sense. We prove a new lower bound on the
chromatic number of the deletion channel graphs. Colorings of the deletion
channel graphs that match this bound do not necessarily produce asymptotically
optimal codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4077</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4077</id><created>2012-11-17</created><updated>2013-07-16</updated><authors><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author><author><keyname>Vincent</keyname><forenames>Tyrone L.</forenames></author></authors><title>Technical Report: Observability with Random Observations</title><categories>cs.SY</categories><comments>A companion technical report which contains additional details on
  several topics from our revised submission &quot;Observability with Random
  Observations&quot; to the IEEE Transactions on Automatic Control, Special Issue on
  Relaxation Methods in Identification and Estimation Problems, on July 16,
  2013 - Initial submission on November 16, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovery of the initial state of a high-dimensional system can require a
large number of measurements. In this paper, we explain how this burden can be
significantly reduced when randomized measurement operators are employed. Our
work builds upon recent results from Compressive Sensing (CS). In particular,
we make the connection to CS analysis for random block diagonal matrices. By
deriving Concentration of Measure (CoM) inequalities, we show that the
observability matrix satisfies the Restricted Isometry Property (RIP) (a
sufficient condition for stable recovery of sparse vectors) under certain
conditions on the state transition matrix. For example, we show that if the
state transition matrix is unitary, and if independent, randomly-populated
measurement matrices are employed, then it is possible to uniquely recover a
sparse high-dimensional initial state when the total number of measurements
scales linearly in the sparsity level (the number of non-zero entries) of the
initial state and logarithmically in the state dimension. We further extend our
RIP analysis for scaled unitary and symmetric state transition matrices. We
support our analysis with a case study of a two-dimensional diffusion process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4081</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4081</id><created>2012-11-17</created><authors><author><keyname>Dikaliotis</keyname><forenames>Theodoros K.</forenames></author><author><keyname>Yao</keyname><forenames>Hongyi</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Network Equivalence in the Presence of an Eavesdropper</title><categories>cs.IT math.IT</categories><comments>8 pages, presented at the 50th Annual Allerton Conference on
  Communication, Control and Computing 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider networks of noisy degraded wiretap channels in the presence of an
eavesdropper. For the case where the eavesdropper can wiretap at most one
channel at a time, we show that the secrecy capacity region, for a broad class
of channels and any given network topology and communication demands, is
equivalent to that of a corresponding network where each noisy wiretap channel
is replaced by a noiseless wiretap channel. Thus in this case there is a
separation between wiretap channel coding on each channel and secure network
coding on the resulting noiseless network. We show with an example that such
separation does not hold when the eavesdropper can access multiple channels at
the same time, for which case we provide upper and lower bounding noiseless
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4090</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4090</id><created>2012-11-17</created><authors><author><keyname>Kleijn</keyname><forenames>Jetty</forenames></author><author><keyname>Koutny</keyname><forenames>Maciej</forenames></author><author><keyname>Pietkiewicz-Koutny</keyname><forenames>Marta</forenames></author><author><keyname>Rozenberg</keyname><forenames>Grzegorz</forenames></author></authors><title>Membrane Systems and Petri Net Synthesis</title><categories>cs.LO cs.ET</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 100, 2012, pp. 1-13</journal-ref><doi>10.4204/EPTCS.100.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated synthesis from behavioural specifications is an attractive and
powerful way of constructing concurrent systems. Here we focus on the problem
of synthesising a membrane system from a behavioural specification given in the
form of a transition system which specifies the desired state space of the
system to be constructed. We demonstrate how a Petri net solution to this
problem, based on the notion of region of a transition system, yields a method
of automated synthesis of membrane systems from state spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4091</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4091</id><created>2012-11-17</created><authors><author><keyname>Antonaki</keyname><forenames>Margarita</forenames><affiliation>University of Cyprus</affiliation></author><author><keyname>Philippou</keyname><forenames>Anna</forenames><affiliation>University of Cyprus</affiliation></author></authors><title>A Process Calculus for Spatially-explicit Ecological Models</title><categories>cs.LO q-bio.PE</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 100, 2012, pp. 14-28</journal-ref><doi>10.4204/EPTCS.100.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose PALPS, a Process Algebra with Locations for Population Systems.
PALPS allows us to produce spatially-explicit, individual-based models and to
reason about their behavior. Our calculus has two levels: at the first level we
may define the behavior of an individual of a population while, at the second
level, we may specify a system as the collection of individuals of various
species located in space, moving through their life cycle while changing their
location, if they so wish, and interacting with each other in various ways such
as preying on each other. Furthermore, we propose a probabilistic temporal
logic for reasoning about the behavior of PALPS processes. We illustrate our
framework via models of dispersal in metapopulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4092</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4092</id><created>2012-11-17</created><authors><author><keyname>de Vink</keyname><forenames>E. P.</forenames><affiliation>Technische Universiteit Eindhoven and Centrum Wiskunde Informatica</affiliation></author><author><keyname>Zantema</keyname><forenames>H.</forenames><affiliation>Technische Universiteit Eindhoven and Radboud University Nijmegen</affiliation></author><author><keyname>Bo&#x161;na&#x10d;ki</keyname><forenames>D.</forenames><affiliation>Technische Universiteit Eindhoven</affiliation></author></authors><title>Combining Insertion and Deletion in RNA-editing Preserves Regularity</title><categories>cs.FL</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><acm-class>F.1.1; J.3</acm-class><journal-ref>EPTCS 100, 2012, pp. 48-62</journal-ref><doi>10.4204/EPTCS.100.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by RNA-editing as occurs in transcriptional processes in the living
cell, we introduce an abstract notion of string adjustment, called guided
rewriting. This formalism allows simultaneously inserting and deleting
elements. We prove that guided rewriting preserves regularity: for every
regular language its closure under guided rewriting is regular too. This
contrasts an earlier abstraction of RNA-editing separating insertion and
deletion for which it was proved that regularity is not preserved. The
particular automaton construction here relies on an auxiliary notion of slice
sequence which enables to sweep from left to right through a completed rewrite
sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4093</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4093</id><created>2012-11-17</created><authors><author><keyname>Dr&#xe1;bik</keyname><forenames>Peter</forenames><affiliation>IIT-CNR</affiliation></author><author><keyname>Maggiolo-Schettini</keyname><forenames>Andrea</forenames><affiliation>University of Pisa</affiliation></author><author><keyname>Milazzo</keyname><forenames>Paolo</forenames><affiliation>University of Pisa</affiliation></author></authors><title>Towards modular verification of pathways: fairness and assumptions</title><categories>cs.LO</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 100, 2012, pp. 63-81</journal-ref><doi>10.4204/EPTCS.100.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modular verification is a technique used to face the state explosion problem
often encountered in the verification of properties of complex systems such as
concurrent interactive systems. The modular approach is based on the
observation that properties of interest often concern a rather small portion of
the system. As a consequence, reduced models can be constructed which
approximate the overall system behaviour thus allowing more efficient
verification.
  Biochemical pathways can be seen as complex concurrent interactive systems.
Consequently, verification of their properties is often computationally very
expensive and could take advantage of the modular approach.
  In this paper we report preliminary results on the development of a modular
verification framework for biochemical pathways. We view biochemical pathways
as concurrent systems of reactions competing for molecular resources. A modular
verification technique could be based on reduced models containing only
reactions involving molecular resources of interest.
  For a proper description of the system behaviour we argue that it is
essential to consider a suitable notion of fairness, which is a
well-established notion in concurrency theory but novel in the field of pathway
modelling. We propose a modelling approach that includes fairness and we
identify the assumptions under which verification of properties can be done in
a modular way.
  We prove the correctness of the approach and demonstrate it on the model of
the EGF receptor-induced MAP kinase cascade by Schoeberl et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4094</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4094</id><created>2012-11-17</created><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames><affiliation>Department of Mathematics and Computer Science, University of Udine, Italy</affiliation></author><author><keyname>Sambarino</keyname><forenames>Ilaria</forenames><affiliation>Department of Mathematics and Computer Science, University of Udine, Italy</affiliation></author></authors><title>Implementing the Stochastics Brane Calculus in a Generic Stochastic
  Abstract Machine</title><categories>cs.CE cs.LO</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><acm-class>F.1.2; F.4.2; G.3; J.3</acm-class><journal-ref>EPTCS 100, 2012, pp. 82-100</journal-ref><doi>10.4204/EPTCS.100.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the problem of implementing an abstract machine
for a stochastic version of the Brane Calculus. Instead of defining an ad hoc
abstract machine, we consider the generic stochastic abstract machine
introduced by Lakin, Paulev\'e and Phillips. The nested structure of membranes
is flattened into a set of species where the hierarchical structure is
represented by means of names. In order to reduce the overhead introduced by
this encoding, we modify the machine by adding a copy-on-write optimization
strategy. We prove that this implementation is adequate with respect to the
stochastic structural operational semantics recently given for the Brane
Calculus. These techniques can be ported also to other stochastic calculi
dealing with nested structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4095</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4095</id><created>2012-11-17</created><authors><author><keyname>Hamano</keyname><forenames>Masahiro</forenames><affiliation>PRESTO, Japan Science and Technology Agency</affiliation></author></authors><title>RNA interference and Register Machines (extended abstract)</title><categories>cs.LO cs.CE q-bio.MN</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 100, 2012, pp. 107-112</journal-ref><doi>10.4204/EPTCS.100.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RNA interference (RNAi) is a mechanism whereby small RNAs (siRNAs) directly
control gene expression without assistance from proteins. This mechanism
consists of interactions between RNAs and small RNAs both of which may be
single or double stranded. The target of the mechanism is mRNA to be degraded
or aberrated, while the initiator is double stranded RNA (dsRNA) to be cleaved
into siRNAs. Observing the digital nature of RNAi, we represent RNAi as a
Minsky register machine such that (i) The two registers hold single and double
stranded RNAs respectively, and (ii) Machine's instructions are interpreted by
interactions of enzyme (Dicer), siRNA (with RISC com- plex) and polymerization
(RdRp) to the appropriate registers. Interpreting RNAi as a computational
structure, we can investigate the computational meaning of RNAi, especially its
complexity. Initially, the machine is configured as a Chemical Ground Form
(CGF), which generates incorrect jumps. To remedy this problem, the system is
remodeled as recursive RNAi, in which siRNA targets not only mRNA but also the
machine instructional analogues of Dicer and RISC. Finally, probabilistic
termination is investigated in the recursive RNAi system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4097</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4097</id><created>2012-11-17</created><authors><author><keyname>Dominici</keyname><forenames>Maurizio</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Della Rocca</keyname><forenames>Simona Ronchi</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Tranquilli</keyname><forenames>Paolo</forenames><affiliation>Universit&#xe0; di Bologna</affiliation></author></authors><title>Standardization in resource lambda-calculus</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2012, arXiv:1211.3480</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 101, 2012, pp. 1-11</journal-ref><doi>10.4204/EPTCS.101.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The resource calculus is an extension of the lambda-calculus allowing to
model resource consumption. It is intrinsically non-deterministic and has two
general notions of reduction - one parallel, preserving all the possible
results as a formal sum, and one non-deterministic, performing an exclusive
choice at every step. We prove that the non-deterministic reduction enjoys a
notion of standardization, which is the natural extension with respect to the
similar one in classical lambda-calculus. The full parallel reduction only
enjoys a weaker notion of standardization instead. The result allows an
operational characterization of may-solvability, which has been introduced and
already characterized (from the syntactical and logical points of view) by
Pagani and Ronchi Della Rocca.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4098</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4098</id><created>2012-11-17</created><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames><affiliation>King's College London</affiliation></author><author><keyname>Maulat</keyname><forenames>S&#xe9;bastien</forenames><affiliation>&#xc9;cole Normale Sup&#xe9;rieure de Lyon</affiliation></author></authors><title>Higher-order port-graph rewriting</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2012, arXiv:1211.3480</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 101, 2012, pp. 25-37</journal-ref><doi>10.4204/EPTCS.101.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The biologically inspired framework of port-graphs has been successfully used
to specify complex systems. It is the basis of the PORGY modelling tool. To
facilitate the specification of proof normalisation procedures via graph
rewriting, in this paper we add higher-order features to the original
port-graph syntax, along with a generalised notion of graph morphism. We
provide a matching algorithm which enables to implement higher-order port-graph
rewriting in PORGY, thus one can visually study the dynamics of the systems
modelled. We illustrate the expressive power of higher-order port-graphs with
examples taken from proof-net reduction systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4099</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4099</id><created>2012-11-17</created><authors><author><keyname>Baltazar</keyname><forenames>Pedro</forenames><affiliation>University of Lisbon, Faculty of Sciences and LaSIGE</affiliation></author><author><keyname>Mostrous</keyname><forenames>Dimitris</forenames><affiliation>University of Lisbon, Faculty of Sciences and LaSIGE</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco T.</forenames><affiliation>University of Lisbon, Faculty of Sciences and LaSIGE</affiliation></author></authors><title>Linearly Refined Session Types</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2012, arXiv:1211.3480</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 101, 2012, pp. 38-49</journal-ref><doi>10.4204/EPTCS.101.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session types capture precise protocol structure in concurrent programming,
but do not specify properties of the exchanged values beyond their basic type.
Refinement types are a form of dependent types that can address this
limitation, combining types with logical formulae that may refer to program
values and can constrain types using arbitrary predicates. We present a pi
calculus with assume and assert operations, typed using a session discipline
that incorporates refinement formulae written in a fragment of Multiplicative
Linear Logic. Our original combination of session and refinement types,
together with the well established benefits of linearity, allows very
fine-grained specifications of communication protocols in which refinement
formulae are treated as logical resources rather than persistent truths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4100</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4100</id><created>2012-11-17</created><authors><author><keyname>Deng</keyname><forenames>Yuxin</forenames><affiliation>Carnegie Mellon University and Shanghai Jiao Tong University</affiliation></author><author><keyname>Cervesato</keyname><forenames>Iliano</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Simmons</keyname><forenames>Robert J.</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Relating Reasoning Methodologies in Linear Logic and Process Algebra</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2012, arXiv:1211.3480</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 101, 2012, pp. 50-60</journal-ref><doi>10.1017/S0960129514000413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the proof-theoretic notion of logical preorder coincides with
the process-theoretic notion of contextual preorder for a CCS-like calculus
obtained from the formula-as-process interpretation of a fragment of linear
logic. The argument makes use of other standard notions in process algebra,
namely a labeled transition system and a coinductively defined simulation
relation. This result establishes a connection between an approach to reason
about process specifications and a method to reason about logic specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4101</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4101</id><created>2012-11-17</created><updated>2013-02-28</updated><authors><author><keyname>Liao</keyname><forenames>Gang</forenames></author><author><keyname>Qin</keyname><forenames>Zhi-hui</forenames></author><author><keyname>Ma</keyname><forenames>Long-fei</forenames></author><author><keyname>Sun</keyname><forenames>Qi</forenames></author></authors><title>Optimizing Synchronization Algorithm for Auto-parallelizing Compiler</title><categories>cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the need for two approaches to optimize producer
and consumer synchronization for auto-parallelizing compiler. Emphasis is
placed on the construction of a criterion model by which the compiler reduce
the number of synchronization operations needed to synchronize the dependence
in a loop and perform optimization reduces the overhead of enforcing all
dependence. In accordance with our study, we transform to modify and eliminate
dependence on iteration space diagram (ISD), and carry out the problems of
acyclic and cyclic dependence in detail. we eliminate partial dependence and
optimize the synchronize instructions. Some didactic examples are included to
illustrate the optimize procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4102</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4102</id><created>2012-11-17</created><authors><author><keyname>Jiresch</keyname><forenames>Eugen</forenames><affiliation>Vienna University of Technology</affiliation></author></authors><title>Extending the Interaction Nets Calculus by Generic Rules</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2012, arXiv:1211.3480</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 101, 2012, pp. 12-24</journal-ref><doi>10.4204/EPTCS.101.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the textual calculus for interaction nets by generic rules and
propose constraints to preserve uniform confluence. Furthermore, we discuss the
implementation of generic rules in the language inets, which is based on the
lightweight interaction nets calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4113</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4113</id><created>2012-11-17</created><authors><author><keyname>Guo</keyname><forenames>Ivan</forenames></author></authors><title>Unilaterally Competitive Multi-Player Stopping Games</title><categories>cs.GT math.OC</categories><comments>25 pages, 2 figures</comments><msc-class>91A06, 91A10, 91A15, 91A50, 60G40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-player competitive Dynkin stopping game is constructed. Each player
can either exit the game for a fixed payoff, determined a priori, or stay and
receive an adjusted payoff depending on the decision of other players. The
single period case is shown to be &quot;weakly unilaterally competitive&quot;. We present
an explicit construction of the unique value at which Nash and optimal
equilibria are attained. Multiple period generalisations are explored. The game
has interpretations in economic and financial contexts, for example, as a
consumption model with bounded resources. It also serves as a starting point to
the construction of multi-person financial game options. In particular, the
concept of optimal equilibria becomes pivotal in the pricing of the game
options via super-replication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4116</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4116</id><created>2012-11-17</created><updated>2014-08-19</updated><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J.</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author></authors><title>The Algebraic Combinatorial Approach for Low-Rank Matrix Completion</title><categories>cs.LG cs.NA math.AG math.CO stat.ML</categories><comments>37 pages, with an appendix by Takeaki Uno</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algebraic combinatorial view on low-rank matrix completion
based on studying relations between a few entries with tools from algebraic
geometry and matroid theory. The intrinsic locality of the approach allows for
the treatment of single entries in a closed theoretical and practical
framework. More specifically, apart from introducing an algebraic combinatorial
theory of low-rank matrix completion, we present probability-one algorithms to
decide whether a particular entry of the matrix can be completed. We also
describe methods to complete that entry from a few others, and to estimate the
error which is incurred by any method completing that entry. Furthermore, we
show how known results on matrix completion and their sampling assumptions can
be related to our new perspective and interpreted in terms of a completability
phase transition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4122</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4122</id><created>2012-11-17</created><authors><author><keyname>Xu</keyname><forenames>Zilong</forenames></author><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Cost-sensitive C4.5 with post-pruning and competition</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision tree is an effective classification approach in data mining and
machine learning. In applications, test costs and misclassification costs
should be considered while inducing decision trees. Recently, some
cost-sensitive learning algorithms based on ID3 such as CS-ID3, IDX,
\lambda-ID3 have been proposed to deal with the issue. These algorithms deal
with only symbolic data. In this paper, we develop a decision tree algorithm
inspired by C4.5 for numeric data. There are two major issues for our
algorithm. First, we develop the test cost weighted information gain ratio as
the heuristic information. According to this heuristic information, our
algorithm is to pick the attribute that provides more gain ratio and costs less
for each selection. Second, we design a post-pruning strategy through
considering the tradeoff between test costs and misclassification costs of the
generated decision tree. In this way, the total cost is reduced. Experimental
results indicate that (1) our algorithm is stable and effective; (2) the
post-pruning technique reduces the total cost significantly; (3) the
competition strategy is effective to obtain a cost-sensitive decision tree with
low cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4123</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4123</id><created>2012-11-17</created><authors><author><keyname>Chopra</keyname><forenames>Amit K.</forenames></author><author><keyname>Singh</keyname><forenames>Munindar P.</forenames></author></authors><title>Interaction-Oriented Software Engineering: Concepts and Principles</title><categories>cs.SE cs.MA</categories><acm-class>H.1.0; D.2.1; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following established tradition, software engineering today is rooted in a
conceptually centralized way of thinking. The primary SE artifact is a
specification of a machine -- a computational artifact -- that would meet the
(elicited and) stated requirements. Therein lies a fundamental mismatch with
(open) sociotechnical systems, which involve multiple autonomous social
participants or principals who interact with each other to further their
individual goals. No central machine governs the behaviors of the various
principals.
  We introduce Interaction-Oriented Software Engineering (IOSE) as an approach
expressly suited to the needs of open sociotechnical systems. In IOSE,
specifying a system amounts to specifying the interactions among the principals
as protocols. IOSE reinterprets the classical software engineering principles
of modularity, abstraction, separation of concerns, and encapsulation in a
manner that accords with the realities of sociotechnical systems. To highlight
the novelty of IOSE, we show where well-known SE methodologies, especially
those that explicitly aim to address either sociotechnical systems or the
modeling of interactions among autonomous principals, fail to satisfy the IOSE
principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4125</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4125</id><created>2012-11-17</created><updated>2012-12-22</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaoqiang</forenames></author><author><keyname>Li</keyname><forenames>Qingguo</forenames></author></authors><title>Some new similarity measures for hesitant fuzzy sets and their
  applications in multiple attribute decision making</title><categories>cs.IT math.IT</categories><comments>17 pages</comments><msc-class>03E72</msc-class><acm-class>A.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity measure is a very important topic in fuzzy set theory. Torra
(2010) proposed the notion of hesitant fuzzy set(HFS), which is a
generalization of the notion of Zadeh' fuzzy set. In this paper, some new
similarity measures for HFSs are developed. Based on the proposed similarity
measures, a method of multiple attribute decision making under hesitant fuzzy
environment is also introduced. Additionally, a numerical example is given to
illustrate the application of the proposed similarity measures of HFSs to
decision-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4133</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4133</id><created>2012-11-17</created><authors><author><keyname>Bitar</keyname><forenames>Ibrahim El</forenames></author><author><keyname>Belouadha</keyname><forenames>Fatima-Zahra</forenames></author><author><keyname>Roudies</keyname><forenames>Ounsa</forenames></author></authors><title>A Logic and Adaptive Approach for Efficient Diagnosis Systems using CBR</title><categories>cs.AI</categories><comments>5 pages,3 figures, 1 table</comments><journal-ref>http://www.ijcaonline.org/archives/volume39/number15/4893-7393
  year: 2012</journal-ref><doi>10.5120/4893-7393</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Case Based Reasoning (CBR) is an intelligent way of thinking based on
experience and capitalization of already solved cases (source cases) to find a
solution to a new problem (target case). Retrieval phase consists on
identifying source cases that are similar to the target case. This phase may
lead to erroneous results if the existing knowledge imperfections are not taken
into account. This work presents a novel solution based on Fuzzy logic
techniques and adaptation measures which aggregate weighted similarities to
improve the retrieval results. To confirm the efficiency of our solution, we
have applied it to the industrial diagnosis domain. The obtained results are
more efficient results than those obtained by applying typical measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4142</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4142</id><created>2012-11-17</created><authors><author><keyname>Abbey</keyname><forenames>Ralph</forenames></author><author><keyname>Diepenbrock</keyname><forenames>Jeremy</forenames></author><author><keyname>Langville</keyname><forenames>Amy</forenames></author><author><keyname>Meyer</keyname><forenames>Carl</forenames></author><author><keyname>Race</keyname><forenames>Shaina</forenames></author><author><keyname>Zhou</keyname><forenames>Dexin</forenames></author></authors><title>Data Clustering via Principal Direction Gap Partitioning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the geometrical interpretation of the PCA based clustering
algorithm Principal Direction Divisive Partitioning (PDDP). We give several
examples where this algorithm breaks down, and suggest a new method, gap
partitioning, which takes into account natural gaps in the data between
clusters. Geometric features of the PCA space are derived and illustrated and
experimental results are given which show our method is comparable on the
datasets used in the original paper on PDDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4150</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4150</id><created>2012-11-17</created><authors><author><keyname>Zadimoghaddam</keyname><forenames>Morteza</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Efficiently Learning from Revealed Preference</title><categories>cs.GT cs.DS cs.LG</categories><comments>Extended abstract appears in WINE 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the revealed preferences problem from a learning
perspective. Every day, a price vector and a budget is drawn from an unknown
distribution, and a rational agent buys his most preferred bundle according to
some unknown utility function, subject to the given prices and budget
constraint. We wish not only to find a utility function which rationalizes a
finite set of observations, but to produce a hypothesis valuation function
which accurately predicts the behavior of the agent in the future. We give
efficient algorithms with polynomial sample-complexity for agents with linear
valuation functions, as well as for agents with linearly separable, concave
valuation functions with bounded second derivative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4161</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4161</id><created>2012-11-17</created><authors><author><keyname>Ahn</keyname><forenames>Ae-Lim</forenames><affiliation>DICORA</affiliation></author><author><keyname>Laporte</keyname><forenames>&#xc9;ric</forenames><affiliation>LIGM</affiliation></author><author><keyname>Nam</keyname><forenames>Jee-Sun</forenames><affiliation>DICORA, LIGM</affiliation></author></authors><title>Semantic Polarity of Adjectival Predicates in Online Reviews</title><categories>cs.CL</categories><comments>electronic version (10 pp.)</comments><proxy>ccsd</proxy><journal-ref>Seoul International Conference on Linguistics (SICOL'10), Seoul :
  Korea, Republic Of (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web users produce more and more documents expressing opinions. Because these
have become important resources for customers and manufacturers, many have
focused on them. Opinions are often expressed through adjectives with positive
or negative semantic values. In extracting information from users' opinion in
online reviews, exact recognition of the semantic polarity of adjectives is one
of the most important requirements. Since adjectives have different semantic
orientations according to contexts, it is not satisfying to extract opinion
information without considering the semantic and lexical relations between the
adjectives and the feature nouns appropriate to a given domain. In this paper,
we present a classification of adjectives by polarity, and we analyze
adjectives that are undetermined in the absence of contexts. Our research
should be useful for accurately predicting semantic orientations of opinion
sentences, and should be taken into account before relying on an automatic
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4174</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4174</id><created>2012-11-17</created><updated>2014-01-09</updated><authors><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Energy-Efficient Nonstationary Spectrum Sharing</title><categories>cs.IT cs.GT math.IT</categories><comments>41 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1201.3328</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel design framework for energy-efficient spectrum sharing
among autonomous users who aim to minimize their energy consumptions subject to
minimum throughput requirements. Most existing works proposed stationary
spectrum sharing policies, in which users transmit at fixed power levels. Since
users transmit simultaneously under stationary policies, to fulfill minimum
throughput requirements, they need to transmit at high power levels to overcome
interference. To improve energy efficiency, we construct nonstationary spectrum
sharing policies, in which the users transmit at time-varying power levels.
Specifically, we focus on TDMA (time-division multiple access) policies in
which one user transmits at each time (but not in a round-robin fashion). The
proposed policy can be implemented by each user running a low-complexity
algorithm in a decentralized manner. It achieves high energy efficiency even
when the users have erroneous and binary feedback about their interference
levels. Moreover, it can adapt to the dynamic entry and exit of users. The
proposed policy is also deviation-proof, namely autonomous users will find it
in their self-interests to follow it. Compared to existing policies, the
proposed policy can achieve an energy saving of up to 90% when the number of
users is high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4191</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4191</id><created>2012-11-17</created><authors><author><keyname>Zhang</keyname><forenames>Fengrong</forenames></author><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Hu</keyname><forenames>Yupu</forenames></author><author><keyname>Zhang</keyname><forenames>Wenzheng</forenames></author></authors><title>Secondary Constructions of Bent Functions and Highly Nonlinear Resilient
  Functions</title><categories>cs.CR cs.IT math.IT</categories><comments>27 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we first present a new secondary construction of bent
functions (building new bent functions from two already defined ones).
Furthermore, we apply the construction using as initial functions some specific
bent functions and then provide several concrete constructions of bent
functions. The second part of the paper is devoted to the constructions of
resilient functions. We give a generalization of the indirect sum construction
for constructing resilient functions with high nonlinearity. In addition, we
modify the generalized construction to ensure a high nonlinearity of the
constructed function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4193</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4193</id><created>2012-11-17</created><authors><author><keyname>Wu</keyname><forenames>Jian-Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Li</keyname><forenames>Hailun</forenames></author></authors><title>Equitable vertex arboricity of graphs</title><categories>math.CO cs.DM</categories><msc-class>05C10, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An equitable $(t,k,d)$-tree-coloring of a graph $G$ is a coloring to vertices
of $G$ such that the sizes of any two color classes differ by at most one and
the subgraph induced by each color class is a forest of maximum degree at most
$k$ and diameter at most $d$. The minimum $t$ such that $G$ has an equitable
$(t',k,d)$-tree-coloring for every $t'\geq t$ is called the strong equitable
$(k,d)$-vertex-arboricity and denoted by $va^{\equiv}_{k,d}(G)$. In this paper,
we give sharp upper bounds for $va^{\equiv}_{1,1}(K_{n,n})$ and
$va^{\equiv}_{k,\infty}(K_{n,n})$ by showing that
$va^{\equiv}_{1,1}(K_{n,n})=O(n)$ and
$va^{\equiv}_{k,\infty}(K_{n,n})=O(n^{\1/2})$ for every $k\geq 2$. It is also
proved that $va^{\equiv}_{\infty,\infty}(G)\leq 3$ for every planar graph $G$
with girth at least 5 and $va^{\equiv}_{\infty,\infty}(G)\leq 2$ for every
planar graph $G$ with girth at least 6 and for every outerplanar graph. We
conjecture that $va^{\equiv}_{\infty,\infty}(G)=O(1)$ for every planar graph
and $va^{\equiv}_{\infty,\infty}(G)\leq \lceil\frac{\Delta(G)+1}{2}\rceil$ for
every graph $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4198</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4198</id><created>2012-11-18</created><updated>2014-03-24</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Xu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Gunawan</keyname><forenames>Erry</forenames></author><author><keyname>Wang</keyname><forenames>Chenwei</forenames></author></authors><title>Degrees of Freedom of the 3-User Rank-Deficient MIMO Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>28 pages, 7 figures. To appear in IEEE transactions on wireless
  communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the degrees of freedom (DoF) characterization for the $3$-user
$M_T\times M_R$ multiple-input multiple-output (MIMO) interference channel (IC)
with \emph{rank-deficient} channel matrices, where each transmitter is equipped
with $M_T$ antennas and each receiver with $M_R$ antennas, and the interfering
channel matrices from each transmitter to the other two receivers are of ranks
$D_1$ and $D_2$, respectively. One important intermediate step for both the
converse and achievability arguments is to convert the fully-connected
rank-deficient channel into an equivalent partially-connected full-rank MIMO-IC
by invertible linear transformations. As such, existing techniques developed
for full-rank MIMO-IC can be incorporated to derive the DoF outer and inner
bounds for the rank-deficient case. Our result shows that when the interfering
links are weak in terms of the channel ranks, i.e., $D_1+D_2\leq \min(M_T,
M_R)$, zero forcing is sufficient to achieve the optimal DoF. On the other
hand, when $D_1+D_2&gt; \min(M_T, M_R)$, a combination of zero forcing and
interference alignment is in general required for DoF optimality. The DoF
characterization obtained in this paper unifies several existing results in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4206</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4206</id><created>2012-11-18</created><authors><author><keyname>Magli</keyname><forenames>Enrico</forenames></author><author><keyname>Wang</keyname><forenames>Mea</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>Network Coding Meets Multimedia: a Review</title><categories>cs.MM cs.NI</categories><comments>Part of this work is under publication in IEEE Transactions on
  Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While every network node only relays messages in a traditional communication
system, the recent network coding (NC) paradigm proposes to implement simple
in-network processing with packet combinations in the nodes. NC extends the
concept of &quot;encoding&quot; a message beyond source coding (for compression) and
channel coding (for protection against errors and losses). It has been shown to
increase network throughput compared to traditional networks implementation, to
reduce delay and to provide robustness to transmission errors and network
dynamics. These features are so appealing for multimedia applications that they
have spurred a large research effort towards the development of
multimedia-specific NC techniques. This paper reviews the recent work in NC for
multimedia applications and focuses on the techniques that fill the gap between
NC theory and practical applications. It outlines the benefits of NC and
presents the open challenges in this area. The paper initially focuses on
multimedia-specific aspects of network coding, in particular delay, in-network
error control, and media-specific error control. These aspects permit to handle
varying network conditions as well as client heterogeneity, which are critical
to the design and deployment of multimedia systems. After introducing these
general concepts, the paper reviews in detail two applications that lend
themselves naturally to NC via the cooperation and broadcast models, namely
peer-to-peer multimedia streaming and wireless networking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4213</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4213</id><created>2012-11-18</created><authors><author><keyname>Park</keyname><forenames>Juho</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author></authors><title>On the Pareto-Optimal Beam Structure and Design for Multi-User MIMO
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>27 pages, 4 figures, Submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2013.2281784</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Pareto-optimal beam structure for multi-user
multiple-input multiple-output (MIMO) interference channels is investigated and
a necessary condition for any Pareto-optimal transmit signal covariance matrix
is presented for the K-pair Gaussian (N,M_1,...,M_K) interference channel. It
is shown that any Pareto-optimal transmit signal covariance matrix at a
transmitter should have its column space contained in the union of the
eigen-spaces of the channel matrices from the transmitter to all receivers.
Based on this necessary condition, an efficient parameterization for the beam
search space is proposed. The proposed parameterization is given by the product
manifold of a Stiefel manifold and a subset of a hyperplane and enables us to
construct a very efficient beam design algorithm by exploiting its rich
geometrical structure and existing tools for optimization on Stiefel manifolds.
Reduction in the beam search space dimension and computational complexity by
the proposed parameterization and the proposed beam design approach is
significant when the number of transmit antennas is larger than the sum of the
numbers of receive antennas, as in upcoming cellular networks adopting massive
MIMO technologies. Numerical results validate the proposed parameterization and
the proposed cooperative beam design method based on the parameterization for
MIMO interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4218</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4218</id><created>2012-11-18</created><updated>2012-11-21</updated><authors><author><keyname>Melnikova</keyname><forenames>N. B.</forenames></author><author><keyname>Krzhizhanovskaya</keyname><forenames>V. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Modeling Earthen Dike Stability: Sensitivity Analysis and Automatic
  Calibration of Diffusivities Based on Live Sensor Data</title><categories>cs.CE physics.geo-ph</categories><journal-ref>Journal of Hydrology 496 (2013), pp. 154-165</journal-ref><doi>10.1016/j.jhydrol.2013.05.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes concept and implementation details of integrating a
finite element module for dike stability analysis Virtual Dike into an early
warning system for flood protection. The module operates in real-time mode and
includes fluid and structural sub-models for simulation of porous flow through
the dike and for dike stability analysis. Real-time measurements obtained from
pore pressure sensors are fed into the simulation module, to be compared with
simulated pore pressure dynamics. Implementation of the module has been
performed for a real-world test case - an earthen levee protecting a sea-port
in Groningen, the Netherlands. Sensitivity analysis and calibration of
diffusivities have been performed for tidal fluctuations. An algorithm for
automatic diffusivities calibration for a heterogeneous dike is proposed and
studied. Analytical solutions describing tidal propagation in one-dimensional
saturated aquifer are employed in the algorithm to generate initial estimates
of diffusivities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4226</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4226</id><created>2012-11-18</created><authors><author><keyname>Perera</keyname><forenames>Rivindu</forenames></author></authors><title>Education for All: Remote testing system with gesture recognition and
  recording</title><categories>cs.CY cs.HC</categories><comments>5 pages, International Journal of Advances in Engineering, Science
  and Technology 2012</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Etymologically, in Latin expresses &quot;educare&quot;, that means to bring out, or be
engaged in the infinite process of learning to present to the society as a
valuable citizen. However, unfortunately especially in third world countries,
education cannot be achieved due to, lack of inorganic and organic resources.
However, many third world countries have embraced the concepts such as One
Laptop per Child, facilitating the students to learn. The effective adaptation
of these concepts has being launched through many government and non-government
projects, providing inorganic resources. However, inorganic resources alone
cannot provide quality education, as learning needs assessment procedures,
feedback generators and trainers who could guide the students to gain
knowledge. This paper attempts to introduce an acceptable solution that can be
used to address facilitating resources to enhance the learning experience
through enabling organic resources such as teachers, instructors and trainers
on a remote mode through technology. This paper introduces a software system
that is used to design and distribute examinations and detect gestures of
students while answering remotely. The feature enables the teacher or
instructor to gain a better understanding about the learner's attitude when
taking the assessment. The content of the paper is organized to give the basic
idea of the system and it includes description of the system and practical
effectiveness of the system with evaluations from different views. A java
enabled computer with a webcam and internet access is the minimum requirements
to be able to use the proposed system. The development platform is based on
java, with the use of &quot;Chilkat&quot; to maintain an asynchronous connection with the
FTP server. &quot;iGesture&quot; and &quot;Yuille&quot; approach play major role in gesture
detection and recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4235</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4235</id><created>2012-11-18</created><authors><author><keyname>Dhanjal</keyname><forenames>Charanpal</forenames><affiliation>LTCI</affiliation></author><author><keyname>Blanchemanche</keyname><forenames>Sandrine</forenames><affiliation>M&#xc9;T@RISK</affiliation></author><author><keyname>Cl&#xe9;men&#xe7;on</keyname><forenames>St&#xe9;phan</forenames><affiliation>LTCI</affiliation></author><author><keyname>Rona-Tas</keyname><forenames>Akos</forenames><affiliation>M&#xc9;T@RISK</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>LTCI, SAMM</affiliation></author></authors><title>Dissemination of Health Information within Social Networks</title><categories>cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>Networks in Social Policy Problems (2012) 15-46</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate, how information about a common food born
health hazard, known as Campylobacter, spreads once it was delivered to a
random sample of individuals in France. The central question addressed here is
how individual characteristics and the various aspects of social network
influence the spread of information. A key claim of our paper is that
information diffusion processes occur in a patterned network of social ties of
heterogeneous actors. Our percolation models show that the characteristics of
the recipients of the information matter as much if not more than the
characteristics of the sender of the information in deciding whether the
information will be transmitted through a particular tie. We also found that at
least for this particular advisory, it is not the perceived need of the
recipients for the information that matters but their general interest in the
topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4246</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4246</id><created>2012-11-18</created><updated>2014-08-19</updated><authors><author><keyname>Alain</keyname><forenames>Guillaume</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>What Regularized Auto-Encoders Learn from the Data Generating
  Distribution</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What do auto-encoders learn about the underlying data generating
distribution? Recent work suggests that some auto-encoder variants do a good
job of capturing the local manifold structure of data. This paper clarifies
some of these previous observations by showing that minimizing a particular
form of regularized reconstruction error yields a reconstruction function that
locally characterizes the shape of the data generating density. We show that
the auto-encoder captures the score (derivative of the log-density with respect
to the input). It contradicts previous interpretations of reconstruction error
as an energy function. Unlike previous results, the theorems provided here are
completely generic and do not depend on the parametrization of the
auto-encoder: they show what the auto-encoder would tend to if given enough
capacity and examples. These results are for a contractive training criterion
we show to be similar to the denoising auto-encoder training criterion with
small corruption noise, but with contraction applied on the whole
reconstruction function rather than just encoder. Similarly to score matching,
one can consider the proposed training criterion as a convenient alternative to
maximum likelihood because it does not involve a partition function. Finally,
we show how an approximate Metropolis-Hastings MCMC can be setup to recover
samples from the estimated distribution, and this is confirmed in sampling
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4254</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4254</id><created>2012-11-18</created><updated>2012-11-30</updated><authors><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Minimum CSIT to achieve Maximum Degrees of Freedom for the MISO BC</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel state information at the transmitter (CSIT) is a key ingredient in
realizing the multiplexing gain provided by distributed MIMO systems. For a
downlink multiple-input single output (MISO) broadcast channel, with M antennas
at the transmitters and K single antenna receivers, the maximum multiplexing
gain or the maximum degrees of freedom (DoF) is min(M,K). The optimal DoF of
min(M,K) is achievable if the transmitter has access to perfect, instantaneous
CSIT from all receivers. In this paper, we pose the question that what is
minimum amount of CSIT required per user in order to achieve the maximum DoF of
min(M,K). By minimum amount of CSIT per user, we refer to the minimum fraction
of time that the transmitter has access to perfect and instantaneous CSIT from
a user. Through a novel converse proof and an achievable scheme, it is shown
that the minimum fraction of time, perfect CSIT is required per user in order
to achieve the DoF of min(M,K) is given by min(M,K)/K.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4258</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4258</id><created>2012-11-18</created><updated>2013-01-03</updated><authors><author><keyname>Andrews</keyname><forenames>Matthew</forenames></author><author><keyname>Zhang</keyname><forenames>Lisa</forenames></author></authors><title>Utility Optimization in Heterogeneous Networks via CSMA-Based Algorithms</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms for carrier and rate allocation in cellular systems with
distributed components such as a heterogeneous LTE system with macrocells and
femtocells. Existing work on LTE systems often involves centralized techniques
or requires significant signaling, and is therefore not always applicable in
the presence of femtocells. More distributed CSMA-based algorithms
(carrier-sense multiple access) were developed in the context of 802.11 systems
and have been proven to be utility optimal. However, the proof typically
assumes a single transmission rate on each carrier. Further, it relies on the
CSMA collision detection mechanisms to know whether a transmission is feasible.
  In this paper we present a framework for LTE scheduling that is based on CSMA
techniques. In particular we first prove that CSMA-based algorithms can be
generalized to handle multiple transmission rates in a multi-carrier setting
while maintaining utility optimality. We then show how such an algorithm can be
implemented in a heterogeneous LTE system where the existing Channel Quality
Indication (CQI) mechanism is used to decide transmission feasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4264</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4264</id><created>2012-11-18</created><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Non-Local Patch Regression: Robust Image Denoising in Patch Space</title><categories>cs.CV</categories><comments>Submitted</comments><journal-ref>IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently demonstrated in [Chaudhury et al.,Non-Local Euclidean
Medians,2012] that the denoising performance of Non-Local Means (NLM) can be
improved at large noise levels by replacing the mean by the robust Euclidean
median. Numerical experiments on synthetic and natural images showed that the
latter consistently performed better than NLM beyond a certain noise level, and
significantly so for images with sharp edges. The Euclidean mean and median can
be put into a common regression (on the patch space) framework, in which the
l_2 norm of the residuals is considered in the former, while the l_1 norm is
considered in the latter. The natural question then is what happens if we
consider l_p (0&lt;p&lt;1) regression? We investigate this possibility in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4266</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4266</id><created>2012-11-18</created><authors><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author></authors><title>A Dynamical System for PageRank with Time-Dependent Teleportation</title><categories>cs.SI cs.IR math.DS physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.6098</comments><acm-class>G.2.2; H.2.8; G.1.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dynamical system that captures changes to the network centrality
of nodes as external interest in those nodes vary. We derive this system by
adding time-dependent teleportation to the PageRank score. The result is not a
single set of importance scores, but rather a time-dependent set. These can be
converted into ranked lists in a variety of ways, for instance, by taking the
largest change in the importance score. For an interesting class of the dynamic
teleportation functions, we derive closed form solutions for the dynamic
PageRank vector. The magnitude of the deviation from a static PageRank vector
is given by a PageRank problem with complex-valued teleportation parameters.
Moreover, these dynamical systems are easy to evaluate. We demonstrate the
utility of dynamic teleportation on both the article graph of Wikipedia, where
the external interest information is given by the number of hourly visitors to
each page, and the Twitter social network, where external interest is the
number of tweets per month. For these problems, we show that using information
from the dynamical system helps improve a prediction task and identify trends
in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4272</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4272</id><created>2012-11-18</created><authors><author><keyname>Zhou</keyname><forenames>Haichuan</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharm</forenames></author></authors><title>On Achievable Schemes of Interference Alignment in Constant Channels via
  Finite Amplify-and-Forward Relays</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper elaborates on the achievable schemes of interference alignment in
constant channels via finite amplify-and-forward (AF) relays. Consider $K$
sources communicating with $K$ destinations without direct links besides the
relay connections. The total number of relays is finite. The objective is to
achieve interference alignment for all user pairs to obtain half of their
interference-free degrees of freedom. In general, two strategies are employed:
coding at the edge and coding in the middle, in which relays show different
roles. The contributions are that two fundamental and critical elements are
captured to enable interference alignment in this network: channel randomness
or relativity; subspace dimension suppression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4275</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4275</id><created>2012-11-18</created><authors><author><keyname>Zhou</keyname><forenames>Haichuan</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharm</forenames></author></authors><title>Close-Form Design of Antenna-Constrained Multi-Cell Multi-User Downlink
  Interference Alignment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the downlink channels in multi-cell multi-user
interfering networks. The goal is to propose close-form designs to obtain
degrees of freedom (DoF) in high SNR region for the network composed of base
stations (BS) as transmitters and mobile stations (MS) as receivers. Consider
the realistic system, both BS and MS have finite antennas, so that the design
of interference alignment is highly constrained by the feasibility conditions.
The focus of design is to explore potential opportunities of alignment in the
subspace both from the BS transmit side and from the MS receive side. The new
IA schemes for cellular downlink channels are in the form of causal dynamic
processes in contrary to conventional static IA schemes. For different
implementations, system conditions are compared from all aspects, which include
antenna usage, CSI overhead and computational complexity. This research scope
covers a wide range of typical multi-cell multi-user network models. The first
one is a $K$-cell fully connected cellular network; the second one is a Wyner
cyclic cellular network with two adjacent interfering links; the third one is a
Wyner cyclic cellular network with single adjacent interfering link considering
cell-edge and cell-interior users respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4276</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4276</id><created>2012-11-18</created><authors><author><keyname>Zhou</keyname><forenames>Haichuan</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharm</forenames></author></authors><title>On Achievable Schemes of Interference Alignment with Double-Layered
  Symbol Extensions in Interference Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks into the $K$-user interference channel. Interference
Alignment is much likely to be applied with double-layered symbol extensions,
either for constant channels in the H$\o$st-Madsen-Nosratinia conjecture or
slowly changing channels. In our work, the core idea relies on double-layered
symbol extensions to artificially construct equivalent time-variant channels to
provide crucial \textit{channel randomness or relativity} required by
conventional Cadambe-Jafar scheme in time-variant channels
\cite{IA-DOF-Kuser-Interference}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4289</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4289</id><created>2012-11-18</created><updated>2013-07-11</updated><authors><author><keyname>Tran</keyname><forenames>Loc</forenames></author></authors><title>Application of three graph Laplacian based semi-supervised learning
  methods to protein function prediction problem</title><categories>cs.LG cs.CE q-bio.QM stat.ML</categories><comments>16 pages, 9 tables</comments><acm-class>H.2.8</acm-class><doi>10.5121/ijbb.2013.3202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein function prediction is the important problem in modern biology. In
this paper, the un-normalized, symmetric normalized, and random walk graph
Laplacian based semi-supervised learning methods will be applied to the
integrated network combined from multiple networks to predict the functions of
all yeast proteins in these multiple networks. These multiple networks are
network created from Pfam domain structure, co-participation in a protein
complex, protein-protein interaction network, genetic interaction network, and
network created from cell cycle gene expression measurements. Multiple networks
are combined with fixed weights instead of using convex optimization to
determine the combination weights due to high time complexity of convex
optimization method. This simple combination method will not affect the
accuracy performance measures of the three semi-supervised learning methods.
Experiment results show that the un-normalized and symmetric normalized graph
Laplacian based methods perform slightly better than random walk graph
Laplacian based method for integrated network. Moreover, the accuracy
performance measures of these three semi-supervised learning methods for
integrated network are much better than the best accuracy performance measures
of these three methods for the individual network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4290</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4290</id><created>2012-11-18</created><updated>2012-11-19</updated><authors><author><keyname>Rahman</keyname><forenames>Muntasir Raihan</forenames></author><author><keyname>Golab</keyname><forenames>Wojciech</forenames></author><author><keyname>AuYoung</keyname><forenames>Alvin</forenames></author><author><keyname>Keeton</keyname><forenames>Kimberly</forenames></author><author><keyname>Wylie</keyname><forenames>Jay J.</forenames></author></authors><title>Toward a Principled Framework for Benchmarking Consistency</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale key-value storage systems sacrifice consistency in the interest
of dependability (i.e., partition tolerance and availability), as well as
performance (i.e., latency). Such systems provide eventual
consistency,which---to this point---has been difficult to quantify in real
systems. Given the many implementations and deployments of
eventually-consistent systems (e.g., NoSQL systems), attempts have been made to
measure this consistency empirically, but they suffer from important drawbacks.
For example, state-of-the art consistency benchmarks exercise the system only
in restricted ways and disrupt the workload, which limits their accuracy.
  In this paper, we take the position that a consistency benchmark should paint
a comprehensive picture of the relationship between the storage system under
consideration, the workload, the pattern of failures, and the consistency
observed by clients. To illustrate our point, we first survey prior efforts to
quantify eventual consistency. We then present a benchmarking technique that
overcomes the shortcomings of existing techniques to measure the consistency
observed by clients as they execute the workload under consideration. This
method is versatile and minimally disruptive to the system under test. As a
proof of concept, we demonstrate this tool on Cassandra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4293</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4293</id><created>2012-11-18</created><updated>2016-02-20</updated><authors><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Exact Recovery of Sparse Signals via Orthogonal Matching Pursuit: How
  Many Iterations Do We Need?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal matching pursuit (OMP) is a greedy algorithm widely used for the
recovery of sparse signals from compressed measurements. In this paper, we
analyze the number of iterations required for the OMP algorithm to perform
exact recovery of sparse signals. Our analysis shows that OMP can accurately
recover all $K$-sparse signals within $\lceil 2.8 K \rceil$ iterations when the
measurement matrix satisfies a restricted isometry property (RIP). Our result
improves upon the recent result of Zhang and also bridges the gap between
Zhang's result and the fundamental limit of OMP at which exact recovery of
$K$-sparse signals cannot be uniformly guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4294</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4294</id><created>2012-11-18</created><authors><author><keyname>Awon</keyname><forenames>Nuzhat Tasneem</forenames></author><author><keyname>Islam</keyname><forenames>Md. Ashraful</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Mizanur</forenames></author><author><keyname>Islam</keyname><forenames>A. Z. M. Touhidul</forenames></author></authors><title>Effect of AWGN &amp; Fading (Raleigh &amp; Rician) channels on BER performance
  of a WiMAX communication System</title><categories>cs.NI</categories><comments>7 pages, 6 figures</comments><report-no>paper id in International Journal of Computer Science and
  Information security(IJCSIS):31071207</report-no><journal-ref>International Journal of Computer Science and Information
  security(IJCSIS), Vol 10, No 8, 2012, 11-17</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of WIMAX has attracted significant interests from all fields of
wireless communications including students, researchers, system engineers and
operators. The WIMAX can also be considered to be the main technology in the
implementation of other networks like wireless sensor networks. Developing an
understanding of the WIMAX system can be achieved by looking at the model of
the WIMAX system. This paper discusses the model building of the WIMAX physical
layer using computer MATLAB 7.5 versions. This model is a useful tool for BER
(Bit error rate) performance evaluation for the real data communication by the
WIMAX physical layer under different communication channels AWGN and fading
channel (Rayleigh and Rician), different channel encoding rates and digital
modulation schemes which is described in this paper. This paper investigates
the effect of communication channels of IEEE 802.16 OFDM based WIMAX Physical
Layer. The performance measures we presented in this paper are: the bit error
rate (BER) versus the ratio of bit energy to noise power spectral density
(Eb/No). The system parameters used in this paper are based on IEEE 802.16
standards. The simulation model built for this research work, demonstrates that
AWGN channel has better performance than Rayleigh and Rician fading channels.
Synthetic data is used to simulate this research work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4307</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4307</id><created>2012-11-19</created><authors><author><keyname>Li</keyname><forenames>Han</forenames></author><author><keyname>Gai</keyname><forenames>Kun</forenames></author><author><keyname>Gong</keyname><forenames>Pinghua</forenames></author><author><keyname>Zhang</keyname><forenames>Changshui</forenames></author></authors><title>Efficient Superimposition Recovering Algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we address the issue of recovering latent transparent layers
from superimposition images. Here, we assume we have the estimated
transformations and extracted gradients of latent layers. To rapidly recover
high-quality image layers, we propose an Efficient Superimposition Recovering
Algorithm (ESRA) by extending the framework of accelerated gradient method. In
addition, a key building block (in each iteration) in our proposed method is
the proximal operator calculating. Here we propose to employ a dual approach
and present our Parallel Algorithm with Constrained Total Variation (PACTV)
method. Our recovering method not only reconstructs high-quality layers without
color-bias problem, but also theoretically guarantees good convergence
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4321</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4321</id><created>2012-11-19</created><authors><author><keyname>Caron</keyname><forenames>Francois</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, IMB</affiliation></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Bayesian nonparametric models for ranked data</title><categories>stat.ML cs.LG stat.ME</categories><comments>NIPS - Neural Information Processing Systems (2012)</comments><proxy>ccsd</proxy><report-no>RR-8140</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a Bayesian nonparametric extension of the popular Plackett-Luce
choice model that can handle an infinite number of choice items. Our framework
is based on the theory of random atomic measures, with the prior specified by a
gamma process. We derive a posterior characterization and a simple and
effective Gibbs sampler for posterior simulation. We develop a time-varying
extension of our model, and apply it to the New York Times lists of weekly
bestselling books.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4328</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4328</id><created>2012-11-19</created><authors><author><keyname>Zawoad</keyname><forenames>Shams</forenames></author><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author></authors><title>I Have the Proof: Providing Proofs of Past Data Possession in Cloud
  Forensics</title><categories>cs.CR cs.DC</categories><comments>To appear at the Proceedings of the 2012 ASE International Conference
  on Cyber Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has emerged as a popular computing paradigm in recent years.
However, today's cloud computing architectures often lack support for computer
forensic investigations. A key task of digital forensics is to prove the
presence of a particular file in a given storage system. Unfortunately, it is
very hard to do so in a cloud given the black-box nature of clouds and the
multi-tenant cloud models. In clouds, analyzing the data from a virtual machine
instance or data stored in a cloud storage only allows us to investigate the
current content of the cloud storage, but not the previous contents. In this
paper, we introduce the idea of building proofs of past data possession in the
context of a cloud storage service. We present a scheme for creating such
proofs and evaluate its performance in a real cloud provider. We also discuss
how this proof of past data possession can be used effectively in cloud
forensics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4332</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4332</id><created>2012-11-19</created><authors><author><keyname>Liang</keyname><forenames>Ye</forenames></author></authors><title>Real root refinements for univariate polynomial equations</title><categories>cs.NA</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real root finding of polynomial equations is a basic problem in computer
algebra. This task is usually divided into two parts: isolation and refinement.
In this paper, we propose two algorithms LZ1 and LZ2 to refine real roots of
univariate polynomial equations. Our algorithms combine Newton's method and the
secant method to bound the unique solution in an interval of a monotonic convex
isolation (MCI) of a polynomial, and have quadratic and cubic convergence
rates, respectively. To avoid the swell of coefficients and speed up the
computation, we implement the two algorithms by using the floating-point
interval method in Maple15 with the package intpakX. Experiments show that our
methods are effective and much faster than the function RefineBox in the
software Maple15 on benchmark polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4346</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4346</id><created>2012-11-19</created><updated>2014-07-22</updated><authors><author><keyname>Tkachev</keyname><forenames>Ilya</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Characterization and computation of infinite horizon specifications over
  Markov processes</title><categories>math.OC cs.LO cs.SY math.PR</categories><msc-class>60J05, 68Q60</msc-class><journal-ref>Theoretical Computer Science 515 (2014), pp. 1-18</journal-ref><doi>10.1016/j.tcs.2013.09.032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the formal verification of specifications over
general discrete-time Markov processes, with an emphasis on infinite-horizon
properties. These properties, formulated in a modal logic known as PCTL, can be
expressed through value functions defined over the state space of the process.
The main goal is to understand how structural features of the model (primarily
the presence of absorbing sets) influence the uniqueness of the solutions of
corresponding Bellman equations. Furthermore, this contribution shows that the
investigation of these structural features leads to new computational
techniques to calculate the specifications of interest: the emphasis is to
derive approximation techniques with associated explicit convergence rates and
formal error bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4347</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4347</id><created>2012-11-19</created><updated>2012-12-21</updated><authors><author><keyname>Dzerzhinskiy</keyname><forenames>Fedor</forenames></author></authors><title>How many software engineering professionals hold this certificate?</title><categories>cs.SE</categories><comments>14 pages, in English, 15 pages, in Russian, 4 tables, In Roundtable
  materials, see http://www.labrate.ru/20121120/stenogramma.htm (Minor
  corrections)</comments><acm-class>D.2; K.7.3</acm-class><journal-ref>Issledovano v Rossii (electronic journal), Vol. 15 (2012), paper
  no. 033, pp. 470-484 (in Russian), 033e, pp. 485-498 (in English)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimates of quantity of the certificates issued during 10 years of existence
of the professionals certification program in the area of software engineering
implemented by one of the leading professional associations are presented. The
estimates have been obtained by way of processing certificate records openly
accessible at the certification program Web-site. Comparison of these estimates
and the known facts about evolution of the certification program indicates that
as of the present day this evolution has not led to a large scale issuance of
these certificates. But the same estimates, possibly, indicate that the meaning
of these certificates differs from what is usually highlighted, and their real
value is much greater. Also these estimates can be viewed, besides all else, as
reflecting an outcome of a decade long experimental verification of the known
idea about &quot;software engineering as a mature engineering profession,&quot; and they
possibly show that this idea deserves partial revision.
  Keywords: software engineering certification, actual results vs.
expectations, software engineering profession.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4370</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4370</id><created>2012-11-19</created><authors><author><keyname>Hanjani</keyname><forenames>Elahe Moghimi</forenames></author><author><keyname>Javanmard</keyname><forenames>Mahdi</forenames></author></authors><title>An Algorithm for Optimized Searching using NON-Overlapping Iterative
  Neighbor intervals</title><categories>cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have attempted in this paper to reduce the number of checked condition
through saving frequency of the tandem replicated words, and also using
non-overlapping iterative neighbor intervals on plane sweep algorithm. The
essential idea of non-overlapping iterative neighbor search in a document lies
in focusing the search not on the full space of solutions but on a smaller
subspace considering non-overlapping intervals defined by the solutions.
Subspace is defined by the range near the specified minimum keyword. We
repeatedly pick a range up and flip the unsatisfied keywords, so the relevant
ranges are detected. The proposed method tries to improve the plane sweep
algorithm by efficiently calculating the minimal group of words and enumerating
intervals in a document which contain the minimum frequency keyword. It
decreases the number of comparison and creates the best state of optimized
search algorithm especially in a high volume of data. Efficiency and
reliability are also increased compared to the previous modes of the technical
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4371</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4371</id><created>2012-11-19</created><authors><author><keyname>Sheta</keyname><forenames>Osama El-Sayed</forenames></author><author><keyname>Eldeen</keyname><forenames>Ahmed Nour</forenames></author></authors><title>Building a health care data warehouse for cancer diseases</title><categories>cs.DB</categories><comments>8 pages,4 figures</comments><doi>10.5121/ijdms.2012.4503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents architecture for health care data warehouse specific to
cancer diseases which could be used by executive managers, doctors, physicians
and other health professionals to support the healthcare process. The data
today existing in multi-sources with different formats makes it necessary to
have some techniques for data integration. Executive managers need access to
Information so that decision makers can react in real time to changing needs.
Information is one of the most factors to an organization success that
executive managers or physicians would need to base their decisions on, during
decision making. A health care data warehouse is therefore necessary to
integrate the different data sources into a central data repository and
analysis this data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4372</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4372</id><created>2012-11-19</created><authors><author><keyname>Tabassum</keyname><forenames>Hina</forenames><affiliation>Student Member, IEEE</affiliation></author><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames><affiliation>Fellow, IEEE</affiliation></author></authors><title>A Framework for Uplink Intercell Interference Modeling with
  Channel-Based Scheduling</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>IEEE Transactions on Wireless Communications, 2013. arXiv admin note:
  substantial text overlap with arXiv:1206.2292</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel framework for modeling the uplink intercell
interference (ICI) in a multiuser cellular network. The proposed framework
assists in quantifying the impact of various fading channel models and
state-of-the-art scheduling schemes on the uplink ICI. Firstly, we derive a
semianalytical expression for the distribution of the location of the scheduled
user in a given cell considering a wide range of scheduling schemes. Based on
this, we derive the distribution and moment generating function (MGF) of the
uplink ICI considering a single interfering cell. Consequently, we determine
the MGF of the cumulative ICI observed from all interfering cells and derive
explicit MGF expressions for three typical fading models. Finally, we utilize
the obtained expressions to evaluate important network performance metrics such
as the outage probability, ergodic capacity, and average fairness numerically.
Monte-Carlo simulation results are provided to demonstrate the efficacy of the
derived analytical expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4381</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4381</id><created>2012-11-19</created><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Degrees-of-Freedom Region of Time Correlated MISO Broadcast Channel with
  Perfect Delayed CSIT and Asymmetric Partial Current CSIT</title><categories>cs.IT math.IT</categories><comments>6 pages, 1 figure, submitted to IEEE ICC 2013 Communication Theory
  Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact of imperfect CSIT on the degrees of freedom (DoF) of a time
correlated MISO Broadcast Channel has drawn a lot of attention recently.
Maddah-Ali and Tse have shown that the completely stale CSIT still benefit the
DoF. In very recent works, Yang et al. have extended the results by integrating
the partial current CSIT for a two-user MISO broadcast channel. However, those
researches so far focused on a symmetric case. In this contribution, we
investigate a more general case where the transmitter has knowledge of current
CSI of both users with unequal qualities. The essential ingredient in our work
lies in the way to multicast the overheard interference to boost the DoF. The
optimal DoF region is simply proved and its achievability is shown using a
novel transmission scheme assuming an infinite number of channel uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4384</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4384</id><created>2012-11-19</created><authors><author><keyname>Oksanen</keyname><forenames>Jan</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Sensing Policy Based on Confidence Bounds and a Restless Multi-Armed
  Bandit Model</title><categories>cs.IT cs.LG math.IT</categories><comments>In proceedings of the 46th Asilomar conference 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sensing policy for the restless multi-armed bandit problem with stationary
but unknown reward distributions is proposed. The work is presented in the
context of cognitive radios in which the bandit problem arises when deciding
which parts of the spectrum to sense and exploit. It is shown that the proposed
policy attains asymptotically logarithmic weak regret rate when the rewards are
bounded independent and identically distributed or finite state Markovian.
Simulation results verifying uniformly logarithmic weak regret are also
presented. The proposed policy is a centrally coordinated index policy, in
which the index of a frequency band is comprised of a sample mean term and a
confidence term. The sample mean term promotes spectrum exploitation whereas
the confidence term encourages exploration. The confidence term is designed
such that the time interval between consecutive sensing instances of any
suboptimal band grows exponentially. This exponential growth between suboptimal
sensing time instances leads to logarithmically growing weak regret. Simulation
results demonstrate that the proposed policy performs better than other similar
methods in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4385</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4385</id><created>2012-11-19</created><authors><author><keyname>Shrivastava</keyname><forenames>Vivek</forenames></author><author><keyname>Sharma</keyname><forenames>Navdeep</forenames></author></authors><title>Artificial Neural Network Based Optical Character Recognition</title><categories>cs.CV cs.NE</categories><comments>Signal &amp; Image Processing : An International Journal (SIPIJ) Vol.3,
  No.5, October 2012</comments><doi>10.5121/sipij.2012.3506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Character Recognition deals in recognition and classification of
characters from an image. For the recognition to be accurate, certain
topological and geometrical properties are calculated, based on which a
character is classified and recognized. Also, the Human psychology perceives
characters by its overall shape and features such as strokes, curves,
protrusions, enclosures etc. These properties, also called Features are
extracted from the image by means of spatial pixel-based calculation. A
collection of such features, called Vectors, help in defining a character
uniquely, by means of an Artificial Neural Network that uses these Feature
Vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4392</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4392</id><created>2012-11-19</created><authors><author><keyname>Kang</keyname><forenames>Du Ho</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>Cost Efficient High Capacity Indoor Wireless Access: Denser Wi-Fi or
  Coordinated Pico-cellular?</title><categories>cs.IT cs.NI math.IT</categories><comments>26 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapidly increasing traffic demand has forced indoor operators to deploy more
and more Wi-Fi access points (APs). As AP density increases, inter-AP
interference rises and may limit the capacity. Alternatively, cellular
technologies using centralized interference coordination can provide the same
capacity with the fewer number of APs at the price of more expensive equipment
and installation cost. It is still not obvious at what demand level more
sophisticated coordination pays off in terms of total system cost. To make this
comparison, we assess the required AP density of three candidate systems for a
given average demand: a Wi-Fi network, a conventional pico-cellular network
with frequency planning, and an advanced system employing multi-cell joint
processing. Numerical results show that dense Wi-Fi is the cheapest solution at
a relatively low demand level. However, the AP density grows quickly at a
critical demand level regardless of propagation conditions. Beyond this Wi-Fi
network limit, the conventional pico-cellular network works and is cheaper than
the joint processing in obstructed environments, e.g., furnished offices with
walls. In line of sight condition such as stadiums, the joint processing
becomes the most viable solution. The drawback is that extremely accurate
channel state information at transmitters is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4410</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4410</id><created>2012-11-19</created><updated>2013-01-25</updated><authors><author><keyname>Platanios</keyname><forenames>Emmanouil A.</forenames></author><author><keyname>Chatzis</keyname><forenames>Sotirios P.</forenames></author></authors><title>Mixture Gaussian Process Conditional Heteroscedasticity</title><categories>cs.LG stat.ML</categories><comments>Technical Report, under preparation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized autoregressive conditional heteroscedasticity (GARCH) models have
long been considered as one of the most successful families of approaches for
volatility modeling in financial return series. In this paper, we propose an
alternative approach based on methodologies widely used in the field of
statistical machine learning. Specifically, we propose a novel nonparametric
Bayesian mixture of Gaussian process regression models, each component of which
models the noise variance process that contaminates the observed data as a
separate latent Gaussian process driven by the observed data. This way, we
essentially obtain a mixture Gaussian process conditional heteroscedasticity
(MGPCH) model for volatility modeling in financial return series. We impose a
nonparametric prior with power-law nature over the distribution of the model
mixture components, namely the Pitman-Yor process prior, to allow for better
capturing modeled data distributions with heavy tails and skewness. Finally, we
provide a copula- based approach for obtaining a predictive posterior for the
covariances over the asset returns modeled by means of a postulated MGPCH
model. We evaluate the efficacy of our approach in a number of benchmark
scenarios, and compare its performance to state-of-the-art methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4414</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4414</id><created>2012-11-19</created><authors><author><keyname>Keller</keyname><forenames>Joaqu&#xed;n</forenames><affiliation>LIP6</affiliation></author><author><keyname>Diaconu</keyname><forenames>Raluca</forenames><affiliation>LIP6</affiliation></author><author><keyname>Valero</keyname><forenames>Mathieu</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author></authors><title>Towards a Scalable Dynamic Spatial Database System</title><categories>cs.DB cs.CG cs.DC</categories><comments>(2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of GPS-enabled smartphones and other similar mobile devices,
massive amounts of location data are available. However, no scalable solutions
for soft real-time spatial queries on large sets of moving objects have yet
emerged. In this paper we explore and measure the limits of actual algorithms
and implementations regarding different application scenarios. And finally we
propose a novel distributed architecture to solve the scalability issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4415</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4415</id><created>2012-11-19</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Discrete-Time Poles and Dynamics of Discontinuous Mode Boost and Buck
  Converters Under Various Control Schemes</title><categories>cs.SY math.DS nlin.CD</categories><comments>An extension of an IEEE paper (for the DCM buck converter) of the
  author, DOI: 10.1109/TPEL.2010.2096517, to three other cases: boost
  converters, non-resistive load, and variable switching frequency</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear systems, such as switching DC-DC boost or buck converters, have
rich dynamics. A simple one-dimensional discrete-time model is used to analyze
the boost or buck converter in discontinuous conduction mode. Seven different
control schemes (open-loop power stage, voltage mode control, current mode
control, constant power load, constant current load, constant-on-time control,
and boundary conduction mode) are analyzed systematically. The linearized
dynamics is obtained simply by taking partial derivatives with respect to
dynamic variables. In the discrete-time model, there is only a single pole and
no zero. The single closed-loop pole is a linear combination of three terms:
the open-loop pole, a term due to the control scheme, and a term due to the
non-resistive load. Even with a single pole, the phase response of the
discrete-time model can go beyond -90 degrees as in the two-pole average
models. In the boost converter with a resistive load under current mode
control, adding the compensating ramp has no effect on the pole location.
Increasing the ramp slope decreases the DC gain of control-to-output transfer
function and increases the audio-susceptibility. Similar analysis is applied to
the buck converter with a non-resistive load or variable switching frequency.
The derived dynamics agrees closely with the exact switching model and the past
research results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4422</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4422</id><created>2012-11-19</created><authors><author><keyname>Ivanov</keyname><forenames>S. V.</forenames></author><author><keyname>Boukhanovsky</keyname><forenames>A. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Continuous Models of Epidemic Spreading in Heterogeneous Dynamically
  Changing Random Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling spreading processes in complex random networks plays an essential
role in understanding and prediction of many real phenomena like epidemics or
rumor spreading. The dynamics of such systems may be represented
algorithmically by Monte-Carlo simulations on graphs or by ordinary
differential equations (ODEs). Despite many results in the area of network
modeling the selection of the best computational representation of the model
dynamics remains a challenge. While a closed form description is often
straightforward to derive, it generally cannot be solved analytically; as a
consequence the network dynamics requires a numerical solution of the ODEs or a
direct Monte-Carlo simulation on the networks. Moreover, Monte-Carlo
simulations and ODE solutions are not equivalent since ODEs produce a
deterministic solution while Monte-Carlo simulations are stochastic by nature.
Despite some recent advantages in Monte-Carlo simulations, particularly in the
flexibility of implementation, the computational cost of an ODE solution is
much lower and supports accurate and detailed output analysis such as
uncertainty or sensitivity analyses, parameter identification etc. In this
paper we propose a novel approach to model spreading processes in complex
random heterogeneous networks using systems of nonlinear ordinary differential
equations. We successfully apply this approach to predict the dynamics of
HIV-AIDS spreading in sexual networks, and compare it to historical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4433</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4433</id><created>2012-11-19</created><authors><author><keyname>Zheng</keyname><forenames>Baigong</forenames></author><author><keyname>Yang</keyname><forenames>Yuansheng</forenames></author><author><keyname>Xu</keyname><forenames>Xirong</forenames></author></authors><title>An upper bound for the crossing number of bubble-sort graph Bn</title><categories>cs.DM math.CO</categories><comments>20 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crossing number of a graph G is the minimum number of pairwise
intersections of edges in a drawing of G. Motivated by the recent work [Faria,
L., Figueiredo, C.M.H. de, Sykora, O., Vrt'o, I.: An improved upper bound on
the crossing number of the hypercube. J. Graph Theory 59, 145-161 (2008)], we
give an upper bound of the crossing number of n-dimensional bubble-sort graph
Bn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4437</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4437</id><created>2012-11-19</created><authors><author><keyname>Yang</keyname><forenames>Yuansheng</forenames></author><author><keyname>Zheng</keyname><forenames>Baigong</forenames></author><author><keyname>Lin</keyname><forenames>Xiaohui</forenames></author><author><keyname>Xu</keyname><forenames>Xirong</forenames></author></authors><title>The crossing numbers of $K_{n,n}-nK_2$, $K_{n}\times P_2$, $K_{n}\times
  P_3$ and $K_n\times C_4$</title><categories>cs.DM math.CO</categories><comments>14 pages, 33 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crossing number of a graph $G$ is the minimum number of pairwise
intersections of edges among all drawings of $G$. In this paper, we study the
crossing number of $K_{n,n}-nK_2$, $K_n\times P_2$, $K_n\times P_3$ and
$K_n\times C_4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4438</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4438</id><created>2012-11-19</created><authors><author><keyname>Yang</keyname><forenames>Yuansheng</forenames></author><author><keyname>Zheng</keyname><forenames>Baigong</forenames></author><author><keyname>Xu</keyname><forenames>Xirong</forenames></author></authors><title>The crossing number of the generalized Petersen graph P(10, 3) is six</title><categories>cs.DM math.CO</categories><comments>11 pages, 31 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crossing number of a graph is the least number of crossings of edges
among all drawings of the graph in the plane. In this article, we prove that
the crossing number of the generalized Petersen graph P(10, 3) is equal to 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4441</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4441</id><created>2012-11-19</created><authors><author><keyname>Krishnan</keyname><forenames>B. Santhana</forenames></author><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author><author><keyname>Dey</keyname><forenames>Bikash K.</forenames></author></authors><title>On the Separability of Targets Using Binary Proximity Sensors</title><categories>cs.IT math.IT</categories><comments>17 pages, 3 figures, Submitted to IEEE TMC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem where a network of sensors has to detect the presence
of targets at any of $n$ possible locations in a finite region. All such
locations may not be occupied by a target. The data from sensors is fused to
determine the set of locations that have targets. We term this the separability
problem. In this paper, we address the separability of an asymptotically large
number of static target locations by using binary proximity sensors. Two models
for target locations are considered: (i) when target locations lie on a
uniformly spaced grid; and, (ii) when target locations are i.i.d. uniformly
distributed in the area. Sensor locations are i.i.d uniformly distributed in
the same finite region, independent of target locations. We derive conditions
on the sensing radius and the number of sensors required to achieve
separability. Order-optimal scaling laws, on the number of sensors as a
function of the number of target locations, for two types of separability
requirements are derived. The robustness or security aspects of the above
problem is also addressed. It is shown that in the presence of adversarial
sensors, which toggle their sensed reading and inject binary noise, the scaling
laws for separability remain unaffected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4442</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4442</id><created>2012-11-19</created><authors><author><keyname>Balabadrapatruni</keyname><forenames>Sai Suhas</forenames></author></authors><title>Performance Evaluation of DOA Estimation using MATLAB</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the performance analysis of directions of arrival
estimation techniques, Subspace and the Non-Subspace methods. In this paper,
exploring the analysis category of high resolution and super resolution
algorithms, presentation of description, comparison and the performance and
resolution analyses of these algorithms are made. Sensitivity to various
perturbations and the effect of parameters related to the design of the sensor
array itself such as the number of array elements and their spacing are also
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4445</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4445</id><created>2012-11-19</created><authors><author><keyname>Vouyioukas</keyname><forenames>Demosthenes</forenames></author></authors><title>Efficient Spectrum Sharing in the Presence of Multiple Narrowband
  Interference</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN), Vol.
  4, No. 5, pp. 61-78, 2012</journal-ref><doi>10.5121/ijwmn.2012.4505</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the spectrum usage efficiency by applying wideband
methods and systems to the existing analog systems and applications. The
essential motivation of this work is to define the prospective coexistence
between analog FM and digital Spread Spectrum systems in an efficient way
sharing the same frequency band. The potential overlaid Spread Spectrum (SS)
system can spectrally coincide within the existing narrowband Frequency
Modulated (FM) broadcasting system upon several limitations, originating a key
motivation for the use of the FM radio frequency band in many applications,
encompassing wireless personal and sensors networks. The performance of the SS
system due to the overlaying analog FM system, consisting of multiple
narrowband FM stations, is investigated in order to derive the relevant bit
error probability and maximum achievable data rates. The SS system uses direct
sequence (DS) spreading, through maximal length pseudorandom sequences with
long spreading codes. The SS signal is evaluated throughout theoretical and
simulation-based performance analysis, for various types of spreading
scenarios, for different carrier frequency offset ({\Delta}f) and
signal-to-interference ratios, in order to derive valuable results for future
developing and planning of an overlay scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4461</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4461</id><created>2012-11-19</created><updated>2014-03-19</updated><authors><author><keyname>Cools</keyname><forenames>Siegfried</forenames></author><author><keyname>Reps</keyname><forenames>Bram</forenames></author><author><keyname>Vanroose</keyname><forenames>Wim</forenames></author></authors><title>An efficient multigrid calculation of the far field map for Helmholtz
  and Schr\&quot;odinger equations</title><categories>math.NA cs.NA math-ph math.MP</categories><comments>SIAM Journal on Scientific Computing, 29 pages, 10 figures, 5 tables</comments><journal-ref>SIAM Journal on Scientific Computing, 36:3(2014), p. 367-395</journal-ref><doi>10.1137/130921064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new highly efficient calculation method for the
far field amplitude pattern that arises from scattering problems governed by
the d-dimensional Helmholtz equation and, by extension, Schr\&quot;odinger's
equation. The new technique is based upon a reformulation of the classical
real-valued Green's function integral for the far field amplitude to an
equivalent integral over a complex domain. It is shown that the scattered wave,
which is essential for the calculation of the far field integral, can be
computed very efficiently along this complex contour (or manifold, in multiple
dimensions). Using the iterative multigrid method as a solver for the
discretized damped scattered wave system, the proposed approach results in a
fast and scalable calculation method for the far field map. The complex contour
method is successfully validated on Helmholtz and Schr\&quot;odinger model problems
in two and three spatial dimensions, and multigrid convergence results are
provided to substantiate the wavenumber scalability and overall performance of
the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4464</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4464</id><created>2012-11-19</created><authors><author><keyname>Erdbrink</keyname><forenames>C. D.</forenames></author><author><keyname>Krzhizhanovskaya</keyname><forenames>V. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Free-surface flow simulations for discharge-based operation of hydraulic
  structure gates</title><categories>cs.CE physics.flu-dyn</categories><comments>25 pages, 16 figures</comments><msc-class>76D05</msc-class><journal-ref>Journal of Hydroinformatics, V. 16, N 1, pp. 189-206, 2014</journal-ref><doi>10.2166/hydro.2013.215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine non-hydrostatic flow simulations of the free surface with a
discharge model based on elementary gate flow equations for decision support in
operation of hydraulic structure gates. A water level-based gate control used
in most of today's general practice does not take into account the fact that
gate operation scenarios producing similar total discharged volumes and similar
water levels may have different local flow characteristics. Accurate and timely
prediction of local flow conditions around hydraulic gates is important for
several aspects of structure management: ecology, scour, flow-induced gate
vibrations and waterway navigation. The modelling approach is described and
tested for a multi-gate sluice structure regulating discharge from a river to
the sea. The number of opened gates is varied and the discharge is stabilized
with automated control by varying gate openings. The free-surface model was
validated for discharge showing a correlation coefficient of 0.994 compared to
experimental data. Additionally, we show the analysis of CFD results for
evaluating bed stability and gate vibrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4470</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4470</id><created>2012-11-19</created><updated>2014-01-13</updated><authors><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author><author><keyname>Velder</keyname><forenames>Sergey</forenames></author></authors><title>Loop invariants: analysis, classification, and examples</title><categories>cs.SE</categories><journal-ref>ACM Computing Surveys 46, 3, Article 34 (January 2014)</journal-ref><doi>10.1145/2506375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software verification has emerged as a key concern for ensuring the continued
progress of information technology. Full verification generally requires, as a
crucial step, equipping each loop with a &quot;loop invariant&quot;. Beyond their role in
verification, loop invariants help program understanding by providing
fundamental insights into the nature of algorithms. In practice, finding sound
and useful invariants remains a challenge. Fortunately, many invariants seem
intuitively to exhibit a common flavor. Understanding these fundamental
invariant patterns could therefore provide help for understanding and verifying
a large variety of programs.
  We performed a systematic identification, validation, and classification of
loop invariants over a range of fundamental algorithms from diverse areas of
computer science. This article analyzes the patterns, as uncovered in this
study, governing how invariants are derived from postconditions; it proposes a
taxonomy of invariants according to these patterns, and presents its
application to the algorithms reviewed. The discussion also shows the need for
high-level specifications based on &quot;domain theory&quot;. It describes how the
invariants and the corresponding algorithms have been mechanically verified
using an automated program prover; the proof source files are available. The
contributions also include suggestions for invariant inference and for
model-based specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4473</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4473</id><created>2012-11-19</created><updated>2013-04-25</updated><authors><author><keyname>Lu</keyname><forenames>Lian</forenames></author><author><keyname>Tu</keyname><forenames>Jinlong</forenames></author><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author><author><keyname>Lin</keyname><forenames>Xiaojun</forenames></author></authors><title>Online Energy Generation Scheduling for Microgrids with Intermittent
  Energy Sources and Co-Generation</title><categories>cs.OH</categories><comments>26 pages, 13 figures. It will appear in Proc. of ACM SIGMETRICS, 2013</comments><acm-class>C.4; F.1.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microgrids represent an emerging paradigm of future electric power systems
that can utilize both distributed and centralized generations. Two recent
trends in microgrids are the integration of local renewable energy sources
(such as wind farms) and the use of co-generation (i.e., to supply both
electricity and heat). However, these trends also bring unprecedented
challenges to the design of intelligent control strategies for microgrids.
Traditional generation scheduling paradigms rely on perfect prediction of
future electricity supply and demand. They are no longer applicable to
microgrids with unpredictable renewable energy supply and with co-generation
(that needs to consider both electricity and heat demand). In this paper, we
study online algorithms for the microgrid generation scheduling problem with
intermittent renewable energy sources and co-generation, with the goal of
maximizing the cost-savings with local generation. Based on the insights from
the structure of the offline optimal solution, we propose a class of
competitive online algorithms, called CHASE (Competitive Heuristic Algorithm
for Scheduling Energy-generation), that track the offline optimal in an online
fashion. Under typical settings, we show that CHASE achieves the best
competitive ratio among all deterministic online algorithms, and the ratio is
no larger than a small constant 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4475</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4475</id><created>2012-11-19</created><authors><author><keyname>Kimmig</keyname><forenames>Angelika</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author></authors><title>Algebraic Model Counting</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted model counting (WMC) is a well-known inference task on knowledge
bases, used for probabilistic inference in graphical models. We introduce
algebraic model counting (AMC), a generalization of WMC to a semiring
structure. We show that AMC generalizes many well-known tasks in a variety of
domains such as probabilistic inference, soft constraints and network and
database analysis. Furthermore, we investigate AMC from a knowledge compilation
perspective and show that all AMC tasks can be evaluated using sd-DNNF
circuits. We identify further characteristics of AMC instances that allow for
the use of even more succinct circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4487</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4487</id><created>2012-11-19</created><updated>2013-04-07</updated><authors><author><keyname>Di Ventra</keyname><forenames>M.</forenames></author><author><keyname>Pershin</keyname><forenames>Y. V.</forenames></author></authors><title>Memcomputing: a computing paradigm to store and process information on
  the same physical platform</title><categories>cs.ET cond-mat.mes-hall q-bio.NC</categories><comments>The first part of this paper has been published in Nature Physics 9,
  200-202 (2013). The second part has been expanded and is now included in
  arXiv:1304.1675</comments><journal-ref>Nature Physics 9, 200-202 (2013)</journal-ref><doi>10.1038/nphys2566</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In present day technology, storing and processing of information occur on
physically distinct regions of space. Not only does this result in space
limitations; it also translates into unwanted delays in retrieving and
processing of relevant information. There is, however, a class of two-terminal
passive circuit elements with memory, memristive, memcapacitive and
meminductive systems -- collectively called memelements -- that perform both
information processing and storing of the initial, intermediate and final
computational data on the same physical platform. Importantly, the states of
these memelements adjust to input signals and provide analog capabilities
unavailable in standard circuit elements, resulting in adaptive circuitry, and
providing analog massively-parallel computation. All these features are
tantalizingly similar to those encountered in the biological realm, thus
offering new opportunities for biologically-inspired computation. Of particular
importance is the fact that these memelements emerge naturally in nanoscale
systems, and are therefore a consequence and a natural by-product of the
continued miniaturization of electronic devices. We will discuss the various
possibilities offered by memcomputing, discuss the criteria that need to be
satisfied to realize this paradigm, and provide an example showing the solution
of the shortest-path problem and demonstrate the healing property of the
solution path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4488</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4488</id><created>2012-11-19</created><authors><author><keyname>Ram&#xed;rez</keyname><forenames>Jessica C.</forenames></author><author><keyname>Matsumoto</keyname><forenames>Yuji</forenames></author></authors><title>A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A
  Comparable Corpora</title><categories>cs.CL cs.AI</categories><comments>International Journal on Natural Language Computing (IJNLC) Vol.1,
  No.3, October 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The performance of a Statistical Machine Translation System (SMT) system is
proportionally directed to the quality and length of the parallel corpus it
uses. However for some pair of languages there is a considerable lack of them.
The long term goal is to construct a Japanese-Spanish parallel corpus to be
used for SMT, whereas, there are a lack of useful Japanese-Spanish parallel
Corpus. To address this problem, In this study we proposed a method for
extracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging
and Rule-Based approach. The main focus of this approach is the syntactic
features of both languages. Human evaluation was performed over a sample and
shows promising results, in comparison with the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4493</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4493</id><created>2012-11-19</created><updated>2012-11-19</updated><authors><author><keyname>Bhuyan</keyname><forenames>Monowar H.</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>D. K.</forenames></author><author><keyname>Kalita</keyname><forenames>J. K.</forenames></author></authors><title>Survey on Incremental Approaches for Network Anomaly Detection</title><categories>cs.CR cs.NI</categories><comments>14 pages, 1 figure, 11 tables referred journal publication</comments><msc-class>68T10</msc-class><acm-class>K.6.5</acm-class><journal-ref>International Journal of Communication Networks and Information
  Security (KUST), vol. 3, no. 3, pp. 226-239, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the communication industry has connected distant corners of the globe
using advances in network technology, intruders or attackers have also
increased attacks on networking infrastructure commensurately. System
administrators can attempt to prevent such attacks using intrusion detection
tools and systems. There are many commercially available signature-based
Intrusion Detection Systems (IDSs). However, most IDSs lack the capability to
detect novel or previously unknown attacks. A special type of IDSs, called
Anomaly Detection Systems, develop models based on normal system or network
behavior, with the goal of detecting both known and unknown attacks. Anomaly
detection systems face many problems including high rate of false alarm,
ability to work in online mode, and scalability. This paper presents a
selective survey of incremental approaches for detecting anomaly in normal
system or network traffic. The technological trends, open problems, and
challenges over anomaly detection using incremental approach are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4499</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4499</id><created>2012-11-19</created><authors><author><keyname>Rajaei</keyname><forenames>Boshra</forenames></author><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Pourreza</keyname><forenames>Hamid-Reza</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Rate-Distortion Analysis of Multiview Coding in a DIBR Framework</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth image based rendering techniques for multiview applications have been
recently introduced for efficient view generation at arbitrary camera
positions. Encoding rate control has thus to consider both texture and depth
data. Due to different structures of depth and texture images and their
different roles on the rendered views, distributing the available bit budget
between them however requires a careful analysis. Information loss due to
texture coding affects the value of pixels in synthesized views while errors in
depth information lead to shift in objects or unexpected patterns at their
boundaries. In this paper, we address the problem of efficient bit allocation
between textures and depth data of multiview video sequences. We adopt a
rate-distortion framework based on a simplified model of depth and texture
images. Our model preserves the main features of depth and texture images.
Unlike most recent solutions, our method permits to avoid rendering at encoding
time for distortion estimation so that the encoding complexity is not
augmented. In addition to this, our model is independent of the underlying
inpainting method that is used at decoder. Experiments confirm our theoretical
results and the efficiency of our rate allocation strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4500</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4500</id><created>2012-11-19</created><authors><author><keyname>Broekens</keyname><forenames>Joost</forenames></author><author><keyname>Qu</keyname><forenames>Chao</forenames></author><author><keyname>Brinkman</keyname><forenames>Willem-Paul</forenames></author></authors><title>Dynamic Facial Expression of Emotion Made Easy</title><categories>cs.HC cs.GR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Facial emotion expression for virtual characters is used in a wide variety of
areas. Often, the primary reason to use emotion expression is not to study
emotion expression generation per se, but to use emotion expression in an
application or research project. What is then needed is an easy to use and
flexible, but also validated mechanism to do so. In this report we present such
a mechanism. It enables developers to build virtual characters with dynamic
affective facial expressions. The mechanism is based on Facial Action Coding.
It is easy to implement, and code is available for download. To show the
validity of the expressions generated with the mechanism we tested the
recognition accuracy for 6 basic emotions (joy, anger, sadness, surprise,
disgust, fear) and 4 blend emotions (enthusiastic, furious, frustrated, and
evil). Additionally we investigated the effect of VC distance (z-coordinate),
the effect of the VC's face morphology (male vs. female), the effect of a
lateral versus a frontal presentation of the expression, and the effect of
intensity of the expression. Participants (n=19, Western and Asian subjects)
rated the intensity of each expression for each condition (within subject
setup) in a non forced choice manner. All of the basic emotions were uniquely
perceived as such. Further, the blends and confusion details of basic emotions
are compatible with findings in psychology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4503</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4503</id><created>2012-11-19</created><authors><author><keyname>Bhuyan</keyname><forenames>Monowar H.</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>D. K.</forenames></author></authors><title>An Effective Fingerprint Classification and Search Method</title><categories>cs.CV cs.CR</categories><comments>10 pages, 8 figures, 6 tables, referred journal publication</comments><msc-class>68U35</msc-class><acm-class>I.5.3</acm-class><journal-ref>International Journal of Computer Science and Network Security,
  Vol. 9, No.11, pp. 39-48, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an effective fingerprint classification method designed
based on a hierarchical agglomerative clustering technique. The performance of
the technique was evaluated in terms of several real-life datasets and a
significant improvement in reducing the misclassification error has been
noticed. This paper also presents a query based faster fingerprint search
method over the clustered fingerprint databases. The retrieval accuracy of the
search method has been found effective in light of several real-life databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4516</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4516</id><created>2012-11-19</created><authors><author><keyname>Salia</keyname><forenames>Nika</forenames></author><author><keyname>Gamkrelidze</keyname><forenames>Alexander</forenames></author><author><keyname>Ephremidze</keyname><forenames>Lasha</forenames></author></authors><title>Numerical comparison of different algorithms for construction of wavelet
  matrices</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Factorization of compact wavelet matrices into primitive ones has been known
for more than 20 years. This method makes it possible to generate wavelet
matrix coefficients and also to specify them by their first row. Recently, a
new parametrization of compact wavelet matrices of the same order and degree
has been introduced by the last author. This method also enables us to fulfill
the above mentioned tasks of matrix constructions. In the present paper, we
briefly describe the corresponding algorithms based on two different methods,
and numerically compare their performance
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4517</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4517</id><created>2012-11-19</created><authors><author><keyname>Silva</keyname><forenames>Pedro V.</forenames></author><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author></authors><title>Fixed points of endomorphisms of trace monoids</title><categories>math.GR cs.FL</categories><msc-class>20M35, 68Q85, 54E50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that the fixed point submonoid and the periodic point submonoid
of a trace monoid endomorphism are always finitely generated. Considering the
Foata normal form metric on trace monoids and uniformly continuous
endomorphisms, a finiteness theorem is proved for the infinite fixed points of
the continuous extension to real traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4518</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4518</id><created>2012-11-19</created><updated>2013-03-25</updated><authors><author><keyname>Zhang</keyname><forenames>Zhenliang</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Moran</keyname><forenames>William</forenames></author></authors><title>Hypothesis Testing in Feedforward Networks with Broadcast Failures</title><categories>cs.IT cs.LG math.IT</categories><doi>10.1109/JSTSP.2013.2258657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a countably infinite set of nodes, which sequentially make decisions
between two given hypotheses. Each node takes a measurement of the underlying
truth, observes the decisions from some immediate predecessors, and makes a
decision between the given hypotheses. We consider two classes of broadcast
failures: 1) each node broadcasts a decision to the other nodes, subject to
random erasure in the form of a binary erasure channel; 2) each node broadcasts
a randomly flipped decision to the other nodes in the form of a binary
symmetric channel. We are interested in whether there exists a decision
strategy consisting of a sequence of likelihood ratio tests such that the node
decisions converge in probability to the underlying truth. In both cases, we
show that if each node only learns from a bounded number of immediate
predecessors, then there does not exist a decision strategy such that the
decisions converge in probability to the underlying truth. However, in case 1,
we show that if each node learns from an unboundedly growing number of
predecessors, then the decisions converge in probability to the underlying
truth, even when the erasure probabilities converge to 1. We also derive the
convergence rate of the error probability. In case 2, we show that if each node
learns from all of its previous predecessors, then the decisions converge in
probability to the underlying truth when the flipping probabilities of the
binary symmetric channels are bounded away from 1/2. In the case where the
flipping probabilities converge to 1/2, we derive a necessary condition on the
convergence rate of the flipping probabilities such that the decisions still
converge to the underlying truth. We also explicitly characterize the
relationship between the convergence rate of the error probability and the
convergence rate of the flipping probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4520</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4520</id><created>2012-11-19</created><updated>2013-04-28</updated><authors><author><keyname>Zhang</keyname><forenames>Chuan</forenames></author><author><keyname>Dangelmayr</keyname><forenames>Gerhard</forenames></author><author><keyname>Oprea</keyname><forenames>Iuliana</forenames></author></authors><title>Storing cycles in Hopfield-type networks with pseudoinverse learning
  rule: admissibility and network topology</title><categories>cs.NE</categories><comments>48 pages, 3 figures</comments><msc-class>15A04</msc-class><journal-ref>Neural Networks, Volume 46, October 2013, Pages 283-298</journal-ref><doi>10.1016/j.neunet.2013.06.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic patterns of neuronal activity are ubiquitous in animal nervous
systems, and partially responsible for generating and controlling rhythmic
movements such as locomotion, respiration, swallowing and so on. Clarifying the
role of the network connectivities for generating cyclic patterns is
fundamental for understanding the generation of rhythmic movements. In this
paper, the storage of binary cycles in neural networks is investigated. We call
a cycle $\Sigma$ admissible if a connectivity matrix satisfying the cycle's
transition conditions exists, and construct it using the pseudoinverse learning
rule. Our main focus is on the structural features of admissible cycles and
corresponding network topology. We show that $\Sigma$ is admissible if and only
if its discrete Fourier transform contains exactly $r={rank}(\Sigma)$ nonzero
columns. Based on the decomposition of the rows of $\Sigma$ into loops, where a
loop is the set of all cyclic permutations of a row, cycles are classified as
simple cycles, separable or inseparable composite cycles. Simple cycles contain
rows from one loop only, and the network topology is a feedforward chain with
feedback to one neuron if the loop-vectors in $\Sigma$ are cyclic permutations
of each other. Composite cycles contain rows from at least two disjoint loops,
and the neurons corresponding to the rows in $\Sigma$ from the same loop are
identified with a cluster. Networks constructed from separable composite cycles
decompose into completely isolated clusters. For inseparable composite cycles
at least two clusters are connected, and the cluster-connectivity is related to
the intersections of the spaces spanned by the loop-vectors of the clusters.
Simulations showing successfully retrieved cycles in continuous-time
Hopfield-type networks and in networks of spiking neurons are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4521</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4521</id><created>2012-11-19</created><authors><author><keyname>Clemons</keyname><forenames>Tyler</forenames></author><author><keyname>Faisal</keyname><forenames>S. M.</forenames></author><author><keyname>Tatikonda</keyname><forenames>Shirish</forenames></author><author><keyname>Aggarawl</keyname><forenames>Charu</forenames></author><author><keyname>Parthasarathy</keyname><forenames>Srinivasan</forenames></author></authors><title>Hash in a Flash: Hash Tables for Solid State Devices</title><categories>cs.DB cs.DS cs.IR</categories><comments>16 pages 10 figures</comments><acm-class>H.2.7; H.2.8; H.3.1; E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, information retrieval algorithms have taken center stage for
extracting important data in ever larger datasets. Advances in hardware
technology have lead to the increasingly wide spread use of flash storage
devices. Such devices have clear benefits over traditional hard drives in terms
of latency of access, bandwidth and random access capabilities particularly
when reading data. There are however some interesting trade-offs to consider
when leveraging the advanced features of such devices. On a relative scale
writing to such devices can be expensive. This is because typical flash devices
(NAND technology) are updated in blocks. A minor update to a given block
requires the entire block to be erased, followed by a re-writing of the block.
On the other hand, sequential writes can be two orders of magnitude faster than
random writes. In addition, random writes are degrading to the life of the
flash drive, since each block can support only a limited number of erasures.
TF-IDF can be implemented using a counting hash table. In general, hash tables
are a particularly challenging case for the flash drive because this data
structure is inherently dependent upon the randomness of the hash function, as
opposed to the spatial locality of the data. This makes it difficult to avoid
the random writes incurred during the construction of the counting hash table
for TF-IDF. In this paper, we will study the design landscape for the
development of a hash table for flash storage devices. We demonstrate how to
effectively design a hash table with two related hash functions, one of which
exhibits a data placement property with respect to the other. Specifically, we
focus on three designs based on this general philosophy and evaluate the
trade-offs among them along the axes of query performance, insert and update
times and I/O time through an implementation of the TF-IDF algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4524</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4524</id><created>2012-11-19</created><authors><author><keyname>Parseh</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Pashazadeh</keyname><forenames>Saeid</forenames></author></authors><title>Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using
  Particle Filtering</title><categories>cs.CV cs.AI</categories><comments>13 pages, 7 Figures, 1 Table</comments><journal-ref>International Journal of Information Technology, Control and
  Automation (IJITCA), Vol. 2, No. 4, pp. 37-49, 2012</journal-ref><doi>10.5121/ijitca.2012.2404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we applied a dynamic model for manoeuvring targets in SIR
particle filter algorithm for improving tracking accuracy of multiple
manoeuvring targets. In our proposed approach, a color distribution model is
used to detect changes of target's model . Our proposed approach controls
deformation of target's model. If deformation of target's model is larger than
a predetermined threshold, then the model will be updated. Global Nearest
Neighbor (GNN) algorithm is used as data association algorithm. We named our
proposed method as Deformation Detection Particle Filter (DDPF) . DDPF approach
is compared with basic SIR-PF algorithm on real airshow videos. Comparisons
results show that, the basic SIR-PF algorithm is not able to track the
manoeuvring targets when the rotation or scaling is occurred in target' s
model. However, DDPF approach updates target's model when the rotation or
scaling is occurred. Thus, the proposed approach is able to track the
manoeuvring targets more efficiently and accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4547</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4547</id><created>2012-10-29</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author><author><keyname>Dodig-Crnkovic</keyname><forenames>Gordana</forenames></author></authors><title>From the Closed Classical Algorithmic Universe to an Open World of
  Algorithmic Constellations</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze methodological and philosophical implications of
algorithmic aspects of unconventional computation. At first, we describe how
the classical algorithmic universe developed and analyze why it became closed
in the conventional approach to computation. Then we explain how new models of
algorithms turned the classical closed algorithmic universe into the open world
of algorithmic constellations, allowing higher flexibility and expressive
power, supporting constructivism and creativity in mathematical modeling. As
Goedels undecidability theorems demonstrate, the closed algorithmic universe
restricts essential forms of mathematical cognition. In contrast, the open
algorithmic universe, and even more the open world of algorithmic
constellations, remove such restrictions and enable new, richer understanding
of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4552</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4552</id><created>2012-11-19</created><authors><author><keyname>Synnaeve</keyname><forenames>Gabriel</forenames><affiliation>LIG, LPPA</affiliation></author><author><keyname>Bessiere</keyname><forenames>Pierre</forenames><affiliation>LPPA</affiliation></author></authors><title>A Dataset for StarCraft AI \&amp; an Example of Armies Clustering</title><categories>cs.AI</categories><comments>Artificial Intelligence in Adversarial Real-Time Games 2012, Palo
  Alto : United States (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates the exploration of the full state of recorded real-time
strategy (RTS) games, by human or robotic players, to discover how to reason
about tactics and strategy. We present a dataset of StarCraft games
encompassing the most of the games' state (not only player's orders). We
explain one of the possible usages of this dataset by clustering armies on
their compositions. This reduction of armies compositions to mixtures of
Gaussian allow for strategic reasoning at the level of the components. We
evaluated this clustering method by predicting the outcomes of battles based on
armies compositions' mixtures components
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4555</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4555</id><created>2012-11-19</created><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author></authors><title>Distributed Control of Generation in a Transmission Grid with a High
  Penetration of Renewables</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deviations of grid frequency from the nominal frequency are an indicator of
the global imbalance between genera- tion and load. Two types of control, a
distributed propor- tional control and a centralized integral control, are cur-
rently used to keep frequency deviations small. Although generation-load
imbalance can be very localized, both controls primarily rely on frequency
deviation as their in- put. The time scales of control require the outputs of
the centralized integral control to be communicated to distant generators every
few seconds. We reconsider this con- trol/communication architecture and
suggest a hybrid ap- proach that utilizes parameterized feedback policies that
can be implemented in a fully distributed manner because the inputs to these
policies are local observables at each generator. Using an ensemble of
forecasts of load and time-intermittent generation representative of possible
fu- ture scenarios, we perform a centralized off-line stochas- tic optimization
to select the generator-specific feedback parameters. These parameters need
only be communi- cated to generators once per control period (60 minutes in our
simulations). We show that inclusion of local power flows as feedback inputs is
crucial and reduces frequency deviations by a factor of ten. We demonstrate our
con- trol on a detailed transmission model of the Bonneville Power
Administration (BPA). Our findings suggest that a smart automatic and
distributed control, relying on ad- vanced off-line and system-wide
computations commu- nicated to controlled generators infrequently, may be a
viable control and communication architecture solution. This architecture is
suitable for a future situation when generation-load imbalances are expected to
grow because of increased penetration of time-intermittent generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4559</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4559</id><created>2012-11-19</created><authors><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Sack</keyname><forenames>J&#xf6;rg-R&#xfc;diger</forenames></author><author><keyname>Shahbaz</keyname><forenames>Kaveh</forenames></author></authors><title>Visiting All Sites with Your Dog</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a polygonal curve P, a pointset S, and an \epsilon &gt; 0, we study the
problem of finding a polygonal curve Q whose vertices are from S and has a
Frechet distance less or equal to \epsilon to curve P. In this problem, Q must
visit every point in S and we are allowed to reuse points of pointset in
building Q. First, we show that this problem in NP-Complete. Then, we present a
polynomial time algorithm for a special cases of this problem, when P is a
convex polygon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4591</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4591</id><created>2012-11-19</created><authors><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author><author><keyname>Qassim</keyname><forenames>Hind E.</forenames></author></authors><title>Five Modulus Method For Image Compression</title><categories>cs.CV cs.MM</categories><comments>10 pages, 2 figures, 9 tables</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ),
  Vol.3, No.5, October 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data is compressed by reducing its redundancy, but this also makes the data
less reliable, more prone to errors. In this paper a novel approach of image
compression based on a new method that has been created for image compression
which is called Five Modulus Method (FMM). The new method consists of
converting each pixel value in an 8-by-8 block into a multiple of 5 for each of
the R, G and B arrays. After that, the new values could be divided by 5 to get
new values which are 6-bit length for each pixel and it is less in storage
space than the original value which is 8-bits. Also, a new protocol for
compression of the new values as a stream of bits has been presented that gives
the opportunity to store and transfer the new compressed image easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4627</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4627</id><created>2012-11-19</created><updated>2015-04-28</updated><authors><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Blackburn</keyname><forenames>Jeremy</forenames></author><author><keyname>Borcea</keyname><forenames>Cristian</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author></authors><title>Enabling Social Applications via Decentralized Social Data Management</title><categories>cs.SI cs.CY cs.DC physics.soc-ph</categories><comments>27 pages, single ACM column, 9 figures, accepted in Special Issue of
  Foundations of Social Computing, ACM Transactions on Internet Technology</comments><acm-class>H.3.4</acm-class><journal-ref>ACM Trans. Internet Technol. 15, 1, (March 2015)</journal-ref><doi>10.1145/2700057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unprecedented information wealth produced by online social networks,
further augmented by location/collocation data, is currently fragmented across
different proprietary services. Combined, it can accurately represent the
social world and enable novel socially-aware applications. We present
Prometheus, a socially-aware peer-to-peer service that collects social
information from multiple sources into a multigraph managed in a decentralized
fashion on user-contributed nodes, and exposes it through an interface
implementing non-trivial social inferences while complying with user-defined
access policies. Simulations and experiments on PlanetLab with emulated
application workloads show the system exhibits good end-to-end response time,
low communication overhead and resilience to malicious attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4629</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4629</id><created>2012-11-19</created><updated>2016-02-05</updated><authors><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author></authors><title>Single Exponential FPT Algorithm for Interval Vertex Deletion and
  Interval Completion Problem</title><categories>cs.DS cs.DM math.CO</categories><comments>There are faster algorithms available</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be an input graph with n vertices and m edges and let k be a fixed
parameter. We provide a single exponential FPT algorithm with running time
O(c^kn(n+m)), c= min {18,k} that turns graph G into an interval graph by
deleting at most k vertices from G. This solves an open problem posed by D.Marx
[19]. We also provide a single exponential FPT algorithm with running time
O(c^kn(n+m)), c= min {17,k} that turns G into an interval graph by adding at
most$k edges. The first FPT algorithm with run time O(k^{2k}n^3m) appeared in
STOC 2007 [24]. Our algorithm is the the first single exponential FPT algorithm
that improves the running time of the previous algorithm. The algorithms are
based on a structural decomposition of G into smaller subgraphs when G is free
from small interval graph obstructions. The decomposition allows us to manage
the search tree more efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4641</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4641</id><created>2012-11-19</created><authors><author><keyname>Yang</keyname><forenames>Yuansheng</forenames></author><author><keyname>Zheng</keyname><forenames>Baigong</forenames></author><author><keyname>Xu</keyname><forenames>Xirong</forenames></author><author><keyname>Lin</keyname><forenames>Xiaohui</forenames></author></authors><title>The crossing numbers of $K_m\times P_n$ and $K_m\times C_n$</title><categories>cs.DM</categories><comments>16 pages, 30 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\it crossing number} of a graph $G$ is the minimum number of pairwise
intersections of edges in a drawing of $G$. In this paper, we study the
crossing numbers of $K_{m}\times P_n$ and $K_{m}\times C_n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4642</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4642</id><created>2012-11-19</created><authors><author><keyname>Yang</keyname><forenames>Yuansheng</forenames></author><author><keyname>Lv</keyname><forenames>Bo</forenames></author><author><keyname>Zheng</keyname><forenames>Baigong</forenames></author><author><keyname>Xu</keyname><forenames>Xirong</forenames></author><author><keyname>Zhang</keyname><forenames>Ke</forenames></author></authors><title>The crossing number of pancake graph $P_4$ is six</title><categories>cs.DM math.CO</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\it crossing number} of a graph $G$ is the least number of pairwise
crossings of edges among all the drawings of $G$ in the plane. The pancake
graph is an important topology for interconnecting processors in parallel
computers. In this paper, we prove the exact value of the crossing number of
pancake graph $P_4$ is six.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4649</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4649</id><created>2012-11-19</created><authors><author><keyname>Khist</keyname><forenames>Ashish</forenames></author><author><keyname>Zhang</keyname><forenames>Dongye</forenames></author></authors><title>Artificial-Noise Alignment for Secure Multicast using Multiple Antennas</title><categories>cs.IT math.IT</categories><comments>3 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an artificial-noise alignment scheme for multicasting a
common-confidential message to a group of receivers. Our scheme transmits a
superposition of information and noise symbols. The noise symbols are aligned
at each legitimate receiver and hence the information symbols can be decoded.
In contrast, the noise symbols completely mask the information symbols at the
eavesdroppers. Our proposed scheme does not require the knowledge of the
eavesdropper's channel gains at the transmitter for alignment, yet it achieves
the best-known lower bound on the secure degrees of freedom. Our scheme is also
a natural generalization of the approach of transmitting artificial noise in
the null-space of the legitimate receiver's channel, previously proposed in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4651</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4651</id><created>2012-11-19</created><updated>2013-02-15</updated><authors><author><keyname>Laroussinie</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>Universit&#xe9; Paris Diderot - Paris 7</affiliation></author><author><keyname>Meyer</keyname><forenames>Antoine</forenames><affiliation>Universit&#xe9; Paris Est - Marne-la-Vall&#xe9;e</affiliation></author><author><keyname>Petonnet</keyname><forenames>Eudes</forenames><affiliation>Universit&#xe9; Paris Diderot - Paris 7</affiliation></author></authors><title>Counting CTL</title><categories>cs.LO</categories><comments>34 pages</comments><proxy>LMCS</proxy><acm-class>F.4.1; D.2.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (February
  15, 2013) lmcs:1058</journal-ref><doi>10.2168/LMCS-9(1:3)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a range of quantitative extensions for the temporal logic
CTL. We enhance temporal modalities with the ability to constrain the number of
states satisfying certain sub-formulas along paths. By selecting the
combinations of Boolean and arithmetic operations allowed in constraints, one
obtains several distinct logics generalizing CTL. We provide a thorough
analysis of their expressiveness and succinctness, and of the complexity of
their model-checking and satisfiability problems (ranging from P-complete to
undecidable). Finally, we present two alternative logics with similar features
and provide a comparative study of the properties of both variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4654</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4654</id><created>2012-11-19</created><authors><author><keyname>Saha</keyname><forenames>Suprativ</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author></authors><title>Application of Data mining in Protein sequence Classification</title><categories>cs.CE</categories><comments>16 Pages, 7 Figures, 3 Tables</comments><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.4, No.5, October 2012</journal-ref><doi>10.5121/ijdms.2012.4508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein sequence classification involves feature selection for accurate
classification. Popular protein sequence classification techniques involve
extraction of specific features from the sequences. Researchers apply some
well-known classification techniques like neural networks, Genetic algorithm,
Fuzzy ARTMAP,Rough Set Classifier etc for accurate classification. This paper
presents a review is with three different classification models such as neural
network model, fuzzy ARTMAP model and Rough set classifier model. This is
followed by a new technique for classifying protein sequences. The proposed
model is typically implemented with an own designed tool and tries to reduce
the computational overheads encountered by earlier approaches and increase the
accuracy of classification
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4657</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4657</id><created>2012-11-19</created><updated>2014-05-01</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Li</keyname><forenames>Yeqing</forenames></author><author><keyname>Huang</keyname><forenames>Junzhou</forenames></author></authors><title>Forest Sparsity for Multi-channel Compressive Sensing</title><categories>cs.LG cs.CV cs.IT math.IT stat.ML</categories><comments>Accepted by IEEE Transactions on Signal Processing, 2014</comments><doi>10.1109/TSP.2014.2318138</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a new compressive sensing model for
multi-channel sparse data where each channel can be represented as a
hierarchical tree and different channels are highly correlated. Therefore, the
full data could follow the forest structure and we call this property as
\emph{forest sparsity}. It exploits both intra- and inter- channel correlations
and enriches the family of existing model-based compressive sensing theories.
The proposed theory indicates that only $\mathcal{O}(Tk+\log(N/k))$
measurements are required for multi-channel data with forest sparsity, where
$T$ is the number of channels, $N$ and $k$ are the length and sparsity number
of each channel respectively. This result is much better than
$\mathcal{O}(Tk+T\log(N/k))$ of tree sparsity, $\mathcal{O}(Tk+k\log(N/k))$ of
joint sparsity, and far better than $\mathcal{O}(Tk+Tk\log(N/k))$ of standard
sparsity. In addition, we extend the forest sparsity theory to the multiple
measurement vectors problem, where the measurement matrix is a block-diagonal
matrix. The result shows that the required measurement bound can be the same as
that for dense random measurement matrix, when the data shares equal energy in
each channel. A new algorithm is developed and applied on four example
applications to validate the benefit of the proposed model. Extensive
experiments demonstrate the effectiveness and efficiency of the proposed theory
and algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4658</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4658</id><created>2012-11-19</created><authors><author><keyname>Bhuyan</keyname><forenames>Monowar H.</forenames></author><author><keyname>Saharia</keyname><forenames>Sarat</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Dhruba Kr</forenames></author></authors><title>An Effective Method for Fingerprint Classification</title><categories>cs.CV cs.CR</categories><comments>9 pages, 7 figures, 6 tables referred journal publication. arXiv
  admin note: substantial text overlap with arXiv:1211.4503</comments><msc-class>68U35</msc-class><acm-class>I.5.3</acm-class><journal-ref>International A. Journal of e-Technology, Vol. 1, No. 3, pp.
  89-97, January, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an effective method for fingerprint classification using
data mining approach. Initially, it generates a numeric code sequence for each
fingerprint image based on the ridge flow patterns. Then for each class, a seed
is selected by using a frequent itemsets generation technique. These seeds are
subsequently used for clustering the fingerprint images. The proposed method
was tested and evaluated in terms of several real-life datasets and a
significant improvement in reducing the misclassification errors has been
noticed in comparison to its other counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4660</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4660</id><created>2012-11-19</created><authors><author><keyname>Georgiadis</keyname><forenames>Leonidas</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios S.</forenames></author><author><keyname>Libman</keyname><forenames>Lavy</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Minimal Evacuation Times and Stability</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a system where packets (jobs) arrive for processing using one of
the policies in a given class. We study the connection between the minimal
evacuation times and the stability region of the system under the given class
of policies. The result is used to establish the equality of information
theoretic capacity region and system stability region for the multiuser
broadcast erasure channel with feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4665</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4665</id><created>2012-11-20</created><authors><author><keyname>Wai</keyname><forenames>Hoi-To</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author></authors><title>A Decentralized Method for Joint Admission Control and Beamforming in
  Coordinated Multicell Downlink</title><categories>cs.IT math.IT</categories><comments>2012 IEEE Asilomar Conference on Signals, Systems, and Computers</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In cellular networks, admission control and beamforming optimization are
intertwined problems. While beamforming optimization aims at satisfying users'
quality-of-service (QoS) requirements or improving the QoS levels, admission
control looks at how a subset of users should be selected so that the
beamforming optimization problem can yield a reasonable solution in terms of
the QoS levels provided. However, in order to simplify the design, the two
problems are usually seen as separate problems. This paper considers joint
admission control and beamforming (JACoB) under a coordinated multicell MISO
downlink scenario. We formulate JACoB as a user number maximization problem,
where selected users are guaranteed to receive the QoS levels they requested.
The formulated problem is combinatorial and hard, and we derive a convex
approximation to the problem. A merit of our convex approximation formulation
is that it can be easily decomposed for per-base-station decentralized
optimization, namely, via block coordinate decent. The efficacy of the proposed
decentralized method is demonstrated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4674</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4674</id><created>2012-11-20</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author></authors><title>On Whitespace Identification Using Randomly Deployed Sensors</title><categories>cs.IT math.IT</categories><comments>25 pages, 5 figures. Submitted to IEEE J. Sel. Areas in Commun.,
  Series on Cognitive Radio</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the identification of the available whitespace, i.e., the
regions that are not covered by any of the existing transmitters, within a
given geographical area. To this end, $n$ sensors are deployed at random
locations within the area. These sensors detect for the presence of a
transmitter within their radio range $r_s$, and their individual decisions are
combined to estimate the available whitespace. The limiting behavior of the
recovered whitespace as a function of $n$ and $r_s$ is analyzed. It is shown
that both the fraction of the available whitespace that the nodes fail to
recover as well as their radio range both optimally scale as $\log(n)/n$ as $n$
gets large. The analysis is extended to the case of unreliable sensors, and it
is shown that, surprisingly, the optimal scaling is still $\log(n)/n$ even in
this case. A related problem of estimating the number of transmitters and their
locations is also analyzed, with the sum absolute error in localization as
performance metric. The optimal scaling of the radio range and the necessary
minimum transmitter separation is determined, that ensure that the sum absolute
error in transmitter localization is minimized, with high probability, as $n$
gets large. Finally, the optimal distribution of sensor deployment is
determined, given the distribution of the transmitters, and the resulting
performance benefit is characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4683</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4683</id><created>2012-11-20</created><authors><author><keyname>Patel</keyname><forenames>B. V.</forenames></author><author><keyname>Meshram</keyname><forenames>B. B.</forenames></author></authors><title>Content based video retrieval</title><categories>cs.MM cs.CV</categories><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.4, No.5, October 2012</journal-ref><doi>10.5121/ijma.2012.4506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content based video retrieval is an approach for facilitating the searching
and browsing of large image collections over World Wide Web. In this approach,
video analysis is conducted on low level visual properties extracted from video
frame. We believed that in order to create an effective video retrieval system,
visual perception must be taken into account. We conjectured that a technique
which employs multiple features for indexing and retrieval would be more
effective in the discrimination and search tasks of videos. In order to
validate this claim, content based indexing and retrieval systems were
implemented using color histogram, various texture features and other
approaches. Videos were stored in Oracle 9i Database and a user study measured
correctness of response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4695</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4695</id><created>2012-11-20</created><authors><author><keyname>Pai</keyname><forenames>Smitha N.</forenames></author><author><keyname>Shet</keyname><forenames>K. C.</forenames></author><author><keyname>Mruthyunjaya</keyname><forenames>H. S.</forenames></author></authors><title>Energy Aware Path Search for Sensor with parameters as used in
  agricultural field</title><categories>cs.NI</categories><comments>12 pages, 5 figures</comments><journal-ref>International Journal of Ad hoc, Sensor &amp; Ubiquitous Computing
  (IJASUC) Vol.3, No.5, October 2012</journal-ref><doi>10.5121/ijasuc.2012.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensors placed in agricultural field should have long network life. Failure
of node or link allows rerouting and establishing a new path from the source to
the sink. In this paper, a new path is established such that it is energy aware
during path discovery and is active for longer interval of time once it is
established. The parameters used for simulation are as those used in
agricultural application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4704</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4704</id><created>2012-11-20</created><authors><author><keyname>Herrmann</keyname><forenames>Dominik</forenames></author><author><keyname>Arndt</keyname><forenames>Christine</forenames></author><author><keyname>Federrath</keyname><forenames>Hannes</forenames></author></authors><title>IPv6 Prefix Alteration: An Opportunity to Improve Online Privacy</title><categories>cs.CR</categories><comments>This paper was peer-reviewed and presented at the 1st Workshop on
  Privacy and Data Protection Technology (PDPT 2012), co-located with the
  Amsterdam Privacy Conference (APC 2012), October 9, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is focused on privacy issues related to the prefix part of IPv6
addresses. Long-lived prefixes may introduce additional tracking opportunities
for communication partners and third parties. We outline a number of prefix
alteration schemes that may be deployed to maintain the unlinkability of users'
activities. While none of the schemes will solve all privacy problems on the
Internet on their own, we argue that the development of practical prefix
alteration techniques constitutes a worthwile avenue to pursue: They would
allow Internet Service Providers to increase the attainable privacy level well
above the status quo in today's IPv4 networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4705</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4705</id><created>2012-11-20</created><authors><author><keyname>K</keyname><forenames>Manjula Shenoy.</forenames></author><author><keyname>Shet</keyname><forenames>K. C.</forenames></author><author><keyname>Acharya</keyname><forenames>U. Dinesh</forenames></author></authors><title>Secured Ontology Mapping</title><categories>cs.OH</categories><comments>arXiv admin note: substantial text overlap with arXiv:cs/0407061 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Todays market evolution and high volatility of business requirements put an
increasing emphasis on the ability for systems to accommodate the changes
required by new organizational needs while maintaining security objectives
satisfiability. This is all the more true in case of collaboration and
interoperability between different organizations and thus between their
information systems. Ontology mapping has been used for interoperability and
several mapping systems have evolved to support the same. Usual solutions do
not take care of security. That is almost all systems do a mapping of
ontologies which are unsecured.We have developed a system for mapping secured
ontologies using graph similarity concept. Here we give no importance to the
strings that describe ontology concepts, properties etc. Because these strings
may be encrypted in the secured ontology. Instead we use the pure graphical
structure to determine mapping between various concepts of given two secured
ontologies. The paper also gives the measure of accuracy of experiment in a
tabular form in terms of precision, recall and F-measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4709</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4709</id><created>2012-11-20</created><authors><author><keyname>K</keyname><forenames>Manjula Shenoy.</forenames></author><author><keyname>Shet</keyname><forenames>K. C.</forenames></author><author><keyname>Acharya</keyname><forenames>U. Dinesh</forenames></author></authors><title>A New Similarity Measure for Taxonomy Based on Edge Counting</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new similarity measure based on edge counting in a
taxonomy like WorldNet or Ontology. Measurement of similarity between text
segments or concepts is very useful for many applications like information
retrieval, ontology matching, text mining, and question answering and so on.
Several measures have been developed for measuring similarity between two
concepts: out of these we see that the measure given by Wu and Palmer [1] is
simple, and gives good performance. Our measure is based on their measure but
strengthens it. Wu and Palmer [1] measure has a disadvantage that it does not
consider how far the concepts are semantically. In our measure we include the
shortest path between the concepts and the depth of whole taxonomy together
with the distances used in Wu and Palmer [1]. Also the measure has following
disadvantage i.e. in some situations, the similarity of two elements of an IS-A
ontology contained in the neighborhood exceeds the similarity value of two
elements contained in the same hierarchy. Our measure introduces a penalization
factor for this case based upon shortest length between the concepts and depth
of whole taxonomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4720</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4720</id><created>2012-11-20</created><authors><author><keyname>Orue-Esquivel</keyname><forenames>Priscill</forenames></author><author><keyname>Rubio</keyname><forenames>Bartolom&#xe9;</forenames></author></authors><title>WiSANCloud: a set of UML-based specifications for the integration of
  Wireless Sensor and Actor Networks (WSANs) with the Cloud Computing</title><categories>cs.SE cs.DC</categories><comments>WSAN-Cloud integration proposal, 31 pages, 31 figures</comments><msc-class>68M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Giving the current trend to combine the advantages of Wireless Sensor and
Actor Networks (WSANs)with the Cloud Computing technology, this work proposes a
set of specifications, based on the Unified Modeling Language - UML, in order
to provide the general framework for the design of the integration of said
components. One of the keys of the integration is the architecture of the WSAN,
due to its structural relationship with the Cloud in the definition of the
combination. Regarding the standard applied in the integration, UML and its
subset, Systems Modeling Language - SysML, are proposed by the Object
Management Group - OMG to deal with cloud applications; so, this indicates the
starting point of the process of the design of specifications for WSAN-Cloud
integration. Based on the current state of UML tools for analysis and design,
there are several aspects to take into account in order to define the
integration process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4723</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4723</id><created>2012-11-20</created><authors><author><keyname>Sarkar</keyname><forenames>Arindam</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Key Generation and Certification using Multilayer Perceptron in Wireless
  communication(KGCMLP)</title><categories>cs.CR</categories><comments>17 pages, International Journal of Security, Privacy and Trust
  Management (IJSPTM), Vol. 1, No 5, October 2012. arXiv admin note:
  substantial text overlap with arXiv:1208.2334; and text overlap with
  arXiv:0711.2411 by other authors</comments><doi>10.5121/ijsptm.2012.1503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a key generation and certification technique using multilayer
perceptron (KGCMLP) has been proposed in wireless communication of
data/information. In this proposed KGCMLP technique both sender and receiver
uses an identical multilayer perceptrons. Both perceptrons are start
synchronization by exchanging some control frames. During synchronization
process message integrity test and synchronization test has been carried out.
Only the synchronization test does not guarantee the security for this reason
key certification phase also been introduced in KGCMLP technique. After Key
generation and certification procedure synchronized identical weight vector
forms the key for encryption/decryption. Parametric tests have been done and
results are compared with some existing classical techniques, which show
comparable results for the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4728</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4728</id><created>2012-11-20</created><updated>2014-01-20</updated><authors><author><keyname>Matsui</keyname><forenames>Hajime</forenames></author></authors><title>Lemma for Linear Feedback Shift Registers and DFTs Applied to Affine
  Variety Codes</title><categories>cs.IT cs.DM math.AC math.CO math.IT</categories><comments>37 pages, 1 column, 10 figures, 2 tables, resubmitted to IEEE
  Transactions on Information Theory on Jan. 8, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish a lemma in algebraic coding theory that
frequently appears in the encoding and decoding of, e.g., Reed-Solomon codes,
algebraic geometry codes, and affine variety codes. Our lemma corresponds to
the non-systematic encoding of affine variety codes, and can be stated by
giving a canonical linear map as the composition of an extension through linear
feedback shift registers from a Grobner basis and a generalized inverse
discrete Fourier transform. We clarify that our lemma yields the error-value
estimation in the fast erasure-and-error decoding of a class of dual affine
variety codes. Moreover, we show that systematic encoding corresponds to a
special case of erasure-only decoding. The lemma enables us to reduce the
computational complexity of error-evaluation from O(n^3) using Gaussian
elimination to O(qn^2) with some mild conditions on n and q, where n is the
code length and q is the finite-field size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4753</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4753</id><created>2012-11-20</created><authors><author><keyname>Foti</keyname><forenames>Nicholas J.</forenames></author><author><keyname>Futoma</keyname><forenames>Joseph D.</forenames></author><author><keyname>Rockmore</keyname><forenames>Daniel N.</forenames></author><author><keyname>Williamson</keyname><forenames>Sinead</forenames></author></authors><title>A unifying representation for a class of dependent random measures</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general construction for dependent random measures based on
thinning Poisson processes on an augmented space. The framework is not
restricted to dependent versions of a specific nonparametric model, but can be
applied to all models that can be represented using completely random measures.
Several existing dependent random measures can be seen as specific cases of
this framework. Interesting properties of the resulting measures are derived
and the efficacy of the framework is demonstrated by constructing a
covariate-dependent latent feature model and topic model that obtain superior
predictive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4755</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4755</id><created>2012-11-20</created><updated>2014-01-02</updated><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>J&#xe4;kel</keyname><forenames>Holger</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Interference in Poisson Networks with Isotropically Distributed Nodes</title><categories>cs.IT math.IT</categories><comments>This work was presented in part at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practical wireless networks are finite, and hence non-stationary with nodes
typically non-homo-geneously deployed over the area. This leads to a
location-dependent performance and to boundary effects which are both often
neglected in network modeling. In this work, interference in networks with
nodes distributed according to an isotropic but not necessarily stationary
Poisson point process (PPP) are studied. The resulting link performance is
precisely characterized as a function of (i) an arbitrary receiver location and
of (ii) an arbitrary isotropic shape of the spatial distribution. Closed-form
expressions for the first moment and the Laplace transform of the interference
are derived for the path loss exponents $\alpha=2$ and $\alpha=4$, and simple
bounds are derived for other cases. The developed model is applied to practical
problems in network analysis: for instance, the accuracy loss due to neglecting
border effects is shown to be undesirably high within transition regions of
certain deployment scenarios. Using a throughput metric not relying on the
stationarity of the spatial node distribution, the spatial throughput locally
around a given node is characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4767</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4767</id><created>2012-11-20</created><authors><author><keyname>Ren</keyname><forenames>Dongni</forenames></author><author><keyname>Chan</keyname><forenames>S. -H. Gary</forenames></author><author><keyname>Cheung</keyname><forenames>Gene</forenames></author><author><keyname>Zhao</keyname><forenames>Vicky</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Collaborative P2P Streaming of Interactive Live Free Viewpoint Video</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an interactive live streaming scenario where multiple peers pull
streams of the same free viewpoint video that are synchronized in time but not
necessarily in view. In free viewpoint video, each user can periodically select
a virtual view between two anchor camera views for display. The virtual view is
synthesized using texture and depth videos of the anchor views via
depth-image-based rendering (DIBR). In general, the distortion of the virtual
view increases with the distance to the anchor views, and hence it is
beneficial for a peer to select the closest anchor views for synthesis. On the
other hand, if peers interested in different virtual views are willing to
tolerate larger distortion in using more distant anchor views, they can
collectively share the access cost of common anchor views.
  Given anchor view access cost and synthesized distortion of virtual views
between anchor views, we study the optimization of anchor view allocation for
collaborative peers. We first show that, if the network reconfiguration costs
due to view-switching are negligible, the problem can be optimally solved in
polynomial time using dynamic programming. We then consider the case of
non-negligible reconfiguration costs (e.g., large or frequent view-switching
leading to anchor-view changes). In this case, the view allocation problem
becomes NP-hard. We thus present a locally optimal and centralized allocation
algorithm inspired by Lloyd's algorithm in non-uniform scalar quantization. We
also propose a distributed algorithm with guaranteed convergence where each
peer group independently make merge-and-split decisions with a well-defined
fairness criteria. The results show that depending on the problem settings, our
proposed algorithms achieve respective optimal and close-to-optimal performance
in terms of total cost, and outperform a P2P scheme without collaborative
anchor selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4771</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4771</id><created>2012-11-20</created><authors><author><keyname>Sundaramoorthi</keyname><forenames>Ganesh</forenames></author><author><keyname>Yang</keyname><forenames>Yanchao</forenames></author></authors><title>Matching Through Features and Features Through Matching</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses how to construct features for the problem of image
correspondence, in particular, the paper addresses how to construct features so
as to maintain the right level of invariance versus discriminability. We show
that without additional prior knowledge of the 3D scene, the right tradeoff
cannot be established in a pre-processing step of the images as is typically
done in most feature-based matching methods. However, given knowledge of the
second image to match, the tradeoff between invariance and discriminability of
features in the first image is less ambiguous. This suggests to setup the
problem of feature extraction and matching as a joint estimation problem. We
develop a possible mathematical framework, a possible computational algorithm,
and we give example demonstration on finding correspondence on images related
by a scene that undergoes large 3D deformation of non-planar objects and camera
viewpoint change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4775</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4775</id><created>2012-11-20</created><updated>2014-02-26</updated><authors><author><keyname>Estler</keyname><forenames>H. -Christian</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Nordio</keyname><forenames>Martin</forenames></author><author><keyname>Piccioni</keyname><forenames>Marco</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Contracts in Practice</title><categories>cs.SE</categories><journal-ref>Proceedings of the 19th International Symposium on Formal Methods
  (FM). Lecture Notes in Computer Science, 8442:230--246, Springer, May 2014</journal-ref><doi>10.1007/978-3-319-06410-9_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contracts are a form of lightweight formal specification embedded in the
program text. Being executable parts of the code, they encourage programmers to
devote proper attention to specifications, and help maintain consistency
between specification and implementation as the program evolves. The present
study investigates how contracts are used in the practice of software
development. Based on an extensive empirical analysis of 21 contract-equipped
Eiffel, C#, and Java projects totaling more than 260 million lines of code over
7700 revisions, it explores, among other questions: 1) which kinds of contract
elements (preconditions, postconditions, class invariants) are used more often;
2) how contracts evolve over time; 3) the relationship between implementation
changes and contract changes; and 4) the role of inheritance in the process. It
has found, among other results, that: the percentage of program elements that
include contracts is above 33% for most projects and tends to be stable over
time; there is no strong preference for a certain type of contract element;
contracts are quite stable compared to implementations; and inheritance does
not significantly affect qualitative trends of contract usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4779</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4779</id><created>2012-11-17</created><authors><author><keyname>Compagnoni</keyname><forenames>Adriana</forenames><affiliation>Stevens Institute of Technology</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Universita' di Torino</affiliation></author><author><keyname>Giannini</keyname><forenames>Paola</forenames><affiliation>Universita' del Piemonte Orientale</affiliation></author><author><keyname>Sauer</keyname><forenames>Karin</forenames><affiliation>Binghamton University</affiliation></author><author><keyname>Sharma</keyname><forenames>Vishakha</forenames><affiliation>Stevens Institute of Technology</affiliation></author><author><keyname>Troina</keyname><forenames>Angelo</forenames><affiliation>Universita' di Torino</affiliation></author></authors><title>Parallel BioScape: A Stochastic and Parallel Language for Mobile and
  Spatial Interactions</title><categories>cs.LO cs.FL</categories><comments>In Proceedings MeCBIC 2012, arXiv:1211.3476</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 100, 2012, pp. 101-106</journal-ref><doi>10.4204/EPTCS.100.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BioScape is a concurrent language motivated by the biological landscapes
found at the interface of biology and biomaterials. It has been motivated by
the need to model antibacterial surfaces, biofilm formation, and the effect of
DNAse in treating and preventing biofilm infections. As its predecessor, SPiM,
BioScape has a sequential semantics based on Gillespie's algorithm, and its
implementation does not scale beyond 1000 agents. However, in order to model
larger and more realistic systems, a semantics that may take advantage of the
new multi-core and GPU architectures is needed. This motivates the introduction
of parallel semantics, which is the contribution of this paper: Parallel
BioScape, an extension with fully parallel semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4783</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4783</id><created>2012-11-20</created><updated>2013-05-13</updated><authors><author><keyname>Dijkstra</keyname><forenames>L. J.</forenames></author><author><keyname>Yakushev</keyname><forenames>A. V.</forenames></author><author><keyname>Duijn</keyname><forenames>P. A. C.</forenames></author><author><keyname>Boukhanovsky</keyname><forenames>A. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Inference of the Russian drug community from one of the largest social
  networks in the Russian Federation</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The criminal nature of narcotics complicates the direct assessment of a drug
community, while having a good understanding of the type of people drawn or
currently using drugs is vital for finding effective intervening strategies.
Especially for the Russian Federation this is of immediate concern given the
dramatic increase it has seen in drug abuse since the fall of the Soviet Union
in the early nineties. Using unique data from the Russian social network
'LiveJournal' with over 39 million registered users worldwide, we were able for
the first time to identify the on-line drug community by context sensitive text
mining of the users' blogs using a dictionary of known drug-related official
and 'slang' terminology. By comparing the interests of the users that most
actively spread information on narcotics over the network with the interests of
the individuals outside the on-line drug community, we found that the 'average'
drug user in the Russian Federation is generally mostly interested in topics
such as Russian rock, non-traditional medicine, UFOs, Buddhism, yoga and the
occult. We identify three distinct scale-free sub-networks of users which can
be uniquely classified as being either 'infectious', 'susceptible' or 'immune'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4795</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4795</id><created>2012-11-20</created><updated>2016-02-04</updated><authors><author><keyname>Park</keyname><forenames>Sangwoo</forenames></author><author><keyname>Serpedin</keyname><forenames>Erchin</forenames></author><author><keyname>Qaraqe</keyname><forenames>Khalid</forenames></author></authors><title>A Unifying Variational Perspective on Some Fundamental Information
  Theoretic Inequalities</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a unifying variational approach for proving and extending
some fundamental information theoretic inequalities. Fundamental information
theory results such as maximization of differential entropy, minimization of
Fisher information (Cram\'er-Rao inequality), worst additive noise lemma,
entropy power inequality (EPI), and extremal entropy inequality (EEI) are
interpreted as functional problems and proved within the framework of calculus
of variations. Several applications and possible extensions of the proposed
results are briefly mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4798</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4798</id><created>2012-11-20</created><authors><author><keyname>Foti</keyname><forenames>Nicholas J.</forenames></author><author><keyname>Williamson</keyname><forenames>Sinead</forenames></author></authors><title>A survey of non-exchangeable priors for Bayesian nonparametric models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependent nonparametric processes extend distributions over measures, such as
the Dirichlet process and the beta process, to give distributions over
collections of measures, typically indexed by values in some covariate space.
Such models are appropriate priors when exchangeability assumptions do not
hold, and instead we want our model to vary fluidly with some set of
covariates. Since the concept of dependent nonparametric processes was
formalized by MacEachern [1], there have been a number of models proposed and
used in the statistics and machine learning literatures. Many of these models
exhibit underlying similarities, an understanding of which, we hope, will help
in selecting an appropriate prior, developing new models, and leveraging
inference techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4812</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4812</id><created>2012-11-20</created><authors><author><keyname>Abgrall</keyname><forenames>Erwan</forenames><affiliation>Uni.lu</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>Uni.lu, S'nT</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Gombault</keyname><forenames>Sylvain</forenames><affiliation>RSM</affiliation></author><author><keyname>Heiderich</keyname><forenames>Mario</forenames></author><author><keyname>Ribault</keyname><forenames>Alain</forenames></author></authors><title>XSS-FP: Browser Fingerprinting using HTML Parser Quirks</title><categories>cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many scenarios in which inferring the type of a client browser is
desirable, for instance to fight against session stealing. This is known as
browser fingerprinting. This paper presents and evaluates a novel
fingerprinting technique to determine the exact nature (browser type and
version, eg Firefox 15) of a web-browser, exploiting HTML parser quirks
exercised through XSS. Our experiments show that the exact version of a web
browser can be determined with 71% of accuracy, and that only 6 tests are
sufficient to quickly determine the exact family a web browser belongs to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4839</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4839</id><created>2012-11-20</created><authors><author><keyname>Farag</keyname><forenames>Mohamed</forenames></author></authors><title>An Insight View of Kernel Visual Debugger in System Boot up</title><categories>cs.OS cs.SY</categories><comments>10 pages, International Journal</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 5, October 2012</journal-ref><doi>10.5121/ijcsit.2012.4510</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many years, developers could not figure out the mystery of OS kernels.
The main source of this mystery is the interaction between operating systems
and hardware while system's boot up and kernel initialization. In addition,
many operating system kernels differ in their behavior toward many situations.
For instance, kernels act differently in racing conditions, kernel
initialization and process scheduling. For such operations, kernel debuggers
were designed to help in tracing kernel behavior and solving many kernel bugs.
The importance of kernel debuggers is not limited to kernel code tracing but
also, they can be used in verification and performance comparisons. However,
developers had to be aware of debugger commands thus introducing some
difficulties to non-expert programmers. Later, several visual kernel debuggers
were presented to make it easier for programmers to trace their kernel code and
analyze kernel behavior. Nowadays, several kernel debuggers exist for solving
this mystery but only very few support line-by-line debugging at run-time. In
this paper, a generic approach for operating system source code debugging in
graphical mode with line-by-line tracing support is proposed. In the context of
this approach, system boot up and evaluation of two operating system schedulers
from several points of views will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4840</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4840</id><created>2012-11-20</created><authors><author><keyname>Farag</keyname><forenames>Mohamed</forenames></author></authors><title>Multicore Dynamic Kernel Modules Attachment Technique for Kernel
  Performance Enhancement</title><categories>cs.OS</categories><comments>13 pages, International Journal of Computer Science &amp; Information
  Technology (IJCSIT) Vol 4, No 4, August 2012</comments><doi>10.5121/ijcsit.2012.4405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional monolithic kernels dominated kernel structures for long time
along with small sized kernels,few hardware companies and limited kernel
functionalities. Monolithic kernel structure was not applicable when the number
of hardware companies increased and kernel services consumed by different users
for many purposes. One of the biggest disadvantages of the monolithic kernels
is the inflexibility due to the need to include all the available modules in
kernel compilation causing high time consuming. Lately, new kernel structure
was introduced through multicore operating systems. Unfortunately, many
multicore operating systems such as barrelfish and FOS are experimental. This
paper aims to simulate the performance of multicore hybrid kernels through
dynamic kernel module customized attachment/ deattachment for multicore
machines. In addition, this paper proposes a new technique for loading dynamic
kernel modules based on the user needs and machine capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4852</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4852</id><created>2012-11-20</created><authors><author><keyname>Park</keyname><forenames>Sangwoo</forenames></author><author><keyname>Serpedin</keyname><forenames>Erchin</forenames></author><author><keyname>Qaraqe</keyname><forenames>Khalid</forenames></author></authors><title>Gaussian Assumption: the Least Favorable but the Most Useful</title><categories>cs.IT math.IT</categories><doi>10.1109/MSP.2013.2238691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on three contributions. First, a connection between the
result, proposed by Stoica and Babu, and the recent information theoretic
results, the worst additive noise lemma and the isoperimetric inequality for
entropies, is illustrated. Second, information theoretic and estimation
theoretic justifications for the fact that the Gaussian assumption leads to the
largest Cram\'{e}r-Rao lower bound (CRLB) is presented. Third, a slight
extension of this result to the more general framework of correlated
observations is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4853</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4853</id><created>2012-11-20</created><updated>2015-09-05</updated><authors><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>Reducing the rank of a matroid</title><categories>cs.DS cs.DM</categories><comments>v2: Minor changes made following helpful comments by the referees</comments><journal-ref>Discrete Mathematics and Theoretical Computer Science,
  17/2:143--156, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the rank reduction problem for matroids: Given a matroid M and an
integer k, find a minimum size subset of elements of M whose removal reduces
the rank of M by at least k. When M is a graphical matroid this problem is the
minimum k-cut problem, which admits a 2-approximation algorithm. In this paper
we show that the rank reduction problem for transversal matroids is essentially
at least as hard to approximate as the densest k-subgraph problem. We also
prove that, while the problem is easily solvable in polynomial time for
partition matroids, it is NP-hard when considering the intersection of two
partition matroids. Our proof shows, in particular, that the maximum vertex
cover problem is NP-hard on bipartite graphs, which answers an open problem of
B. Simeone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4860</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4860</id><created>2012-11-20</created><authors><author><keyname>Beijbom</keyname><forenames>Oscar</forenames></author></authors><title>Domain Adaptations for Computer Vision Applications</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic assumption of statistical learning theory is that train and test data
are drawn from the same underlying distribution. Unfortunately, this assumption
doesn't hold in many applications. Instead, ample labeled data might exist in a
particular `source' domain while inference is needed in another, `target'
domain. Domain adaptation methods leverage labeled data from both domains to
improve classification on unseen data in the target domain. In this work we
survey domain transfer learning methods for various application domains with
focus on recent work in Computer Vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4864</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4864</id><created>2012-11-19</created><authors><author><keyname>Habib</keyname><forenames>Salman</forenames></author><author><keyname>Morozov</keyname><forenames>Vitali</forenames></author><author><keyname>Finkel</keyname><forenames>Hal</forenames></author><author><keyname>Pope</keyname><forenames>Adrian</forenames></author><author><keyname>Heitmann</keyname><forenames>Katrin</forenames></author><author><keyname>Kumaran</keyname><forenames>Kalyan</forenames></author><author><keyname>Peterka</keyname><forenames>Tom</forenames></author><author><keyname>Insley</keyname><forenames>Joe</forenames></author><author><keyname>Daniel</keyname><forenames>David</forenames></author><author><keyname>Fasel</keyname><forenames>Patricia</forenames></author><author><keyname>Frontiere</keyname><forenames>Nicholas</forenames></author><author><keyname>Lukic</keyname><forenames>Zarija</forenames></author></authors><title>The Universe at Extreme Scale: Multi-Petaflop Sky Simulation on the BG/Q</title><categories>cs.DC astro-ph.CO astro-ph.IM cs.PF physics.comp-ph</categories><comments>11 pages, 11 figures, final version of paper for talk presented at
  SC12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remarkable observational advances have established a compelling
cross-validated model of the Universe. Yet, two key pillars of this model --
dark matter and dark energy -- remain mysterious. Sky surveys that map billions
of galaxies to explore the `Dark Universe', demand a corresponding
extreme-scale simulation capability; the HACC (Hybrid/Hardware Accelerated
Cosmology Code) framework has been designed to deliver this level of
performance now, and into the future. With its novel algorithmic structure,
HACC allows flexible tuning across diverse architectures, including accelerated
and multi-core systems.
  On the IBM BG/Q, HACC attains unprecedented scalable performance -- currently
13.94 PFlops at 69.2% of peak and 90% parallel efficiency on 1,572,864 cores
with an equal number of MPI ranks, and a concurrency of 6.3 million. This level
of performance was achieved at extreme problem sizes, including a benchmark run
with more than 3.6 trillion particles, significantly larger than any
cosmological simulation yet performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4866</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4866</id><created>2012-11-19</created><authors><author><keyname>Saha</keyname><forenames>Suprativ</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author></authors><title>A Brief Review of Data Mining Application Involving Protein Sequence
  Classification</title><categories>cs.DB cs.NE</categories><comments>10 pages, 1 table, 1 figure. arXiv admin note: substantial text
  overlap with arXiv:1211.4654</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining techniques have been used by researchers for analyzing protein
sequences. In protein analysis, especially in protein sequence classification,
selection of feature is most important. Popular protein sequence classification
techniques involve extraction of specific features from the sequences.
Researchers apply some well-known classification techniques like neural
networks, Genetic algorithm, Fuzzy ARTMAP, Rough Set Classifier etc for
accurate classification. This paper presents a review is with three different
classification models such as neural network model, fuzzy ARTMAP model and
Rough set classifier model. A new technique for classifying protein sequences
have been proposed in the end. The proposed technique tries to reduce the
computational overheads encountered by earlier approaches and increase the
accuracy of classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4867</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4867</id><created>2012-11-20</created><authors><author><keyname>Felhi</keyname><forenames>Fa&#xee;&#xe7;al</forenames></author><author><keyname>Akaichi</keyname><forenames>Jalel</forenames></author></authors><title>Adaptation of Web services to the context based on workflow: Approach
  for self-adaptation of service-oriented architectures to the context</title><categories>cs.SE</categories><comments>14 pages, 4 figures, 1 Code; International Journal of Web &amp; Semantic
  Technology (IJWesT) Vol.3, No.4, October 2012. arXiv admin note: substantial
  text overlap with arXiv:1203.0400 by other authors</comments><doi>10.5121/ijwest.2012.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of Web services in the information space, as well as the
advanced technology of SOA, give tremendous opportunities for users in an
ambient space or distant, empowerment and organizations in various fields
application, such as geolocation, E-learning, healthcare, digital government,
etc.. In fact, Web services are a solution for the integration of distributed
information systems, autonomous, heterogeneous and self-adaptable to the
context. However, as Web services can evolve in a dynamic environment in a
well-defined context and according to events automatically, such as time,
temperature, location, authentication, etc.. We are interested in improving
their SOA to empower the Web services to be self adaptive contexts. In this
paper, we propose a new trend of self adaptability of Web services context.
Then applying these requirements in the architecture of the platform of
adaptability to context WComp, by integrating the workflow. Our work is
illustrated by a case study of authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4888</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4888</id><created>2012-11-20</created><authors><author><keyname>Sahai</keyname><forenames>Tuhin</forenames></author><author><keyname>Klus</keyname><forenames>Stefan</forenames></author><author><keyname>Dellnitz</keyname><forenames>Michael</forenames></author></authors><title>A Traveling Salesman Learns Bayesian Networks</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structure learning of Bayesian networks is an important problem that arises
in numerous machine learning applications. In this work, we present a novel
approach for learning the structure of Bayesian networks using the solution of
an appropriately constructed traveling salesman problem. In our approach, one
computes an optimal ordering (partially ordered set) of random variables using
methods for the traveling salesman problem. This ordering significantly reduces
the search space for the subsequent greedy optimization that computes the final
structure of the Bayesian network. We demonstrate our approach of learning
Bayesian networks on real world census and weather datasets. In both cases, we
demonstrate that the approach very accurately captures dependencies between
random variables. We check the accuracy of the predictions based on independent
studies in both application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4889</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4889</id><created>2012-11-20</created><updated>2013-04-15</updated><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Statistical Tests for Contagion in Observational Social Network Studies</title><categories>cs.SI physics.soc-ph stat.ME</categories><comments>9 pages, 4 figures. Appearing at AISTATS-13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current tests for contagion in social network studies are vulnerable to the
confounding effects of latent homophily (i.e., ties form preferentially between
individuals with similar hidden traits). We demonstrate a general method to
lower bound the strength of causal effects in observational social network
studies, even in the presence of arbitrary, unobserved individual traits. Our
tests require no parametric assumptions and each test is associated with an
algebraic proof. We demonstrate the effectiveness of our approach by correctly
deducing the causal effects for examples previously shown to expose defects in
existing methodology. Finally, we discuss preliminary results on data taken
from the Framingham Heart Study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4891</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4891</id><created>2012-11-20</created><updated>2013-02-13</updated><authors><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author></authors><title>Correspondence and Independence of Numerical Evaluations of Algorithmic
  Information Measures</title><categories>cs.IT cs.CC cs.FL math.IT</categories><comments>22 pages, 8 images. This article draws heavily from arXiv:1211.1302</comments><doi>10.3233/COM-13019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that real-value approximations of Kolmogorov-Chaitin (K_m) using the
algorithmic Coding theorem as calculated from the output frequency of a large
set of small deterministic Turing machines with up to 5 states (and 2 symbols),
is in agreement with the number of instructions used by the Turing machines
producing s, which is consistent with strict integer-value program-size
complexity. Nevertheless, K_m proves to be a finer-grained measure and a
potential alternative approach to lossless compression algorithms for small
entities, where compression fails. We also show that neither K_m nor the number
of instructions used shows any correlation with Bennett's Logical Depth LD(s)
other than what's predicted by the theory. The agreement between theory and
numerical calculations shows that despite the undecidability of these
theoretical measures, approximations are stable and meaningful, even for small
programs and for short strings. We also announce a first Beta version of an
Online Algorithmic Complexity Calculator (OACC), based on a combination of
theoretical concepts, as a numerical implementation of the Coding Theorem
Method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4892</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4892</id><created>2012-11-20</created><authors><author><keyname>Manzyuk</keyname><forenames>Oleksandr</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author><author><keyname>Radul</keyname><forenames>Alexey Andreyevich</forenames></author><author><keyname>Rush</keyname><forenames>David R.</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Confusion of Tagged Perturbations in Forward Automatic Differentiation
  of Higher-Order Functions</title><categories>cs.SC cs.MS math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forward Automatic Differentiation (AD) is a technique for augmenting programs
to both perform their original calculation and also compute its directional
derivative. The essence of Forward AD is to attach a derivative value to each
number, and propagate these through the computation. When derivatives are
nested, the distinct derivative calculations, and their associated attached
values, must be distinguished. In dynamic languages this is typically
accomplished by creating a unique tag for each application of the derivative
operator, tagging the attached values, and overloading the arithmetic
operators. We exhibit a subtle bug, present in fielded implementations, in
which perturbations are confused despite the tagging machinery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4896</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4896</id><created>2012-11-20</created><authors><author><keyname>Hassan</keyname><forenames>A. H.</forenames></author><author><keyname>Fluke</keyname><forenames>C. J.</forenames></author><author><keyname>Barnes</keyname><forenames>D. G.</forenames></author><author><keyname>Kilborn</keyname><forenames>V. A.</forenames></author></authors><title>Tera-scale Astronomical Data Analysis and Visualization</title><categories>astro-ph.IM cs.DC cs.GR</categories><comments>16 pages, 14 Figures, accepted for publication in Monthly Notices of
  the Royal Astronomical Society</comments><doi>10.1093/mnras/sts513</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a high-performance, graphics processing unit (GPU)-based framework
for the efficient analysis and visualization of (nearly) terabyte (TB)-sized
3-dimensional images. Using a cluster of 96 GPUs, we demonstrate for a 0.5 TB
image: (1) volume rendering using an arbitrary transfer function at 7--10
frames per second; (2) computation of basic global image statistics such as the
mean intensity and standard deviation in 1.7 s; (3) evaluation of the image
histogram in 4 s; and (4) evaluation of the global image median intensity in
just 45 s. Our measured results correspond to a raw computational throughput
approaching one teravoxel per second, and are 10--100 times faster than the
best possible performance with traditional single-node, multi-core CPU
implementations. A scalability analysis shows the framework will scale well to
images sized 1 TB and beyond. Other parallel data analysis algorithms can be
added to the framework with relative ease, and accordingly, we present our
framework as a possible solution to the image analysis and visualization
requirements of next-generation telescopes, including the forthcoming Square
Kilometre Array pathfinder radiotelescopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4907</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4907</id><created>2012-11-20</created><updated>2013-01-04</updated><authors><author><keyname>Coelho</keyname><forenames>Luis Pedro</forenames></author></authors><title>Mahotas: Open source software for scriptable computer vision</title><categories>cs.CV cs.SE</categories><journal-ref>Journal of Open Research Software 1(1):e3 2013</journal-ref><doi>10.5334/jors.ac</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mahotas is a computer vision library for Python. It contains traditional
image processing functionality such as filtering and morphological operations
as well as more modern computer vision functions for feature computation,
including interest point detection and local descriptors.
  The interface is in Python, a dynamic programming language, which is very
appropriate for fast development, but the algorithms are implemented in C++ and
are tuned for speed. The library is designed to fit in with the scientific
software ecosystem in this language and can leverage the existing
infrastructure developed in that language.
  Mahotas is released under a liberal open source license (MIT License) and is
available from (http://github.com/luispedro/mahotas) and from the Python
Package Index (http://pypi.python.org/pypi/mahotas).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4909</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4909</id><created>2012-11-20</created><updated>2013-09-29</updated><authors><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Fan</keyname><forenames>Hongqi</forenames></author><author><keyname>Fu</keyname><forenames>Qiang</forenames></author></authors><title>Fast Marginalized Block Sparse Bayesian Learning Algorithm</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of sparse signal recovery from noise corrupted,
underdetermined measurements can be improved if both sparsity and correlation
structure of signals are exploited. One typical correlation structure is the
intra-block correlation in block sparse signals. To exploit this structure, a
framework, called block sparse Bayesian learning (BSBL), has been proposed
recently. Algorithms derived from this framework showed superior performance
but they are not very fast, which limits their applications. This work derives
an efficient algorithm from this framework, using a marginalized likelihood
maximization method. Compared to existing BSBL algorithms, it has close
recovery performance but is much faster. Therefore, it is more suitable for
large scale datasets and applications requiring real-time implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4918</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4918</id><created>2012-11-20</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Paparas</keyname><forenames>Dimitris</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>The Complexity of Non-Monotone Markets</title><categories>cs.CC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of non-monotone utilities, which covers a wide
variety of utility functions in economic theory. We then prove that it is
PPAD-hard to compute an approximate Arrow-Debreu market equilibrium in markets
with linear and non-monotone utilities. Building on this result, we settle the
long-standing open problem regarding the computation of an approximate
Arrow-Debreu market equilibrium in markets with CES utility functions, by
proving that it is PPAD-complete when the Constant Elasticity of Substitution
parameter \rho is any constant less than -1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4927</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4927</id><created>2012-11-20</created><updated>2013-05-21</updated><authors><author><keyname>Bereg</keyname><forenames>Sergey</forenames></author><author><keyname>Rozario</keyname><forenames>Timothy</forenames></author></authors><title>Angle Optimization of Graphs Embedded in the Plane</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study problems of drawing graphs in the plane using edge
length constraints and angle optimization. Specifically we consider the problem
of maximizing the minimum angle, the MMA problem. We solve the MMA problem
using a spring-embedding approach where two forces are applied to the vertices
of the graph: a force optimizing edge lengths and a force optimizing angles. We
solve analytically the problem of computing an optimal displacement of a graph
vertex optimizing the angles between edges incident to it if the degree of the
vertex is at most three. We also apply a numerical approach for computing the
forces applied to vertices of higher degree. We implemented our algorithm in
Java and present drawings of some graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4929</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4929</id><created>2012-11-20</created><authors><author><keyname>Nguyen</keyname><forenames>Trung V.</forenames></author><author><keyname>Oh</keyname><forenames>Alice H.</forenames></author></authors><title>Summarizing Reviews with Variable-length Syntactic Patterns and Topic
  Models</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel summarization framework for reviews of products and
services by selecting informative and concise text segments from the reviews.
Our method consists of two major steps. First, we identify five frequently
occurring variable-length syntactic patterns and use them to extract candidate
segments. Then we use the output of a joint generative sentiment topic model to
filter out the non-informative segments. We verify the proposed method with
quantitative and qualitative experiments. In a quantitative study, our approach
outperforms previous methods in producing informative segments and summaries
that capture aspects of products and services as expressed in the
user-generated pros and cons lists. Our user study with ninety users resonates
with this result: individual segments extracted and filtered by our method are
rated as more useful by users compared to previous approaches by users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4935</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4935</id><created>2012-11-20</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author><author><keyname>Kang</keyname><forenames>Daeseong</forenames></author></authors><title>Mutually Exclusive Rules in LogicWeb</title><categories>cs.LO cs.SE</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LogicWeb has traditionally lacked devices for expressing mutually exclusive
clauses. We address this limitation by adopting choice-conjunctive clauses of
the form $D_0 \adc D_1$ where $D_0, D_1$ are Horn clauses and $\adc$ is a
linear logic connective. Solving a goal $G$ using $D_0 \adc D_1$ -- $\prov(D_0
\adc D_1,G)$ -- has the following operational semantics: choose a successful
one between $\prov(D_0,G)$ and $\prov(D_1,G)$. In other words, if $D_o$ is
chosen in the course of solving $G$, then $D_1$ will be discarded and vice
versa. Hence, the class of choice-conjunctive clauses precisely captures the
notion of mutually exclusive clauses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4940</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4940</id><created>2012-11-21</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Kim</keyname><forenames>Byoung-Jo J.</forenames></author><author><keyname>Henry</keyname><forenames>Paul</forenames></author><author><keyname>Rozner</keyname><forenames>Eric</forenames></author></authors><title>A Wireless Channel Sounding System for Rapid Propagation Measurements</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2013 (2012 AT\&amp;T Intellectual Property. All rights
  reserved.)</comments><journal-ref>IEEE International Conference on Communications 2013, page
  5720-5725</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless systems are getting deployed in many new environments with different
antenna heights, frequency bands and multipath conditions. This has led to an
increasing demand for more channel measurements to understand wireless
propagation in specific environments and assist deployment engineering. We
design and implement a rapid wireless channel sounding system, using the
Universal Software Radio Peripheral (USRP) and GNU Radio software, to address
these demands. Our design measures channel propagation characteristics
simultaneously from multiple transmitter locations. The system consists of
multiple battery-powered transmitters and receivers. Therefore, we can set-up
the channel sounder rapidly at a field location and measure expeditiously by
analyzing different transmitters signals during a single walk or drive through
the environment. Our design can be used for both indoor and outdoor channel
measurements in the frequency range of 1 MHz to 6 GHz. We expect that the
proposed approach, with a few further refinements, can transform the task of
propagation measurement as a routine part of day-to-day wireless network
engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4949</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4949</id><created>2012-11-21</created><authors><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author><author><keyname>Okuno</keyname><forenames>Yasushi</forenames></author></authors><title>On the behavior of tile assembly system at high temperatures</title><categories>cs.CC cs.DM</categories><comments>This paper is an extended version of the following paper: S. Seki and
  Y. Okuno. On the behavior of tile assembly system at high temperatures. In
  CiE 2012: How the World Computes - Turing Centenary Conference and 8th
  Conference on Computability in Europe, LNCS 7318, pages 549-559, Springer,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behaviors of Winfree's tile assembly systems (TASs) at high temperatures are
investigated in combination with integer programming of a specific form called
threshold programming. First, we propose a way to build bridges from the
Boolean satisfiability problem (SAT) to threshold programming, and further to
TAS's behavior, in order to prove the NP-hardness of optimizing temperatures of
TASs that behave in a way given as input. These bridges will take us further to
two important results on the behavior of TASs at high temperatures. The first
says that arbitrarily high temperatures are required to assemble some shape by
a TAS of &quot;reasonable&quot; size. The second is that for any temperature at least 4
given as a parameter, it is NP-hard to find the minimum size TAS that
self-assembles a given shape and works at the given temperature or below.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4957</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4957</id><created>2012-11-21</created><updated>2012-11-23</updated><authors><author><keyname>Pisasale</keyname><forenames>Antonio</forenames></author><author><keyname>Cantone</keyname><forenames>Domenico</forenames></author></authors><title>An Experiment on the Connection between the DLs' Family DL&lt;ForAllPiZero&gt;
  and the Real World</title><categories>cs.AI cs.LO</categories><comments>15 pages, 2 sections, 2 appendices, 4 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes the analysis of a selected testbed of Semantic Web
ontologies, by a SPARQL query, which determines those ontologies that can be
related to the description logic DL&lt;ForAllPiZero&gt;, introduced in [4] and
studied in [9]. We will see that a reasonable number of them is expressible
within such computationally efficient language. We expect that, in a long-term
view, a temporalization of description logics, and consequently, of OWL(2), can
open new perspectives for the inclusion in this language of a greater number of
ontologies of the testbed and, hopefully, of the &quot;real world&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4971</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4971</id><created>2012-11-21</created><authors><author><keyname>Narendhar</keyname><forenames>S.</forenames></author><author><keyname>Amudha</keyname><forenames>T.</forenames></author></authors><title>A Hybrid Bacterial Foraging Algorithm For Solving Job Shop Scheduling
  Problems</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio-Inspired computing is the subset of Nature-Inspired computing. Job Shop
Scheduling Problem is categorized under popular scheduling problems. In this
research work, Bacterial Foraging Optimization was hybridized with Ant Colony
Optimization and a new technique Hybrid Bacterial Foraging Optimization for
solving Job Shop Scheduling Problem was proposed. The optimal solutions
obtained by proposed Hybrid Bacterial Foraging Optimization algorithms are much
better when compared with the solutions obtained by Bacterial Foraging
Optimization algorithm for well-known test problems of different sizes. From
the implementation of this research work, it could be observed that the
proposed Hybrid Bacterial Foraging Optimization was effective than Bacterial
Foraging Optimization algorithm in solving Job Shop Scheduling Problems. Hybrid
Bacterial Foraging Optimization is used to implement real world Job Shop
Scheduling Problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4974</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4974</id><created>2012-11-21</created><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Norbert Th.</forenames></author><author><keyname>R&#xf6;snick</keyname><forenames>Carsten</forenames></author><author><keyname>Ziegler</keyname><forenames>Martin</forenames></author></authors><title>Parameterized Uniform Complexity in Numerics: from Smooth to Analytic,
  from NP-hard to Polytime</title><categories>cs.NA cs.CC math.NA</categories><msc-class>65Y20, 68Q15</msc-class><acm-class>F.1.3; G.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synthesis of classical Computational Complexity Theory with Recursive
Analysis provides a quantitative foundation to reliable numerics. Here the
operators of maximization, integration, and solving ordinary differential
equations are known to map (even high-order differentiable) polynomial-time
computable functions to instances which are `hard' for classical complexity
classes NP, #P, and CH; but, restricted to analytic functions, map
polynomial-time computable ones to polynomial-time computable ones --
non-uniformly!
  We investigate the uniform parameterized complexity of the above operators in
the setting of Weihrauch's TTE and its second-order extension due to
Kawamura&amp;Cook (2010). That is, we explore which (both continuous and discrete,
first and second order) information and parameters on some given f is
sufficient to obtain similar data on Max(f) and int(f); and within what running
time, in terms of these parameters and the guaranteed output precision 2^(-n).
  It turns out that Gevrey's hierarchy of functions climbing from analytic to
smooth corresponds to the computational complexity of maximization growing from
polytime to NP-hard. Proof techniques involve mainly the Theory of (discrete)
Computation, Hard Analysis, and Information-Based Complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4976</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4976</id><created>2012-11-21</created><updated>2012-12-03</updated><authors><author><keyname>Varcoe</keyname><forenames>Benjamin T. H.</forenames></author></authors><title>Channel Independent Cryptographic Key Distribution</title><categories>cs.IT cs.CR math.IT</categories><comments>7 Pages, 5 Figures, Submitted to IEEE Transactions on Information
  Theory, Corrected typo in eqn 6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method of cryptographic key distribution using an
`artificially' noisy channel. This is an important development because, while
it is known that a noisy channel can be used to generate unconditional secrecy,
there are many circumstances in which it is not possible to have a noisy
information exchange, such as in error corrected communication stacks. It is
shown that two legitimate parties can simulate a noisy channel by adding local
noise onto the communication and that the simulated channel has a secrecy
capacity even if the underlying channel does not. A derivation of the secrecy
conditions is presented along with numerical simulations of the channel
function to show that key exchange is feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4986</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4986</id><created>2012-11-21</created><authors><author><keyname>Kabicher-Fuchs</keyname><forenames>Sonja</forenames></author><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author><author><keyname>Recker</keyname><forenames>Jan</forenames></author><author><keyname>Indulska</keyname><forenames>Marta</forenames></author><author><keyname>Charoy</keyname><forenames>Francois</forenames></author><author><keyname>Christiaanse</keyname><forenames>Rob</forenames></author><author><keyname>Dunkl</keyname><forenames>Reinhold</forenames></author><author><keyname>Grambow</keyname><forenames>Gregor</forenames></author><author><keyname>Kolb</keyname><forenames>Jens</forenames></author><author><keyname>Leopold</keyname><forenames>Henrik</forenames></author><author><keyname>Mendling</keyname><forenames>Jan</forenames></author></authors><title>Human-Centric Process-Aware Information Systems (HC-PAIS)</title><categories>cs.HC cs.CY</categories><comments>8 pages</comments><report-no>TR-201211213534</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process-Aware Information Systems (PAIS) support organizations in managing
and automating their processes. A full automation of processes is in particular
industries, such as service-oriented markets, not practicable. The integration
of humans in PAIS is necessary to manage and perform processes that require
human capabilities, judgments and decisions. A challenge of interdisciplinary
PAIS research is to provide concepts and solutions that support human
integration in PAIS and human orientation of PAIS in a way that provably
increase the PAIS users' satisfaction and motivation with working with the
Human-Centric Process Aware Information System (HC-PAIS) and consequently
influence users' performance of tasks. This work is an initial step of research
that aims at providing a definition of Human-Centric Process Aware Information
Systems (HC-PAIS) and future research challenges of HC-PAIS. Results of focus
group research are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.4998</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.4998</id><created>2012-11-21</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Wu</keyname><forenames>Jian-Liang</forenames></author></authors><title>A conjecture on equitable vertex arboricity of graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wu, Zhang and Li [4] conjectured that the set of vertices of any simple graph
$G$ can be equitably partitioned into $\lceil(\Delta(G)+1)/2\rceil$ subsets so
that each of them induces a forest of $G$. In this note, we prove this
conjecture for graphs $G$ with $\Delta(G)\geq |G|/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5009</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5009</id><created>2012-11-21</created><authors><author><keyname>Beheshti</keyname><forenames>Seyed-Mehdi-Reza</forenames></author><author><keyname>Motahari-Nezhad</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author></authors><title>Temporal Provenance Model (TPM): Model and Query Language</title><categories>cs.DB</categories><comments>31 pages</comments><report-no>UNSW-CSE-TR-1116</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provenance refers to the documentation of an object's lifecycle. This
documentation (often represented as a graph) should include all the information
necessary to reproduce a certain piece of data or the process that led to it.
In a dynamic world, as data changes, it is important to be able to get a piece
of data as it was, and its provenance graph, at a certain point in time.
Supporting time-aware provenance querying is challenging and requires: (i)
explicitly representing the time information in the provenance graphs, and (ii)
providing abstractions and efficient mechanisms for time-aware querying of
provenance graphs over an ever growing volume of data. The existing provenance
models treat time as a second class citizen (i.e. as an optional annotation).
This makes time-aware querying of provenance data inefficient and sometimes
inaccessible. We introduce an extended provenance graph model to explicitly
represent time as an additional dimension of provenance data. We also provide a
query language, novel abstractions and efficient mechanisms to query and
analyze timed provenance graphs. The main contributions of the paper include:
(i) proposing a Temporal Provenance Model (TPM) as a timed provenance model;
and (ii) introducing two concepts of timed folder, as a container of related
set of objects and their provenance relationship over time, and timed paths, to
represent the evolution of objects tracing information over time, for analyzing
and querying TPM graphs. We have implemented the approach on top of FPSPARQL, a
query engine for large graphs, and have evaluated for querying TPM models. The
evaluation shows the viability and efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5027</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5027</id><created>2012-11-21</created><authors><author><keyname>Clazzer</keyname><forenames>Federico</forenames></author><author><keyname>Kissling</keyname><forenames>Christian</forenames></author></authors><title>Enhanced Contention Resolution Aloha - ECRA</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for pubblication at 9th International ITG Conference on
  Systems, Communications and Coding - SCC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Access (RA) Medium Access (MAC) protocols are simple and effective
when the nature of the traffic is unpredictable and random. In the following
paper, a novel RA protocol called Enhanced Contention Resolution ALOHA (ECRA)
is presented. This evolution, based on the previous Contention Resolution ALOHA
(CRA) protocol, exploits the nature of the interference in unslotted Aloha-like
channels for trying to resolve most of the partial collision that can occur
there. In the paper, the idea behind ECRA is presented together with numerical
simulations and a mathematical analysis of its performance gain. It is shown
that relevant performance increases in both throughput and Packet Error Rate
(PER) can be reached by ECRA with respect to CRA. A comparison with Contention
Resolution Diversity Slotted ALOHA (CRDSA) is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5031</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5031</id><created>2012-11-21</created><updated>2014-03-02</updated><authors><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Beyond the Vizing's bound for at most seven colors</title><categories>cs.DS cs.DM math.CO</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a simple graph of maximum degree $\Delta$. The edges of $G$
can be colored with at most $\Delta +1$ colors by Vizing's theorem. We study
lower bounds on the size of subgraphs of $G$ that can be colored with $\Delta$
colors.
  Vizing's Theorem gives a bound of $\frac{\Delta}{\Delta+1}|E|$. This is known
to be tight for cliques $K_{\Delta+1}$ when $\Delta$ is even. However, for
$\Delta=3$ it was improved to $26/31|E|$ by Albertson and Haas [Parsimonious
edge colorings, Disc. Math. 148, 1996] and later to $6/7|E|$ by Rizzi
[Approximating the maximum 3-edge-colorable subgraph problem, Disc. Math. 309,
2009]. It is tight for $B_3$, the graph isomorphic to a $K_4$ with one edge
subdivided.
  We improve previously known bounds for $\Delta\in{3,...,7}$, under the
assumption that for $\Delta=3,4,6$ graph $G$ is not isomorphic to $B_3$, $K_5$
and $K_7$, respectively. For $\Delta \geq 4$ these are the first results which
improve over the Vizing's bound. We also show a new bound for subcubic
multigraphs not isomorphic to $K_3$ with one edge doubled.
  In the second part, we give approximation algorithms for the Maximum
k-Edge-Colorable Subgraph problem, where given a graph G (without any bound on
its maximum degree or other restrictions) one has to find a k-edge-colorable
subgraph with maximum number of edges. In particular, when G is simple for
k=3,4,5,6,7 we obtain approximation ratios of 13/15, 9/11, 19/22, 23/27 and
22/25, respectively. We also present a 7/9-approximation for k=3 when G is a
multigraph. The approximation algorithms follow from a new general framework
that can be used for any value of k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5037</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5037</id><created>2012-11-21</created><updated>2014-08-01</updated><authors><author><keyname>Caron</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author><author><keyname>Murphy</keyname><forenames>Thomas Brendan</forenames></author></authors><title>Bayesian nonparametric Plackett-Luce models for the analysis of
  preferences for college degree programmes</title><categories>stat.ML cs.LG stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOAS717 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS717</report-no><journal-ref>Annals of Applied Statistics 2014, Vol. 8, No. 2, 1145-1181</journal-ref><doi>10.1214/14-AOAS717</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a Bayesian nonparametric model for clustering
partial ranking data. We start by developing a Bayesian nonparametric extension
of the popular Plackett-Luce choice model that can handle an infinite number of
choice items. Our framework is based on the theory of random atomic measures,
with the prior specified by a completely random measure. We characterise the
posterior distribution given data, and derive a simple and effective Gibbs
sampler for posterior simulation. We then develop a Dirichlet process mixture
extension of our model and apply it to investigate the clustering of
preferences for college degree programmes amongst Irish secondary school
graduates. The existence of clusters of applicants who have similar preferences
for degree programmes is established and we determine that subject matter and
geographical location of the third level institution characterise these
clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5052</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5052</id><created>2012-11-21</created><authors><author><keyname>Skliar</keyname><forenames>Osvaldo</forenames></author><author><keyname>Monge</keyname><forenames>Ricardo E.</forenames></author><author><keyname>Gapper</keyname><forenames>Sherry</forenames></author><author><keyname>Oviedo</keyname><forenames>Guillermo</forenames></author></authors><title>A Mathematical Random Number Generator (MRNG)</title><categories>cs.NA stat.CO</categories><comments>17 pages, 5 figures</comments><msc-class>65C10, 11K45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel Mathematical Random Number Generator (MRNG) is presented here. In
this case, &quot;mathematical&quot; refers to the fact that to construct that generator
it is not necessary to resort to a physical phenomenon, such as the thermal
noise of an electronic device, but rather to a mathematical procedure. The MRNG
generates binary strings - in principle, as long as desired - which may be
considered genuinely random in the sense that they pass the statistical tests
currently accepted to evaluate the randomness of those strings. From those
strings, the MRNG also generates random numbers expressed in base 10. An MRNG
has been installed as a facility on the following web page:
http://www.appliedmathgroup.org. This generator may be used for applications in
tasks in: a) computational simulation of probabilistic-type systems, and b) the
random selection of samples of different populations. Users interested in
applications in cryptography can build another MRNG, but they would have to
withhold information - specified in section 5 - from people who are not
authorized to decode messages encrypted using that resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5058</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5058</id><created>2012-11-21</created><authors><author><keyname>Golbabaee</keyname><forenames>Mohammad</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Compressed Sensing of Simultaneous Low-Rank and Joint-Sparse Matrices</title><categories>cs.IT math.IT</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of recovering a high dimensional data
matrix from a set of incomplete and noisy linear measurements. We introduce a
new model that can efficiently restrict the degrees of freedom of the problem
and is generic enough to find a lot of applications, for instance in
multichannel signal compressed sensing (e.g. sensor networks, hyperspectral
imaging) and compressive sparse principal component analysis (s-PCA). We assume
data matrices have a simultaneous low-rank and joint sparse structure, and we
propose a novel approach for efficient compressed sensing (CS) of such data.
Our CS recovery approach is based on a convex minimization problem that
incorporates this restrictive structure by jointly regularizing the solutions
with their nuclear (trace) norm and l2/l1 mixed norm. Our theoretical analysis
uses a new notion of restricted isometry property (RIP) and shows that, for
sampling schemes satisfying RIP, our approach can stably recover all low-rank
and joint-sparse matrices. For a certain class of random sampling schemes
satisfying a particular concentration bound (e.g. the subgaussian ensembles) we
derive a lower bound on the number of CS measurements indicating the
near-optimality of our recovery approach as well as a significant enhancement
compared to the state-of-the-art. We introduce an iterative algorithm based on
proximal calculus in order to solve the joint nuclear and l2/l1 norms
minimization problem and, finally, we illustrate the empirical recovery phase
transition of this approach by series of numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5060</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5060</id><created>2012-11-21</created><updated>2013-07-12</updated><authors><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Huynh</keyname><forenames>Khahn</forenames></author><author><keyname>Bamieh</keyname><forenames>Bassam</forenames></author><author><keyname>Khammash</keyname><forenames>Mustafa</forenames></author></authors><title>On sensor fusion for airborne wind energy systems</title><categories>cs.SY math.OC</categories><comments>This manuscript is a preprint of a paper accepted for publication on
  the IEEE Transactions on Control Systems Technology and is subject to IEEE
  Copyright. The copy of record is available at IEEEXplore library:
  http://ieeexplore.ieee.org/</comments><doi>10.1109/TCST.2013.2269865</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A study on filtering aspects of airborne wind energy generators is presented.
This class of renewable energy systems aims to convert the aerodynamic forces
generated by tethered wings, flying in closed paths transverse to the wind
flow, into electricity. The accurate reconstruction of the wing's position,
velocity and heading is of fundamental importance for the automatic control of
these kinds of systems. The difficulty of the estimation problem arises from
the nonlinear dynamics, wide speed range, large accelerations and fast changes
of direction that the wing experiences during operation. It is shown that the
overall nonlinear system has a specific structure allowing its partitioning
into sub-systems, hence leading to a series of simpler filtering problems.
Different sensor setups are then considered, and the related sensor fusion
algorithms are presented. The results of experimental tests carried out with a
small-scale prototype and wings of different sizes are discussed. The designed
filtering algorithms rely purely on kinematic laws, hence they are independent
from features like wing area, aerodynamic efficiency, mass, etc. Therefore, the
presented results are representative also of systems with larger size and
different wing design, different number of tethers and/or rigid wings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5063</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5063</id><created>2012-11-21</created><updated>2013-02-15</updated><authors><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the difficulty of training Recurrent Neural Networks</title><categories>cs.LG</categories><comments>Improved description of the exploding gradient problem and
  description and analysis of the vanishing gradient problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two widely known issues with properly training Recurrent Neural
Networks, the vanishing and the exploding gradient problems detailed in Bengio
et al. (1994). In this paper we attempt to improve the understanding of the
underlying issues by exploring these problems from an analytical, a geometric
and a dynamical systems perspective. Our analysis is used to justify a simple
yet effective solution. We propose a gradient norm clipping strategy to deal
with exploding gradients and a soft constraint for the vanishing gradients
problem. We validate empirically our hypothesis and proposed solutions in the
experimental section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5067</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5067</id><created>2012-11-21</created><authors><author><keyname>Suthisopapan</keyname><forenames>Puripong</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Meesomboon</keyname><forenames>Anupap</forenames></author><author><keyname>Imtawil</keyname><forenames>Virasit</forenames></author></authors><title>Approaching the Capacity of Large-Scale MIMO Systems via Non-Binary LDPC
  Codes</title><categories>cs.IT math.IT</categories><comments>29 pages. arXiv admin note: substantial text overlap with
  arXiv:1203.0960, arXiv:1204.4151</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the application of non-binary low-density parity-check
(NBLDPC) codes to MIMO systems which employ hundreds of antennas at both the
transmitter and the receiver has been proposed. Together with the well-known
low-complexity MMSE detection, the moderate length NBLDPC codes can operate
closer to the MIMO capacity, e.g., capacity-gap about 3.5 dB (the best known
gap is more than 7 dB). To further reduce the complexity of MMSE detection, a
novel soft output detection that can provide an excellent coded performance in
low SNR region with 99% complexity reduction is also proposed. The asymptotic
performance is analysed by using the Monte Carlo density evolution. It is found
that the NBLDPC codes can operate within 1.6 dB from the MIMO capacity.
Furthermore, the merit of using the NBLDPC codes in large MIMO systems with the
presence of imperfect channel estimation and spatial fading correlation which
are both the realistic scenarios for large MIMO systems is also pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5080</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5080</id><created>2012-11-21</created><authors><author><keyname>Jindal</keyname><forenames>Poonam</forenames></author><author><keyname>Singh</keyname><forenames>Brahmjit</forenames></author></authors><title>Study And Performance Evaluation Of Security-Throughput Tradeoff With
  Link Adaptive Encryption Scheme</title><categories>cs.CR</categories><comments>14 pages; IJSPTM, AIRCC, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ever increasing volume of information over wireless medium, security
has assumed an important dimension. The security of transmitted data over a
wireless channel aims at protecting the data from unauthorized intrusion.
Wireless network security is achieved using cryptographic primitives. Some
properties that give encryption mechanism their cryptographic strength also
make them very sensitive to channel error as well. Therefore, security for data
transmission over wireless channel results in throughput loss. Trade-off
between security and throughput is always a major concern in wireless networks.
In this paper, a Link Adaptive Encryption scheme is evaluated that adapts to
channel variations and enhances the security level of WLANs without making any
compromise with the network performance. Numerical results obtained through
simulation are compared with the fixed block length encryption technique in two
different modes of operation- Electronic Code Book (ECB) &amp; Cipher Block
Chaining (CBC). Optimal block length is also computed, which is assumed to be
the effective strength of the cipher. It has been observed that security
attained with link adaptive scheme operating in ECB mode of cipher is a better
solution for security and throughput trade-off. However, it is found that if
computational security is a major concern, link adaptive scheme in CBC mode
should be preferred.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5082</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5082</id><created>2012-11-20</created><authors><author><keyname>Clausel</keyname><forenames>Marianne</forenames></author><author><keyname>Oberlin</keyname><forenames>Thomas</forenames></author><author><keyname>Perrier</keyname><forenames>Val&#xe9;rie</forenames></author></authors><title>The Monogenic Synchrosqueezed Wavelet Transform: A tool for the
  Decomposition/Demodulation of AM-FM images</title><categories>math.NA cs.NA</categories><msc-class>65T60, 92C55, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synchrosqueezing method aims at decomposing 1D functions as
superpositions of a small number of &quot;Intrinsic Modes&quot;, supposed to be well
separated both in time and frequency. Based on the unidimensional wavelet
transform and its reconstruction properties, the synchrosqueezing transform
provides a powerful representation of multicomponent signals in the
time-frequency plane, together with a reconstruction of each mode.
  In this paper, a bidimensional version of the synchrosqueezing transform is
defined, by considering a well-adapted extension of the concept of analytic
signal to images: the monogenic signal. The natural bidimensional counterpart
of the notion of Intrinsic Mode is then the concept of &quot;Intrinsic Monogenic
Mode&quot; that we define. Thereafter, we investigate the properties of its
associated Monogenic Wavelet Decomposition. This leads to a natural bivariate
extension of the Synchrosqueezed Wavelet Transform, for decomposing and
processing multicomponent images. Numerical tests validate the effectiveness of
the method for different examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5084</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5084</id><created>2012-11-21</created><updated>2014-11-28</updated><authors><author><keyname>Wang</keyname><forenames>Haitao</forenames></author><author><keyname>Zhang</keyname><forenames>Wuzhou</forenames></author></authors><title>On Top-$k$ Weighted SUM Aggregate Nearest and Farthest Neighbors in the
  $L_1$ Plane</title><categories>cs.CG cs.DB cs.DS</categories><comments>24 pages; this version extends our results in the previous version to
  more general problem settings, and the title has been changed accordingly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study top-$k$ aggregate (or group) nearest neighbor queries
using the weighted SUM operator under the $L_1$ metric in the plane. Given a
set $P$ of $n$ points, for any query consisting of a set $Q$ of $m$ weighted
points and an integer $k$, $ 1 \le k \le n$, the top-$k$ aggregate nearest
neighbor query asks for the $k$ points of $P$ whose aggregate distances to $Q$
are the smallest, where the aggregate distance of each point $p$ of $P$ to $Q$
is the sum of the weighted distances from $p$ to all points of $Q$. We build an
$O(n\log n\log\log n)$-size data structure in $O(n\log n \log\log n)$ time,
such that each top-$k$ query can be answered in $O(m\log m+(k+m)\log^2 n)$
time. We also obtain other results with trade-off between preprocessing and
query. Even for the special case where $k=1$, our results are better than the
previously best method (in PODS 2012), which requires $O(n\log^2 n)$
preprocessing time, $O(n\log^2 n)$ space, and $O(m^2\log^3 n)$ query time. In
addition, for the one-dimensional version of this problem, our approach can
build an $O(n)$-size data structure in $O(n\log n)$ time that can support
$O(\min\{k,\log m\}\cdot m+k+\log n)$ time queries. Further, we extend our
techniques to the top-$k$ aggregate farthest neighbor queries, with the same
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5086</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5086</id><created>2012-11-21</created><authors><author><keyname>Fischer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Reinhardt</keyname><forenames>Marc</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Optimal Sequence-Based Control and Estimation of Networked Linear
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a unified approach to sequence-based control and estimation of
linear networked systems with multiple sensors is proposed. Time delays and
data losses in the controller-actuator-channel are compensated by sending
sequences of control inputs. The sequence-based design paradigm is further
extended to the sensor-controller-channels without increasing the load of the
network. In this context, we present a recursive solution based on the
Hypothesizing Distributed Kalman Filter (HKF) that is included in the overall
sequence-based controller design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5098</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5098</id><created>2012-11-21</created><authors><author><keyname>Cody-Kenny</keyname><forenames>Brendan</forenames></author><author><keyname>Barrett</keyname><forenames>Stephen</forenames></author></authors><title>Scaling Genetic Programming for Source Code Modification</title><categories>cs.NE cs.SE</categories><comments>4 pages, Accepted for Graduate Student Workshop, GECCO 2012,
  Retracted by Authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Search Based Software Engineering, Genetic Programming has been used for
bug fixing, performance improvement and parallelisation of programs through the
modification of source code. Where an evolutionary computation algorithm, such
as Genetic Programming, is to be applied to similar code manipulation tasks,
the complexity and size of source code for real-world software poses a
scalability problem. To address this, we intend to inspect how the Software
Engineering concepts of modularity, granularity and localisation of change can
be reformulated as additional mechanisms within a Genetic Programming
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5107</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5107</id><created>2012-11-21</created><authors><author><keyname>Schlipf</keyname><forenames>Lena</forenames></author></authors><title>Notes on Convex Transversals</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove the problem of stabbing a set of disjoint bends by a
convex stabber to be NP-hard. We also consider the optimization version of the
convex stabber problem and prove this problem to be APX-hard for sets of line
segments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5108</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5108</id><created>2012-11-21</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Mignosi</keyname><forenames>Filippo</forenames></author></authors><title>The Rightmost Equal-Cost Position Problem</title><categories>cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LZ77-based compression schemes compress the input text by replacing factors
in the text with an encoded reference to a previous occurrence formed by the
couple (length, offset). For a given factor, the smallest is the offset, the
smallest is the resulting compression ratio. This is optimally achieved by
using the rightmost occurrence of a factor in the previous text. Given a cost
function, for instance the minimum number of bits used to represent an integer,
we define the Rightmost Equal-Cost Position (REP) problem as the problem of
finding one of the occurrences of a factor which cost is equal to the cost of
the rightmost one. We present the Multi-Layer Suffix Tree data structure that,
for a text of length n, at any time i, it provides REP(LPF) in constant time,
where LPF is the longest previous factor, i.e. the greedy phrase, a reference
to the list of REP({set of prefixes of LPF}) in constant time and REP(p) in
time O(|p| log log n) for any given pattern p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5122</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5122</id><created>2012-11-21</created><authors><author><keyname>Seamone</keyname><forenames>Ben</forenames></author></authors><title>The 1-2-3 Conjecture and related problems: a survey</title><categories>math.CO cs.DM</categories><comments>30 pages, 2 tables, submitted for publication</comments><msc-class>05C15, 05C78, 01-02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 1-2-3 Conjecture, posed in 2004 by Karonski, Luczak, and Thomason, is as
follows: &quot;If G is a graph with no connected component having exactly 2
vertices, then the edges of G may be assigned weights from the set {1,2,3} so
that, for any adjacent vertices u and v, the sum of weights of edges incident
to u differs from the sum of weights of edges incident to v.&quot; This survey paper
presents the current state of research on the 1-2-3 Conjecture and the many
variants that have been proposed in its short but active history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5124</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5124</id><created>2012-11-21</created><authors><author><keyname>van Raan</keyname><forenames>Anthony F. J.</forenames></author></authors><title>Universities Scale Like Cities</title><categories>nlin.AO cs.DL physics.soc-ph</categories><comments>16 pages, 17 figures</comments><doi>10.1371/journal.pone.0059384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies of urban scaling show that important socioeconomic city
characteristics such as wealth and innovation capacity exhibit a nonlinear,
particularly a power law scaling with population size. These nonlinear effects
are common to all cities, with similar power law exponents. These findings mean
that the larger the city, the more disproportionally they are places of wealth
and innovation. Local properties of cities cause a deviation from the expected
behavior as predicted by the power law scaling. In this paper we demonstrate
that universities show a similar behavior as cities in the distribution of the
gross university income in terms of total number of citations over size in
terms of total number of publications. Moreover, the power law exponents for
university scaling are comparable to those for urban scaling. We find that
deviations from the expected behavior can indeed be explained by specific local
properties of universities, particularly the field-specific composition of a
university, and its quality in terms of field-normalized citation impact. By
studying both the set of the 500 largest universities worldwide and a specific
subset of these 500 universities -- the top-100 European universities -- we are
also able to distinguish between properties of universities with as well as
without selection of one specific local property, the quality of a university
in terms of its average field-normalized citation impact. It also reveals an
interesting observation concerning the working of a crucial property in
networked systems, preferential attachment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5157</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5157</id><created>2012-11-21</created><authors><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>To Relay or Not To Relay in Cognitive Radio Sensor Networks</title><categories>cs.NI cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works proposed the relaying at the MAC layer in cognitive radio
networks whereby the primary packets are forwarded by the secondary node
maintaining an extra queue devoted to the relaying function. However, relaying
of primary packets may introduce delays on the secondary packets (called
secondary delay) and require additional power budget in order to forward the
primary packets that is especially crucial when the network is deployed using
sensors with limited power resources. To this end, an admission control can be
employed in order to manage efficiently the relaying in cognitive radio sensor
networks. In this paper, we first analyse and formulate the secondary delay and
the required power budget of the secondary sensor node in relation with the
acceptance factor that indicates whether the primary packets are allowed to be
forwarded or not. Having defined the above, we present the tradeoff between the
secondary delay and the required power budget when the acceptance factor is
adapted. In the sequel, we formulate an optimization problem to minimize the
secondary delay over the admission control parameter subject to a limit on the
required power budget plus the constraints related to the stabilities of the
individual queues due to their interdependencies observed by the analysis. The
solution of this problem is provided using iterative decomposition methods i.e.
dual and primal decompositions using Lagrange multipliers that simplifies the
original complicated problem resulting in a final equivalent dual problem that
includes the initial Karush Kuhn Tucker conditions. Using the derived
equivalent dual problem, we obtain the optimal acceptance factor while in
addition we highlight the possibilities for extra delay minimization that is
provided by relaxing the initial constraints through changing the values of the
Lagrange multipliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5164</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5164</id><created>2012-11-21</created><updated>2012-12-30</updated><authors><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>State Evolution for General Approximate Message Passing Algorithms, with
  Applications to Spatial Coupling</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>29 pages, 1 figure, minor updates in citations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of approximated message passing (AMP) algorithms and
characterize their high-dimensional behavior in terms of a suitable state
evolution recursion. Our proof applies to Gaussian matrices with independent
but not necessarily identically distributed entries. It covers --in
particular-- the analysis of generalized AMP, introduced by Rangan, and of AMP
reconstruction in compressed sensing with spatially coupled sensing matrices.
The proof technique builds on the one of [BM11], while simplifying and
generalizing several steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5167</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5167</id><created>2012-11-21</created><authors><author><keyname>Bettencourt</keyname><forenames>Luis M. A.</forenames></author><author><keyname>Trancik</keyname><forenames>Jessika E.</forenames></author><author><keyname>Kaur</keyname><forenames>Jasleen</forenames></author></authors><title>Determinants of the Pace of Global Innovation in Energy Technologies</title><categories>physics.soc-ph cs.CY stat.AP</categories><doi>10.1371/journal.pone.0067864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the factors driving innovation in energy technologies is of
critical importance to mitigating climate change and addressing other
energy-related global challenges. Low levels of innovation, measured in terms
of energy patent filings, were noted in the 1980s and 90s as an issue of
concern and were attributed to low investment in public and private research
and development (R&amp;D). Here we build a comprehensive global database of energy
patents covering the period 1970-2009 which is unique in its temporal and
geographical scope. Analysis of the data reveals a recent, marked departure
from historical trends. A sharp increase in rates of patenting has occurred
over the last decade, particularly in renewable technologies, despite continued
low levels of R&amp;D funding. To solve the puzzle of fast innovation despite
modest R&amp;D increases we develop a model that explains the nonlinear response
observed in the empirical data of technological innovation to various types of
investment. The model reveals a regular relationship between patents, R&amp;D
funding, and growing markets across technologies, and accurately predicts
patenting rates at different stages of technological maturity and market
development. We show quantitatively how growing markets have formed a vital
complement to public R&amp;D in driving innovative activity; these two forms of
investment have each leveraged the effect of the other in driving patenting
trends over long periods of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5173</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5173</id><created>2012-11-21</created><authors><author><keyname>Mulali&#x107;</keyname><forenames>Edin H.</forenames></author><author><keyname>Stankovi&#x107;</keyname><forenames>Miomir S.</forenames></author><author><keyname>Stankovi&#x107;</keyname><forenames>Radomir S.</forenames></author></authors><title>Memoization technique for optimizing functions with stochastic input</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a strategy for optimization functions with
stochastic input. The main idea is to take advantage of decomposition in
combination with a look-up table. Deciding what input values should be used for
memoization is determined based on the underlying probability distribution of
input variables. Special attention is given to difficulties caused by
combinatorial explosion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5183</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5183</id><created>2012-11-21</created><updated>2013-07-30</updated><authors><author><keyname>Chaabane</keyname><forenames>Abdelberi</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohammed-Ali</forenames></author><author><keyname>Uzun</keyname><forenames>Ersin</forenames></author></authors><title>Privacy in Content-Oriented Networking: Threats and Countermeasures</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the Internet struggles to cope with scalability, mobility, and security
issues, new network architectures are being proposed to better accommodate the
needs of modern systems and applications. In particular, Content-Oriented
Networking (CON) has emerged as a promising next-generation Internet
architecture: it sets to decouple content from hosts, at the network layer, by
naming data rather than hosts. CON comes with a potential for a wide range of
benefits, including reduced congestion and improved delivery speed by means of
content caching, simpler configuration of network devices, and security at the
data level. However, it remains an interesting open question whether or not,
and to what extent, this emerging networking paradigm bears new privacy
challenges. In this paper, we provide a systematic privacy analysis of CON and
the common building blocks among its various architectural instances in order
to highlight emerging privacy threats, and analyze a few potential
countermeasures. Finally, we present a comparison between CON and today's
Internet in the context of a few privacy concepts, such as, anonymity,
censoring, traceability, and confidentiality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5184</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5184</id><created>2012-11-21</created><authors><author><keyname>Zhou</keyname><forenames>Zhuojie</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Gong</keyname><forenames>Zhiguo</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Faster Random Walks By Rewiring Online Social Networks On-The-Fly</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>15 pages, 14 figure, technical report for ICDE2013 paper. Appendix
  has all the theorems' proofs; ICDE'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many online social networks feature restrictive web interfaces which only
allow the query of a user's local neighborhood through the interface. To enable
analytics over such an online social network through its restrictive web
interface, many recent efforts reuse the existing Markov Chain Monte Carlo
methods such as random walks to sample the social network and support analytics
based on the samples. The problem with such an approach, however, is the large
amount of queries often required (i.e., a long &quot;mixing time&quot;) for a random walk
to reach a desired (stationary) sampling distribution.
  In this paper, we consider a novel problem of enabling a faster random walk
over online social networks by &quot;rewiring&quot; the social network on-the-fly.
Specifically, we develop Modified TOpology (MTO)-Sampler which, by using only
information exposed by the restrictive web interface, constructs a &quot;virtual&quot;
overlay topology of the social network while performing a random walk, and
ensures that the random walk follows the modified overlay topology rather than
the original one. We show that MTO-Sampler not only provably enhances the
efficiency of sampling, but also achieves significant savings on query cost
over real-world online social networks such as Google Plus, Epinion etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5189</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5189</id><created>2012-11-21</created><updated>2013-10-22</updated><authors><author><keyname>Shankar</keyname><forenames>Karthik H.</forenames></author><author><keyname>Howard</keyname><forenames>Marc W.</forenames></author></authors><title>Optimally fuzzy temporal memory</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any learner with the ability to predict the future of a structured
time-varying signal must maintain a memory of the recent past. If the signal
has a characteristic timescale relevant to future prediction, the memory can be
a simple shift register---a moving window extending into the past, requiring
storage resources that linearly grows with the timescale to be represented.
However, an independent general purpose learner cannot a priori know the
characteristic prediction-relevant timescale of the signal. Moreover, many
naturally occurring signals show scale-free long range correlations implying
that the natural prediction-relevant timescale is essentially unbounded. Hence
the learner should maintain information from the longest possible timescale
allowed by resource availability. Here we construct a fuzzy memory system that
optimally sacrifices the temporal accuracy of information in a scale-free
fashion in order to represent prediction-relevant information from
exponentially long timescales. Using several illustrative examples, we
demonstrate the advantage of the fuzzy memory system over a shift register in
time series forecasting of natural signals. When the available storage
resources are limited, we suggest that a general purpose learner would be
better off committing to such a fuzzy memory system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5190</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5190</id><created>2012-11-21</created><updated>2012-11-28</updated><authors><author><keyname>Mardare</keyname><forenames>Radu</forenames><affiliation>Aalborg University, Denmark</affiliation></author><author><keyname>Cardelli</keyname><forenames>Luca</forenames><affiliation>Microsoft Research Cambridge, UK</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University, Denmark</affiliation></author></authors><title>Continuous Markovian Logics - Axiomatization and Quantified Metatheory</title><categories>cs.LO</categories><comments>Extended version of a paper presented at CSL2011</comments><proxy>LMCS</proxy><acm-class>F.4.1, G.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (November
  29, 2012) lmcs:937</journal-ref><doi>10.2168/LMCS-8(4:19)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous Markovian Logic (CML) is a multimodal logic that expresses
quantitative and qualitative properties of continuous-time labelled Markov
processes with arbitrary (analytic) state-spaces, henceforth called continuous
Markov processes (CMPs). The modalities of CML evaluate the rates of the
exponentially distributed random variables that characterize the duration of
the labeled transitions of a CMP. In this paper we present weak and strong
complete axiomatizations for CML and prove a series of metaproperties,
including the finite model property and the construction of canonical models.
CML characterizes stochastic bisimilarity and it supports the definition of a
quantified extension of the satisfiability relation that measures the
&quot;compatibility&quot; between a model and a property. In this context, the
metaproperties allows us to prove two robustness theorems for the logic stating
that one can perturb formulas and maintain &quot;approximate satisfaction&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5207</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5207</id><created>2012-11-22</created><authors><author><keyname>Seong</keyname><forenames>Jin-Taek</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author></authors><title>On the Compressed Measurements over Finite Fields: Sparse or Dense
  Sampling</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures, other essential info</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider compressed sampling over finite fields and investigate the number
of compressed measurements needed for successful L0 recovery. Our results are
obtained while the sparseness of the sensing matrices as well as the size of
the finite fields are varied. One of interesting conclusions includes that
unless the signal is &quot;ultra&quot; sparse, the sensing matrices do not have to be
dense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5221</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5221</id><created>2012-11-22</created><authors><author><keyname>Malekian</keyname><forenames>Reza</forenames></author><author><keyname>Abdullah</keyname><forenames>Abdul Hanan</forenames></author></authors><title>Traffic Engineering Based on Effective Envelope Algorithm on Novel
  Resource Reservation Method over Mobile Internet Protocol Version 6</title><categories>cs.NI</categories><comments>International Journal of Innovative Computing, Information and
  Control, 2012</comments><msc-class>68U35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first decade of the 21st century has seen tremendous improvements in
mobile internet and its technologies. The high traffic volume of services such
as video conference and other real-time traffic applications are imposing a
great challenge on networks. In the meantime, demand for the use of mobile
devices in computation and communication such as smart phones, personal digital
assistants, and mobile-enabled laptops has grown rapidly. These services have
driven the demand for increasing and guaranteing bandwidth requirements in the
network. A direction of this paper is in the case of resource reservation
protocol (RSVP) over mobile IPv6 networks. There are numbers of proposed
solutions for RSVP and quality of service provision over mobile IPv6 networks,
but most of them using advanced resource reservation. In this paper, we propose
a mathematical model to determine maximum end-to-end delay bound through
intermediate routers along the network. These bounds are sent back to the home
agent for further processing. Once the home agent receives maximum end-to-end
delay bounds, it calculates cumulative bound and compares this bound with the
desired application end-to-end delay bound to make final decision on resource
reservation. This approach improves network resource utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5227</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5227</id><created>2012-11-22</created><authors><author><keyname>Mannava</keyname><forenames>Vishnuvardhan</forenames></author><author><keyname>Ramesh</keyname><forenames>T.</forenames></author></authors><title>Service Composition Design Pattern for Autonomic Computing Systems using
  Association Rule based Learning and Service-Oriented Architecture</title><categories>cs.SE cs.DC cs.LG</categories><comments>19 pages, 7 figures, International Journal of Grid Computing &amp;
  Applications (IJGCA). arXiv admin note: text overlap with arXiv:1208.3836</comments><acm-class>D.2.11; D.2.10; D.3.3; H.2.8; I.2.6</acm-class><journal-ref>IJGCA, 3(3), 21-39 (2012)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we present a Service Injection and composition Design Pattern
for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented
design patterns, and amalgamation of the Strategy, Worker Object, and
Check-List Design Patterns used to design the Self-Adaptive Systems. It will
apply self reconfiguration planes dynamically without the interruption or
intervention of the administrator for handling service failures at the servers.
When a client requests for a complex service, Service Composition should be
done to fulfil the request. If a service is not available in the memory, it
will be injected as Aspectual Feature Module code. We used Service Oriented
Architecture (SOA) with Web Services in Java to Implement the composite Design
Pattern. As far as we know, there are no studies on composition of design
patterns for Peer-to-peer computing domain. The pattern is described using a
java-like notation for the classes and interfaces. A simple UML class and
Sequence diagrams are depicted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5231</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5231</id><created>2012-11-22</created><authors><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author><author><keyname>Kopsinis</keyname><forenames>Yannis</forenames></author><author><keyname>Slavakis</keyname><forenames>Konstantinos</forenames></author></authors><title>Sparsity-Aware Learning and Compressed Sensing: An Overview</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is based on a chapter of a new book on Machine Learning, by the
first and third author, which is currently under preparation. We provide an
overview of the major theoretical advances as well as the main trends in
algorithmic developments in the area of sparsity-aware learning and compressed
sensing. Both batch processing and online processing techniques are considered.
A case study in the context of time-frequency analysis of signals is also
presented. Our intent is to update this review from time to time, since this is
a very hot research area with a momentum and speed that is sometimes difficult
to follow up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5248</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5248</id><created>2012-11-22</created><authors><author><keyname>Dutta</keyname><forenames>Chaitali Biswas</forenames></author><author><keyname>Garai</keyname><forenames>Partha</forenames></author><author><keyname>Sinha</keyname><forenames>Amitabha</forenames></author></authors><title>Design Of A Reconfigurable DSP Processor With Bit Efficient Residue
  Number System</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Residue Number System (RNS), which originates from the Chinese Remainder
Theorem, offers a promising future in VLSI because of its carry-free operations
in addition, subtraction and multiplication. This property of RNS is very
helpful to reduce the complexity of calculation in many applications. A residue
number system represents a large integer using a set of smaller integers,
called residues. But the area overhead, cost and speed not only depend on this
word length, but also the selection of moduli, which is a very crucial step for
residue system. This parameter determines bit efficiency, area, frequency etc.
In this paper a new moduli set selection technique is proposed to improve bit
efficiency which can be used to construct a residue system for digital signal
processing environment. Subsequently, it is theoretically proved and
illustrated using examples, that the proposed solution gives better results
than the schemes reported in the literature. The novelty of the architecture is
shown by comparison the different schemes reported in the literature. Using the
novel moduli set, a guideline for a Reconfigurable Processor is presented here
that can process some predefined functions. As RNS minimizes the carry
propagation, the scheme can be implemented in Real Time Signal Processing &amp;
other fields where high speed computations are required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5251</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5251</id><created>2012-11-22</created><authors><author><keyname>del Rio</keyname><forenames>&#xc1;ngel</forenames></author><author><keyname>Rif&#xe0;</keyname><forenames>Josep</forenames></author></authors><title>Families of Hadamard Z2Z4Q8-codes</title><categories>cs.IT math.CO math.IT</categories><comments>Submitted to IT-IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Z2Z4Q8-code is a non-empty subgroup of a direct product of copies of Z_2,
Z_4 and Q_8 (the binary field, the ring of integers modulo 4 and the quaternion
group on eight elements, respectively). Such Z2Z4Q8-codes are translation
invariant propelinear codes as the well known Z_4-linear or Z_2Z_4-linear
codes.
  In the current paper, we show that there exist &quot;pure&quot; Z2Z4Q8-codes, that is,
codes that do not admit any abelian translation invariant propelinear
structure. We study the dimension of the kernel and rank of the Z2Z4Q8-codes,
and we give upper and lower bounds for these parameters. We give tools to
construct a new class of Hadamard codes formed by several families of
Z2Z4Q8-codes; we study and show the different shapes of such a codes and we
improve the upper and lower bounds for the rank and the dimension of the kernel
when the codes are Hadamard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5252</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5252</id><created>2012-11-22</created><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Non-Asymptotic Analysis of Privacy Amplification via Renyi Entropy and
  Inf-Spectral Entropy</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the privacy amplification problem, and compares the
existing two bounds: the exponential bound derived by one of the authors and
the min-entropy bound derived by Renner. It turns out that the exponential
bound is better than the min-entropy bound when a security parameter is rather
small for a block length, and that the min-entropy bound is better than the
exponential bound when a security parameter is rather large for a block length.
Furthermore, we present another bound that interpolates the exponential bound
and the min-entropy bound by a hybrid use of the Renyi entropy and the
inf-spectral entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5256</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5256</id><created>2012-11-22</created><updated>2013-03-25</updated><authors><author><keyname>Boral</keyname><forenames>Anudhyan</forenames></author><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Model Checking Parse Trees</title><categories>cs.LO</categories><comments>21 + x pages</comments><acm-class>F.4; D.3.1; I.2.7</acm-class><journal-ref>LICS 2013, pp. 153-162, IEEE Computer Society Press</journal-ref><doi>10.1109/LICS.2013.21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parse trees are fundamental syntactic structures in both computational
linguistics and compilers construction. We argue in this paper that, in both
fields, there are good incentives for model-checking sets of parse trees for
some word according to a context-free grammar. We put forward the adequacy of
propositional dynamic logic (PDL) on trees in these applications, and study as
a sanity check the complexity of the corresponding model-checking problem:
although complete for exponential time in the general case, we find natural
restrictions on grammars for our applications and establish complexities
ranging from nondeterministic polynomial time to polynomial space in the
relevant cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5257</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5257</id><created>2012-11-22</created><updated>2013-03-17</updated><authors><author><keyname>Rif&#xe0;</keyname><forenames>Josep</forenames></author><author><keyname>Zinoviev</keyname><forenames>Victor</forenames></author></authors><title>On two families of binary quadratic bent functions</title><categories>cs.IT math.IT</categories><comments>Submitted to &quot;Problems of Information Transmission&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct two families of binary quadratic bent functions in a
combinatorial way. They are self-dual and anti-self-dual quadratic bent
functions, respectively, which are not of the Maiorana-McFarland type, but
affine equivalent to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5259</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5259</id><created>2012-11-22</created><authors><author><keyname>Karandikar</keyname><forenames>Prateek</forenames></author><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>The Parametric Ordinal-Recursive Complexity of Post Embedding Problems</title><categories>cs.LO</categories><comments>16 + vii pages</comments><acm-class>F.2</acm-class><journal-ref>FoSSaCS 2013, LNCS 7794, pp. 273--288, Springer</journal-ref><doi>10.1007/978-3-642-37075-5_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post Embedding Problems are a family of decision problems based on the
interaction of a rational relation with the subword embedding ordering, and are
used in the literature to prove non multiply-recursive complexity lower bounds.
We refine the construction of Chambart and Schnoebelen (LICS 2008) and prove
parametric lower bounds depending on the size of the alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5264</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5264</id><created>2012-11-22</created><updated>2014-02-20</updated><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author></authors><title>Source and Channel Polarization over Finite Fields and Reed-Solomon
  Matrices</title><categories>cs.IT math.IT</categories><comments>17 pages, 3 figures, accepted for publication in the IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polarization phenomenon over any finite field $\mathbb{F}_{q}$ with size $q$
being a power of a prime is considered. This problem is a generalization of the
original proposal of channel polarization by Arikan for the binary field, as
well as its extension to a prime field by Sasoglu, Telatar, and Arikan. In this
paper, a necessary and sufficient condition of a matrix over a finite field
$\mathbb{F}_q$ is shown under which any source and channel are polarized.
Furthermore, the result of the speed of polarization for the binary alphabet
obtained by Arikan and Telatar is generalized to arbitrary finite field. It is
also shown that the asymptotic error probability of polar codes is improved by
using the Reed-Solomon matrix, which can be regarded as a natural
generalization of the $2\times 2$ binary matrix used in the original proposal
by Arikan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5283</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5283</id><created>2012-11-22</created><updated>2013-05-07</updated><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author></authors><title>DNF-AF Selection Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to the requirement of
  the submitted journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error propagation and noise propagation at the relay node would highly
degrade system performance in two-way relay networks. In this paper, we
introduce DNF-AF selection two-way relaying scheme which aims to avoid error
propagation and mitigate noise propagation. If the relay successfully decodes
the exclusive or (XOR) of the messages sent by the two transceivers, it applies
denoise-and-forward (DNF). Otherwise, amplify-and-forward (AF) strategy will be
utilized. In this way, decoding error propagation is avoided at the relay.
Meanwhile, since the relay attempts to decode the XOR of the two messages
instead of explicitly decoding the two messages, the larger usable range of XOR
network coding can be obtained. As XOR network coding can avoid noise
propagation, DNF-AF would mitigate noise propagation. In addition, bit error
rate (BER) performance of DNF-AF selection scheme with BPSK modulation is
theoretically analyzed in this paper. Numerical results verify that the
proposed scheme has better BER performance than existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5292</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5292</id><created>2012-11-22</created><updated>2014-07-15</updated><authors><author><keyname>Bernabeu</keyname><forenames>Miguel O.</forenames></author><author><keyname>Nash</keyname><forenames>Rupert W.</forenames></author><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Carver</keyname><forenames>Hywel B.</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Kr&#xfc;ger</keyname><forenames>Timm</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Impact of blood rheology on wall shear stress in a model of the middle
  cerebral artery</title><categories>cs.CE physics.flu-dyn physics.med-ph</categories><comments>14 pages, 6 figures, published at Interface Focus</comments><journal-ref>Interface Focus 6 April 2013 vol. 3 no. 2 20120094</journal-ref><doi>10.1098/rsfs.2012.0094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perturbations to the homeostatic distribution of mechanical forces exerted by
blood on the endothelial layer have been correlated with vascular pathologies
including intracranial aneurysms and atherosclerosis. Recent computational work
suggests that in order to correctly characterise such forces, the
shear-thinning properties of blood must be taken into account. To the best of
our knowledge, these findings have never been compared against experimentally
observed pathological thresholds. In the current work, we apply the three-band
diagram (TBD) analysis due to Gizzi et al. to assess the impact of the choice
of blood rheology model on a computational model of the right middle cerebral
artery. Our results show that, in the model under study, the differences
between the wall shear stress predicted by a Newtonian model and the well known
Carreau-Yasuda generalized Newtonian model are only significant if the vascular
pathology under study is associated with a pathological threshold in the range
0.94 Pa to 1.56 Pa, where the results of the TBD analysis of the rheology
models considered differs. Otherwise, we observe no significant differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5307</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5307</id><created>2012-11-22</created><authors><author><keyname>Petrosyan</keyname><forenames>P. A.</forenames></author><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>On sum edge-coloring of regular, bipartite and split graphs</title><categories>math.CO cs.DM</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An edge-coloring of a graph $G$ with natural numbers is called a sum
edge-coloring if the colors of edges incident to any vertex of $G$ are distinct
and the sum of the colors of the edges of $G$ is minimum. The edge-chromatic
sum of a graph $G$ is the sum of the colors of edges in a sum edge-coloring of
$G$. It is known that the problem of finding the edge-chromatic sum of an
$r$-regular ($r\geq 3$) graph is $NP$-complete. In this paper we give a
polynomial time $(1+\frac{2r}{(r+1)^{2}})$-approximation algorithm for the
edge-chromatic sum problem on $r$-regular graphs for $r\geq 3$. Also, it is
known that the problem of finding the edge-chromatic sum of bipartite graphs
with maximum degree 3 is $NP$-complete. We show that the problem remains
$NP$-complete even for some restricted class of bipartite graphs with maximum
degree 3. Finally, we give upper bounds for the edge-chromatic sum of some
split graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5311</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5311</id><created>2012-11-22</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author></authors><title>Interval colorings of complete balanced multipartite graphs</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A graph $G$ is called a complete $k$-partite ($k\geq 2$) graph if its
vertices can be partitioned into $k$ independent sets $V_{1},...,V_{k}$ such
that each vertex in $V_{i}$ is adjacent to all the other vertices in $V_{j}$
for $1\leq i&lt;j\leq k$. A complete $k$-partite graph $G$ is a complete balanced
$k$-partite graph if $|V_{1}| = |V_{2}| =... = |V_{k}|$. An edge-coloring of a
graph $G$ with colors $1,...,t$ is an interval $t$-coloring if all colors are
used, and the colors of edges incident to each vertex of $G$ are distinct and
form an interval of integers. A graph $G$ is interval colorable if $G$ has an
interval $t$-coloring for some positive integer $t$. In this paper we show that
a complete balanced $k$-partite graph $G$ with $n$ vertices in each part is
interval colorable if and only if $nk$ is even. We also prove that if $nk$ is
even and $(k-1)n\leq t\leq ((3/2)k-1)n-1$, then a complete balanced $k$-partite
graph $G$ admits an interval $t$-coloring. Moreover, if $k=p2^{q}$, where $p$
is odd and $q\in \mathbb{N}$, then a complete balanced $k$-partite graph $G$
has an interval $t$-coloring for each positive integer $t$ satisfying
$(k-1)n\leq t\leq (2k-p-q)n-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5322</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5322</id><created>2012-11-22</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>What is Nature-like Computation? A Behavioural Approach and a Notion of
  Programmability</title><categories>cs.LO cs.CY</categories><comments>31 pages, 4 figures, special issue on History and Philosophy of
  Computing. Philosophy &amp; Technology, Springer, 2012. The final publication is
  available at http://springerlink.com</comments><doi>10.1007/s13347-012-0095-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to propose an alternative behavioural definition of
computation (and of a computer) based simply on whether a system is capable of
reacting to the environment-the input-as reflected in a measure of
programmability. This definition is intended to have relevance beyond the realm
of digital computers, particularly vis-a-vis natural systems. This will be done
by using an extension of a phase transition coefficient previously defined in
an attempt to characterise the dynamical behaviour of cellular automata and
other systems. The transition coefficient measures the sensitivity of a system
to external stimuli, and will be used to define the susceptibility of a system
to being (efficiently) programmed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5329</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5329</id><created>2012-11-22</created><authors><author><keyname>Viossat</keyname><forenames>Yannick</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Game Dynamics and Nash Equilibria</title><categories>cs.GT math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a game has a unique Nash equilibrium, then this equilibrium is arguably
the solution of the game from the refinement's literature point of view.
However, it might be that for almost all initial conditions, all strategies in
the support of this equilibrium are eliminated by the replicator dynamics and
the best-reply dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5350</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5350</id><created>2012-11-22</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Mignosi</keyname><forenames>Filippo</forenames></author></authors><title>Note on the Greedy Parsing Optimality for Dictionary-Based Text
  Compression</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic dictionary-based compression schemes are the most daily used data
compression schemes since they appeared in the foundational papers of Ziv and
Lempel in 1977, commonly referred to as LZ77. Their work is the base of
Deflate, gZip, WinZip, 7Zip and many others compression software. All of those
compression schemes use variants of the greedy approach to parse the text into
dictionary phrases. Greedy parsing optimality was proved by Cohn et al. (1996)
for fixed length code and unbounded dictionaries. The optimality of the greedy
parsing was never proved for bounded size dictionary which actually all of
those schemes require. We define the suffix-closed property for dynamic
dictionaries and we show that any LZ77-based dictionary, including the bounded
variants, satisfy this property. Under this condition we prove the optimality
of the greedy parsing as a variant of the proof by Cohn et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5353</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5353</id><created>2012-11-22</created><authors><author><keyname>Konow</keyname><forenames>Roberto</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Faster Compact Top-k Document Retrieval</title><categories>cs.DS cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimal index solving top-k document retrieval [Navarro and Nekrich,
SODA12] takes O(m + k) time for a pattern of length m, but its space is at
least 80n bytes for a collection of n symbols. We reduce it to 1.5n to 3n
bytes, with O(m+(k+log log n) log log n) time, on typical texts. The index is
up to 25 times faster than the best previous compressed solutions, and requires
at most 5% more space in practice (and in some cases as little as one half).
Apart from replacing classical by compressed data structures, our main idea is
to replace suffix tree sampling by frequency thresholding to achieve
compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5355</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5355</id><created>2012-11-22</created><authors><author><keyname>Kundu</keyname><forenames>Raka</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Lenka</keyname><forenames>Prasanna K.</forenames></author></authors><title>Cobb Angle Measurement of Scoliosis with Reduced Variability</title><categories>cs.CV</categories><comments>MedImage2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cobb angle, which is a measure of spinal curvature is the standard method for
quantifying the magnitude of Scoliosis related to spinal deformity in
orthopedics. Determining the Cobb angle through manual process is subject to
human errors. In this work, we propose a methodology to measure the magnitude
of Cobb angle, which appreciably reduces the variability related to its
measurement compared to the related works. The proposed methodology is
facilitated by using a suitable new improved version of Non-Local Means for
image denoisation and Otsus automatic threshold selection for Canny edge
detection. We have selected NLM for preprocessing of the image as it is one of
the fine states of art for image denoisation and helps in retaining the image
quality. Trimmedmean, median are more robust to outliners than mean and
following this concept we observed that NLM denoising quality performance can
be enhanced by using Euclidean trimmed-mean replacing the mean. To prove the
better performance of the Non-Local Euclidean Trimmed-mean denoising filter, we
have provided some comparative study results of the proposed denoising
technique with traditional NLM and NonLocal Euclidean Medians. The experimental
results for Cobb angle measurement over intra observer and inter observer
experimental data reveals the better performance and superiority of the
proposed approach compared to the related works. MATLAB2009b image processing
toolbox was used for the purpose of simulation and verification of the proposed
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5358</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5358</id><created>2012-11-22</created><updated>2014-08-30</updated><authors><author><keyname>Athanasiadou</keyname><forenames>Sophia</forenames></author><author><keyname>Gatzianas</keyname><forenames>Marios</forenames></author><author><keyname>Georgiadis</keyname><forenames>Leonidas</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Stable XOR-based Policies for the Broadcast Erasure Channel with
  Feedback</title><categories>cs.IT math.IT</categories><comments>40 pages, submitted to IEEE Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a network coding scheme for the Broadcast Erasure
Channel with multiple unicast stochastic flows, in the case of a single source
transmitting packets to $N$ users, where per-slot feedback is fed back to the
transmitter in the form of ACK/NACK messages. This scheme performs only binary
(XOR) operations and involves a network of queues, along with special rules for
coding and moving packets among the queues, that ensure instantaneous
decodability. The system under consideration belongs to a class of networks
whose stability properties have been analyzed in earlier work, which is used to
provide a stabilizing policy employing the currently proposed coding scheme.
Finally, we show the optimality of the proposed policy for $N=4$ and i.i.d.
erasure events, in the sense that the policy's stability region matches a
derived outer bound (which coincides with the system's information-theoretic
capacity region), even when a restricted set of coding rules is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5371</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5371</id><created>2012-11-22</created><authors><author><keyname>Ma</keyname><forenames>Tai-Yu</forenames><affiliation>LET</affiliation></author></authors><title>A hybrid cross entropy algorithm for solving dynamic transit network
  design problem</title><categories>cs.NI cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a hybrid multiagent learning algorithm for solving the
dynamic simulation-based bilevel network design problem. The objective is to
determine the op-timal frequency of a multimodal transit network, which
minimizes total users' travel cost and operation cost of transit lines. The
problem is formulated as a bilevel programming problem with equilibrium
constraints describing non-cooperative Nash equilibrium in a dynamic
simulation-based transit assignment context. A hybrid algorithm combing the
cross entropy multiagent learning algorithm and Hooke-Jeeves algorithm is
proposed. Computational results are provided on the Sioux Falls network to
illustrate the perform-ance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5380</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5380</id><created>2012-11-22</created><updated>2014-04-24</updated><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Interference Alignment with Incomplete CSIT Sharing</title><categories>cs.IT math.IT</categories><comments>Final version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the impact of having only incomplete channel state
information at the transmitters (CSIT) over the feasibility of interference
alignment (IA) in a K-user MIMO interference channel (IC). Incompleteness of
CSIT refers to the perfect knowledge at each transmitter (TX) of only a
sub-matrix of the global channel matrix, where the sub-matrix is specific to
each TX. This paper investigates the notion of IA feasibility for CSIT
configurations being as incomplete as possible, as this leads to feedback
overhead reductions in practice. We distinguish between antenna configurations
where (i) removing a single antenna makes IA unfeasible, referred to as
tightly-feasible settings, and (ii) cases where extra antennas are available,
referred to as super-feasible settings. We show conditions for which IA is
feasible in strictly incomplete CSIT scenarios, even in tightly-feasible
settings. For such cases, we provide a CSIT allocation policy preserving IA
feasibility while reducing significantly the amount of CSIT required. For
super-feasible settings, we develop a heuristic CSIT allocation algorithm which
exploits the additional antennas to further reduce the size of the CSIT
allocation. As a byproduct of our approach, a simple and intuitive algorithm
for testing feasibility of single stream IA is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5389</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5389</id><created>2012-11-22</created><updated>2013-06-10</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Lecroq</keyname><forenames>Thierry</forenames></author><author><keyname>Lefebvre</keyname><forenames>Arnaud</forenames></author><author><keyname>Prieur-Gaston</keyname><forenames>Elise</forenames></author></authors><title>Algorithms for Computing Abelian Periods of Words</title><categories>cs.DS cs.DM math.CO</categories><comments>Accepted for publication in Discrete Applied Mathematics</comments><journal-ref>Discrete Applied Mathematics 163: 287-297 (2014)</journal-ref><doi>10.1016/j.dam.2013.08.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constantinescu and Ilie (Bulletin EATCS 89, 167--170, 2006) introduced the
notion of an \emph{Abelian period} of a word. A word of length $n$ over an
alphabet of size $\sigma$ can have $\Theta(n^{2})$ distinct Abelian periods.
The Brute-Force algorithm computes all the Abelian periods of a word in time
$O(n^2 \times \sigma)$ using $O(n \times \sigma)$ space. We present an off-line
algorithm based on a $\sel$ function having the same worst-case theoretical
complexity as the Brute-Force one, but outperforming it in practice. We then
present on-line algorithms that also enable to compute all the Abelian periods
of all the prefixes of $w$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5400</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5400</id><created>2012-11-22</created><authors><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author><author><keyname>De Wilde</keyname><forenames>Philippe</forenames></author></authors><title>Ecosystem-Oriented Distributed Evolutionary Computing</title><categories>cs.NE</categories><comments>8 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1112.0204, arXiv:0712.4159, arXiv:0712.4153, arXiv:0712.4102,
  arXiv:0910.0674</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We create a novel optimisation technique inspired by natural ecosystems,
where the optimisation works at two levels: a first optimisation, migration of
genes which are distributed in a peer-to-peer network, operating continuously
in time; this process feeds a second optimisation based on evolutionary
computing that operates locally on single peers and is aimed at finding
solutions to satisfy locally relevant constraints. We consider from the domain
of computer science distributed evolutionary computing, with the relevant
theory from the domain of theoretical biology, including the fields of
evolutionary and ecological theory, the topological structure of ecosystems,
and evolutionary processes within distributed environments. We then define
ecosystem- oriented distributed evolutionary computing, imbibed with the
properties of self-organisation, scalability and sustainability from natural
ecosystems, including a novel form of distributed evolu- tionary computing.
Finally, we conclude with a discussion of the apparent compromises resulting
from the hybrid model created, such as the network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5402</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5402</id><created>2012-11-22</created><authors><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author><author><keyname>Keranen</keyname><forenames>Krista</forenames></author><author><keyname>Parry</keyname><forenames>Glenn</forenames></author></authors><title>Understanding Complex Service Systems Through Different Lenses: An
  Overview</title><categories>cs.OH</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2011 Grand Challenge in Service conference aimed to explore, analyse and
evaluate complex service systems, utilising a case scenario of delivering on
improved perception of safety in the London Borough of Sutton, which provided a
common context to link the contributions. The key themes that emerged included
value co-creation, systems and networks, ICT and complexity, for which we
summarise the contributions. Contributions on value co-creation are based
mainly on empirical research and provide a variety of insights including the
importance of better understanding collaboration within value co-creation.
Contributions on the systems perspective, considered to arise from networks of
value co-creation, include efforts to understand the implications of the
interactions within service systems, as well as their interactions with social
systems, to co-create value. Contributions within the technological sphere,
providing ever greater connectivity between entities, focus on the creation of
new value constellations and new demand being fulfilled through hybrid
offerings of physical assets, information and people. Contributions on
complexity, arising from the value co- creation networks of technology enabled
services systems, focus on the challenges in understanding, managing and
analysing these complex service systems. The theory and applications all show
the importance of understanding service for the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5405</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5405</id><created>2012-11-22</created><updated>2013-11-10</updated><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Lee</keyname><forenames>Kangwook</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>The MDS Queue: Analysing the Latency Performance of Erasure Codes</title><categories>cs.IT cs.NI math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to scale economically, data centers are increasingly evolving their
data storage methods from the use of simple data replication to the use of more
powerful erasure codes, which provide the same level of reliability as
replication but at a significantly lower storage cost. In particular, it is
well known that Maximum-Distance-Separable (MDS) codes, such as Reed-Solomon
codes, provide the maximum storage efficiency. While the use of codes for
providing improved reliability in archival storage systems, where the data is
less frequently accessed (or so-called &quot;cold data&quot;), is well understood, the
role of codes in the storage of more frequently accessed and active &quot;hot data&quot;,
where latency is the key metric, is less clear.
  In this paper, we study data storage systems based on MDS codes through the
lens of queueing theory, and term this the &quot;MDS queue.&quot; We analytically
characterize the (average) latency performance of MDS queues, for which we
present insightful scheduling policies that form upper and lower bounds to
performance, and are observed to be quite tight. Extensive simulations are also
provided and used to validate our theoretical analysis. We also employ the
framework of the MDS queue to analyse different methods of performing so-called
degraded reads (reading of partial data) in distributed data storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5407</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5407</id><created>2012-11-22</created><authors><author><keyname>Ng</keyname><forenames>Irene</forenames></author><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author></authors><title>Value, Variety and Viability: New Business Models for Co-Creation in
  Outcome-based Contracts</title><categories>cs.OH</categories><comments>26 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1111.2651</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose that designing a manufacturer's equipment-based service value
proposition in outcome-based contracts is the design of a new business model
capable of managing threats to the firm's viability that can arise from the
contextual variety of use that customers may subject the firm's value
propositions. Furthermore, manufacturers need to understand these emerging
business models as the capability of managing both asset and service provision
to achieve use outcomes with customers, including emotional outcomes such as
customer experience. Service-Dominant Logic proposes that all &quot;goods are a
distribution mechanism for service provision&quot;, upon which we propose a
value-centric approach to understanding the interactions between the asset and
service provision, and suggest a viable systems approach towards reorganising
the firm to achieve such a business model. Three case studies of B2B
equipment-based service systems were analysed to understand customers'
co-creation activities in achieving outcomes, in which we found that the
co-creation of complex multi-dimensional value could be delivered through the
different value propositions of the firm catering to different aspects
(dimensions) of the value to be co-created. The study provides a way for
managers to understand the effectiveness (rather than efficiency) of firms in
adopting emerging business models that design for value co-creation in what are
ultimately complex socio- technical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5414</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5414</id><created>2012-11-23</created><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Analysis of a randomized approximation scheme for matrix multiplication</title><categories>cs.DS cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note gives a simple analysis of a randomized approximation scheme for
matrix multiplication proposed by Sarlos (2006) based on a random rotation
followed by uniform column sampling. The result follows from a matrix version
of Bernstein's inequality and a tail inequality for quadratic forms in
subgaussian random vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5418</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5418</id><created>2012-11-23</created><authors><author><keyname>Selvarani</keyname><forenames>D. Roselin</forenames></author><author><keyname>Ravi</keyname><forenames>T. N.</forenames></author></authors><title>A survey on data and transaction management in mobile databases</title><categories>cs.DB</categories><comments>20 Pages; International Journal of Database Management Systems
  (IJDMS) Vol.4, No.5, October 2012. arXiv admin note: text overlap with
  arXiv:0908.0076, arXiv:1005.1747, arXiv:1108.6195 by other authors</comments><doi>10.5121/ijdms.2012.4501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of the Mobile Database is increasing day by day as people need
information even on the move in the fast changing world. This database
technology permits employees using mobile devices to connect to their corporate
networks, hoard the needed data, work in the disconnected mode and reconnect to
the network to synchronize with the corporate database. In this scenario, the
data is being moved closer to the applications in order to improve the
performance and autonomy. This leads to many interesting problems in mobile
database research and Mobile Database has become a fertile land for many
researchers. In this paper a survey is presented on data and Transaction
management in Mobile Databases from the year 2000 onwards. The survey focuses
on the complete study on the various types of Architectures used in Mobile
databases and Mobile Transaction Models. It also addresses the data management
issues namely Replication and Caching strategies and the transaction management
functionalities such as Concurrency Control and Commit protocols,
Synchronization, Query Processing, Recovery and Security. It also provides
Research Directions in Mobile databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5425</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5425</id><created>2012-11-23</created><updated>2013-11-28</updated><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author></authors><title>A Cross-layer Perspective on Energy Harvesting Aided Green
  Communications over Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the power allocation of the physical layer and the buffer delay
of the upper application layer in energy harvesting green networks. The total
power required for reliable transmission includes the transmission power and
the circuit power. The harvested power (which is stored in a battery) and the
grid power constitute the power resource. The uncertainty of data generated
from the upper layer, the intermittence of the harvested energy, and the
variation of the fading channel are taken into account and described as
independent Markov processes. In each transmission, the transmitter decides the
transmission rate as well as the allocated power from the battery, and the rest
of the required power will be supplied by the power grid. The objective is to
find an allocation sequence of transmission rate and battery power to minimize
the long-term average buffer delay under the average grid power constraint. A
stochastic optimization problem is formulated accordingly to find such
transmission rate and battery power sequence. Furthermore, the optimization
problem is reformulated as a constrained MDP problem whose policy is a
two-dimensional vector with the transmission rate and the power allocation of
the battery as its elements. We prove that the optimal policy of the
constrained MDP can be obtained by solving the unconstrained MDP. Then we focus
on the analysis of the unconstrained average-cost MDP. The structural
properties of the average optimal policy are derived. Moreover, we discuss the
relations between elements of the two-dimensional policy. Next, based on the
theoretical analysis, the algorithm to find the constrained optimal policy is
presented for the finite state space scenario. In addition, heuristic policies
with low-complexity are given for the general state space. Finally, simulations
are performed under these policies to demonstrate the effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5433</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5433</id><created>2012-11-23</created><updated>2013-07-31</updated><authors><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Fredriksson</keyname><forenames>Kimmo</forenames></author></authors><title>Approximate pattern matching with k-mismatches in packed text</title><categories>cs.DS</categories><comments>This paper is an extended version of the article that appeared in
  Information Processing Letters 113(19-21):693-697 (2013),
  http://dx.doi.org/10.1016/j.ipl.2013.07.002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given strings $P$ of length $m$ and $T$ of length $n$ over an alphabet of
size $\sigma$, the string matching with $k$-mismatches problem is to find the
positions of all the substrings in $T$ that are at Hamming distance at most $k$
from $P$. If $T$ can be read only one character at the time the best known
bounds are $O(n\sqrt{k\log k})$ and $O(n + n\sqrt{k/w}\log k)$ in the word-RAM
model with word length $w$. In the RAM models (including $AC^0$ and word-RAM)
it is possible to read up to $\floor{w / \log \sigma}$ characters in constant
time if the characters of $T$ are encoded using $\ceil{\log \sigma}$ bits. The
only solution for $k$-mismatches in packed text works in $O((n \log\sigma/\log
n)\ceil{m \log (k + \log n / \log\sigma) / w} + n^{\varepsilon})$ time, for any
$\varepsilon &gt; 0$. We present an algorithm that runs in time
$O(\frac{n}{\floor{w/(m\log\sigma)}} (1 + \log \min(k,\sigma) \log m /
\log\sigma))$ in the $AC^0$ model if $m=O(w / \log\sigma)$ and $T$ is given
packed. We also describe a simpler variant that runs in time
$O(\frac{n}{\floor{w/(m\log\sigma)}}\log \min(m, \log w / \log\sigma))$ in the
word-RAM model. The algorithms improve the existing bound for $w =
\Omega(\log^{1+\epsilon}n)$, for any $\epsilon &gt; 0$. Based on the introduced
technique, we present algorithms for several other approximate matching
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5451</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5451</id><created>2012-11-23</created><authors><author><keyname>Henard</keyname><forenames>Christopher</forenames><affiliation>SnT</affiliation></author><author><keyname>Papadakis</keyname><forenames>Mike</forenames><affiliation>SnT</affiliation></author><author><keyname>Perrouin</keyname><forenames>Gilles</forenames><affiliation>PReCISE</affiliation></author><author><keyname>Klein</keyname><forenames>Jacques</forenames><affiliation>SnT</affiliation></author><author><keyname>Heymans</keyname><forenames>Patrick</forenames><affiliation>PReCISE, INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>Uni.lu, S'nT</affiliation></author></authors><title>Bypassing the Combinatorial Explosion: Using Similarity to Generate and
  Prioritize T-wise Test Suites for Large Software Product Lines</title><categories>cs.SE</categories><comments>Technical Report</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Product Lines (SPLs) are families of products whose commonalities
and variability can be captured by Feature Models (FMs). T-wise testing aims at
finding errors triggered by all interactions amongst t features, thus reducing
drastically the number of products to test. T-wise testing approaches for SPLs
are limited to small values of t -- which miss faulty interactions -- or
limited by the size of the FM. Furthermore, they neither prioritize the
products to test nor provide means to finely control the generation process.
This paper offers (a) a search-based approach capable of generating products
for large SPLs, forming a scalable and flexible alternative to current
techniques and (b) prioritization algorithms for any set of products.
Experiments conducted on 124 FMs (including large FMs such as the Linux kernel)
demonstrate the feasibility and the practicality of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5481</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5481</id><created>2012-11-23</created><authors><author><keyname>Cavuoti</keyname><forenames>Stefano</forenames></author><author><keyname>Garofalo</keyname><forenames>Mauro</forenames></author><author><keyname>Brescia</keyname><forenames>Massimo</forenames></author><author><keyname>Pescap&#xe9;</keyname><forenames>Antonio</forenames></author><author><keyname>Longo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ventre</keyname><forenames>Giorgio</forenames></author></authors><title>Genetic Algorithm Modeling with GPU Parallel Computing Technology</title><categories>astro-ph.IM cs.DC cs.NE</categories><comments>11 pages, 2 figures, refereed proceedings; Neural Nets and
  Surroundings, Proceedings of 22nd Italian Workshop on Neural Nets, WIRN 2012;
  Smart Innovation, Systems and Technologies, Vol. 19, Springer</comments><doi>10.1007/978-3-642-35467-0_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a multi-purpose genetic algorithm, designed and implemented with
GPGPU / CUDA parallel computing technology. The model was derived from a
multi-core CPU serial implementation, named GAME, already scientifically
successfully tested and validated on astrophysical massive data classification
problems, through a web application resource (DAMEWARE), specialized in data
mining based on Machine Learning paradigms. Since genetic algorithms are
inherently parallel, the GPGPU computing paradigm has provided an exploit of
the internal training features of the model, permitting a strong optimization
in terms of processing performances and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5484</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5484</id><created>2012-11-23</created><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Li</keyname><forenames>Deyi</forenames></author><author><keyname>Chen</keyname><forenames>Guisheng</forenames></author><author><keyname>Du</keyname><forenames>Wenhua</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author></authors><title>Ranking the Importance of Nodes of Complex Networks by the Equivalence
  Classes Approach</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the importance of nodes of complex networks is of interest to the
research of Social Networks, Biological Networks etc.. Current researchers have
proposed several measures or algorithms, such as betweenness, PageRank and HITS
etc., to identify the node importance. However, these measures are based on
different aspects of properties of nodes, and often conflict with the others. A
reasonable, fair standard is needed for evaluating and comparing these
algorithms. This paper develops a framework as the standard for ranking the
importance of nodes. Four intuitive rules are suggested to measure the node
importance, and the equivalence classes approach is employed to resolve the
conflicts and aggregate the results of the rules. To quantitatively compare the
algorithms, the performance indicators are also proposed based on a similarity
measure. Three widely used real-world networks are used as the test-beds. The
experimental results illustrate the feasibility of this framework and show that
both algorithms, PageRank and HITS, perform well with bias when dealing with
the tested networks. Furthermore, this paper uses the proposed approach to
analyze the structure of the Internet, and draws out the kernel of the Internet
with dense links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5492</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5492</id><created>2012-11-23</created><updated>2014-11-28</updated><authors><author><keyname>Soleymani</keyname><forenames>Mohammad</forenames></author><author><keyname>Larson</keyname><forenames>Martha</forenames></author><author><keyname>Pun</keyname><forenames>Thierry</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>Corpus Development for Affective Video Indexing</title><categories>cs.MM cs.HC cs.IR</categories><comments>Manuscript published</comments><journal-ref>IEEE Transactions on Multimedia 16(4):1075-1089, 2014</journal-ref><doi>10.1109/TMM.2014.2305573</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affective video indexing is the area of research that develops techniques to
automatically generate descriptions of video content that encode the emotional
reactions which the video content evokes in viewers. This paper provides a set
of corpus development guidelines based on state-of-the-art practice intended to
support researchers in this field. Affective descriptions can be used for video
search and browsing systems offering users affective perspectives. The paper is
motivated by the observation that affective video indexing has yet to fully
profit from the standard corpora (data sets) that have benefited conventional
forms of video indexing. Affective video indexing faces unique challenges,
since viewer-reported affective reactions are difficult to assess. Moreover
affect assessment efforts must be carefully designed in order to both cover the
types of affective responses that video content evokes in viewers and also
capture the stable and consistent aspects of these responses. We first present
background information on affect and multimedia and related work on affective
multimedia indexing, including existing corpora. Three dimensions emerge as
critical for affective video corpora, and form the basis for our proposed
guidelines: the context of viewer response, personal variation among viewers,
and the effectiveness and efficiency of corpus creation. Finally, we present
examples of three recent corpora and discuss how these corpora make progressive
steps towards fulfilling the guidelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5494</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5494</id><created>2012-11-23</created><authors><author><keyname>Zolotas</keyname><forenames>A. C.</forenames></author><author><keyname>Halikias</keyname><forenames>G. D.</forenames></author></authors><title>Optimal design of PID controllers using the QFT method</title><categories>cs.SY math.OC</categories><comments>This is the author's version of a paper that appeared in IEE
  Proceedings - Control Theory and Applications, the official version is on:
  http://ieeexplore.ieee.org/servlet/opac?punumber=2193</comments><journal-ref>IEEE Proceedings - Control Theory and Applications (1999), 146
  (6), pp. 585 - 589</journal-ref><doi>10.1049/ip-cta:19990746</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimisation algorithm is proposed for designing PID controllers, which
minimises the asymptotic open-loop gain of a system, subject to appropriate
robust- stability and performance QFT constraints. The algorithm is simple and
can be used to automate the loop-shaping step of the QFT design procedure. The
effectiveness of the method is illustrated with an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5498</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5498</id><created>2012-11-23</created><updated>2013-12-09</updated><authors><author><keyname>Flegel</keyname><forenames>F.</forenames></author><author><keyname>Sokolov</keyname><forenames>I. M.</forenames></author></authors><title>Canonical fitness model for simple scale-free graphs</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 2 figures; published in Phys. Rev. E. To improve
  readability, formulas and text were added between Eq. (1) and (2)</comments><journal-ref>Phys. Rev. E 87, 022806 (2013)</journal-ref><doi>10.1103/PhysRevE.87.022806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fitness model assumed to generate simple graphs with power-law
heavy-tailed degree sequence: P(k) \propto k^{-1-\alpha} with 0 &lt; \alpha &lt; 1,
in which the corresponding distributions do not posses a mean. We discuss the
situations in which the model is used to produce a multigraph and examine what
happens if the multiple edges are merged to a single one and thus a simple
graph is built. We give the relation between the (normalized) fitness parameter
r and the expected degree \nu of a node and show analytically that it possesses
non-trivial intermediate and final asymptotic behaviors. We show that the model
produces P(k) \propto k^{-2} for large values of k independent of \alpha. Our
analytical findings are confirmed by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5508</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5508</id><created>2012-11-23</created><authors><author><keyname>Jacobs</keyname><forenames>Naomi</forenames></author><author><keyname>Amos</keyname><forenames>Martyn</forenames></author></authors><title>NanoInfoBio: A case-study in interdisciplinary research</title><categories>cs.GL</categories><comments>Appears in Kettunen, J., Hyrkkanen, U. &amp; Lehto, A. (Eds.) Applied
  Research and Professional Education, p.p. 289-309. Turku University of
  Applied Sciences (2012). http://julkaisut.turkuamk.fi/isbn9789522162519.pdf.
  arXiv admin note: substantial text overlap with arXiv:1012.4170</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant amount of high-impact contemporary scientific research occurs
where biology, computer science, engineering and chemistry converge. Although
programmes have been put in place to support such work, the complex dynamics of
interdisciplinarity are still poorly understood. In this paper we highlight
potential barriers to effective research across disciplines, and suggest, using
a case study, possible mechanisms for removing these impediments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5514</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5514</id><created>2012-11-23</created><authors><author><keyname>Glazek</keyname><forenames>Stanislaw D.</forenames></author></authors><title>Estimate of resources required for a meaningful reform of education</title><categories>physics.ed-ph cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple estimate in terms of currency units shows that a meaningful
educational reform process can be launched and sustained over many generations
of teachers with support of parents of students. In the estimate, the steady
inflow of resources from parents provides support for advanced studies by
teachers. Not to waste the resources on spurious activities, the estimated
inflow proceeds directly from the parents as clients to the providers of
required reform program. The providers are the experts in various disciplines
who excel in helping teachers become great. Their services to teachers are
ultimately assessed by parents on the basis of changes in behavior of children.
The resulting reform program grows slowly from small seeds. The running cost of
the reform process to parents appears surprisingly low while its development
leads to the desired changes over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5520</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5520</id><created>2012-11-23</created><authors><author><keyname>Samant</keyname><forenames>Vivekanand</forenames></author><author><keyname>Hulgeri</keyname><forenames>Arvind</forenames></author><author><keyname>Valencia</keyname><forenames>Alfonso</forenames></author><author><keyname>Tendulkar</keyname><forenames>Ashish V.</forenames></author></authors><title>Accurate Demarcation of Protein Domain Linkers based on Structural
  Analysis of Linker Probable Region</title><categories>cs.CE q-bio.BM</categories><comments>18 pages, 2 figures</comments><acm-class>J.3; I.2.6</acm-class><journal-ref>International Journal of Computational Biology, 0001:01-19, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-domain proteins, the domains are connected by a flexible
unstructured region called as protein domain linker. The accurate demarcation
of these linkers holds a key to understanding of their biochemical and
evolutionary attributes. This knowledge helps in designing a suitable linker
for engineering stable multi-domain chimeric proteins. Here we propose a novel
method for the demarcation of the linker based on a three-dimensional protein
structure and a domain definition. The proposed method is based on biological
knowledge about structural flexibility of the linkers. We performed structural
analysis on a linker probable region (LPR) around domain boundary points of
known SCOP domains. The LPR was described using a set of overlapping peptide
fragments of fixed size. Each peptide fragment was then described by geometric
invariants (GIs) and subjected to clustering process where the fragments
corresponding to actual linker come up as outliers. We then discover the actual
linkers by finding the longest continuous stretch of outlier fragments from
LPRs. This method was evaluated on a benchmark dataset of 51 continuous
multi-domain proteins, where it achieves F1 score of 0.745 (0.83 precision and
0.66 recall). When the method was applied on 725 continuous multi-domain
proteins, it was able to identify novel linkers that were not reported
previously. This method can be used in combination with supervised / sequence
based linker prediction methods for accurate linker demarcation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5530</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5530</id><created>2012-11-23</created><authors><author><keyname>Dokulil</keyname><forenames>Jiri</forenames></author><author><keyname>Bajrovic</keyname><forenames>Enes</forenames></author><author><keyname>Benkner</keyname><forenames>Siegfried</forenames></author><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author><author><keyname>Sandrieser</keyname><forenames>Martin</forenames></author><author><keyname>Bachmayer</keyname><forenames>Beverly</forenames></author></authors><title>Efficient Hybrid Execution of C++ Applications using Intel(R) Xeon
  Phi(TM) Coprocessor</title><categories>cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of Intel(R) Xeon Phi(TM) coprocessors opened up new
possibilities in development of highly parallel applications. The familiarity
and flexibility of the architecture together with compiler support integrated
into the Intel C++ Composer XE allows the developers to use familiar
programming paradigms and techniques, which are usually not suitable for other
accelerated systems. It is now easy to use complex C++ template-heavy codes on
the coprocessor, including for example the Intel Threading Building Blocks
(TBB) parallelization library. These techniques are not only possible, but
usually efficient as well, since host and coprocessor are of the same
architectural family, making optimization techniques designed for the Xeon CPU
also beneficial on Xeon Phi. As a result, highly optimized Xeon codes (like the
TBB library) work well on both.
  In this paper we present a new parallel library construct, which makes it
easy to apply a function to every member of an array in parallel, dynamically
distributing the work between the host CPUs and one or more coprocessor cards.
We describe the associated runtime support and use a physical simulation
example to demonstrate that our library construct can be used to quickly create
a C++ application that will significantly benefit from hybrid execution,
simultaneously exploiting CPU cores and coprocessor cores. Experimental results
show that one optimized source code is sufficient to make the host and the
coprocessors run efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5544</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5544</id><created>2012-11-23</created><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>A Note on Kolmogorov-Uspensky Machines</title><categories>cs.CC cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving an open problem stated by Shvachko, it is shown that a language which
is not real-time recognizable by some variants of pointer machines can be
accepted by a Kolmogorov-Uspensky machine in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5556</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5556</id><created>2012-11-23</created><authors><author><keyname>Pele</keyname><forenames>Ofir</forenames></author><author><keyname>Werman</keyname><forenames>Michael</forenames></author></authors><title>Improving Perceptual Color Difference using Basic Color Terms</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest a new color distance based on two observations. First, perceptual
color differences were designed to be used to compare very similar colors. They
do not capture human perception for medium and large color differences well.
Thresholding was proposed to solve the problem for large color differences,
i.e. two totally different colors are always the same distance apart. We show
that thresholding alone cannot improve medium color differences. We suggest to
alleviate this problem using basic color terms. Second, when a color distance
is used for edge detection, many small distances around the just noticeable
difference may account for false edges. We suggest to reduce the effect of
small distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5562</identifier>
 <datestamp>2013-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5562</id><created>2012-11-23</created><updated>2013-08-28</updated><authors><author><keyname>Sreedharan</keyname><forenames>Jithin K.</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Spectrum Sensing using Distributed Sequential Detection via Noisy
  Reporting MAC</title><categories>cs.IT math.IT stat.AP</categories><comments>13 pages. 12 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers cooperative spectrum sensing algorithms for Cognitive
Radios which focus on reducing the number of samples to make a reliable
detection. We develop an energy efficient detector with low detection delay
using decentralized sequential hypothesis testing. Our algorithm at the
Cognitive Radios employs an asynchronous transmission scheme which takes into
account the noise at the fusion center. We start with a distributed algorithm,
DualSPRT, in which Cognitive Radios sequentially collect the observations, make
local decisions using SPRT (Sequential Probability Ratio Test) and send them to
the fusion center. The fusion center sequentially processes these received
local decisions corrupted by noise, using an SPRT-like procedure to arrive at a
final decision. We theoretically analyse its probability of error and average
detection delay. We also asymptotically study its performance. Even though
DualSPRT performs asymptotically well, a modification at the fusion node
provides more control over the design of the algorithm parameters which then
performs better at the usual operating probabilities of error in Cognitive
Radio systems. We also analyse the modified algorithm theoretically. Later we
modify these algorithms to handle uncertainties in SNR and fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5566</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5566</id><created>2012-11-23</created><authors><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>Irene</forenames></author><author><keyname>Mart&#xed;nez-Moro</keyname><forenames>Edgar</forenames></author><author><keyname>Su&#xe1;rez-Canedo</keyname><forenames>Emilio</forenames></author></authors><title>On the Composition of Secret Sharing Schemes Related to Codes</title><categories>cs.IT math.IT</categories><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct a subclass of the composite access structure
introduced by Mart\'inez et al. based on schemes realizing the structure given
by the set of codewords of minimal support of linear codes. This class enlarges
the iterated threshold class studied in the same paper. Furthermore all the
schemes on this paper are ideal (in fact they allow a vector space
construction) and we arrived to give a partial answer to a conjecture stated in
the above paper. Finally, as a corollary we proof that all the monotone access
structures based on all the minimal supports of a code can be realized by a
vector space construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5568</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5568</id><created>2012-11-23</created><updated>2014-11-03</updated><authors><author><keyname>Borges-Quintana</keyname><forenames>M.</forenames></author><author><keyname>Borges-Trenard</keyname><forenames>M. A.</forenames></author><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>I.</forenames></author><author><keyname>Mart&#xed;nez-Moro</keyname><forenames>E.</forenames></author></authors><title>Computing coset leaders and leader codewords of binary codes</title><categories>cs.IT math.IT</categories><msc-class>94B05, 13P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use the Gr\&quot;obner representation of a binary linear code
$\mathcal C$ to give efficient algorithms for computing the whole set of coset
leaders, denoted by $\mathrm{CL}(\mathcal C)$ and the set of leader codewords,
denoted by $\mathrm L(\mathcal C)$. The first algorithm could be adapted to
provide not only the Newton and the covering radius of $\mathcal C$ but also to
determine the coset leader weight distribution. Moreover, providing the set of
leader codewords we have a test-set for decoding by a gradient-like decoding
algorithm. Another contribution of this article is the relation stablished
between zero neighbours and leader codewords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5577</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5577</id><created>2012-11-23</created><updated>2013-03-14</updated><authors><author><keyname>Cabitza</keyname><forenames>Federico</forenames></author><author><keyname>Simone</keyname><forenames>Carla</forenames></author></authors><title>Design Ltd.: Renovated Myths for the Development of Socially Embedded
  Technologies</title><categories>cs.HC</categories><comments>This is the peer-unreviewed of a manuscript that is to appear in D.
  Randall, K. Schmidt, &amp; V. Wulf (Eds.), Designing Socially Embedded
  Technologies: A European Challenge (2013, forthcoming) with the title
  &quot;Building Socially Embedded Technologies: Implications on Design&quot; within an
  EUSSET editorial initiative (www.eusset.eu/)</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues that traditional and mainstream mythologies, which have
been continually told within the Information Technology domain among designers
and advocators of conceptual modelling since the 1960s in different fields of
computing sciences, could now be renovated or substituted in the mould of more
recent discourses about performativity, complexity and end-user creativity that
have been constructed across different fields in the meanwhile. In the paper,
it is submitted that these discourses could motivate IT professionals in
undertaking alternative approaches toward the co-construction of
socio-technical systems, i.e., social settings where humans cooperate to reach
common goals by means of mediating computational tools. The authors advocate
further discussion about and consolidation of some concepts in design research,
design practice and more generally Information Technology (IT) development,
like those of: task-artifact entanglement, universatility (sic) of End-User
Development (EUD) environments, bricolant/bricoleur end-user, logic of
bricolage, maieuta-designers (sic), and laissez-faire method to socio-technical
construction. Points backing these and similar concepts are made to promote
further discussion on the need to rethink the main assumptions underlying IT
design and development some fifty years later the coming of age of software and
modern IT in the organizational domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5590</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5590</id><created>2012-11-23</created><authors><author><keyname>Bastien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Lamblin</keyname><forenames>Pascal</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Bergstra</keyname><forenames>James</forenames></author><author><keyname>Goodfellow</keyname><forenames>Ian</forenames></author><author><keyname>Bergeron</keyname><forenames>Arnaud</forenames></author><author><keyname>Bouchard</keyname><forenames>Nicolas</forenames></author><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Theano: new features and speed improvements</title><categories>cs.SC cs.LG</categories><comments>Presented at the Deep Learning Workshop, NIPS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theano is a linear algebra compiler that optimizes a user's
symbolically-specified mathematical computations to produce efficient low-level
implementations. In this paper, we present new features and efficiency
improvements to Theano, and benchmarks demonstrating Theano's performance
relative to Torch7, a recently introduced machine learning library, and to
RNNLM, a C++ library targeted at recurrent neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5595</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5595</id><created>2012-11-21</created><authors><author><keyname>Adeshina</keyname><forenames>A. M.</forenames></author><author><keyname>Hashim</keyname><forenames>R.</forenames></author><author><keyname>Khalid</keyname><forenames>N. E. A.</forenames></author><author><keyname>Abidin</keyname><forenames>Siti Z. Z.</forenames></author></authors><title>Hardware-Accelerated Raycasting: Towards an Effective Brain MRI
  Visualization</title><categories>cs.CG</categories><comments>Journal of Computing, Volume 3, Issue 10, October 2011, ISSN
  2151-9617</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development in information technology has immensely contributed to
the use of modern approaches for visualizing volumetric data. Consequently,
medical volume visualization is increasingly attracting attention towards
achieving an effective visualization algorithm for medical diagnosis and
pre-treatment planning. Previously, research has been addressing implementation
of algorithm that can visualize 2-D images into 3-D. Meanwhile, achieving such
a rendering algorithm at an interactive speed and of good robustness to handle
mass data still remains a challenging issue. However, in medical diagnosis,
finding the exact location of brain tumor or diseases is an important step of
surgery / disease management. This paper proposes a GPU-based raycasting
algorithm for accurate allocation and localization of human brain abnormalities
using magnetic resonance (MRI) images of normal and abnormal patients.
  Keywords: Brain tumor, graphic processing units, magnetic resonance imaging,
volume visualization
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5596</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5596</id><created>2012-11-22</created><authors><author><keyname>Mannava</keyname><forenames>Vishnuvardhan</forenames></author><author><keyname>Ramesh</keyname><forenames>T.</forenames></author></authors><title>A Composite Design Pattern for Service Injection and Composition of Web
  Services for Peer-To-Peer Computing with Service Oriented Architecture</title><categories>cs.SE cs.DC cs.NI</categories><comments>15 pages, 9 figures, International Journal on Web Service Computing
  (IJWSC). arXiv admin note: substantial text overlap with arXiv:1208.3836,
  arXiv:1211.5227</comments><acm-class>D.2.11; D.2.10; D.3.3; C.2.4; D.1.3</acm-class><journal-ref>International Journal on Web Service Computing 3 (3), 49-63, 2012</journal-ref><doi>10.5121/ijwsc</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we present a Service Injection and composition Design Pattern
for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented
design patterns, and amalgamation of the Strategy, Worker Object, and
Check-List Design Patterns used to design the Self-Adaptive Systems. It will
apply self reconfiguration planes dynamically without the interruption or
intervention of the administrator for handling service failures at the servers.
When a client requests for a complex service, Service Composition should be
done to fulfil the request. If a service is not available in the memory, it
will be injected as Aspectual Feature Module code. We used Service Oriented
Architecture (SOA) with Web Services in Java to Implement the composite Design
Pattern. As far as we know, there are no studies on composition of design
patterns for Peer-to-peer computing domain. The pattern is described using a
java-like notation for the classes and interfaces. A simple UML class and
Sequence diagrams are depicted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5608</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5608</id><created>2012-11-21</created><updated>2013-07-09</updated><authors><author><keyname>Ahmed</keyname><forenames>Ali</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Blind Deconvolution using Convex Programming</title><categories>cs.IT math.IT</categories><comments>40 pages, 8 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering two unknown vectors, $\w$ and $\x$, of
length $L$ from their circular convolution. We make the structural assumption
that the two vectors are members of known subspaces, one with dimension $N$ and
the other with dimension $K$. Although the observed convolution is nonlinear in
both $\w$ and $\x$, it is linear in the rank-1 matrix formed by their outer
product $\w\x^*$. This observation allows us to recast the deconvolution
problem as low-rank matrix recovery problem from linear measurements, whose
natural convex relaxation is a nuclear norm minimization program.
  We prove the effectiveness of this relaxation by showing that for &quot;generic&quot;
signals, the program can deconvolve $\w$ and $\x$ exactly when the maximum of
$N$ and $K$ is almost on the order of $L$. That is, we show that if $\x$ is
drawn from a random subspace of dimension $N$, and $\w$ is a vector in a
subspace of dimension $K$ whose basis vectors are &quot;spread out&quot; in the frequency
domain, then nuclear norm minimization recovers $\w\x^*$ without error.
  We discuss this result in the context of blind channel estimation in
communications. If we have a message of length $N$ which we code using a random
$L\times N$ coding matrix, and the encoded message travels through an unknown
linear time-invariant channel of maximum length $K$, then the receiver can
recover both the channel response and the message when $L\gtrsim N+K$, to
within constant and log factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5611</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5611</id><created>2012-11-23</created><updated>2013-02-14</updated><authors><author><keyname>Lee</keyname><forenames>Soomin</forenames></author><author><keyname>Nedich</keyname><forenames>Angelia</forenames></author></authors><title>Distributed Random Projection Algorithm for Convex Optimization</title><categories>math.OC cs.SY</categories><doi>10.1109/JSTSP.2013.2247023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random projection algorithm is an iterative gradient method with random
projections. Such an algorithm is of interest for constrained optimization when
the constraint set is not known in advance or the projection operation on the
whole constraint set is computationally prohibitive. This paper presents a
distributed random projection (DRP) algorithm for fully distributed constrained
convex optimization problems that can be used by multiple agents connected over
a time-varying network, where each agent has its own objective function and its
own constrained set. With reasonable assumptions, we prove that the iterates of
all agents converge to the same point in the optimal set almost surely. In
addition, we consider a variant of the method that uses a mini-batch of
consecutive random projections and establish its convergence in almost sure
sense. Experiments on distributed support vector machines demonstrate fast
convergence of the algorithm. It actually shows that the number of iteration
required until convergence is much smaller than scanning over all training
samples just once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5613</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5613</id><created>2012-11-23</created><authors><author><keyname>Pleva</keyname><forenames>Peter</forenames></author></authors><title>A Revised Classification of Anonymity</title><categories>cs.CR</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper primarily addresses the issue of identifying all possible levels
of digital anonymity, thereby allowing electronic services and mechanisms to be
categorised. For this purpose, we sophisticate the generic idea of anonymity
and, filling a niche in the field, bring the scope of trust into the focus of
categorisation. One major concern of our work is to propose a novel and
universal taxonomy which enables a dynamic, trust-based comparison between
systems at an abstract level. On the other hand, our contribution intentionally
does not offer an alternative to anonymity metrics, but neither is it concerned
with methods of anonymous data retrieval (cf. data-mining techniques). However,
for ease of comprehension, it provides a systematic 'application manual' and
also presents a lucid overview of the correspondence between the current and
related taxonomies. Additionally, as a generalisation of group signatures, we
introduce the notion of group schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5614</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5614</id><created>2012-11-23</created><updated>2013-04-16</updated><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Vasavada</keyname><forenames>J.</forenames></author><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Kumar</keyname><forenames>S.</forenames></author><author><keyname>Sharma</keyname><forenames>M.</forenames></author></authors><title>A Hash based Approach for Secure Keyless Steganography in Lossless RGB
  Images</title><categories>cs.CR cs.CV cs.MM</categories><comments>The paper is withdrawn due to license issue</comments><journal-ref>The 22nd International Conference on Computer Graphics and Vision,
  2012, pp.80-83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an improved steganography approach for hiding text
messages in lossless RGB images. The objective of this work is to increase the
security level and to improve the storage capacity with compression techniques.
The security level is increased by randomly distributing the text message over
the entire image instead of clustering within specific image portions. Storage
capacity is increased by utilizing all the color channels for storing
information and providing the source text message compression. The degradation
of the images can be minimized by changing only one least significant bit per
color channel for hiding the message, incurring a very little change in the
original image. Using steganography alone with simple LSB has a potential
problem that the secret message is easily detectable from the histogram
analysis method. To improve the security as well as the image embedding
capacity indirectly, a compression based scheme is introduced. Various tests
have been done to check the storage capacity and message distribution. These
testes show the superiority of the proposed approach with respect to other
existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5617</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5617</id><created>2012-11-23</created><authors><author><keyname>Sridharan</keyname><forenames>Srinivas</forenames></author><author><keyname>Yanagisawa</keyname><forenames>Masahiro</forenames></author><author><keyname>Combes</keyname><forenames>Joshua</forenames></author></authors><title>Optimal rotation control for a qubit subject to continuous measurement</title><categories>math.OC cs.SY quant-ph</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we analyze the optimal control strategy for rotating a
monitored qubit from an initial pure state to an orthogonal state in minimum
time. This strategy is described for two different cost functions of interest
which do not have the usual regularity properties. Hence, as classically smooth
cost functions may not exist, we interpret these functions as viscosity
solutions to the optimal control problem. Specifically we prove their existence
and uniqueness in this weak-solution setting. In addition, we also give bounds
on the time optimal control to prepare any pure state from a mixed state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5625</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5625</id><created>2012-11-23</created><authors><author><keyname>Srihari</keyname><forenames>Sriganesh</forenames></author><author><keyname>Leong</keyname><forenames>Hon Wai</forenames></author></authors><title>A survey of computational methods for protein complex prediction from
  protein interaction networks</title><categories>cs.CE q-bio.MN</categories><comments>27 pages, 5 figures, 4 tables</comments><msc-class>92-08</msc-class><journal-ref>Srihari, S., Leong, HW., J Bioinform Comput Biol 11(2): 1230002,
  2013</journal-ref><doi>10.1142/S021972001230002X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complexes of physically interacting proteins are one of the fundamental
functional units responsible for driving key biological mechanisms within the
cell. Their identification is therefore necessary not only to understand
complex formation but also the higher level organization of the cell. With the
advent of high-throughput techniques in molecular biology, significant amount
of physical interaction data has been cataloged from organisms such as yeast,
which has in turn fueled computational approaches to systematically mine
complexes from the network of physical interactions among proteins (PPI
network). In this survey, we review, classify and evaluate some of the key
computational methods developed till date for the identification of protein
complexes from PPI networks. We present two insightful taxonomies that reflect
how these methods have evolved over the years towards improving automated
complex prediction. We also discuss some open challenges facing accurate
reconstruction of complexes, the crucial ones being presence of high proportion
of errors and noise in current high-throughput datasets and some key aspects
overlooked by current complex detection methods. We hope this review will not
only help to condense the history of computational complex detection for easy
reference, but also provide valuable insights to drive further research in this
area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5629</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5629</id><created>2012-11-23</created><authors><author><keyname>Yoo</keyname><forenames>Wook-Sung</forenames></author></authors><title>Prototype for Extended XDB Using Wiki</title><categories>cs.DB cs.SE</categories><comments>8 pages</comments><journal-ref>International Journal of Database Management Systems (IJDMS)
  Vol.4, No.5, October 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a prototype of extended XDB. XDB is an open-source and
extensible database architecture developed by National Aeronautics and Space
Administration (NASA) to provide integration of heterogeneous and distributed
information resources for scientific and engineering applications. XDB enables
an unlimited number of desktops and distributed information sources to be
linked seamlessly and efficiently into an information grid using Data Access
and Retrieval Composition (DARC) protocol which provides a contextual search
and retrieval capability useful for lightweight web applications. This paper
shows the usage of XDB on common data management in the enterprise without
burdening users and application developers with unnecessary complexity and
formal schemas. Supported by NASA Ames Research Center through NASA Exploration
System Mission Directorate (ESMD) Higher Education grant, a project team at
Fairfield University extended this concept and developed an extended XDB
protocol and a prototype providing text-searches for Wiki. The technical
specification of the protocol was posted to Source Forge (sourceforge.net) and
a prototype providing text-searches for Wiki was developed. The prototype was
created for 16 tags of the MediaWiki dialect. As part of future works, the
prototype will be further extended to the complete Wiki markups and other
dialects of Wiki.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5643</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5643</id><created>2012-11-23</created><authors><author><keyname>Boloni</keyname><forenames>Ladislau</forenames></author></authors><title>Shadows and headless shadows: a worlds-based, autobiographical approach
  to reasoning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many cognitive systems deploy multiple, closed, individually consistent
models which can represent interpretations of the present state of the world,
moments in the past, possible futures or alternate versions of reality. While
they appear under different names, these structures can be grouped under the
general term of worlds. The Xapagy architecture is a story-oriented cognitive
system which relies exclusively on the autobiographical memory implemented as a
raw collection of events organized into world-type structures called {\em
scenes}. The system performs reasoning by shadowing current events with events
from the autobiography. The shadows are then extrapolated into headless shadows
corresponding to predictions, hidden events or inferred relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5644</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5644</id><created>2012-11-23</created><authors><author><keyname>Boloni</keyname><forenames>Ladislau</forenames></author></authors><title>Modeling problems of identity in Little Red Riding Hood</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1105.3486</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues that the problem of identity is a critical challenge in
agents which are able to reason about stories. The Xapagy architecture has been
built from scratch to perform narrative reasoning and relies on a somewhat
unusual approach to represent instances and identity. We illustrate the
approach by a representation of the story of Little Red Riding Hood in the
architecture, with a focus on the problem of identity raised by the narrative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5648</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5648</id><created>2012-11-24</created><authors><author><keyname>Coleman</keyname><forenames>Alfred</forenames></author><author><keyname>Herselman</keyname><forenames>Marlien E</forenames></author><author><keyname>Coleman</keyname><forenames>Mary</forenames></author></authors><title>Improving Computer-Mediated Synchronous Communication of Doctors in
  Rural Communities through Cloud Computing: A Case Study of Rural Hospitals in
  South Africa</title><categories>cs.CY cs.HC</categories><comments>10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigated how doctors in remote rural hospitals in South Africa
use computer-mediated tool to communicate with experienced and specialist
doctors for professional advice to improve on their clinical practices. A case
study approach was used. Ten doctors were purposively selected from ten
hospitals in the North West Province. Data was collected using semi-structured
open ended interview questions. The interviewees were asked to tell in their
own words the average number of patients served per week, processes used in
consultation with other doctors, communication practices using
computer-mediated tool, transmission speed of the computer-mediated tool and
satisfaction in using the computer-mediated communication tool. The findings
revealed that an average of 15 consultations per doctor to a specialist doctor
per week was done through face to face or through telephone conversation
instead of using a computer-mediated tool. Participants cited reasons for not
using computer-mediated tool for communication due to slow transmission speed
of the Internet and regular down turn of the Internet connectivity, constant
electricity power outages and lack of e-health application software to support
real time computer-mediated communication. The results led to the
recommendation of a hybrid cloud computing architecture for improving
communication between doctors in hospitals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5656</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5656</id><created>2012-11-24</created><authors><author><keyname>Otto</keyname><forenames>Martin</forenames></author></authors><title>On Groupoids and Hypergraphs</title><categories>math.CO cs.DM cs.LO</categories><msc-class>05C, 08A, 03C, 20L05</msc-class><acm-class>F.4.1; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel construction of finite groupoids whose Cayley graphs have
large girth even w.r.t. a discounted distance measure that contracts
arbitrarily long sequences of edges from the same colour class (sub-groupoid),
and only counts transitions between colour classes (cosets). These groupoids
are employed towards a generic construction method for finite hypergraphs that
realise specified overlap patterns and avoid small cyclic configurations. The
constructions are based on reduced products with groupoids generated by the
elementary local extension steps, and can be made to preserve the symmetries of
the given overlap pattern. In particular, we obtain highly symmetric, finite
hypergraph coverings without short cycles. The groupoids and their application
in reduced products are sufficiently generic to be applicable to other
constructions that are specified in terms of local glueing operations and
require global finite closure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5669</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5669</id><created>2012-11-24</created><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Scott</keyname><forenames>M. A.</forenames></author></authors><title>Analysis-suitable T-splines: characterization, refineability, and
  approximation</title><categories>cs.GR</categories><journal-ref>Mathematical Models and Methods in Applied Sciences,Vol. 24, No.
  06, pp. 1141-1164 (2014)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We establish several fundamental properties of analysis-suitable T-splines
which are important for design and analysis. First, we characterize T-spline
spaces and prove that the space of smooth bicubic polynomials, defined over the
extended T-mesh of an analysis-suitable T-spline, is contained in the
corresponding analysis-suitable T-spline space. This is accomplished through
the theory of perturbed analysis-suitable T-spline spaces and a simple
topological dimension formula. Second, we establish the theory of
analysis-suitable local refinement and describe the conditions under which two
analysis-suitable T-spline spaces are nested. Last, we demonstrate that these
results can be used to establish basic approximation results which are critical
for analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5687</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5687</id><created>2012-11-24</created><authors><author><keyname>Luo</keyname><forenames>Heng</forenames></author><author><keyname>Carrier</keyname><forenames>Pierre Luc</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep
  Extensions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture
modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or
surpasses the state-of-the-art on texture synthesis and inpainting by
parametric models. We also develop a novel RBM model with a spike-and-slab
visible layer and binary variables in the hidden layer. This model is designed
to be stacked on top of the TssRBM. We show the resulting deep belief network
(DBN) is a powerful generative model that improves on single-layer models and
is capable of modeling not only single high-resolution and challenging textures
but also multiple textures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5694</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5694</id><created>2012-11-24</created><updated>2012-12-22</updated><authors><author><keyname>Louidor</keyname><forenames>Oren</forenames></author><author><keyname>Tessler</keyname><forenames>Ran J.</forenames></author><author><keyname>Vandenberg-Rodes</keyname><forenames>Alexander</forenames></author></authors><title>The Williams Bjerknes Model on Regular Trees</title><categories>math.PR cs.SI math-ph math.MP</categories><comments>25 pages</comments><msc-class>60K35 (primary), 82B41 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Williams Bjerknes model, also known as the biased voter model
on the $d$-regular tree $\bbT^d$, where $d \geq 3$. Starting from an initial
configuration of &quot;healthy&quot; and &quot;infected&quot; vertices, infected vertices infect
their neighbors at Poisson rate $\lambda \geq 1$, while healthy vertices heal
their neighbors at Poisson rate 1. All vertices act independently. It is well
known that starting from a configuration with a positive but finite number of
infected vertices, infected vertices will continue to exist at all time with
positive probability iff $\lambda &gt; 1$. We show that there exists a threshold
$\lambda_c \in (1, \infty)$ such that if $\lambda &gt; \lambda_c$ then in the
above setting with positive probability all vertices will become eventually
infected forever, while if $\lambda &lt; \lambda_c$, all vertices will become
eventually healthy with probability 1. In particular, this yields a complete
convergence theorem for the model and its dual, a certain branching coalescing
random walk on $\bbT^d$ -- above $\lambda_c$. We also treat the case of initial
configurations chosen according to a distribution which is invariant or ergodic
with respect to the group of automorphisms of $\bbT^d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5708</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5708</id><created>2012-11-24</created><authors><author><keyname>Hurd</keyname><forenames>T. R.</forenames></author><author><keyname>Gleeson</keyname><forenames>James P.</forenames></author></authors><title>On Watts' Cascade Model with Random Link Weights</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>24 pages, 5 figures</comments><msc-class>05C80, 05C82, 90B15, 91B74, 91G50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an extension of Duncan Watts' 2002 model of information cascades in
social networks where edge weights are taken to be random, an innovation
motivated by recent applications of cascade analysis to systemic risk in
financial networks. The main result is a probabilistic analysis that
characterizes the cascade in an infinite network as the fixed point of a
vector-valued mapping, explicit in terms of convolution integrals that can be
efficiently evaluated numerically using the fast Fourier transform algorithm. A
second result gives an approximate probabilistic analysis of cascades on &quot;real
world networks&quot;, finite networks based on a fixed deterministic graph.
Extensive cross testing with Monte Carlo estimates shows that this approximate
analysis performs surprisingly well, and provides a flexible microscope that
can be used to investigate properties of information cascades in real world
networks over a wide range of model parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5712</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5712</id><created>2012-11-24</created><authors><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author><author><keyname>Misztal</keyname><forenames>Krzysztof</forenames></author></authors><title>Detection of elliptical shapes via cross-entropy clustering</title><categories>cs.CV</categories><doi>10.1007/978-3-642-38628-2_78</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding elliptical shapes in an image will be considered. We
discuss the solution which uses cross-entropy clustering. The proposed method
allows the search for ellipses with predefined sizes and position in the space.
Moreover, it works well for search of ellipsoids in higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5718</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5718</id><created>2012-11-24</created><authors><author><keyname>Haramaty</keyname><forenames>Elad</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Deterministic Compression with Uncertain Priors</title><categories>cs.IT cs.CC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of compression of information when the source of the
information and the destination do not agree on the prior, i.e., the
distribution from which the information is being generated. This setting was
considered previously by Kalai et al. (ICS 2011) who suggested that this was a
natural model for human communication, and efficient schemes for compression
here could give insights into the behavior of natural languages. Kalai et al.
gave a compression scheme with nearly optimal performance, assuming the source
and destination share some uniform randomness. In this work we explore the need
for this randomness, and give some non-trivial upper bounds on the
deterministic communication complexity for this problem. In the process we
introduce a new family of structured graphs of constant fractional chromatic
number whose (integral) chromatic number turns out to be a key component in the
analysis of the communication complexity. We provide some non-trivial upper
bounds on the chromatic number of these graphs to get our upper bound, while
using lower bounds on variants of these graphs to prove lower bounds for some
natural approaches to solve the communication complexity question. Tight
analysis of communication complexity of our problems and the chromatic number
of the underlying graphs remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5720</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5720</id><created>2012-11-24</created><authors><author><keyname>ElSamadouny</keyname><forenames>Ahmed</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author></authors><title>Cognitive Radio Transmission Strategies for Primary Markovian Channels</title><categories>cs.NI</categories><comments>Journal paper. arXiv admin note: substantial text overlap with
  arXiv:1008.3998</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in cognitive radio systems is that the cognitive radio
is ignorant of the primary channel state and, hence, of the amount of actual
harm it inflicts on the primary license holder. Sensing the primary transmitter
does not help in this regard. To tackle this issue, we assume in this paper
that the cognitive user can eavesdrop on the ACK/NACK Automatic Repeat reQuest
(ARQ) fed back from the primary receiver to the primary transmitter. Assuming a
primary channel state that follows a Markov chain, this feedback gives the
cognitive radio an indication of the primary link quality. Based on the
ACK/NACK received, we devise optimal transmission strategies for the cognitive
radio so as to maximize a weighted sum of primary and secondary throughput. The
actual weight used during network operation is determined by the degree of
protection afforded to the primary link. We begin by formulating the problem
for a channel with a general number of states. We then study a two-state model
where we characterize a scheme that spans the boundary of the primary-secondary
rate region. Moreover, we study a three-state model where we derive the optimal
strategy using dynamic programming. We also extend our two-state model to a
two-channel case, where the secondary user can decide to transmit on a
particular channel or not to transmit at all. We provide numerical results for
our optimal strategies and compare them with simple greedy algorithms for a
range of primary channel parameters. Finally, we investigate the case where
some of the parameters are unknown and are learned using hidden Markov models
(HMM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5723</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5723</id><created>2012-11-24</created><authors><author><keyname>Padhy</keyname><forenames>Neelamadhab</forenames></author><author><keyname>Mishra</keyname><forenames>Dr. Pragnyaban</forenames></author><author><keyname>Panigrahi</keyname><forenames>Rasmita</forenames></author></authors><title>The Survey of Data Mining Applications And Feature Scope</title><categories>cs.DB cs.IR</categories><comments>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.2, No.3, June 2012, 16 pages, 1 table</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.2, No.3,page no-43 June 2012 ,DOI :
  10.5121/ijcseit.2012.2303</journal-ref><doi>10.5121/ijcseit.2012.2303</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we have focused a variety of techniques, approaches and
different areas of the research which are helpful and marked as the important
field of data mining Technologies. As we are aware that many Multinational
companies and large organizations are operated in different places of the
different countries.Each place of operation may generate large volumes of data.
Corporate decision makers require access from all such sources and take
strategic decisions.The data warehouse is used in the significant business
value by improving the effectiveness of managerial decision-making. In an
uncertain and highly competitive business environment, the value of strategic
information systems such as these are easily recognized however in todays
business environment,efficiency or speed is not the only key for
competitiveness.This type of huge amount of data are available in the form of
tera-topeta-bytes which has drastically changed in the areas of science and
engineering.To analyze,manage and make a decision of such type of huge amount
of data we need techniques called the data mining which will transforming in
many fields.This paper imparts more number of applications of the data mining
and also focuses scope of the data mining which will helpful in the further
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5724</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5724</id><created>2012-11-24</created><authors><author><keyname>Padhy</keyname><forenames>Neelamadhab</forenames></author><author><keyname>Panigrahi</keyname><forenames>Rasmita</forenames></author></authors><title>Data Mining: A prediction Technique for the workers in the PR Department
  of Orissa (Block and Panchayat)</title><categories>cs.DB</categories><comments>2tables,3 diagrams, volume-2, Number-5,in the month of November2012</comments><doi>10.5121/ijcseit.2012.2503</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the method of mining the data and which contains the
information about the large information about the PR (Panchayat Raj
Department)of Orissa.We have focused some of the techniques,approaches and
different methodologies of the demand forecasting. Every organizations are
operated in different places of the country. Each place of operation may
generate a huge amount of data. In an organization, worker prediction is the
difficult task of the manager. It is the complex process not only because its
nature of feature prediction but also various approaches methodologies always
makes user confused. This paper aims to deal with the problem selection
process. In this paper we have used some of the approaches from literature are
been introduced and analyzed to find its suitable organization and situation.
Based on this we have designed with automatic selection function to help users
make a prejudgment. This information about each approach will be showed to
users with examples to help understanding. This system also provides
calculation function to help users work out a predication result. Generally the
new developed system has a more comprehensive functions compared with existing
ones. It aims to improve the accuracy of demand forecasting by implementing the
forecasting algorithm. While it is still a decision support system with no
ability of make the final judgment.This type of huge amount of data are are
available in the form of different ways which has drastically changed in the
areas of science and engineering.To analyze, manage and make a decision of such
type of huge amount of data we need techniques called the data mining which
will transforming in many fields. We have implemented the algorithms in JAVA
technology. This paper provides the prediction algorithm Linear Regression,
result which will helpful in the further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5729</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5729</id><created>2012-11-25</created><updated>2014-01-14</updated><authors><author><keyname>Zhu</keyname><forenames>Xiaojun</forenames></author><author><keyname>Li</keyname><forenames>Qun</forenames></author><author><keyname>Mao</keyname><forenames>Weizhen</forenames></author><author><keyname>Chen</keyname><forenames>Guihai</forenames></author></authors><title>Online Vector Scheduling and Generalized Load Balancing</title><categories>cs.CC cs.DS</categories><comments>This work has been accepted to JPDC. Please refer to
  http://dx.doi.org/10.1016/j.jpdc.2013.12.006</comments><report-no>WM-CS-2012-01</report-no><doi>10.1016/j.jpdc.2013.12.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial time reduction from vector scheduling problem (VS) to
generalized load balancing problem (GLB). This reduction gives the first
non-trivial online algorithm for VS where vectors come in an online fashion.
The online algorithm is very simple in that each vector only needs to minimize
the $L_{\ln(md)}$ norm of the resulting load when it comes, where $m$ is the
number of partitions and $d$ is the dimension of vectors. It has an
approximation bound of $e\log(md)$, which is in $O(\ln(md))$, so it also
improves the $O(\ln^2d)$ bound of the existing polynomial time algorithm for
VS. Additionally, the reduction shows that GLB does not have constant
approximation algorithms that run in polynomial time unless $P=NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5735</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5735</id><created>2012-11-25</created><updated>2013-01-26</updated><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Generalized Degrees of Freedom for Network-Coded Cognitive Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>submitted to ISIT2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a two-user cognitive interference channel (CIC) where one of the
transmitters (primary) has knowledge of a linear combination (over an
appropriate finite field) of the two information messages. We refer to this
channel model as Network-Coded CIC, since the linear combination may be the
result of some linear network coding scheme implemented in the backbone wired
network.In this paper, we characterize the generalized degrees of freedom
(GDoF) for the Gaussian Network-Coded CIC. For achievability, we use the novel
Precoded Compute-and-Forward (PCoF) and Dirty Paper Coding (DPC), based on
nested lattice codes. As a consequence of the GDoF characterization, we show
that knowing &quot;mixed data&quot; (linear combinations of the information messages)
provides a {\em multiplicative} gain for the Gaussian CIC, if the power ratio
of signal-to-noise (SNR) to interference-to-noise (INR) is larger than certain
threshold. For example, when $\SNR=\INR$, the Network-Coded cognition yields a
100% gain over the classical Gaussian CIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5736</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5736</id><created>2012-11-25</created><authors><author><keyname>Dondossola</keyname><forenames>Giovanna</forenames><affiliation>ISTI</affiliation></author><author><keyname>Deconinck</keyname><forenames>Geert</forenames><affiliation>ISTI</affiliation></author><author><keyname>Di Giandomenico</keyname><forenames>Felicita</forenames><affiliation>ISTI</affiliation></author><author><keyname>Donatelli</keyname><forenames>Susanna</forenames><affiliation>LAAS</affiliation></author><author><keyname>Kaaniche</keyname><forenames>Mohamed</forenames><affiliation>LAAS</affiliation></author><author><keyname>Verissimo</keyname><forenames>Paulo</forenames></author></authors><title>Critical Utility Infrastructural Resilience</title><categories>cs.PF</categories><proxy>ccsd</proxy><journal-ref>International Workshop on Complex Network and Infrastructure
  Protection (CNIP-06), Rome : Italy (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper refers to CRUTIAL, CRitical UTility InfrastructurAL Resilience, a
European project within the research area of Critical Information
Infrastructure Protection, with a specific focus on the infrastructures
operated by power utilities, widely recognized as fundamental to national and
international economy, security and quality of life. Such infrastructures faced
with the recent market deregulations and the multiple interdependencies with
other infrastructures are becoming more and more vulnerable to various threats,
including accidental failures and deliberate sabotage and malicious attacks.
The subject of CRUTIAL research are small scale networked ICT systems used to
control and manage the electric power grid, in which artifacts controlling the
physical process of electricity transportation need to be connected with
corporate and societal applications performing management and maintenance
functionality. The peculiarity of such ICT-supported systems is that they are
related to the power system dynamics and its emergency conditions. Specific
effort need to be devoted by the Electric Power community and by the
Information Technology community to influence the technological progress in
order to allow commercial intelligent electronic devices to be effectively
deployed for the protection of citizens against cyber threats to electric power
management and control systems. A well-founded know-how needs to be built
inside the industrial power sector to allow all the involved stakeholders to
achieve their service objectives without compromising the resilience properties
of the logical and physical assets that support the electric power provision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5738</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5738</id><created>2012-11-25</created><authors><author><keyname>Kaaniche</keyname><forenames>Mohamed</forenames><affiliation>LAAS</affiliation></author><author><keyname>Lollini</keyname><forenames>Paolo</forenames><affiliation>University of Florence</affiliation></author><author><keyname>Bondavalli</keyname><forenames>Andrea</forenames><affiliation>University of Florence</affiliation></author><author><keyname>Kanoun</keyname><forenames>Karama</forenames><affiliation>LAAS</affiliation></author></authors><title>Modeling the resilience of large and evolving systems</title><categories>cs.PF</categories><proxy>ccsd</proxy><journal-ref>International Journal of Performability Engineering 4, 2 (2008)
  153-168</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the state of knowledge and ongoing research on methods
and techniques for resilience evaluation, taking into account the
resilience-scaling challenges and properties related to the ubiquitous
computerized systems. We mainly focus on quantitative evaluation approaches
and, in particular, on model-based evaluation techniques that are commonly used
to evaluate and compare, from the dependability point of view, different
architecture alternatives at the design stage. We outline some of the main
modeling techniques aiming at mastering the largeness of analytical
dependability models at the construction level. Actually, addressing the model
largeness problem is important with respect to the investigation of the
scalability of current techniques to meet the complexity challenges of
ubiquitous systems. Finally we present two case studies in which some of the
presented techniques are applied for modeling web services and General Packet
Radio Service (GPRS) mobile telephone networks, as prominent examples of large
and evolving systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5739</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5739</id><created>2012-11-25</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Optimal Selection of Measurement Configurations for Stiffness Model
  Calibration of Anthropomorphic Manipulators</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>Applied Mechanics and Materials 162 (2012) 161-170</journal-ref><doi>10.4028/www.scientific.net/AMM.162.161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the calibration of elastostatic parameters of spatial
anthropomorphic robots. It proposes a new strategy for optimal selection of the
measurement configurations that essentially increases the efficiency of robot
calibration. This strategy is based on the concept of the robot test-pose and
ensures the best compliance error compensation for the test configuration. The
advantages of the proposed approach and its suitability for practical
applications are illustrated by numerical examples, which deal with calibration
of elastostatic parameters of a 3 degrees of freedom anthropomorphic
manipulator with rigid links and compliant actuated joints
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5740</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5740</id><created>2012-11-25</created><authors><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Industry-oriented Performance Measures for Design of Robot Calibration
  Experiment</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>New Trends in Mechanism and Machine Science, F. Viadero and M.
  Ceccarelli ( (Ed.) (2012) 519-527</journal-ref><doi>10.1007/978-94-007-4902-3 55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the accuracy improvement of geometric and elasto-static
calibration of industrial robots. It proposes industry-oriented performance
measures for the calibration experiment design. They are based on the concept
of manipulator test-pose and referred to the end-effector location accuracy
after application of the error compensation algorithm, which implements the
identified parameters. This approach allows the users to define optimal
measurement configurations for robot calibration for given work piece location
and machining forces/torques. These performance measures are suitable for
comparing the calibration plans for both simple and complex trajectories to be
performed. The advantages of the developed techniques are illustrated by an
example that deals with machining using robotic manipulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5747</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5747</id><created>2012-11-25</created><authors><author><keyname>ValadBeigi</keyname><forenames>Majed</forenames></author><author><keyname>Safaei</keyname><forenames>Farshad</forenames></author><author><keyname>Pourshirazi</keyname><forenames>Bahareh</forenames></author></authors><title>DBR: A Simple, Fast and Efficient Dynamic Network Reconfiguration
  Mechanism Based on Deadlock Recovery Scheme</title><categories>cs.DC cs.NI</categories><comments>14 pages, 8 figures, 1 table</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.3, No.5, 2012, 13-26</journal-ref><doi>10.5121/vlsic.2012.3502</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Dynamic network reconfiguration is described as the process of replacing one
routing function with another while the network keeps running. The main
challenge is avoiding deadlock anomalies while keeping limitations on message
injection and forwarding minimal. Current approaches, whose complexity is so
high that their practical applicability is limited, either require the
existence of extra network resources like virtual channels, or they affect the
performance of the network during the reconfiguration process. In this paper we
present a simple, fast and efficient mechanism for dynamic network
reconfiguration which is based on regressive deadlock recoveries instead of
avoiding deadlocks. The mechanism which is referred to as DBR guarantees a
deadlock-free reconfiguration based on wormhole switching (WS) and it does not
require additional resources. In this approach, the need for a reliable message
transmission has led to a modified WS mechanism which includes additional flits
or control signals. DBR allows cycles to be formed and in such conditions when
a deadlock occurs, the messages suffer from time-out. Then, this method
releases the buffers and channels from the current node and thus the source
retransmits the message after a random time gap. Evaluating results reveal that
the mechanism shows substantial performance improvements over the other methods
and it works efficiently in different topologies with various routing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5757</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5757</id><created>2012-11-25</created><updated>2013-05-03</updated><authors><author><keyname>Punekar</keyname><forenames>Mayur</forenames></author><author><keyname>Vontobel</keyname><forenames>Pascal O.</forenames></author><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author></authors><title>Low-Complexity LP Decoding of Nonbinary Linear Codes</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Programming (LP) decoding of Low-Density Parity-Check (LDPC) codes has
attracted much attention in the research community in the past few years. LP
decoding has been derived for binary and nonbinary linear codes. However, the
most important problem with LP decoding for both binary and nonbinary linear
codes is that the complexity of standard LP solvers such as the simplex
algorithm remains prohibitively large for codes of moderate to large block
length. To address this problem, two low-complexity LP (LCLP) decoding
algorithms for binary linear codes have been proposed by Vontobel and Koetter,
henceforth called the basic LCLP decoding algorithm and the subgradient LCLP
decoding algorithm.
  In this paper, we generalize these LCLP decoding algorithms to nonbinary
linear codes. The computational complexity per iteration of the proposed
nonbinary LCLP decoding algorithms scales linearly with the block length of the
code. A modified BCJR algorithm for efficient check-node calculations in the
nonbinary basic LCLP decoding algorithm is also proposed, which has complexity
linear in the check node degree.
  Several simulation results are presented for nonbinary LDPC codes defined
over Z_4, GF(4), and GF(8) using quaternary phase-shift keying and
8-phase-shift keying, respectively, over the AWGN channel. It is shown that for
some group-structured LDPC codes, the error-correcting performance of the
nonbinary LCLP decoding algorithms is similar to or better than that of the
min-sum decoding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5758</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5758</id><created>2012-11-25</created><authors><author><keyname>Stumper</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Kennel</keyname><forenames>Ralph</forenames></author></authors><title>Inversion of Linear and Nonlinear Observable Systems with Series-defined
  Output Trajectories</title><categories>cs.SY math.DS</categories><comments>Proceedings of the IEEE International Symposium on Computer-Aided
  Control System Design, pp. 1993-1998, Yokohama, Japan, September 8-10, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of inverting a system in presence of a series-defined output is
analyzed. Inverse models are derived that consist of a set of algebraic
equations. The inversion is performed explicitly for an output trajectory
functional, which is a linear combination of some basis functions with
arbitrarily free coefficients. The observer canonical form is exploited, and
the input-output representation is solved using a series method. It is shown
that the only required system characteristic is observability, which implies
that there is no need for output redefinition. An exact inverse model is found
for linear systems. For general nonlinear systems, a good approximation of the
inverse model valid on a finite time interval is found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5759</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5759</id><created>2012-11-25</created><authors><author><keyname>Stumper</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Svaricek</keyname><forenames>Ferdinand</forenames></author><author><keyname>Kennel</keyname><forenames>Ralph</forenames></author></authors><title>Trajectory Tracking Control with Flat Inputs and a Dynamic Compensator</title><categories>cs.SY math.DS</categories><comments>Proceedings of the European Control Conference, pp. 248-253,
  Budapest, Hungary, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a tracking controller based on the concept of flat inputs
and a dynamic compensator. Flat inputs represent a dual approach to flat
outputs. In contrast to conventional flatness-based control design, the
regulated output may be a non-flat output, or the system may be non-flat. The
method is applicable to observable systems with stable internal dynamics. The
performance of the new design is demonstrated on the variable-length pendulum,
a non-flat nonlinear system with a singularity in the relative degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5761</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5761</id><created>2012-11-25</created><authors><author><keyname>Stumper</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Kennel</keyname><forenames>Ralph</forenames></author></authors><title>Computationally Efficient Trajectory Optimization for Linear Control
  Systems with Input and State Constraints</title><categories>cs.SY math.OC</categories><comments>Proceedings of the American Control Conference (ACC), pp. 1904-1909,
  San Francisco, USA, June 29 - July 1, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a trajectory generation method that optimizes a quadratic
cost functional with respect to linear system dynamics and to linear input and
state constraints. The method is based on continuous-time flatness-based
trajectory generation, and the outputs are parameterized using a polynomial
basis. A method to parameterize the constraints is introduced using a result on
polynomial nonpositivity. The resulting parameterized problem remains
linear-quadratic and can be solved using quadratic programming. The problem can
be further simplified to a linear programming problem by linearization around
the unconstrained optimum. The method promises to be computationally efficient
for constrained systems with a high optimization horizon. As application, a
predictive torque controller for a permanent magnet synchronous motor which is
based on real-time optimization is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5766</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5766</id><created>2012-11-25</created><authors><author><keyname>Hamou</keyname><forenames>Reda Mohamed</forenames></author><author><keyname>Amine</keyname><forenames>Abdelmalek</forenames></author><author><keyname>Lokbani</keyname><forenames>Ahmed Chaouki</forenames></author><author><keyname>Simonet</keyname><forenames>Michel</forenames></author></authors><title>Visualization and clustering by 3D cellular automata: Application to
  unstructured data</title><categories>cs.AI cs.IR</categories><comments>10 pages, 8 figures</comments><journal-ref>International Journal Of Data Mining And Emerging Technologies.
  2-1 (2012) 15-25</journal-ref><doi>10.5958/j.2249-3212.2.1.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the limited performance of 2D cellular automata in terms of space when
the number of documents increases and in terms of visualization clusters, our
motivation was to experiment these cellular automata by increasing the size to
view the impact of size on quality of results. The representation of textual
data was carried out by a vector model whose components are derived from the
overall balancing of the used corpus, Term Frequency Inverse Document Frequency
(TF-IDF). The WorldNet thesaurus has been used to address the problem of the
lemmatization of the words because the representation used in this study is
that of the bags of words. Another independent method of the language was used
to represent textual records is that of the n-grams. Several measures of
similarity have been tested. To validate the classification we have used two
measures of assessment based on the recall and precision (f-measure and
entropy). The results are promising and confirm the idea to increase the
dimension to the problem of the spatiality of the classes. The results obtained
in terms of purity class (i.e. the minimum value of entropy) shows that the
number of documents over longer believes the results are better for 3D cellular
automata, which was not obvious to the 2D dimension. In terms of spatial
navigation, cellular automata provide very good 3D performance visualization
than 2D cellular automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5773</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5773</id><created>2012-11-25</created><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>Value Constraint and Monotone circuit</title><categories>cs.CC</categories><comments>3 pages, English and Japanese (see Other formats - Source)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper talks about that monotone circuit is P-Complete.
  Decision problem that include P-Complete is mapping that classify input with
a similar property. Therefore equivalence relation of input value is important
for computation. But monotone circuit cannot compute the equivalence relation
of the value because monotone circuit can compute only monotone function.
Therefore, I make the value constraint explicitly in the input and monotone
circuit can compute equivalence relation. As a result, we can compute
P-Complete problem with monotone circuit. We can reduce implicit value
constraint to explicit with logarithm space. Therefore, monotone circuit is
P-Complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5787</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5787</id><created>2012-11-25</created><updated>2015-09-13</updated><authors><author><keyname>Feinerman</keyname><forenames>Ofer</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author><author><keyname>Kutten</keyname><forenames>Shay</forenames></author><author><keyname>Rodeh</keyname><forenames>Yoav</forenames></author></authors><title>Fast Rendezvous on a Cycle by Agents with Different Speeds</title><categories>cs.DC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The difference between the speed of the actions of different processes is
typically considered as an obstacle that makes the achievement of cooperative
goals more difficult. In this work, we aim to highlight potential benefits of
such asynchrony phenomena to tasks involving symmetry breaking. Specifically,
in this paper, identical (except for their speeds) mobile agents are placed at
arbitrary locations on a cycle of length $n$ and use their speed difference in
order to rendezvous fast. We normalize the speed of the slower agent to be 1,
and fix the speed of the faster agent to be some $c&gt;1$. (An agent does not know
whether it is the slower agent or the faster one.) The straightforward
distributed-race DR algorithm is the one in which both agents simply start
walking until rendezvous is achieved. It is easy to show that, in the worst
case, the rendezvous time of DR is $n/(c-1)$. Note that in the interesting
case, where $c$ is very close to 1 this bound becomes huge. Our first result is
a lower bound showing that, up to a multiplicative factor of 2, this bound is
unavoidable, even in a model that allows agents to leave arbitrary marks, even
assuming sense of direction, and even assuming $n$ and $c$ are known to agents.
That is, we show that under such assumptions, the rendezvous time of any
algorithm is at least $\frac{n}{2(c-1)}$ if $c\leq 3$ and slightly larger if
$c&gt;3$. We then construct an algorithm that precisely matches the lower bound
for the case $c\leq 2$, and almost matches it when $c&gt;2$. Moreover, our
algorithm performs under weaker assumptions than those stated above, as it does
not assume sense of direction, and it allows agents to leave only a single mark
(a pebble) and only at the place where they start the execution. Finally, we
investigate the setting in which no marks can be used at all, and show tight
bounds for $c\leq 2$, and almost tight bounds for $c&gt;2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5793</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5793</id><created>2012-11-25</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Hovland</keyname><forenames>Geir</forenames></author></authors><title>Compliance error compensation technique for parallel robots composed of
  non-perfect serial chains</title><categories>cs.RO</categories><comments>arXiv admin note: text overlap with arXiv:1204.1757</comments><proxy>ccsd</proxy><journal-ref>Robotics and Computer-Integrated Manufacturing 29, 2 (2012)
  385-393</journal-ref><doi>10.1016/j.rcim.2012.09.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents the compliance errors compensation technique for
over-constrained parallel manipulators under external and internal loadings.
This technique is based on the non-linear stiffness modeling which is able to
take into account the influence of non-perfect geometry of serial chains caused
by manufacturing errors. Within the developed technique, the deviation
compensation reduces to an adjustment of a target trajectory that is modified
in the off-line mode. The advantages and practical significance of the proposed
technique are illustrated by an example that deals with groove milling by the
Orthoglide manipulator that considers different locations of the workpiece. It
is also demonstrated that the impact of the compliance errors and the errors
caused by inaccuracy in serial chains cannot be taken into account using the
superposition principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5795</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5795</id><created>2012-11-25</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Stiffness modeling of non-perfect parallel manipulators</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2012), Vilamoura : Portugal (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the stiffness modeling of parallel manipulators composed
of non-perfect serial chains, whose geometrical parameters differ from the
nominal ones. In these manipulators, there usually exist essential internal
forces/torques that considerably affect the stiffness properties and also
change the end-effector location. These internal load-ings are caused by
elastic deformations of the manipulator ele-ments during assembling, while the
geometrical errors in the chains are compensated for by applying appropriate
forces. For this type of manipulators, a non-linear stiffness modeling
tech-nique is proposed that allows us to take into account inaccuracy in the
chains and to aggregate their stiffness models for the case of both small and
large deflections. Advantages of the developed technique and its ability to
compute and compensate for the compliance errors caused by different factors
are illustrated by an example that deals with parallel manipulators of the
Or-thoglide family
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5803</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5803</id><created>2012-11-25</created><updated>2014-11-27</updated><authors><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author></authors><title>Fast community detection by SCORE</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOS1265 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1265</report-no><journal-ref>Annals of Statistics 2015, Vol. 43, No. 1, 57-89</journal-ref><doi>10.1214/14-AOS1265</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a network where the nodes split into $K$ different communities. The
community labels for the nodes are unknown and it is of major interest to
estimate them (i.e., community detection). Degree Corrected Block Model (DCBM)
is a popular network model. How to detect communities with the DCBM is an
interesting problem, where the main challenge lies in the degree heterogeneity.
We propose a new approach to community detection which we call the Spectral
Clustering On Ratios-of-Eigenvectors (SCORE). Compared to classical spectral
methods, the main innovation is to use the entry-wise ratios between the first
leading eigenvector and each of the other leading eigenvectors for clustering.
Let $A$ be the adjacency matrix of the network. We first obtain the $K$ leading
eigenvectors of $A$, say, $\hat{\eta}_1,\ldots,\hat{\eta}_K$, and let $\hat{R}$
be the $n\times (K-1)$ matrix such that
$\hat{R}(i,k)=\hat{\eta}_{k+1}(i)/\hat{\eta}_1(i)$, $1\leq i\leq n$, $1\leq
k\leq K-1$. We then use $\hat{R}$ for clustering by applying the $k$-means
method. The central surprise is, the effect of degree heterogeneity is largely
ancillary, and can be effectively removed by taking entry-wise ratios between
$\hat{\eta}_{k+1}$ and $\hat{\eta}_1$, $1\leq k\leq K-1$. The method is
successfully applied to the web blogs data and the karate club data, with error
rates of $58/1222$ and $1/34$, respectively. These results are more
satisfactory than those by the classical spectral methods. Additionally,
compared to modularity methods, SCORE is easier to implement, computationally
faster, and also has smaller error rates. We develop a theoretic framework
where we show that under mild conditions, the SCORE stably yields consistent
community detection. In the core of the analysis is the recent development on
Random Matrix Theory (RMT), where the matrix-form Bernstein inequality is
especially helpful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5811</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5811</id><created>2012-11-25</created><authors><author><keyname>Krivulin</keyname><forenames>N. K.</forenames></author></authors><title>A max-algebra approach to modeling and simulation of tandem queueing
  systems</title><categories>math.NA cs.SY</categories><comments>19 pages</comments><msc-class>15A80 (Primary), 90B22, 68U20, 68M20 68W10 (Secondary)</msc-class><journal-ref>Mathematical and Computer Modelling, 1995. Vol. 22, no. 3, pp.
  25-31</journal-ref><doi>10.1016/0895-7177(95)00117-K</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-algebra models of tandem single-server queueing systems with both finite
and infinite buffers are developed. The dynamics of each system is described by
a linear vector state equation similar to those in the conventional linear
systems theory, and it is determined by a transition matrix inherent in the
system. The departure epochs of a customer from the queues are considered as
state variables, whereas its service times are assumed to be system parameters.
We show how transition matrices may be calculated from the service times, and
present the matrices associated with particular models. We also give a
representation of system performance measures including the system time and the
waiting time of customers, associated with the models. As an application, both
serial and parallel simulation procedures are presented, and their performance
is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5817</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5817</id><created>2012-11-21</created><authors><author><keyname>Beheshti</keyname><forenames>Seyed-Mehdi-Reza</forenames></author><author><keyname>Sakr</keyname><forenames>Sherif</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author><author><keyname>Motahari-Nezhad</keyname><forenames>Hamid Reza</forenames></author></authors><title>Extending SPARQL to Support Entity Grouping and Path Queries</title><categories>cs.DB</categories><comments>23 pages. arXiv admin note: text overlap with arXiv:1211.5009</comments><report-no>UNSW-CSE-TR-1019</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to efficiently find relevant subgraphs and paths in a large graph
to a given query is important in many applications including scientific data
analysis, social networks, and business intelligence. Currently, there is
little support and no efficient approaches for expressing and executing such
queries. This paper proposes a data model and a query language to address this
problem. The contributions include supporting the construction and selection
of: (i) folder nodes, representing a set of related entities, and (ii) path
nodes, representing a set of paths in which a path is the transitive
relationship of two or more entities in the graph. Folders and paths can be
stored and used for future queries. We introduce FPSPARQL which is an extension
of the SPARQL supporting folder and path nodes. We have implemented a query
engine that supports FPSPARQL and the evaluation results shows its viability
and efficiency for querying large graph datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5820</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5820</id><created>2012-11-25</created><authors><author><keyname>Yan</keyname><forenames>Erjia</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Cronin</keyname><forenames>Blaise</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>A bird's-eye view of scientific trading: Dependency relations among
  fields of science</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a trading metaphor to study knowledge transfer in the sciences as well
as the social sciences. The metaphor comprises four dimensions: (a) Discipline
Self-dependence, (b) Knowledge Exports/Imports, (c) Scientific Trading
Dynamics, and (d) Scientific Trading Impact. This framework is applied to a
dataset of 221 Web of Science subject categories. We find that: (i) the
Scientific Trading Impact and Dynamics of Materials Science And Transportation
Science have increased; (ii) Biomedical Disciplines, Physics, And Mathematics
are significant knowledge exporters, as is Statistics &amp; Probability; (iii) in
the social sciences, Economics, Business, Psychology, Management, And Sociology
are important knowledge exporters; (iv) Discipline Self-dependence is
associated with specialized domains which have ties to professional practice
(e.g., Law, Ophthalmology, Dentistry, Oral Surgery &amp; Medicine, Psychology,
Psychoanalysis, Veterinary Sciences, And Nursing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5829</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5829</id><created>2012-11-25</created><authors><author><keyname>Oji</keyname><forenames>Reza</forenames></author></authors><title>An Automatic Algorithm for Object Recognition and Detection Based on
  ASIFT Keypoints</title><categories>cs.AI cs.CV</categories><comments>11 pages - 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1210.7038</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.3, No.5, 2012, pp 29-39</journal-ref><doi>10.5121/sipij.2012.3503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition is an important task in image processing and computer
vision. This paper presents a perfect method for object recognition with full
boundary detection by combining affine scale invariant feature transform
(ASIFT) and a region merging algorithm. ASIFT is a fully affine invariant
algorithm that means features are invariant to six affine parameters namely
translation (2 parameters), zoom, rotation and two camera axis orientations.
The features are very reliable and give us strong keypoints that can be used
for matching between different images of an object. We trained an object in
several images with different aspects for finding best keypoints of it. Then, a
robust region merging algorithm is used to recognize and detect the object with
full boundary in the other images based on ASIFT keypoints and a similarity
measure for merging regions in the image. Experimental results show that the
presented method is very efficient and powerful to recognize the object and
detect it with high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5837</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5837</id><created>2012-11-25</created><authors><author><keyname>van Gennip</keyname><forenames>Yves</forenames></author><author><keyname>Hu</keyname><forenames>Huiyi</forenames></author><author><keyname>Hunter</keyname><forenames>Blake</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Geosocial Graph-Based Community Detection</title><categories>cs.SI physics.soc-ph</categories><comments>5 pages, 4 figures Workshop paper for the IEEE International
  Conference on Data Mining 2012: Workshop on Social Media Analysis and Mining</comments><msc-class>62H30, 91C20, 91D30, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply spectral clustering and multislice modularity optimization to a Los
Angeles Police Department field interview card data set. To detect communities
(i.e., cohesive groups of vertices), we use both geographic and social
information about stops involving street gang members in the LAPD district of
Hollenbeck. We then compare the algorithmically detected communities with known
gang identifications and argue that discrepancies are due to sparsity of social
connections in the data as well as complex underlying sociological factors that
blur distinctions between communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5842</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5842</id><created>2012-11-25</created><authors><author><keyname>Mirahmadi</keyname><forenames>Maysam</forenames></author><author><keyname>Shami</keyname><forenames>Abdallah</forenames></author></authors><title>A Novel Algorithm for Real-time Procedural Generation of Building Floor
  Plans</title><categories>cs.GR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Real-time generation of natural-looking floor plans is vital in games with
dynamic environments. This paper presents an algorithm to generate suburban
house floor plans in real-time. The algorithm is based on the work presented in
[1]. However, the corridor placement is redesigned to produce floor plans
similar to real houses. Moreover, an optimization stage is added to find a
corridor placement with the minimum used space, an approach that is designed to
mimic the real-life practices to minimize the wasted spaces in the design. The
results show very similar floor plans to the ones designed by an architect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5852</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5852</id><created>2012-11-25</created><authors><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Misra</keyname><forenames>Vishal</forenames></author></authors><title>On the Evolution of the Internet Economic Ecosystem</title><categories>cs.NI</categories><comments>25 pages, 18 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of the Internet has manifested itself in many ways: the traffic
characteristics, the interconnection topologies and the business relationships
among the autonomous components. It is important to understand why (and how)
this evolution came about, and how the interplay of these dynamics may affect
future evolution and services. We propose a network aware, macroscopic model
that captures the characteristics and interactions of the application and
network providers, and show how it leads to a market equilibrium of the
ecosystem. By analyzing the driving forces and the dynamics of the market
equilibrium, we obtain some fundamental understandings of the cause and effect
of the Internet evolution, which explain why some historical and recent
evolutions have happened. Furthermore, by projecting the likely future
evolutions, our model can help application and network providers to make
informed business decisions so as to succeed in this competitive ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5856</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5856</id><created>2012-11-25</created><updated>2014-01-24</updated><authors><author><keyname>Dall'Anese</keyname><forenames>Emiliano</forenames></author><author><keyname>Zhu</keyname><forenames>Hao</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Distributed Optimal Power Flow for Smart Microgrids</title><categories>math.OC cs.SY</categories><comments>Appeared on IEEE Transactions of Smart Grid. A couple of corrections
  made. IEEE Transactions of Smart Grid, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal power flow (OPF) is considered for microgrids, with the objective of
minimizing either the power distribution losses, or, the cost of power drawn
from the substation and supplied by distributed generation (DG) units, while
effecting voltage regulation. The microgrid is unbalanced, due to unequal loads
in each phase and non-equilateral conductor spacings on the distribution lines.
Similar to OPF formulations for balanced systems, the considered OPF problem is
nonconvex. Nevertheless, a semidefinite programming (SDP) relaxation technique
is advocated to obtain a convex problem solvable in polynomial-time complexity.
Enticingly, numerical tests demonstrate the ability of the proposed method to
attain the globally optimal solution of the original nonconvex OPF. To ensure
scalability with respect to the number of nodes, robustness to isolated
communication outages, and data privacy and integrity, the proposed SDP is
solved in a distributed fashion by resorting to the alternating direction
method of multipliers. The resulting algorithm entails iterative
message-passing among groups of consumers and guarantees faster convergence
compared to competing alternatives
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5857</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5857</id><created>2012-11-25</created><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author></authors><title>Hierarchic Power Allocation for Spectrum Sharing in OFDM-Based Cognitive
  Radio Networks</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a Stackelberg game is built to model the hierarchic power
allocation of primary user (PU) network and secondary user (SU) network in
OFDM-based cognitive radio (CR) networks. We formulate the PU and the SUs as
the leader and the followers, respectively. We consider two constraints: the
total power constraint and the interference-to-signal ratio (ISR) constraint,
in which the ratio between the accumulated interference and the received signal
power at each PU should not exceed certain threshold. Firstly, we focus on the
single-PU and multi-SU scenario. Based on the analysis of the Stackelberg
Equilibrium (SE) for the proposed Stackelberg game, an analytical hierarchic
power allocation method is proposed when the PU can acquire the additional
information to anticipate SUs' reaction. The analytical algorithm has two
steps: 1) The PU optimizes its power allocation with considering the reaction
of SUs to its action. In the power optimization of the PU, there is a sub-game
for power allocation of SUs given fixed transmit power of the PU. The existence
and uniqueness for the Nash Equilibrium (NE) of the sub-game are investigated.
We also propose an iterative algorithm to obtain the NE, and derive the
closed-form solutions of NE for the perfectly symmetric channel. 2) The SUs
allocate the power according to the NE of the sub-game given PU's optimal power
allocation. Furthermore, we design two distributed iterative algorithms for the
general channel even when private information of the SUs is unavailable at the
PU. The first iterative algorithm has a guaranteed convergence performance, and
the second iterative algorithm employs asynchronous power update to improve
time efficiency. Finally, we extend to the multi-PU and multi-SU scenario, and
a distributed iterative algorithm is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5870</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5870</id><created>2012-11-26</created><authors><author><keyname>Fannjiang</keyname><forenames>A.</forenames></author><author><keyname>Liao</keyname><forenames>W.</forenames></author></authors><title>Super-Resolution by Compressive Sensing Algorithms</title><categories>cs.IT math.IT physics.optics</categories><comments>IEEE Proceeding Asilomar conference on signals, systems and
  computers. Nov. 4-7, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, super-resolution by 4 compressive sensing methods (OMP, BP,
BLOOMP, BP-BLOT) with highly coherent partial Fourier measurements is
comparatively studied. An alternative metric more suitable for gauging the
quality of spike recovery is introduced and based on the concept of filtration
with a parameter representing the level of tolerance for support offset. In
terms of the filtered error norm only BLOOMP and BP-BLOT can perform
grid-independent recovery of well separated spikes of Rayleigh index 1 for
arbitrarily large super-resolution factor. Moreover both BLOOMP and BP-BLOT can
localize spike support within a few percent of the Rayleigh length. This is a
weak form of super-resolution. Only BP-BLOT can achieve this feat for closely
spaced spikes separated by a fraction of the Rayleigh length, a strong form of
super-resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5873</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5873</id><created>2012-11-26</created><authors><author><keyname>Cassez</keyname><forenames>Franck</forenames><affiliation>NICTA</affiliation></author><author><keyname>Huuck</keyname><forenames>Ralf</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Klein</keyname><forenames>Gerwin</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Schlich</keyname><forenames>Bastian</forenames><affiliation>ABB</affiliation></author></authors><title>Proceedings Seventh Conference on Systems Software Verification</title><categories>cs.SE cs.LO</categories><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 102, 2012</journal-ref><doi>10.4204/EPTCS.102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers accepted at the 7th Systems Software
Verification Conference (SSV 2012), held in Sydney, November 28-30, 2012.
  The aim of SSV workshops and conference series is to bring together
researchers and developers from both academia and industry who are facing real
software and real problems with the goal of finding real, applicable solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5874</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5874</id><created>2012-11-26</created><authors><author><keyname>Crupi</keyname><forenames>Marilena</forenames></author><author><keyname>Rinaldo</keyname><forenames>Giancarlo</forenames></author></authors><title>Closed graphs are proper interval graphs</title><categories>math.CO cs.DM</categories><comments>7 pages</comments><msc-class>05E40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove that every closed graph $G$ is up to isomorphism a
proper interval graph. As a consequence we obtain that there exist linear-time
algorithms for closed graph recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5877</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5877</id><created>2012-11-26</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author><author><keyname>Noah</keyname><forenames>Shahrul Azman</forenames></author></authors><title>A Methodology to Extract Social Network from the Web Snippet</title><categories>cs.SI cs.IR</categories><comments>7 pages, draft to conference: ICOCSIM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web has been chosen as a basic infrastructure to gain the social
structure information, through the social network extraction, from all over the
world. However, most of the web documents are unstructured and lack of
semantics. Moreover, that network is subject to all kinds of changes and
dynamics, and a network can be very complex due to the large number of nodes
and links Web contains. In this paper, we discuss a methodology that meant to
assists in extracting and modeling the social network from Web snippet. As the
manual social network extraction of web documents is impractical and
unscalable, and fully automated extraction are still at the very early stage to
be implemented, we proposed a (semi)-automatic extraction based on the
superficial methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5882</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5882</id><created>2012-11-26</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Krause</keyname><forenames>Jens</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Multi-User Detection in Multibeam Mobile Satellite Systems: A Fair
  Performance Evaluation</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, conference paper submitted to the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-User Detection (MUD) techniques are currently being examined as
promising technologies for the next generation of broadband, interactive,
multibeam, satellite communication (SatCom) systems. Results in the existing
literature have shown that when full frequency and polarization reuse is
employed and user signals are jointly processed at the gateway, more than
threefold gains in terms of spectral efficiency over conventional systems can
be obtained. However, the information theoretic results for the capacity of the
multibeam satellite channel, are given under ideal assumptions, disregarding
the implementation constraints of such an approach. Considering a real system
implementation, the adoption of full resource reuse is bound to increase the
payload complexity and power consumption. Since the novel techniques require
extra payload resources, fairness issues in the comparison among the two
approaches arise. The present contribution evaluates in a fair manner, the
performance of the return link (RL) of a SatCom system serving mobile users
that are jointly decoded at the receiver. More specifically, the achievable
spectral efficiency of the assumed system is compared to a conventional system
under the constraint of equal physical layer resource utilization. Furthermore,
realistic link budgets for the RL of mobile SatComs are presented, thus
allowing the comparison of the systems in terms of achievable throughput. Since
the proposed systems operate under the same payload requirements as the
conventional systems, the comparison can be regarded as fair. Finally, existing
analytical formulas are also employed to provide closed form descriptions of
the performance of clustered multibeam MUD, thus introducing insights on how
the performance scales with respect to the system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5884</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5884</id><created>2012-11-26</created><authors><author><keyname>Sun</keyname><forenames>Cong</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Low complexity sum rate maximization for single and multiple stream MIMO
  AF relay networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiple-antenna amplify-and-forward two-hop interference network with
multiple links and multiple relays is considered. We optimize transmit
precoders, receive decoders and relay AF matrices to maximize the achievable
sum rate. Under per user and total relay sum power constraints, we propose an
efficient algorithm to maximize the total signal to total interference plus
noise ratio (TSTINR). Computational complexity analysis shows that our proposed
algorithm for TSTINR has lower complexity than the existing weighted minimum
mean square error (WMMSE) algorithm. We analyze and confirm by simulations that
the TSTINR, WMMSE and the total leakage interference plus noise (TLIN)
minimization models with per user and total relay sum power constraints can
only transmit a single data stream for each user. Thus we propose a novel
multiple stream TSTINR model with requirement of orthogonal columns for
precoders, in order to support multiple data streams and thus utilize higher
Degrees of Freedom. Multiple data streams and larger multiplexing gains are
guaranteed. Simulation results show that for single stream models, our TSTINR
algorithm outperforms the TLIN algorithm generally and outperforms WMMSE in
medium to high Signal-to-Noise-Ratio scenarios; the system sum rate
significantly benefits from multiple data streams in medium to high SNR
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5888</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5888</id><created>2012-11-26</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>User Scheduling for Coordinated Dual Satellite Systems with Linear
  Precoding</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, conference paper submitted to the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The constantly increasing demand for interactive broadband satellite
communications is driving current research to explore novel system
architectures that reuse frequency in a more aggressive manner. To this end,
the topic of dual satellite systems, in which satellites share spatial (i.e.
same coverage area) and spectral (i.e. full frequency reuse) degrees of freedom
is introduced. In each multibeam satellite, multiuser interferences are
mitigated by employing zero forcing precoding with realistic per antenna power
constraints. However, the two sets of users that the transmitters are
separately serving, interfere. The present contribution, proposes the partial
cooperation, namely coordination between the two coexisting transmitters in
order to reduce interferences and enhance the performance of the whole system,
while maintaining moderate system complexity. In this direction, a heuristic,
iterative, low complexity algorithm that allocates users in the two interfering
sets is proposed. This novel algorithm, improves the performance of each
satellite and of the overall system, simultaneously. The first is achieved by
maximizing the orthogonality between users allocated in the same set, hence
optimizing the zero forcing performance, whilst the second by minimizing the
level of interferences between the two sets. Simulation results show that the
proposed method, compared to conventional techniques, significantly increases
spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5890</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5890</id><created>2012-11-26</created><authors><author><keyname>Ostapov</keyname><forenames>Yuriy</forenames></author></authors><title>Adaptive Control of Enterprise</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern progress in artificial intelligence permits to realize algorithms of
adaptation for critical events (in addition to ERP). A production emergence, an
appearance of new competitive goods, a major change in financial state of
partners, a radical change in exchange rate, a change in custom and tax
legislation, a political and energy crisis, an ecocatastrophe can lead up to a
decrease of profit or bankruptcy of enterprise. Therefore it is necessary to
assess a probability of threat and to take preventive actions. If a critical
event took place, one must estimate restoration expenses and possible
consequences as well as to prepare appropriate propositions. This is provided
using modern methods of diagnostics, prediction, and decision making as well as
an inference engine and semantic analysis. Mathematical methods in use are
called in algorithms of adaptation automatically. Because the enterprise is a
complex system, to overcome complexity of control it is necessary to apply
semantic representations. Such representations are formed from descriptions of
events, facts, persons, organizations, goods, operations, scripts on a natural
language. Semantic representations permit as well to formulate actual problems
and to find ways to resolve these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5901</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5901</id><created>2012-11-26</created><authors><author><keyname>Singh</keyname><forenames>Sumeetpal S.</forenames></author><author><keyname>Chopin</keyname><forenames>Nicolas</forenames></author><author><keyname>Whiteley</keyname><forenames>Nick</forenames></author></authors><title>Bayesian learning of noisy Markov decision processes</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the inverse reinforcement learning problem, that is, the problem
of learning from, and then predicting or mimicking a controller based on
state/action data. We propose a statistical model for such data, derived from
the structure of a Markov decision process. Adopting a Bayesian approach to
inference, we show how latent variables of the model can be estimated, and how
predictions about actions can be made, in a unified framework. A new Markov
chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior
distribution. This step includes a parameter expansion step, which is shown to
be essential for good convergence properties of the MCMC sampler. As an
illustration, the method is applied to learning a human controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5903</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5903</id><created>2012-11-26</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Arnau</keyname><forenames>Jesus</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Mosquera</keyname><forenames>Carlos</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>MMSE Performance Analysis of Generalized Multibeam Satellite Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, submitted to the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggressive frequency reuse in the return link (RL) of multibeam satellite
communications (SatComs) is crucial towards the implementation of next
generation, interactive satellite services. In this direction, multiuser
detection has shown great potential in mitigating the increased intrasystem
interferences, induced by a tight spectrum reuse. Herein we present an analytic
framework to describe the linear Minimum Mean Square Error (MMSE) performance
of multiuser channels that exhibit full receive correlation: an inherent
attribute of the RL of multibeam SatComs. Analytic, tight approximations on the
MMSE performance are proposed for cases where closed form solutions are not
available in the existing literature. The proposed framework is generic, thus
providing a generalized solution straightforwardly extendable to various fading
models over channels that exhibit full receive correlation. Simulation results
are provided to show the tightness of the proposed approximation with respect
to the available transmit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5904</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5904</id><created>2012-11-26</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Application-tailored Linear Algebra Algorithms: A search-based Approach</title><categories>cs.MS cs.NA cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of automatically generating algorithms
for linear algebra operations by taking advantage of problem-specific
knowledge. In most situations, users possess much more information about the
problem at hand than what current libraries and computing environments accept;
evidence shows that if properly exploited, such information leads to
uncommon/unexpected speedups. We introduce a knowledge-aware linear algebra
compiler that allows users to input matrix equations together with properties
about the operands and the problem itself; for instance, they can specify that
the equation is part of a sequence, and how successive instances are related to
one another. The compiler exploits all this information to guide the generation
of algorithms, to limit the size of the search space, and to avoid redundant
computations. We applied the compiler to equations arising as part of
sensitivity and genome studies; the algorithms produced exhibit, respectively,
100- and 1000-fold speedups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5908</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5908</id><created>2012-11-26</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Maaser</keyname><forenames>Nicola</forenames></author><author><keyname>Napel</keyname><forenames>Stefan</forenames></author></authors><title>On the Egalitarian Weights of Nations</title><categories>cs.GT stat.AP</categories><comments>42 pages, 2 figures</comments><msc-class>91B12, 62P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voters from m disjoint constituencies (regions, federal states, etc.) are
represented in an assembly which contains one delegate from each constituency
and applies a weighted voting rule. All agents are assumed to have
single-peaked preferences over an interval; each delegate's preferences match
his constituency's median voter; and the collective decision equals the
assembly's Condorcet winner. We characterize the asymptotic behavior of the
probability of a given delegate determining the outcome (i.e., being the
weighted median of medians) in order to address a contentious practical
question: which voting weights w_1, ..., w_m ought to be selected if
constituency sizes differ and all voters are to have a priori equal influence
on collective decisions? It is shown that if ideal point distributions have
identical median M and are suitably continuous, the probability for a given
delegate i's ideal point \lambda_i being the Condorcet winner becomes
asymptotically proportional to i's voting weight w_i times \lambda_i's density
at M as $m\to \infty$. Indirect representation of citizens is approximately
egalitarian for weights proportional to the square root of constituency sizes
if all individual ideal points are i.i.d. In contrast, weights that are linear
in-- or, better, induce a Shapley value linear in-- size are egalitarian when
preferences are sufficiently strongly affiliated within constituencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5914</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5914</id><created>2012-11-26</created><updated>2013-09-20</updated><authors><author><keyname>Ricaud</keyname><forenames>Benjamin</forenames></author><author><keyname>Torresani</keyname><forenames>Bruno</forenames></author></authors><title>A survey of uncertainty principles and some signal processing
  applications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to review the main trends in the domain of
uncertainty principles and localization, emphasize their mutual connections and
investigate practical consequences. The discussion is strongly oriented
towards, and motivated by signal processing problems, from which significant
advances have been made recently. Relations with sparse approximation and
coding problems are emphasized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5931</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5931</id><created>2012-11-26</created><updated>2012-12-16</updated><authors><author><keyname>Zafar</keyname><forenames>Ammar</forenames></author><author><keyname>Radaydeh</keyname><forenames>Redha M.</forenames></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Power Allocation Strategies for Fixed-Gain Half-Duplex
  Amplify-and-Forward Relaying in Nakagami-m Fading</title><categories>cs.IT math.IT</categories><comments>8 figures, 28 pages. Journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study power allocation strategies for a fixed-gain
amplify-and-forward relay network employing multiple relays. We consider two
optimization problems for the relay network: 1) optimal power allocation to
maximize the end-to-end signal-to-noise ratio (SNR) and 2) minimizing the total
consumed power while maintaining the end-to-end SNR over a threshold value. We
investigate these two problems for two relaying protocols of all-participate
relaying and selective relaying and multiple cases of available channel state
information (CSI) at the relays. We show that the SNR maximization problem is
concave and the power minimization problem is convex for all protocols and CSI
cases considered. We obtain closed-form expressions for the two problems in the
case for full CSI and CSI of all the relay-destination links at the relays and
solve the problems through convex programming when full CSI or CSI of the
relay-destination links are not available at the relays. Numerical results show
the benefit of having full CSI at the relays for both optimization problems.
However, they also show that CSI overhead can be reduced by having only partial
CSI at the relays with only a small degradation in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5933</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5933</id><created>2012-11-26</created><updated>2014-05-06</updated><authors><author><keyname>Cao</keyname><forenames>Yixin</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Interval Deletion is Fixed-Parameter Tractable</title><categories>cs.DS cs.DM</categories><comments>Final version, to appear in ACM Transactions on Algorithms</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minimum \emph{interval deletion} problem, which asks for the
removal of a set of at most $k$ vertices to make a graph of $n$ vertices into
an interval graph. We present a parameterized algorithm of runtime $10^k \cdot
n^{O(1)}$ for this problem, that is, we show the problem is fixed-parameter
tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5937</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5937</id><created>2012-11-26</created><updated>2014-09-02</updated><authors><author><keyname>Wang</keyname><forenames>Zitao</forenames></author><author><keyname>Szeto</keyname><forenames>Kwok Yip</forenames></author></authors><title>Comparing the reliability of networks by spectral analysis</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>7 pages, 3 figures</comments><journal-ref>Eur. Phys. J. B (2014) 87: 234</journal-ref><doi>10.1140/epjb/e2014-50498-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a method for the ranking of the reliability of two networks with
the same connectance. Our method is based on the Cheeger constant linking the
topological property of a network with its spectrum. We first analyze a set of
twisted rings with the same connectance and degree distribution, and obtain the
ranking of their reliability using their eigenvalue gaps. The results are
generalized to general networks using the method of rewiring. The success of
our ranking method is verified numerically for the IEEE57, the
Erd\H{o}s-R\'enyi, and the Small-World networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5938</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5938</id><created>2012-11-26</created><updated>2013-04-08</updated><authors><author><keyname>Simon</keyname><forenames>Sunil</forenames></author><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Social Network Games</title><categories>cs.GT cs.SI</categories><comments>42 pages. To appear in the Journal of Logic and Computation. A
  preliminary version of this paper appeared as arXiv:1202.2209</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the natural objectives of the field of the social networks is to
predict agents' behaviour. To better understand the spread of various products
through a social network arXiv:1105.2434 introduced a threshold model, in which
the nodes influenced by their neighbours can adopt one out of several
alternatives. To analyze the consequences of such product adoption we associate
here with each such social network a natural strategic game between the agents.
  In these games the payoff of each player weakly increases when more players
choose his strategy, which is exactly opposite to the congestion games. The
possibility of not choosing any product results in two special types of (pure)
Nash equilibria.
  We show that such games may have no Nash equilibrium and that determining an
existence of a Nash equilibrium, also of a special type, is NP-complete. This
implies the same result for a more general class of games, namely polymatrix
games. The situation changes when the underlying graph of the social network is
a DAG, a simple cycle, or, more generally, has no source nodes. For these three
classes we determine the complexity of an existence of (a special type of) Nash
equilibria.
  We also clarify for these categories of games the status and the complexity
of the finite best response property (FBRP) and the finite improvement property
(FIP). Further, we introduce a new property of the uniform FIP which is
satisfied when the underlying graph is a simple cycle, but determining it is
co-NP-hard in the general case and also when the underlying graph has no source
nodes. The latter complexity results also hold for the property of being a
weakly acyclic game. A preliminary version of this paper appeared as [19].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5968</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5968</id><created>2012-11-26</created><updated>2015-09-09</updated><authors><author><keyname>Robert</keyname><forenames>Philippe</forenames></author><author><keyname>V&#xe9;ber</keyname><forenames>Amandine</forenames></author></authors><title>A stochastic analysis of resource sharing with logarithmic weights</title><categories>math.PR cs.NI</categories><comments>Published at http://dx.doi.org/10.1214/14-AAP1057 in the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP1057</report-no><journal-ref>Annals of Applied Probability 2015, Vol. 25, No. 5, 2626-2670</journal-ref><doi>10.1214/14-AAP1057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates the properties of a class of resource allocation
algorithms for communication networks: if a node of this network has $x$
requests to transmit, then it receives a fraction of the capacity proportional
to $\log(1+x)$, the logarithm of its current load. A detailed fluid scaling
analysis of such a network with two nodes is presented. It is shown that the
interaction of several time scales plays an important role in the evolution of
such a system, in particular its coordinates may live on very different time
and space scales. As a consequence, the associated stochastic processes turn
out to have unusual scaling behaviors. A heavy traffic limit theorem for the
invariant distribution is also proved. Finally, we present a generalization to
the resource sharing algorithm for which the $\log$ function is replaced by an
increasing function. Possible generalizations of these results with $J&gt;2$ nodes
or with the function $\log$ replaced by another slowly increasing function are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.5986</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.5986</id><created>2012-11-26</created><authors><author><keyname>Aguirre</keyname><forenames>Carlos</forenames></author><author><keyname>Mendes</keyname><forenames>R. Vilela</forenames></author></authors><title>Signal recognition and adapted filtering by non-commutative tomography</title><categories>physics.data-an cs.IR math.NA</categories><comments>19 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1107.0929</comments><journal-ref>IET Signal Processing 8 (2014) 67 - 75</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tomograms, a generalization of the Radon transform to arbitrary pairs of
non-commuting operators, are positive bilinear transforms with a rigorous
probabilistic interpretation which provide a full characterization of the
signal and are robust in the presence of noise. Tomograms based on the
time-frequency operator pair, were used in the past for component separation
and denoising. Here we show how, by the construction of an operator pair
adapted to the signal, meaningful information with good time resolution is
extracted even in very noisy situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6013</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6013</id><created>2012-11-26</created><updated>2013-07-13</updated><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Online Stochastic Optimization with Multiple Objectives</title><categories>cs.LG math.OC</categories><comments>NIPS Workshop on Optimization for Machine Learning</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we propose a general framework to characterize and solve the
stochastic optimization problems with multiple objectives underlying many real
world learning applications. We first propose a projection based algorithm
which attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on the
theory of Lagrangian in constrained optimization, we devise a novel primal-dual
stochastic approximation algorithm which attains the optimal convergence rate
of $O(T^{-1/2})$ for general Lipschitz continuous objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6014</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6014</id><created>2012-11-26</created><authors><author><keyname>Cs&#xe1;ji</keyname><forenames>Bal&#xe1;zs Cs.</forenames></author><author><keyname>Browet</keyname><forenames>Arnaud</forenames></author><author><keyname>Traag</keyname><forenames>V. A.</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Huens</keyname><forenames>Etienne</forenames></author><author><keyname>Van Dooren</keyname><forenames>Paul</forenames></author><author><keyname>Smoreda</keyname><forenames>Zbigniew</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author></authors><title>Exploring the Mobility of Mobile Phone Users</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 12 figures</comments><journal-ref>Physica A 392(6), pp. 1459-1473 (2013)</journal-ref><doi>10.1016/j.physa.2012.11.040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phone datasets allow for the analysis of human behavior on an
unprecedented scale. The social network, temporal dynamics and mobile behavior
of mobile phone users have often been analyzed independently from each other
using mobile phone datasets. In this article, we explore the connections
between various features of human behavior extracted from a large mobile phone
dataset. Our observations are based on the analysis of communication data of
100000 anonymized and randomly chosen individuals in a dataset of
communications in Portugal. We show that clustering and principal component
analysis allow for a significant dimension reduction with limited loss of
information. The most important features are related to geographical location.
In particular, we observe that most people spend most of their time at only a
few locations. With the help of clustering methods, we then robustly identify
home and office locations and compare the results with official census data.
Finally, we analyze the geographic spread of users' frequent locations and show
that commuting distances can be reasonably well explained by a gravity model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6020</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6020</id><created>2012-11-26</created><updated>2013-10-21</updated><authors><author><keyname>Garoufalidis</keyname><forenames>Stavros</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>Irreducibility of q-difference operators and the knot 7_4</title><categories>math.GT cs.SC math.CO</categories><comments>20 pages, 3 figures, 2 tables</comments><msc-class>57N10 (Primary) 57M25, 33F10, 39A13 (Secondary)</msc-class><journal-ref>Algebr. Geom. Topol. 13 (2013) 3261-3286</journal-ref><doi>10.2140/agt.2013.13.3261</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to compute the minimal-order recurrence of the colored Jones
polynomial of the 7_4 knot, as well as for the first four double twist knots.
As a corollary, we verify the AJ Conjecture for the simplest knot 7_4 with
reducible non-abelian SL(2,C) character variety. To achieve our goal, we use
symbolic summation techniques of Zeilberger's holonomic systems approach and an
irreducibility criterion for q-difference operators. For the latter we use an
improved version of the qHyper algorithm of Abramov-Paule-Petkovsek to show
that a given q-difference operator has no linear right factors. En route, we
introduce exterior power Adams operations on the ring of bivariate polynomials
and on the corresponding affine curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6024</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6024</id><created>2012-11-26</created><updated>2014-01-28</updated><authors><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Chamberland</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Huff</keyname><forenames>Gregory H.</forenames></author></authors><title>Reconfigurable Antennas, Preemptive Switching and Virtual Channel
  Management</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><journal-ref>IEEE Transactions on Communications, ISSN 0090-6778, Vol. 62, No.
  4, pp. 1272-1282, April 2014</journal-ref><doi>10.1109/TCOMM.2014.020514.130592</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the performance of wireless communication systems that
utilize reconfigurable or pattern-dynamic antennas. The focus is on
finite-state channels with memory and performance is assessed in terms of
real-time behavior. In a wireless setting, when a slow fading channel enters a
deep fade, the corresponding communication system faces the threat of
successive decoding failures at the destination. Under such circumstances,
rapidly getting out of deep fades becomes a priority. Recent advances in fast
reconfigurable antennas provide new means to alter the statistical profile of
fading channels and thereby reduce the probability of prolonged fades. Fast
reconfigurable antennas are therefore poised to improve overall performance,
especially for delay-sensitive traffic in slow-fading environments. This
potential for enhanced performance motivates this study of the temporal
behavior of point-to-point communication systems with reconfigurable antennas.
Specifically, agile wireless communication schemes over erasure channels are
analyzed; situations where using reconfigurable antennas yield substantial
performance gains in terms of throughput and average delay are identified.
Scenarios where only partial state information is available at the receiver are
also examined, naturally leading to partially observable decision processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6039</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6039</id><created>2012-11-26</created><updated>2012-11-27</updated><authors><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Rendezvous of two robots with visible bits</title><categories>cs.MA cs.CG cs.RO</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the rendezvous problem for two robots moving in the plane (or on a
line). Robots are autonomous, anonymous, oblivious, and carry colored lights
that are visible to both. We consider deterministic distributed algorithms in
which robots do not use distance information, but try to reduce (or increase)
their distance by a constant factor, depending on their lights' colors.
  We give a complete characterization of the number of colors that are
necessary to solve the rendezvous problem in every possible model, ranging from
fully synchronous to semi-synchronous to asynchronous, rigid and non-rigid,
with preset or arbitrary initial configuration.
  In particular, we show that three colors are sufficient in the non-rigid
asynchronous model with arbitrary initial configuration. In contrast, two
colors are insufficient in the rigid asynchronous model with arbitrary initial
configuration and in the non-rigid asynchronous model with preset initial
configuration.
  Additionally, if the robots are able to distinguish between zero and non-zero
distances, we show how they can solve rendezvous and detect termination using
only three colors, even in the non-rigid asynchronous model with arbitrary
initial configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6048</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6048</id><created>2012-11-26</created><updated>2013-10-19</updated><authors><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Pfander</keyname><forenames>G&#xf6;tz</forenames></author></authors><title>Local sampling and approximation of operators with bandlimited
  Kohn-Nirenberg symbols</title><categories>math.FA cs.IT math.CA math.IT</categories><comments>22 pages</comments><msc-class>41A35, 94A20 (Primary) 42B35, 47B35, 47G30, 94A20 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent sampling theorems allow for the recovery of operators with bandlimited
Kohn-Nirenberg symbols from their response to a single discretely supported
identifier signal. The available results are inherently non-local. For example,
we show that in order to recover a bandlimited operator precisely, the
identifier cannot decay in time nor in frequency. Moreover, a concept of local
and discrete representation is missing from the theory. In this paper, we
develop tools that address these shortcomings.
  We show that to obtain a local approximation of an operator, it is sufficient
to test the operator on a truncated and mollified delta train, that is, on a
compactly supported Schwarz class function. To compute the operator
numerically, discrete measurements can be obtained from the response function
which are localized in the sense that a local selection of the values yields a
local approximation of the operator.
  Central to our analysis is to conceptualize the meaning of localization for
operators with bandlimited Kohn-Nirenberg symbol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6080</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6080</id><created>2012-11-26</created><authors><author><keyname>Rei&#xdf;ig</keyname><forenames>Gunther</forenames></author></authors><title>Convexity of reachable sets of nonlinear ordinary differential equations</title><categories>math.OC cs.SY</categories><comments>Accepted version</comments><msc-class>93B03 (Primary), 52A05, 93C10, 93C15 (Secondary)</msc-class><journal-ref>Automation and Remote Control, vol. 68, no. 9, 2007, pp. 1527-1543</journal-ref><doi>10.1134/S000511790709007X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a necessary and sufficient condition for the reachable set, i.e.,
the set of states reachable from a ball of initial states at some time, of an
ordinary differential equation to be convex. In particular, convexity is
guaranteed if the ball of initial states is sufficiently small, and we provide
an upper bound on the radius of that ball, which can be directly obtained from
the right hand side of the differential equation. In finite dimensions, our
results cover the case of ellipsoids of initial states. A potential application
of our results is inner and outer polyhedral approximation of reachable sets,
which becomes extremely simple and almost universally applicable if these sets
are known to be convex. We demonstrate by means of an example that the balls of
initial states for which the latter property follows from our results are large
enough to be used in actual computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6085</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6085</id><created>2012-11-26</created><updated>2014-04-17</updated><authors><author><keyname>Paul</keyname><forenames>Saurabh</forenames></author><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>Random Projections for Linear Support Vector Machines</title><categories>cs.LG stat.ML</categories><comments>To appear in ACM TKDD, 2014. Shorter version appeared at AISTATS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let X be a data matrix of rank \rho, whose rows represent n points in
d-dimensional space. The linear support vector machine constructs a hyperplane
separator that maximizes the 1-norm soft margin. We develop a new oblivious
dimension reduction technique which is precomputed and can be applied to any
input matrix X. We prove that, with high probability, the margin and minimum
enclosing ball in the feature space are preserved to within \epsilon-relative
error, ensuring comparable generalization as in the original space in the case
of classification. For regression, we show that the margin is preserved to
\epsilon-relative error with high probability. We present extensive experiments
with real and synthetic data to support our theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6086</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6086</id><created>2012-11-26</created><updated>2012-12-05</updated><authors><author><keyname>Zhao</keyname><forenames>Kang</forenames></author><author><keyname>Greer</keyname><forenames>Greta</forenames></author><author><keyname>Qiu</keyname><forenames>Baojun</forenames></author><author><keyname>Mitra</keyname><forenames>Prasenjit</forenames></author><author><keyname>Portier</keyname><forenames>Kenneth</forenames></author><author><keyname>Yen</keyname><forenames>John</forenames></author></authors><title>Finding influential users of an online health community: a new metric
  based on sentiment influence</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Working paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What characterizes influential users in online health communities (OHCs)? We
hypothesize that (1) the emotional support received by OHC members can be
assessed from their sentiment ex-pressed in online interactions, and (2) such
assessments can help to identify influential OHC members. Through text mining
and sentiment analysis of users' online interactions, we propose a novel metric
that directly measures a user's ability to affect the sentiment of others.
Using dataset from an OHC, we demonstrate that this metric is highly effective
in identifying influential users. In addition, combining the metric with other
traditional measures further improves the identification of influential users.
This study can facilitate online community management and advance our
understanding of social influence in OHCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6089</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6089</id><created>2012-11-26</created><authors><author><keyname>Karavelas</keyname><forenames>Menelaos I.</forenames></author><author><keyname>Konaxis</keyname><forenames>Christos</forenames></author><author><keyname>Tzanaki</keyname><forenames>Eleni</forenames></author></authors><title>The maximum number of faces of the Minkowski sum of three convex
  polytopes</title><categories>cs.CG math.CO</categories><comments>44 pages, 3 figures</comments><msc-class>52B05, 52B11, 52C45, 68U05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive tight expressions for the maximum number of $k$-faces, $0\le k\le
d-1$, of the Minkowski sum, $P_1+P_2+P_3$, of three $d$-dimensional convex
polytopes $P_1$, $P_2$ and $P_3$, as a function of the number of vertices of
the polytopes, for any $d\ge 2$. Expressing the Minkowski sum of the three
polytopes as a section of their Cayley polytope $\mathcal{C}$, the problem of
counting the number of $k$-faces of $P_1+P_2+P_3$, reduces to counting the
number of $(k+2)$-faces of the subset of $\mathcal{C}$ comprising of the faces
that contain at least one vertex from each $P_i$. In two dimensions our
expressions reduce to known results, while in three dimensions, the tightness
of our bounds follows by exploiting known tight bounds for the number of faces
of $r$ $d$-polytopes, where $r\ge d$. For $d\ge 4$, the maximum values are
attained when $P_1$, $P_2$ and $P_3$ are $d$-polytopes, whose vertex sets are
chosen appropriately from three distinct $d$-dimensional moment-like curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6097</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6097</id><created>2012-11-23</created><authors><author><keyname>Boloni</keyname><forenames>Ladislau</forenames></author></authors><title>Shadows and Headless Shadows: an Autobiographical Approach to Narrative
  Reasoning</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.5643</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Xapagy architecture is a story-oriented cognitive system which relies
exclusively on the autobiographical memory implemented as a raw collection of
events. Reasoning is performed by shadowing current events with events from the
autobiography. The shadows are then extrapolated into headless shadows (HLSs).
In a story following mood, HLSs can be used to track the level of surprise of
the agent, to infer hidden actions or relations between the participants, and
to summarize ongoing events. In recall mood, the HLSs can be used to create new
stories ranging from exact recall to free-form confabulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6101</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6101</id><created>2012-11-25</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Design of Calibration Experiments for Identification of Manipulator
  Elastostatic Parameters</title><categories>cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.5739</comments><proxy>ccsd</proxy><journal-ref>Journal of Mechanics Engineering and Automation 2 (2012) 531-542</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the elastostatic calibration of industrial robots,
which is used for precise machining of large-dimensional parts made of
composite materials. In this technological process, the interaction between the
robot and the workpiece causes essential elastic deflections of the manipulator
components that should be compensated by the robot controller using relevant
elastostatic model of this mechanism. To estimate parameters of this model, an
advanced calibration technique is applied that is based on the non-linear
experiment design theory, which is adopted for this particular application. In
contrast to previous works, it is proposed a concept of the user-defined
test-pose, which is used to evaluate the calibration experiments quality. In
the frame of this concept, the related optimization problem is defined and
numerical routines are developed, which allow generating optimal set of
manipulator configurations and corresponding forces/torques for a given number
of the calibration experiments. Some specific kinematic constraints are also
taken into account, which insure feasibility of calibration experiments for the
obtained configurations and allow avoiding collision between the robotic
manipulator and the measurement equipment. The efficiency of the developed
technique is illustrated by an application example that deals with elastostatic
calibration of the serial manipulator used for robot-based machining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6120</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6120</id><created>2012-11-26</created><updated>2013-09-06</updated><authors><author><keyname>Hayden</keyname><forenames>Patrick</forenames></author><author><keyname>Milner</keyname><forenames>Kevin</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Two-message quantum interactive proofs and the quantum separability
  problem</title><categories>quant-ph cs.CC</categories><comments>34 pages, 6 figures; v2: technical improvements and new result for
  the multipartite quantum separability problem; v3: minor changes to address
  referee comments, accepted for presentation at the 2013 IEEE Conference on
  Computational Complexity; v4: changed problem names; v5: updated references
  and added a paragraph to the conclusion to connect with prior work on
  separability testing</comments><journal-ref>Proceedings of the 28th IEEE Conference on Computational
  Complexity, pages 156-167, Palo Alto, California, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a polynomial-time mixed-state quantum circuit, described as a
sequence of local unitary interactions followed by a partial trace, generates a
quantum state shared between two parties. One might then wonder, does this
quantum circuit produce a state that is separable or entangled? Here, we give
evidence that it is computationally hard to decide the answer to this question,
even if one has access to the power of quantum computation. We begin by
exhibiting a two-message quantum interactive proof system that can decide the
answer to a promise version of the question. We then prove that the promise
problem is hard for the class of promise problems with &quot;quantum statistical
zero knowledge&quot; (QSZK) proof systems by demonstrating a polynomial-time Karp
reduction from the QSZK-complete promise problem &quot;quantum state
distinguishability&quot; to our quantum separability problem. By exploiting Knill's
efficient encoding of a matrix description of a state into a description of a
circuit to generate the state, we can show that our promise problem is NP-hard
with respect to Cook reductions. Thus, the quantum separability problem (as
phrased above) constitutes the first nontrivial promise problem decidable by a
two-message quantum interactive proof system while being hard for both NP and
QSZK. We also consider a variant of the problem, in which a given
polynomial-time mixed-state quantum circuit accepts a quantum state as input,
and the question is to decide if there is an input to this circuit which makes
its output separable across some bipartite cut. We prove that this problem is a
complete promise problem for the class QIP of problems decidable by quantum
interactive proof systems. Finally, we show that a two-message quantum
interactive proof system can also decide a multipartite generalization of the
quantum separability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6158</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6158</id><created>2012-11-26</created><authors><author><keyname>Saha</keyname><forenames>Ankan</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>The Interplay Between Stability and Regret in Online Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the stability of online learning algorithms and its
implications for learnability (bounded regret). We introduce a novel quantity
called {\em forward regret} that intuitively measures how good an online
learning algorithm is if it is allowed a one-step look-ahead into the future.
We show that given stability, bounded forward regret is equivalent to bounded
regret. We also show that the existence of an algorithm with bounded regret
implies the existence of a stable algorithm with bounded regret and bounded
forward regret. The equivalence results apply to general, possibly non-convex
problems. To the best of our knowledge, our analysis provides the first general
connection between stability and regret in the online setting that is not
restricted to a particular class of algorithms. Our stability-regret connection
provides a simple recipe for analyzing regret incurred by any online learning
algorithm. Using our framework, we analyze several existing online learning
algorithms as well as the &quot;approximate&quot; versions of algorithms like RDA that
solve an optimization problem at each iteration. Our proofs are simpler than
existing analysis for the respective algorithms, show a clear trade-off between
stability and forward regret, and provide tighter regret bounds in some cases.
Furthermore, using our recipe, we analyze &quot;approximate&quot; versions of several
algorithms such as follow-the-regularized-leader (FTRL) that requires solving
an optimization problem at each step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6159</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6159</id><created>2012-11-26</created><authors><author><keyname>Rojas</keyname><forenames>Manuel</forenames></author></authors><title>A semantic association page rank algorithm for web search engines</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of Semantic Web search engines retrieve information by focusing
on the use of concepts and relations restricted to the query provided by the
user. By trying to guess the implicit meaning between these concepts and
relations, probabilities are calculated to give the pages a score for ranking.
In this study, I propose a relation-based page rank algorithm to be used as a
Semantic Web search engine. Relevance is measured as the probability of finding
the connections made by the user at the time of the query, as well as the
information contained in the base knowledge of the Semantic Web environment. By
the use of &quot;virtual links&quot; between the concepts in a page, which are obtained
from the knowledge base, we can connect concepts and components of a page and
increase the probability score for a better ranking. By creating these
connections, this study also looks to eliminate the possibility of getting
results equal to zero, and to provide a tie-breaker solution when two or more
pages obtain the same score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6166</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6166</id><created>2012-11-26</created><authors><author><keyname>Zhu</keyname><forenames>Tao</forenames></author><author><keyname>Phipps</keyname><forenames>David</forenames></author><author><keyname>Pridgen</keyname><forenames>Adam</forenames></author><author><keyname>Crandall</keyname><forenames>Jedidiah R.</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>Tracking and Quantifying Censorship on a Chinese Microblogging Site</title><categories>cs.IR cs.CR</categories><acm-class>H.3.3; H.3.5; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present measurements and analysis of censorship on Weibo, a popular
microblogging site in China. Since we were limited in the rate at which we
could download posts, we identified users likely to participate in sensitive
topics and recursively followed their social contacts. We also leveraged new
natural language processing techniques to pick out trending topics despite the
use of neologisms, named entities, and informal language usage in Chinese
social media. We found that Weibo dynamically adapts to the changing interests
of its users through multiple layers of filtering. The filtering includes both
retroactively searching posts by keyword or repost links to delete them, and
rejecting posts as they are posted. The trend of sensitive topics is
short-lived, suggesting that the censorship is effective in stopping the
&quot;viral&quot; spread of sensitive issues. We also give evidence that sensitive topics
in Weibo only scarcely propagate beyond a core of sensitive posters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6170</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6170</id><created>2012-11-26</created><authors><author><keyname>Cockett</keyname><forenames>Robin</forenames></author><author><keyname>Garner</keyname><forenames>Richard</forenames></author></authors><title>Restriction categories as enriched categories</title><categories>math.CT cs.LO</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restriction categories were introduced to provide an axiomatic setting for
the study of partially defined mappings; they are categories equipped with an
operation called restriction which assigns to every morphism an endomorphism of
its domain, to be thought of as the partial identity that is defined to just
the same degree as the original map. In this paper, we show that restriction
categories can be identified with \emph{enriched categories} in the sense of
Kelly for a suitable enrichment base. By varying that base appropriately, we
are also able to capture the notions of join and range restriction category in
terms of enriched category theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6176</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6176</id><created>2012-11-26</created><authors><author><keyname>Xin</keyname><forenames>Reynold</forenames></author><author><keyname>Rosen</keyname><forenames>Josh</forenames></author><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Shenker</keyname><forenames>Scott</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>Shark: SQL and Rich Analytics at Scale</title><categories>cs.DB</categories><report-no>UCB/EECS-2012-214</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shark is a new data analysis system that marries query processing with
complex analytics on large clusters. It leverages a novel distributed memory
abstraction to provide a unified engine that can run SQL queries and
sophisticated analytics functions (e.g., iterative machine learning) at scale,
and efficiently recovers from failures mid-query. This allows Shark to run SQL
queries up to 100x faster than Apache Hive, and machine learning programs up to
100x faster than Hadoop. Unlike previous systems, Shark shows that it is
possible to achieve these speedups while retaining a MapReduce-like execution
engine, and the fine-grained fault tolerance properties that such engines
provide. It extends such an engine in several ways, including column-oriented
in-memory storage and dynamic mid-query replanning, to effectively execute SQL.
The result is a system that matches the speedups reported for MPP analytic
databases over MapReduce, while offering fault tolerance properties and complex
analytics capabilities that they lack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6181</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6181</id><created>2012-11-26</created><updated>2014-02-05</updated><authors><author><keyname>Travers</keyname><forenames>Nicholas F.</forenames></author></authors><title>Exponential Bounds for Convergence of Entropy Rate Approximations in
  Hidden Markov Models Satisfying a Path-Mergeability Condition</title><categories>math.PR cs.IT math.IT</categories><comments>23 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hidden Markov model (HMM) is said to have path-mergeable states if for any
two states i,j there exists a word w and state k such that it is possible to
transition from both i and j to k while emitting w. We show that for a finite
HMM with path-mergeable states the block estimates of the entropy rate converge
exponentially fast. We also show that the path-mergeability property is
asymptotically typical in the space of HMM topolgies and easily testable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6182</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6182</id><created>2012-11-26</created><authors><author><keyname>Blanca</keyname><forenames>Antonio</forenames></author><author><keyname>Galvin</keyname><forenames>David</forenames></author><author><keyname>Tetali</keyname><forenames>Dana Randall ans Prasad</forenames></author></authors><title>Phase Coexistence and Slow Mixing for the Hard-Core Model on Z^2</title><categories>math.CO cs.DM math-ph math.MP</categories><comments>23 pages</comments><msc-class>82B20, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the hard-core model on a finite graph we are given a parameter lambda&gt;0,
and an independent set I arises with probability proportional to lambda^|I|. On
infinite graphs a Gibbs distribution is defined as a suitable limit with the
correct conditional probabilities. In the infinite setting we are interested in
determining when this limit is unique and when there is phase coexistence,
i.e., existence of multiple Gibbs states. On finite graphs we are interested in
determining the mixing time of local Markov chains.
  On Z^2 it is conjectured that these problems are related and that both
undergo a phase transition at some critical point lambda_c approx 3.79. For
phase coexistence, much of the work to date has focused on the regime of
uniqueness, with the best result being recent work of Restrepo et al. showing
that there is a unique Gibbs state for all lambda &lt; 2.3882. Here we give the
first non-trivial result in the other direction, showing that there are
multiple Gibbs states for all lambda &gt; 5.3646. Our proof adds two significant
innovations to the standard Peierls argument. First, building on the idea of
fault lines introduced by Randall, we construct an event that distinguishes two
boundary conditions and always has long contours associated with it, obviating
the need to accurately enumerate short contours. Second, we obtain vastly
improved bounds on the number of contours by relating them to a new class of
self-avoiding walks on an oriented version of Z^2.
  We extend our characterization of fault lines to show that local Markov
chains will mix slowly when lambda &gt; 5.3646 on lattice regions with periodic
(toroidal) boundary conditions and when lambda &gt; 7.1031 with non-periodic
(free) boundary conditions. The arguments here rely on a careful analysis that
relates contours to taxi walks and represent a sevenfold improvement to the
previously best known values of \lambda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6185</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6185</id><created>2012-11-26</created><authors><author><keyname>Amani</keyname><forenames>Sidney</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Chubb</keyname><forenames>Peter</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Donaldson</keyname><forenames>Alastair F.</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Legg</keyname><forenames>Alexander</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Ryzhyk</keyname><forenames>Leonid</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Zhu</keyname><forenames>Yanjin</forenames><affiliation>NICTA and UNSW</affiliation></author></authors><title>Automatic Verification of Message-Based Device Drivers</title><categories>cs.OS cs.SE</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><acm-class>D.4.4; B.4.2; D.2.4</acm-class><journal-ref>EPTCS 102, 2012, pp. 4-17</journal-ref><doi>10.4204/EPTCS.102.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a practical solution to the problem of automatic verification of
the interface between device drivers and the OS. Our solution relies on a
combination of improved driver architecture and verification tools. It supports
drivers written in C and can be implemented in any existing OS, which sets it
apart from previous proposals for verification-friendly drivers. Our
Linux-based evaluation shows that this methodology amplifies the power of
existing verification tools in detecting driver bugs, making it possible to
verify properties beyond the reach of traditional techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6186</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6186</id><created>2012-11-26</created><authors><author><keyname>Baumann</keyname><forenames>Christoph</forenames><affiliation>Saarland University, Saarbr&#xfc;cken, Germany</affiliation></author><author><keyname>Beckert</keyname><forenames>Bernhard</forenames><affiliation>Karlsruhe Institute of Technology, Karlsruhe, Germany</affiliation></author><author><keyname>Blasum</keyname><forenames>Holger</forenames><affiliation>SYSGO AG, Klein-Winternheim, Germany</affiliation></author><author><keyname>Bormer</keyname><forenames>Thorsten</forenames><affiliation>Karlsruhe Institute of Technology, Karlsruhe, Germany</affiliation></author></authors><title>Lessons Learned From Microkernel Verification -- Specification is the
  New Bottleneck</title><categories>cs.SE cs.LO</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 18-32</journal-ref><doi>10.4204/EPTCS.102.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software verification tools have become a lot more powerful in recent years.
Even verification of large, complex systems is feasible, as demonstrated in the
L4.verified and Verisoft XT projects. Still, functional verification of large
software systems is rare - for reasons beyond the large scale of verification
effort needed due to the size alone. In this paper we report on lessons learned
for verification of large software systems based on the experience gained in
microkernel verification in the Verisoft XT project. We discuss a number of
issues that impede widespread introduction of formal verification in the
software life-cycle process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6187</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6187</id><created>2012-11-26</created><authors><author><keyname>Ernst</keyname><forenames>Gidon</forenames><affiliation>University of Augsburg</affiliation></author><author><keyname>Schellhorn</keyname><forenames>Gerhard</forenames><affiliation>University of Augsburg</affiliation></author><author><keyname>Haneberg</keyname><forenames>Dominik</forenames><affiliation>University of Augsburg</affiliation></author><author><keyname>Pf&#xe4;hler</keyname><forenames>J&#xf6;rg</forenames><affiliation>University of Augsburg</affiliation></author><author><keyname>Reif</keyname><forenames>Wolfgang</forenames><affiliation>University of Augsburg</affiliation></author></authors><title>A Formal Model of a Virtual Filesystem Switch</title><categories>cs.LO cs.OS cs.SE</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 33-45</journal-ref><doi>10.4204/EPTCS.102.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a formal model that is part of our effort to construct a
verified file system for Flash memory. To modularize the verification we factor
out generic aspects into a common component that is inspired by the Linux
Virtual Filesystem Switch (VFS) and provides POSIX compatible operations. It
relies on an abstract specification of its internal interface to concrete file
system implementations (AFS). We proved that preconditions of AFS are respected
and that the state is kept consistent. The model can be made executable and
mounted into the Linux directory tree using FUSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6188</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6188</id><created>2012-11-26</created><authors><author><keyname>Matichuk</keyname><forenames>Daniel</forenames><affiliation>NICTA</affiliation></author></authors><title>Automatic Function Annotations for Hoare Logic</title><categories>cs.LO</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><acm-class>D.2.4;F.3.1</acm-class><journal-ref>EPTCS 102, 2012, pp. 46-56</journal-ref><doi>10.4204/EPTCS.102.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In systems verification we are often concerned with multiple, inter-dependent
properties that a program must satisfy. To prove that a program satisfies a
given property, the correctness of intermediate states of the program must be
characterized. However, this intermediate reasoning is not always phrased such
that it can be easily re-used in the proofs of subsequent properties. We
introduce a function annotation logic that extends Hoare logic in two important
ways: (1) when proving that a function satisfies a Hoare triple, intermediate
reasoning is automatically stored as function annotations, and (2) these
function annotations can be exploited in future Hoare logic proofs. This
reduces duplication of reasoning between the proofs of different properties,
whilst serving as a drop-in replacement for traditional Hoare logic to avoid
the costly process of proof refactoring. We explain how this was implemented in
Isabelle/HOL and applied to an experimental branch of the seL4 microkernel to
significantly reduce the size and complexity of existing proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6189</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6189</id><created>2012-11-26</created><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames><affiliation>Fortiss GmbH</affiliation></author><author><keyname>Yan</keyname><forenames>Rongjie</forenames><affiliation>ISCAS</affiliation></author><author><keyname>Bensalem</keyname><forenames>Saddek</forenames><affiliation>Verimag</affiliation></author><author><keyname>Ruess</keyname><forenames>Harald</forenames><affiliation>Fortiss GmbH</affiliation></author></authors><title>Distributed Priority Synthesis</title><categories>cs.SY cs.LO</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 57-72</journal-ref><doi>10.4204/EPTCS.102.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of interacting components with non-deterministic variable update
and given safety requirements, the goal of priority synthesis is to restrict,
by means of priorities, the set of possible interactions in such a way as to
guarantee the given safety conditions for all possible runs. In distributed
priority synthesis we are interested in obtaining local sets of priorities,
which are deployed in terms of local component controllers sharing intended
next moves between components in local neighborhoods only. These possible
communication paths between local controllers are specified by means of a
communication architecture. We formally define the problem of distributed
priority synthesis in terms of a multi-player safety game between players for
(angelically) selecting the next transition of the components and an
environment for (demonically) updating uncontrollable variables. We analyze the
complexity of the problem, and propose several optimizations including a
solution-space exploration based on a diagnosis method using a nested extension
of the usual attractor computation in games together with a reduction to
corresponding SAT problems. When diagnosis fails, the method proposes potential
candidates to guide the exploration. These optimized algorithms for solving
distributed priority synthesis problems have been integrated into the VissBIP
framework. An experimental validation of this implementation is performed using
a range of case studies including scheduling in multicore processors and
modular robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6190</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6190</id><created>2012-11-26</created><authors><author><keyname>Tews</keyname><forenames>Hendrik</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>V&#xf6;lp</keyname><forenames>Marcus</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>Weber</keyname><forenames>Tjark</forenames><affiliation>Uppsala University</affiliation></author></authors><title>On the Use of Underspecified Data-Type Semantics for Type Safety in
  Low-Level Code</title><categories>cs.LO cs.OS cs.PL</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 73-87</journal-ref><doi>10.4204/EPTCS.102.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent projects on operating-system verification, C and C++ data types are
often formalized using a semantics that does not fully specify the precise byte
encoding of objects. It is well-known that such an underspecified data-type
semantics can be used to detect certain kinds of type errors. In general,
however, underspecified data-type semantics are unsound: they assign
well-defined meaning to programs that have undefined behavior according to the
C and C++ language standards.
  A precise characterization of the type-correctness properties that can be
enforced with underspecified data-type semantics is still missing. In this
paper, we identify strengths and weaknesses of underspecified data-type
semantics for ensuring type safety of low-level systems code. We prove
sufficient conditions to detect certain classes of type errors and, finally,
identify a trade-off between the complexity of underspecified data-type
semantics and their type-checking capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6191</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6191</id><created>2012-11-26</created><authors><author><keyname>Mangels</keyname><forenames>Tatiana</forenames></author><author><keyname>Peleska</keyname><forenames>Jan</forenames></author></authors><title>CTGEN - a Unit Test Generator for C</title><categories>cs.SE</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 88-102</journal-ref><doi>10.4204/EPTCS.102.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new unit test generator for C code, CTGEN. It generates test
data for C1 structural coverage and functional coverage based on
pre-/post-condition specifications or internal assertions. The generator
supports automated stub generation, and data to be returned by the stub to the
unit under test (UUT) may be specified by means of constraints. The typical
application field for CTGEN is embedded systems testing; therefore the tool can
cope with the typical aliasing problems present in low-level C, including
pointer arithmetics, structures and unions. CTGEN creates complete test
procedures which are ready to be compiled and run against the UUT. In this
paper we describe the main features of CTGEN, their technical realisation, and
we elaborate on its performance in comparison to a list of competing test
generation tools. Since 2011, CTGEN is used in industrial scale test campaigns
for embedded systems code in the automotive domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6192</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6192</id><created>2012-11-26</created><authors><author><keyname>Beckschulze</keyname><forenames>Eva</forenames><affiliation>Embedded Software Laboratory RWTH Aachen University, Germany</affiliation></author><author><keyname>Biallas</keyname><forenames>Sebastian</forenames><affiliation>Embedded Software Laboratory RWTH Aachen University, Germany</affiliation></author><author><keyname>Kowalewski</keyname><forenames>Stefan</forenames><affiliation>Embedded Software Laboratory RWTH Aachen University, Germany</affiliation></author></authors><title>Static Analysis of Lockless Microcontroller C Programs</title><categories>cs.PL cs.AR</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 103-114</journal-ref><doi>10.4204/EPTCS.102.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrently accessing shared data without locking is usually a subject to
race conditions resulting in inconsistent or corrupted data. However, there are
programs operating correctly without locking by exploiting the atomicity of
certain operations on a specific hardware. In this paper, we describe how to
precisely analyze lockless microcontroller C programs with interrupts by taking
the hardware architecture into account. We evaluate this technique in an
octagon-based value range analysis using access-based localization to increase
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6193</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6193</id><created>2012-11-26</created><authors><author><keyname>Hathhorn</keyname><forenames>Chris</forenames><affiliation>University of Missouri</affiliation></author><author><keyname>Becchi</keyname><forenames>Michela</forenames><affiliation>University of Missouri</affiliation></author><author><keyname>Harrison</keyname><forenames>William L.</forenames><affiliation>University of Missouri</affiliation></author><author><keyname>Procter</keyname><forenames>Adam</forenames><affiliation>University of Missouri</affiliation></author></authors><title>Formal Semantics of Heterogeneous CUDA-C: A Modular Approach with
  Applications</title><categories>cs.PL</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 115-124</journal-ref><doi>10.4204/EPTCS.102.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend an off-the-shelf, executable formal semantics of C (Ellison and
Rosu's K Framework semantics) with the core features of CUDA-C. The hybrid
CPU/GPU computation model of CUDA-C presents challenges not just for
programmers, but also for practitioners of formal methods. Our formal semantics
helps expose and clarify these issues. We demonstrate the usefulness of our
semantics by generating a tool from it capable of detecting some race
conditions and deadlocks in CUDA-C programs. We discuss limitations of our
model and argue that its extensibility can easily enable a wider range of
verification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6194</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6194</id><created>2012-11-26</created><authors><author><keyname>David</keyname><forenames>Alexandre</forenames></author><author><keyname>Jacobsen</keyname><forenames>Lasse</forenames></author><author><keyname>Jacobsen</keyname><forenames>Morten</forenames></author><author><keyname>Srba</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>A Forward Reachability Algorithm for Bounded Timed-Arc Petri Nets</title><categories>cs.LO cs.DS</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.4.7</acm-class><journal-ref>EPTCS 102, 2012, pp. 125-140</journal-ref><doi>10.4204/EPTCS.102.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timed-arc Petri nets (TAPN) are a well-known time extension of the Petri net
model and several translations to networks of timed automata have been proposed
for this model. We present a direct, DBM-based algorithm for forward
reachability analysis of bounded TAPNs extended with transport arcs, inhibitor
arcs and age invariants. We also give a complete proof of its correctness,
including reduction techniques based on symmetries and extrapolation. Finally,
we augment the algorithm with a novel state-space reduction technique
introducing a monotonic ordering on markings and prove its soundness even in
the presence of monotonicity-breaking features like age invariants and
inhibitor arcs. We implement the algorithm within the model-checker TAPAAL and
the experimental results document an encouraging performance compared to
verification approaches that translate TAPN models to UPPAAL timed automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6195</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6195</id><created>2012-11-26</created><authors><author><keyname>J&#xf8;rgensen</keyname><forenames>Kenneth Y.</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Srba</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Time-Darts: A Data Structure for Verification of Closed Timed Automata</title><categories>cs.DS cs.LO</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.4.7</acm-class><journal-ref>EPTCS 102, 2012, pp. 141-155</journal-ref><doi>10.4204/EPTCS.102.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic data structures for model checking timed systems have been subject
to a significant research, with Difference Bound Matrices (DBMs) still being
the preferred data structure in several mature verification tools. In
comparison, discretization offers an easy alternative, with all operations
having linear-time complexity in the number of clocks, and yet valid for a
large class of closed systems. Unfortunately, fine-grained discretization
causes itself a state-space explosion. We introduce a new data structure called
time-darts for the symbolic representation of state-spaces of timed automata.
Compared with the complete discretization, a single time-dart allows to
represent an arbitrary large set of states, yet the time complexity of
operations on time-darts remain linear in the number of clocks. We prove the
correctness of the suggested reachability algorithm and perform several
experiments in order to compare the performance of time-darts and the complete
discretization. The main conclusion is that in all our experiments the
time-dart method outperforms the complete discretization and it scales
significantly better for models with larger constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6196</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6196</id><created>2012-11-26</created><authors><author><keyname>Baier</keyname><forenames>Christel</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>Daum</keyname><forenames>Marcus</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>Engel</keyname><forenames>Benjamin</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>H&#xe4;rtig</keyname><forenames>Hermann</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>Klein</keyname><forenames>Joachim</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>Kl&#xfc;ppelholz</keyname><forenames>Sascha</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>M&#xe4;rcker</keyname><forenames>Steffen</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>Tews</keyname><forenames>Hendrik</forenames><affiliation>TU Dresden</affiliation></author><author><keyname>V&#xf6;lp</keyname><forenames>Marcus</forenames><affiliation>TU Dresden</affiliation></author></authors><title>Chiefly Symmetric: Results on the Scalability of Probabilistic Model
  Checking for Operating-System Code</title><categories>cs.LO cs.OS</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 156-166</journal-ref><doi>10.4204/EPTCS.102.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliability in terms of functional properties from the safety-liveness
spectrum is an indispensable requirement of low-level operating-system (OS)
code. However, with evermore complex and thus less predictable hardware,
quantitative and probabilistic guarantees become more and more important.
Probabilistic model checking is one technique to automatically obtain these
guarantees. First experiences with the automated quantitative analysis of
low-level operating-system code confirm the expectation that the naive
probabilistic model checking approach rapidly reaches its limits when
increasing the numbers of processes. This paper reports on our work-in-progress
to tackle the state explosion problem for low-level OS-code caused by the
exponential blow-up of the model size when the number of processes grows. We
studied the symmetry reduction approach and carried out our experiments with a
simple test-and-test-and-set lock case study as a representative example for a
wide range of protocols with natural inter-process dependencies and long-run
properties. We quickly see a state-space explosion for scenarios where
inter-process dependencies are insignificant. However, once inter-process
dependencies dominate the picture models with hundred and more processes can be
constructed and analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6197</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6197</id><created>2012-11-26</created><authors><author><keyname>Cock</keyname><forenames>David</forenames><affiliation>NICTA and School of Computer Science and Engineering, University of New South Wales</affiliation></author></authors><title>Verifying Probabilistic Correctness in Isabelle with pGCL</title><categories>cs.LO cs.PL</categories><comments>In Proceedings SSV 2012, arXiv:1211.5873</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 102, 2012, pp. 167-178</journal-ref><doi>10.4204/EPTCS.102.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formalisation of pGCL in Isabelle/HOL. Using a shallow
embedding, we demonstrate close integration with existing automation support.
We demonstrate the facility with which the model can be extended to incorporate
existing results, including those of the L4.verified project. We motivate the
applicability of the formalism to the mechanical verification of probabilistic
security properties, including the effectiveness of side-channel
countermeasures in real systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6205</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6205</id><created>2012-11-26</created><authors><author><keyname>Merrikh-Bayat</keyname><forenames>Farnood</forenames></author><author><keyname>Merrikh-Bayat</keyname><forenames>Farshad</forenames></author><author><keyname>Shouraki</keyname><forenames>Saeed Bagheri</forenames></author></authors><title>Neuro-Fuzzy Computing System with the Capacity of Implementation on
  Memristor-Crossbar and Optimization-Free Hardware Training</title><categories>cs.NE cs.AI</categories><comments>16 pages, 11 images, submitted to IEEE Trans. on Fuzzy systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, first we present a new explanation for the relation between
logical circuits and artificial neural networks, logical circuits and fuzzy
logic, and artificial neural networks and fuzzy inference systems. Then, based
on these results, we propose a new neuro-fuzzy computing system which can
effectively be implemented on the memristor-crossbar structure. One important
feature of the proposed system is that its hardware can directly be trained
using the Hebbian learning rule and without the need to any optimization. The
system also has a very good capability to deal with huge number of input-out
training data without facing problems like overtraining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6216</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6216</id><created>2012-11-27</created><updated>2014-03-04</updated><authors><author><keyname>Megow</keyname><forenames>Nicole</forenames></author><author><keyname>Verschae</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Dual techniques for scheduling on a machine with varying speed</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study scheduling problems on a machine with varying speed. Assuming a
known speed function we ask for a cost-efficient scheduling solution. Our main
result is a PTAS for minimizing the total weighted completion time in this
setting. This also implies a PTAS for the closely related problem of scheduling
to minimize generalized global cost functions. The key to our results is a
re-interpretation of the problem within the well-known two-dimensional Gantt
chart: instead of the standard approach of scheduling in the {\em
time-dimension}, we construct scheduling solutions in the weight-dimension.
  We also consider a dynamic problem variant in which deciding upon the speed
is part of the scheduling problem and we are interested in the tradeoff between
scheduling cost and speed-scaling cost, which is typically the energy
consumption. We observe that the optimal order is independent of the energy
consumption and that the problem can be reduced to the setting where the speed
of the machine is fixed, and thus admits a PTAS. Furthermore, we provide an
FPTAS for the NP-hard problem variant in which the machine can run only on a
fixed number of discrete speeds. Finally, we show how our results can be used
to obtain a~$(2+\eps)$-approximation for scheduling preemptive jobs with
release dates on multiple identical parallel machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6218</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6218</id><created>2012-11-27</created><authors><author><keyname>Xie</keyname><forenames>Baile</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Minn</keyname><forenames>Hlaing</forenames></author><author><keyname>Nosratinia</keyname><forenames>Aria</forenames></author></authors><title>Adaptive Interference Alignment with CSI Uncertainty</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is known to significantly increase sum-throughput
at high SNR in the presence of multiple interfering nodes, however, the
reliability of IA is little known, which is the subject of this paper. We study
the error performance of IA and compare it with conventional orthogonal
transmission schemes. Since most IA algorithms require extensive channel state
information (CSI), we also investigate the impact of CSI imperfection
(uncertainty) on the error performance. Our results show that under identical
rates, IA attains a better error performance than the orthogonal scheme for
practical signal to noise ratio (SNR) values but is more sensitive to CSI
uncertainty. We design bit loading algorithms that significantly improve error
performance of the existing IA schemes. Furthermore, we propose an adaptive
transmission scheme that not only considerably reduces error probability, but
also produces robustness to CSI uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6239</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6239</id><created>2012-11-27</created><updated>2013-07-07</updated><authors><author><keyname>Luo</keyname><forenames>Shixin</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Optimal Power and Range Adaptation for Green Broadcasting</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving energy efficiency is key to network providers maintaining profit
levels and an acceptable carbon footprint in the face of rapidly increasing
data traffic in cellular networks in the coming years. The energy-saving
concept studied in this paper is the adaptation of a base station's (BS's)
transmit power levels and coverage area according to channel conditions and
traffic load. The traffic load in cellular networks exhibits significant
fluctuations in both space and time, which can be exploited, through cell range
adaptation, for energy saving. In this paper, we design short- and long-term BS
power control (STPC and LTPC respectively) policies for the OFDMA-based
downlink of a single-cell system, where bandwidth is dynamically and equally
shared among a random number of mobile users (MUs). STPC is a function of all
MUs' channel gains that maintains the required user-level quality of service
(QoS), while LTPC (including BS on-off control) is a function of traffic
density that minimizes the long-term energy consumption at the BS under a
minimum throughput constraint. We first develop a power scaling law that
relates the (short-term) average transmit power at BS with the given cell range
and MU density. Based on this result, we derive the optimal (long-term)
transmit adaptation policy by considering a joint range adaptation and LTPC
problem. By identifying the fact that energy saving at BS essentially comes
from two major energy saving mechanisms (ESMs), i.e. range adaptation and BS
on-off power control, we propose low-complexity suboptimal schemes with various
combinations of the two ESMs to investigate their impacts on system energy
consumption. It is shown that when the network throughput is low, BS on-off
power control is the most effective ESM, while when the network throughput is
higher, range adaptation becomes more effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6244</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6244</id><created>2012-11-27</created><updated>2014-10-21</updated><authors><author><keyname>Amoozgar</keyname><forenames>Masoud</forenames></author><author><keyname>Ramezanian</keyname><forenames>Rasoul</forenames></author></authors><title>A Computational Model and Convergence Theorem for Rumor Dissemination in
  Social Networks</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>29 pages, 7 figures</comments><journal-ref>Amoozgar, M., Ramezanian, R. (2013) 'A Computational Model and
  Convergence Theorem for Rumor Dissemination in Social Networks', The ISC
  International Journal of Information Security, 5(2), pp. 141-154</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of rumors, which are known as unverified statements of uncertain
origin, may cause tremendous number of social problems. If it would be possible
to identify factors affecting spreading a rumor (such as agents' desires, trust
network, etc.), then this could be used to slowdown or stop its spreading. A
computational model that includes rumor features and the way a rumor is spread
among society's members, based on their desires, is therefore needed. Our
research is centering on the relation between the homogeneity of the society
and rumor convergence in it and result shows that the homogeneity of the
society is a necessary condition for convergence of the spreading rumor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6248</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6248</id><created>2012-11-27</created><updated>2012-12-04</updated><authors><author><keyname>Bleier</keyname><forenames>Arnim</forenames></author></authors><title>A simple non-parametric Topic Mixture for Authors and Documents</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reviews the Author-Topic Model and presents a new non-parametric
extension based on the Hierarchical Dirichlet Process. The extension is
especially suitable when no prior information about the number of components
necessary is available. A blocked Gibbs sampler is described and focus put on
staying as close as possible to the original model with only the minimum of
theoretical and implementation overhead necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6251</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6251</id><created>2012-11-27</created><authors><author><keyname>Ho</keyname><forenames>Ivan Wang-Hei</forenames></author><author><keyname>Leung</keyname><forenames>Kin K.</forenames></author><author><keyname>Polak</keyname><forenames>John W.</forenames></author></authors><title>A Methodology for Studying VANET Performance with Practical Vehicle
  Distribution in Urban Environment</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a Vehicular Ad-hoc Network (VANET), the amount of interference from
neighboring nodes to a communication link is governed by the vehicle density
dynamics in vicinity and transmission probabilities of terminals. It is obvious
that vehicles are distributed non-homogeneously along a road segment due to
traffic controls and speed limits at different portions of the road. The common
assumption of homogeneous node distribution in the network in most of the
previous work in mobile ad-hoc networks thus appears to be inappropriate in
VANETs. In light of the inadequacy, we present in this paper an original
methodology to study the performance of VANETs with practical vehicle
distribution in urban environment. Specifically, we introduce the stochastic
traffic model to characterize the general vehicular traffic flow as well as the
randomness of individual vehicles, from which we can acquire the mean dynamics
and the probability distribution of vehicular density. As illustrative
examples, we demonstrate how the density knowledge from the stochastic traffic
model can be utilized to derive the throughput and progress performance of
three routing strategies in different channel access protocols. We confirm the
accuracy of the analytical results through extensive simulations. Our results
demonstrate the applicability of the proposed methodology on modeling protocol
performance, and shed insight into the performance analysis of other
transmission protocols and network configurations in vehicular networks.
Furthermore, we illustrate that the optimal transmission probability for
optimized network performance can be obtained as a function of the location
space from our results. Such information can be computed by road-side nodes and
then broadcasted to road users for optimized multi-hop packet transmission in
the communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6254</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6254</id><created>2012-11-27</created><updated>2015-10-07</updated><authors><author><keyname>Tancer</keyname><forenames>Martin</forenames></author></authors><title>Recognition of collapsible complexes is NP-complete</title><categories>cs.CG</categories><comments>21 pages, 13 figures (Appendix was reworked in version v2. Other
  changes are mainly in the introduction + numerous minor fixes.)</comments><msc-class>05E45, 68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that it is NP-complete to decide whether a given (3-dimensional)
simplicial complex is collapsible. This work extends a result of Malgouyres and
Franc\'{e}s showing that it is NP-complete to decide whether a given simplicial
complex collapses to a 1-complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6255</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6255</id><created>2012-11-27</created><updated>2014-03-05</updated><authors><author><keyname>Bocus</keyname><forenames>Mohammud Z.</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammed R.</forenames></author></authors><title>Keyhole and Reflection Effects in Network Connectivity Analysis</title><categories>cs.IT cs.NI math.IT</categories><comments>21 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has demonstrated the importance of boundary effects on the
overall connection probability of wireless networks, but has largely focused on
convex domains. We consider two generic scenarios of practical importance to
wireless communications, in which one or more nodes are located outside the
convex space where the remaining nodes reside. Consequently, conventional
approaches with the underlying assumption that only line-of-sight (LOS) or
direct connections between nodes are possible, fail to provide the correct
analysis for the connectivity. We present an analytical framework that
explicitly considers the effects of reflections from the system boundaries on
the full connection probability. This study provides a different strategy to
ray tracing tools for predicting the wireless propagation environment. A simple
two-dimensional geometry is first considered, followed by a more practical
three-dimensional system. We investigate the effects of different system
parameters on the connectivity of the network though analysis corroborated by
numerical simulations, and highlight the potential of our approach for more
general non-convex geometries.t system parameters on the connectivity of the
network through simulation and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6258</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6258</id><created>2012-11-27</created><updated>2013-03-25</updated><authors><author><keyname>Ellis-Braithwaite</keyname><forenames>Richard</forenames></author><author><keyname>Lock</keyname><forenames>Russell</forenames></author><author><keyname>Dawson</keyname><forenames>Ray</forenames></author><author><keyname>Haque</keyname><forenames>Badr</forenames></author></authors><title>Modelling the Strategic Alignment of Software Requirements using Goal
  Graphs</title><categories>cs.SE</categories><comments>v2 minor updates: 1) bitmap images replaced with vector, 2) reworded
  related work ref[6] for clarity</comments><acm-class>D.2.1</acm-class><journal-ref>2012 International Conference on Software Engineering Advances</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper builds on existing Goal Oriented Requirements Engineering (GORE)
research by presenting a methodology with a supporting tool for analysing and
demonstrating the alignment between software requirements and business
objectives. Current GORE methodologies can be used to relate business goals to
software goals through goal abstraction in goal graphs. However, we argue that
unless the extent of goal-goal contribution is quantified with verifiable
metrics and confidence levels, goal graphs are not sufficient for demonstrating
the strategic alignment of software requirements. We introduce our methodology
using an example software project from Rolls-Royce. We conclude that our
methodology can improve requirements by making the relationships to business
problems explicit, thereby disambiguating a requirement's underlying purpose
and value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6273</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6273</id><created>2012-11-27</created><authors><author><keyname>Amini</keyname><forenames>Amineh</forenames></author><author><keyname>Saboohi</keyname><forenames>Hadi</forenames></author><author><keyname>bakhsh</keyname><forenames>Nasser Nemat</forenames></author></authors><title>A RDF-based Data Integration Framework</title><categories>cs.DB</categories><comments>National Electrical Engineering Conference (NEEC) 2008, Najafabad,
  Iran, March 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data integration is one of the main problems in distributed data sources. An
approach is to provide an integrated mediated schema for various data sources.
This research work aims at developing a framework for defining an integrated
schema and querying on it. The basic idea is to employ recent standard
languages and tools to provide a unified data integration framework. RDF is
used for integrated schema descriptions as well as providing a unified view of
data. RDQL is used for query reformulation. Furthermore, description logic
inference services provide necessary means for satisfiability checking of
concepts in integrated schema. The framework has tools to display integrated
schema, query on it, and provides enough flexibilities to be used in different
application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6279</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6279</id><created>2012-11-27</created><authors><author><keyname>Tavakoli</keyname><forenames>H.</forenames></author><author><keyname>Ahmadian</keyname><forenames>M.</forenames></author><author><keyname>Peyghami</keyname><forenames>M. Reza</forenames></author></authors><title>Optimal Rate Irregular LDPC Codes in Binary Erasure Channel</title><categories>cs.IT math.IT</categories><comments>published in IET Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design the optimal rate capacity approaching irregular
Low-Density Parity-Check code ensemble over Binary Erasure Channel, by using
practical Semi-Definite Programming approach. Our method does not use any
relaxation or any approximate solution unlike previous works. Our simulation
results include two parts; first, we present some codes and their degree
distribution functions that their rates are close to the capacity. Second, the
maximum achievable rate behavior of codes in our method is illustrated through
some figures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6302</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6302</id><created>2012-11-27</created><updated>2013-10-18</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>Duality between subgradient and conditional gradient methods</title><categories>cs.LG math.OC stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a convex optimization problem and its dual, there are many possible
first-order algorithms. In this paper, we show the equivalence between mirror
descent algorithms and algorithms generalizing the conditional gradient method.
This is done through convex duality, and implies notably that for certain
problems, such as for supervised machine learning problems with non-smooth
losses or problems regularized by non-smooth regularizers, the primal
subgradient method and the dual conditional gradient method are formally
equivalent. The dual interpretation leads to a form of line search for mirror
descent, as well as guarantees of convergence for primal-dual certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6315</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6315</id><created>2012-11-27</created><updated>2013-10-12</updated><authors><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Peri</keyname><forenames>Sathya</forenames></author></authors><title>Non-Interference and Local Correctness in Transactional Memory</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory promises to make concurrent programming tractable and
efficient by allowing the user to assemble sequences of actions in atomic
transactions with all-or-nothing semantics. It is believed that, by its very
virtue, transactional memory must ensure that all committed transactions
constitute a serial execution respecting the real-time order. In contrast,
aborted or incomplete transactions should not &quot;take effect.&quot; But what does &quot;not
taking effect&quot; mean exactly?
  It seems natural to expect that aborted or incomplete transactions do not
appear in the global serial execution, and, thus, no committed transaction can
be affected by them. We investigate another, less obvious, feature of &quot;not
taking effect&quot; called non-interference: aborted or incomplete transactions
should not force any other transaction to abort. In the strongest form of
non-interference that we explore in this paper, by removing a subset of aborted
or incomplete transactions from the history, we should not be able to turn an
aborted transaction into a committed one without violating the correctness
criterion.
  We show that non-interference is, in a strict sense, not implementable with
respect to the popular criterion of opacity that requires all transactions (be
they committed, aborted or incomplete) to witness the same global serial
execution. In contrast, when we only require local correctness,
non-interference is implementable. Informally, a correctness criterion is local
if it only requires that every transaction can be serialized along with (a
subset of) the transactions committed before its last event (aborted or
incomplete transactions ignored). We give a few examples of local correctness
properties, including the recently proposed criterion of virtual world
consistency, and present a simple though efficient implementation that
satisfies non-interference and local opacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6320</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6320</id><created>2012-11-27</created><updated>2013-11-07</updated><authors><author><keyname>Massarenti</keyname><forenames>Alex</forenames></author><author><keyname>Raviolo</keyname><forenames>Emanuele</forenames></author></authors><title>On the rank of $n\times n$ matrix multiplication</title><categories>cs.CC math.AG</categories><comments>10 pages. New version, title and main result changed. Linear Algebra
  and its Applications 2013</comments><msc-class>Primary 14Q20, Secondary 13P99, 68W30</msc-class><doi>10.1016/j.laa.2013.01.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every $p\leq n$ positive integer we obtain the lower bound
$(3-\frac{1}{p+1})n^2-\big(2\binom{2p}{p+1}-\binom{2p-2}{p-1}+2\big)n$ for the
rank of the $n\times n$ matrix multiplication. This bound improves the previous
one $(3-\frac{1}{p+1})n^2-\big(1+2p\binom{2p}{p}\big)n$ due to Landsberg.
Furthermore our bound improves the classic bound $\frac{5}{2}n^2-3n$, due to
Bl\&quot;aser, for every $n\geq 132$. Finally, for $p = 2$, with a sligtly different
strategy we menage to obtain the lower bound $\frac{8}{3}n^2-7n$ which improves
Bl\&quot;aser's bound for any $n\geq 24$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6321</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6321</id><created>2012-11-27</created><authors><author><keyname>Zhang</keyname><forenames>Guo</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Citation content analysis (cca): A framework for syntactic and semantic
  analysis of citation content</title><categories>cs.DL cs.IR cs.IT math.IT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new framework for Citation Content Analysis (CCA), for
syntactic and semantic analysis of citation content that can be used to better
analyze the rich sociocultural context of research behavior. The framework
could be considered the next generation of citation analysis. This paper
briefly reviews the history and features of content analysis in traditional
social sciences, and its previous application in Library and Information
Science. Based on critical discussion of the theoretical necessity of a new
method as well as the limits of citation analysis, the nature and purposes of
CCA are discussed, and potential procedures to conduct CCA, including
principles to identify the reference scope, a two-dimensional (citing and
cited) and two-modular (syntactic and semantic modules) codebook, are provided
and described. Future works and implications are also suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6322</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6322</id><created>2012-11-27</created><updated>2012-11-29</updated><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Monahan</keyname><forenames>Rosemary</forenames></author><author><keyname>Power</keyname><forenames>James F.</forenames></author></authors><title>Metamodel Instance Generation: A systematic literature review</title><categories>cs.SE</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling and thus metamodelling have become increasingly important in
Software Engineering through the use of Model Driven Engineering. In this paper
we present a systematic literature review of instance generation techniques for
metamodels, i.e. the process of automatically generating models from a given
metamodel. We start by presenting a set of research questions that our review
is intended to answer. We then identify the main topics that are related to
metamodel instance generation techniques, and use these to initiate our
literature search. This search resulted in the identification of 34 key papers
in the area, and each of these is reviewed here and discussed in detail. The
outcome is that we are able to identify a knowledge gap in this field, and we
offer suggestions as to some potential directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6324</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6324</id><created>2012-11-27</created><updated>2013-08-29</updated><authors><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alexander</forenames></author><author><keyname>Vankeerberghen</keyname><forenames>Guillaume</forenames></author></authors><title>Graph diameter, eigenvalues, and minimum-time consensus</title><categories>math.OC cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of achieving average consensus in the minimum number
of linear iterations on a fixed, undirected graph. We are motivated by the task
of deriving lower bounds for consensus protocols and by the so-called
&quot;definitive consensus conjecture&quot; which states that for an undirected connected
graph G with diameter D there exist D matrices whose nonzero-pattern complies
with the edges in G and whose product equals the all-ones matrix. Our first
result is a counterexample to the definitive consensus conjecture, which is the
first improvement of the diameter lower bound for linear consensus protocols.
We then provide some algebraic conditions under which this conjecture holds,
which we use to establish that all distance-regular graphs satisfy the
definitive consensus conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6340</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6340</id><created>2012-11-09</created><authors><author><keyname>Shovon</keyname><forenames>Md. Hedayetul Islam</forenames></author><author><keyname>Haque</keyname><forenames>Mahfuza</forenames></author></authors><title>An Approach of Improving Students Academic Performance by using k means
  clustering algorithm and Decision tree</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1002.2425 by other authors</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA),Vol. 3, No. 8, Page no. 146-149, 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Improving students academic performance is not an easy task for the academic
community of higher learning. The academic performance of engineering and
science students during their first year at university is a turning point in
their educational path and usually encroaches on their General Point
Average,GPA in a decisive manner. The students evaluation factors like class
quizzes mid and final exam assignment lab work are studied. It is recommended
that all these correlated information should be conveyed to the class teacher
before the conduction of final exam. This study will help the teachers to
reduce the drop out ratio to a significant level and improve the performance of
students. In this paper, we present a hybrid procedure based on Decision Tree
of Data mining method and Data Clustering that enables academicians to predict
students GPA and based on that instructor can take necessary step to improve
student academic performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6341</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6341</id><created>2012-11-27</created><authors><author><keyname>Keller</keyname><forenames>Chantal</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author><author><keyname>Lasson</keyname><forenames>Marc</forenames><affiliation>LIP</affiliation></author></authors><title>The Refined Calculus of Inductive Construction: Parametricity and
  Abstraction</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>LICS - 27th Annual IEEE Symposium on Logic in Computer Science -
  2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a refinement of the Calculus of Inductive Constructions in which
one can easily define a notion of relational parametricity. It provides a new
way to automate proofs in an interactive theorem prover like Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6353</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6353</id><created>2012-11-27</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>On the inverse power index problem</title><categories>math.OC cs.GT</categories><comments>17 pages, 2 figures, 12 tables</comments><msc-class>91B12, 90B99</msc-class><journal-ref>Optimization, Vol. 61, Nr. 8 (2012), Pages 989-1011</journal-ref><doi>10.1080/02331934.2011.587008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted voting games are frequently used in decision making. Each voter has
a weight and a proposal is accepted if the weight sum of the supporting voters
exceeds a quota. One line of research is the efficient computation of so-called
power indices measuring the influence of a voter. We treat the inverse problem:
Given an influence vector and a power index, determine a weighted voting game
such that the distribution of influence among the voters is as close as
possible to the given target value. We present exact algorithms and
computational results for the Shapley-Shubik and the (normalized) Banzhaf power
index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6370</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6370</id><created>2012-11-24</created><authors><author><keyname>Saboohi</keyname><forenames>Hadi</forenames></author><author><keyname>Kareem</keyname><forenames>Sameem Abdul</forenames></author></authors><title>Increasing the failure recovery probability of atomic replacement
  approaches</title><categories>cs.SE</categories><comments>2012 Asia-Oceania Top University League on Engineering Student
  Conference (AOTULE)</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web processes are made up of services as their units of functionality. The
services are represented as a graph and compose a synergy of service. The
composite service is prone to failure due to various causes. However, the
end-user should receive a smooth and non-interrupted execution. Atomic
replacement of a failed Web service to recover the system is a straightforward
approach. Nevertheless, finding a similar service is not reliable. In order to
increase the probability of the recovery of a failed composite service, a set
of services is replaced with another similar set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6401</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6401</id><created>2012-11-27</created><updated>2013-06-06</updated><authors><author><keyname>Tang</keyname><forenames>Yujie</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>On the Performance Bound of Sparse Estimation with Sensing Matrix
  Perturbation</title><categories>cs.IT math.IT</categories><comments>32 pages, 8 Figures, 1 Table</comments><doi>10.1109/TSP.2013.2271481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focusses on the sparse estimation in the situation where both the
the sensing matrix and the measurement vector are corrupted by additive
Gaussian noises. The performance bound of sparse estimation is analyzed and
discussed in depth. Two types of lower bounds, the constrained Cram\'{e}r-Rao
bound (CCRB) and the Hammersley-Chapman-Robbins bound (HCRB), are discussed. It
is shown that the situation with sensing matrix perturbation is more complex
than the one with only measurement noise. For the CCRB, its closed-form
expression is deduced. It demonstrates a gap between the maximal and nonmaximal
support cases. It is also revealed that a gap lies between the CCRB and the MSE
of the oracle pseudoinverse estimator, but it approaches zero asymptotically
when the problem dimensions tend to infinity. For a tighter bound, the HCRB,
despite of the difficulty in obtaining a simple expression for general sensing
matrix, a closed-form expression in the unit sensing matrix case is derived for
a qualitative study of the performance bound. It is shown that the gap between
the maximal and nonmaximal cases is eliminated for the HCRB. Numerical
simulations are performed to verify the theoretical results in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6409</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6409</id><created>2012-11-23</created><authors><author><keyname>El-Dosuky</keyname><forenames>Mohammed</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>Ahmed</forenames></author><author><keyname>Hamza</keyname><forenames>Taher</forenames></author><author><keyname>Rashad</keyname><forenames>Magdy</forenames></author></authors><title>Obesity Heuristic, New Way On Artificial Immune Systems</title><categories>cs.AI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a need for new metaphors from immunology to flourish the application
areas of Artificial Immune Systems. A metaheuristic called Obesity Heuristic
derived from advances in obesity treatment is proposed. The main forces of the
algorithm are the generation omega-6 and omega-3 fatty acids. The algorithm
works with Just-In-Time philosophy; by starting only when desired. A case study
of data cleaning is provided. With experiments conducted on standard tables,
results show that Obesity Heuristic outperforms other algorithms, with 100%
recall. This is a great improvement over other algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6410</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6410</id><created>2012-11-23</created><authors><author><keyname>El-Dosuky</keyname><forenames>Mohammed</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>Ahmed</forenames></author><author><keyname>Hamza</keyname><forenames>Taher</forenames></author><author><keyname>Rashad</keyname><forenames>Magdy</forenames></author></authors><title>New Hoopoe Heuristic Optimization</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most optimization problems in real life applications are often highly
nonlinear. Local optimization algorithms do not give the desired performance.
So, only global optimization algorithms should be used to obtain optimal
solutions. This paper introduces a new nature-inspired metaheuristic
optimization algorithm, called Hoopoe Heuristic (HH). In this paper, we will
study HH and validate it against some test functions. Investigations show that
it is very promising and could be seen as an optimization of the powerful
algorithm of cuckoo search. Finally, we discuss the features of Hoopoe
Heuristic and propose topics for further studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6411</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6411</id><created>2012-11-23</created><authors><author><keyname>El-Dosuky</keyname><forenames>Mohammed</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>Ahmed</forenames></author><author><keyname>Hamza</keyname><forenames>Taher</forenames></author><author><keyname>Rashad</keyname><forenames>Magdy</forenames></author></authors><title>New Heuristics for Interfacing Human Motor System using Brain Waves</title><categories>cs.HC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many new forms of interfacing human users to machines. We persevere
here electric mechanical form of interaction between human and machine. The
emergence of brain-computer interface allows mind-to-movement systems. The
story of the Pied Piper inspired us to devise some new heuristics for
interfacing human motor system using brain waves by combining head helmet and
LumbarMotionMonitor For the simulation we use java GridGain Brain responses of
classified subjects during training indicates that Probe can be the best
stimulus to rely on in distinguishing between knowledgeable and not
knowledgeable
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6418</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6418</id><created>2012-11-27</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author></authors><title>Measuring Time in Sporting Competitions with the Domain-Specific
  Language EasyTime</title><categories>cs.PL</categories><journal-ref>Elektrotehniski vestnik, 78(1-2): 36-41, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring time in mass sporting competitions is unthinkable manually today
because of their long duration and unreliability. Besides, automatic timing
devices based on the RFID technology have become cheaper. However, these
devices cannot operate stand-alone. To work efficiently, they need a computer
timing system for monitoring results. Such system should be capable of
processing the incoming events, encoding and assigning results to a individual
competitor, sorting results according to the achieved time and printing them.
In this paper, a domain-specific language named EasyTime will be defined. It
enables controlling an agent by writing events to a database. Using the agent,
the number of measuring devices can be reduced. Also, EasyTime is of a
universal type that can be applied to many different sporting competitions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6462</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6462</id><created>2012-11-27</created><updated>2013-04-11</updated><authors><author><keyname>Manoel</keyname><forenames>Andre</forenames></author><author><keyname>Vicente</keyname><forenames>Renato</forenames></author></authors><title>Statistical mechanics of reputation systems in autonomous networks</title><categories>cond-mat.dis-nn cs.SI physics.soc-ph</categories><comments>20 pages, 14 figures</comments><journal-ref>Journal of Statistical Mechanics (2013) P08002</journal-ref><doi>10.1088/1742-5468/2013/08/P08002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reputation systems seek to infer which members of a community can be trusted
based on ratings they issue about each other. We construct a Bayesian inference
model and simulate approximate estimates using belief propagation (BP). The
model is then mapped onto computing equilibrium properties of a spin glass in a
random field and analyzed by employing the replica symmetric cavity approach.
Having the fraction of trustful nodes and environment noise level as control
parameters, we evaluate the theoretical performance in terms of estimation
error and the robustness of the BP approximation in different scenarios.
Regions of degraded performance are then explained by the convergence
properties of the BP algorithm and by the emergence of a glassy phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6466</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6466</id><created>2012-11-27</created><authors><author><keyname>Mishra</keyname><forenames>Aurosish</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author></authors><title>Small H-coloring problems for bounded degree digraphs</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An NP-complete coloring or homomorphism problem may become polynomial time
solvable when restricted to graphs with degrees bounded by a small number, but
remain NP-complete if the bound is higher. For instance, 3-colorability of
graphs with degrees bounded by 3 can be decided by Brooks' theorem, while for
graphs with degrees bounded by 4, the 3-colorability problem is NP-complete. We
investigate an analogous phenomenon for digraphs, focusing on the three
smallest digraphs H with NP-complete H-colorability problems. It turns out that
in all three cases the H-coloring problem is polynomial time solvable for
digraphs with degree bounds $\Delta^{+} \leq 1$, $\Delta^{-} \leq 2$ (or
$\Delta^{+} \leq 2$, $\Delta^{-} \leq 1$). On the other hand with degree bounds
$\Delta^{+} \leq 2$, $\Delta^{-} \leq 2$, all three problems are again
NP-complete. A conjecture proposed for graphs H by Feder, Hell and Huang states
that any variant of the $H$-coloring problem which is NP-complete without
degree constraints is also NP-complete with degree constraints, provided the
degree bounds are high enough. Our study is the first confirmation that the
conjecture may also apply to digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6468</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6468</id><created>2012-11-27</created><updated>2013-01-18</updated><authors><author><keyname>Stannett</keyname><forenames>Mike</forenames></author><author><keyname>N&#xe9;meti</keyname><forenames>Istv&#xe1;n</forenames></author></authors><title>Using Isabelle to verify special relativity, with application to
  hypercomputation theory</title><categories>cs.LO gr-qc</categories><comments>14 pages, reformatted with minor corrections</comments><acm-class>F.4.1; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logicians at the R\'enyi Mathematical Institute in Budapest have spent
several years developing versions of relativity theory (special, general, and
other variants) based wholly on first order logic, and have argued in favour of
the physical decidability, via exploitation of cosmological phenomena, of
formally undecidable questions such as the Halting Problem and the consistency
of set theory.
  The Hungarian theories are very extensive, and their associated proofs are
intuitively very satisfying, but this brings its own risks since intuition can
sometimes be misleading. As part of a joint project, researchers at Sheffield
have recently started generating rigorous machine-verified versions of the
Hungarian proofs, so as to demonstrate the soundness of their work. In this
paper, we explain the background to the project and demonstrate an Isabelle
proof of the theorem &quot;No inertial observer can travel faster than light&quot;.
  This approach to physical theories and physical computability has several
pay-offs: (a) we can be certain our intuition hasn't led us astray (or if it
has, we can identify where this has happened); (b) we can identify which axioms
are specifically required in the proof of each theorem and to what extent those
axioms can be weakened (the fewer assumptions we make up-front, the stronger
the results); and (c) we can identify whether new formal proof techniques and
tactics are needed when tackling physical as opposed to mathematical theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6470</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6470</id><created>2012-11-27</created><updated>2014-03-16</updated><authors><author><keyname>Harp</keyname><forenames>G. R.</forenames></author><author><keyname>Ackermann</keyname><forenames>R. F.</forenames></author><author><keyname>Blair</keyname><forenames>Samantha K.</forenames></author><author><keyname>Arbunich</keyname><forenames>J.</forenames></author><author><keyname>Backus</keyname><forenames>P. R.</forenames></author><author><keyname>Tarter</keyname><forenames>J. C.</forenames></author><author><keyname>Team</keyname><forenames>the ATA</forenames></author></authors><title>A new class of SETI beacons that contain information (22-aug-2010)</title><categories>astro-ph.IM cs.OH</categories><comments>33 pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the cm-wavelength range, an extraterrestrial electromagnetic narrow band
(sine wave) beacon is an excellent choice to get alien attention across
interstellar distances because 1) it is not strongly affected by interstellar /
interplanetary dispersion or scattering, and 2) searching for narrowband
signals is computationally efficient (scales as Ns log(Ns) where Ns = number of
voltage samples). Here we consider a special case wideband signal where two or
more delayed copies of the same signal are transmitted over the same frequency
and bandwidth, with the result that ISM dispersion and scattering cancel out
during the detection stage. Such a signal is both a good beacon (easy to find)
and carries arbitrarily large information rate (limited only by the atmospheric
transparency to about 10 GHz). The discovery process uses an autocorrelation
algorithm, and we outline a compute scheme where the beacon discovery search
can be accomplished with only 2x the processing of a conventional sine wave
search, and discuss signal to background response for sighting the beacon. Once
the beacon is discovered, the focus turns to information extraction.
Information extraction requires similar processing as for generic wideband
signal searches, but since we have already identified the beacon, the
efficiency of information extraction is negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6471</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6471</id><created>2012-11-25</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Optimization of measurement configurations for geometrical calibration
  of industrial robot</title><categories>cs.RO</categories><comments>arXiv admin note: text overlap with arXiv:1211.6101</comments><proxy>ccsd</proxy><journal-ref>Intelligent Robotics and Applications, C.-Y. Su, S. Rakheja, H.
  Liu (Ed.) (2012) 132-143</journal-ref><doi>10.1007/978-3-642-33509-9_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the geometrical calibration of industrial robots
employed in precise manufacturing. To identify geometric parameters, an
advanced calibration technique is proposed that is based on the non-linear
experiment design theory, which is adopted for this particular application. In
contrast to previous works, the calibration experiment quality is evaluated
using a concept of the user-defined test-pose. In the frame of this concept,
the related optimization problem is formulated and numerical routines are
developed, which allow user to generate optimal set of manipulator
configurations for a given number of calibration experiments. The efficiency of
the developed technique is illustrated by several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6473</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6473</id><created>2012-11-27</created><authors><author><keyname>C&#xe9;rin</keyname><forenames>Christophe</forenames><affiliation>LIPN</affiliation></author><author><keyname>Takoudjou</keyname><forenames>Alain</forenames><affiliation>LIPN</affiliation></author><author><keyname>Gren&#xe8;che</keyname><forenames>Nicolas</forenames><affiliation>LIPN</affiliation></author></authors><title>Int\'egration des intergiciels de grilles de PC dans le nuage SlapOS :
  le cas de BOINC</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we describe the problems and solutions related to the
integration of desktop grid middleware in a cloud, in this case the open source
SlapOS cloud. We focus on the issues about recipes that describe the
integration and the problem of the confinement of execution. They constitute
two aspects of service-oriented architecture and Cloud Computing. These two
issues solved with SlapOS are not in relation to what is traditionally done in
the clouds because we do not rely on virtual machines and, there is no data
center (as defined in cloud). Moreover, we show that from the initial
deployment model we take into account not only Web applications, B2B
applications... but also applications from the field of grids; here desktop
grid middleware which is a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6491</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6491</id><created>2012-11-27</created><authors><author><keyname>Yun</keyname><forenames>Yeo Hun</forenames></author><author><keyname>Cho</keyname><forenames>Joon Ho</forenames></author></authors><title>Sum-Rate Optimal Multi-Code CDMA Systems: An Equivalence Result</title><categories>cs.IT math.IT</categories><comments>66 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the sum rate of a multi-code CDMA system with asymmetric-power
users is maximized, given a processing gain and a power profile of users.
Unlike the sum-rate maximization for a single-code CDMA system, the
optimization requires the joint optimal distribution of each user's power to
its multiple data streams as well as the optimal design of signature sequences.
The crucial step is to establish an equivalence of the multi-code CDMA system
to restricted FDMA and TDMA systems. The CDMA system has upper limits on the
numbers of multi-codes of users, while the FDMA and the TDMA systems have upper
limits on the bandwidths and the duty cycles of users, respectively, in
addition to total bandwidth constraint. The equivalence facilitates the
complete characterization of the maximum sum rate of the multi-code CDMA system
and also provides new insights into the single- and the multi-code CDMA systems
in terms of the parameters of the equivalent FDMA and TDMA systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6496</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6496</id><created>2012-11-27</created><updated>2012-11-30</updated><authors><author><keyname>UzZaman</keyname><forenames>Naushad</forenames></author><author><keyname>Blanco</keyname><forenames>Roi</forenames></author><author><keyname>Matthews</keyname><forenames>Michael</forenames></author></authors><title>TwitterPaul: Extracting and Aggregating Twitter Predictions</title><categories>cs.SI cs.AI physics.soc-ph</categories><comments>Check out the blog post with a summary and Prediction Retrieval
  information here: http://bitly.com/TwitterPaul</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces TwitterPaul, a system designed to make use of Social
Media data to help to predict game outcomes for the 2010 FIFA World Cup
tournament. To this end, we extracted over 538K mentions to football games from
a large sample of tweets that occurred during the World Cup, and we classified
into different types with a precision of up to 88%. The different mentions were
aggregated in order to make predictions about the outcomes of the actual games.
We attempt to learn which Twitter users are accurate predictors and explore
several techniques in order to exploit this information to make more accurate
predictions. We compare our results to strong baselines and against the betting
line (prediction market) and found that the quality of extractions is more
important than the quantity, suggesting that high precision methods working on
a medium-sized dataset are preferable over low precision methods that use a
larger amount of data. Finally, by aggregating some classes of predictions, the
system performance is close to the one of the betting line. Furthermore, we
believe that this domain independent framework can help to predict other
sports, elections, product release dates and other future events that people
talk about in social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6512</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6512</id><created>2012-11-27</created><authors><author><keyname>Garcia-Herranz</keyname><forenames>Manuel</forenames></author><author><keyname>Egido</keyname><forenames>Esteban Moro</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Christakis</keyname><forenames>Nicholas A.</forenames></author><author><keyname>Fowler</keyname><forenames>James H.</forenames></author></authors><title>Using Friends as Sensors to Detect Global-Scale Contagious Outbreaks</title><categories>cs.SI physics.soc-ph</categories><comments>Press embargo in place until publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has focused on the monitoring of global-scale online data for
improved detection of epidemics, mood patterns, movements in the stock market,
political revolutions, box-office revenues, consumer behaviour and many other
important phenomena. However, privacy considerations and the sheer scale of
data available online are quickly making global monitoring infeasible, and
existing methods do not take full advantage of local network structure to
identify key nodes for monitoring. Here, we develop a model of the contagious
spread of information in a global-scale, publicly-articulated social network
and show that a simple method can yield not just early detection, but advance
warning of contagious outbreaks. In this method, we randomly choose a small
fraction of nodes in the network and then we randomly choose a &quot;friend&quot; of each
node to include in a group for local monitoring. Using six months of data from
most of the full Twittersphere, we show that this friend group is more central
in the network and it helps us to detect viral outbreaks of the use of novel
hashtags about 7 days earlier than we could with an equal-sized randomly chosen
group. Moreover, the method actually works better than expected due to network
structure alone because highly central actors are both more active and exhibit
increased diversity in the information they transmit to others. These results
suggest that local monitoring is not just more efficient, it is more effective,
and it is possible that other contagious processes in global-scale networks may
be similarly monitored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6522</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6522</id><created>2012-11-28</created><authors><author><keyname>Park</keyname><forenames>Jeonghun</forenames></author><author><keyname>Hwang</keyname><forenames>Seunggye</forenames></author><author><keyname>Yang</keyname><forenames>Janghoon</forenames></author><author><keyname>Kim</keyname><forenames>Dongku</forenames></author></authors><title>Generalized Distributed Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Compressive Sensing (DCS) improves the signal recovery
performance of multi signal ensembles by exploiting both intra- and
inter-signal correlation and sparsity structure. However, the existing DCS was
proposed for a very limited ensemble of signals that has single common
information \cite{Baron:2009vd}. In this paper, we propose a generalized DCS
(GDCS) which can improve sparse signal detection performance given arbitrary
types of common information which are classified into not just full common
information but also a variety of partial common information. The theoretical
bound on the required number of measurements using the GDCS is obtained.
Unfortunately, the GDCS may require much a priori-knowledge on various inter
common information of ensemble of signals to enhance the performance over the
existing DCS. To deal with this problem, we propose a novel algorithm that can
search for the correlation structure among the signals, with which the proposed
GDCS improves detection performance even without a priori-knowledge on
correlation structure for the case of arbitrarily correlated multi signal
ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6526</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6526</id><created>2012-11-28</created><authors><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Munagala</keyname><forenames>Kamesh</forenames></author></authors><title>Complexity Measures for Map-Reduce, and Comparison to Parallel Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The programming paradigm Map-Reduce and its main open-source implementation,
Hadoop, have had an enormous impact on large scale data processing. Our goal in
this expository writeup is two-fold: first, we want to present some complexity
measures that allow us to talk about Map-Reduce algorithms formally, and
second, we want to point out why this model is actually different from other
models of parallel programming, most notably the PRAM (Parallel Random Access
Memory) model. We are looking for complexity measures that are detailed enough
to make fine-grained distinction between different algorithms, but which also
abstract away many of the implementation details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6535</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6535</id><created>2012-11-28</created><updated>2015-07-01</updated><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author><author><keyname>Park</keyname><forenames>Mi-Young</forenames></author></authors><title>Towards Interactive Logic Programming</title><categories>cs.LO cs.PL</categories><comments>8 pages. It describes two execution models for interactive logic
  programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear logic programming uses provability as the basis for computation. In
the operational semantics based on provability, executing the
additive-conjunctive goal $G_1 \&amp; G_2$ from a program $P$ simply terminates
with a success if both $G_1$ and $G_2$ are solvable from $P$. This is an
unsatisfactory situation, as a central action of \&amp; -- the action of choosing
either $G_1$ or $G_2$ by the user -- is missing in this semantics.
  We propose to modify the operational semantics above to allow for more active
participation from the user. We illustrate our idea via muProlog, an extension
of Prolog with additive goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6537</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6537</id><created>2012-11-28</created><updated>2013-06-06</updated><authors><author><keyname>Olhede</keyname><forenames>Sofia C.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Degree-based network models</title><categories>math.ST cs.SI math.CO stat.ME stat.TH</categories><comments>31 pages, 3 figures, submitted for publication</comments><msc-class>05C80 (Primary) 62G05, 60B20 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the sampling properties of random networks based on weights whose
pairwise products parameterize independent Bernoulli trials. This enables an
understanding of many degree-based network models, in which the structure of
realized networks is governed by properties of their degree sequences. We
provide exact results and large-sample approximations for power-law networks
and other more general forms. This enables us to quantify sampling variability
both within and across network populations, and to characterize the limiting
extremes of variation achievable through such models. Our results highlight
that variation explained through expected degree structure need not be
attributed to more complicated generative mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6553</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6553</id><created>2012-11-28</created><updated>2015-10-07</updated><authors><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Neumann</keyname><forenames>Adrian</forenames></author><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author></authors><title>Certifying 3-Edge-Connectivity</title><categories>cs.DS cs.DM</categories><comments>29 pages in Algorithmica, 2015</comments><doi>10.1007/s00453-015-0075-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a certifying algorithm that tests graphs for 3-edge-connectivity;
the algorithm works in linear time. If the input graph is not 3-edge-connected,
the algorithm returns a 2-edge-cut. If it is 3-edge-connected, it returns a
construction sequence that constructs the input graph from the graph with two
vertices and three parallel edges using only operations that (obviously)
preserve 3-edge-connectivity. Additionally, we show how compute and certify the
3-edge-connected components and a cactus representation of the 2-cuts in linear
time. For 3-vertex-connectivity, we show how to compute the 3-vertex-connected
components of a 2-connected graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6566</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6566</id><created>2012-11-28</created><authors><author><keyname>Sboui</keyname><forenames>Lokman</forenames></author><author><keyname>Rezki</keyname><forenames>Zouheir</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>A Unified Framework for the Ergodic Capacity of Spectrum Sharing
  Cognitive Radio Systems</title><categories>cs.IT math.IT</categories><comments>12 pages, 8 figures, To appear IEEE Transactions on Wireless
  Communications 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a spectrum sharing communication scenario in which a primary and
a secondary users are communicating, simultaneously, with their respective
destinations using the same frequency carrier. Both optimal power profile and
ergodic capacity are derived for fading channels, under an average transmit
power and an instantaneous interference outage constraints. Unlike previous
studies, we assume that the secondary user has a noisy version of the cross
link and the secondary link Channel State Information (CSI). After deriving the
capacity in this case, we provide an ergodic capacity generalization, through a
unified expression, that encompasses several previously studied spectrum
sharing settings. In addition, we provide an asymptotic capacity analysis at
high and low signal-to-noise ratio (SNR). Numerical results, applied for
independent Rayleigh fading channels, show that at low SNR regime, only the
secondary channel estimation matters with no effect of the cross link on the
capacity; whereas at high SNR regime, the capacity is rather driven by the
cross link CSI. Furthermore, a practical on-off power allocation scheme is
proposed and is shown, through numerical results, to achieve the full capacity
at high and low SNR
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6572</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6572</id><created>2012-11-28</created><authors><author><keyname>Fa&#xff;</keyname><forenames>Gilles</forenames></author><author><keyname>Kang</keyname><forenames>Sinuk</forenames></author></authors><title>Average sampling of band-limited stochastic processes</title><categories>cs.IT math.IT</categories><msc-class>42C15, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing a wide sense stationary
band-limited process from its local averages taken either at the Nyquist rate
or above. As a result, we obtain a sufficient condition under which average
sampling expansions hold in mean square and for almost all sample functions.
Truncation and aliasing errors of the expansion are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6581</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6581</id><created>2012-11-28</created><updated>2016-01-27</updated><authors><author><keyname>Spyromitros-Xioufis</keyname><forenames>Eleftherios</forenames></author><author><keyname>Tsoumakas</keyname><forenames>Grigorios</forenames></author><author><keyname>Groves</keyname><forenames>William</forenames></author><author><keyname>Vlahavas</keyname><forenames>Ioannis</forenames></author></authors><title>Multi-Target Regression via Input Space Expansion: Treating Targets as
  Inputs</title><categories>cs.LG</categories><comments>Accepted for publication in Machine Learning journal. This
  replacement contains major improvements compared to the previous version,
  including a deeper theoretical and experimental analysis and an extended
  discussion of related work</comments><doi>10.1007/s10994-016-5546-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical applications of supervised learning the task involves the
prediction of multiple target variables from a common set of input variables.
When the prediction targets are binary the task is called multi-label
classification, while when the targets are continuous the task is called
multi-target regression. In both tasks, target variables often exhibit
statistical dependencies and exploiting them in order to improve predictive
accuracy is a core challenge. A family of multi-label classification methods
address this challenge by building a separate model for each target on an
expanded input space where other targets are treated as additional input
variables. Despite the success of these methods in the multi-label
classification domain, their applicability and effectiveness in multi-target
regression has not been studied until now. In this paper, we introduce two new
methods for multi-target regression, called Stacked Single-Target and Ensemble
of Regressor Chains, by adapting two popular multi-label classification methods
of this family. Furthermore, we highlight an inherent problem of these methods
- a discrepancy of the values of the additional input variables between
training and prediction - and develop extensions that use out-of-sample
estimates of the target variables during training in order to tackle this
problem. The results of an extensive experimental evaluation carried out on a
large and diverse collection of datasets show that, when the discrepancy is
appropriately mitigated, the proposed methods attain consistent improvements
over the independent regressions baseline. Moreover, two versions of Ensemble
of Regression Chains perform significantly better than four state-of-the-art
methods including regularization-based multi-task learning methods and a
multi-objective random forest approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6598</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6598</id><created>2012-11-28</created><authors><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Estimation of Bandlimited Signals in Additive Gaussian Noise: a
  &quot;Precision Indifference&quot; Principle</title><categories>cs.IT math.IT</categories><comments>Single column, 12 pages, 2 figures, abridged version submitted to
  IEEE ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sampling, quantization, and estimation of a bounded dynamic-range
bandlimited signal affected by additive independent Gaussian noise is studied
in this work. For bandlimited signals, the distortion due to additive
independent Gaussian noise can be reduced by oversampling (statistical
diversity). The pointwise expected mean-squared error is used as a distortion
metric for signal estimate in this work. Two extreme scenarios of quantizer
precision are considered: (i) infinite precision (real scalars); and (ii)
one-bit quantization (sign information). If $N$ is the oversampling ratio with
respect to the Nyquist rate, then the optimal law for distortion is $O(1/N)$.
We show that a distortion of $O(1/N)$ can be achieved irrespective of the
quantizer precision by considering the above-mentioned two extreme scenarios of
quantization. Thus, a quantization precision indifference principle is
discovered, where the reconstruction distortion law, up to a proportionality
constant, is unaffected by quantizer's accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6610</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6610</id><created>2012-11-28</created><authors><author><keyname>Halilovic</keyname><forenames>Muhamed</forenames></author><author><keyname>Subasi</keyname><forenames>Abdulhamit</forenames></author></authors><title>Intrusion Detection on Smartphones</title><categories>cs.CR cs.AI</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smartphone technology is more and more becoming the predominant communication
tool for people across the world. People use their smartphones to keep their
contact data, to browse the internet, to exchange messages, to keep notes,
carry their personal files and documents, etc. Users while browsing are also
capable of shopping online, thus provoking a need to type their credit card
numbers and security codes. As the smartphones are becoming widespread so do
the security threats and vulnerabilities facing this technology. Recent news
and articles indicate huge increase in malware and viruses for operating
systems employed on smartphones (primarily Android and iOS). Major limitations
of smartphone technology are its processing power and its scarce energy source
since smartphones rely on battery usage. Since smartphones are devices which
change their network location as the user moves between different places,
intrusion detection systems for smartphone technology are most often classified
as IDSs designed for mobile ad-hoc networks. The aim of this research is to
give a brief overview of IDS technology, give an overview of major machine
learning and pattern recognition algorithms used in IDS technologies, give an
overview of security models of iOS and Android and propose a new host-based IDS
model for smartphones and create proof-of-concept application for Android
platform for the newly proposed model. Keywords: IDS, SVM, Android, iOS;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6616</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6616</id><created>2012-11-28</created><updated>2014-04-04</updated><authors><author><keyname>Li</keyname><forenames>Rongpeng</forenames></author><author><keyname>Zhao</keyname><forenames>Zhifeng</forenames></author><author><keyname>Chen</keyname><forenames>Xianfu</forenames></author><author><keyname>Palicot</keyname><forenames>Jacques</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author></authors><title>TACT: A Transfer Actor-Critic Learning Framework for Energy Saving in
  Cellular Radio Access Networks</title><categories>cs.NI cs.AI cs.IT cs.LG math.IT</categories><comments>11 figures, 30 pages, accepted in IEEE Transactions on Wireless
  Communications 2014. IEEE Trans. Wireless Commun., Feb. 2014</comments><doi>10.1109/TWC.2014.022014.130840</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent works have validated the possibility of improving energy efficiency in
radio access networks (RANs), achieved by dynamically turning on/off some base
stations (BSs). In this paper, we extend the research over BS switching
operations, which should match up with traffic load variations. Instead of
depending on the dynamic traffic loads which are still quite challenging to
precisely forecast, we firstly formulate the traffic variations as a Markov
decision process. Afterwards, in order to foresightedly minimize the energy
consumption of RANs, we design a reinforcement learning framework based BS
switching operation scheme. Furthermore, to avoid the underlying curse of
dimensionality in reinforcement learning, a transfer actor-critic algorithm
(TACT), which utilizes the transferred learning expertise in historical periods
or neighboring regions, is proposed and provably converges. In the end, we
evaluate our proposed scheme by extensive simulations under various practical
configurations and show that the proposed TACT algorithm contributes to a
performance jumpstart and demonstrates the feasibility of significant energy
efficiency improvement at the expense of tolerable delay performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6624</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6624</id><created>2012-11-28</created><updated>2012-12-03</updated><authors><author><keyname>Bonnabel</keyname><forenames>Silvere</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>A contraction theory-based analysis of the stability of the Extended
  Kalman Filter</title><categories>cs.SY math.OC</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contraction properties of the Extended Kalman Filter, viewed as a
deterministic observer for nonlinear systems, are analyzed. This yields new
conditions under which exponential convergence of the state error can be
guaranteed. As contraction analysis studies the evolution of an infinitesimal
discrepancy between neighboring trajectories, and thus stems from a
differential framework, the sufficient convergence conditions are different
from the ones that previously appeared in the literature, which were derived in
a Lyapunov framework. This article sheds another light on the theoretical
properties of this popular observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6631</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6631</id><created>2012-11-28</created><authors><author><keyname>Ozdemir</keyname><forenames>Onur</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author><author><keyname>Su</keyname><forenames>Wei</forenames></author><author><keyname>Drozd</keyname><forenames>Andrew L.</forenames></author></authors><title>Asymptotic Properties of Likelihood Based Linear Modulation
  Classification Systems</title><categories>cs.IT math.IT stat.AP</categories><comments>12 pages double-column, 6 figures, submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of linear modulation classification using likelihood based
methods is considered. Asymptotic properties of most commonly used classifiers
in the literature are derived. These classifiers are based on hybrid likelihood
ratio test (HLRT) and average likelihood ratio test (ALRT), respectively. Both
a single-sensor setting and a multi-sensor setting that uses a distributed
decision fusion approach are analyzed. For a modulation classification system
using a single sensor, it is shown that HLRT achieves asymptotically vanishing
probability of error (Pe) whereas the same result cannot be proven for ALRT. In
a multi-sensor setting using soft decision fusion, conditions are derived under
which Pe vanishes asymptotically. Furthermore, the asymptotic analysis of the
fusion rule that assumes independent sensor decisions is carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6636</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6636</id><created>2012-11-28</created><updated>2013-03-09</updated><authors><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Chen</keyname><forenames>Zhaoqun</forenames></author><author><keyname>Liu</keyname><forenames>Pengfei</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Edge Balance Ratio: Power Law from Vertices to Edges in Directed Complex
  Network</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 6 figures, Journal manuscript</comments><doi>10.1109/JSTSP.2013.2245299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power law distribution is common in real-world networks including online
social networks. Many studies on complex networks focus on the characteristics
of vertices, which are always proved to follow the power law. However, few
researches have been done on edges in directed networks. In this paper, edge
balance ratio is firstly proposed to measure the balance property of edges in
directed networks. Based on edge balance ratio, balance profile and positivity
are put forward to describe the balance level of the whole network. Then the
distribution of edge balance ratio is theoretically analyzed. In a directed
network whose vertex in-degree follows the power law with scaling exponent
$\gamma$, it is proved that the edge balance ratio follows a piecewise power
law, with the scaling exponent of each section linearly dependents on $\gamma$.
The theoretical analysis is verified by numerical simulations. Moreover, the
theoretical analysis is confirmed by statistics of real-world online social
networks, including Twitter network with 35 million users and Sina Weibo
network with 110 million users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6643</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6643</id><created>2012-11-28</created><authors><author><keyname>Rao</keyname><forenames>Shodhan</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan</forenames></author><author><keyname>Jayawardhana</keyname><forenames>Bayu</forenames></author></authors><title>A Graph-Theoretical Approach for the Analysis and Model Reduction of
  Complex-Balanced Chemical Reaction Networks</title><categories>math.DS cs.SY math.OC physics.chem-ph</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive a compact mathematical formulation describing the
dynamics of chemical reaction networks that are complex-balanced and are
governed by mass action kinetics. The formulation is based on the graph of
(substrate and product) complexes and the stoichiometric information of these
complexes, and crucially uses a balanced weighted Laplacian matrix. It is shown
that this formulation leads to elegant methods for characterizing the space of
all equilibria for complex-balanced networks and for deriving stability
properties of such networks. We propose a method for model reduction of
complex-balanced networks, which is similar to the Kron reduction method for
electrical networks and involves the computation of Schur complements of the
balanced weighted Laplacian matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6653</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6653</id><created>2012-11-28</created><authors><author><keyname>Wang</keyname><forenames>Yuyang</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author></authors><title>Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process
  Approach</title><categories>cs.LG stat.ML</categories><comments>Preliminary version appeared in ECML2012</comments><doi>10.1007/978-3-642-33460-3_51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning models using Gaussian processes (GP) have been developed
and successfully applied in various applications. The main difficulty with this
approach is the computational cost of inference using the union of examples
from all tasks. Therefore sparse solutions, that avoid using the entire data
directly and instead use a set of informative &quot;representatives&quot; are desirable.
The paper investigates this problem for the grouped mixed-effect GP model where
each individual response is given by a fixed-effect, taken from one of a set of
unknown groups, plus a random individual effect function that captures
variations among individuals. Such models have been widely used in previous
work but no sparse solutions have been developed. The paper presents the first
sparse solution for such problems, showing how the sparse approximation can be
obtained by maximizing a variational lower bound on the marginal likelihood,
generalizing ideas from single-task Gaussian processes to handle the
mixed-effect model as well as grouping. Experiments using artificial and real
data validate the approach showing that it can recover the performance of
inference with the full sample, that it outperforms baseline methods, and that
it outperforms state of the art sparse solutions for other multi-task GP
formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6656</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6656</id><created>2012-11-28</created><authors><author><keyname>Escoffier</keyname><forenames>Bruno</forenames></author><author><keyname>Kim</keyname><forenames>EunJung</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Subexponential and FPT-time Inapproximability of Independent Set and
  Related Problems</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixed-parameter algorithms, approximation algorithms and moderately
exponential algorithms are three major approaches to algorithms design. While
each of them being very active in its own, there is an increasing attention to
the connection between different approaches. In particular, whether Maximum
Independent Set would be better approximable once endowed with
subexponential-time or FPT-time is a central question. In this paper, we
present a strong link between the linear PCP conjecture and the
inapproximability, thus partially answering this question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6658</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6658</id><created>2012-11-28</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>Nature-Inspired Mateheuristic Algorithms: Success and New Challenges</title><categories>math.OC cs.NE</categories><comments>6 pages</comments><msc-class>90C26</msc-class><journal-ref>J Comput. Eng. Inf. Technol., Vol. 1, Issue 1, pp. 1-3 (2012)</journal-ref><doi>10.4172/2324-9307.1000e101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the increasing popularity of metaheuristics, many crucially important
questions remain unanswered. There are two important issues: theoretical
framework and the gap between theory and applications. At the moment, the
practice of metaheuristics is like heuristic itself, to some extent, by trial
and error. Mathematical analysis lags far behind, apart from a few, limited,
studies on convergence analysis and stability, there is no theoretical
framework for analyzing metaheuristic algorithms. I believe mathematical and
statistical methods using Markov chains and dynamical systems can be very
useful in the future work. There is no doubt that any theoretical progress will
provide potentially huge insightful into meteheuristic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6660</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6660</id><created>2012-11-28</created><authors><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>An Equivalence between Network Coding and Index Coding</title><categories>cs.IT cs.DM cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the network coding and index coding problems are equivalent.
This equivalence holds in the general setting which includes linear and
non-linear codes. Specifically, we present an efficient reduction that maps a
network coding instance to an index coding one while preserving feasibility.
Previous connections were restricted to the linear case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6664</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6664</id><created>2012-11-28</created><authors><author><keyname>Campagne</keyname><forenames>Fabien</forenames></author><author><keyname>Dorff</keyname><forenames>Kevin C.</forenames></author><author><keyname>Chambwe</keyname><forenames>Nyasha</forenames></author><author><keyname>Robinson</keyname><forenames>James T.</forenames></author><author><keyname>Mesirov</keyname><forenames>Jill P.</forenames></author><author><keyname>Wu</keyname><forenames>Thomas D.</forenames></author></authors><title>Compression of structured high-throughput sequencing data</title><categories>q-bio.QM cs.DB q-bio.GN</categories><comments>main article: 2 figures, 2 tables. Supplementary material: 2 figures,
  4 tables. Comment on this manuscript on Twitter or Google Plus using handle
  #Goby2Paper</comments><doi>10.1371/journal.pone.0079871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large biological datasets are being produced at a rapid pace and create
substantial storage challenges, particularly in the domain of high-throughput
sequencing (HTS). Most approaches currently used to store HTS data are either
unable to quickly adapt to the requirements of new sequencing or analysis
methods (because they do not support schema evolution), or fail to provide
state of the art compression of the datasets. We have devised new approaches to
store HTS data that support seamless data schema evolution and compress
datasets substantially better than existing approaches. Building on these new
approaches, we discuss and demonstrate how a multi-tier data organization can
dramatically reduce the storage, computational and network burden of
collecting, analyzing, and archiving large sequencing datasets. For instance,
we show that spliced RNA-Seq alignments can be stored in less than 4% the size
of a BAM file with perfect data fidelity. Compared to the previous compression
state of the art, these methods reduce dataset size more than 20% when storing
gene expression and epigenetic datasets. The approaches have been integrated in
a comprehensive suite of software tools (http://goby.campagnelab.org) that
support common analyses for a range of high-throughput sequencing assays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6674</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6674</id><created>2012-11-28</created><authors><author><keyname>Vu</keyname><forenames>Dinh Thang</forenames></author><author><keyname>Renaux</keyname><forenames>Alexandre</forenames></author><author><keyname>Boyer</keyname><forenames>Remy</forenames></author><author><keyname>Marcos</keyname><forenames>Sylvie</forenames></author></authors><title>Some results on the Weiss-Weinstein bound for conditional and
  unconditional signal models in array processing</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Weiss-Weinstein bound is analyzed in the context of
sources localization with a planar array of sensors. Both conditional and
unconditional source signal models are studied. First, some results are given
in the multiple sources context without specifying the structure of the
steering matrix and of the noise covariance matrix. Moreover, the case of an
uniform or Gaussian prior are analyzed. Second, these results are applied to
the particular case of a single source for two kinds of array geometries: a
non-uniform linear array (elevation only) and an arbitrary planar (azimuth and
elevation) array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6675</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6675</id><created>2012-11-28</created><authors><author><keyname>Ersoy</keyname><forenames>Dalton Lunga 'and' Okan</forenames></author></authors><title>Nonlinear Dynamic Field Embedding: On Hyperspectral Scene Visualization</title><categories>cs.CV cs.CE stat.ML</categories><comments>49 pages, 18 figures</comments><report-no>TR-ECE-12-14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph embedding techniques are useful to characterize spectral signature
relations for hyperspectral images. However, such images consists of disjoint
classes due to spatial details that are often ignored by existing graph
computing tools. Robust parameter estimation is a challenge for kernel
functions that compute such graphs. Finding a corresponding high quality
coordinate system to map signature relations remains an open research question.
We answer positively on these challenges by first proposing a kernel function
of spatial and spectral information in computing neighborhood graphs. Secondly,
the study exploits the force field interpretation from mechanics and devise a
unifying nonlinear graph embedding framework. The generalized framework leads
to novel unsupervised multidimensional artificial field embedding techniques
that rely on the simple additive assumption of pair-dependent attraction and
repulsion functions. The formulations capture long range and short range
distance related effects often associated with living organisms and help to
establish algorithmic properties that mimic mutual behavior for the purpose of
dimensionality reduction. The main benefits from the proposed models includes
the ability to preserve the local topology of data and produce quality
visualizations i.e. maintaining disjoint meaningful neighborhoods. As part of
evaluation, visualization, gradient field trajectories, and semisupervised
classification experiments are conducted for image scenes acquired by multiple
sensors at various spatial resolutions over different types of objects. The
results demonstrate the superiority of the proposed embedding framework over
various widely used methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6687</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6687</id><created>2012-11-28</created><updated>2013-05-31</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author></authors><title>Robustness Analysis of Hottopixx, a Linear Programming Model for
  Factoring Nonnegative Matrices</title><categories>stat.ML cs.LG cs.NA math.OC</categories><comments>23 pages; new numerical results; Comparison with Arora et al.;
  Accepted in SIAM J. Mat. Anal. Appl</comments><journal-ref>SIAM J. Matrix Anal. &amp; Appl. 34 (3), pp. 1189-1212, 2013</journal-ref><doi>10.1137/120900629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although nonnegative matrix factorization (NMF) is NP-hard in general, it has
been shown very recently that it is tractable under the assumption that the
input nonnegative data matrix is close to being separable (separability
requires that all columns of the input matrix belongs to the cone spanned by a
small subset of these columns). Since then, several algorithms have been
designed to handle this subclass of NMF problems. In particular, Bittorf,
Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs',
NIPS 2012) proposed a linear programming model, referred to as Hottopixx. In
this paper, we provide a new and more general robustness analysis of their
method. In particular, we design a provably more robust variant using a
post-processing strategy which allows us to deal with duplicates and near
duplicates in the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6697</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6697</id><created>2012-11-28</created><authors><author><keyname>Altug</keyname><forenames>Yucel</forenames></author><author><keyname>Wagner</keyname><forenames>Aaron B.</forenames></author></authors><title>Refinement of the Sphere-Packing Bound: Asymmetric Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a refinement of the sphere-packing bound for constant composition
codes over asymmetric discrete memoryless channels that improves the pre-factor
in front of the exponential term. The order of our pre-factor is
$\Omega(N^{-1/2(1+\epsilon + \rho_{R}^{\ast})})$ for any $\epsilon &gt;0$, where
$\rho_{R}^{\ast}$ is the maximum absolute-value subdifferential of the
sphere-packing exponent at rate $R$ and $N$ is the blocklength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6706</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6706</id><created>2012-11-28</created><authors><author><keyname>Michalski</keyname><forenames>Rafal</forenames></author></authors><title>Examining users preferences towards vertical graphical toolbars in
  simple search and point tasks</title><categories>cs.HC</categories><journal-ref>Computers in Human Behavior, 27(6) (2011), 2308-2321</journal-ref><doi>10.1016/j.chb.2011.07.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this study was to investigate the nature of preferences
and their relation to the objective measures in simple direct manipulation
tasks involving both the cognitive process as well as the visually guided
pointing activities. The conducted experiment was concerned with the graphical
structures resembling toolbars widely used in graphical interfaces. The
influence of the graphical panel location, panel configuration as well as the
target size on the user task efficiency and subjects' preferences were
examined. The participants were requested to express their attitudes towards
the tested panels before and after the efficiency examination. This subjective
evaluation was carried out within the framework of Analytic Hierarchy Process
(AHP; Saaty, 1977; 1980). The subjective results that were obtained showed
significant differences in the subjects' preferences towards examined panels
before and after completing the tasks. It seems that the users are able to
comparatively quickly change their minds after gaining some experience with the
investigated stimuli. Additionally, the applied cluster analysis revealed that
the subjects were not homogenous in their opinions, and they formed groups
having similar preference structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6709</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6709</id><created>2012-11-28</created><authors><author><keyname>Grobelny</keyname><forenames>Jerzy</forenames></author><author><keyname>Michalski</keyname><forenames>Rafal</forenames></author></authors><title>Various approaches to a human preference analysis in a digital signage
  display design</title><categories>cs.HC</categories><journal-ref>Human Factors and Ergonomics in Manufacturing &amp; Service
  Industries, 21(6), 529-542 (2011)</journal-ref><doi>10.1002/hfm.20295</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with various ways of analysing the subjective
assessment of displaying digital signage content. In the beginning the brief
description of the signage system evolution is described and next, the carried
out ex-periment is depicted. The preferences of the 32 subjects were obtained
using pairwise comparisons of the designed screen formats. Then the priorities
were derived by applying the Analytic Hierarchy Process (Saaty, 1977; 1980)
framework. The gathered data were modelled and analysed by means of the
analysis of variance, multiple regression, conjoint and factor analysis. The
results suggest that the application of different methods of preference
analysis may provide additional information, which could facilitate more
in-depth understanding of the given preference structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6711</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6711</id><created>2012-11-28</created><authors><author><keyname>Michalski</keyname><forenames>Rafal</forenames></author><author><keyname>Grobelny</keyname><forenames>Jerzy</forenames></author></authors><title>The role of colour preattentive processing in human-computer interaction
  task efficiency: a preliminary study</title><categories>cs.HC</categories><journal-ref>International Journal of Industrial Ergonomics, 38, 321-332 (2008)</journal-ref><doi>10.1016/j.ergon.2007.11.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, results of experimental research on the preattentive mechanism
in the human-computer interaction (HCI) were presented. Fifty four subjects
were asked to find interface elements from various panel structures. The
arrangements were differentiated by their orientation (vertical, horizontal),
colour pattern (ordered, unordered) and object background colours (green-blue,
green-red, blue-red). The main finding of the study generally confirms the
profits provided by the visual preattentive processing of the colour feature in
graphical panel operation efficiency. However, the vertical way of arranging
the items in search layouts resulted in decreasing the preattentive effect
related to the item background colour. In regular, chessboard-like patterns of
different coloured items, the effect of the early vision was less salient than
in the case of structures with randomly dispersed colours. The reported results
can help in designing efficient graphical user-computer interfaces in many
interactive information systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6712</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6712</id><created>2012-11-28</created><authors><author><keyname>Michalski</keyname><forenames>Rafal</forenames></author><author><keyname>Grobelny</keyname><forenames>Jerzy</forenames></author><author><keyname>Karwowski</keyname><forenames>Waldemar</forenames></author></authors><title>The effects of graphical interface design characteristics on
  human-computer interaction task efficiency</title><categories>cs.HC</categories><journal-ref>International Journal of Industrial Ergonomics, 36, 959-977 (2006)</journal-ref><doi>10.1016/j.ergon.2006.06.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper was to investigate the effects of a computer
screen interface design and its related geometrical characteristics of 36
graphical objects on a user's task efficiency. A total of 490 subjects took
part in laboratory experiments that focused on the direct manipulation of a
visual dialogue between a user and a computer. The subjects were asked to
select an object from among a group of items randomly placed on the computer
screen that were visible exclusively during the visual search process. A model
expressing the mean object acquisition time as a function of graphical object
size and the configuration was developed and statistically validated. The model
showed an influence of geometrical design characteristics of the graphical
objects (icons) and their groupings (icon structures) on the observed task
efficiency. The reported results can be used at those stages of a software
lifecycle that concentrate on prototyping, designing, and implementing
graphical solutions for the efficient graphical user-computer interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6715</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6715</id><created>2012-11-28</created><authors><author><keyname>McRae</keyname><forenames>George</forenames></author><author><keyname>Plessas</keyname><forenames>Demitri</forenames></author><author><keyname>Rafferty</keyname><forenames>Liam</forenames></author></authors><title>On the Concrete Categories of Graphs</title><categories>cs.DM</categories><comments>42 pages, 12 figures</comments><msc-class>05C60 (Primary) 05C99, 18B99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the standard Category of Graphs, the graphs allow only one edge to be
incident to any two vertices, not necessarily distinct, and the graph morphisms
must map edges to edges and vertices to vertices while preserving incidence. We
refer to these graph morphisms as Strict Morphisms. We relax the condition on
the graphs allowing any number of edges to be incident to any two vertices, as
well as relaxing the condition on graph morphisms by allowing edges to be
mapped to vertices, provided that incidence is still preserved. We call this
broader graph category The Category of Conceptual Graphs, and define four other
graph categories created by combinations of restrictions of the graph morphisms
as well as restrictions on the allowed graphs. We investigate which Lawvere
axioms for the category of Sets and Functions apply to each of these Categories
of Graphs, as well as the other categorial constructions of free objects,
projective objects, generators, and their categorial duals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6719</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6719</id><created>2012-11-28</created><authors><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Cooperative Sparsity Pattern Recovery in Distributed Networks Via
  Distributed-OMP</title><categories>cs.IT cs.NA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of collaboratively estimating the
sparsity pattern of a sparse signal with multiple measurement data in
distributed networks. We assume that each node makes Compressive Sensing (CS)
based measurements via random projections regarding the same sparse signal. We
propose a distributed greedy algorithm based on Orthogonal Matching Pursuit
(OMP), in which the sparse support is estimated iteratively while fusing
indices estimated at distributed nodes. In the proposed distributed framework,
each node has to perform less number of iterations of OMP compared to the
sparsity index of the sparse signal. Thus, with each node having a very small
number of compressive measurements, a significant performance gain in support
recovery is achieved via the proposed collaborative scheme compared to the case
where each node estimates the sparsity pattern independently and then fusion is
performed to get a global estimate. We further extend the algorithm to estimate
the sparsity pattern in a binary hypothesis testing framework, where the
algorithm first detects the presence of a sparse signal collaborating among
nodes with a fewer number of iterations of OMP and then increases the number of
iterations to estimate the sparsity pattern only if the signal is detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6724</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6724</id><created>2012-11-28</created><updated>2012-11-29</updated><authors><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Kaligounder</keyname><forenames>Lakshmi</forenames></author></authors><title>On Approximating Graph Bipartization via Node Deletion</title><categories>cs.CC cs.DM</categories><comments>Although the results are correct, it was pointed out that the results
  follow from some previously known results. Accordingly, this version of the
  paper is withdrawn by the authors</comments><msc-class>68Q25, 05C85</msc-class><acm-class>F.2.2; G.1.6; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the results are correct, it was pointed out that the results follow
from some previously known results. Accordingly, this version of the paper is
withdrawn by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6727</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6727</id><created>2012-11-28</created><authors><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Que</keyname><forenames>Qichao</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author><author><keyname>Zhou</keyname><forenames>Xueyuan</forenames></author></authors><title>Graph Laplacians on Singular Manifolds: Toward understanding complex
  spaces: graph Laplacians on manifolds with singularities and boundaries</title><categories>cs.AI cs.CG cs.LG</categories><journal-ref>JMLR W&amp;CP 23: 36.1 - 36.26, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, much of the existing work in manifold learning has been done under
the assumption that the data is sampled from a manifold without boundaries and
singularities or that the functions of interest are evaluated away from such
points. At the same time, it can be argued that singularities and boundaries
are an important aspect of the geometry of realistic data.
  In this paper we consider the behavior of graph Laplacians at points at or
near boundaries and two main types of other singularities: intersections, where
different manifolds come together and sharp &quot;edges&quot;, where a manifold sharply
changes direction. We show that the behavior of graph Laplacian near these
singularities is quite different from that in the interior of the manifolds. In
fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of
Fourier series, can be observed in the behavior of graph Laplacian near such
points. Unlike in the interior of the domain, where graph Laplacian converges
to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a
first-order differential operator, which exhibits different scaling behavior as
a function of the kernel width. One important implication is that while points
near the singularities occupy only a small part of the total volume, the
difference in scaling results in a disproportionately large contribution to the
total behavior. Another significant finding is that while the scaling behavior
of the operator is the same near different types of singularities, they are
very distinct at a more refined level of analysis.
  We believe that a comprehensive understanding of these structures in addition
to the standard case of a smooth manifold can take us a long way toward better
methods for analysis of complex non-linear data and can lead to significant
progress in algorithm design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6778</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6778</id><created>2012-11-28</created><authors><author><keyname>Ermakov</keyname><forenames>Sergei M.</forenames></author><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>Efficient parallel algorithms for tandem queueing system simulation</title><categories>math.NA cs.DC cs.SY</categories><comments>The 3rd Beijing International Conference on System Simulation and
  Scientific Computing, October 17-19, 1995, Beijing, China</comments><msc-class>68U20 (Primary), 65Y05, 37M05, 68M20, 90B22</msc-class><journal-ref>Proceedings of the 3rd Beijing International Conference on System
  Simulation and Scientific Computing: Delayed papers / Ed. by Xingren Wang et
  al. International Academic Publishers, 1995, pp. 8-12</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel algorithms designed for simulation and performance evaluation of
single-server tandem queueing systems with both infinite and finite buffers are
presented. The algorithms exploit a simple computational procedure based on
recursive equations as a representation of system dynamics. A brief analysis of
the performance of the algorithms are given to show that they involve low time
and memory requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6781</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6781</id><created>2012-11-28</created><authors><author><keyname>Tyszkiewicz</keyname><forenames>Jerzy</forenames></author><author><keyname>Balson</keyname><forenames>Dermot</forenames></author></authors><title>User Defined Spreadsheet Functions in Excel</title><categories>cs.SE</categories><comments>11 Pages, 9 B&amp;W &amp; Colour Figures</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating user defined functions (UDFs) is a powerful method to improve the
quality of computer applications, in particular spreadsheets. However, the only
direct way to use UDFs in spreadsheets is to switch from the functional and
declarative style of spreadsheet formulas to the imperative VBA, which creates
a high entry barrier even for proficient spreadsheet users. It has been
proposed to extend Excel by UDFs declared by a spreadsheet: user defined
spreadsheet functions (UDSFs). In this paper we present a method to create a
limited form of UDSFs in Excel without any use of VBA. Calls to those UDSFs
utilize what-if data tables to execute the same part of a worksheet several
times, thus turning it into a reusable function definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6786</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6786</id><created>2012-11-28</created><updated>2014-11-20</updated><authors><author><keyname>Jiang</keyname><forenames>Tian-Yi</forenames></author><author><keyname>Scully</keyname><forenames>Ziv</forenames></author><author><keyname>Zhang</keyname><forenames>Yan X</forenames></author></authors><title>Motors and Impossible Firing Patterns in the Parallel Chip-Firing Game</title><categories>math.CO cs.DM math.DS</categories><comments>19 pages; added higher-level explanation of main theorem's proof,
  typo corrections</comments><msc-class>37B15, 82C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel chip-firing game is an automaton on graphs in which vertices
&quot;fire&quot; chips to their neighbors when they have enough chips to do so. The game
is always periodic, and we concern ourselves with the firing sequences of
vertices. We introduce the concepts of motorized parallel chip-firing games and
motor vertices, study the effects of motors connected to a tree and show that
motorized games can be transformed into ordinary games if the motors' firing
sequences occur in some ordinary game. We then characterize exactly which
periodic firing patterns can occur in an ordinary game and state some
implications of the finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6799</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6799</id><created>2012-11-28</created><authors><author><keyname>Weng</keyname><forenames>Lilian</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Context Visualization for Social Bookmark Management</title><categories>cs.HC cs.IR</categories><comments>11 pages, 3 figures, 1 table</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present the design of a new social bookmark manager, named GalViz, as part
of the interface of the GiveA-Link system. Unlike the interfaces of traditional
social tagging tools, which usually display information in a list view, GalViz
visualizes tags, resources, social links, and social context in an interactive
network, combined with the tag cloud. Evaluations through a scenario case study
and log analysis provide evidence of the effectiveness of our design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6807</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6807</id><created>2012-11-28</created><updated>2013-09-23</updated><authors><author><keyname>Kim</keyname><forenames>Sungmin</forenames></author><author><keyname>Shi</keyname><forenames>Tao</forenames></author></authors><title>Scalable Spectral Algorithms for Community Detection in Directed
  Networks</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>Single column, 40 pages, 6 figures and 7 tables</comments><msc-class>62H30, 91C20, 91D30, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection has been one of the central problems in network studies
and directed network is particularly challenging due to asymmetry among its
links. In this paper, we found that incorporating the direction of links
reveals new perspectives on communities regarding to two different roles,
source and terminal, that a node plays in each community. Intriguingly, such
communities appear to be connected with unique spectral property of the graph
Laplacian of the adjacency matrix and we exploit this connection by using
regularized SVD methods. We propose harvesting algorithms, coupled with
regularized SVDs, that are linearly scalable for efficient identification of
communities in huge directed networks. The proposed algorithm shows great
performance and scalability on benchmark networks in simulations and
successfully recovers communities in real network applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6821</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6821</id><created>2012-11-29</created><updated>2014-01-07</updated><authors><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Du</keyname><forenames>Guangxun</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Additive-State-Decomposition Dynamic Inversion Stabilized Control for a
  Class of Uncertain MIMO Systems</title><categories>cs.SY</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new control, namely additive-state-decomposition
dynamic inversion stabilized control, that is used to stabilize a class of
multi-input multi-output (MIMO) systems subject to nonparametric time-varying
uncertainties with respect to both state and input. By additive state
decomposition and a new definition of output, the considered uncertain system
is transformed into a minimum-phase uncertainty-free system with relative
degree one, in which all uncertainties are lumped into a new disturbance at the
output. Subsequently, dynamic inversion control is applied to reject the lumped
disturbance. Performance analysis of the resulting closed-loop dynamics shows
that the stability can be ensured. Finally, to demonstrate its effectiveness,
the proposed control is applied to two existing problems by numerical
simulation. Furthermore, in order to show its practicability, the proposed
control is also performed on a real quadrotor to stabilize its attitude when
its inertia moment matrix is subject to a large uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6822</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6822</id><created>2012-11-29</created><updated>2013-01-18</updated><authors><author><keyname>Koyama</keyname><forenames>Tamio</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>Calculation of orthant probabilities by the holonomic gradient method</title><categories>cs.NA</categories><comments>17 pages</comments><msc-class>32C38, 16S32, 62H10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the holonomic gradient method (HGM) introduced by [9] to the
calculation of orthant probabilities of multivariate normal distribution. The
holonomic gradient method applied to orthant probabilities is found to be a
variant of Plackett's recurrence relation ([14]). However an implementation of
the method yields recurrence relations more suitable for numerical computation
than Plackett's recurrence relation. We derive some theoretical results on the
holonomic system for the orthant probabilities. These results show that
multivariate normal orthant probabilities possess some remarkable properties
from the viewpoint of holonomic systems. Finally we show that numerical
performance of our method is comparable or superior compared to existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6827</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6827</id><created>2012-11-29</created><authors><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Additive-State-Decomposition-Based Tracking Control for TORA Benchmark</title><categories>cs.SY</categories><comments>19 pages</comments><doi>10.1016/j.jsv.2013.04.033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new control scheme, called additive state decomposition
based tracking control, is proposed to solve the tracking (rejection) problem
for rotational position of the TORA (a nonlinear nonminimum phase system). By
the additive state decomposition, the tracking (rejection) task for the
considered nonlinear system is decomposed into two independent subtasks: a
tracking (rejection) subtask for a linear time invariant (LTI) system, leaving
a stabilization subtask for a derived nonlinear system. By the decomposition,
the proposed tracking control scheme avoids solving regulation equations and
can tackle the tracking (rejection) problem in the presence of any external
signal (except for the frequencies at +1 or -1) generated by a marginally
stable autonomous LTI system. To demonstrate the effectiveness, numerical
simulation is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6834</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6834</id><created>2012-11-29</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Huang</keyname><forenames>Ting</forenames></author><author><keyname>Zhu</keyname><forenames>Peijun</forenames></author></authors><title>On unbiased performance evaluation for protein inference</title><categories>stat.AP cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter is a response to the comments of Serang (2012) on Huang and He
(2012) in Bioinformatics. Serang (2012) claimed that the parameters for the
Fido algorithm should be specified using the grid search method in Serang et
al. (2010) so as to generate a deserved accuracy in performance comparison. It
seems that it is an argument on parameter tuning. However, it is indeed the
issue of how to conduct an unbiased performance evaluation for comparing
different protein inference algorithms. In this letter, we would explain why we
don't use the grid search for parameter selection in Huang and He (2012) and
show that this procedure may result in an over-estimated performance that is
unfair to competing algorithms. In fact, this issue has also been pointed out
by Li and Radivojac (2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6839</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6839</id><created>2012-11-29</created><authors><author><keyname>Cardillo</keyname><forenames>Alessio</forenames></author><author><keyname>Zanin</keyname><forenames>Massimiliano</forenames></author><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Romance</keyname><forenames>Miguel</forenames></author><author><keyname>del Amo</keyname><forenames>Alejandro J. Garc&#xed;a</forenames></author><author><keyname>Boccaletti</keyname><forenames>Stefano</forenames></author></authors><title>Modeling the Multi-layer Nature of the European Air Transport Network:
  Resilience and Passengers Re-scheduling under random failures</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 5 figures - Accepted for publication in European Physical
  Journal Special Topics</comments><journal-ref>Eur. Phys. J. Special Topics 215, 23-33 (2013)</journal-ref><doi>10.1140/epjst/e2013-01712-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamics of the European Air Transport Network by using a
multiplex network formalism. We will consider the set of flights of each
airline as an interdependent network and we analyze the resilience of the
system against random flight failures in the passenger's rescheduling problem.
A comparison between the single-plex approach and the corresponding multiplex
one is presented illustrating that the multiplexity strongly affects the
robustness of the European Air Network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6847</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6847</id><created>2012-11-29</created><authors><author><keyname>Ycart</keyname><forenames>Bernard</forenames><affiliation>LJK</affiliation></author></authors><title>Letter counting: a stem cell for Cryptology, Quantitative Linguistics,
  and Statistics</title><categories>math.HO cs.CL cs.CR</categories><proxy>ccsd</proxy><journal-ref>Historiographia Linguistica 40, 3 (2013) 303-329</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting letters in written texts is a very ancient practice. It has
accompanied the development of Cryptology, Quantitative Linguistics, and
Statistics. In Cryptology, counting frequencies of the different characters in
an encrypted message is the basis of the so called frequency analysis method.
In Quantitative Linguistics, the proportion of vowels to consonants in
different languages was studied long before authorship attribution. In
Statistics, the alternation vowel-consonants was the only example that Markov
ever gave of his theory of chained events. A short history of letter counting
is presented. The three domains, Cryptology, Quantitative Linguistics, and
Statistics, are then examined, focusing on the interactions with the other two
fields through letter counting. As a conclusion, the eclectism of past
centuries scholars, their background in humanities, and their familiarity with
cryptograms, are identified as contributing factors to the mutual enrichment
process which is described here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6851</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6851</id><created>2012-11-29</created><authors><author><keyname>N'Cir</keyname><forenames>Chiheb-Eddine Ben</forenames></author><author><keyname>Essoussi</keyname><forenames>Nadia</forenames></author></authors><title>Classification Recouvrante Bas\'ee sur les M\'ethodes \`a Noyau</title><categories>cs.LG stat.CO stat.ME stat.ML</categories><comments>Les 43\`emes Journ\'ees de Statistique</comments><journal-ref>Les 43\`emes Journ\'ees de Statistique 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overlapping clustering problem is an important learning issue in which
clusters are not mutually exclusive and each object may belongs simultaneously
to several clusters. This paper presents a kernel based method that produces
overlapping clusters on a high feature space using mercer kernel techniques to
improve separability of input patterns. The proposed method, called
OKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping
$k$-means) method to produce overlapping schemes. Experiments are performed on
overlapping dataset and empirical results obtained with OKM-K outperform
results obtained with OKM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6859</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6859</id><created>2012-11-29</created><authors><author><keyname>N'Cir</keyname><forenames>Chiheb-Eddine Ben</forenames></author><author><keyname>Essoussi</keyname><forenames>Nadia</forenames></author><author><keyname>Bertrand</keyname><forenames>Patrice</forenames></author></authors><title>Overlapping clustering based on kernel similarity metric</title><categories>stat.ML cs.LG stat.ME</categories><comments>Second Meeting on Statistics and Data Mining 2010</comments><journal-ref>Second Meeting on Statistics and Data Mining Second Meeting on
  Statistics and Data Mining March 11-12, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Producing overlapping schemes is a major issue in clustering. Recent proposed
overlapping methods relies on the search of an optimal covering and are based
on different metrics, such as Euclidean distance and I-Divergence, used to
measure closeness between observations. In this paper, we propose the use of
another measure for overlapping clustering based on a kernel similarity metric
.We also estimate the number of overlapped clusters using the Gram matrix.
Experiments on both Iris and EachMovie datasets show the correctness of the
estimation of number of clusters and show that measure based on kernel
similarity metric improves the precision, recall and f-measure in overlapping
clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6868</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6868</id><created>2012-11-29</created><updated>2013-09-04</updated><authors><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Simultaneous Information and Power Transfer for Broadband Wireless
  Systems</title><categories>cs.IT math.IT</categories><comments>This paper has been presented in part at IEEE ICASSP 2013. The full
  version will appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2281026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Far-field microwave power transfer (MPT) will free wireless sensors and other
mobile devices from the constraints imposed by finite battery capacities.
Integrating MPT with wireless communications to support simultaneous
information and power transfer (SIPT) allows the same spectrum to be used for
dual purposes without compromising the quality of service. A novel approach is
presented in this paper for realizing SIPT in a broadband system where
orthogonal frequency division multiplexing and transmit beamforming are
deployed to create a set of parallel sub-channels for SIPT, which simplifies
resource allocation. Supported by a proposed reconfigurable mobile
architecture, different system configurations are considered by combining
single-user/multiuser systems, downlink/uplink information transfer, and
variable/fixed coding rates. Optimizing the power control for these
configurations results in a new class of multiuser power-control problems
featuring circuit-power constraints, specifying that the transferred power must
be sufficiently large to support the operation of the receiver circuitry.
Solving these problems gives a set of power-control algorithms that exploit
channel diversity in frequency for simultaneously enhancing the throughput and
the MPT efficiency. For the system configurations with variable coding rates,
the algorithms are variants of water-filling that account for the circuit-power
constraints. The optimal algorithms for those configurations with fixed coding
rates are shown to sequentially allocate mobiles their required power for
decoding in the ascending order until the entire budgeted power is spent. The
required power for a mobile is derived as simple functions of the minimum
signal-to-noise ratio for correct decoding, the circuit power and sub-channel
gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6887</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6887</id><created>2012-11-29</created><authors><author><keyname>Mi&#x142;kowski</keyname><forenames>Marcin</forenames></author></authors><title>Automating rule generation for grammar checkers</title><categories>cs.CL cs.LG</categories><comments>Draft of the chapter published In: Explorations Across Languages and
  Corpora. PALC 2009, ed. by S. Go\'zd\'z-Roszkowski, Peter Lang, 2011, p.
  123-133</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I describe several approaches to automatic or semi-automatic
development of symbolic rules for grammar checkers from the information
contained in corpora. The rules obtained this way are an important addition to
manually-created rules that seem to dominate in rule-based checkers. However,
the manual process of creation of rules is costly, time-consuming and
error-prone. It seems therefore advisable to use machine-learning algorithms to
create the rules automatically or semi-automatically. The results obtained seem
to corroborate my initial hypothesis that symbolic machine learning algorithms
can be useful for acquiring new rules for grammar checking. It turns out,
however, that for practical uses, error corpora cannot be the sole source of
information used in grammar checking. I suggest therefore that only by using
different approaches, grammar-checkers, or more generally, computer-aided
proofreading tools, will be able to cover most frequent and severe mistakes and
avoid false alarms that seem to distract users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6898</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6898</id><created>2012-11-29</created><authors><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Lesner</keyname><forenames>Boris</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>On the Use of Non-Stationary Policies for Stationary Infinite-Horizon
  Markov Decision Processes</title><categories>cs.LG cs.AI</categories><proxy>ccsd</proxy><journal-ref>NIPS 2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider infinite-horizon stationary $\gamma$-discounted Markov Decision
Processes, for which it is known that there exists a stationary optimal policy.
Using Value and Policy Iteration with some error $\epsilon$ at each iteration,
it is well-known that one can compute stationary policies that are
$\frac{2\gamma}{(1-\gamma)^2}\epsilon$-optimal. After arguing that this
guarantee is tight, we develop variations of Value and Policy Iteration for
computing non-stationary policies that can be up to
$\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significant
improvement in the usual situation when $\gamma$ is close to 1. Surprisingly,
this shows that the problem of &quot;computing near-optimal non-stationary policies&quot;
is much simpler than that of &quot;computing near-optimal stationary policies&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6918</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6918</id><created>2012-11-29</created><authors><author><keyname>Seidl</keyname><forenames>Mathis</forenames></author><author><keyname>Schenk</keyname><forenames>Andreas</forenames></author><author><keyname>Stierstorfer</keyname><forenames>Clemens</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Aspects of Polar-Coded Modulation</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at International ITG Conference on Systems,
  Communications and Coding, Munich, Germany, January 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the joint design of polar coding and higher-order modulation
schemes for ever increased spectral efficiency. The close connection between
the polar code construction and the multi-level coding approach is described in
detail. Relations between different modulation schemes such as bit-interleaved
coded modulation (BICM) and multi-level coding (MLC) in case of polar-coded
modulation as well as the influence of the applied labeling rule and the
selection of frozen channels are demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6940</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6940</id><created>2012-11-29</created><updated>2015-09-04</updated><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Choice Disjunctive Queries in Logic Programming</title><categories>cs.LO cs.PL</categories><comments>8 pages. Implementation scheme from the previous version contains an
  error. A new, correct implementation scheme is described</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the long-standing research problems on logic programming is to treat
the cut predicate in a logical, high-level way. We argue that this problem can
be solved by adopting linear logic and choice-disjunctive goal formulas of the
form $G_0 \add G_1$ where $G_0, G_1$ are goals. These goals have the following
intended semantics: $choose$ the true disjunct $G_i$ and execute $G_i$ where $i
(= 0\ {\rm or}\ 1)$, while $discarding$ the unchosen disjunct. Note that only
one goal can remain alive during execution. These goals thus allow us to
specify mutually exclusive tasks in a high-level way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6950</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6950</id><created>2012-11-29</created><authors><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Rajawat</keyname><forenames>Ketan</forenames></author></authors><title>Dynamic Network Cartography</title><categories>cs.NI cs.IT cs.MA math.IT stat.ML</categories><comments>To appear in the IEEE Signal Processing Magazine - Special Issue on
  Adaptation and Learning over Complex Networks</comments><doi>10.1109/MSP.2012.2232355</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication networks have evolved from specialized, research and tactical
transmission systems to large-scale and highly complex interconnections of
intelligent devices, increasingly becoming more commercial, consumer-oriented,
and heterogeneous. Propelled by emergent social networking services and
high-definition streaming platforms, network traffic has grown explosively
thanks to the advances in processing speed and storage capacity of
state-of-the-art communication technologies. As &quot;netizens&quot; demand a seamless
networking experience that entails not only higher speeds, but also resilience
and robustness to failures and malicious cyber-attacks, ample opportunities for
signal processing (SP) research arise. The vision is for ubiquitous smart
network devices to enable data-driven statistical learning algorithms for
distributed, robust, and online network operation and management, adaptable to
the dynamically-evolving network landscape with minimal need for human
intervention. The present paper aims at delineating the analytical background
and the relevance of SP tools to dynamic network monitoring, introducing the SP
readership to the concept of dynamic network cartography -- a framework to
construct maps of the dynamic network state in an efficient and scalable manner
tailored to large-scale heterogeneous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6969</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6969</id><created>2012-11-29</created><updated>2013-07-03</updated><authors><author><keyname>Ramezanpour</keyname><forenames>A.</forenames></author></authors><title>Computing loop corrections by message passing</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DS</categories><comments>12 pages, 4 figures, a bit expanded and typos corrected</comments><journal-ref>Phys. Rev. E 87, 060103(R) (2013)</journal-ref><doi>10.1103/PhysRevE.87.060103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any spanning tree in a loopy interaction graph can be used for communicating
the effect of the loopy interactions by introducing messages that are passed
along the edges in the spanning tree. This defines an exact mapping of the
problem on the loopy interaction graph onto an extended problem on a tree
interaction graph, where the thermodynamic quantities can be computed by a
message-passing algorithm based on the Bethe equations. We propose an
approximation loop correction algorithm for the Ising model relying on the
above representation of the problem. The algorithm deals at the same time with
the short and long loops, and can be used to obtain upper and lower bounds for
the free energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6971</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6971</id><created>2012-11-29</created><authors><author><keyname>Qaffou</keyname><forenames>Issam</forenames></author><author><keyname>Sadgal</keyname><forenames>Mohamed</forenames></author><author><keyname>Elfazziki</keyname><forenames>Aziz</forenames></author></authors><title>A New Automatic Method to Adjust Parameters for Object Recognition</title><categories>cs.CV cs.AI</categories><journal-ref>IJACSA 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To recognize an object in an image, the user must apply a combination of
operators, where each operator has a set of parameters. These parameters must
be well adjusted in order to reach good results. Usually, this adjustment is
made manually by the user. In this paper we propose a new method to automate
the process of parameter adjustment for an object recognition task. Our method
is based on reinforcement learning, we use two types of agents: User Agent that
gives the necessary information and Parameter Agent that adjusts the parameters
of each operator. Due to the nature of reinforcement learning the results do
not depend only on the system characteristics but also on the user favorite
choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6984</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6984</id><created>2012-11-29</created><updated>2014-04-05</updated><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author></authors><title>New Approach for CCA2-Secure Post-Quantum Cryptosystem Using Knapsack
  Problem</title><categories>cs.CR cs.IT math.IT</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chosen-ciphertext security, which guarantees confidentiality of encrypted
messages even in the presence of a decryption oracle, has become the defacto
notion of security for public-key encryption under active attack. In this
manuscript, for the first time, we propose a new approach for constructing
post-quantum cryptosystems secure against adaptive chosen ciphertext attack
(CCA2-secure) in the standard model using the knapsack problem. The
computational version of the knapsack problem is NP-hard. Thus, this problem is
expected to be difficult to solve using quantum computers. Our construction is
a precoding-based encryption algorithm and uses the knapsack problem to perform
a permutation and pad some random fogged data to the message bits. Compared to
other approaches were introduced today, our approach is very simple and more
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6988</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6988</id><created>2012-11-29</created><authors><author><keyname>Meyer</keyname><forenames>Florian</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Hlinka</keyname><forenames>Ondrej</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>Simultaneous Distributed Sensor Self-Localization and Target Tracking
  Using Belief Propagation and Likelihood Consensus</title><categories>cs.NI cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the framework of cooperative simultaneous localization and
tracking (CoSLAT), which provides a consistent combination of cooperative
self-localization (CSL) and distributed target tracking (DTT) in sensor
networks without a fusion center. CoSLAT extends simultaneous localization and
tracking (SLAT) in that it uses also intersensor measurements. Starting from a
factor graph formulation of the CoSLAT problem, we develop a particle-based,
distributed message passing algorithm for CoSLAT that combines nonparametric
belief propagation with the likelihood consensus scheme. The proposed CoSLAT
algorithm improves on state-of-the-art CSL and DTT algorithms by exchanging
probabilistic information between CSL and DTT. Simulation results demonstrate
substantial improvements in both self-localization and tracking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6989</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6989</id><created>2012-11-29</created><updated>2013-10-18</updated><authors><author><keyname>Farrell</keyname><forenames>Patrick E.</forenames></author><author><keyname>Cotter</keyname><forenames>Colin J.</forenames></author><author><keyname>Funke</keyname><forenames>Simon W.</forenames></author></authors><title>A framework for the automation of generalised stability theory</title><categories>cs.MS</categories><comments>Accepted in SISC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional approach to investigating the stability of a physical system
is to linearise the equations about a steady base solution, and to examine the
eigenvalues of the linearised operator. Over the past several decades, it has
been recognised that this approach only determines the asymptotic stability of
the system, and neglects the possibility of transient perturbation growth
arising due to the nonnormality of the system. This observation motivated the
development of a more powerful generalised stability theory (GST), which
focusses instead on the singular value decomposition of the linearised
propagator of the system. While GST has had significant successes in
understanding the stability of phenomena in geophysical fluid dynamics, its
more widespread applicability has been hampered by the fact that computing the
SVD requires both the tangent linear operator and its adjoint: deriving the
tangent linear and adjoint models is usually a considerable challenge, and
manually embedding them inside an eigensolver is laborious. In this paper, we
present a framework for the automation of generalised stability theory, which
overcomes these difficulties. Given a compact high-level symbolic
representation of a finite element discretisation implemented in the FEniCS
system, efficient C++ code is automatically generated to assemble the forward,
tangent linear and adjoint models; these models are then used to calculate the
optimally growing perturbations to the forward model, and their growth rates.
By automating the stability computations, we hope to make these powerful tools
a more routine part of computational analysis. The efficiency and generality of
the framework is demonstrated with applications drawn from geophysical fluid
dynamics, phase separation and quantum mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.6997</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.6997</id><created>2012-11-29</created><updated>2012-11-30</updated><authors><author><keyname>Dani</keyname><forenames>Varsha</forenames></author><author><keyname>Diaz</keyname><forenames>Josep</forenames></author><author><keyname>Hayes</keyname><forenames>Thomas</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>The Power of Choice for Random Satisfiability</title><categories>cs.CC math.CO</categories><comments>typo fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Achlioptas processes for k-SAT formulas. We create a semi-random
formula with n variables and m clauses, where each clause is a choice, made
on-line, between two or more uniformly random clauses. Our goal is to delay the
satisfiability/unsatisfiability transition, keeping the formula satisfiable up
to densities m/n beyond the satisfiability threshold alpha_k for random k-SAT.
We show that three choices suffice to raise the threshold for any k &gt;= 3, and
that two choices suffice for all 3 &lt;= k &lt;= 25. We also show that two choices
suffice to lower the threshold for all k &gt;= 3, making the formula unsatisfiable
at a density below alpha_k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7012</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7012</id><created>2012-11-29</created><updated>2014-10-26</updated><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Learning-Assisted Automated Reasoning with Flyspeck</title><categories>cs.AI cs.DL cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The considerable mathematical knowledge encoded by the Flyspeck project is
combined with external automated theorem provers (ATPs) and machine-learning
premise selection methods trained on the proofs, producing an AI system capable
of answering a wide range of mathematical queries automatically. The
performance of this architecture is evaluated in a bootstrapping scenario
emulating the development of Flyspeck from axioms to the last theorem, each
time using only the previous theorems and proofs. It is shown that 39% of the
14185 theorems could be proved in a push-button mode (without any high-level
advice and user interaction) in 30 seconds of real time on a fourteen-CPU
workstation. The necessary work involves: (i) an implementation of sound
translations of the HOL Light logic to ATP formalisms: untyped first-order,
polymorphic typed first-order, and typed higher-order, (ii) export of the
dependency information from HOL Light and ATP proofs for the machine learners,
and (iii) choice of suitable representations and methods for learning from
previous proofs, and their integration as advisors with HOL Light. This work is
described and discussed here, and an initial analysis of the body of proofs
that were found fully automatically is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7020</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7020</id><created>2012-11-29</created><authors><author><keyname>Devillers</keyname><forenames>Olivier</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Glisse</keyname><forenames>Marc</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Goaoc</keyname><forenames>Xavier</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Reitzner</keyname><forenames>Matthias</forenames></author></authors><title>The monotonicity of f-vectors of random polytopes</title><categories>cs.CG math.MG</categories><proxy>ccsd</proxy><report-no>RR-8154</report-no><journal-ref>N&amp;deg; RR-8154 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let K be a compact convex body in Rd, let Kn be the convex hull of n points
chosen uniformly and independently in K, and let fi(Kn) denote the number of
i-dimensional faces of Kn. We show that for planar convex sets, E(f0(Kn)) is
increasing in n. In dimension d&gt;=3 we prove that if lim(E((f[d
-1](Kn))/(An^c)-&gt;1 when n-&gt;infinity for some constants A and c &gt; 0 then the
function E(f[d-1](Kn)) is increasing for n large enough. In particular, the
number of facets of the convex hull of n random points distributed uniformly
and independently in a smooth compact convex body is asymptotically increasing.
Our proof relies on a random sampling argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7045</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7045</id><created>2012-11-29</created><updated>2013-04-10</updated><authors><author><keyname>Wang</keyname><forenames>Lanhui</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author><author><keyname>Wen</keyname><forenames>Zaiwen</forenames></author></authors><title>Orientation Determination from Cryo-EM images Using Least Unsquared
  Deviation</title><categories>cs.LG math.NA math.OC q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge in single particle reconstruction from cryo-electron
microscopy is to establish a reliable ab-initio three-dimensional model using
two-dimensional projection images with unknown orientations. Common-lines based
methods estimate the orientations without additional geometric information.
However, such methods fail when the detection rate of common-lines is too low
due to the high level of noise in the images. An approximation to the least
squares global self consistency error was obtained using convex relaxation by
semidefinite programming. In this paper we introduce a more robust global self
consistency error and show that the corresponding optimization problem can be
solved via semidefinite relaxation. In order to prevent artificial clustering
of the estimated viewing directions, we further introduce a spectral norm term
that is added as a constraint or as a regularization term to the relaxed
minimization problem. The resulted problems are solved by using either the
alternating direction method of multipliers or an iteratively reweighted least
squares procedure. Numerical experiments with both simulated and real images
demonstrate that the proposed methods significantly reduce the orientation
estimation error when the detection rate of common-lines is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7046</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7046</id><created>2012-11-29</created><updated>2014-02-16</updated><authors><author><keyname>Miller</keyname><forenames>Ezra</forenames></author><author><keyname>Owen</keyname><forenames>Megan</forenames></author><author><keyname>Provan</keyname><forenames>J. Scott</forenames></author></authors><title>Polyhedral computational geometry for averaging metric phylogenetic
  trees</title><categories>math.MG cs.CG math.CO math.ST q-bio.PE stat.TH</categories><comments>43 pages, 6 figures; v2: fixed typos, shortened Sections 1 and 5,
  added counter example for polyhedrality of vistal subdivision in general
  CAT(0) cubical complexes; v1: 43 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the computational geometry relevant to calculations
of the Frechet mean and variance for probability distributions on the
phylogenetic tree space of Billera, Holmes and Vogtmann, using the theory of
probability measures on spaces of nonpositive curvature developed by Sturm. We
show that the combinatorics of geodesics with a specified fixed endpoint in
tree space are determined by the location of the varying endpoint in a certain
polyhedral subdivision of tree space. The variance function associated to a
finite subset of tree space has a fixed $C^\infty$ algebraic formula within
each cell of the corresponding subdivision, and is continuously differentiable
in the interior of each orthant of tree space. We use this subdivision to
establish two iterative methods for producing sequences that converge to the
Frechet mean: one based on Sturm's Law of Large Numbers, and another based on
descent algorithms for finding optima of smooth functions on convex polyhedra.
We present properties and biological applications of Frechet means and extend
our main results to more general globally nonpositively curved spaces composed
of Euclidean orthants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7052</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7052</id><created>2012-11-29</created><updated>2013-10-22</updated><authors><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author></authors><title>Quantifying the effect of temporal resolution on time-varying networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><journal-ref>Scientific Reports 3, 3006 (2013)</journal-ref><doi>10.1038/srep03006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-varying networks describe a wide array of systems whose constituents and
interactions evolve over time. They are defined by an ordered stream of
interactions between nodes, yet they are often represented in terms of a
sequence of static networks, each aggregating all edges and nodes present in a
time interval of size \Delta t. In this work we quantify the impact of an
arbitrary \Delta t on the description of a dynamical process taking place upon
a time-varying network. We focus on the elementary random walk, and put forth a
simple mathematical framework that well describes the behavior observed on real
datasets. The analytical description of the bias introduced by time integrating
techniques represents a step forward in the correct characterization of
dynamical processes on time-varying graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7075</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7075</id><created>2012-11-29</created><authors><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Ma</keyname><forenames>Jianfeng</forenames></author><author><keyname>Shi</keyname><forenames>Weisong</forenames></author></authors><title>Secure and Reliable Transmission with Cooperative Relays in Two-Hop
  Wireless Networks</title><categories>cs.NI cs.CR cs.IT math.IT</categories><comments>6 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This work considers the secure and reliable information transmission in
two-hop relay wireless networks without the information of both eavesdropper
channels and locations. While the previous work on this problem mainly studied
infinite networks and their asymptotic behavior and scaling law results, this
papers focuses on a more practical network with finite number of system nodes
and explores the corresponding exact results on the number of eavesdroppers the
network can tolerant to ensure a desired secrecy and reliability. For achieving
secure and reliable information transmission in a finite network, two
transmission protocols are considered in this paper, one adopts an optimal but
complex relay selection process with less load balance capacity while the other
adopts a random but simple relay selection process with good load balance
capacity. Theoretical analysis is further provided to determine the exact and
maximum number of independent and also uniformly distributed eavesdroppers one
network can tolerate to satisfy a specified requirement in terms of the maximum
secrecy outage probability and maximum transmission outage probability allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7080</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7080</id><created>2012-11-29</created><authors><author><keyname>Kovalchuk</keyname><forenames>Sergey V.</forenames></author><author><keyname>Smirnov</keyname><forenames>Pavel A.</forenames></author><author><keyname>Kosukhin</keyname><forenames>Sergey S.</forenames></author><author><keyname>Boukhanovsky</keyname><forenames>Alexander V.</forenames></author></authors><title>Virtual Simulation Objects Concept as a Framework for System-Level
  Simulation</title><categories>cs.SY cs.HC cs.SE</categories><comments>Proceedings of IEEE e-Sceince Conference. 2012. CD-ROM. ISBN
  978-1-4673-4465-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Virtual Simulation Objects (VSO) concept which forms
theoretical basis for building tools and framework that is developed for
system-level simulations using existing software modules available within
cyber-infrastructure. Presented concept is implemented by the software tool for
building composite solutions using VSO-based GUI and running them using CLAVIRE
simulation environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7089</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7089</id><created>2012-11-29</created><updated>2014-04-27</updated><authors><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>The Convergence Guarantees of a Non-convex Approach for Sparse Recovery</title><categories>cs.IT math.IT</categories><comments>33 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of sparse recovery, numerous researches hint that non-convex
penalties might induce better sparsity than convex ones, but up until now those
corresponding non-convex algorithms lack convergence guarantees from the
initial solution to the global optimum. This paper aims to provide performance
guarantees of a non-convex approach for sparse recovery. Specifically, the
concept of weak convexity is incorporated into a class of sparsity-inducing
penalties to characterize the non-convexity. Borrowing the idea of the
projected subgradient method, an algorithm is proposed to solve the non-convex
optimization problem. In addition, a uniform approximate projection is adopted
in the projection step to make this algorithm computationally tractable for
large scale problems. The convergence analysis is provided in the noisy
scenario. It is shown that if the non-convexity of the penalty is below a
threshold (which is in inverse proportion to the distance between the initial
solution and the sparse signal), the recovered solution has recovery error
linear in both the step size and the noise term. Numerical simulations are
implemented to test the performance of the proposed approach and verify the
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7100</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7100</id><created>2012-11-29</created><authors><author><keyname>Ferreira</keyname><forenames>Miguel A.</forenames></author><author><keyname>Visser</keyname><forenames>Joost</forenames></author></authors><title>Governance of Spreadsheets through Spreadsheet Change Reviews</title><categories>cs.SE</categories><comments>13 Pages, 1 Figure, 1 Table; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2012, ISBN: 978-0-9569258-6-2</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a pragmatic method for management of risks that arise due to
spreadsheet use in large organizations. We combine peer-review, tool-assisted
evaluation and other pre-existing approaches into a single organization-wide
approach that reduces spreadsheet risk without overly restricting spreadsheet
use. The method was developed in the course of several spreadsheet evaluation
assignments for a corporate customer. Our method addresses a number of issues
pertinent to spreadsheet risks that were raised by the Sarbanes-Oxley act.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7102</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7102</id><created>2012-11-29</created><authors><author><keyname>Sadek</keyname><forenames>Rowayda A.</forenames></author></authors><title>SVD Based Image Processing Applications: State of The Art, Contributions
  and Research Challenges</title><categories>cs.CV cs.MM</categories><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 7, 2012 26-34</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singular Value Decomposition (SVD) has recently emerged as a new paradigm for
processing different types of images. SVD is an attractive algebraic transform
for image processing applications. The paper proposes an experimental survey
for the SVD as an efficient transform in image processing applications. Despite
the well-known fact that SVD offers attractive properties in imaging, the
exploring of using its properties in various image applications is currently at
its infancy. Since the SVD has many attractive properties have not been
utilized, this paper contributes in using these generous properties in newly
image applications and gives a highly recommendation for more research
challenges. In this paper, the SVD properties for images are experimentally
presented to be utilized in developing new SVD-based image processing
applications. The paper offers survey on the developed SVD based image
applications. The paper also proposes some new contributions that were
originated from SVD properties analysis in different image processing. The aim
of this paper is to provide a better understanding of the SVD in image
processing and identify important various applications and open research
directions in this increasingly important area; SVD based image processing in
the future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7104</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7104</id><created>2012-11-29</created><authors><author><keyname>Kulesz</keyname><forenames>Daniel</forenames></author><author><keyname>Zitzelsberger</keyname><forenames>Sebastian</forenames></author></authors><title>Investigating Effects of Common Spreadsheet Design Practices on
  Correctness and Maintainability</title><categories>cs.SE</categories><comments>16 Pages, 5 Colour Figures; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2012, ISBN: 978-0-9569258-6-2</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets are software programs which are typically created by end-users
and often used for business-critical tasks. Many studies indicate that errors
in spreadsheets are very common. Thus, a number of vendors offer auditing tools
which promise to detect errors by checking spreadsheets against so-called Best
Practices such as &quot;Don't put constants in fomulae&quot;. Unfortunately, it is
largely unknown which Best Practices have which actual effects on which
spreadsheet quality aspects in which settings.
  We have conducted a controlled experiment with 42 subjects to investigate the
question whether observance of three commonly suggested Best Practices is
correlated with desired positive effects regarding correctness and
maintainability: &quot;Do not put constants in formulae&quot;, &quot;keep formula complexity
low&quot; and &quot;refer to the left and above&quot;. The experiment was carried out in two
phases which covered the creation of new and the modification of existing
spreadsheets. It was evaluated using a novel construction kit for spreadsheet
auditing tools called Spreadsheet Inspection Framework.
  The experiment produced a small sample of directly comparable spreadsheets
which all try to solve the same task. Our analysis of the obtained spreadsheets
indicates that the correctness of &quot;bottom-line&quot; results is not affected by the
observance of the three Best Practices. However, initially correct spreadsheets
with high observance of these Best Practices tend to be the ones whose later
modifications yield the most correct results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7110</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7110</id><created>2012-11-29</created><authors><author><keyname>Magnusson</keyname><forenames>Hjalti</forenames></author><author><keyname>Ulfarsson</keyname><forenames>Henning</forenames></author></authors><title>Algorithms for discovering and proving theorems about permutation
  patterns</title><categories>math.CO cs.DM cs.DS cs.MS</categories><comments>13 pages, 3 figures</comments><msc-class>05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm, called BiSC, that describes the patterns avoided by
a given set of permutations. It automatically conjectures the statements of
known theorems such as the descriptions of stack-sortable (Knuth 1975) and
West-2-stack-sortable permutations (West 1990), smooth (Lakshmibai and Sandhya
1990) and forest-like permutations (Bousquet-Melou and Butler 2007), and simsun
permutations (Branden and Claesson 2011). The algorithm has also been used to
discover new theorems and conjectures related to Young tableaux,
Wilf-equivalences and sorting devices. We further give algorithms to prove a
complete description of preimages of pattern classes under certain sorting
devices. These generalize an algorithm of Claesson and Ulfarsson (2012) and
allow us to prove a linear time algorithm for finding occurrences of the
pattern 4312.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7113</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7113</id><created>2012-11-29</created><authors><author><keyname>Meddour</keyname><forenames>Djamal-Eddine</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author><author><keyname>Gourhant</keyname><forenames>Yvon</forenames></author></authors><title>On the Role of Infrastructure sharing for Mobile Network Operators in
  Emerging Markets</title><categories>cs.NI</categories><journal-ref>The International Journal of Computer and Telecommunications
  Networking, Volume 55, Issue 7, 2011, Pages 1576-1591</journal-ref><doi>10.1016/j.comnet.2011.01.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional model of single ownership of all the physical network
elements and network layers by mobile network operators is beginning to be
challenged. This has been attributed to the rapid and complex technology
migration compounded with rigorous regulatory requirements and ever increasing
capital expenditures. These trends, combined together with the increasing
competition, rapid commoditization of telecommunication equipments and rising
separation of network and service provisioning are pushing the operators to
adopt multiple strategies, with network infrastructure sharing in the core and
radio access networks emerging as a more radical mechanism to substantially and
sustainably improve network costs. Through infrastructure sharing, developing
countries and other emerging economies can harness the technological, market
and regulatory developments that have fostered affordable access to mobile and
broadband services. Similarly, the network operators entering or consolidating
in the emerging markets can aim for substantial savings on capital and
operating expenses. The present paper aims to investigate the current
technological solutions and regulatory and the technical-economical dimensions
in connection with the sharing of mobile telecommunication networks in emerging
countries. We analyze the estimated savings on capital and operating expenses,
while assessing the technical constraints, applicability and benefits of the
network sharing solutions in an emerging market context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7133</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7133</id><created>2012-11-29</created><updated>2013-05-07</updated><authors><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Ma</keyname><forenames>Qiang</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Thompson</keyname><forenames>Brian</forenames></author></authors><title>Socializing the h-index</title><categories>cs.DL cs.IR cs.SI physics.soc-ph</categories><comments>5 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of bibliometric measures have been proposed to quantify the impact
of researchers and their work. The h-index is a notable and widely-used example
which aims to improve over simple metrics such as raw counts of papers or
citations. However, a limitation of this measure is that it considers authors
in isolation and does not account for contributions through a collaborative
team. To address this, we propose a natural variant that we dub the Social
h-index. The idea is to redistribute the h-index score to reflect an
individual's impact on the research community. In addition to describing this
new measure, we provide examples, discuss its properties, and contrast with
other measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7138</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7138</id><created>2012-11-29</created><updated>2014-05-26</updated><authors><author><keyname>Heilman</keyname><forenames>Steven</forenames></author></authors><title>Euclidean Partitions Optimizing Noise Stability</title><categories>cs.CC math.FA math.MG</categories><comments>40 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Standard Simplex Conjecture of Isaksson and Mossel asks for the partition
$\{A_{i}\}_{i=1}^{k}$ of $\mathbb{R}^{n}$ into $k\leq n+1$ pieces of equal
Gaussian measure of optimal noise stability. That is, for $\rho&gt;0$, we maximize
$$
\sum_{i=1}^{k}\int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}1_{A_{i}}(x)1_{A_{i}}(x\rho+y\sqrt{1-\rho^{2}})
e^{-(x_{1}^{2}+\cdots+x_{n}^{2})/2}e^{-(y_{1}^{2}+\cdots+y_{n}^{2})/2}dxdy. $$
Isaksson and Mossel guessed the best partition for this problem and proved some
applications of their conjecture. For example, the Standard Simplex Conjecture
implies the Plurality is Stablest Conjecture. For $k=3,n\geq2$ and
$0&lt;\rho&lt;\rho_{0}(k,n)$, we prove the Standard Simplex Conjecture. The full
conjecture has applications to theoretical computer science, and to geometric
multi-bubble problems (after Isaksson and Mossel).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7139</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7139</id><created>2012-11-29</created><updated>2015-12-23</updated><authors><author><keyname>Hwang</keyname><forenames>June</forenames></author><author><keyname>Choi</keyname><forenames>Jinho</forenames></author><author><keyname>Jantti</keyname><forenames>Riku</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>On the Aggregate Interference in Random CSMA/CA Networks: A Stochastic
  Geometry Approach</title><categories>cs.NI math.NA</categories><comments>This paper has been withdrawn by the author due to some errors in
  proof</comments><msc-class>60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the cumulative distribution function (CDF) of
the aggregate interference in carrier sensing multiple access/collision
avoidance (CSMA/CA) networks measured at an arbitrary time and position. We
assume that nodes are deployed in an infinite two-dimensional plane by Poisson
point process (PPP) and the channel model follows the singular path loss
function and Rayleigh fading. To find the effective active node density we
analyze the distributed coordinate function (DCF) dynamics in a common sensing
area and obtain the steady-state power distribution within a spatial disk of
radius $R/2$, where $R$ is the effective carrier sensing distance. The results
of massive simulation using Network Simulator-2 (NS-2) show a high correlation
with the derived CDF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7141</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7141</id><created>2012-11-29</created><authors><author><keyname>Koay</keyname><forenames>Cheng Guan</forenames></author></authors><title>Pseudometrically Constrained Centroidal Voronoi Tessellations:
  Generating uniform antipodally symmetric points on the unit sphere with a
  novel acceleration strategy and its applications to Diffusion and 3D radial
  MRI</title><categories>physics.med-ph cs.CE cs.CG math.MG math.OC</categories><comments>33 pages, 5 figures</comments><journal-ref>Magnetic Resonance in Medicine 2014; 71: 723-734</journal-ref><doi>10.1002/mrm.24715</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: The purpose of this work is to investigate the hypothesis that
uniform sampling measurements that are endowed with antipodal symmetry play an
important role when the raw data and image data are related through the Fourier
relationship as in q-space diffusion MRI and 3D radial MRI. Currently, it is
extremely challenging to generate large uniform antipodally symmetric point
sets suitable for 3D radial MRI. A novel approach is proposed to solve this
important and long-standing problem.
  Methods: The proposed method is based upon constrained centroidal Voronoi
tessellations of the upper hemisphere with a novel pseudometric. Geometrically
intuitive approach to tessellating the upper hemisphere is also proposed.
  Results: The average time complexity of the proposed centroidal tessellations
was shown to be effectively on the order of the product of the number of
iterations and the number of generators. For small sample size, the proposed
method was comparable to the state-of-the-art iterative method in terms of the
uniformity. For large sample size, in which the state-of-the-art method is
infeasible, the reconstructed images from the proposed method has less streak
and ringing artifact as compared to those of the commonly used methods.
  Conclusion: This work solved a long-standing problem on generating uniform
sampling points for 3D radial MRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7152</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7152</id><created>2012-11-29</created><authors><author><keyname>Cowen</keyname><forenames>Robert</forenames></author></authors><title>A Method for Constructing Minimally Unsatisfiable CNFs</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize a method of Ivor Spence (J. of Experimental Algorithms 15(March
2010)) that produces unsatisfiable cnfs and show experimentally that, for the
most part, the resulting cnfs are minimally unsatisfiable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7161</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7161</id><created>2012-11-30</created><authors><author><keyname>Buss</keyname><forenames>Sam</forenames></author><author><keyname>Soltys</keyname><forenames>Michael</forenames></author></authors><title>Unshuffling a Square is NP-Hard</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A shuffle of two strings is formed by interleaving the characters into a new
string, keeping the characters of each string in order. A string is a square if
it is a shuffle of two identical strings. There is a known polynomial time
dynamic programming algorithm to determine if a given string z is the shuffle
of two given strings x,y; however, it has been an open question whether there
is a polynomial time algorithm to determine if a given string z is a square. We
resolve this by proving that this problem is NP-complete via a many-one
reduction from 3- Partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7164</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7164</id><created>2012-11-30</created><authors><author><keyname>Oishi</keyname><forenames>Koji</forenames></author><author><keyname>Shimada</keyname><forenames>Takashi</forenames></author><author><keyname>Ito</keyname><forenames>Nobuyasu</forenames></author></authors><title>Group Formation through Indirect Reciprocity</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.PE</categories><journal-ref>Phys. Rev. E 87, 030801(R) (2013)</journal-ref><doi>10.1103/PhysRevE.87.030801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of structure in cooperative relation is studied in a game
theoretical model. It is proved that specific types of reciprocity norm lead
individuals to split into two groups. The condition for the evolutionary
stability of the norms is also revealed. This result suggests a connection
between group formation and a specific type of reciprocity norm in our society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7180</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7180</id><created>2012-11-30</created><authors><author><keyname>Hu</keyname><forenames>Huiyi</forenames></author><author><keyname>van Gennip</keyname><forenames>Yves</forenames></author><author><keyname>Hunter</keyname><forenames>Blake</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Bertozzi</keyname><forenames>Andrea L.</forenames></author></authors><title>Multislice Modularity Optimization in Community Detection and Image
  Segmentation</title><categories>cs.SI cs.CV physics.data-an physics.soc-ph</categories><comments>3 pages, 2 figures, to appear in IEEE International Conference on
  Data Mining PhD forum conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because networks can be used to represent many complex systems, they have
attracted considerable attention in physics, computer science, sociology, and
many other disciplines. One of the most important areas of network science is
the algorithmic detection of cohesive groups (i.e., &quot;communities&quot;) of nodes. In
this paper, we algorithmically detect communities in social networks and image
data by optimizing multislice modularity. A key advantage of modularity
optimization is that it does not require prior knowledge of the number or sizes
of communities, and it is capable of finding network partitions that are
composed of communities of different sizes. By optimizing multislice modularity
and subsequently calculating diagnostics on the resulting network partitions,
it is thereby possible to obtain information about network structure across
multiple system scales. We illustrate this method on data from both social
networks and images, and we find that optimization of multislice modularity
performs well on these two tasks without the need for extensive
problem-specific adaptation. However, improving the computational speed of this
method remains a challenging open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7184</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7184</id><created>2012-11-30</created><authors><author><keyname>Oliveto</keyname><forenames>Pietro S.</forenames></author><author><keyname>Witt</keyname><forenames>Carsten</forenames></author></authors><title>Erratum: Simplified Drift Analysis for Proving Lower Bounds in
  Evolutionary Computation</title><categories>cs.NE</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This erratum points out an error in the simplified drift theorem (SDT)
[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modification
of one of its conditions is sufficient to establish a valid result. In many
respects, the new theorem is more general than before. We no longer assume a
Markov process nor a finite search space. Furthermore, the proof of the theorem
is more compact than the previous ones. Finally, previous applications of the
SDT are revisited. It turns out that all of these either meet the modified
condition directly or by means of few additional arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7200</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7200</id><created>2012-11-30</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author></authors><title>Using Differential Evolution for the Graph Coloring</title><categories>math.CO cs.NE</categories><journal-ref>Proceedings of IEEE SSCI2011 Symposium Series on Computational
  Intelligence, pp. 150--156 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential evolution was developed for reliable and versatile function
optimization. It has also become interesting for other domains because of its
ease to use. In this paper, we posed the question of whether differential
evolution can also be used by solving of the combinatorial optimization
problems, and in particular, for the graph coloring problem. Therefore, a
hybrid self-adaptive differential evolution algorithm for graph coloring was
proposed that is comparable with the best heuristics for graph coloring today,
i.e. Tabucol of Hertz and de Werra and the hybrid evolutionary algorithm of
Galinier and Hao. We have focused on the graph 3-coloring. Therefore, the
evolutionary algorithm with method SAW of Eiben et al., which achieved
excellent results for this kind of graphs, was also incorporated into this
study. The extensive experiments show that the differential evolution could
become a competitive tool for the solving of graph coloring problem in the
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7203</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7203</id><created>2012-11-30</created><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Robust Filtering for Adaptive Homodyne Estimation of Continuously
  Varying Optical Phase</title><categories>quant-ph cs.SY math.OC</categories><comments>5 pages, 6 figures, Proceedings of the 2012 Australian Control
  Conference</comments><journal-ref>ISBN 978-1-922107-63-3, 2012, pp. 454-458</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been demonstrated experimentally that adaptive estimation of
a continuously varying optical phase provides superior accuracy in the phase
estimate compared to static estimation. Here, we show that the mean-square
error in the adaptive phase estimate may be further reduced for the stochastic
noise process considered by using an optimal Kalman filter in the feedback
loop. Further, the estimation process can be made robust to fluctuations in the
underlying parameters of the noise process modulating the system phase to be
estimated. This has been done using a guaranteed cost robust filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7210</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7210</id><created>2012-11-30</created><authors><author><keyname>Yu</keyname><forenames>Tina</forenames></author><author><keyname>Ben-Av</keyname><forenames>Radel</forenames></author></authors><title>Evolutionarily Stable Sets in Quantum Penny Flip Games</title><categories>quant-ph cs.AI cs.GT</categories><comments>25 pages, Quantum Information Processing Journal</comments><msc-class>91A22</msc-class><journal-ref>Quantum Information Processing Journal, Volume 12, Issue 6, pp
  2143-2165, June 2013</journal-ref><doi>10.1007/s11128-012-0515-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In game theory, an Evolutionarily Stable Set (ES set) is a set of Nash
Equilibrium (NE) strategies that give the same payoffs. Similar to an
Evolutionarily Stable Strategy (ES strategy), an ES set is also a strict NE.
This work investigates the evolutionary stability of classical and quantum
strategies in the quantum penny flip games. In particular, we developed an
evolutionary game theory model to conduct a series of simulations where a
population of mixed classical strategies from the ES set of the game were
invaded by quantum strategies. We found that when only one of the two players'
mixed classical strategies were invaded, the results were different. In one
case, due to the interference phenomenon of superposition, quantum strategies
provided more payoff, hence successfully replaced the mixed classical
strategies in the ES set. In the other case, the mixed classical strategies
were able to sustain the invasion of quantum strategies and remained in the ES
set. Moreover, when both players' mixed classical strategies were invaded by
quantum strategies, a new quantum ES set emerged. The strategies in the quantum
ES set give both players payoff 0, which is the same as the payoff of the
strategies in the mixed classical ES set of this game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7219</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7219</id><created>2012-11-30</created><authors><author><keyname>Zhao</keyname><forenames>Qian</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>A recursive divide-and-conquer approach for sparse principal component
  analysis</title><categories>cs.CV cs.LG stat.ML</categories><comments>35 pages, 4 figures</comments><msc-class>62H25, 68T10</msc-class><acm-class>I.5.0; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new method is proposed for sparse PCA based on the recursive
divide-and-conquer methodology. The main idea is to separate the original
sparse PCA problem into a series of much simpler sub-problems, each having a
closed-form solution. By recursively solving these sub-problems in an
analytical way, an efficient algorithm is constructed to solve the sparse PCA
problem. The algorithm only involves simple computations and is thus easy to
implement. The proposed method can also be very easily extended to other sparse
PCA problems with certain constraints, such as the nonnegative sparse PCA
problem. Furthermore, we have shown that the proposed algorithm converges to a
stationary point of the problem, and its computational complexity is
approximately linear in both data size and dimensionality. The effectiveness of
the proposed method is substantiated by extensive experiments implemented on a
series of synthetic and real data in both reconstruction-error-minimization and
data-variance-maximization viewpoints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7230</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7230</id><created>2012-11-30</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Park</keyname><forenames>Han Woo</forenames></author><author><keyname>Lengyel</keyname><forenames>Balazs</forenames></author></authors><title>A Routine for Measuring Synergy in University-Industry-Government
  Relations: Mutual Information as a Triple-Helix and Quadruple-Helix Indicator</title><categories>cs.CY</categories><comments>submitted to a special issue of Scientometrics entitled &quot;Mapping
  Triple Helix Innovation for Developing and Transitional Economies:
  Webometrics, Scientometrics, and Informetrics&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutual information in three (or more) dimensions can be considered as a
Triple-Helix indicator of synergy in university-industry-government relations.
An open-source routine th4.exe makes the computation of this indicator
interactively available at the Internet, and thus applicable to large sets of
data. Th4.exe computes all probabilistic entropies and mutual information in
two, three, and, if available in the data, four dimensions among, for example,
classes such as geographical addresses (cities, regions), technological codes
(e.g., OECD's NACE codes), and size categories; or, alternatively, among
institutional addresses (academic, industrial, public sector) in document sets.
The relations between the Triple-Helix indicator -- as an indicator of synergy
-- and the Triple-Helix model that specifies the possibility of feedback by an
overlay of communications, are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7232</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7232</id><created>2012-11-30</created><updated>2013-01-16</updated><authors><author><keyname>Haralabopoulos</keyname><forenames>Giannis</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Real Time Enhanced Random Sampling of Online Social Networks</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>16 Pages, 9 Figures, 4 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social graphs can be easily extracted from Online Social Networks. However
these networks are getting larger from day to day. Sampling methods used to
evaluate graph information cannot accurately extract graph properties.
Furthermore Social Networks are limiting the access to their data, making the
crawling process even harder. A novel approach on Random Sampling is proposed,
considering both limitation and resources. We evaluate this proposal with 4
different settings on 5 different Test Graphs, crawled directly from Twitter.
Through comparing the results we observe the pros and cons of its method as
well as their resource allocation. Concluding we present their best area of
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7239</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7239</id><created>2012-11-30</created><authors><author><keyname>Ho</keyname><forenames>Zuleita</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author><author><keyname>Gerbracht</keyname><forenames>Sabrina</forenames></author></authors><title>Information Leakage Neutralization for the Multi-Antenna
  Non-Regenerative Relay-Assisted Multi-Carrier Interference Channel</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures, currently under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In heterogeneous dense networks where spectrum is shared, users privacy
remains one of the major challenges. On a multi-antenna relay-assisted
multi-carrier interference channel, each user shares the frequency and spatial
resources with all other users. When the receivers are not only interested in
their own signals but also in eavesdropping other users' signals, the cross
talk on the frequency and spatial channels becomes information leakage. In this
paper, we propose a novel secrecy rate enhancing relay strategy that utilizes
both frequency and spatial resources, termed as information leakage
neutralization. To this end, the relay matrix is chosen such that the effective
channel from the transmitter to the colluding eavesdropper is equal to the
negative of the effective channel over the relay to the colluding eavesdropper
and thus the information leakage to zero. Interestingly, the optimal relay
matrix in general is not block-diagonal which encourages users' encoding over
the frequency channels. We proposed two information leakage neutralization
strategies, namely efficient information leakage neutralization (EFFIN) and
optimized information leakage neutralization (OPTIN). EFFIN provides a simple
and efficient design of relay processing matrix and precoding matrices at the
transmitters in the scenario of limited power and computational resources.
OPTIN, despite its higher complexity, provides a better sum secrecy rate
performance by optimizing the relay processing matrix and the precoding
matrices jointly. The proposed methods are shown to improve the sum secrecy
rates over several state-of-the-art baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7276</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7276</id><created>2012-11-26</created><authors><author><keyname>Pham</keyname><forenames>Duc Son</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Efficient algorithms for robust recovery of images from compressed data</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Sequel of a related IEEE Transactions on Image Processing paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is an important theory for sub-Nyquist sampling and
recovery of compressible data. Recently, it has been extended by Pham and
Venkatesh to cope with the case where corruption to the CS data is modeled as
impulsive noise. The new formulation, termed as robust CS, combines robust
statistics and CS into a single framework to suppress outliers in the CS
recovery. To solve the newly formulated robust CS problem, Pham and Venkatesh
suggested a scheme that iteratively solves a number of CS problems, the
solutions from which converge to the true robust compressed sensing solution.
However, this scheme is rather inefficient as it has to use existing CS solvers
as a proxy. To overcome limitation with the original robust CS algorithm, we
propose to solve the robust CS problem directly in this paper and drive more
computationally efficient algorithms by following latest advances in
large-scale convex optimization for non-smooth regularization. Furthermore, we
also extend the robust CS formulation to various settings, including additional
affine constraints, $\ell_1$-norm loss function, mixed-norm regularization, and
multi-tasking, so as to further improve robust CS. We also derive simple but
effective algorithms to solve these extensions. We demonstrate that the new
algorithms provide much better computational advantage over the original robust
CS formulation, and effectively solve more sophisticated extensions where the
original methods simply cannot. We demonstrate the usefulness of the extensions
on several CS imaging tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7277</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7277</id><created>2012-11-30</created><authors><author><keyname>Soares</keyname><forenames>Claudia</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author><author><keyname>Gomes</keyname><forenames>Joao</forenames></author></authors><title>DCOOL-NET: Distributed cooperative localization for sensor networks</title><categories>math.OC cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present DCOOL-NET, a scalable distributed in-network algorithm for sensor
network localization based on noisy range measurements. DCOOL-NET operates by
parallel, collaborative message passing between single-hop neighbor sensors,
and involves simple computations at each node. It stems from an application of
the majorization-minimization (MM) framework to the nonconvex optimization
problem at hand, and capitalizes on a novel convex majorizer. The proposed
majorizer is endowed with several desirable properties and represents a key
contribution of this work. It is a more accurate match to the underlying
nonconvex cost function than popular MM quadratic majorizers, and is readily
amenable to distributed minimization via the alternating direction method of
multipliers (ADMM). Moreover, it allows for low-complexity, fast Nesterov
gradient methods to tackle the ADMM subproblems induced at each node. Computer
simulations show that DCOOL-NET achieves comparable or better sensor position
accuracies than a state-of-art method which, furthermore, is not parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7283</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7283</id><created>2012-11-30</created><authors><author><keyname>Herzet</keyname><forenames>Cedric</forenames></author><author><keyname>Soussen</keyname><forenames>Charles</forenames></author><author><keyname>Idier</keyname><forenames>Jerome</forenames></author><author><keyname>Gribonval</keyname><forenames>Remi</forenames></author></authors><title>Coherence-based Partial Exact Recovery Condition for OMP/OLS</title><categories>cs.IT math.IT physics.data-an stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the exact recovery of the support of a k-sparse vector with
Orthogonal Matching Pursuit (OMP) and Orthogonal Least Squares (OLS) in a
noiseless setting. We consider the scenario where OMP/OLS have selected good
atoms during the first l iterations (l&lt;k) and derive a new sufficient and
worst-case necessary condition for their success in k steps. Our result is
based on the coherence \mu of the dictionary and relaxes Tropp's well-known
condition \mu&lt;1/(2k-1) to the case where OMP/OLS have a partial knowledge of
the support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7302</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7302</id><created>2012-11-30</created><authors><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Exploiting Metric Structure for Efficient Private Query Release</title><categories>cs.DS cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of privately answering queries defined on databases
which are collections of points belonging to some metric space. We give simple,
computationally efficient algorithms for answering distance queries defined
over an arbitrary metric. Distance queries are specified by points in the
metric space, and ask for the average distance from the query point to the
points contained in the database, according to the specified metric. Our
algorithms run efficiently in the database size and the dimension of the space,
and operate in both the online query release setting, and the offline setting
in which they must in polynomial time generate a fixed data structure which can
answer all queries of interest. This represents one of the first subclasses of
linear queries for which efficient algorithms are known for the private query
release problem, circumventing known hardness results for generic linear
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7308</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7308</id><created>2012-11-30</created><updated>2014-09-06</updated><authors><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author></authors><title>Godel's Incompleteness Phenomenon - Computationally</title><categories>math.LO cs.LO</categories><comments>To Appear in Philosophia Scientiae, vol. 18 no. 3, 2014</comments><msc-class>03B25, 03D35, 03F40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that Godel's completeness theorem is equivalent to completability of
consistent theories, and Godel's incompleteness theorem is equivalent to the
fact that this completion is not constructive, in the sense that there are some
consistent and recursively enumerable theories which cannot be extended to any
complete and consistent and recursively enumerable theory. Though any
consistent and decidable theory can be extended to a complete and consistent
and decidable theory. Thus deduction and consistency are not decidable in
logic, and an analogue of Rice's Theorem holds for recursively enumerable
theories: all the non-trivial properties of such theories are undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7309</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7309</id><created>2012-11-30</created><authors><author><keyname>Gilboa</keyname><forenames>Elad</forenames></author><author><keyname>Chavali</keyname><forenames>Phani</forenames></author><author><keyname>Yang</keyname><forenames>Peng</forenames></author><author><keyname>Nehorai</keyname><forenames>Arye</forenames></author></authors><title>Distributed Optimization via Adaptive Regularization for Large Problems
  with Separable Constraints</title><categories>cs.DC math.OC</categories><comments>5 Pages, 2 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many practical applications require solving an optimization over large and
high-dimensional data sets, which makes these problems hard to solve and
prohibitively time consuming. In this paper, we propose a parallel distributed
algorithm that uses an adaptive regularizer (PDAR) to solve a joint
optimization problem with separable constraints. The regularizer is adaptive
and depends on the step size between iterations and the iteration number. We
show theoretical converge of our algorithm to an optimal solution, and use a
multi-agent three-bin resource allocation example to illustrate the
effectiveness of the proposed algorithm. Numerical simulations show that our
algorithm converges to the same optimal solution as other distributed methods,
with significantly reduced computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7326</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7326</id><created>2012-11-30</created><authors><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Repeated Root Constacyclic Codes of Length $mp^s$ over
  $\mathbb{F}_{p^r}+u \mathbb{F}_{p^r}+...+ u^{e-1}\mathbb{F}_{p^r}$</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We give the structure of $\lambda$-constacyclic codes of length $p^sm$ over
$R=\mathbb{F}_{p^r}+u \mathbb{F}_{p^r}+...+ u^{e-1}\mathbb{F}_{p^r}$ with
$\lambda \in \F_{p^r}^*$. We also give the structure of $\lambda$-constacyclic
codes of length $p^sm$ with $\lambda=\alpha_1+u\alpha_2+...+u^{e-1}
\alpha_{e-1}$, where $\alpha_1,\alpha_2 \neq 0$ and study the self-duality of
these codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7343</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7343</id><created>2012-11-30</created><authors><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Eagle</keyname><forenames>Nathan</forenames></author></authors><title>Persistence and periodicity in a dynamic proximity network</title><categories>physics.data-an cs.SI physics.soc-ph</categories><comments>5 pages, 6 figures, part of the Reality Mining Project at
  http://realitycommons.media.mit.edu/ . Originally published in 2007;
  Proceedings of the DIMACS Workshop on Computational Methods for Dynamic
  Interaction Networks (Piscataway), 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topology of social networks can be understood as being inherently
dynamic, with edges having a distinct position in time. Most characterizations
of dynamic networks discretize time by converting temporal information into a
sequence of network &quot;snapshots&quot; for further analysis. Here we study a highly
resolved data set of a dynamic proximity network of 66 individuals. We show
that the topology of this network evolves over a very broad distribution of
time scales, that its behavior is characterized by strong periodicities driven
by external calendar cycles, and that the conversion of inherently
continuous-time data into a sequence of snapshots can produce highly biased
estimates of network structure. We suggest that dynamic social networks exhibit
a natural time scale \Delta_{nat}, and that the best conversion of such dynamic
data to a discrete sequence of networks is done at this natural rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7345</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7345</id><created>2012-11-30</created><authors><author><keyname>Ponge</keyname><forenames>Julien</forenames><affiliation>CITI</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI</affiliation></author></authors><title>JooFlux : modification de code \`a chaud et injection d'aspects
  directement dans une JVM 7</title><categories>cs.SE</categories><comments>Conf\'erence d'informatique en Parall\'elisme, Architecture et
  Syst\`eme (ComPAS) - Conf\'erence Fran\c{c}aise en Syst\`emes d'Exploitation
  (CFSE) (2013) (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changing functional and non-functional software implementation at runtime is
useful and even sometimes critical both in development and production
environments. JooFlux is a JVM agent that allows both the dynamic replacement
of method implementations and the application of aspect advices. It works by
doing bytecode transformation to take advantage of the new invokedynamic
instruction added in Java SE 7 to help implementing dynamic languages for the
JVM. JooFlux can be managed using a JMX agent so as to operate dynamic
modifications at runtime, without resorting to a dedicated domain-specific
language. We compared JooFlux with existing AOP platforms and dynamic
languages. Results demonstrate that JooFlux performances are close to the Java
ones --- with most of the time a marginal overhead, and sometimes a gain ---
where AOP platforms and dynamic languages present significant overheads. This
paves the way for interesting future evolutions and applications of JooFlux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7346</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7346</id><created>2012-11-30</created><authors><author><keyname>Demirci</keyname><forenames>H. G&#xf6;kalp</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author></authors><title>Checking generalized debates with small space and randomness</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model of probabilistic debate checking, where a silent
resource-bounded verifier reads a dialogue about the membership of the string
in the language under consideration between a prover and a refuter. Our model
combines and generalizes the concepts of one-way interactive proof systems,
games of incomplete information, and probabilistically checkable
complete-information debate systems. We consider debates of partial and zero
information, where the prover is prevented from seeing some or all of the
messages of the refuter, as well as those of complete information. The classes
of languages with debates checkable by verifiers operating under severe bounds
on the memory and randomness are studied.
  We give full characterizations of versions of these classes corresponding to
simultaneous bounds of O(1) space and O(1) random bits, and of logarithmic
space and polynomial time. It turns out that constant-space verifiers, which
can only check complete-information debates for regular languages
deterministically, can check for membership in any language in P when allowed
to use a constant number of random bits. Similar increases also occur for zero-
and partial- information debates, from NSPACE(n) to PSPACE, and from E to
EXPTIME, respectively. Adding logarithmic space to these constant-randomness
verifiers does not change their power. When logspace debate checkers are
restricted to run in polynomial time without a bound on the number of random
bits, the class of debatable languages equals PSPACE for all debate types. We
also present a result on the hardness of approximating the quantified max word
problem for matrices that is a corollary of this characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7353</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7353</id><created>2012-11-30</created><updated>2015-10-14</updated><authors><author><keyname>Diestel</keyname><forenames>Reinhard</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Malte</forenames></author></authors><title>Connected tree-width</title><categories>math.CO cs.DM</categories><comments>18 pages</comments><msc-class>05C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connected tree-width of a graph is the minimum width of a
tree-decomposition whose parts induce connected subgraphs. Long cycles are
examples of graphs that have small tree-width but large connected tree-width.
We show that a graph has small connected tree-width if and only if it has small
tree-width and contains no long geodesic cycle.
  We further prove a connected analogue of the duality theorem for tree-width:
a finite graph has small connected tree-width if and only if it has no bramble
whose connected covers are all large. Both these results are qualitative: the
bounds are good but not tight.
  We show that graphs of connected tree-width $k$ are $k$-hyperbolic, which is
tight, and that graphs of tree-width $k$ whose geodesic cycles all have length
at most $\ell$ are $\lfloor{3\over2}\ell(k-1)\rfloor$-hyperbolic. The existence
of such a function $h(k,\ell)$ had been conjectured by Sullivan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7356</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7356</id><created>2012-11-30</created><authors><author><keyname>N.</keyname><forenames>Sai Shankar</forenames></author><author><keyname>Dash</keyname><forenames>Debashis</forenames></author><author><keyname>Madi</keyname><forenames>Hassan El</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Guru</forenames></author></authors><title>WiGig and IEEE 802.11ad - For multi-gigabyte-per-second WPAN and WLAN</title><categories>cs.NI</categories><comments>24 pages, 18 figures. Submitted to ZTE Communications 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wireless Gigabit Alliance (WiGig) and IEEE 802.11ad are developing a
multigigabit wireless personal and local area network (WPAN/WLAN) specification
in the 60 GHz millimeter wave band. Chipset manufacturers, original equipment
manufacturers (OEMs), and telecom companies are also assisting in this
development. 60 GHz millimeter wave transmission will scale the speed of WLANs
and WPANs to 6.75 Gbit/s over distances less than 10 meters. This technology is
the first of its kind and will eliminate the need for cable around personal
computers, docking stations, and other consumer electronic devices.
High-definition multimedia interface (HDMI), display port, USB 3.0, and
peripheral component interconnect express (PCIe) 3.0 cables will all be
eliminated. Fast downloads and uploads, wireless sync, and
multi-gigabit-per-second WLANs will be possible over shorter distances. 60 GHz
millimeter wave supports fast session transfer (FST) protocol, which makes it
backward compatible with 5 GHz or 2.4 GHz WLAN so that end users experience the
same range as in today's WLANs. IEEE 802.11ad specifies the physical (PHY)
sublayer and medium access control (MAC) sublayer of the protocol stack. The
MAC protocol is based on time-division multiple access (TDMA), and the PHY
layer uses single carrier (SC) and orthogonal frequency division multiplexing
(OFDM) to simultaneously enable low-power, high-performance applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7359</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7359</id><created>2012-11-30</created><updated>2013-02-13</updated><authors><author><keyname>McDonald</keyname><forenames>Ross B.</forenames></author><author><keyname>Katzgraber</keyname><forenames>Helmut G.</forenames></author></authors><title>Genetic braid optimization: A heuristic approach to compute
  quasiparticle braids</title><categories>quant-ph cond-mat.mes-hall cs.NE</categories><comments>6 pages 4 figures</comments><journal-ref>Phys. Rev. B 87, 054414 (2013)</journal-ref><doi>10.1103/PhysRevB.87.054414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In topologically-protected quantum computation, quantum gates can be carried
out by adiabatically braiding two-dimensional quasiparticles, reminiscent of
entangled world lines. Bonesteel et al. [Phys. Rev. Lett. 95, 140503 (2005)],
as well as Leijnse and Flensberg [Phys. Rev. B 86, 104511 (2012)] recently
provided schemes for computing quantum gates from quasiparticle braids.
Mathematically, the problem of executing a gate becomes that of finding a
product of the generators (matrices) in that set that approximates the gate
best, up to an error. To date, efficient methods to compute these gates only
strive to optimize for accuracy. We explore the possibility of using a generic
approach applicable to a variety of braiding problems based on evolutionary
(genetic) algorithms. The method efficiently finds optimal braids while
allowing the user to optimize for the relative utilities of accuracy and/or
length. Furthermore, when optimizing for error only, the method can quickly
produce efficient braids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.7369</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.7369</id><created>2012-11-30</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J.</forenames></author><author><keyname>Ziehe</keyname><forenames>Andreas</forenames></author></authors><title>Approximate Rank-Detecting Factorization of Low-Rank Tensors</title><categories>stat.ML cs.LG math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3
tensor and calculates its factorization into rank-one components. We provide
generative conditions for the algorithm to work and demonstrate on both
synthetic and real world data that AROFAC2 is a potentially outperforming
alternative to the gold standard PARAFAC over which it has the advantages that
it can intrinsically detect the true rank, avoids spurious components, and is
stable with respect to outliers and non-Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0017</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0017</id><created>2012-11-30</created><updated>2012-12-04</updated><authors><author><keyname>Aridhi</keyname><forenames>Sabeur</forenames></author><author><keyname>d'Orazio</keyname><forenames>Laurent</forenames></author><author><keyname>Maddouri</keyname><forenames>Mondher</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>A large-scale and fault-tolerant approach of subgraph mining using
  density-based partitioning</title><categories>cs.DB cs.DC cs.SI</categories><comments>The paper is under reviewing and we want to cancel the submission.
  Thank you for your understanding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, graph mining approaches have become very popular, especially in
domains such as bioinformatics, chemoinformatics and social networks. In this
scope, one of the most challenging tasks is frequent subgraph discovery. This
task has been motivated by the tremendously increasing size of existing graph
databases. Since then, an important problem of designing efficient and scaling
approaches for frequent subgraph discovery in large clusters, has taken place.
However, failures are a norm rather than being an exception in large clusters.
In this context, the MapReduce framework was designed so that node failures are
automatically handled by the framework. In this paper, we propose a large-scale
and fault-tolerant approach of subgraph mining by means of a density-based
partitioning technique, using MapReduce. Our partitioning aims to balance
computation load on a collection of machines. We experimentally show that our
approach decreases significantly the execution time and scales the subgraph
discovery process to large graph databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0018</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0018</id><created>2012-11-30</created><updated>2013-09-19</updated><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Collective Phenomena and Non-Finite State Computation in a Human Social
  System</title><categories>cs.SI cs.FL nlin.AO physics.soc-ph q-bio.PE stat.AP</categories><comments>23 pages, 4 figures, 3 tables; to appear in PLoS ONE</comments><journal-ref>PLoS ONE 8(10): e75818 (2013)</journal-ref><doi>10.1371/journal.pone.0075818</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational structure of a paradigmatic example of
distributed social interaction: that of the open-source Wikipedia community. We
examine the statistical properties of its cooperative behavior, and perform
model selection to determine whether this aspect of the system can be described
by a finite-state process, or whether reference to an effectively unbounded
resource allows for a more parsimonious description. We find strong evidence,
in a majority of the most-edited pages, in favor of a collective-state model,
where the probability of a &quot;revert&quot; action declines as the square root of the
number of non-revert actions seen since the last revert. We provide evidence
that the emergence of this social counter is driven by collective interaction
effects, rather than properties of individual users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0020</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0020</id><created>2012-11-30</created><updated>2015-09-11</updated><authors><author><keyname>Hernest</keyname><forenames>Dan</forenames></author><author><keyname>Trifonov</keyname><forenames>Trifon</forenames></author></authors><title>Modal Functional (`Dialectica') Interpretation</title><categories>cs.LO math.LO</categories><comments>30 pages, three Appendix sections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adapt our light Dialectica interpretation to usual and light modal
formulas (with universal quantification on boolean and natural variables) and
prove it sound for a non-standard modal arithmetic based on Goedel's T and
classical S_4. The range of this light modal Dialectica is the usual
(non-modal) classical Arithmetic in all finite types (with booleans); the
propositional kernel of its domain is Boolean and not S_4. The `heavy' modal
Dialectica interpretation is a new technique; it cannot be simulated within our
previous light Dialectica. The synthesized functionals are at least as good as
before; the translation process is much improved and could be more suitable for
the human operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0022</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0022</id><created>2012-11-30</created><updated>2012-12-10</updated><authors><author><keyname>Joe-Wong</keyname><forenames>Carlee</forenames></author><author><keyname>Sen</keyname><forenames>Soumya</forenames></author></authors><title>Mathematical Frameworks for Pricing in the Cloud: Revenue, Fairness, and
  Resource Allocations</title><categories>cs.CY cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As more and more users begin to use the cloud for their computing needs,
datacenter operators are increasingly pressed to effectively allocate their
resources among these client users. Yet while much work has been done in this
area, relatively little attention has been paid to studying perhaps the
ultimate lever of resource allocation: pricing. Most data centers today charge
users by &quot;bundling&quot; heterogeneous resources together in a fixed ratio and
selling these bundles to their clients. But bundling masks the fact that
different users require different combinations of resources (e.g., CPUs,
memory, bandwidth) to process their jobs. The presence of multiple resources in
fact allows an operator to offer many different types of pricing strategies,
which may have different effects on its revenue. Moreover, to avoid user
dissatisfaction, operators must consider the impact of their chosen prices on
the fairness of the jobs processed for different users. In this paper, we
develop an analytical framework that accounts for the fairness and revenue
tradeoffs that arise in a datacenter's multi-resource setting and the impact
that different pricing plans can have on this tradeoff. We characterize the
implications of different pricing plans on various fairness metrics and derive
analytical limits on the operator's fairness-revenue tradeoff. We then provide
an algorithm to navigate this tradeoff and compare the tradeoff points for
different pricing strategies on a data trace taken from a Google cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0023</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0023</id><created>2012-11-30</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Emergence of Self-Organized Amoeboid Movement in a Multi-Agent
  Approximation of Physarum polycephalum</title><categories>cs.MA q-bio.CB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The giant single-celled slime mould Physarum polycephalum exhibits complex
morphological adaptation and amoeboid movement as it forages for food and may
be seen as a minimal example of complex robotic behaviour. Swarm computation
has previously been used to explore how spatiotemporal complexity can emerge
from, and be distributed within, simple component parts and their interactions.
Using a particle based swarm approach we explore the question of how to
generate collective amoeboid movement from simple non-oscillatory component
parts in a model of P. polycephalum. The model collective behaves as a cohesive
and deformable virtual material, approximating the local coupling within the
plasmodium matrix. The collective generates de-novo and complex oscillatory
patterns from simple local interactions. The origin of this motor behaviour is
distributed within the collective rendering is morphologically adaptive,
amenable to external influence, and robust to simulated environmental insult.
We show how to gain external influence over the collective movement by
simulated chemo-attraction (pulling towards nutrient stimuli) and simulated
light irradiation hazards (pushing from stimuli). The amorphous and distributed
properties of the collective are demonstrated by cleaving it into two
independent entities and fusing two separate entities to form a single device,
thus enabling it to traverse narrow, separate or tortuous paths. We conclude by
summarising the contribution of the model to swarm based robotics and
soft-bodied modular robotics and discuss the future potential of such material
approaches to the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0025</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0025</id><created>2012-11-30</created><updated>2012-12-04</updated><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Hance</keyname><forenames>Travis</forenames></author></authors><title>Generalizing and Derandomizing Gurvits's Approximation Algorithm for the
  Permanent</title><categories>quant-ph cs.CC</categories><comments>19 pages, 1 figure, minor additions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Around 2002, Leonid Gurvits gave a striking randomized algorithm to
approximate the permanent of an n*n matrix A. The algorithm runs in
O(n^2/eps^2) time, and approximates Per(A) to within eps*||A||^n additive
error. A major advantage of Gurvits's algorithm is that it works for arbitrary
matrices, not just for nonnegative matrices. This makes it highly relevant to
quantum optics, where the permanents of bounded-norm complex matrices play a
central role. Indeed, the existence of Gurvits's algorithm is why, in their
recent work on the hardness of quantum optics, Aaronson and Arkhipov (AA) had
to talk about sampling problems rather than estimation problems.
  In this paper, we improve Gurvits's algorithm in two ways. First, using an
idea from quantum optics, we generalize the algorithm so that it yields a
better approximation when the matrix A has either repeated rows or repeated
columns. Translating back to quantum optics, this lets us classically estimate
the probability of any outcome of an AA-type experiment---even an outcome
involving multiple photons &quot;bunched&quot; in the same mode---at least as well as
that probability can be estimated by the experiment itself. (This does not, of
course, let us solve the AA sampling problem.) It also yields a general upper
bound on the probabilities of &quot;bunched&quot; outcomes, which resolves a conjecture
of Gurvits and might be of independent physical interest.
  Second, we use eps-biased sets to derandomize Gurvits's algorithm, in the
special case where the matrix A is nonnegative. More interestingly, we
generalize the notion of eps-biased sets to the complex numbers, construct
&quot;complex eps-biased sets,&quot; then use those sets to derandomize even our
generalization of Gurvits's algorithm to the multirow/multicolumn case (again
for nonnegative A). Whether Gurvits's algorithm can be derandomized for general
A remains an outstanding problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0027</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0027</id><created>2012-11-30</created><updated>2014-05-21</updated><authors><author><keyname>Arrighi</keyname><forenames>Pablo</forenames></author><author><keyname>Martiel</keyname><forenames>Simon</forenames></author><author><keyname>Nesme</keyname><forenames>Vincent</forenames></author></authors><title>Generalized Cayley Graphs and Cellular Automata over them</title><categories>cs.DM cs.FL gr-qc math-ph math.MP</categories><comments>34 pages, 10 figures, LaTeX</comments><msc-class>37B15, 68Q80, 37N20, 05C82, 83C27, 90B10</msc-class><acm-class>B.6.1; F.1.1; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cayley graphs have a number of useful features: the ability to graphically
represent finitely generated group elements and their relations; to name all
vertices relative to a point; and the fact that they have a well-defined notion
of translation. We propose a notion of graph associated to a language, which
conserves or generalizes these features. Whereas Cayley graphs are very
regular; associated graphs are arbitrary, although of a bounded degree.
Moreover, it is well-known that cellular automata can be characterized as the
set of translation-invariant continuous functions for a distance on the set of
configurations that makes it a compact metric space; this point of view makes
it easy to extend their definition from grids to Cayley graphs. Similarly, we
extend their definition to these arbitrary, bounded degree, time-varying
graphs. The obtained notion of Cellular Automata over generalized Cayley graphs
is stable under composition and under inversion. KEYWORDS: Causal Graph
Dynamics, Curtis-Hedlund-Lyndon, Dynamical networks, Boolean networks,
Generative networks automata, Graph Automata, Graph rewriting automata,
L-systems, parallel graph transformations, Amalgamated graph transformations,
Time-varying graphs, Regge calculus, Local, No-signalling, Reversibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0030</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0030</id><created>2012-11-30</created><authors><author><keyname>Khalil</keyname><forenames>Osama</forenames></author><author><keyname>Habib</keyname><forenames>Andrew</forenames></author></authors><title>Viewpoint Invariant Object Detector</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Object Detection is the task of identifying the existence of an object class
instance and locating it within an image. Difficulties in handling high
intra-class variations constitute major obstacles to achieving high performance
on standard benchmark datasets (scale, viewpoint, lighting conditions and
orientation variations provide good examples). Suggested model aims at
providing more robustness to detecting objects suffering severe distortion due
to &lt; 60{\deg} viewpoint changes. In addition, several model computational
bottlenecks have been resolved leading to a significant increase in the model
performance (speed and space) without compromising the resulting accuracy.
Finally, we produced two illustrative applications showing the potential of the
object detection technology being deployed in real life applications; namely
content-based image search and content-based video search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0041</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0041</id><created>2012-11-30</created><authors><author><keyname>Suprijadi</keyname></author><author><keyname>Aprilia</keyname><forenames>Ely</forenames></author><author><keyname>Yusfi</keyname><forenames>Meiqorry</forenames></author></authors><title>Compression Stress Effect on Dislocations Movement and Crack propagation
  in Cubic Crystal</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.CE</categories><journal-ref>Recent Development on Computational Science, 2011, Vol.2, page 1-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fracture material is seriously problem in daily life, and it has connection
with mechanical properties itself. The mechanical properties is belief depend
on dislocation movement and crack propagation in the crystal. Information about
this is very important to characterize the material. In FCC crystal structure
the competition between crack propagation and dislocation wake is very
interesting, in a ductile material like copper (Cu) dislocation can be seen in
room temperature, but in a brittle material like Si only cracks can be seen
observed. Different techniques were applied to material to study the mechanical
properties, in this study we did compression test in one direction. Combination
of simulation and experimental on cubic material are reported in this paper. We
found that the deflection of crack direction in Si caused by vacancy of
lattice,while compression stress on Cu cause the atoms displacement in one
direction. Some evidence of dislocation wake in Si crystal under compression
stress at high temperature will reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0042</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0042</id><created>2012-11-30</created><authors><author><keyname>Johnson</keyname><forenames>R. C.</forenames></author><author><keyname>Scheirer</keyname><forenames>Walter J.</forenames></author><author><keyname>Boult</keyname><forenames>Terrance E.</forenames></author></authors><title>Secure voice based authentication for mobile devices: Vaulted Voice
  Verification</title><categories>cs.CR cs.CV</categories><doi>10.1117/12.2015649</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the use of biometrics becomes more wide-spread, the privacy concerns that
stem from the use of biometrics are becoming more apparent. As the usage of
mobile devices grows, so does the desire to implement biometric identification
into such devices. A large majority of mobile devices being used are mobile
phones. While work is being done to implement different types of biometrics
into mobile phones, such as photo based biometrics, voice is a more natural
choice. The idea of voice as a biometric identifier has been around a long
time. One of the major concerns with using voice as an identifier is the
instability of voice. We have developed a protocol that addresses those
instabilities and preserves privacy. This paper describes a novel protocol that
allows a user to authenticate using voice on a mobile/remote device without
compromising their privacy. We first discuss the \vv protocol, which has
recently been introduced in research literature, and then describe its
limitations. We then introduce a novel adaptation and extension of the vaulted
verification protocol to voice, dubbed $V^3$. Following that we show a
performance evaluation and then conclude with a discussion of security and
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0047</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0047</id><created>2012-11-30</created><authors><author><keyname>Nam</keyname><forenames>Wooseok</forenames></author><author><keyname>Bai</keyname><forenames>Dongwoon</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>On the capacity limit of wireless channels under colored scattering</title><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been generally believed that the multiple-input multiple-output (MIMO)
channel capacity grows linearly with the size of antenna arrays. In terms of
degrees of freedom, linear transmit and receive arrays of length $L$ in a
scattering environment of total angular spread $|\Omega|$ asymptotically have
$|\Omega| L$ degrees of freedom. In this paper, it is claimed that the linear
increase in degrees of freedom may not be attained when scattered
electromagnetic fields in the underlying scattering environment are
statistically correlated. After introducing a model of correlated scattering,
which is referred to as the colored scattering model, we derive the number of
degrees of freedom. Unlike the uncorrelated case, the number of degrees of
freedom in the colored scattering channel is asymptotically limited by
$|\Omega| \cdot \min \{L, 1/\Gamma}$, where $\Gamma$ is a parameter determining
the extent of correlation. In other words, for very large arrays in the colored
scattering environment, degrees of freedom can get saturated to an intrinsic
limit rather than increasing linearly with the array size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0048</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0048</id><created>2012-11-30</created><authors><author><keyname>Imbert</keyname><forenames>Laurent</forenames></author><author><keyname>Philippe</keyname><forenames>Fabrice</forenames></author></authors><title>Strictly chained (p,q)-ary partitions</title><categories>math.NT cs.DM math.CO</categories><comments>18 pages</comments><msc-class>05A17 (Primary)</msc-class><journal-ref>Contributions to Discrete Mathematics, Volume 5, Number 2, Pages
  119-136, 2010, ISSN 1715-0868</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a special type of integer partitions in which the parts of the
form $p^aq^b$, for some relatively prime integers $p$ and $q$, are restricted
by divisibility conditions. We investigate the problems of generating and
encoding those partitions and give some estimates for several partition
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0052</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0052</id><created>2012-11-30</created><updated>2013-03-17</updated><authors><author><keyname>Mousavi</keyname><forenames>Hamoon</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Repetition Avoidance in Circular Factors</title><categories>cs.FL math.CO</categories><comments>12 pages; added references; DLT 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following novel variation on a classical avoidance problem
from combinatorics on words: instead of avoiding repetitions in all factors of
a word, we avoid repetitions in all factors where each individual factor is
considered as a &quot;circular word&quot;, i.e., the end of the word wraps around to the
beginning. We determine the best possible avoidance exponent for alphabet size
2 and 3, and provide a lower bound for larger alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0059</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0059</id><created>2012-11-30</created><authors><author><keyname>Sharma</keyname><forenames>Minakshi</forenames></author></authors><title>Artificial Neural Network Fuzzy Inference System (ANFIS) For Brain Tumor
  Detection</title><categories>cs.CV cs.AI</categories><comments>5 pages</comments><journal-ref>IJFLS 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection and segmentation of Brain tumor is very important because it
provides anatomical information of normal and abnormal tissues which helps in
treatment planning and patient follow-up. There are number of techniques for
image segmentation. Proposed research work uses ANFIS (Artificial Neural
Network Fuzzy Inference System) for image classification and then compares the
results with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includes
benefits of both ANN and the fuzzy logic systems. A comprehensive feature set
and fuzzy rules are selected to classify an abnormal image to the corresponding
tumor type. Experimental results illustrate promising results in terms of
classification accuracy. A comparative analysis is performed with the FCM and
K-NN to show the superior nature of ANFIS systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0064</identifier>
 <datestamp>2012-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0064</id><created>2012-12-01</created><updated>2012-12-26</updated><authors><author><keyname>Malinina</keyname><forenames>Natalia</forenames></author></authors><title>Properties of the dual planar triangulations</title><categories>cs.DM cs.CG</categories><comments>22 pages, 17 figures</comments><msc-class>05-C10, 05-C15</msc-class><acm-class>G.2.0; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is devoted to the properties of the planar triangulations. The
conjugated planar triangulation will be introduced and on the base of the
properties, which were achieved by the other authors there will be proved some
theorems, which will show the properties of the dual triangulations. Also the
numeric properties of the dual planar triangulations will be examined for the
sake of understanding the interdependences of the cyclimatic numbers of
different graphs between themselves. We'll see how the cyclomatic number of the
planar conjugated triangulation depends on the cyclomatic number of the planar
triangulation and how its increment depends on the number of the vertexes.
These characteristics will be further very important for examining of Four
Color Problem. The properties of the dual matrixes will also be examined. We
will see that both matrixes on the one hand must meet the equal requirements,
but on the other hand we will see that one characteristic cannot be fulfilled.
This fact will further form the restrictions for the solution of Four Color
Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0074</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0074</id><created>2012-12-01</created><authors><author><keyname>Esmaili</keyname><forenames>Kyumars Sheykh</forenames></author></authors><title>Challenges in Kurdish Text Processing</title><categories>cs.IR cs.CL</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite having a large number of speakers, the Kurdish language is among the
less-resourced languages. In this work we highlight the challenges and problems
in providing the required tools and techniques for processing texts written in
Kurdish. From a high-level perspective, the main challenges are: the inherent
diversity of the language, standardization and segmentation issues, and the
lack of language resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0075</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0075</id><created>2012-12-01</created><updated>2013-05-23</updated><authors><author><keyname>Huang</keyname><forenames>Chuan</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Optimal Power Allocation for Outage Minimization in Fading Channels with
  Energy Harvesting Constraints</title><categories>cs.NI cs.PF</categories><comments>submitted for possible Journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the optimal power allocation for outage minimization in
point-to-point fading channels with the energy-harvesting constraints and
channel distribution information (CDI) at the transmitter. Both the cases with
non-causal and causal energy state information (ESI) are considered, which
correspond to the energy harvesting rates being known and unknown prior to the
transmissions, respectively. For the non-causal ESI case, the average outage
probability minimization problem over a finite horizon is shown to be
non-convex for a large class of practical fading channels. However, the
globally optimal &quot;offline&quot; power allocation is obtained by a forward search
algorithm with at most $N$ one-dimensional searches, and the optimal power
profile is shown to be non-decreasing over time and have an interesting
&quot;save-then-transmit&quot; structure. In particular, for the special case of N=1, our
result revisits the classic outage capacity for fading channels with uniform
power allocation. Moreover, for the case with causal ESI, we propose both the
optimal and suboptimal &quot;online&quot; power allocation algorithms, by applying the
technique of dynamic programming and exploring the structure of optimal offline
solutions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0079</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0079</id><created>2012-12-01</created><authors><author><keyname>Governatori</keyname><forenames>Guido</forenames></author><author><keyname>Olivieri</keyname><forenames>Francesco</forenames></author><author><keyname>Rotolo</keyname><forenames>Antonino</forenames></author><author><keyname>Scannapieco</keyname><forenames>Simone</forenames></author></authors><title>Computing Strong and Weak Permissions in Defeasible Logic</title><categories>cs.LO cs.AI</categories><journal-ref>Journal of Philosophical Logic (2013) 42:799-829</journal-ref><doi>10.1007/s10992-013-9295-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an extension of Defeasible Logic to represent and
compute three concepts of defeasible permission. In particular, we discuss
different types of explicit permissive norms that work as exceptions to
opposite obligations. Moreover, we show how strong permissions can be
represented both with, and without introducing a new consequence relation for
inferring conclusions from explicit permissive norms. Finally, we illustrate
how a preference operator applicable to contrary-to-duty obligations can be
combined with a new operator representing ordered sequences of strong
permissions which derogate from prohibitions. The logical system is studied
from a computational standpoint and is shown to have liner computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0083</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0083</id><created>2012-12-01</created><authors><author><keyname>Bougrain</keyname><forenames>Laurent</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Rochel</keyname><forenames>Olivier</forenames><affiliation>INRIA</affiliation></author><author><keyname>Boussaton</keyname><forenames>Octave</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Havet</keyname><forenames>Lionel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>From the decoding of cortical activities to the control of a JACO
  robotic arm: a whole processing chain</title><categories>cs.NE cs.HC cs.RO q-bio.NC</categories><proxy>ccsd</proxy><journal-ref>CAR - Control Architecture of Robots - 2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a complete processing chain for decoding intracranial
data recorded in the cortex of a monkey and replicates the associated movements
on a JACO robotic arm by Kinova. We developed specific modules inside the
OpenViBE platform in order to build a Brain-Machine Interface able to read the
data, compute the position of the robotic finger and send this position to the
robotic arm. More pre- cisely, two client/server protocols have been tested to
transfer the finger positions: VRPN and a light protocol based on TCP/IP
sockets. According to the requested finger position, the server calls the
associ- ated functions of an API by Kinova to move the fin- gers properly.
Finally, we monitor the gap between the requested and actual fingers positions.
This chain can be generalized to any movement of the arm or wrist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0085</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0085</id><created>2012-12-01</created><authors><author><keyname>Somani</keyname><forenames>Gaurav</forenames></author><author><keyname>Khandelwal</keyname><forenames>Prateek</forenames></author><author><keyname>Phatnani</keyname><forenames>Kapil</forenames></author></authors><title>VUPIC: Virtual Machine Usage Based Placement in IaaS Cloud</title><categories>cs.DC</categories><comments>9 Pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient resource allocation is one of the critical performance challenges
in an Infrastructure as a Service (IaaS) cloud. Virtual machine (VM) placement
and migration decision making methods are integral parts of these resource
allocation mechanisms. We present a novel virtual machine placement algorithm
which takes performance isolation amongst VMs and their continuous resource
usage into account while taking placement decisions. Performance isolation is a
form of resource contention between virtual machines interested in basic low
level hardware resources (CPU, memory, storage, and networks bandwidth).
Resource contention amongst multiple co-hosted neighbouring VMs form the basis
of the presented novel approach. Experiments are conducted to show the various
categories of applications and effect of performance isolation and resource
contention amongst them. A per-VM 3-dimensional Resource Utilization Vector
(RUV) has been continuously calculated and used for placement decisions while
taking conflicting resource interests of VMs into account. Experiments using
the novel placement algorithm: VUPIC, show effective improvements in VM
performance as well as overall resource utilization of the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0087</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0087</id><created>2012-12-01</created><authors><author><keyname>Jelassi</keyname><forenames>Mohamed Nader</forenames></author><author><keyname>Yahia</keyname><forenames>Sadok Ben</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>A scalable mining of frequent quadratic concepts in d-folksonomies</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Folksonomy mining is grasping the interest of web 2.0 community since it
represents the core data of social resource sharing systems. However, a
scrutiny of the related works interested in mining folksonomies unveils that
the time stamp dimension has not been considered. For example, the wealthy
number of works dedicated to mining tri-concepts from folksonomies did not take
into account time dimension. In this paper, we will consider a folksonomy
commonly composed of triples &lt;users, tags, resources&gt; and we shall consider the
time as a new dimension. We motivate our approach by highlighting the battery
of potential applications. Then, we present the foundations for mining
quadri-concepts, provide a formal definition of the problem and introduce a new
efficient algorithm, called QUADRICONS for its solution to allow for mining
folksonomies in time, i.e., d-folksonomies. We also introduce a new closure
operator that splits the induced search space into equivalence classes whose
smallest elements are the quadri-minimal generators. Carried out experiments on
large-scale real-world datasets highlight good performances of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0094</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0094</id><created>2012-12-01</created><authors><author><keyname>Blake</keyname><forenames>Samuel T.</forenames></author><author><keyname>Tirkel</keyname><forenames>Andrew Z.</forenames></author></authors><title>A Construction for Periodic ZCZ Sequences</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a construction for periodic zero correlation zone (ZCZ)
sequences over roots of unity. The sequences share similarities to the perfect
periodic sequence constructions of Liu, Frank, and Milewski. The sequences have
two non-zero off-peak autocorrelation values which asymptotically approach $\pm
2 \pi$, so the sequences are asymptotically perfect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0096</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0096</id><created>2012-12-01</created><authors><author><keyname>Stumper</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Doetlinger</keyname><forenames>Alexander</forenames></author><author><keyname>Jung</keyname><forenames>Janos</forenames></author><author><keyname>Kennel</keyname><forenames>Ralph</forenames></author></authors><title>Predictive Control of a Permanent Magnet Synchronous Machine based on
  Real-Time Dynamic Optimization</title><categories>cs.SY</categories><comments>Proceedings of the European Conference on Power Electronics and
  Applications (EPE - ECCE Europe), paper 99, Birmingham, August 30 - September
  1, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A predictive control scheme for a permanent-magnet synchronous machine (PMSM)
is presented. It is based on a suboptimal method for computationally efficient
trajectory generation based on continuous parameterization and linear
programming. The torque controller optimizes a quadratic cost consisting of
control error and machine losses in real-time respecting voltage and current
limitations. The multivariable controller decouples the two current components
and exploits cross-coupling effects in the long-range constrained predictive
control strategy. The optimization results in fast and smooth torque dynamics
while inherently using field-weakening to improve the power efficiency and the
current dynamics in high speed operation. The performance of the scheme is
demonstrated by experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0101</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0101</id><created>2012-12-01</created><updated>2014-10-12</updated><authors><author><keyname>Cheng</keyname><forenames>Fan</forenames></author><author><keyname>Yeung</keyname><forenames>Raymond W.</forenames></author></authors><title>Performance Bounds on a Wiretap Network with Arbitrary Wiretap Sets</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Consider a communication network represented by a directed graph
$\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of
nodes and $\mathcal{E}$ is the set of point-to-point channels in the network.
On the network a secure message $M$ is transmitted, and there may exist
wiretappers who want to obtain information about the message. In secure network
coding, we aim to find a network code which can protect the message against the
wiretapper whose power is constrained. Cai and Yeung \cite{cai2002secure}
studied the model in which the wiretapper can access any one but not more than
one set of channels, called a wiretap set, out of a collection $\mathcal{A}$ of
all possible wiretap sets. In order to protect the message, the message needs
to be mixed with a random key $K$. They proved tight fundamental performance
bounds when $\mathcal{A}$ consists of all subsets of $\mathcal{E}$ of a fixed
size $r$. However, beyond this special case, obtaining such bounds is much more
difficult. In this paper, we investigate the problem when $\mathcal{A}$
consists of arbitrary subsets of $\mathcal{E}$ and obtain the following
results: 1) an upper bound on $H(M)$; 2) a lower bound on $H(K)$ in terms of
$H(M)$. The upper bound on $H(M)$ is explicit, while the lower bound on $H(K)$
can be computed in polynomial time when $|\mathcal{A}|$ is fixed. The tightness
of the lower bound for the point-to-point communication system is also proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0106</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0106</id><created>2012-12-01</created><authors><author><keyname>Crowston</keyname><forenames>R.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author><author><keyname>Raman</keyname><forenames>V.</forenames></author><author><keyname>Saurabh</keyname><forenames>S.</forenames></author><author><keyname>Yeo</keyname><forenames>A.</forenames></author></authors><title>Fixed-parameter tractability of satisfying beyond the number of
  variables</title><categories>cs.DS</categories><journal-ref>Algorithmica, 2012</journal-ref><doi>10.1007/s00453-012-9697-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a CNF formula $F$ as a multiset of clauses: $F=\{c_1,..., c_m\}$.
The set of variables of $F$ will be denoted by $V(F)$. Let $B_F$ denote the
bipartite graph with partite sets $V(F)$ and $F$ and with an edge between $v
\in V(F)$ and $c \in F$ if $v \in c$ or $\bar{v} \in c$. The matching number
$\nu(F)$ of $F$ is the size of a maximum matching in $B_F$. In our main result,
we prove that the following parameterization of {\sc MaxSat} (denoted by
$(\nu(F)+k)$-\textsc{SAT}) is fixed-parameter tractable: Given a formula $F$,
decide whether we can satisfy at least $\nu(F)+k$ clauses in $F$, where $k$ is
the parameter.
  A formula $F$ is called variable-matched if $\nu(F)=|V(F)|.$ Let
$\delta(F)=|F|-|V(F)|$ and $\delta^*(F)=\max_{F'\subseteq F} \delta(F').$ Our
main result implies fixed-parameter tractability of {\sc MaxSat} parameterized
by $\delta(F)$ for variable-matched formulas $F$; this complements related
results of Kullmann (2000) and Szeider (2004) for {\sc MaxSat} parameterized by
$\delta^*(F)$.
  To obtain our main result, we reduce $(\nu(F)+k)$-\textsc{SAT} into the
following parameterization of the {\sc Hitting Set} problem (denoted by
$(m-k)$-{\sc Hitting Set}): given a collection $\cal C$ of $m$ subsets of a
ground set $U$ of $n$ elements, decide whether there is $X\subseteq U$ such
that $C\cap X\neq \emptyset$ for each $C\in \cal C$ and $|X|\le m-k,$ where $k$
is the parameter. Gutin, Jones and Yeo (2011) proved that $(m-k)$-{\sc Hitting
Set} is fixed-parameter tractable by obtaining an exponential kernel for the
problem. We obtain two algorithms for $(m-k)$-{\sc Hitting Set}: a
deterministic algorithm of runtime $O((2e)^{2k+O(\log^2 k)} (m+n)^{O(1)})$ and
a randomized algorithm of expected runtime $O(8^{k+O(\sqrt{k})} (m+n)^{O(1)})$.
Our deterministic algorithm improves an algorithm that follows from the
kernelization result of Gutin, Jones and Yeo (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0114</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0114</id><created>2012-12-01</created><authors><author><keyname>Godbole</keyname><forenames>Bhalchandra B.</forenames></author><author><keyname>Aldar</keyname><forenames>Dilip S.</forenames></author></authors><title>Performance Improvement by Changing Modulation Methods for Software
  Defined Radios</title><categories>cs.OH</categories><comments>IJACSA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an automatic switching of modulation method to
reconfigure transceivers of Software Defined Radio (SDR) based wireless
communication system. The programmable architecture of Software Radio promotes
a flexible implementation of modulation methods. This flexibility also
translates into adaptively, which is used here to optimize the throughput of a
wireless network, operating under varying channel conditions. It is robust and
efficient with processing time overhead that still allows the SDR to maintain
its real-time operating objectives. This technique is studied for digital
wireless communication systems. Tests and simulations using an AWGN channel
show that the SNR threshold is 5dB for the case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0116</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0116</id><created>2012-12-01</created><authors><author><keyname>Aldar</keyname><forenames>Dilip s.</forenames></author></authors><title>Centralized Integrated Spectrum Sensing for Cognitive Radios</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sensing is the challenge for cognitive radio design and
implementation, which allows the secondary user to access the primary bands
without interference with primary users. Cognitive radios should decide on the
best spectrum band to meet the Quality of service requirements over all
available spectrum bands. This paper investigates the integrated centralized
spectrum sensing techniques in multipath fading environment and the performance
was analyzed with energy detection and wavelet based sensing techniques for
unknown signal. Keywords: Cognitive Radio, Spectrum Sensing, Signal Detection,
Primary User, Secondary User
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0117</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0117</id><created>2012-12-01</created><authors><author><keyname>Crowston</keyname><forenames>R.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author><author><keyname>Saurabh</keyname><forenames>S.</forenames></author><author><keyname>Yeo</keyname><forenames>A.</forenames></author></authors><title>Parameterized Study of the Test Cover Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We carry out a systematic study of a natural covering problem, used for
identification across several areas, in the realm of parameterized complexity.
In the {\sc Test Cover} problem we are given a set $[n]=\{1,...,n\}$ of items
together with a collection, $\cal T$, of distinct subsets of these items called
tests. We assume that $\cal T$ is a test cover, i.e., for each pair of items
there is a test in $\cal T$ containing exactly one of these items. The
objective is to find a minimum size subcollection of $\cal T$, which is still a
test cover. The generic parameterized version of {\sc Test Cover} is denoted by
$p(k,n,|{\cal T}|)$-{\sc Test Cover}. Here, we are given $([n],\cal{T})$ and a
positive integer parameter $k$ as input and the objective is to decide whether
there is a test cover of size at most $p(k,n,|{\cal T}|)$. We study four
parameterizations for {\sc Test Cover} and obtain the following:
  (a) $k$-{\sc Test Cover}, and $(n-k)$-{\sc Test Cover} are fixed-parameter
tractable (FPT).
  (b) $(|{\cal T}|-k)$-{\sc Test Cover} and $(\log n+k)$-{\sc Test Cover} are
W[1]-hard. Thus, it is unlikely that these problems are FPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0121</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0121</id><created>2012-12-01</created><authors><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author><author><keyname>Servedio</keyname><forenames>Vito D. P.</forenames></author><author><keyname>Tria</keyname><forenames>Francesca</forenames></author></authors><title>Opinion dynamics with disagreement and modulated information</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Journal of Statistical Physics, Volume 151, Issue 1-2, pp 218-237,
  2013</journal-ref><doi>10.1007/s10955-013-0724-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion dynamics concerns social processes through which populations or
groups of individuals agree or disagree on specific issues. As such, modelling
opinion dynamics represents an important research area that has been
progressively acquiring relevance in many different domains. Existing
approaches have mostly represented opinions through discrete binary or
continuous variables by exploring a whole panoply of cases: e.g. independence,
noise, external effects, multiple issues. In most of these cases the crucial
ingredient is an attractive dynamics through which similar or similar enough
agents get closer. Only rarely the possibility of explicit disagreement has
been taken into account (i.e., the possibility for a repulsive interaction
among individuals' opinions), and mostly for discrete or 1-dimensional
opinions, through the introduction of additional model parameters. Here we
introduce a new model of opinion formation, which focuses on the interplay
between the possibility of explicit disagreement, modulated in a
self-consistent way by the existing opinions' overlaps between the interacting
individuals, and the effect of external information on the system. Opinions are
modelled as a vector of continuous variables related to multiple possible
choices for an issue. Information can be modulated to account for promoting
multiple possible choices. Numerical results show that extreme information
results in segregation and has a limited effect on the population, while milder
messages have better success and a cohesion effect. Additionally, the initial
condition plays an important role, with the population forming one or multiple
clusters based on the initial average similarity between individuals, with a
transition point depending on the number of opinion choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0124</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0124</id><created>2012-12-01</created><authors><author><keyname>Martinez</keyname><forenames>Genaro J.</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Seck-Tuoh-Mora</keyname><forenames>Juan C.</forenames></author><author><keyname>Alonso-Sanz</keyname><forenames>Ramon</forenames></author></authors><title>How to make dull cellular automata complex by adding memory: Rule 126
  case study</title><categories>nlin.CG cs.ET</categories><journal-ref>Complexity 15(6), 34-49, 2010</journal-ref><doi>10.1002/cplx.20311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Rule 126 elementary cellular automaton (ECA) we demonstrate that a
chaotic discrete system --- when enriched with memory -- hence exhibits complex
dynamics where such space exploits on an ample universe of periodic patterns
induced from original information of the ahistorical system. First we analyse
classic ECA Rule 126 to identify basic characteristics with mean field theory,
basins, and de Bruijn diagrams. In order to derive this complex dynamics, we
use a kind of memory on Rule 126; from here interactions between gliders are
studied for detecting stationary patterns, glider guns and simulating specific
simple computable functions produced by glider collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0134</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0134</id><created>2012-12-01</created><authors><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author></authors><title>Fingertip Detection: A Fast Method with Natural Hand</title><categories>cs.CV</categories><journal-ref>International Journal of Embedded Systems and Computer
  Engineering, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many vision based applications have used fingertips to track or manipulate
gestures in their applications. Gesture identification is a natural way to pass
the signals to the machine, as the human express its feelings most of the time
with hand expressions. Here a novel time efficient algorithm has been described
for fingertip detection. This method is invariant to hand direction and in
preprocessing it cuts only hand part from the full image, hence further
computation would be much faster than processing full image. Binary silhouette
of the input image is generated using HSV color space based skin filter and
hand cropping done based on intensity histogram of the hand image
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0139</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0139</id><created>2012-12-01</created><authors><author><keyname>Chotard</keyname><forenames>Alexandre</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Cumulative Step-size Adaptation on Linear Functions</title><categories>cs.LG stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1206.1208</comments><proxy>ccsd</proxy><journal-ref>PPSN 2012 (2012) 72-81</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,
where the step size is adapted measuring the length of a so-called cumulative
path. The cumulative path is a combination of the previous steps realized by
the algorithm, where the importance of each step decreases with time. This
article studies the CSA-ES on composites of strictly increasing functions with
affine linear functions through the investigation of its underlying Markov
chains. Rigorous results on the change and the variation of the step size are
derived with and without cumulation. The step-size diverges geometrically fast
in most cases. Furthermore, the influence of the cumulation parameter is
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0141</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0141</id><created>2012-12-01</created><authors><author><keyname>Purohit</keyname><forenames>Hemant</forenames></author><author><keyname>Ruan</keyname><forenames>Yiye</forenames></author><author><keyname>Fuhry</keyname><forenames>David</forenames></author><author><keyname>Parthasarathy</keyname><forenames>Srinivasan</forenames></author><author><keyname>Sheth</keyname><forenames>Amit</forenames></author></authors><title>On the Role of Social Identity and Cohesion in Characterizing Online
  Social Communities</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.5.3; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two prevailing theories for explaining social group or community structure
are cohesion and identity. The social cohesion approach posits that social
groups arise out of an aggregation of individuals that have mutual
interpersonal attraction as they share common characteristics. These
characteristics can range from common interests to kinship ties and from social
values to ethnic backgrounds. In contrast, the social identity approach posits
that an individual is likely to join a group based on an intrinsic
self-evaluation at a cognitive or perceptual level. In other words group
members typically share an awareness of a common category membership.
  In this work we seek to understand the role of these two contrasting theories
in explaining the behavior and stability of social communities in Twitter. A
specific focal point of our work is to understand the role of these theories in
disparate contexts ranging from disaster response to socio-political activism.
We extract social identity and social cohesion features-of-interest for large
scale datasets of five real-world events and examine the effectiveness of such
features in capturing behavioral characteristics and the stability of groups.
We also propose a novel measure of social group sustainability based on the
divergence in group discussion. Our main findings are: 1) Sharing of social
identities (especially physical location) among group members has a positive
impact on group sustainability, 2) Structural cohesion (represented by high
group density and low average shortest path length) is a strong indicator of
group sustainability, and 3) Event characteristics play a role in shaping group
sustainability, as social groups in transient events behave differently from
groups in events that last longer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0142</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0142</id><created>2012-12-01</created><updated>2013-04-02</updated><authors><author><keyname>Sermanet</keyname><forenames>Pierre</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author><author><keyname>Chintala</keyname><forenames>Soumith</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Pedestrian Detection with Unsupervised Multi-Stage Feature Learning</title><categories>cs.CV cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedestrian detection is a problem of considerable practical interest. Adding
to the list of successful applications of deep learning methods to vision, we
report state-of-the-art and competitive results on all major pedestrian
datasets with a convolutional network model. The model uses a few new twists,
such as multi-stage features, connections that skip layers to integrate global
shape information with local distinctive motif information, and an unsupervised
method based on convolutional sparse coding to pre-train the filters at each
stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0146</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0146</id><created>2012-12-01</created><authors><author><keyname>Ruan</keyname><forenames>Yiye</forenames></author><author><keyname>Fuhry</keyname><forenames>David</forenames></author><author><keyname>Parthasarathy</keyname><forenames>Srinivasan</forenames></author></authors><title>Efficient Community Detection in Large Networks using Content and Links</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss a very simple approach of combining content and link
information in graph structures for the purpose of community discovery, a
fundamental task in network analysis. Our approach hinges on the basic
intuition that many networks contain noise in the link structure and that
content information can help strengthen the community signal. This enables ones
to eliminate the impact of noise (false positives and false negatives), which
is particularly prevalent in online social networks and Web-scale information
networks.
  Specifically we introduce a measure of signal strength between two nodes in
the network by fusing their link strength with content similarity. Link
strength is estimated based on whether the link is likely (with high
probability) to reside within a community. Content similarity is estimated
through cosine similarity or Jaccard coefficient. We discuss a simple mechanism
for fusing content and link similarity. We then present a biased edge sampling
procedure which retains edges that are locally relevant for each graph node.
The resulting backbone graph can be clustered using standard community
discovery algorithms such as Metis and Markov clustering.
  Through extensive experiments on multiple real-world datasets (Flickr,
Wikipedia and CiteSeer) with varying sizes and characteristics, we demonstrate
the effectiveness and efficiency of our methods over state-of-the-art learning
and mining approaches several of which also attempt to combine link and content
analysis for the purposes of community discovery. Specifically we always find a
qualitative benefit when combining content with link analysis. Additionally our
biased graph sampling approach realizes a quantitative benefit in that it is
typically several orders of magnitude faster than competing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0156</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0156</id><created>2012-12-01</created><authors><author><keyname>Zhang</keyname><forenames>Miranda</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Haller</keyname><forenames>Armin</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Menzel</keyname><forenames>Michael</forenames></author><author><keyname>Nepal</keyname><forenames>Surya</forenames></author></authors><title>An Ontology based System for Cloud Infrastructure Services Discovery</title><categories>cs.DC</categories><comments>Accepted as an invited paper by Collaboratecom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cloud infrastructure services landscape advances steadily leaving users
in the agony of choice. As a result, Cloud service identification and discovery
remains a hard problem due to different service descriptions, non standardised
naming conventions and heterogeneous types and features of Cloud services. In
this paper, we present an OWL based ontology, the Cloud Computing Ontology
(CoCoOn) that defines functional and non functional concepts, attributes and
relations of infrastructure services. We also present a system...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0160</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0160</id><created>2012-12-01</created><authors><author><keyname>Korkmaz</keyname><forenames>Fatih</forenames></author><author><keyname>Cakir</keyname><forenames>M. Faruk</forenames></author><author><keyname>Korkmaz</keyname><forenames>Yilmaz</forenames></author><author><keyname>Topaloglu</keyname><forenames>Ismail</forenames></author></authors><title>Fuzzy Based Stator Flux Optimizer Design For Direct Torque Control</title><categories>cs.SY</categories><comments>9 pages</comments><doi>10.5121/ijics.2012.2404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct Torque Control (DTC) is well known as an effective control technique
for high performance drives in a wide variety of industrial applications and
conventional DTC technique uses two constant reference value: torque and stator
flux. In this paper, a new fuzzy based stator flux optimizer has been proposed
for DTC controlled induction motor drivers and simulation studies have been
carried out with Matlab/Simulink to compare the proposed system behaviours at
vary load conditions. The most important feature of the proposed fuzzy logic
based stator flux optimizer that it self-regulates the stator flux reference
value using the motor load situation without need of any motor parameters.
Simulation results show that the performance of the proposed DTC technique has
been improved and especially at low-load conditions torque ripples are greatly
reduced with respect to the conventional DTC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0167</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0167</id><created>2012-12-01</created><authors><author><keyname>Chen</keyname><forenames>Zhaoqun</forenames></author><author><keyname>Liu</keyname><forenames>Pengfei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Follow Whom? Chinese Users Have Different Choice</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sina Weibo, which was launched in 2009, is the most popular Chinese
micro-blogging service. It has been reported that Sina Weibo has more than 400
million registered users by the end of the third quarter in 2012. Sina Weibo
and Twitter have a lot in common, however, in terms of the following
preference, Sina Weibo users, most of whom are Chinese, behave differently
compared with those of Twitter.
  This work is based on a data set of Sina Weibo which contains 80.8 million
users' profiles and 7.2 billion relations and a large data set of Twitter.
Firstly some basic features of Sina Weibo and Twitter are analyzed such as
degree and activeness distribution, correlation between degree and activeness,
and the degree of separation. Then the following preference is investigated by
studying the assortative mixing, friend similarities, following distribution,
edge balance ratio, and ranking correlation, where edge balance ratio is newly
proposed to measure balance property of graphs. It is found that Sina Weibo has
a lower reciprocity rate, more positive balanced relations and is more
disassortative. Coinciding with Asian traditional culture, the following
preference of Sina Weibo users is more concentrated and hierarchical: they are
more likely to follow people at higher or the same social levels and less
likely to follow people lower than themselves. In contrast, the same kind of
following preference is weaker in Twitter. Twitter users are open as they
follow people from levels, which accords with its global characteristic and the
prevalence of western civilization. The message forwarding behavior is studied
by displaying the propagation levels, delays, and critical users. The following
preference derives from not only the usage habits but also underlying reasons
such as personalities and social moralities that is worthy of future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0169</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0169</id><created>2012-12-01</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Popovi&#x107;</keyname><forenames>Sini&#x161;a</forenames></author><author><keyname>&#x106;osi&#x107;</keyname><forenames>Kre&#x161;imir</forenames></author></authors><title>Towards semantic and affective coupling in emotionally annotated
  databases</title><categories>cs.HC cs.MM</categories><comments>6 pages, 6 figures</comments><journal-ref>Proceedings of the 35nd International Convention MIPRO 2012 (2012)
  1003-1008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotionally annotated databases are repositories of multimedia documents with
annotated affective content that elicit emotional responses in exposed human
subjects. They are primarily used in research of human emotions, attention and
development of stress-related mental disorders. This can be successfully
exploited in larger processes like selection, evaluation and training of
personnel for occupations involving high stress levels. Emotionally annotated
databases are also used in multimodal affective user interfaces to facilitate
richer and more intuitive human-computer interaction. Multimedia documents in
emotionally annotated databases must have maximum personal ego relevance to be
the most effective in all these applications. For this reason flexible
construction of subject-specific of emotionally annotated databases is
imperative. But current construction process is lengthy and labor intensive
because it inherently includes an elaborate tagging experiment involving a team
of human experts. This is unacceptable since the creation of new databases or
modification of the existing ones becomes slow and difficult. We identify a
positive correlation between the affect and semantics in the existing
emotionally annotated databases and propose to exploit this feature with an
interactive relevance feedback for a more efficient construction of emotionally
annotated databases. Automatic estimation of affective annotations from
existing semantics enhanced with information refinement processes may lead to
an efficient construction of high-quality emotionally annotated databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0170</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0170</id><created>2012-12-01</created><authors><author><keyname>Mbikayi</keyname><forenames>Herve Kabamba</forenames></author></authors><title>An Evolution Strategy Approach toward Rule-set Generation for Network
  Intrusion Detection Systems (IDS)</title><categories>cs.CR cs.NE</categories><comments>5 pages</comments><journal-ref>International Journal of Soft Computing and Engineering(TM), 2(5),
  201 - 205, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing number of intrusions in system and network
infrastructures, Intrusion Detection Systems (IDS) have become an active area
of research to develop reliable and effective solutions to detect and counter
them. The use of Evolutionary Algorithms in IDS has proved its maturity over
the times. Although most of the research works have been based on the use of
genetic algorithms in IDS, this paper presents an approach toward the
generation of rules for the identification of anomalous connections using
evolution strategies . The emphasis is given on how the problem can be modeled
into ES primitives and how the fitness of the population can be evaluated in
order to find the local optima, therefore resulting in optimal rules that can
be used for detecting intrusions in intrusion detection systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0171</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0171</id><created>2012-12-01</created><authors><author><keyname>Ruozzi</keyname><forenames>Nicholas</forenames></author><author><keyname>Tatikonda</keyname><forenames>Sekhar</forenames></author></authors><title>Message-Passing Algorithms for Quadratic Minimization</title><categories>cs.IT cs.LG math.IT stat.ML</categories><journal-ref>Journal of Machine Learning Research. 14 (Aug) :2287-2314, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian belief propagation (GaBP) is an iterative algorithm for computing
the mean of a multivariate Gaussian distribution, or equivalently, the minimum
of a multivariate positive definite quadratic function. Sufficient conditions,
such as walk-summability, that guarantee the convergence and correctness of
GaBP are known, but GaBP may fail to converge to the correct solution given an
arbitrary positive definite quadratic function. As was observed in previous
work, the GaBP algorithm fails to converge if the computation trees produced by
the algorithm are not positive definite. In this work, we will show that the
failure modes of the GaBP algorithm can be understood via graph covers, and we
prove that a parameterized generalization of the min-sum algorithm can be used
to ensure that the computation trees remain positive definite whenever the
input matrix is positive definite. We demonstrate that the resulting algorithm
is closely related to other iterative schemes for quadratic minimization such
as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,
that there always exists a choice of parameters such that the above
generalization of the GaBP algorithm converges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0190</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0190</id><created>2012-12-02</created><updated>2012-12-11</updated><authors><author><keyname>He</keyname><forenames>Xu</forenames></author><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>A Comparative Study of Discretization Approaches for Granular
  Association Rule Mining</title><categories>cs.DB cs.IR</categories><comments>17 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1210.0065</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Granular association rule mining is a new relational data mining approach to
reveal patterns hidden in multiple tables. The current research of granular
association rule mining considers only nominal data. In this paper, we study
the impact of discretization approaches on mining semantically richer and
stronger rules from numeric data. Specifically, the Equal Width approach and
the Equal Frequency approach are adopted and compared. The setting of interval
numbers is a key issue in discretization approaches, so we compare different
settings through experiments on a well-known real life data set. Experimental
results show that: 1) discretization is an effective preprocessing technique in
mining stronger rules; 2) the Equal Frequency approach helps generating more
rules than the Equal Width approach; 3) with certain settings of interval
numbers, we can obtain much more rules than others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0191</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0191</id><created>2012-12-02</created><updated>2015-01-27</updated><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>Finite and infinite basis in P and NP</title><categories>cs.CC</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provide new approach to solve P vs NP problem by using
cardinality of bases function. About NP-Complete problems, we can divide to
infinite disjunction of P-Complete problems. These P-Complete problems are
independent of each other in disjunction. That is, NP-Complete problem is in
infinite dimension function space that bases are P-Complete. The other hand,
any P-Complete problem have at most a finite number of P-Complete basis. The
reason is that each P problems have at most finite number of Least fixed point
operator. Therefore, we cannot describe NP-Complete problems in P. We can also
prove this result from incompleteness of P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0200</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0200</id><created>2012-12-02</created><authors><author><keyname>Tsukerman</keyname><forenames>Emmanuel</forenames></author></authors><title>Discrete Conics</title><categories>math.MG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce discrete conics, polygonal analogues of conics.
We show that discrete conics satisfy a number of nice properties analogous to
those of conics, and arise naturally from several constructions, including the
discrete negative pedal construction and an action of a group acting on a
focus-sharing pencil of conics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0202</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0202</id><created>2012-12-02</created><updated>2012-12-04</updated><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author></authors><title>Approximating Large Frequency Moments with Pick-and-Drop Sampling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given data stream $D = \{p_1,p_2,...,p_m\}$ of size $m$ of numbers from
$\{1,..., n\}$, the frequency of $i$ is defined as $f_i = |\{j: p_j = i\}|$.
The $k$-th \emph{frequency moment} of $D$ is defined as $F_k = \sum_{i=1}^n
f_i^k$. We consider the problem of approximating frequency moments in
insertion-only streams for $k\ge 3$. For any constant $c$ we show an
$O(n^{1-2/k}\log(n)\log^{(c)}(n))$ upper bound on the space complexity of the
problem. Here $\log^{(c)}(n)$ is the iterative $\log$ function. To simplify the
presentation, we make the following assumptions: $n$ and $m$ are polynomially
far; approximation error $\epsilon$ and parameter $k$ are constants. We observe
a natural bijection between streams and special matrices. Our main technical
contribution is a non-uniform sampling method on matrices. We call our method a
\emph{pick-and-drop sampling}; it samples a heavy element (i.e., element $i$
with frequency $\Omega(F_k)$) with probability $\Omega(1/n^{1-2/k})$ and gives
approximation $\tilde{f_i} \ge (1-\epsilon)f_i$. In addition, the estimations
never exceed the real values, that is $ \tilde{f_j} \le f_j$ for all $j$. As a
result, we reduce the space complexity of finding a heavy element to
$O(n^{1-2/k}\log(n))$ bits. We apply our method of recursive sketches and
resolve the problem with $O(n^{1-2/k}\log(n)\log^{(c)}(n))$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0207</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0207</id><created>2012-12-02</created><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Wu</keyname><forenames>Hongrun</forenames></author><author><keyname>Qin</keyname><forenames>Jun</forenames></author><author><keyname>Lan</keyname><forenames>Wenfei</forenames></author><author><keyname>Du</keyname><forenames>Wenhua</forenames></author></authors><title>Modelling Multi-Trait Scale-free Networks by Optimization</title><categories>cs.SI math-ph math.MP nlin.AO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, one paper in Nature(Papadopoulos, 2012) raised an old debate on the
origin of the scale-free property of complex networks, which focuses on whether
the scale-free property origins from the optimization or not. Because the
real-world complex networks often have multiple traits, any explanation on the
scale-free property of complex networks should be capable of explaining the
other traits as well. This paper proposed a framework which can model
multi-trait scale-free networks based on optimization, and used three examples
to demonstrate its effectiveness. The results suggested that the optimization
is a more generalized explanation because it can not only explain the origin of
the scale-free property, but also the origin of the other traits in a uniform
way. This paper provides a universal method to get ideal networks for the
researches such as epidemic spreading and synchronization on complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0215</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0215</id><created>2012-12-02</created><authors><author><keyname>Chakraborty</keyname><forenames>Mriganka</forenames></author></authors><title>Artificial Neural Network for Performance Modeling and Optimization of
  CMOS Analog Circuits</title><categories>cs.NE</categories><comments>International Journal of Computer Applications November 2012</comments><doi>10.5120/9380-3731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an implementation of multilayer feed forward neural
networks (NN) to optimize CMOS analog circuits. For modeling and design
recently neural network computational modules have got acceptance as an
unorthodox and useful tool. To achieve high performance of active or passive
circuit component neural network can be trained accordingly. A well trained
neural network can produce more accurate outcome depending on its learning
capability. Neural network model can replace empirical modeling solutions
limited by range and accuracy.[2] Neural network models are easy to obtain for
new circuits or devices which can replace analytical methods. Numerical
modeling methods can also be replaced by neural network model due to their
computationally expansive behavior.[2][10][20]. The pro- posed implementation
is aimed at reducing resource requirement, without much compromise on the
speed. The NN ensures proper functioning by assigning the appropriate inputs,
weights, biases, and excitation function of the layer that is currently being
computed. The concept used is shown to be very effective in reducing resource
requirements and enhancing speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0217</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0217</id><created>2012-12-02</created><authors><author><keyname>Xi</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Cultural evolution and personalization</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social sciences, there is currently no consensus on the mechanism for
cultural evolution. The evolution of first names of newborn babies offers a
remarkable example for the researches in the field. Here we perform statistical
analyses on over 100 years of data in the United States. We focus in particular
on how the frequency-rank distribution and inequality of baby names change over
time. We propose a stochastic model where name choice is determined by
personalized preference and social influence. Remarkably, variations on the
strength of personalized preference can account satisfactorily for the observed
empirical features. Therefore, we claim that personalization drives cultural
evolution, at least in the example of baby names.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0220</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0220</id><created>2012-12-02</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>Metaheuristic Optimization: Algorithm Analysis and Open Problems</title><categories>math.OC cs.NE</categories><comments>14 pages 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1208.0527</comments><msc-class>90C26</msc-class><journal-ref>Lecture Notes in Computer Sciences, Vol. 6630 (2011) pp. 21-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metaheuristic algorithms are becoming an important part of modern
optimization. A wide range of metaheuristic algorithms have emerged over the
last two decades, and many metaheuristics such as particle swarm optimization
are becoming increasingly popular. Despite their popularity, mathematical
analysis of these algorithms lacks behind. Convergence analysis still remains
unsolved for the majority of metaheuristic algorithms, while efficiency
analysis is equally challenging. In this paper, we intend to provide an
overview of convergence and efficiency studies of metaheuristics, and try to
provide a framework for analyzing metaheuristics in terms of convergence and
efficiency. This can form a basis for analyzing other algorithms. We also
outline some open questions as further research topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0224</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0224</id><created>2012-12-02</created><authors><author><keyname>Babenko</keyname><forenames>Maxim A.</forenames></author><author><keyname>Karzanov</keyname><forenames>Alexander V.</forenames></author></authors><title>On Weighted Multicommodity Flows in Directed Networks</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><msc-class>05C21, 05C85, 90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G = (VG, AG)$ be a directed graph with a set $S \subseteq VG$ of
terminals and nonnegative integer arc capacities $c$. A feasible multiflow is a
nonnegative real function $F(P)$ of &quot;flows&quot; on paths $P$ connecting distinct
terminals such that the sum of flows through each arc $a$ does not exceed
$c(a)$. Given $\mu \colon S \times S \to \R_+$, the \emph{$\mu$-value} of $F$
is $\sum_P F(P) \mu(s_P, t_P)$, where $s_P$ and $t_P$ are the start and end
vertices of a path $P$, respectively.
  Using a sophisticated topological approach, Hirai and Koichi showed that the
maximum $\mu$-value multiflow problem has an integer optimal solution when
$\mu$ is the distance generated by subtrees of a weighted directed tree and
$(G,S,c)$ satisfies certain Eulerian conditions.
  We give a combinatorial proof of that result and devise a strongly polynomial
combinatorial algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0229</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0229</id><created>2012-12-02</created><authors><author><keyname>Wolff</keyname><forenames>James Gerard</forenames></author></authors><title>Simplification and integration in computing and cognition: the SP theory
  and the multiple alignment concept</title><categories>cs.AI cs.CL</categories><comments>32 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this article is to describe potential benefits and
applications of the SP theory, a unique attempt to simplify and integrate ideas
across artificial intelligence, mainstream computing and human cognition, with
information compression as a unifying theme. The theory, including a concept of
multiple alignment, combines conceptual simplicity with descriptive and
explanatory power in several areas including representation of knowledge,
natural language processing, pattern recognition, several kinds of reasoning,
the storage and retrieval of information, planning and problem solving,
unsupervised learning, information compression, and human perception and
cognition. In the SP machine -- an expression of the SP theory which is
currently realised in the form of computer models -- there is potential for an
overall simplification of computing systems, including software. As a theory
with a broad base of support, the SP theory promises useful insights in many
areas and the integration of structures and functions, both within a given area
and amongst different areas. There are potential benefits in natural language
processing (with potential for the understanding and translation of natural
languages), the need for a versatile intelligence in autonomous robots,
computer vision, intelligent databases, maintaining multiple versions of
documents or web pages, software engineering, criminal investigations, the
management of big data and gaining benefits from it, the semantic web, medical
diagnosis, the detection of computer viruses, the economical transmission of
data, and data fusion. Further development of these ideas would be facilitated
by the creation of a high-parallel, web-based, open-source version of the SP
machine, with a good user interface. This would provide a means for researchers
to explore what can be done with the system and to refine it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0240</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0240</id><created>2012-12-02</created><updated>2014-10-21</updated><authors><author><keyname>Jain</keyname><forenames>Abhisekh</forenames></author><author><keyname>Seshadri</keyname><forenames>Arvind</forenames></author><author><keyname>S</keyname><forenames>Balaji B.</forenames></author><author><keyname>Parasuraman</keyname><forenames>Ramviyas</forenames></author></authors><title>Onboard Dynamic Rail Track Safety Monitoring System</title><categories>cs.MA cs.SY</categories><comments>International Conference on Advanced Communication Systems 2007,
  Coimbatore, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This proposal aims at solving one of the long prevailing problems in the
Indian Railways. This simple method of continuous monitoring and assessment of
the condition of the rail tracks can prevent major disasters and save precious
human lives. Our method is capable of alerting the train in case of any
dislocations in the track or change in strength of the soil. Also it can avert
the collisions of the train with other or with the vehicles trying to move
across the unmanned level crossings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0243</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0243</id><created>2012-12-02</created><updated>2014-04-08</updated><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author></authors><title>Estimation for Monotone Sampling: Competitiveness and Customization</title><categories>math.ST cs.DB stat.TH</categories><comments>28 pages; Improved write up, presentation in the context of the more
  general monotone sampling formulation (instead of coordinated sampling).
  Bounds on universal ratio removed to make the paper more focused, since it is
  mainly of theoretical interest</comments><acm-class>G.3; E.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random samples are lossy summaries which allow queries posed over the data to
be approximated by applying an appropriate estimator to the sample. The
effectiveness of sampling, however, hinges on estimator selection. The choice
of estimators is subjected to global requirements, such as unbiasedness and
range restrictions on the estimate value, and ideally, we seek estimators that
are both efficient to derive and apply and {\em admissible} (not dominated, in
terms of variance, by other estimators). Nevertheless, for a given data domain,
sampling scheme, and query, there are many admissible estimators. We study the
choice of admissible nonnegative and unbiased estimators for monotone sampling
schemes. Monotone sampling schemes are implicit in many applications of massive
data set analysis. Our main contribution is general derivations of admissible
estimators with desirable properties. We present a construction of {\em
order-optimal} estimators, which minimize variance according to {\em any}
specified priorities over the data domain. Order-optimality allows us to
customize the derivation to common patterns that we can learn or observe in the
data. When we prioritize lower values (e.g., more similar data sets when
estimating difference), we obtain the L$^*$ estimator, which is the unique
monotone admissible estimator. We show that the L$^*$ estimator is
4-competitive and dominates the classic Horvitz-Thompson estimator. These
properties make the L$^*$ estimator a natural default choice. We also present
the U$^*$ estimator, which prioritizes large values (e.g., less similar data
sets). Our estimator constructions are both easy to apply and possess desirable
properties, allowing us to make the most from our summarized data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0248</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0248</id><created>2012-12-02</created><updated>2012-12-07</updated><authors><author><keyname>Linden</keyname><forenames>Noah</forenames></author><author><keyname>Mosonyi</keyname><forenames>Mil&#xe1;n</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>The structure of Renyi entropic inequalities</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>15 pages. v2: minor technical changes in Theorems 10 and 11</comments><journal-ref>Proc. R. Soc. A 469(2158):20120737, 2013</journal-ref><doi>10.1098/rspa.2012.0737</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the universal inequalities relating the alpha-Renyi entropies
of the marginals of a multi-partite quantum state. This is in analogy to the
same question for the Shannon and von Neumann entropy (alpha=1) which are known
to satisfy several non-trivial inequalities such as strong subadditivity.
Somewhat surprisingly, we find for 0&lt;alpha&lt;1, that the only inequality is
non-negativity: In other words, any collection of non-negative numbers assigned
to the nonempty subsets of n parties can be arbitrarily well approximated by
the alpha-entropies of the 2^n-1 marginals of a quantum state.
  For alpha&gt;1 we show analogously that there are no non-trivial homogeneous (in
particular no linear) inequalities. On the other hand, it is known that there
are further, non-linear and indeed non-homogeneous, inequalities delimiting the
alpha-entropies of a general quantum state.
  Finally, we also treat the case of Renyi entropies restricted to classical
states (i.e. probability distributions), which in addition to non-negativity
are also subject to monotonicity. For alpha different from 0 and 1 we show that
this is the only other homogeneous relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0253</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0253</id><created>2012-12-02</created><authors><author><keyname>Polonowski</keyname><forenames>Emmanuel</forenames></author></authors><title>DBGen User Manual</title><categories>cs.LO</categories><report-no>TR--LACL--2012--4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DBGen is a tool for Coq developers. It takes as input the definition of a
term structure with bindings annotations and generates definitions and
properties for lifting and substitution in the De Bruijn setting, up to the
substitution lemma. It provides also a named syntax and a translation function
to the De Bruijn syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0254</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0254</id><created>2012-12-02</created><authors><author><keyname>Marnette</keyname><forenames>Bruno</forenames></author></authors><title>Resolution and Datalog Rewriting Under Value Invention and Equality
  Constraints</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper present several refinements of the Datalog +/- framework based on
resolution and Datalog-rewriting. We first present a resolution algorithm which
is complete for arbitrary sets of tgds and egds. We then show that a technique
of saturation can be used to achieve completeness with respect to First-Order
(FO) query rewriting. We then investigate the class of guarded tgds (with a
loose definition of guardedness), and show that every set of tgds in this class
can be rewritten into an equivalent set of standard Datalog rules. On the
negative side, this implies that Datalog +/- has (only) the same expressive
power as standard Datalog in terms of query answering. On the positive side
however, this mean that known results and existing optimization techniques
(such as Magic-Set) may be applied in the context of Datalog +/- despite its
richer syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0272</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0272</id><created>2012-12-02</created><authors><author><keyname>Qin</keyname><forenames>Junjie</forenames></author><author><keyname>Su</keyname><forenames>Han-I</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Risk Limiting Dispatch with Fast Ramping Storage</title><categories>math.OC cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk Limiting Dispatch (RLD) was proposed recently as a mechanism that
utilizes information and market recourse to reduce reserve capacity
requirements, emissions and achieve other system operator objectives. It
induces a set of simple dispatch rules that can be easily embedded into the
existing dispatch systems to provide computationally efficient and reliable
decisions. Storage is emerging as an alternative to mitigate the uncertainty in
the grid. This paper extends the RLD framework to incorporate fast-ramping
storage. It developed a closed form threshold rule for the optimal stochastic
dispatch incorporating a sequence of markets and real-time information. An
efficient algorithm to evaluate the thresholds is developed based on analysis
of the optimal storage operation. Simple approximations that rely on
continuous-time approximations of the solution for the discrete time control
problem are also studied. The benefits of storage with respect to prediction
quality and storage capacity are examined, and the overall effect on dispatch
is quantified. Numerical experiments illustrate the proposed procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0277</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0277</id><created>2012-12-02</created><authors><author><keyname>Blake</keyname><forenames>Samuel T.</forenames></author><author><keyname>Tirkel</keyname><forenames>Andrew Z.</forenames></author></authors><title>A Construction for Perfect Periodic Autocorrelation Sequences</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a construction for perfect periodic autocorrelation sequences
over roots of unity. The sequences share similarities to the perfect periodic
sequence constructions of Liu, Frank, and Milewski.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0280</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0280</id><created>2012-12-02</created><authors><author><keyname>Bard</keyname><forenames>Gregory</forenames></author><author><keyname>Basyrov</keyname><forenames>Alexander</forenames></author></authors><title>Error Bounds on Derivatives during Simulations</title><categories>math.NA cs.CE</categories><comments>Six page paper with five pages of appendices</comments><msc-class>65D25, 65D15, 68U20, 68W30, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The methods commonly used for numerical differentiation, such as the
&quot;center-difference formula&quot; and &quot;four-points formula&quot; are unusable in
simulations or real-time data analysis because they require knowledge of the
future. In Bard'11, an algorithm was shown that generates formulas that require
knowledge only of the past and present values of $f(t)$ to estimate $f'(t)$.
Furthermore, the algorithm can handle irregularly spaced data and higher-order
derivatives. That work did not include a rigorous proof of correctness nor the
error bounds. In this paper, the correctness and error bounds of that algorithm
are proven, explicit forms are given for the coefficients, and several
interesting corollaries are proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0286</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0286</id><created>2012-12-03</created><updated>2012-12-08</updated><authors><author><keyname>Hou</keyname><forenames>I-Hong</forenames></author><author><keyname>Liu</keyname><forenames>Yao</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>A Non-Monetary Protocol for Peer-to-Peer Content Distribution in
  Wireless Broadcast Networks with Network Coding</title><categories>cs.NI</categories><comments>Under submission to WiOpt</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of content distribution in wireless
peer-to-peer networks where all nodes are selfish and non-cooperative. We
propose a model that considers both the broadcast nature of wireless channels
and the incentives of nodes, where each node aims to increase its own download
rate and reduces its upload rate through the course of content distribution. We
then propose a protocol for these selfish nodes to exchange contents. Our
protocol is distributed and does not require the exchange of money, reputation,
etc., and hence can be easily implemented without additional infrastructure.
Moreover, we show that our protocol can be easily modified to employ network
coding.
  The performance of our protocol is studied. We derive a closed-form
expression of Nash Equilibriums when there are only two files in the system.
The prices of anarchy, both from each node's perspective and the whole system's
perspective, are also characterized. Moreover, we propose a distributed
mechanism where each node adjusts its strategies only based on local
information and show that the mechanism converges to a Nash Equilibrium. We
also introduce an approach for calculating Nash Equilibriums for systems that
incorporate network coding when there are more than two files.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0287</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0287</id><created>2012-12-03</created><authors><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Ma</keyname><forenames>Jianfeng</forenames></author><author><keyname>Shi</keyname><forenames>Weisong</forenames></author></authors><title>Exploring Relay Cooperation for Secure and Reliable Transmission in
  Two-Hop Wireless Networks</title><categories>cs.CR cs.IT cs.NI math.IT</categories><comments>13 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1211.7075</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This work considers the problem of secure and reliable information
transmission via relay cooperation in two-hop relay wireless networks without
the information of both eavesdropper channels and locations. While previous
work on this problem mainly studied infinite networks and their asymptotic
behavior and scaling law results, this papers focuses on a more practical
network with finite number of system nodes and explores the corresponding exact
result on the number of eavesdroppers one network can tolerant to ensure
desired secrecy and reliability. We first study the scenario where path-loss is
equal between all pairs of nodes and consider two transmission protocols there,
one adopts an optimal but complex relay selection process with less load
balance capacity while the other adopts a random but simple relay selection
process with good load balance capacity. Theoretical analysis is then provided
to determine the maximum number of eavesdroppers one network can tolerate to
ensure a desired performance in terms of the secrecy outage probability and
transmission outage probability. We further extend our study to the more
general scenario where path-loss between each pair of nodes also depends the
distance between them, for which a new transmission protocol with both
preferable relay selection and good load balance as well as the corresponding
theoretical analysis are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0291</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0291</id><created>2012-12-03</created><authors><author><keyname>Prabhakar</keyname><forenames>C. J.</forenames></author><author><keyname>Kumar</keyname><forenames>P. U. Praveen</forenames></author></authors><title>An Image Based Technique for Enhancement of Underwater Images</title><categories>cs.CV</categories><journal-ref>International Journal of Machine Intelligence, Volume 3, Issue 4,
  pages 217-224, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The underwater images usually suffers from non-uniform lighting, low
contrast, blur and diminished colors. In this paper, we proposed an image based
preprocessing technique to enhance the quality of the underwater images. The
proposed technique comprises a combination of four filters such as homomorphic
filtering, wavelet denoising, bilateral filter and contrast equalization. These
filters are applied sequentially on degraded underwater images. The literature
survey reveals that image based preprocessing algorithms uses standard filter
techniques with various combinations. For smoothing the image, the image based
preprocessing algorithms uses the anisotropic filter. The main drawback of the
anisotropic filter is that iterative in nature and computation time is high
compared to bilateral filter. In the proposed technique, in addition to other
three filters, we employ a bilateral filter for smoothing the image. The
experimentation is carried out in two stages. In the first stage, we have
conducted various experiments on captured images and estimated optimal
parameters for bilateral filter. Similarly, optimal filter bank and optimal
wavelet shrinkage function are estimated for wavelet denoising. In the second
stage, we conducted the experiments using estimated optimal parameters, optimal
filter bank and optimal wavelet shrinkage function for evaluating the proposed
technique. We evaluated the technique using quantitative based criteria such as
a gradient magnitude histogram and Peak Signal to Noise Ratio (PSNR). Further,
the results are qualitatively evaluated based on edge detection results. The
proposed technique enhances the quality of the underwater images and can be
employed prior to apply computer vision techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0297</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0297</id><created>2012-12-03</created><authors><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>The Geometry of Differential Privacy: the Sparse and Approximate Cases</title><categories>cs.DS</categories><doi>10.1145/2488608.2488652</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study trade-offs between accuracy and privacy in the context
of linear queries over histograms. This is a rich class of queries that
includes contingency tables and range queries, and has been a focus of a long
line of work. For a set of $d$ linear queries over a database $x \in \R^N$, we
seek to find the differentially private mechanism that has the minimum mean
squared error. For pure differential privacy, an $O(\log^2 d)$ approximation to
the optimal mechanism is known. Our first contribution is to give an $O(\log^2
d)$ approximation guarantee for the case of $(\eps,\delta)$-differential
privacy. Our mechanism is simple, efficient and adds correlated Gaussian noise
to the answers. We prove its approximation guarantee relative to the hereditary
discrepancy lower bound of Muthukrishnan and Nikolov, using tools from convex
geometry.
  We next consider this question in the case when the number of queries exceeds
the number of individuals in the database, i.e. when $d &gt; n \triangleq
\|x\|_1$. It is known that better mechanisms exist in this setting. Our second
main contribution is to give an $(\eps,\delta)$-differentially private
mechanism which is optimal up to a $\polylog(d,N)$ factor for any given query
set $A$ and any given upper bound $n$ on $\|x\|_1$. This approximation is
achieved by coupling the Gaussian noise addition approach with a linear
regression step. We give an analogous result for the $\eps$-differential
privacy setting. We also improve on the mean squared error upper bound for
answering counting queries on a database of size $n$ by Blum, Ligett, and Roth,
and match the lower bound implied by the work of Dinur and Nissim up to
logarithmic factors.
  The connection between hereditary discrepancy and the privacy mechanism
enables us to derive the first polylogarithmic approximation to the hereditary
discrepancy of a matrix $A$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0304</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0304</id><created>2012-12-03</created><updated>2013-07-24</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Stefaner</keyname><forenames>Moritz</forenames></author><author><keyname>Anegon</keyname><forenames>Felix de Moya</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>Ranking and mapping of universities and research-focused institutions
  worldwide based on highly-cited papers: A visualization of results from
  multi-level models</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>Accepted for publication in the journal Online Information Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web application presented in this paper allows for an analysis to reveal
centres of excellence in different fields worldwide using publication and
citation data. Only specific aspects of institutional performance are taken
into account and other aspects such as teaching performance or societal impact
of research are not considered. Based on data gathered from Scopus,
field-specific excellence can be identified in institutions where highly-cited
papers have been frequently published. The web application combines both a list
of institutions ordered by different indicator values and a map with circles
visualizing indicator values for geocoded institutions. Compared to the mapping
and ranking approaches introduced hitherto, our underlying statistics
(multi-level models) are analytically oriented by allowing (1) the estimation
of values for the number of excellent papers for an institution which are
statistically more appropriate than the observed values; (2) the calculation of
confidence intervals as measures of accuracy for the institutional citation
impact; (3) the comparison of a single institution with an &quot;average&quot;
institution in a subject area, and (4) the direct comparison of at least two
institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0308</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0308</id><created>2012-12-03</created><authors><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author></authors><title>Random matrix over a DVR and LU factorization</title><categories>cs.DS math.PR</categories><comments>23 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let R be a discrete valuation ring (DVR) and K be its fraction field. If M is
a matrix over R admitting a LU decomposition, it could happen that the entries
of the factors L and U do not lie in R, but just in K. Having a good control on
the valuations of these entries is very important for algorithmic applications.
In the paper, we prove that in average these valuations are not too large and
explain how one can apply this result to provide an efficient algorithm
computing a basis of a coherent sheaf over A^1 from the knowledge of its
stalks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0309</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0309</id><created>2012-12-03</created><authors><author><keyname>Srungaram</keyname><forenames>Kartheek</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>Enhanced Cluster Based Routing Protocol for MANETS</title><categories>cs.NI</categories><comments>6 pages</comments><journal-ref>Social Informatics and Telecommunications Engineering Volume 84,
  2012, pp 346-352</journal-ref><doi>10.1007/978-3-642-27299-8_36</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad-hoc networks (MANETs) are a set of self organized wireless mobile
nodes that works without any predefined infrastructure. For routing data in
MANETs, the routing protocols relay on mobile wireless nodes. In general, any
routing protocol performance suffers i) with resource constraints and ii) due
to the mobility of the nodes. Due to existing routing challenges in MANETs
clustering based protocols suffers frequently with cluster head failure
problem, which degrades the cluster stability. This paper proposes, Enhanced
CBRP, a schema to improve the cluster stability and in-turn improves the
performance of traditional cluster based routing protocol (CBRP), by electing
better cluster head using weighted clustering algorithm and considering some
crucial routing challenges. Moreover, proposed protocol suggests a secondary
cluster head for each cluster, to increase the stability of the cluster and
implicitly the network infrastructure in case of sudden failure of cluster
head.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0310</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0310</id><created>2012-12-03</created><authors><author><keyname>Moazez</keyname><forenames>Mahsa</forenames></author><author><keyname>Safaei</keyname><forenames>Farshad</forenames></author><author><keyname>Rezazadeh</keyname><forenames>Majid</forenames></author></authors><title>Design and Implementation of Multistage Interconnection Networks for SoC
  Networks</title><categories>cs.AR cs.NI</categories><comments>11 Pages, 14 Figures; International Journal of Computer Science,
  Engineering and Information Technology (IJCSEIT), Vol.2, No.5, October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the focus is on a family of Interconnection Networks (INs)
known as Multistage Interconnection Networks (MINs). When it is exploited in
Network-on-Chip (NoC) architecture designs, smaller circuit area, lower power
consumption, less junctions and broader bandwidth can be achieved. Each MIN can
be considered as an alternative for an NoC architecture design for its simple
topology and easy scalability with low degree. This paper includes two major
contributions. First, it compares the performance of seven prominent MINs (i.e.
Omega, Butterfly, Flattened Butterfly, Flattened Baseline, Generalized Cube,
Bene\v{s} and Clos networks) based on 45nm-CMOS technology and under different
types of Synthetic and Trace-driven workloads. Second, a network called
Meta-Flattened Network (MFN), was introduced that can decrease the blocking
probability by means of reduction the number of hops and increase the
intermediate paths between stages. This is also led into significant decrease
in power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0311</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0311</id><created>2012-12-03</created><authors><author><keyname>Kumar</keyname><forenames>T. Anji</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>Enhanced Multiple Routing Configurations For Fast IP Network Recovery
  From Multiple Failures</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Networks (IJCN), Volume (3),
  Issue (4), 2011, pages: 196 - 208</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now a days, Internet plays a major role in our day to day activities e.g.,
for online transactions, online shopping, and other network related
applications. Internet suffers from slow convergence of routing protocols after
a network failure which becomes a growing problem. Multiple Routing
Configurations [MRC] recovers network from single node/link failures, but does
not support network from multiple node/link failures. In this paper, we propose
Enhanced MRC [EMRC], to support multiple node/link failures during data
transmission in IP networks without frequent global re-convergence. By
recovering these failures, data transmission in network will become fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0312</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0312</id><created>2012-12-03</created><authors><author><keyname>Sridhar</keyname><forenames>M. Bhanu</forenames></author><author><keyname>Srinivas</keyname><forenames>Y.</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>Software Reuse in Medical Database for Cardiac Patients using Pearson
  Family Equations</title><categories>cs.SE</categories><comments>(0975 8887)</comments><journal-ref>International Journal of Computer Applications Volume 58, No.14,
  2012, 12-19</journal-ref><doi>10.5120/9349-3675</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software reuse is a subfield of software engineering that is used to adopt
the existing software for similar purposes. Reuse Metrics determine the extent
to which an existing software component is reused in new software with an
objective to minimize the errors and cost of the new project. In this paper,
medical database related to cardiology is considered. The Pearson Type I
Distribution is used to calculate the probability density function (pdf) and
thereby utilizing it for clustering the data. Further, coupling methodology is
used to bring out the similarity of the new patient data by comparing it with
the existing data. By this, the concerned treatment to be followed for the new
patient is deduced by comparing with that of the previous patients case
history. The metrics proposed by Chidamber and Kemerer are utilized for this
purpose. This model will be useful for the medical field through software,
particularly in remote areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0317</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0317</id><created>2012-12-03</created><authors><author><keyname>Reddy</keyname><forenames>B. Adinarayana</forenames></author><author><keyname>Rao</keyname><forenames>O. Srinivasa</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>An Improved UP-Growth High Utility Itemset Mining</title><categories>cs.DB</categories><comments>(0975 8887)</comments><journal-ref>International Journal of Computer Applications Volume 58, No.2,
  2012, 25-28</journal-ref><doi>10.5120/9255-3424</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient discovery of frequent itemsets in large datasets is a crucial task
of data mining. In recent years, several approaches have been proposed for
generating high utility patterns, they arise the problems of producing a large
number of candidate itemsets for high utility itemsets and probably degrades
mining performance in terms of speed and space. Recently proposed compact tree
structure, viz., UP Tree, maintains the information of transactions and
itemsets, facilitate the mining performance and avoid scanning original
database repeatedly. In this paper, UP Tree (Utility Pattern Tree) is adopted,
which scans database only twice to obtain candidate items and manage them in an
efficient data structured way. Applying UP Tree to the UP Growth takes more
execution time for Phase II. Hence this paper presents modified algorithm
aiming to reduce the execution time by effectively identifying high utility
itemsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0318</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0318</id><created>2012-12-03</created><authors><author><keyname>Rao</keyname><forenames>D. Srinivasa</forenames></author><author><keyname>Seetha</keyname><forenames>M.</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>Comparison of Fuzzy and Neuro Fuzzy Image Fusion Techniques and its
  Applications</title><categories>cs.CV</categories><comments>(0975 8887). arXiv admin note: text overlap with arXiv:1209.4535 by
  other authors</comments><journal-ref>International Journal of Computer Applications Volume 43, No.20,
  2012, pages: 31 - 37</journal-ref><doi>10.5120/6222-8800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image fusion is the process of integrating multiple images of the same scene
into a single fused image to reduce uncertainty and minimizing redundancy while
extracting all the useful information from the source images. Image fusion
process is required for different applications like medical imaging, remote
sensing, medical imaging, machine vision, biometrics and military applications
where quality and critical information is required. In this paper, image fusion
using fuzzy and neuro fuzzy logic approaches utilized to fuse images from
different sensors, in order to enhance visualization. The proposed work further
explores comparison between fuzzy based image fusion and neuro fuzzy fusion
technique along with quality evaluation indices for image fusion like image
quality index, mutual information measure, fusion factor, fusion symmetry,
fusion index, root mean square error, peak signal to noise ratio, entropy,
correlation coefficient and spatial frequency. Experimental results obtained
from fusion process prove that the use of the neuro fuzzy based image fusion
approach shows better performance in first two test cases while in the third
test case fuzzy based image fusion technique gives better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0336</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0336</id><created>2012-12-03</created><authors><author><keyname>Monakhov</keyname><forenames>Yuri</forenames></author><author><keyname>Medvednikova</keyname><forenames>Maria</forenames></author><author><keyname>Abramov</keyname><forenames>Konstantin</forenames></author><author><keyname>Kostina</keyname><forenames>Natalia</forenames></author><author><keyname>Malyshev</keyname><forenames>Roman</forenames></author><author><keyname>Oleg</keyname><forenames>Makarov</forenames></author><author><keyname>Semenova</keyname><forenames>Irina</forenames></author></authors><title>Analytical model of misinformation of a social network node</title><categories>cs.SI physics.soc-ph</categories><comments>3 pages, 1 figure</comments><acm-class>H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the research of the influence of cognitive, behavioral,
representational factors on the susceptibility of the participants in social
networks to misinformation, as well as on the activity of the nodes in this
regard. The importance of this research consists of method of blocking the
propaganda. This is very important because when people involuntarily acquire
information some of them experience an undesired change in their social
attitude. Such phenomena typically lead towards the information warfare. A
model was developed during this research for calculating the level of
misinformation of the social network participant (network node) based on the
model of iterative learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0347</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0347</id><created>2012-12-03</created><authors><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author><author><keyname>Hu</keyname><forenames>Sihuang</forenames></author></authors><title>Association schemes related to Delsarte-Goethals codes</title><categories>math.CO cs.IT math.IT</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct an infinite series of 9-class association schemes
from a refinement of the partition of Delsarte-Goethals codes by their Lee
weights. The explicit expressions of the dual schemes are determined through
direct manipulations of complicated exponential sums. As a byproduct, the other
three infinite families of association schemes are also obtained as fusion
schemes and quotient schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0365</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0365</id><created>2012-12-03</created><authors><author><keyname>Tian</keyname><forenames>Feng</forenames></author><author><keyname>Chai</keyname><forenames>Wenjian</forenames></author><author><keyname>Wang</keyname><forenames>Chuanyun</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoping</forenames></author></authors><title>Design and Implementation of Flight Visual Simulation System</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The design requirement for flight visual simulation system is studied and the
overall structure and development process are proposed in this paper. Through
the construction of 3D scene model library and aircraft model, the rendering
and interaction of visual scene are implemented. The changes of aircraft flight
attitude in visual system are controlled by real-time calculation of aircraft
aerodynamic and dynamic equations and flight simulation effect is enhanced by
this kind of control. Several key techniques for optimizing 3D model and
relative methods for large terrain modeling are explored for improving loading
ability and rendering speed of the system. Experiment shows that, with specific
function and performance guaranteed as a premise, the system achieves expected
results, that is, precise real-time calculation of flight attitude and smooth
realistic screen effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0382</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0382</id><created>2012-12-03</created><authors><author><keyname>Fern&#xe1;ndez-Plazaola</keyname><forenames>Unai</forenames></author><author><keyname>Martos-Naya</keyname><forenames>Eduardo</forenames></author><author><keyname>Paris</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Entrambasaguas</keyname><forenames>Jos&#xe9; T.</forenames></author></authors><title>Comments on Proakis Analysis of the Characteristic Function of Complex
  Gaussian Quadratic Forms</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 talbes and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An analysis of the characteristic function of Gaussian quadratic forms is
presented in [1] to study the performance of multichannel communication
systems. This technical report reviews this analysis, obtaining alternative
expressions to original ones in compact matrix format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0383</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0383</id><created>2012-12-03</created><authors><author><keyname>Asha</keyname><forenames>V.</forenames></author><author><keyname>Bhajantri</keyname><forenames>N. U.</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author></authors><title>GLCM-based chi-square histogram distance for automatic detection of
  defects on patterned textures</title><categories>cs.CV</categories><comments>IJCVR, Vol. 2, No. 4, 2011, pp. 302-313</comments><msc-class>65D19, 11K70, 76M55, 93B17, 62H30</msc-class><acm-class>I.0; I.2.10</acm-class><journal-ref>IJCVR, Vol. 2, No. 4, 2011, pp. 302-313</journal-ref><doi>10.1504/IJCVR.2011.045267</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chi-square histogram distance is one of the distance measures that can be
used to find dissimilarity between two histograms. Motivated by the fact that
texture discrimination by human vision system is based on second-order
statistics, we make use of histogram of gray-level co-occurrence matrix (GLCM)
that is based on second-order statistics and propose a new machine vision
algorithm for automatic defect detection on patterned textures. Input defective
images are split into several periodic blocks and GLCMs are computed after
quantizing the gray levels from 0-255 to 0-63 to keep the size of GLCM compact
and to reduce computation time. Dissimilarity matrix derived from chi-square
distances of the GLCMs is subjected to hierarchical clustering to automatically
identify defective and defect-free blocks. Effectiveness of the proposed method
is demonstrated through experiments on defective real-fabric images of 2 major
wallpaper groups (pmm and p4m groups).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0387</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0387</id><created>2012-12-03</created><authors><author><keyname>Assegaff</keyname><forenames>Setiawan</forenames></author><author><keyname>Hussin</keyname><forenames>Ab Razak Che</forenames></author></authors><title>Review of Knowledge Management Systems As Socio-Technical System</title><categories>cs.OH</categories><comments>6 pages, 3 pigures; International Journal of Computer Science Issues
  (IJCSI)Vol 9. Issue 5 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Management Systems as socio-technical systemperspectives has
recognized for decades. Practitioners and scholars belief Knowledge Management
is best carried out throught the optimization both technological and
social-aspect.Lacking of understand and consider both aspects could lead
organizations in misinterpretation while developing andimplementing Knowledge
Management System. There is a need for practical guidance how Knowledge
Management System should implement in organizations. We propose a framework
that could use by practitioner and manager as guidance in developing and
implementing Knowledge Management System as Socio-Technical Systems. The
framework developed base on Pan and Scarborough view of Knowledge Management as
Socio-Technical system. Our framework consists of: Infrastructure(technology),
Info structure (organizational structure) and Info culture (organizational
culture). This concept would lead practitioners get clear understand aspect
contribute to Knowledge Management System success as Socio-Technical System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0388</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0388</id><created>2012-12-03</created><authors><author><keyname>Tran</keyname><forenames>Loc</forenames></author></authors><title>Hypergraph and protein function prediction with gene expression data</title><categories>stat.ML cs.LG q-bio.QM</categories><comments>12 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1211.4289</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most network-based protein (or gene) function prediction methods are based on
the assumption that the labels of two adjacent proteins in the network are
likely to be the same. However, assuming the pairwise relationship between
proteins or genes is not complete, the information a group of genes that show
very similar patterns of expression and tend to have similar functions (i.e.
the functional modules) is missed. The natural way overcoming the information
loss of the above assumption is to represent the gene expression data as the
hypergraph. Thus, in this paper, the three un-normalized, random walk, and
symmetric normalized hypergraph Laplacian based semi-supervised learning
methods applied to hypergraph constructed from the gene expression data in
order to predict the functions of yeast proteins are introduced. Experiment
results show that the average accuracy performance measures of these three
hypergraph Laplacian based semi-supervised learning methods are the same.
However, their average accuracy performance measures of these three methods are
much greater than the average accuracy performance measures of un-normalized
graph Laplacian based semi-supervised learning method (i.e. the baseline method
of this paper) applied to gene co-expression network created from the gene
expression data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0401</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0401</id><created>2012-12-03</created><updated>2014-05-22</updated><authors><author><keyname>Endrullis</keyname><forenames>Joerg</forenames><affiliation>Vrije Universiteit Amsterdam</affiliation></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames><affiliation>Vrije Universiteit Amsterdam</affiliation></author><author><keyname>Klop</keyname><forenames>Jan Willem</forenames><affiliation>Vrije Universiteit Amsterdam</affiliation></author><author><keyname>Polonsky</keyname><forenames>Andrew</forenames><affiliation>Vrije Universiteit Amsterdam</affiliation></author></authors><title>Discriminating Lambda-Terms Using Clocked Boehm Trees</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1002.2578</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (May 28,
  2014) lmcs:880</journal-ref><doi>10.2168/LMCS-10(2:4)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As observed by Intrigila, there are hardly techniques available in the
lambda-calculus to prove that two lambda-terms are not beta-convertible.
Techniques employing the usual Boehm Trees are inadequate when we deal with
terms having the same Boehm Tree (BT). This is the case in particular for fixed
point combinators, as they all have the same BT. Another interesting equation,
whose consideration was suggested by Scott, is BY = BYS, an equation valid in
the classical model P-omega of lambda-calculus, and hence valid with respect to
BT-equality but nevertheless the terms are beta-inconvertible. To prove such
beta-inconvertibilities, we employ `clocked' BT's, with annotations that convey
information of the tempo in which the data in the BT are produced. Boehm Trees
are thus enriched with an intrinsic clock behaviour, leading to a refined
discrimination method for lambda-terms. The corresponding equality is strictly
intermediate between beta-convertibility and Boehm Tree equality, the equality
in the model P-omega. An analogous approach pertains to Levy-Longo and
Berarducci Trees. Our refined Boehm Trees find in particular an application in
beta-discriminating fixed point combinators (fpc's). It turns out that Scott's
equation BY = BYS is the key to unlocking a plethora of fpc's, generated by a
variety of production schemes of which the simplest was found by Boehm, stating
that new fpc's are obtained by postfixing the term SI, also known as Smullyan's
Owl. We prove that all these newly generated fpc's are indeed new, by
considering their clocked BT's. Even so, not all pairs of new fpc's can be
discriminated this way. For that purpose we increase the discrimination power
by a precision of the clock notion that we call `atomic clock'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0402</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0402</id><created>2012-12-03</created><authors><author><keyname>Soomro</keyname><forenames>Khurram</forenames></author><author><keyname>Zamir</keyname><forenames>Amir Roshan</forenames></author><author><keyname>Shah</keyname><forenames>Mubarak</forenames></author></authors><title>UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title><categories>cs.CV</categories><report-no>CRCV-TR-12-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce UCF101 which is currently the largest dataset of human actions.
It consists of 101 action classes, over 13k clips and 27 hours of video data.
The database consists of realistic user uploaded videos containing camera
motion and cluttered background. Additionally, we provide baseline action
recognition results on this new dataset using standard bag of words approach
with overall performance of 44.5%. To the best of our knowledge, UCF101 is
currently the most challenging dataset of actions due to its large number of
classes, large number of clips and also unconstrained nature of such clips.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0415</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0415</id><created>2012-12-03</created><updated>2013-09-03</updated><authors><author><keyname>Ballico</keyname><forenames>Edoardo</forenames></author><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>On the minimum distance and the minimum weight of Goppa codes from a
  quotient of the Hermitian curve</title><categories>math.AG cs.IT math.IT</categories><comments>Applicable Algebra in Engineering, Communication and Computing (to
  appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study evaluation codes arising from plane quotients of the
Hermitian curve, defined by affine equations of the form $y^q+y=x^m$, $q$ being
a prime power and $m$ a positive integer which divides $q+1$. The dual minimum
distance and minimum weight of such codes are studied from a geometric point of
view. In many cases we completely describe the minimum-weight codewords of
their dual codes through a geometric characterization of the supports, and
provide their number. Finally, we apply our results to describe Goppa codes of
classical interest on such curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0417</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0417</id><created>2012-12-03</created><authors><author><keyname>Jarlebring</keyname><forenames>Elias</forenames></author><author><keyname>Kvaal</keyname><forenames>Simen</forenames></author><author><keyname>Michiels</keyname><forenames>Wim</forenames></author></authors><title>An inverse iteration method for eigenvalue problems with eigenvector
  nonlinearities</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a symmetric matrix $A(v)\in\RR^{n\times n}$ depending on a vector
$v\in\RR^n$ and satisfying the property $A(\alpha v)=A(v)$ for any
$\alpha\in\RR\backslash{0}$. We will here study the problem of finding
$(\lambda,v)\in\RR\times \RR^n\backslash\{0\}$ such that $(\lambda,v)$ is an
eigenpair of the matrix $A(v)$ and we propose a generalization of inverse
iteration for eigenvalue problems with this type of eigenvector nonlinearity.
The convergence of the proposed method is studied and several convergence
properties are shown to be analogous to inverse iteration for standard
eigenvalue problems, including local convergence properties. The algorithm is
also shown to be equivalent to a particular discretization of an associated
ordinary differential equation, if the shift is chosen in a particular way. The
algorithm is adapted to a variant of the Schr\&quot;odinger equation known as the
Gross-Pitaevskii equation. We use numerical simulations toillustrate the
convergence properties, as well as the efficiency of the algorithm and the
adaption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0421</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0421</id><created>2012-12-03</created><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Rzadca</keyname><forenames>Krzysztof</forenames></author></authors><title>Network delay-aware load balancing in selfish and cooperative
  distributed systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a request processing system composed of organizations and their
servers connected by the Internet.
  The latency a user observes is a sum of communication delays and the time
needed to handle the request on a server. The handling time depends on the
server congestion, i.e. the total number of requests a server must handle. We
analyze the problem of balancing the load in a network of servers in order to
minimize the total observed latency. We consider both cooperative and selfish
organizations (each organization aiming to minimize the latency of the
locally-produced requests). The problem can be generalized to the task
scheduling in a distributed cloud; or to content delivery in an
organizationally-distributed CDNs.
  In a cooperative network, we show that the problem is polynomially solvable.
We also present a distributed algorithm iteratively balancing the load. We show
how to estimate the distance between the current solution and the optimum based
on the amount of load exchanged by the algorithm. During the experimental
evaluation, we show that the distributed algorithm is efficient, therefore it
can be used in networks with dynamically changing loads.
  In a network of selfish organizations, we prove that the price of anarchy
(the worst-case loss of performance due to selfishness) is low when the network
is homogeneous and the servers are loaded (the request handling time is high
compared to the communication delay). After relaxing these assumptions, we
assess the loss of performance caused by the selfishness experimentally,
showing that it remains low.
  Our results indicate that a network of servers handling requests can be
efficiently managed by a distributed algorithm. Additionally, even if the
network is organizationally distributed, with individual organizations
optimizing performance of their requests, the network remains efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0427</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0427</id><created>2012-12-03</created><updated>2013-04-20</updated><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Rzadca</keyname><forenames>Krzysztof</forenames></author></authors><title>Exploring heterogeneity of unreliable machines for p2p backup</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P2P architecture is a viable option for enterprise backup. In contrast to
dedicated backup servers, nowadays a standard solution, making backups directly
on organization's workstations should be cheaper (as existing hardware is
used), more efficient (as there is no single bottleneck server) and more
reliable (as the machines are geographically dispersed).
  We present the architecture of a p2p backup system that uses pairwise
replication contracts between a data owner and a replicator. In contrast to
standard p2p storage systems using directly a DHT, the contracts allow our
system to optimize replicas' placement depending on a specific optimization
strategy, and so to take advantage of the heterogeneity of the machines and the
network. Such optimization is particularly appealing in the context of backup:
replicas can be geographically dispersed, the load sent over the network can be
minimized, or the optimization goal can be to minimize the backup/restore time.
However, managing the contracts, keeping them consistent and adjusting them in
response to dynamically changing environment is challenging.
  We built a scientific prototype and ran the experiments on 150 workstations
in the university's computer laboratories and, separately, on 50 PlanetLab
nodes. We found out that the main factor affecting the quality of the system is
the availability of the machines. Yet, our main conclusion is that it is
possible to build an efficient and reliable backup system on highly unreliable
machines (our computers had just 13% average availability).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0433</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0433</id><created>2012-12-03</created><authors><author><keyname>Sudhakar</keyname><forenames>Prasad</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Dubois</keyname><forenames>Xavier</forenames></author><author><keyname>Antoine</keyname><forenames>Philippe</forenames></author><author><keyname>Joannes</keyname><forenames>Luc</forenames></author></authors><title>Compressive Schlieren Deflectometry</title><categories>cs.CV</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schlieren deflectometry aims at characterizing the deflections undergone by
refracted incident light rays at any surface point of a transparent object. For
smooth surfaces, each surface location is actually associated with a sparse
deflection map (or spectrum). This paper presents a novel method to
compressively acquire and reconstruct such spectra. This is achieved by
altering the way deflection information is captured in a common Schlieren
Deflectometer, i.e., the deflection spectra are indirectly observed by the
principle of spread spectrum compressed sensing. These observations are
realized optically using a 2-D Spatial Light Modulator (SLM) adjusted to the
corresponding sensing basis and whose modulations encode the light deviation
subsequently recorded by a CCD camera. The efficiency of this approach is
demonstrated experimentally on the observation of few test objects. Further,
using a simple parametrization of the deflection spectra we show that relevant
key parameters can be directly computed using the measurements, avoiding full
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0435</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0435</id><created>2012-12-03</created><updated>2013-11-12</updated><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>Network Growth with Arbitrary Initial Conditions: Analytical Results for
  Uniform and Preferential Attachment</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><journal-ref>Phys. Rev. E 88, 062801 (2013)</journal-ref><doi>10.1103/PhysRevE.88.062801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides time-dependent expressions for the expected degree
distribution of a given network that is subject to growth, as a function of
time. We consider both uniform attachment, where incoming nodes form links to
existing nodes selected uniformly at random, and preferential attachment, when
probabilities are assigned proportional to the degrees of the existing nodes.
We consider the cases of single and multiple links being formed by each
newly-introduced node. The initial conditions are arbitrary, that is, the
solution depends on the degree distribution of the initial graph which is the
substrate of the growth. Previous work in the literature focuses on the
asymptotic state, that is, when the number of nodes added to the initial graph
tends to infinity, rendering the effect of the initial graph negligible. Our
contribution provides a solution for the expected degree distribution as a
function of time, for arbitrary initial condition. Previous results match our
results in the asymptotic limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0451</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0451</id><created>2012-12-03</created><updated>2015-01-24</updated><authors><author><keyname>Rambhatla</keyname><forenames>Sirisha</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis D.</forenames></author></authors><title>Semi-blind Source Separation via Sparse Representations and Online
  Dictionary Learning</title><categories>cs.SD stat.AP stat.ML</categories><comments>5 pages, In Proceedings of the 47th Asilomar Conference on Signals
  Systems and Computers, 2013</comments><doi>10.1109/ACSSC.2013.6810587</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work examines a semi-blind single-channel source separation problem. Our
specific aim is to separate one source whose local structure is approximately
known, from another a priori unspecified background source, given only a single
linear combination of the two sources. We propose a separation technique based
on local sparse approximations along the lines of recent efforts in sparse
representations and dictionary learning. A key feature of our procedure is the
online learning of dictionaries (using only the data itself) to sparsely model
the background source, which facilitates its separation from the
partially-known source. Our approach is applicable to source separation
problems in various application domains; here, we demonstrate the performance
of our proposed approach via simulation on a stylized audio source separation
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0463</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0463</id><created>2012-12-03</created><authors><author><keyname>McDonald</keyname><forenames>Daniel J.</forenames></author><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author><author><keyname>Schervish</keyname><forenames>Mark</forenames></author></authors><title>Time series forecasting: model evaluation and selection using
  nonparametric risk bounds</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>26 pages, 3 figures</comments><msc-class>62M20 (Primary) 91B84, 62G99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive generalization error bounds --- bounds on the expected inaccuracy
of the predictions --- for traditional time series forecasting models. Our
results hold for many standard forecasting tools including autoregressive
models, moving average models, and, more generally, linear state-space models.
These bounds allow forecasters to select among competing models and to
guarantee that with high probability, their chosen model will perform well
without making strong assumptions about the data generating process or
appealing to asymptotic theory. We motivate our techniques with and apply them
to standard economic and financial forecasting tools --- a GARCH model for
predicting equity volatility and a dynamic stochastic general equilibrium model
(DSGE), the standard tool in macroeconomic forecasting. We demonstrate in
particular how our techniques can aid forecasters and policy makers in choosing
models which behave well under uncertainty and mis-specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0467</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0467</id><created>2012-12-03</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Low-rank Matrix Completion using Alternating Minimization</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternating minimization represents a widely applicable and empirically
successful approach for finding low-rank matrices that best fit the given data.
For example, for the problem of low-rank matrix completion, this method is
believed to be one of the most accurate and efficient, and formed a major
component of the winning entry in the Netflix Challenge.
  In the alternating minimization approach, the low-rank target matrix is
written in a bi-linear form, i.e. $X = UV^\dag$; the algorithm then alternates
between finding the best $U$ and the best $V$. Typically, each alternating step
in isolation is convex and tractable. However the overall problem becomes
non-convex and there has been almost no theoretical understanding of when this
approach yields a good result.
  In this paper we present first theoretical analysis of the performance of
alternating minimization for matrix completion, and the related problem of
matrix sensing. For both these problems, celebrated recent results have shown
that they become well-posed and tractable once certain (now standard)
conditions are imposed on the problem. We show that alternating minimization
also succeeds under similar conditions. Moreover, compared to existing results,
our paper shows that alternating minimization guarantees faster (in particular,
geometric) convergence to the true matrix, while allowing a simpler analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0469</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0469</id><created>2012-12-03</created><updated>2013-02-07</updated><authors><author><keyname>Wang</keyname><forenames>Po T.</forenames></author><author><keyname>King</keyname><forenames>Christine E.</forenames></author><author><keyname>Do</keyname><forenames>An H.</forenames></author><author><keyname>Nenadic</keyname><forenames>Zoran</forenames></author></authors><title>Pushing the Communication Speed Limit of a Noninvasive BCI Speller</title><categories>cs.HC q-bio.NC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Electroencephalogram (EEG) based brain-computer interfaces (BCI) may provide
a means of communication for those affected by severe paralysis. However, the
relatively low information transfer rates (ITR) of these systems, currently
limited to 1 bit/sec, present a serious obstacle to their widespread adoption
in both clinical and non-clinical applications. Here, we report on the
development of a novel noninvasive BCI communication system that achieves ITRs
that are severalfold higher than those previously reported with similar
systems. Using only 8 EEG channels, 6 healthy subjects with little to no prior
BCI experience selected characters from a virtual keyboard with sustained,
error-free, online ITRs in excess of 3 bit/sec. By factoring in the time spent
to notify the subjects of their selection, practical, error-free typing rates
as high as 12.75 character/min were achieved, which allowed subjects to
correctly type a 44-character sentence in less than 3.5 minutes. We hypothesize
that ITRs can be further improved by optimizing the parameters of the
interface, while practical typing rates can be significantly improved by
shortening the selection notification time. These results provide compelling
evidence that the ITR limit of noninvasive BCIs has not yet been reached and
that further investigation into this matter is both justified and necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0493</identifier>
 <datestamp>2013-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0493</id><created>2012-12-03</created><updated>2013-12-02</updated><authors><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Multi-user Diversity in Spectrum Sharing Systems over Fading Channels
  with Average Power Constraints</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn since it has been revised significantly
  and submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-user diversity in spectrum sharing cognitive radio systems with
average power constraints over fading channels is investigated. Average power
constraints are imposed for both the transmit power at the secondary
transmitter and the interference power received at the primary receiver in
order to provide optimal power allocation for capacity maximization at the
secondary system and protection at the primary system respectively. Multiple
secondary and primary receivers are considered and the corresponding fading
distributions for the Rayleigh and Nakagami-m fading channels are derived.
Based on the derived formulation of the fading distributions, the average
achievable channel capacity and the outage probability experienced at the
secondary system are obtained, revealing the impact of the average power
constraints on optimal power allocation in multi-user diversity technique in
fading environments with multiple secondary and primary receivers that share
the same channel. The obtained results highlight the advantage of having on one
hand more secondary receivers and on the other hand fewer primary receivers
manifested as an increase in the achievable capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0494</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0494</id><created>2012-12-03</created><authors><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Identification Via Quantum Channels</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>This is a review article, accepted for publication in the LNCS volume
  in memory of Rudolf Ahlswede. 17 pages, requires Springer llncs class</comments><journal-ref>Lecture Notes in Computer Science, vol. 7777 (2013), pp. 217-233</journal-ref><doi>10.1007/978-3-642-36899-8_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the development of the quantum version of Ahlswede and Dueck's
theory of identification via channels. As is often the case in quantum
probability, there is not just one but several quantizations: we know at least
two different concepts of identification of classical information via quantum
channels, and three different identification capacities for quantum
information. In the present summary overview we concentrate on conceptual
points and open problems, referring the reader to the small set of original
articles for details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0504</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0504</id><created>2012-12-03</created><updated>2013-03-18</updated><authors><author><keyname>Menden</keyname><forenames>Michael P.</forenames></author><author><keyname>Iorio</keyname><forenames>Francesco</forenames></author><author><keyname>Garnett</keyname><forenames>Mathew</forenames></author><author><keyname>McDermott</keyname><forenames>Ultan</forenames></author><author><keyname>Benes</keyname><forenames>Cyril</forenames></author><author><keyname>Ballester</keyname><forenames>Pedro J.</forenames></author><author><keyname>Saez-Rodriguez</keyname><forenames>Julio</forenames></author></authors><title>Machine learning prediction of cancer cell sensitivity to drugs based on
  genomic and chemical properties</title><categories>q-bio.GN cs.CE cs.LG q-bio.CB</categories><comments>26 pages, 7 figures, including supplemental information, presented by
  Michael Menden at the 5th annual RECOMB Conference on Regulatory and Systems
  Genomics with DREAM Challenges; accepted in PLOS ONE</comments><doi>10.1371/journal.pone.0061318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the response of a specific cancer to a therapy is a major goal in
modern oncology that should ultimately lead to a personalised treatment.
High-throughput screenings of potentially active compounds against a panel of
genomically heterogeneous cancer cell lines have unveiled multiple
relationships between genomic alterations and drug responses. Various
computational approaches have been proposed to predict sensitivity based on
genomic features, while others have used the chemical properties of the drugs
to ascertain their effect. In an effort to integrate these complementary
approaches, we developed machine learning models to predict the response of
cancer cell lines to drug treatment, quantified through IC50 values, based on
both the genomic features of the cell lines and the chemical properties of the
considered drugs. Models predicted IC50 values in a 8-fold cross-validation and
an independent blind test with coefficient of determination R2 of 0.72 and 0.64
respectively. Furthermore, models were able to predict with comparable accuracy
(R2 of 0.61) IC50s of cell lines from a tissue not used in the training stage.
Our in silico models can be used to optimise the experimental design of
drug-cell screenings by estimating a large proportion of missing IC50 values
rather than experimentally measure them. The implications of our results go
beyond virtual drug screening design: potentially thousands of drugs could be
probed in silico to systematically test their potential efficacy as anti-tumour
agents based on their structure, thus providing a computational framework to
identify new drug repositioning opportunities as well as ultimately be useful
for personalized medicine by linking the genomic traits of patients to drug
sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0506</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0506</id><created>2012-12-03</created><updated>2013-04-01</updated><authors><author><keyname>Giles</keyname><forenames>Brett</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author></authors><title>Exact synthesis of multiqubit Clifford+T circuits</title><categories>quant-ph cs.ET</categories><comments>7 pages</comments><journal-ref>Phys. Rev. A 87, 032332 (2013)</journal-ref><doi>10.1103/PhysRevA.87.032332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a unitary matrix has an exact representation over the
Clifford+T gate set with local ancillas if and only if its entries are in the
ring Z[1/sqrt(2),i]. Moreover, we show that one ancilla always suffices. These
facts were conjectured by Kliuchnikov, Maslov, and Mosca. We obtain an
algorithm for synthesizing a exact Clifford+T circuit from any such n-qubit
operator. We also characterize the Clifford+T operators that can be represented
without ancillas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0511</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0511</id><created>2012-12-03</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>EMN, IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EMN, IRCCyN</affiliation></author></authors><title>Design of Experiments for Calibration of Planar Anthropomorphic
  Manipulators</title><categories>cs.RO</categories><comments>Advanced Intelligent Mechatronics (AIM), 2011 IEEE/ASME International
  Conference on, Budapest : Hungary (2011)</comments><proxy>ccsd</proxy><doi>10.1109/AIM.2011.6027017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a novel technique for the design of optimal calibration
experiments for a planar anthropomorphic manipulator with n degrees of freedom.
Proposed approach for selection of manipulator configurations allows
essentially improving calibration accuracy and reducing parameter
identification errors. The results are illustrated by application examples that
deal with typical anthropomorphic manipulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0518</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0518</id><created>2012-12-03</created><updated>2013-02-19</updated><authors><author><keyname>Gabel</keyname><forenames>Alan</forenames></author><author><keyname>Redner</keyname><forenames>S.</forenames></author></authors><title>Sublinear but Never Superlinear Preferential Attachment by Local Network
  Growth</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>10 pages, 3 figure, IOP style; version2: one reference corrected;
  version 3 (final version for JSTAT): some minor typos and citation errors
  fixed</comments><journal-ref>J. Stat. Mech. P02043 (2013)</journal-ref><doi>10.1088/1742-5468/2013/02/P02043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a class of network growth rules that are based on a
redirection algorithm wherein new nodes are added to a network by linking to a
randomly chosen target node with some probability 1-r or linking to the parent
node of the target node with probability r. For fixed 0&lt;r&lt;1, the redirection
algorithm is equivalent to linear preferential attachment. We show that when r
is a decaying function of the degree of the parent of the initial target, the
redirection algorithm produces sublinear preferential attachment network
growth. We also argue that no local redirection algorithm can produce
superlinear preferential attachment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0520</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0520</id><created>2012-12-03</created><authors><author><keyname>Mauerer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Portmann</keyname><forenames>Christopher</forenames></author><author><keyname>Scholz</keyname><forenames>Volkher B.</forenames></author></authors><title>A modular framework for randomness extraction based on Trevisan's
  construction</title><categories>cs.IT cs.MS math.IT quant-ph</categories><comments>21 pages, 15 figures. Source code is available under GPLv2+. Comments
  welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Informally, an extractor delivers perfect randomness from a source that may
be far away from the uniform distribution, yet contains some randomness. This
task is a crucial ingredient of any attempt to produce perfectly random
numbers---required, for instance, by cryptographic protocols, numerical
simulations, or randomised computations. Trevisan's extractor raised
considerable theoretical interest not only because of its data parsimony
compared to other constructions, but particularly because it is secure against
quantum adversaries, making it applicable to quantum key distribution.
  We discuss a modular, extensible and high-performance implementation of the
construction based on various building blocks that can be flexibly combined to
satisfy the requirements of a wide range of scenarios. Besides quantitatively
analysing the properties of many combinations in practical settings, we improve
previous theoretical proofs, and give explicit results for non-asymptotic
cases. The self-contained description does not assume familiarity with
extractors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0526</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0526</id><created>2012-12-03</created><authors><author><keyname>Maubert</keyname><forenames>Bastien</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Pinchinat</keyname><forenames>Sophie</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Uniform Strategies</title><categories>cs.GT</categories><comments>(2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider turn-based game arenas for which we investigate uniformity
properties of strategies. These properties involve bundles of plays, that arise
from some semantical motive. Typically, we can represent constraints on allowed
strategies, such as being observation-based. We propose a formal language to
specify uniformity properties and demonstrate its relevance by rephrasing
various known problems from the literature. Note that the ability to correlate
different plays cannot be achieved by any branching-time logic if not equipped
with an additional modality, so-called R in this contribution. We also study an
automated procedure to synthesize strategies subject to a uniformity property,
which strictly extends existing results based on, say standard temporal logics.
We exhibit a generic solution for the synthesis problem provided the bundles of
plays rely on any binary relation definable by a finite state transducer. This
solution yields a non-elementary procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0536</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0536</id><created>2012-12-01</created><authors><author><keyname>Tsuda</keyname><forenames>Soichiro</forenames></author><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards Physarum Engines</title><categories>nlin.AO cs.ET physics.bio-ph</categories><comments>arXiv admin note: text overlap with arXiv:1212.0023</comments><journal-ref>Tsuda S., Jones J., Adamatzky A. Towards Physarum engines. Applied
  Bionics and Biomechanics 9 (2012) 3, 221-240</journal-ref><doi>10.3233/ABB-2012-0059</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The slime mould Physarum polycephalum is a suitable candidate organism for
soft-matter robotics because it exhibits controllable transport, movement and
guidance behaviour. Physarum may be considered as a smart computing and
actuating material since both its motor and control systems are distributed
within its undifferentiated tissue and can survive trauma such as excision,
fission and fusion of plasmodia. Thus it may be suitable for exploring the
generation and distribution of micro-actuation in individual units or planar
arrays. We experimentally show how the plasmodium of Physarum is shaped to
execute controllable oscillatory transport behaviour applicable in small hybrid
engines. We measure the lifting force of the plasmodium and demonstrate how
protoplasmic transport can be influenced by externally applied illumination
stimuli. We provide an exemplar vehicle mechanism by coupling the oscillations
of the plasmodium to drive the wheels of a Braitenberg vehicle and use light
stimuli to effect a steering mechanism. Using a particle model of Physarum we
show how emergent travelling wave patterns produced by competing oscillatory
domains may be used to to generate spatially represented actuation patterns. We
demonstrate different patterns of controllable motion, including linear,
reciprocal, rotational and helical, and demonstrate in simulation how dynamic
oscillatory patterns may be translated into motive forces for simple transport
of substances within a patterned environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0570</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0570</id><created>2012-12-03</created><updated>2014-08-05</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>The $\theta_5$-graph is a spanner</title><categories>cs.CG</categories><comments>18 pages, 12 figures, forthcoming in CGTA</comments><journal-ref>Computational Geometry: Theory and Applications, 48(2):108-119,
  2015</journal-ref><doi>10.1016/j.comgeo.2014.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of points in the plane, we show that the $\theta$-graph with 5
cones is a geometric spanner with spanning ratio at most $\sqrt{50 + 22
\sqrt{5}} \approx 9.960$. This is the first constant upper bound on the
spanning ratio of this graph. The upper bound uses a constructive argument that
gives a (possibly self-intersecting) path between any two vertices, of length
at most $\sqrt{50 + 22 \sqrt{5}}$ times the Euclidean distance between the
vertices. We also give a lower bound on the spanning ratio of
$\frac{1}{2}(11\sqrt{5} -17) \approx 3.798$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0575</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0575</id><created>2012-12-03</created><updated>2012-12-05</updated><authors><author><keyname>Koay</keyname><forenames>Cheng Guan</forenames></author><author><keyname>&#xd6;zarslan</keyname><forenames>Evren</forenames></author><author><keyname>Johnson</keyname><forenames>Kevin M</forenames></author><author><keyname>Meyerand</keyname><forenames>M. Elizabeth</forenames></author></authors><title>Sparse and Optimal Acquisition Design for Diffusion MRI and Beyond</title><categories>physics.med-ph cs.CE math.OC physics.comp-ph</categories><comments>41 pages, 2 tables and 9 figures</comments><journal-ref>Med. Phys. 39, 2499 (2012)</journal-ref><doi>10.1118/1.3700166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on the development of a sparse and optimal
acquisition (SOA) design for diffusion MRI multiple-shell acquisition and
beyond. A novel optimality criterion is proposed for sparse multiple-shell
acquisition and quasi multiple-shell designs in diffusion MRI and a novel and
effective semi-stochastic and moderately greedy combinatorial search strategy
with simulated annealing to locate the optimum design or configuration. Even
though the number of distinct configurations for a given set of diffusion
gradient directions is very large in general---e.g., in the order of 10^{232}
for a set of 144 diffusion gradient directions, the proposed search strategy
was found to be effective in finding the optimum configuration. It was found
that the square design is the most robust (i.e., with stable condition numbers
and A-optimal measures under varying experimental conditions) among many other
possible designs of the same sample size. Under the same performance
evaluation, the square design was found to be more robust than the widely used
sampling schemes similar to that of 3D radial MRI and of diffusion spectrum
imaging (DSI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0578</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0578</id><created>2012-12-03</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>Max-plus algebra models of queueing networks</title><categories>math.OC cs.SY</categories><comments>International Workshop on Discrete Event Systems (WODES'96),
  University of Edinburgh, UK, August 19-21, 1996</comments><msc-class>15A80 (Primary), 90B22, 93C65, 65C20, 90B15 (Secondary)</msc-class><journal-ref>Proc. Intern. Workshop on Discrete Event Systems WODES'96, London,
  IEE, 1996, pp. 76-81. ISBN 0-85296-664-4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of queueing networks which may have an arbitrary topology, and
consist of single-server fork-join nodes with both infinite and finite buffers
is examined to derive a representation of the network dynamics in terms of
max-plus algebra. For the networks, we present a common dynamic state equation
which relates the departure epochs of customers from the network nodes in an
explicit vector form determined by a state transition matrix. It is shown how
the matrices inherent in particular networks may be calculated from the service
times of customers. Since, in general, an explicit dynamic equation may not
exist for a network, related existence conditions are established in terms of
the network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0582</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0582</id><created>2012-12-03</created><authors><author><keyname>Mjolsness</keyname><forenames>Eric</forenames></author></authors><title>Compositional Stochastic Modeling and Probabilistic Programming</title><categories>cs.AI cs.PL</categories><comments>Extended Abstract for the Neural Information Processing Systems
  (NIPS) Workshop on Probabilistic Programming, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic programming is related to a compositional approach to
stochastic modeling by switching from discrete to continuous time dynamics. In
continuous time, an operator-algebra semantics is available in which processes
proceeding in parallel (and possibly interacting) have summed time-evolution
operators. From this foundation, algorithms for simulation, inference and model
reduction may be systematically derived. The useful consequences are
potentially far-reaching in computational science, machine learning and beyond.
Hybrid compositional stochastic modeling/probabilistic programming approaches
may also be possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0610</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0610</id><created>2012-12-03</created><updated>2013-01-09</updated><authors><author><keyname>Xu</keyname><forenames>Huiqi</forenames></author><author><keyname>Guo</keyname><forenames>Shumin</forenames></author><author><keyname>Chen</keyname><forenames>Keke</forenames></author></authors><title>Building Confidential and Efficient Query Services in the Cloud with
  RASP Data Perturbation</title><categories>cs.DB cs.CR</categories><comments>18 pages, to appear in IEEE TKDE, accepted in December 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the wide deployment of public cloud computing infrastructures, using
clouds to host data query services has become an appealing solution for the
advantages on scalability and cost-saving. However, some data might be
sensitive that the data owner does not want to move to the cloud unless the
data confidentiality and query privacy are guaranteed. On the other hand, a
secured query service should still provide efficient query processing and
significantly reduce the in-house workload to fully realize the benefits of
cloud computing. We propose the RASP data perturbation method to provide secure
and efficient range query and kNN query services for protected data in the
cloud. The RASP data perturbation method combines order preserving encryption,
dimensionality expansion, random noise injection, and random projection, to
provide strong resilience to attacks on the perturbed data and queries. It also
preserves multidimensional ranges, which allows existing indexing techniques to
be applied to speedup range query processing. The kNN-R algorithm is designed
to work with the RASP range query algorithm to process the kNN queries. We have
carefully analyzed the attacks on data and queries under a precisely defined
threat model and realistic security assumptions. Extensive experiments have
been conducted to show the advantages of this approach on efficiency and
security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0616</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0616</id><created>2012-12-04</created><updated>2013-11-03</updated><authors><author><keyname>Boban</keyname><forenames>Mate</forenames></author><author><keyname>Meireles</keyname><forenames>Rui</forenames></author><author><keyname>Barros</keyname><forenames>Joao</forenames></author><author><keyname>Steenkiste</keyname><forenames>Peter</forenames></author><author><keyname>Tonguz</keyname><forenames>Ozan K.</forenames></author></authors><title>TVR - Tall Vehicle Relaying in Vehicular Networks</title><categories>cs.NI</categories><doi>10.1109/TMC.2013.70</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle-to-Vehicle (V2V) communication is a core technology for enabling
safety and non-safety applications in next generation Intelligent
Transportation Systems. Due to relatively low heights of the antennas, V2V
communication is often influenced by topographic features, man-made structures,
and other vehicles located between the communicating vehicles. On highways, it
was shown experimentally that vehicles can obstruct the line of sight (LOS)
communication up to 50 percent of the time; furthermore, a single obstructing
vehicle can reduce the power at the receiver by more than 20 dB. Based on both
experimental measurements and simulations performed using a validated channel
model, we show that the elevated position of antennas on tall vehicles improves
communication performance. Tall vehicles can significantly increase the
effective communication range, with an improvement of up to 50 percent in
certain scenarios. Using these findings, we propose a new V2V relaying scheme
called Tall Vehicle Relaying (TVR) that takes advantage of better channel
characteristics provided by tall vehicles. TVR distinguishes between tall and
short vehicles and, where appropriate, chooses tall vehicles as next hop
relays. We investigate TVR's system-level performance through a combination of
link-level experiments and system-level simulations and show that it
outperforms existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0638</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0638</id><created>2012-12-04</created><updated>2013-02-21</updated><authors><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author></authors><title>Manipulating Google Scholar Citations and Google Scholar Metrics:
  simple, easy and tempting</title><categories>cs.DL</categories><comments>10 pages, 4 figures</comments><report-no>EC36</report-no><journal-ref>Delgado Lopez-Cozar, Emilio; Robinson-Garcia, Nicolas; Torres
  Salinas, Daniel (2012). Manipulating Google Scholar Citations and Google
  Scholar Metrics: simple, easy and tempting. EC3 Working Papers 6: 29 May,
  2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The launch of Google Scholar Citations and Google Scholar Metrics may provoke
a revolution in the research evaluation field as it places within every
researchers reach tools that allow bibliometric measuring. In order to alert
the research community over how easily one can manipulate the data and
bibliometric indicators offered by Google s products we present an experiment
in which we manipulate the Google Citations profiles of a research group
through the creation of false documents that cite their documents, and
consequently, the journals in which they have published modifying their H
index. For this purpose we created six documents authored by a faked author and
we uploaded them to a researcher s personal website under the University of
Granadas domain. The result of the experiment meant an increase of 774
citations in 129 papers (six citations per paper) increasing the authors and
journals H index. We analyse the malicious effect this type of practices can
cause to Google Scholar Citations and Google Scholar Metrics. Finally, we
conclude with several deliberations over the effects these malpractices may
have and the lack of control tools these tools offer
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0639</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0639</id><created>2012-12-04</created><authors><author><keyname>Khalil</keyname><forenames>Osama</forenames></author></authors><title>Evaluation of Particle Swarm Optimization Algorithms for Weighted
  Max-Sat Problem: Technical Report</title><categories>cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An experimental evaluation is conducted to asses the performance of 4
different Particle Swarm Optimization neighborhood structures in solving
Max-Sat problem. The experiment has shown that none of the algorithms achieves
statistically significant performance over the others under confidence level of
0.05.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0640</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0640</id><created>2012-12-04</created><authors><author><keyname>Mandal</keyname><forenames>Ritankar</forenames></author><author><keyname>Ghosh</keyname><forenames>Anirban</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author><author><keyname>Nandy</keyname><forenames>Subhas C.</forenames></author></authors><title>Greedy is good: An experimental study on minimum clique cover and
  maximum independent set problems for randomly generated rectangles</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set ${\cal R}=\{R_1,R_2,..., R_n\}$ of $n$ randomly positioned axis
parallel rectangles in 2D, the problem of computing the minimum clique cover
(MCC) and maximum independent set (MIS) for the intersection graph $G({\cal
R})$ of the members in $\cal R$ are both computationally hard \cite{CC05}. For
the MCC problem, it is proved that polynomial time constant factor
approximation is impossible to obtain \cite{PT11}. Though such a result is not
proved yet for the MIS problem, no polynomial time constant factor
approximation algorithm exists in the literature. We study the performance of
greedy algorithms for computing these two parameters of $G({\cal R})$.
Experimental results shows that for each of the MCC and MIS problems, the
corresponding greedy algorithm produces a solution that is very close to its
optimum solution. Scheinerman \cite{Scheinerman80} showed that the size of MIS
is tightly bounded by $\sqrt{n}$ for a random instance of the 1D version of the
problem, (i.e., for the interval graph). Our experiment shows that the size of
independent set and the clique cover produced by the greedy algorithm is at
least $2\sqrt{n}$ and at most $3\sqrt{n}$, respectively. Thus the
experimentally obtained approximation ratio of the greedy algorithm for MIS
problem is at most 3/2 and the same for the MCC problem is at least 2/3.
Finally we will provide refined greedy algorithms based on a concept of {\it
simplicial rectangle}. The characteristics of this algorithm may be of interest
in getting a provably constant factor approximation algorithm for random
instance of both the problems. We believe that the result also holds true for
any finite dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0647</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0647</id><created>2012-12-04</created><authors><author><keyname>Khanum</keyname><forenames>Mohammadi Akheela</forenames></author><author><keyname>Trivedi</keyname><forenames>Munesh Chandra</forenames></author></authors><title>Take Care: A Study on Usability Evaluation Methods for Children</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays whenever a user buys any gadget, apart from the price his focus
would also be on how easy is the functionality of the gadget. This means users
are more concerned towards the usability of the gadget. Therefore, this study
set to explore usability evaluations methods for children in order to analyze
their roles in the development of technology. Usability evaluation methods
which are successfully tested on the adults are investigated to find out how
successfully they can also be applied to children. The results of the review
indicate that usability evaluation with children is more challenging than with
adults. The study found that depending on one type of usability evaluation
method would not be an appropriate decision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0655</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0655</id><created>2012-12-04</created><updated>2013-12-21</updated><authors><author><keyname>Frosini</keyname><forenames>Patrizio</forenames></author></authors><title>G-invariant Persistent Homology</title><categories>math.AT cs.CG cs.CV</categories><comments>14 pages, 4 figures. Remark 4.2 has been expanded to become
  Subsection 4.2, including a new example (Example 4.3). The section
  &quot;Discussion and further research&quot; and some references have been added. Small
  changes in the text</comments><msc-class>55N35 (Primary) 68U05 (Secondary)</msc-class><acm-class>I.4.7; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical persistent homology is a powerful mathematical tool for shape
comparison. Unfortunately, it is not tailored to study the action of
transformation groups that are different from the group Homeo(X) of all
self-homeomorphisms of a topological space X. This fact restricts its use in
applications. In order to obtain better lower bounds for the natural
pseudo-distance d_G associated with a subgroup G of Homeo(X), we need to adapt
persistent homology and consider G-invariant persistent homology. Roughly
speaking, the main idea consists in defining persistent homology by means of a
set of chains that is invariant under the action of G. In this paper we
formalize this idea, and prove the stability of the persistent Betti number
functions in G-invariant persistent homology with respect to the natural
pseudo-distance d_G. We also show how G-invariant persistent homology could be
used in applications concerning shape comparison, when the invariance group is
a proper subgroup of the group of all self-homeomorphisms of a topological
space. In this paper we will assume that the space X is triangulable, in order
to guarantee that the persistent Betti number functions are finite without
using any tameness assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0657</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0657</id><created>2012-12-04</created><authors><author><keyname>Bagnoli</keyname><forenames>Franco</forenames></author><author><keyname>Borkmann</keyname><forenames>Daniel</forenames></author><author><keyname>Guazzini</keyname><forenames>Andrea</forenames></author><author><keyname>Massaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Rudolph</keyname><forenames>Stefan</forenames></author></authors><title>Modeling Risk Perception in Networks with Community Structure</title><categories>physics.soc-ph cs.SI</categories><comments>Accepted to the 7th International Conference on Bio-Inspired Models
  of Network, Information, and Computing Systems, BIONETICS2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the influence of global, local and community-level risk perception
on the extinction probability of a disease in several models of social
networks. In particular, we study the infection progression as a
susceptible-infected-susceptible (SIS) model on several modular networks,
formed by a certain number of random and scale-free communities. We find that
in the scale-free networks the progression is faster than in random ones with
the same average connectivity degree. For what concerns the role of perception,
we find that the knowledge of the infection level in one's own neighborhood is
the most effective property in stopping the spreading of a disease, but at the
same time the more expensive one in terms of the quantity of required
information, thus the cost/effectiveness optimum is a tradeoff between several
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0689</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0689</id><created>2012-12-04</created><updated>2013-11-08</updated><authors><author><keyname>Tremblay</keyname><forenames>Nicolas</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author></authors><title>Multiscale Community Mining in Networks Using Spectral Graph Wavelets</title><categories>physics.soc-ph cs.DM cs.SI</categories><comments>Proceedings of the European Signal Processing Conference (EUSIPCO
  2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For data represented by networks, the community structure of the underlying
graph is of great interest. A classical clustering problem is to uncover the
overall ``best'' partition of nodes in communities. Here, a more elaborate
description is proposed in which community structures are identified at
different scales. To this end, we take advantage of the local and
scale-dependent information encoded in graph wavelets. After new developments
for the practical use of graph wavelets, studying proper scale boundaries and
parameters and introducing scaling functions, we propose a method to mine for
communities in complex networks in a scale-dependent manner. It relies on
classifying nodes according to their wavelets or scaling functions, using a
scale-dependent modularity function. An example on a graph benchmark having
hierarchical communities shows that we estimate successfully its multiscale
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0692</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0692</id><created>2012-12-04</created><updated>2014-01-04</updated><authors><author><keyname>Amadini</keyname><forenames>Roberto</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Mauro</keyname><forenames>Jacopo</forenames></author></authors><title>An Empirical Evaluation of Portfolios Approaches for solving CSPs</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research in areas such as SAT solving and Integer Linear Programming
has shown that the performances of a single arbitrarily efficient solver can be
significantly outperformed by a portfolio of possibly slower on-average
solvers. We report an empirical evaluation and comparison of portfolio
approaches applied to Constraint Satisfaction Problems (CSPs). We compared
models developed on top of off-the-shelf machine learning algorithms with
respect to approaches used in the SAT field and adapted for CSPs, considering
different portfolio sizes and using as evaluation metrics the number of solved
problems and the time taken to solve them. Results indicate that the best SAT
approaches have top performances also in the CSP field and are slightly more
competitive than simple models built on top of classification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0694</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0694</id><created>2012-12-04</created><updated>2015-01-02</updated><authors><author><keyname>van Dam</keyname><forenames>Edwin R.</forenames></author><author><keyname>Sotirov</keyname><forenames>Renata</forenames></author></authors><title>On bounding the bandwidth of graphs with symmetry</title><categories>math.OC cs.DM math.CO</categories><journal-ref>INFORMS Journal on Computing 27 (2015), 75-88</journal-ref><doi>10.1287/ijoc.2014.0611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a new lower bound for the bandwidth of a graph that is based on a
new lower bound for the minimum cut problem. Our new semidefinite programming
relaxation of the minimum cut problem is obtained by strengthening the known
semidefinite programming relaxation for the quadratic assignment problem (or
for the graph partition problem) by fixing two vertices in the graph; one on
each side of the cut. This fixing results in several smaller subproblems that
need to be solved to obtain the new bound. In order to efficiently solve these
subproblems we exploit symmetry in the data; that is, both symmetry in the
min-cut problem and symmetry in the graphs. To obtain upper bounds for the
bandwidth of graphs with symmetry, we develop a heuristic approach based on the
well-known reverse Cuthill-McKee algorithm, and that improves significantly its
performance on the tested graphs. Our approaches result in the best known lower
and upper bounds for the bandwidth of all graphs under consideration, i.e.,
Hamming graphs, 3-dimensional generalized Hamming graphs, Johnson graphs, and
Kneser graphs, with up to 216 vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0695</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0695</id><created>2012-12-04</created><authors><author><keyname>Frandi</keyname><forenames>Emanuele</forenames></author><author><keyname>Nanculef</keyname><forenames>Ricardo</forenames></author><author><keyname>Gasparo</keyname><forenames>Maria Grazia</forenames></author><author><keyname>Lodi</keyname><forenames>Stefano</forenames></author><author><keyname>Sartori</keyname><forenames>Claudio</forenames></author></authors><title>Training Support Vector Machines Using Frank-Wolfe Optimization Methods</title><categories>cs.LG cs.CV math.OC stat.ML</categories><journal-ref>International Journal on Pattern Recognition and Artificial
  Intelligence, 27(3), 2013</journal-ref><doi>10.1142/S0218001413600033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training a Support Vector Machine (SVM) requires the solution of a quadratic
programming problem (QP) whose computational complexity becomes prohibitively
expensive for large scale datasets. Traditional optimization methods cannot be
directly applied in these cases, mainly due to memory restrictions.
  By adopting a slightly different objective function and under mild conditions
on the kernel used within the model, efficient algorithms to train SVMs have
been devised under the name of Core Vector Machines (CVMs). This framework
exploits the equivalence of the resulting learning problem with the task of
building a Minimal Enclosing Ball (MEB) problem in a feature space, where data
is implicitly embedded by a kernel function.
  In this paper, we improve on the CVM approach by proposing two novel methods
to build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast
method to approximate the solution of a MEB problem. In contrast to CVMs, our
algorithms do not require to compute the solutions of a sequence of
increasingly complex QPs and are defined by using only analytic optimization
steps. Experiments on a large collection of datasets show that our methods
scale better than CVMs in most cases, sometimes at the price of a slightly
lower accuracy. As CVMs, the proposed methods can be easily extended to machine
learning problems other than binary classification. However, effective
classifiers are also obtained using kernels which do not satisfy the condition
required by CVMs and can thus be used for a wider set of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0703</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0703</id><created>2012-12-04</created><updated>2014-04-14</updated><authors><author><keyname>Jurkiewicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author></authors><title>The Cost of Address Translation</title><categories>cs.DS cs.CC cs.PF</categories><comments>A extended abstract of this paper was published in the proceedings of
  ALENEX13, New Orleans, USA</comments><msc-class>68Q05, 68Q15, 03D15</msc-class><acm-class>F.1.1; F.2.3; B.8.2; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern computers are not random access machines (RAMs). They have a memory
hierarchy, multiple cores, and virtual memory. In this paper, we address the
computational cost of address translation in virtual memory. Starting point for
our work is the observation that the analysis of some simple algorithms (random
scan of an array, binary search, heapsort) in either the RAM model or the EM
model (external memory model) does not correctly predict growth rates of actual
running times. We propose the VAT model (virtual address translation) to
account for the cost of address translations and analyze the algorithms
mentioned above and others in the model. The predictions agree with the
measurements. We also analyze the VAT-cost of cache-oblivious algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0724</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0724</id><created>2012-12-04</created><authors><author><keyname>Ellings&#xe6;ter</keyname><forenames>Brage</forenames></author><author><keyname>Skjegstad</keyname><forenames>Magnus</forenames></author><author><keyname>Maseng</keyname><forenames>Torleiv</forenames></author></authors><title>A Potential Game for Power and Frequency Allocation in Large-Scale
  Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze power and frequency allocation in wireless networks
through potential games. Potential games are used frequently in the literature
for this purpose due to their desirable properties, such as convergence and
stability. However, potential games usually assume massive message passing to
obtain the necessary neighbor information at each user to achieve these
properties. In this paper we show an example of a game where we are able to
characterize the necessary neighbor information in order to show that the game
has a potential function and the properties of potential games. We consider a
network consisting of local access points where the goal of each AP is to
allocate power and frequency to achieve some SINR requirement. We use the
physical SINR model to validate a successful allocation, and show that given a
suitable payoff function the game emits a generalized ordinal potential
function under the assumption of sufficient neighbor information. Through
simulations we evaluate the performance of the proposed game on a large scale
in relation to the amount of information at each AP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0725</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0725</id><created>2012-12-04</created><updated>2013-04-20</updated><authors><author><keyname>Yang</keyname><forenames>Li</forenames></author><author><keyname>Liang</keyname><forenames>Min</forenames></author></authors><title>A note on quantum McEliece public-key cryptosystem</title><categories>quant-ph cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by Fujita's analysis [Quantum inf. &amp; comput. 12(3&amp;4), 2012], we
suggest a twice-encryption scheme to improve the security of the original
quantum McEliece public-key encryption algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0748</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0748</id><created>2012-12-02</created><updated>2013-01-03</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Nevels</keyname><forenames>Robert D.</forenames></author></authors><title>Twisted Radio Waves and Twisted Thermodynamics</title><categories>physics.class-ph cs.IT math.IT</categories><comments>Final version, accepted for publication in PLoS ONE</comments><journal-ref>PLoS ONE 8(2): e56086 (2013)</journal-ref><doi>10.1371/journal.pone.0056086</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present and analyze a gedanken experiment and show that the assumption
that an antenna operating at a single frequency can transmit more than two
independent information channels to the far field violates the Second Law of
Thermodynamics. Transmission of a large number of channels, each associated
with an angular momentum &quot;twisted wave&quot; mode, to the far field in free space is
therefore not possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0749</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0749</id><created>2012-12-04</created><updated>2012-12-13</updated><authors><author><keyname>Luduena</keyname><forenames>G. A.</forenames></author><author><keyname>Meixner</keyname><forenames>H.</forenames></author><author><keyname>Kaczor</keyname><forenames>Gregor</forenames></author><author><keyname>Gros</keyname><forenames>Claudius</forenames></author></authors><title>A large-scale study of the World Wide Web: network correlation functions
  with scale-invariant boundaries</title><categories>physics.soc-ph cs.SI</categories><comments>Includes degree and Alexa rank correlations</comments><journal-ref>European Physical Journal B 86, 348 (2013)</journal-ref><doi>10.1140/epjb/e2013-31121-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We performed a large-scale crawl of the World Wide Web, covering 6.9 Million
domains and 57 Million subdomains, including all high-traffic sites of the
Internet. We present a study of the correlations found between quantities
measuring the structural relevance of each node in the network (the in- and
out-degree, the local clustering coefficient, the first-neighbor in-degree and
the Alexa rank). We find that some of these properties show strong correlation
effects and that the dependencies occurring out of these correlations follow
power laws not only for the averages, but also for the boundaries of the
respective density distributions. In addition, these scale-free limits do not
follow the same exponents as the corresponding averages. In our study we retain
the directionality of the hyperlinks and develop a statistical estimate for the
clustering coefficient of directed graphs.
  We include in our study the correlations between the in-degree and the Alexa
traffic rank, a popular index for the traffic volume, finding non-trivial
power-law correlations. We find that sites with more/less than about one
Thousand links from different domains have remarkably different statistical
properties, for all correlation functions studied, indicating towards an
underlying hierarchical structure of the World Wide Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0750</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0750</id><created>2012-12-02</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author><author><keyname>Buckley</keyname><forenames>Sheryl</forenames></author></authors><title>Problem Solving and Computational Thinking in a Learning Environment</title><categories>cs.AI</categories><comments>19 pages, 2 figures</comments><msc-class>68T20</msc-class><journal-ref>Egyptian Computer Science Journal, Vol. 36 (4), 28-46, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational thinking is a new problem soling method named for its extensive
use of computer science techniques. It synthesizes critical thinking and
existing knowledge and applies them in solving complex technological problems.
The term was coined by J. Wing, but the relationship between computational and
critical thinking, the two modes of thiking in solving problems, has not been
yet learly established. This paper aims at shedding some light into this
relationship. We also present two classroom experiments performed recently at
the Graduate Technological Educational Institute of Patras in Greece. The
results of these experiments give a strong indication that the use of computers
as a tool for problem solving enchances the students' abilities in solving real
world problems involving mathematical modelling. This is also crossed by
earlier findings of other researchers for the problem solving process in
general (not only for mathematical problems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0752</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0752</id><created>2012-12-04</created><updated>2012-12-11</updated><authors><author><keyname>Laekhanukit</keyname><forenames>Bundit</forenames></author></authors><title>Parameters of Two-Prover-One-Round Game and The Hardness of Connectivity
  Problems</title><categories>cs.CC cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing parameters of Two-Prover-One-Round Game (2P1R) is an important
task in PCPs literature as it would imply a smaller PCP with the same or
stronger soundness. While this is a basic question in PCPs community, the
connection between the parameters of PCPs and hardness of approximations is
sometime obscure to approximation algorithm community. In this paper, we
investigate the connection between the parameters of 2P1R and the hardness of
approximating the class of so-called connectivity problems, which includes as
subclasses the survivable network design and (multi)cut problems. Based on
recent development on 2P1R by Chan (ECCC 2011) and several techniques in PCPs
literature, we improve hardness results of some connectivity problems that are
in the form $k^\sigma$, for some (very) small constant $\sigma&gt;0$, to hardness
results of the form $k^c$ for some explicit constant $c$, where $k$ is a
connectivity parameter. In addition, we show how to convert these hardness into
hardness results of the form $D^{c'}$, where $D$ is the number of demand pairs
(or the number of terminals).
  Thus, we give improved hardness results of k^{1/2-\epsilon} and
k^{1/10-\epsilon} for the root $k$-connectivity problem on directed and
undirected graphs, k^{1/6-\epsilon} for the vertex-connectivity survivable
network design problem on undirected graphs, and k^{1/6-\epsilon} for the
vertex-connectivity $k$-route cut problem on undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0763</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0763</id><created>2012-12-03</created><authors><author><keyname>Gueye</keyname><forenames>Modou</forenames></author><author><keyname>Abdessalem</keyname><forenames>Talel</forenames></author><author><keyname>Naacke</keyname><forenames>Hubert</forenames></author></authors><title>Dynamic recommender system : using cluster-based biases to improve the
  accuracy of the predictions</title><categories>cs.LG cs.DB cs.IR</categories><comments>31 pages, 7 figures</comments><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is today accepted that matrix factorization models allow a high quality of
rating prediction in recommender systems. However, a major drawback of matrix
factorization is its static nature that results in a progressive declining of
the accuracy of the predictions after each factorization. This is due to the
fact that the new obtained ratings are not taken into account until a new
factorization is computed, which can not be done very often because of the high
cost of matrix factorization.
  In this paper, aiming at improving the accuracy of recommender systems, we
propose a cluster-based matrix factorization technique that enables online
integration of new ratings. Thus, we significantly enhance the obtained
predictions between two matrix factorizations. We use finer-grained user biases
by clustering similar items into groups, and allocating in these groups a bias
to each user. The experiments we did on large datasets demonstrated the
efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0767</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0767</id><created>2012-12-04</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Robust Predictor Feedback for Discrete-Time Systems with Input Delays</title><categories>math.OC cs.SY</categories><comments>20 pages, 1 Figure, submitted for possible publication to the
  International Journal of Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the design problem of feedback stabilizers for
discrete-time systems with input delays. A backstepping procedure is proposed
for disturbance-free discrete-time systems. The feedback law designed by using
backstepping coincides with the predictor-based feedback law used in
continuous-time systems with input delays. However, simple examples demonstrate
that the sensitivity of the closed-loop system with respect to modeling errors
increases as the value of the delay increases. The paper proposes a Lyapunov
redesign procedure which can minimize the effect of the uncertainty. Specific
results are provided for linear single-input discrete-time systems with
multiplicative uncertainty. The feedback law that guarantees robust global
exponential stability is a nonlinear, homogeneous of degree 1 feedback law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0768</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0768</id><created>2012-12-04</created><authors><author><keyname>Morignot</keyname><forenames>Philippe</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Nashashibi</keyname><forenames>Fawzi</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>An ontology-based approach to relax traffic regulation for autonomous
  vehicle assistance</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>12th IASTED International Conference on Artificial Intelligence
  and Applications (AIA'13), Austria (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic regulation must be respected by all vehicles, either human- or
computer- driven. However, extreme traffic situations might exhibit practical
cases in which a vehicle should safely and reasonably relax traffic regulation,
e.g., in order not to be indefinitely blocked and to keep circulating. In this
paper, we propose a high-level representation of an automated vehicle, other
vehicles and their environment, which can assist drivers in taking such
&quot;illegal&quot; but practical relaxation decisions. This high-level representation
(an ontology) includes topological knowledge and inference rules, in order to
compute the next high-level motion an automated vehicle should take, as
assistance to a driver. Results on practical cases are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0785</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0785</id><created>2012-12-04</created><updated>2012-12-05</updated><authors><author><keyname>Winter</keyname><forenames>Frank</forenames></author></authors><title>Gauge Field Generation on Large-Scale GPU-Enabled Systems</title><categories>hep-lat cs.DC physics.comp-ph</categories><comments>The 30th International Symposium on Lattice Field Theory, June 24-29,
  2012, Cairns, Australia (Acknowledgment and Citation added)</comments><journal-ref>PoS(Lattice 2012)185</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past years GPUs have been successfully applied to the task of
inverting the fermion matrix in lattice QCD calculations. Even strong scaling
to capability-level supercomputers, corresponding to O(100) GPUs or more has
been achieved. However strong scaling a whole gauge field generation algorithm
to this regim requires significantly more functionality than just having the
matrix inverter utilizing the GPUs and has not yet been accomplished. This
contribution extends QDP-JIT, the migration of SciDAC QDP++ to GPU-enabled
parallel systems, to help to strong scale the whole Hybrid Monte-Carlo to this
regime. Initial results are shown for gauge field generation with Chroma
simulating pure Wilson fermions on OLCF TitanDev.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0804</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0804</id><created>2012-12-04</created><authors><author><keyname>Di Giacomo</keyname><forenames>Emilio</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author></authors><title>How many vertex locations can be arbitrarily chosen when drawing planar
  graphs?</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proven that every set $S$ of distinct points in the plane with
cardinality $\lceil \frac{\sqrt{\log_2 n}-1}{4} \rceil$ can be a subset of the
vertices of a crossing-free straight-line drawing of any planar graph with $n$
vertices. It is also proven that if $S$ is restricted to be a one-sided convex
point set, its cardinality increases to $\lceil \sqrt[3]{n} \rceil$. The proofs
are constructive and give rise to O(n)-time drawing algorithms. As a part of
our proofs, we show that every maximal planar graph contains a large induced
biconnected outerplanar graphs and a large induced outerpath (an outerplanar
graph whose weak dual is a path).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0819</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0819</id><created>2012-12-04</created><authors><author><keyname>Shchepin</keyname><forenames>Evgeny</forenames></author></authors><title>A Topological Code for Plane Images</title><categories>cs.CV math.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proposed a new code for contours of plane images. This code was applied
for optical character recognition of printed and handwritten characters. One
can apply it to recognition of any visual images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0822</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0822</id><created>2012-12-04</created><updated>2012-12-06</updated><authors><author><keyname>Kliuchnikov</keyname><forenames>Vadym</forenames></author><author><keyname>Maslov</keyname><forenames>Dmitri</forenames></author><author><keyname>Mosca</keyname><forenames>Michele</forenames></author></authors><title>Asymptotically optimal approximation of single qubit unitaries by
  Clifford and T circuits using a constant number of ancillary qubits</title><categories>quant-ph cs.ET</categories><journal-ref>Phys. Rev. Lett. 110, 190502 (2013)</journal-ref><doi>10.1103/PhysRevLett.110.190502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for building a circuit that approximates single qubit
unitaries with precision {\epsilon} using O(log(1/{\epsilon})) Clifford and T
gates and employing up to two ancillary qubits. The algorithm for computing our
approximating circuit requires an average of O(log^2(1/{\epsilon})log
log(1/{\epsilon})) operations. We prove that the number of gates in our circuit
saturates the lower bound on the number of gates required in the scenario when
a constant number of ancillae are supplied, and as such, our circuits are
asymptotically optimal. This results in significant improvement over the
current state of the art for finding an approximation of a unitary, including
the Solovay-Kitaev algorithm that requires O(log^{3+{\delta}}(1/{\epsilon}))
gates and does not use ancillae and the phase kickback approach that requires
O(log^2(1/{\epsilon})log log(1/{\epsilon})) gates, but uses
O(log^2(1/{\epsilon})) ancillae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0823</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0823</id><created>2012-12-04</created><updated>2013-02-16</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Goldstone</keyname><forenames>Robert L.</forenames></author></authors><title>Interdisciplinarity at the Journal and Specialty Level: The changing
  knowledge bases of the journal Cognitive Science</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (JASIST), in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the referencing patterns in articles in Cognitive Science over three
decades, we analyze the knowledge base of this literature in terms of its
changing disciplinary composition. Three periods are distinguished: (1)
construction of the interdisciplinary space in the 1980s; (2) development of an
interdisciplinary orientation in the 1990s; (3) reintegration into &quot;cognitive
psychology&quot; in the 2000s. The fluidity and fuzziness of the interdisciplinary
delineations in the different visualizations can be reduced and clarified using
factor analysis. We also explore newly available routines (&quot;CorText&quot;) to
analyze this development in terms of &quot;tubes&quot; using an alluvial map, and compare
the results with an animation (using &quot;visone&quot;). The historical specificity of
this development can be compared with the development of &quot;artificial
intelligence&quot; into an integrated specialty during this same period.
&quot;Interdisciplinarity&quot; should be defined differently at the level of journals
and of specialties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0836</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0836</id><created>2012-12-04</created><authors><author><keyname>Schaeffer</keyname><forenames>Luke</forenames></author></authors><title>An Improved Lower Bound for Stack Sorting</title><categories>cs.DM math.CO</categories><comments>11 pages, 1 figure</comments><msc-class>05A16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sorting elements on a series of stacks, introduced
by Tarjan and Knuth. We improve the asymptotic lower bound for the number of
stacks necessary to sort $n$ elements to $0.561 \log_2 n + O(1)$. This is the
first significant improvement since the previous lower bound, $1/2 \log_2 n +
O(1)$, was established by Knuth in 1972.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0873</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0873</id><created>2012-12-04</created><updated>2013-11-25</updated><authors><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Parallel Coordinate Descent Methods for Big Data Optimization</title><categories>math.OC cs.AI stat.ML</categories><comments>43 pages, 8 tables, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we show that randomized (block) coordinate descent methods can
be accelerated by parallelization when applied to the problem of minimizing the
sum of a partially separable smooth convex function and a simple separable
convex function. The theoretical speedup, as compared to the serial method, and
referring to the number of iterations needed to approximately solve the problem
with high probability, is a simple expression depending on the number of
parallel processors and a natural and easily computable measure of separability
of the smooth component of the objective function. In the worst case, when no
degree of separability is present, there may be no speedup; in the best case,
when the problem is separable, the speedup is equal to the number of
processors. Our analysis also works in the mode when the number of blocks being
updated at each iteration is random, which allows for modeling situations with
busy or unreliable processors. We show that our algorithm is able to solve a
LASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large
memory node with 24 cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0877</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0877</id><created>2012-12-04</created><authors><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Bai</keyname><forenames>Er-Wei</forenames></author><author><keyname>Cho</keyname><forenames>Myung</forenames></author></authors><title>Toeplitz Matrix Based Sparse Error Correction in System Identification:
  Outliers and Random Noises</title><categories>cs.IT math.IT</categories><comments>conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider robust system identification under sparse outliers
and random noises. In our problem, system parameters are observed through a
Toeplitz matrix. All observations are subject to random noises and a few are
corrupted with outliers. We reduce this problem of system identification to a
sparse error correcting problem using a Toeplitz structured real-numbered
coding matrix. We prove the performance guarantee of Toeplitz structured matrix
in sparse error correction. Thresholds on the percentage of correctable errors
for Toeplitz structured matrices are also established. When both outliers and
observation noise are present, we have shown that the estimation error goes to
0 asymptotically as long as the probability density function for observation
noise is not &quot;vanishing&quot; around 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0884</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0884</id><created>2012-12-04</created><updated>2013-09-20</updated><authors><author><keyname>Borgs</keyname><forenames>Christian</forenames></author><author><keyname>Brautbar</keyname><forenames>Michael</forenames></author><author><keyname>Chayes</keyname><forenames>Jennifer</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>Maximizing Social Influence in Nearly Optimal Time</title><categories>cs.DS cs.SI physics.soc-ph</categories><comments>Title changed in this version; previous title was &quot;Influence
  Maximization in Social Networks: Towards an Optimal Algorithmic Solution&quot;. An
  extended abstract of this work will appear in SODA 2014</comments><acm-class>F.2.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion is a fundamental graph process, underpinning such phenomena as
epidemic disease contagion and the spread of innovation by word-of-mouth. We
address the algorithmic problem of finding a set of k initial seed nodes in a
network so that the expected size of the resulting cascade is maximized, under
the standard independent cascade model of network diffusion. Runtime is a
primary consideration for this problem due to the massive size of the relevant
input networks.
  We provide a fast algorithm for the influence maximization problem, obtaining
the near-optimal approximation factor of (1 - 1/e - epsilon), for any epsilon &gt;
0, in time O((m+n)log(n) / epsilon^3). Our algorithm is runtime-optimal (up to
a logarithmic factor) and substantially improves upon the previously best-known
algorithms which run in time Omega(mnk POLY(1/epsilon)). Furthermore, our
algorithm can be modified to allow early termination: if it is terminated after
O(beta(m+n)log(n)) steps for some beta &lt; 1 (which can depend on n), then it
returns a solution with approximation factor O(beta). Finally, we show that
this runtime is optimal (up to logarithmic factors) for any beta and fixed seed
size k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0888</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0888</id><created>2012-12-04</created><authors><author><keyname>Rajabi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Ghassemian</keyname><forenames>Hassan</forenames></author></authors><title>Unmixing of Hyperspectral Data Using Robust Statistics-based NMF</title><categories>cs.CV</categories><comments>4 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed pixels are presented in hyperspectral images due to low spatial
resolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixels
spectra into endmembers spectra and abundance fractions. In this paper using of
robust statistics-based nonnegative matrix factorization (RNMF) for spectral
unmixing of hyperspectral data is investigated. RNMF uses a robust cost
function and iterative updating procedure, so is not sensitive to outliers.
This method has been applied to simulated data using USGS spectral library,
AVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMF
method based on SAD and AAD measures. Results demonstrate that this method can
be used efficiently for hyperspectral unmixing purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0892</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0892</id><created>2012-12-04</created><updated>2013-07-01</updated><authors><author><keyname>Tereshkov</keyname><forenames>Vasiliy M.</forenames></author></authors><title>An Intuitive Approach to Inertial Sensor Bias Estimation</title><categories>cs.SY math.OC</categories><comments>6 pages, 7 figures</comments><msc-class>70Q05, 93B07, 93B30, 93B52</msc-class><acm-class>I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple approach to gyro and accelerometer bias estimation is proposed. It
does not involve Kalman filtering or similar formal techniques. Instead, it is
based on physical intuition and exploits a duality between gimbaled and
strapdown inertial systems. The estimation problem is decoupled into two
separate stages. At the first stage, inertial system attitude errors are
corrected by means of a feedback from an external aid. In the presence of
uncompensated biases, the steady-state feedback rebalances those biases and can
be used to estimate them. At the second stage, the desired bias estimates are
expressed in a closed form in terms of the feedback signal. The estimator has
only three tunable parameters and is easy to implement and use. The tests
proved the feasibility of the proposed approach for the estimation of low-cost
MEMS inertial sensor biases on a moving land vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0895</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0895</id><created>2012-12-04</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>The max-plus algebra approach in modelling of queueing networks</title><categories>math.OC cs.SY</categories><comments>The SCS Summer Computer Simulation Conference, July 21-25, 1996,
  Portland, OR. ISBN 1-56555-098-6. arXiv admin note: text overlap with
  arXiv:1212.0578</comments><msc-class>15A80 (Primary) 90B22, 93C65, 65C20, 90B15 (Secondary)</msc-class><journal-ref>Proc. Summer Computer Simulation Conf (1996) P. 485-490</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of queueing networks which consist of single-server fork-join nodes
with infinite buffers is examined to derive a representation of the network
dynamics in terms of max-plus algebra. For the networks, we present a common
dynamic state equation which relates the departure epochs of customers from the
network nodes in an explicit vector form determined by a state transition
matrix. We show how the matrix may be calculated from the service time of
customers in the general case, and give examples of matrices inherent in
particular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0901</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0901</id><created>2012-12-04</created><updated>2012-12-13</updated><authors><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Boulanger-Lewandowski</keyname><forenames>Nicolas</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author></authors><title>Advances in Optimizing Recurrent Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After a more than decade-long period of relatively little research activity
in the area of recurrent neural networks, several new developments will be
reviewed here that have allowed substantial progress both in understanding and
in technical solutions towards more efficient training of recurrent networks.
These advances have been motivated by and related to the optimization issues
surrounding deep learning. Although recurrent networks are extremely powerful
in what they can in principle represent in terms of modelling sequences,their
training is plagued by two aspects of the same issue regarding the learning of
long-term dependencies. Experiments reported here evaluate the use of clipping
gradients, spanning longer time ranges with leaky integration, advanced
momentum techniques, using more powerful output probability models, and
encouraging sparser gradients to help symmetry breaking and credit assignment.
The experiments are performed on text and music data and show off the combined
effects of these techniques in generally improving both training and test
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0925</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0925</id><created>2012-12-04</created><authors><author><keyname>Madi</keyname><forenames>Nadim K. M.</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author></authors><title>Optimizing Service Differentiation Scheme with Sized-based Queue
  Management in DiffServ Networks</title><categories>cs.NI</categories><comments>10 pages, 9 figures, 1 table, Submitted to Journal of
  Telecommunications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduced Modified Sized-based Queue Management as a
dropping scheme that aims to fairly prioritize and allocate more service to
VoIP traffic over bulk data like FTP as the former one usually has small packet
size with less impact to the network congestion. In the same time, we want to
guarantee that this prioritization is fair enough for both traffic types. On
the other hand we study the total link delay over the congestive link with the
attempt to alleviate this congestion as much as possible at the by function of
early congestion notification. Our M-SQM scheme has been evaluated with NS2
experiments to measure the packets received from both and total link-delay for
different traffic. The performance evaluation results of M-SQM have been
validated and graphically compared with the performance of other three legacy
AQMs (RED, RIO, and PI). It is depicted that our M-SQM outperformed these AQMs
in providing QoS level of service differentiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0927</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0927</id><created>2012-12-04</created><updated>2013-02-05</updated><authors><author><keyname>Wu</keyname><forenames>Ke</forenames></author><author><keyname>Resnik</keyname><forenames>Philip</forenames></author></authors><title>Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown
  Automaton</title><categories>cs.CL cs.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce efficient algorithms for finding the $k$ shortest paths of a
weighted pushdown automaton (WPDA), a compact representation of a weighted set
of strings with potential applications in parsing and machine translation. Both
of our algorithms are derived from the same weighted deductive logic
description of the execution of a WPDA using different search strategies.
Experimental results show our Algorithm 2 adds very little overhead vs. the
single shortest path algorithm, even with a large $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0935</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0935</id><created>2012-12-05</created><updated>2014-05-14</updated><authors><author><keyname>De La Cruz</keyname><forenames>Livio</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Shen</keyname><forenames>Paul</forenames></author><author><keyname>Veeramoni</keyname><forenames>Sankar</forenames></author></authors><title>Computing Consensus Curves</title><categories>cs.CG cs.CV cs.GT cs.MA</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of extracting accurate average ant trajectories from
many (possibly inaccurate) input trajectories contributed by citizen
scientists. Although there are many generic software tools for motion tracking
and specific ones for insect tracking, even untrained humans are much better at
this task, provided a robust method to computing the average trajectories. We
implemented and tested several local (one ant at a time) and global (all ants
together) method. Our best performing algorithm uses a novel global method,
based on finding edge-disjoint paths in an ant-interaction graph constructed
from the input trajectories. The underlying optimization problem is a new and
interesting variant of network flow. Even though the problem is NP-hard, we
implemented two heuristics, which work very well in practice, outperforming all
other approaches, including the best automated system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0945</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0945</id><created>2012-12-05</created><authors><author><keyname>Garcia-Cardona</keyname><forenames>Cristina</forenames></author><author><keyname>Flenner</keyname><forenames>Arjuna</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>Multiclass Diffuse Interface Models for Semi-Supervised Learning on
  Graphs</title><categories>stat.ML cs.LG math.ST physics.data-an stat.TH</categories><comments>9 pages, to appear in Proceedings of the 2nd International Conference
  on Pattern Recognition Applications and Methods (ICPRAM 2013)</comments><acm-class>I.5.3</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present a graph-based variational algorithm for multiclass classification
of high-dimensional data, motivated by total variation techniques. The energy
functional is based on a diffuse interface model with a periodic potential. We
augment the model by introducing an alternative measure of smoothness that
preserves symmetry among the class labels. Through this modification of the
standard Laplacian, we construct an efficient multiclass method that allows for
sharp transitions between classes. The experimental results demonstrate that
our approach is competitive with the state of the art among other graph-based
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0950</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0950</id><created>2012-12-05</created><authors><author><keyname>Quennouelle</keyname><forenames>Cyril</forenames></author><author><keyname>Gosselin</keyname><forenames>Cl&#xe9;ment M.</forenames></author></authors><title>A General Formulation for the Stiffness Matrix of Parallel Mechanisms</title><categories>physics.class-ph cs.RO</categories><comments>25 pages. The submission of this paper was not completed after two
  reviews because I left the lab in early 2010</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from the definition of a stiffness matrix, the authors present a new
formulation of the Cartesian stiffness matrix of parallel mechanisms. The
proposed formulation is more general than any other stiffness matrix found in
the literature since it can take into account the stiffness of the passive
joints, it can consider additional compliances in the joints or in the links
and it remains valid for large displacements. Then, the validity, the
conservative property, the positive definiteness and the relation with other
formulations of stiffness matrices are discussed theoretically. Finally, a
numerical example is given in order to illustrate the correctness of this
matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0952</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0952</id><created>2012-12-05</created><updated>2015-02-28</updated><authors><author><keyname>Hegde</keyname><forenames>Nidhi</forenames><affiliation>LINCS</affiliation></author><author><keyname>Massouli&#xe9;</keyname><forenames>Laurent</forenames><affiliation>MSR - INRIA</affiliation></author><author><keyname>Viennot</keyname><forenames>Laurent</forenames><affiliation>INRIA Paris-Rocquencourt,LIAFA,LINCS</affiliation></author></authors><title>Self-Organizing Flows in Social Networks</title><categories>cs.SI cs.GT cs.NI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>Theoretical Computer Science, Elsevier, 2015, pp.16</journal-ref><doi>10.1016/j.tcs.2015.02.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks offer users new means of accessing information, essentially
relying on &quot;social filtering&quot;, i.e. propagation and filtering of information by
social contacts. The sheer amount of data flowing in these networks, combined
with the limited budget of attention of each user, makes it difficult to ensure
that social filtering brings relevant content to the interested users. Our
motivation in this paper is to measure to what extent self-organization of the
social network results in efficient social filtering. To this end we introduce
flow games, a simple abstraction that models network formation under selfish
user dynamics, featuring user-specific interests and budget of attention. In
the context of homogeneous user interests, we show that selfish dynamics
converge to a stable network structure (namely a pure Nash equilibrium) with
close-to-optimal information dissemination. We show in contrast, for the more
realistic case of heterogeneous interests, that convergence, if it occurs, may
lead to information dissemination that can be arbitrarily inefficient, as
captured by an unbounded &quot;price of anarchy&quot;. Nevertheless the situation differs
when users' interests exhibit a particular structure, captured by a metric
space with low doubling dimension. In that case, natural autonomous dynamics
converge to a stable configuration. Moreover, users obtain all the information
of interest to them in the corresponding dissemination, provided their budget
of attention is logarithmic in the size of their interest set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0960</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0960</id><created>2012-12-05</created><authors><author><keyname>Jung</keyname><forenames>Hyun Joon</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author></authors><title>Evaluating Classifiers Without Expert Labels</title><categories>cs.LG cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the challenge of evaluating a set of classifiers, as
done in shared task evaluations like the KDD Cup or NIST TREC, without expert
labels. While expert labels provide the traditional cornerstone for evaluating
statistical learners, limited or expensive access to experts represents a
practical bottleneck. Instead, we seek methodology for estimating performance
of the classifiers which is more scalable than expert labeling yet preserves
high correlation with evaluation based on expert labels. We consider both: 1)
using only labels automatically generated by the classifiers (blind
evaluation); and 2) using labels obtained via crowdsourcing. While
crowdsourcing methods are lauded for scalability, using such data for
evaluation raises serious concerns given the prevalence of label noise. In
regard to blind evaluation, two broad strategies are investigated: combine &amp;
score and score &amp; combine methods infer a single pseudo-gold label set by
aggregating classifier labels; classifiers are then evaluated based on this
single pseudo-gold label set. On the other hand, score &amp; combine methods: 1)
sample multiple label sets from classifier outputs, 2) evaluate classifiers on
each label set, and 3) average classifier performance across label sets. When
additional crowd labels are also collected, we investigate two alternative
avenues for exploiting them: 1) direct evaluation of classifiers; or 2)
supervision of combine &amp; score methods. To assess generality of our techniques,
classifier performance is measured using four common classification metrics,
with statistical significance tests. Finally, we measure both score and rank
correlations between estimated classifier performance vs. actual performance
according to expert judgments. Rigorous evaluation of classifiers from the TREC
2011 Crowdsourcing Track shows reliable evaluation can be achieved without
reliance on expert labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0966</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0966</id><created>2012-12-05</created><authors><author><keyname>Maietti</keyname><forenames>Maria Emilia</forenames></author><author><keyname>Rosolini</keyname><forenames>Giuseppe</forenames></author></authors><title>Unifying exact completions</title><categories>math.CT cs.LO math.LO</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the notion of exact completion with respect to an existential
elementary doctrine. We observe that the forgetful functor from the 2-category
exact categories to existential elementary doctrines has a left biadjoint that
can be obtained as a composite of two others. Finally, we conclude how this
notion encompasses both that of the exact completion of a regular category as
well as that of the exact completion of a cartesian category with weak
pullbacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0967</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0967</id><created>2012-12-05</created><authors><author><keyname>Singh</keyname><forenames>Sameer</forenames></author><author><keyname>Graepel</keyname><forenames>Thore</forenames></author></authors><title>Compiling Relational Database Schemata into Probabilistic Graphical
  Models</title><categories>cs.AI cs.DB cs.LG stat.ML</categories><comments>NIPS 2012 Workshop on Probabilistic Programming</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Instead of requiring a domain expert to specify the probabilistic
dependencies of the data, in this work we present an approach that uses the
relational DB schema to automatically construct a Bayesian graphical model for
a database. This resulting model contains customized distributions for columns,
latent variables that cluster the data, and factors that reflect and represent
the foreign key links. Experiments demonstrate the accuracy of the model and
the scalability of inference on synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0975</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0975</id><created>2012-12-05</created><updated>2015-02-15</updated><authors><author><keyname>Masnadi-Shirazi</keyname><forenames>Hamed</forenames></author><author><keyname>Vasconcelos</keyname><forenames>Nuno</forenames></author><author><keyname>Iranmehr</keyname><forenames>Arya</forenames></author></authors><title>Cost-Sensitive Support Vector Machines</title><categories>cs.LG stat.ML</categories><comments>32 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new procedure for learning cost-sensitive SVM(CS-SVM) classifiers is
proposed. The SVM hinge loss is extended to the cost sensitive setting, and the
CS-SVM is derived as the minimizer of the associated risk. The extension of the
hinge loss draws on recent connections between risk minimization and
probability elicitation. These connections are generalized to cost-sensitive
classification, in a manner that guarantees consistency with the cost-sensitive
Bayes risk, and associated Bayes decision rule. This ensures that optimal
decision rules, under the new hinge loss, implement the Bayes-optimal
cost-sensitive classification boundary. Minimization of the new hinge loss is
shown to be a generalization of the classic SVM optimization problem, and can
be solved by identical procedures. The dual problem of CS-SVM is carefully
scrutinized by means of regularization theory and sensitivity analysis and the
CS-SVM algorithm is substantiated. The proposed algorithm is also extended to
cost-sensitive learning with example dependent costs. The minimum cost
sensitive risk is proposed as the performance measure and is connected to ROC
analysis through vector optimization. The resulting algorithm avoids the
shortcomings of previous approaches to cost-sensitive SVM design, and is shown
to have superior experimental performance on a large number of cost sensitive
and imbalanced datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0979</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0979</id><created>2012-12-05</created><updated>2014-03-24</updated><authors><author><keyname>Duan</keyname><forenames>Ran</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author></authors><title>A Combinatorial Polynomial Algorithm for the Linear Arrow-Debreu Market</title><categories>cs.DS</categories><comments>Preliminary version in ICALP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first combinatorial polynomial time algorithm for computing
the equilibrium of the Arrow-Debreu market model with linear utilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0981</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0981</id><created>2012-12-05</created><authors><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author><author><keyname>Wen</keyname><forenames>Chengfeng</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng</forenames></author></authors><title>A Conformal Approach for Surface Inpainting</title><categories>math.DG cs.CG cs.GR cs.MM</categories><comments>19 pages, 12 figures</comments><journal-ref>Journal of Inverse Problems and Imaging. Volume 7, Issue 3, 863 -
  884, 2013</journal-ref><doi>10.3934/ipi.2013.7.863</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We address the problem of surface inpainting, which aims to fill in holes or
missing regions on a Riemann surface based on its surface geometry. In
practical situation, surfaces obtained from range scanners often have holes
where the 3D models are incomplete. In order to analyze the 3D shapes
effectively, restoring the incomplete shape by filling in the surface holes is
necessary. In this paper, we propose a novel conformal approach to inpaint
surface holes on a Riemann surface based on its surface geometry. The basic
idea is to represent the Riemann surface using its conformal factor and mean
curvature. According to Riemann surface theory, a Riemann surface can be
uniquely determined by its conformal factor and mean curvature up to a rigid
motion. Given a Riemann surface $S$, its mean curvature $H$ and conformal
factor $\lambda$ can be computed easily through its conformal parameterization.
Conversely, given $\lambda$ and $H$, a Riemann surface can be uniquely
reconstructed by solving the Gauss-Codazzi equation on the conformal parameter
domain. Hence, the conformal factor and the mean curvature are two geometric
quantities fully describing the surface. With this $\lambda$-$H$ representation
of the surface, the problem of surface inpainting can be reduced to the problem
of image inpainting of $\lambda$ and $H$ on the conformal parameter domain.
Once $\lambda$ and $H$ are inpainted, a Riemann surface can be reconstructed
which effectively restores the 3D surface with missing holes. Since the
inpainting model is based on the geometric quantities $\lambda$ and $H$, the
restored surface follows the surface geometric pattern. We test the proposed
algorithm on synthetic data as well as real surface data. Experimental results
show that our proposed method is an effective surface inpainting algorithm to
fill in surface holes on an incomplete 3D models based their surface geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.0992</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.0992</id><created>2012-12-05</created><authors><author><keyname>Mavandadi</keyname><forenames>Sam</forenames></author><author><keyname>Feng</keyname><forenames>Steve</forenames></author><author><keyname>Yu</keyname><forenames>Frank</forenames></author><author><keyname>Yu</keyname><forenames>Richard</forenames></author><author><keyname>Ozcan</keyname><forenames>Aydogan</forenames></author></authors><title>BigFoot: Analysis, monitoring, tracking and sharing of bio-medical
  features of human appendages using consumer-grade home and office based
  imaging devices</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we describe a system for personal and professional management and
analysis of bio-medical images captured using off-the-shelf, consumer-grade
imaging devices such as scanners, digital cameras, cellphones, webcams and
tablet PCs. Specifically, we describe an implementation of this system for the
analysis, monitoring and tracking of conditions and features of human feet
using a flatbed scanner as the image capture device and a custom-designed set
of algorithms and software to manage and analyze the acquired data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1002</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1002</id><created>2012-12-05</created><authors><author><keyname>Abramov</keyname><forenames>Konstantin</forenames></author><author><keyname>Monakhov</keyname><forenames>Yuri</forenames></author></authors><title>Stochastic Models of Misinformation Distribution in Online Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>4 pages, 1 figure, 1 table</comments><acm-class>H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report contains results of an experimental study of the distribution of
misinformation in online social networks (OSNs). We consider the classification
of the topologies of OSNs and analyze the parameters identified in order to
relate the topology of a real network with one of the classes. We propose an
algorithm for conducting a search for the percolation cluster in the social
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1037</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1037</id><created>2012-12-05</created><authors><author><keyname>Rao</keyname><forenames>Tushar</forenames><affiliation>NSIT-Delhi</affiliation></author><author><keyname>Srivastava</keyname><forenames>Saket</forenames><affiliation>IIIT-Delhi</affiliation></author></authors><title>Modeling Movements in Oil, Gold, Forex and Market Indices using Search
  Volume Index and Twitter Sentiments</title><categories>cs.CE cs.SI q-fin.GN</categories><comments>10 pages, 4 figures, 9 Tables</comments><report-no>IIITD-TR-2012-005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Study of the forecasting models using large scale microblog discussions and
the search behavior data can provide a good insight for better understanding
the market movements. In this work we collected a dataset of 2 million tweets
and search volume index (SVI from Google) for a period of June 2010 to
September 2011. We perform a study over a set of comprehensive causative
relationships and developed a unified approach to a model for various market
securities like equity (Dow Jones Industrial Average-DJIA and NASDAQ-100),
commodity markets (oil and gold) and Euro Forex rates. We also investigate the
lagged and statistically causative relations of Twitter sentiments developed
during active trading days and market inactive days in combination with the
search behavior of public before any change in the prices/ indices. Our results
show extent of lagged significance with high correlation value upto 0.82
between search volumes and gold price in USD. We find weekly accuracy in
direction (up and down prediction) uptil 94.3% for DJIA and 90% for NASDAQ-100
with significant reduction in mean average percentage error for all the
forecasting models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1046</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1046</id><created>2012-12-05</created><authors><author><keyname>Zhu</keyname><forenames>Yuqing</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author></authors><title>Latency Bounding by Trading off Consistency in NoSQL Store: A Staging
  and Stepwise Approach</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latency is a key service factor for user satisfaction. Consistency is in a
trade-off relation with operation latency in the distributed and replicated
scenario. Existing NoSQL stores guarantee either strong or weak consistencies
but none provides the best consistency based on the response latency. In this
paper, we introduce dConssandra, a NoSQL store enabling users to specify
latency bounds for data access operations. dConssandra dynamically bounds data
access latency by trading off replica consistency. dConssandra is based on
Cassandra. In comparison to Cassandra's implementation, dConssandra has a
staged replication strategy enabling synchronous or asynchronous replication on
demand. The main idea to bound latency by trading off consistency is to
decompose the replication process into minute steps and bound latency by
executing only a subset of these steps. dConssandra also implements a different
in-memory storage architecture to support the above features. Experimental
results for dConssandra over an actual cluster demonstrate that (1) the actual
response latency is bounded by the given latency constraint; (2) greater write
latency bounds lead to a lower latency in reading the latest value; and, (3)
greater read latency bounds lead to the return of more recently written values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1053</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1053</id><created>2012-12-03</created><authors><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Lubicz</keyname><forenames>David</forenames><affiliation>IRMAR</affiliation></author></authors><title>Linear Algebra over Z_p[[u]] and related rings</title><categories>math.NT cs.DS math.AC</categories><comments>38 pages</comments><proxy>ccsd</proxy><doi>10.1112/S146115701300034X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let R be a complete discrete valuation ring, S=R[[u]] and n a positive
integer. The aim of this paper is to explain how to compute efficiently usual
operations such as sum and intersection of sub-S-modules of S^d. As S is not
principal, it is not possible to have a uniform bound on the number of
generators of the modules resulting of these operations. We explain how to
mitigate this problem, following an idea of Iwasawa, by computing an
approximation of the result of these operations up to a quasi-isomorphism. In
the course of the analysis of the p-adic and u-adic precisions of the
computations, we have to introduce more general coefficient rings that may be
interesting for their own sake. Being able to perform linear algebra operations
modulo quasi-isomorphism with S-modules has applications in Iwasawa theory and
p-adic Hodge theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1061</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1061</id><created>2012-12-05</created><updated>2013-02-08</updated><authors><author><keyname>Braunstein</keyname><forenames>L. A.</forenames></author><author><keyname>Macri</keyname><forenames>P. A.</forenames></author><author><keyname>Iglesias</keyname><forenames>J. R.</forenames></author></authors><title>Study of a Market Model with Conservative Exchanges on Complex Networks</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>arXiv admin note: substantial text overlap with arXiv:1007.0461</comments><journal-ref>Physica A, 392, 1788-1794 (2013)</journal-ref><doi>10.1016/j.physa.2012.12.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many models of market dynamics make use of the idea of conservative wealth
exchanges among economic agents. A few years ago an exchange model using
extremal dynamics was developed and a very interesting result was obtained: a
self-generated minimum wealth or poverty line. On the other hand, the wealth
distribution exhibited an exponential shape as a function of the square of the
wealth. These results have been obtained both considering exchanges between
nearest neighbors or in a mean field scheme. In the present paper we study the
effect of distributing the agents on a complex network. We have considered
archetypical complex networks: Erd\&quot;{o}s-R\'enyi random networks and scale-free
networks. The presence of a poverty line with finite wealth is preserved but
spatial correlations are important, particularly between the degree of the node
and the wealth. We present a detailed study of the correlations, as well as the
changes in the Gini coefficient, that measures the inequality, as a function of
the type and average degree of the considered networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1068</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1068</id><created>2012-12-05</created><authors><author><keyname>Ermann</keyname><forenames>Leonardo</forenames></author><author><keyname>Frahm</keyname><forenames>Klaus M.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Spectral properties of Google matrix of Wikipedia and other networks</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>10 pages, 9 figures</comments><journal-ref>Eur. Phys. J. B 86, 193 (2013)</journal-ref><doi>10.1140/epjb/e2013-31090-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the properties of eigenvalues and eigenvectors of the Google matrix
of the Wikipedia articles hyperlink network and other real networks. With the
help of the Arnoldi method we analyze the distribution of eigenvalues in the
complex plane and show that eigenstates with significant eigenvalue modulus are
located on well defined network communities. We also show that the correlator
between PageRank and CheiRank vectors distinguishes different organizations of
information flow on BBC and Le Monde web sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1073</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1073</id><created>2012-12-05</created><updated>2014-05-24</updated><authors><author><keyname>Pan</keyname><forenames>Jinshan</forenames></author><author><keyname>Liu</keyname><forenames>Risheng</forenames></author><author><keyname>Su</keyname><forenames>Zhixun</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng</forenames></author></authors><title>Kernel Estimation from Salient Structure for Robust Motion Deblurring</title><categories>cs.CV</categories><comments>This work has been accepted by Signal Processing: Image
  Communication, 2013</comments><journal-ref>Signal Processing: Image Communication, 2013</journal-ref><doi>10.1016/j.image.2013.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind image deblurring algorithms have been improving steadily in the past
years. Most state-of-the-art algorithms, however, still cannot perform
perfectly in challenging cases, especially in large blur setting. In this
paper, we focus on how to estimate a good kernel estimate from a single blurred
image based on the image structure. We found that image details caused by
blurring could adversely affect the kernel estimation, especially when the blur
kernel is large. One effective way to eliminate these details is to apply image
denoising model based on the Total Variation (TV). First, we developed a novel
method for computing image structures based on TV model, such that the
structures undermining the kernel estimation will be removed. Second, to
mitigate the possible adverse effect of salient edges and improve the
robustness of kernel estimation, we applied a gradient selection method. Third,
we proposed a novel kernel estimation method, which is capable of preserving
the continuity and sparsity of the kernel and reducing the noises. Finally, we
developed an adaptive weighted spatial prior, for the purpose of preserving
sharp edges in latent image restoration. The effectiveness of our method is
demonstrated by experiments on various kinds of challenging examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1080</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1080</id><created>2012-12-05</created><authors><author><keyname>McCracken</keyname><forenames>Merrick</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author></authors><title>Hidden Markov Estimation of Bistatic Range From Cluttered Ultra-wideband
  Impulse Responses</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra-wideband (UWB) multistatic radar can be used for target detection and
tracking in buildings and rooms. Target detection and tracking relies on
accurate knowledge of the bistatic delay. Noise, measurement error, and the
problem of dense, overlapping multipath signals in the measured UWB channel
impulse response (CIR) all contribute to make bistatic delay estimation
challenging. It is often assumed that a calibration CIR, that is, a measurement
from when no person is present, is easily subtracted from a newly captured CIR.
We show this is often not the case. We propose modeling the difference between
a current set of CIRs and a set of calibration CIRs as a hidden Markov model
(HMM). Multiple experimental deployments are performed to collect CIR data and
test the performance of this model and compare its performance to existing
methods. Our experimental results show an RMSE of 2.85 ns and 2.76 ns for our
HMM-based approach, compared to a thresholding method which, if the ideal
threshold is known a priori, achieves 3.28 ns and 4.58 ns. By using the
Baum-Welch algorithm, the HMM-based estimator is shown to be very robust to
initial parameter settings. Localization performance is also improved using the
HMM-based bistatic delay estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1089</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1089</id><created>2012-12-05</created><updated>2013-07-29</updated><authors><author><keyname>Ranzato</keyname><forenames>Francesco</forenames></author></authors><title>An Efficient Simulation Algorithm on Kripke Structures</title><categories>cs.LO cs.DS</categories><comments>Conference version appeared in Proceedings of the 38th International
  Symposium on Mathematical Foundations of Computer Science (MFCS'13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of algorithms for computing the simulation preorder (and
equivalence) on Kripke structures are available. Let Sigma denote the state
space, -&gt; the transition relation and Psim the partition of Sigma induced by
simulation equivalence. While some algorithms are designed to reach the best
space bounds, whose dominating additive term is |Psim|^2, other algorithms are
devised to attain the best time complexity O(|Psim||-&gt;|). We present a novel
simulation algorithm which is both space and time efficient: it runs in
O(|Psim|^2 log|Psim| + |Sigma|log|Sigma|) space and O(|Psim||-&gt;|log|Sigma|)
time. Our simulation algorithm thus reaches the best space bounds while closely
approaching the best time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1094</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1094</id><created>2012-12-04</created><updated>2013-04-29</updated><authors><author><keyname>Reem</keyname><forenames>Daniel</forenames></author></authors><title>The geometric stability of Voronoi diagrams in normed spaces which are
  not uniformly convex</title><categories>cs.CG math.FA</categories><comments>29 pages; 10 figures; Sections 1,4,5, the abstract, and the
  bibliography section were modified (mainly extended); a few typos were
  corrected; added a section about topological properties of Voronoi cells</comments><msc-class>46N99, 68U05, 46B20, 65D18</msc-class><acm-class>F.2.2; G.0; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Voronoi diagram is a geometric object which is widely used in many areas.
Recently it has been shown that under mild conditions Voronoi diagrams have a
certain continuity property: small perturbations of the sites yield small
perturbations in the shapes of the corresponding Voronoi cells. However, this
result is based on the assumption that the ambient normed space is uniformly
convex. Unfortunately, simple counterexamples show that if uniform convexity is
removed, then instability can occur. Since Voronoi diagrams in normed spaces
which are not uniformly convex do appear in theory and practice, e.g., in the
plane with the Manhattan (ell_1) distance, it is natural to ask whether the
stability property can be generalized to them, perhaps under additional
assumptions. This paper shows that this is indeed the case assuming the unit
sphere of the space has a certain (non-exotic) structure and the sites satisfy
a certain &quot;general position&quot; condition related to it. The condition on the unit
sphere is that it can be decomposed into at most one &quot;rotund part&quot; and at most
finitely many non-degenerate convex parts. Along the way certain topological
properties of Votonoi cells (e.g., that the induced bisectors are not &quot;fat&quot;)
are proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1095</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1095</id><created>2012-12-04</created><updated>2015-04-02</updated><authors><author><keyname>Reem</keyname><forenames>Daniel</forenames></author></authors><title>On the possibility of simple parallel computing of Voronoi diagrams and
  Delaunay graphs</title><categories>cs.CG cs.DC cs.DS</categories><comments>This is a major revision which includes various improved sections and
  several additions (in particular, algorithmic and mathematical treatments of
  the case where the sites are uniformly distributed). The paper is long but
  more than half of it is composed of proofs and references: it is enough to
  look at pages 6-9 for understanding the algorithm</comments><msc-class>68U05, 68W10, 65D18</msc-class><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Voronoi diagram is a widely used geometric data structure. The theory of
algorithms for computing Euclidean Voronoi diagrams of point sites is rich and
useful, with several different and important algorithms. However, this theory
has been quite steady during the last few decades in the sense that new
algorithms have not entered the game. In addition, most of the known algorithms
are sequential in nature and hence cast inherent difficulties on the
possibility to compute the diagram in parallel. This paper presents a new and
simple algorithm which enables the (combinatorial) computation of the diagram.
The algorithm is significantly different from previous ones and some of the
involved concepts in it are in the spirit of linear programming and optics.
Parallel implementation is naturally supported since each Voronoi cell can be
computed independently of the other cells. A new combinatorial structure for
representing the cells (and any convex polytope) is described along the way and
the computation of the induced Delaunay graph is obtained almost automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1098</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1098</id><created>2012-12-05</created><authors><author><keyname>Fabregas</keyname><forenames>Albert Guillen i</forenames></author><author><keyname>Land</keyname><forenames>Ingmar</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author></authors><title>Extremes of Error Exponents</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, accepted IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper determines the range of feasible values of standard error
exponents for binary-input memoryless symmetric channels of fixed capacity $C$
and shows that extremes are attained by the binary symmetric and the binary
erasure channel. The proof technique also provides analogous extremes for other
quantities related to Gallager's $E_0$ function, such as the cutoff rate, the
Bhattacharyya parameter, and the channel dispersion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1100</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1100</id><created>2012-12-05</created><authors><author><keyname>Smith</keyname><forenames>J. E.</forenames></author><author><keyname>Caleb-Solly</keyname><forenames>P.</forenames></author><author><keyname>Tahir</keyname><forenames>M. A.</forenames></author><author><keyname>Sannen</keyname><forenames>D.</forenames></author><author><keyname>van-Brussel</keyname><forenames>H.</forenames></author></authors><title>Making Early Predictions of the Accuracy of Machine Learning
  Applications</title><categories>cs.LG cs.AI stat.ML</categories><comments>35 pagers, 12 figures</comments><acm-class>I.2.6; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accuracy of machine learning systems is a widely studied research topic.
Established techniques such as cross-validation predict the accuracy on unseen
data of the classifier produced by applying a given learning method to a given
training data set. However, they do not predict whether incurring the cost of
obtaining more data and undergoing further training will lead to higher
accuracy. In this paper we investigate techniques for making such early
predictions. We note that when a machine learning algorithm is presented with a
training set the classifier produced, and hence its error, will depend on the
characteristics of the algorithm, on training set's size, and also on its
specific composition. In particular we hypothesise that if a number of
classifiers are produced, and their observed error is decomposed into bias and
variance terms, then although these components may behave differently, their
behaviour may be predictable.
  We test our hypothesis by building models that, given a measurement taken
from the classifier created from a limited number of samples, predict the
values that would be measured from the classifier produced when the full data
set is presented. We create separate models for bias, variance and total error.
Our models are built from the results of applying ten different machine
learning algorithms to a range of data sets, and tested with &quot;unseen&quot;
algorithms and datasets. We analyse the results for various numbers of initial
training samples, and total dataset sizes. Results show that our predictions
are very highly correlated with the values observed after undertaking the extra
training. Finally we consider the more complex case where an ensemble of
heterogeneous classifiers is trained, and show how we can accurately estimate
an upper bound on the accuracy achievable after further training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1107</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1107</id><created>2012-12-05</created><authors><author><keyname>Rao</keyname><forenames>Tushar</forenames></author><author><keyname>Srivastava</keyname><forenames>Saket</forenames></author></authors><title>Twitter Sentiment Analysis: How To Hedge Your Bets In The Stock Markets</title><categories>cs.CE</categories><comments>21 pages, 10 figures and 6 Tables</comments><report-no>IIITD-TR-2012-004</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging interest of trading companies and hedge funds in mining social web
has created new avenues for intelligent systems that make use of public opinion
in driving investment decisions. It is well accepted that at high frequency
trading, investors are tracking memes rising up in microblogging forums to
count for the public behavior as an important feature while making short term
investment decisions. We investigate the complex relationship between tweet
board literature (like bullishness, volume, agreement etc) with the financial
market instruments (like volatility, trading volume and stock prices). We have
analyzed Twitter sentiments for more than 4 million tweets between June 2010
and July 2011 for DJIA, NASDAQ-100 and 11 other big cap technological stocks.
Our results show high correlation (upto 0.88 for returns) between stock prices
and twitter sentiments. Further, using Granger's Causality Analysis, we have
validated that the movement of stock prices and indices are greatly affected in
the short term by Twitter discussions. Finally, we have implemented Expert
Model Mining System (EMMS) to demonstrate that our forecasted returns give a
high value of R-square (0.952) with low Maximum Absolute Percentage Error
(MaxAPE) of 1.76% for Dow Jones Industrial Average (DJIA). We introduce a novel
way to make use of market monitoring elements derived from public mood to
retain a portfolio within limited risk state (highly improved hedging bets)
during typical market conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1108</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1108</id><created>2012-12-05</created><updated>2015-04-11</updated><authors><author><keyname>Belanich</keyname><forenames>Joshua</forenames></author><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author></authors><title>On the Convergence Properties of Optimal AdaBoost</title><categories>cs.LG cs.AI stat.ML</categories><comments>66 pp, 7 figs, 1 table; Change - presentation; dominated and
  effective weak-classifiers; experiments with dec. stumps in real-world data:
  reduction in #effective and unique stumps, may explain &quot;resistance to
  overfitting;&quot; log-growth #unique stumps with #rounds -&gt; &quot;AdaBoost-cycles
  Conjecture&quot; likely false in general; and new generalization bounds; submitted
  to MLJ 4/10/15</comments><msc-class>68Q32 (Primary) 68T05, 37A99 (Secondary)</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AdaBoost is one of the most popular machine-learning algorithms. It is simple
to implement and often found very effective by practitioners, while still being
mathematically elegant and theoretically sound. AdaBoost's behavior in
practice, and in particular the test-error behavior, has puzzled many eminent
researchers for over a decade: It seems to defy our general intuition in
machine learning regarding the fundamental trade-off between model complexity
and generalization performance. In this paper, we establish the convergence of
&quot;Optimal AdaBoost,&quot; a term coined by Rudin, Daubechies, and Schapire in 2004.
We prove the convergence, with the number of rounds, of the classifier itself,
its generalization error, and its resulting margins for fixed data sets, under
certain reasonable conditions. More generally, we prove that the time/per-round
average of almost any function of the example weights converges. Our approach
is to frame AdaBoost as a dynamical system, to provide sufficient conditions
for the existence of an invariant measure, and to employ tools from ergodic
theory. Unlike previous work, we do not assume AdaBoost cycles; actually, we
present empirical evidence against it on real-world datasets. Our main
theoretical results hold under a weaker condition. We show sufficient empirical
evidence that Optimal AdaBoost always met the condition on every real-world
dataset we tried. Our results formally ground future convergence-rate analyses,
and may even provide opportunities for slight algorithmic modifications to
optimize the generalization ability of AdaBoost classifiers, thus reducing a
practitioner's burden of deciding how long to run the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1115</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1115</id><created>2012-12-05</created><authors><author><keyname>Gregori</keyname><forenames>Maria</forenames></author><author><keyname>Payar&#xf3;</keyname><forenames>Miquel</forenames></author></authors><title>Energy-efficient transmission for wireless energy harvesting nodes</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting is increasingly gaining importance as a means to charge
battery powered devices such as sensor nodes. Efficient transmission strategies
must be developed for Wireless Energy Harvesting Nodes (WEHNs) that take into
account both the availability of energy and data in the node. We consider a
scenario where data and energy packets arrive to the node where the time
instants and amounts of the packets are known (offline approach). In this
paper, the best data transmission strategy is found for a finite battery
capacity WEHN that has to fulfill some Quality of Service (QoS) constraints, as
well as the energy and data causality constraints. As a result of our analysis,
we can state that losing energy due to overflows of the battery is inefficient
unless there is no more data to transmit and that the problem may not have a
feasible solution. Finally, an algorithm that computes the data transmission
curve minimizing the total transmission time that satisfies the aforementioned
constraints has been developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1121</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1121</id><created>2012-12-05</created><authors><author><keyname>Stanton</keyname><forenames>Isabelle</forenames></author></authors><title>Streaming Balanced Graph Partitioning for Random Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a recent explosion in the size of stored data, partially due
to advances in storage technology, and partially due to the growing popularity
of cloud-computing and the vast quantities of data generated. This motivates
the need for streaming algorithms that can compute approximate solutions
without full random access to all of the data.
  We model the problem of loading a graph onto a distributed cluster as
computing an approximately balanced $k$-partitioning of a graph in a streaming
fashion with only one pass over the data. We give lower bounds on this problem,
showing that no algorithm can obtain an $o(n)$ approximation with a random or
adversarial stream ordering. We analyze two variants of a randomized greedy
algorithm, one that prefers the $\arg\max$ and one that is proportional, on
random graphs with embedded balanced $k$-cuts and are able to theoretically
bound the performance of each algorithms - the $\arg\max$ algorithm is able to
recover the embedded $k$-cut, while, surprisingly, the proportional variant can
not. This matches the experimental results in [25].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1131</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1131</id><created>2012-12-05</created><authors><author><keyname>Katz</keyname><forenames>Gilad</forenames></author><author><keyname>Shani</keyname><forenames>Guy</forenames></author><author><keyname>Shapira</keyname><forenames>Bracha</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author></authors><title>Using Wikipedia to Boost SVD Recommender Systems</title><categories>cs.LG cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singular Value Decomposition (SVD) has been used successfully in recent years
in the area of recommender systems. In this paper we present how this model can
be extended to consider both user ratings and information from Wikipedia. By
mapping items to Wikipedia pages and quantifying their similarity, we are able
to use this information in order to improve recommendation accuracy, especially
when the sparsity is high. Another advantage of the proposed approach is the
fact that it can be easily integrated into any other SVD implementation,
regardless of additional parameters that may have been added to it. Preliminary
experimental results on the MovieLens dataset are encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1139</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1139</id><created>2012-12-05</created><authors><author><keyname>Hauck</keyname><forenames>Peter</forenames></author><author><keyname>Huber</keyname><forenames>Michael</forenames></author><author><keyname>Bertram</keyname><forenames>Juliane</forenames></author><author><keyname>Brauchle</keyname><forenames>Dennis</forenames></author><author><keyname>Ziesche</keyname><forenames>Sebastian</forenames></author></authors><title>Efficient Majority-Logic Decoding of Short-Length Reed--Muller Codes at
  Information Positions</title><categories>cs.IT cs.DM cs.ET math.CO math.IT</categories><comments>8 pages; to appear in &quot;IEEE Transactions on Communications&quot;</comments><journal-ref>IEEE Transactions on Communications (2013, Volume:61 , Issue: 3 )</journal-ref><doi>10.1109/TCOMM.2013.012313.110415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short-length Reed--Muller codes under majority-logic decoding are of
particular importance for efficient hardware implementations in real-time and
embedded systems. This paper significantly improves Chen's two-step
majority-logic decoding method for binary Reed--Muller codes $\text{RM}(r,m)$,
$r \leq m/2$, if --- systematic encoding assumed --- only errors at information
positions are to be corrected. Some general results on the minimal number of
majority gates are presented that are particularly good for short codes.
Specifically, with its importance in applications as a 3-error-correcting,
self-dual code, the smallest non-trivial example, $\text{RM}(2,5)$ of dimension
16 and length 32, is investigated in detail. Further, the decoding complexity
of our procedure is compared with that of Chen's decoding algorithm for various
Reed--Muller codes up to length $2^{10}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1143</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1143</id><created>2012-12-05</created><authors><author><keyname>Bouvrie</keyname><forenames>Jake</forenames></author><author><keyname>Maggioni</keyname><forenames>Mauro</forenames></author></authors><title>Multiscale Markov Decision Problems: Compression, Solution, and Transfer
  Learning</title><categories>cs.AI cs.SY math.OC stat.ML</categories><comments>86 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in sequential decision making and stochastic control often have
natural multiscale structure: sub-tasks are assembled together to accomplish
complex goals. Systematically inferring and leveraging hierarchical structure,
particularly beyond a single level of abstraction, has remained a longstanding
challenge. We describe a fast multiscale procedure for repeatedly compressing,
or homogenizing, Markov decision processes (MDPs), wherein a hierarchy of
sub-problems at different scales is automatically determined. Coarsened MDPs
are themselves independent, deterministic MDPs, and may be solved using
existing algorithms. The multiscale representation delivered by this procedure
decouples sub-tasks from each other and can lead to substantial improvements in
convergence rates both locally within sub-problems and globally across
sub-problems, yielding significant computational savings. A second fundamental
aspect of this work is that these multiscale decompositions yield new transfer
opportunities across different problems, where solutions of sub-tasks at
different levels of the hierarchy may be amenable to transfer to new problems.
Localized transfer of policies and potential operators at arbitrary scales is
emphasized. Finally, we demonstrate compression and transfer in a collection of
illustrative domains, including examples involving discrete and continuous
statespaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1149</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1149</id><created>2012-12-05</created><authors><author><keyname>Cloteaux</keyname><forenames>Brian</forenames></author><author><keyname>LaMar</keyname><forenames>M. Drew</forenames></author><author><keyname>Moseman</keyname><forenames>Elizabeth</forenames></author><author><keyname>Shook</keyname><forenames>James</forenames></author></authors><title>Threshold Digraphs</title><categories>math.CO cs.DM</categories><msc-class>05C20</msc-class><doi>10.6028/jres.119.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A digraph whose degree sequence has a unique vertex labeled realization is
called threshold. In this paper we present several characterizations of
threshold digraphs and their degree sequences, and show these characterizations
to be equivalent. One of the characterizations is new, and allows for a shorter
proof of the equivalence of the two known characterizations as well as proving
the final characterization which appears without proof in the literature. Using
this result, we obtain a new, short proof of the Fulkerson-Chen theorem on
degree sequences of general digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1180</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1180</id><created>2012-12-05</created><authors><author><keyname>Kon</keyname><forenames>Mark A.</forenames></author><author><keyname>Plaskota</keyname><forenames>Leszek</forenames></author></authors><title>On Some Integrated Approaches to Inference</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present arguments for the formulation of unified approach to different
standard continuous inference methods from partial information. It is claimed
that an explicit partition of information into a priori (prior knowledge) and a
posteriori information (data) is an important way of standardizing inference
approaches so that they can be compared on a normative scale, and so that
notions of optimal algorithms become farther-reaching. The inference methods
considered include neural network approaches, information-based complexity, and
Monte Carlo, spline, and regularization methods. The model is an extension of
currently used continuous complexity models, with a class of algorithms in the
form of optimization methods, in which an optimization functional (involving
the data) is minimized. This extends the family of current approaches in
continuous complexity theory, which include the use of interpolatory algorithms
in worst and average case settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1185</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1185</id><created>2012-12-05</created><updated>2013-11-07</updated><authors><author><keyname>Bogaerts</keyname><forenames>Mathieu</forenames></author><author><keyname>Dukes</keyname><forenames>Peter</forenames></author></authors><title>Semidefinite programming for permutation codes</title><categories>math.CO cs.IT math.IT</categories><comments>13 pages, 4 tables</comments><msc-class>05E10, 05E30 (primary), 94B65 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate study of the Terwilliger algebra and related semidefinite
programming techniques for the conjugacy scheme of the symmetric group
Sym$(n)$. In particular, we compute orbits of ordered pairs on Sym$(n)$ acted
upon by conjugation and inversion, explore a block diagonalization of the
associated algebra, and obtain improved upper bounds on the size $M(n,d)$ of
permutation codes of lengths up to 7. For instance, these techniques detect the
nonexistence of the projective plane of order six via $M(6,5)&lt;30$ and yield a
new best bound $M(7,4) \le 535$ for a challenging open case. Each of these
represents an improvement on earlier Delsarte linear programming results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1186</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1186</id><created>2012-12-05</created><updated>2013-10-30</updated><authors><author><keyname>Geng</keyname><forenames>Quan</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>The Optimal Mechanism in Differential Privacy</title><categories>cs.CR cs.DS</categories><comments>40 pages, 5 figures. Part of this work was presented in DIMACS
  Workshop on Recent Work on Differential Privacy across Computer Science,
  October 24 - 26, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the optimal $\epsilon$-differentially private mechanism for single
real-valued query function under a very general utility-maximization (or
cost-minimization) framework. The class of noise probability distributions in
the optimal mechanism has {\em staircase-shaped} probability density functions
which are symmetric (around the origin), monotonically decreasing and
geometrically decaying. The staircase mechanism can be viewed as a {\em
geometric mixture of uniform probability distributions}, providing a simple
algorithmic description for the mechanism. Furthermore, the staircase mechanism
naturally generalizes to discrete query output settings as well as more
abstract settings. We explicitly derive the optimal noise probability
distributions with minimum expectation of noise amplitude and power. Comparing
the optimal performances with those of the Laplacian mechanism, we show that in
the high privacy regime ($\epsilon$ is small), Laplacian mechanism is
asymptotically optimal as $\epsilon \to 0$; in the low privacy regime
($\epsilon$ is large), the minimum expectation of noise amplitude and minimum
noise power are $\Theta(\Delta e^{-\frac{\epsilon}{2}})$ and $\Theta(\Delta^2
e^{-\frac{2\epsilon}{3}})$ as $\epsilon \to +\infty$, while the expectation of
noise amplitude and power using the Laplacian mechanism are
$\frac{\Delta}{\epsilon}$ and $\frac{2\Delta^2}{\epsilon^2}$, where $\Delta$ is
the sensitivity of the query function. We conclude that the gains are more
pronounced in the low privacy regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1187</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1187</id><created>2012-12-05</created><authors><author><keyname>Hosseini</keyname><forenames>Mahdi S.</forenames></author><author><keyname>Plataniotis</keyname><forenames>Konstantinos N.</forenames></author></authors><title>Compressed Sensing Recoverability In Imaging Modalities</title><categories>cs.IT math.IT</categories><comments>This paper is submitted to ICASSP2013 and is protected under the IEEE
  copyright permission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a framework for the recoverability analysis in
compressive sensing for imaging applications such as CI cameras, rapid MRI and
coded apertures. This is done using the fact that the Spherical Section
Property (SSP) of a sensing matrix provides a lower bound for unique sparse
recovery condition. The lower bound is evaluated for different sampling
paradigms adopted from the aforementioned imaging modalities. In particular, a
platform is provided to analyze the well-posedness of sub-sampling patterns
commonly used in practical scenarios. The effectiveness of the various designed
patterns for sparse image recovery is studied through numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1192</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1192</id><created>2012-12-05</created><updated>2012-12-07</updated><authors><author><keyname>Espl&#xe0;-Gomis</keyname><forenames>Miquel</forenames></author><author><keyname>S&#xe1;nchez-Mart&#xed;nez</keyname><forenames>Felipe</forenames></author><author><keyname>Forcada</keyname><forenames>Mikel L.</forenames></author></authors><title>Using external sources of bilingual information for on-the-fly word
  alignment</title><categories>cs.CL</categories><comments>4 figures, 3 tables, 19 pages</comments><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we present a new and simple language-independent method for
word-alignment based on the use of external sources of bilingual information
such as machine translation systems. We show that the few parameters of the
aligner can be trained on a very small corpus, which leads to results
comparable to those obtained by the state-of-the-art tool GIZA++ in terms of
precision. Regarding other metrics, such as alignment error rate or F-measure,
the parametric aligner, when trained on a very small gold-standard (450 pairs
of sentences), provides results comparable to those produced by GIZA++ when
trained on an in-domain corpus of around 10,000 pairs of sentences.
Furthermore, the results obtained indicate that the training is
domain-independent, which enables the use of the trained aligner 'on the fly'
on any new pair of sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1198</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1198</id><created>2012-12-05</created><authors><author><keyname>Song</keyname><forenames>Yiwei</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author><author><keyname>Shao</keyname><forenames>Huai-Rong</forenames></author><author><keyname>Ngo</keyname><forenames>Chiu</forenames></author></authors><title>Lattice Coding for the Two-way Two-relay Channel</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory on December 3,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice coding techniques may be used to derive achievable rate regions which
outperform known independent, identically distributed (i.i.d.) random codes in
multi-source relay networks and in particular the two-way relay channel. Gains
stem from the ability to decode the sum of codewords (or messages) using
lattice codes at higher rates than possible with i.i.d. random codes. Here we
develop a novel lattice coding scheme for the Two-way Two-relay Channel: 1 &lt;-&gt;
2 &lt;-&gt; 3 &lt;-&gt; 4, where Node 1 and 4 simultaneously communicate with each other
through two relay nodes 2 and 3. Each node only communicates with its
neighboring nodes. The key technical contribution is the lattice-based
achievability strategy, where each relay is able to remove the noise while
decoding the sum of several signals in a Block Markov strategy and then
re-encode the signal into another lattice codeword using the so-called
&quot;Re-distribution Transform&quot;. This allows nodes further down the line to again
decode sums of lattice codewords. This transform is central to improving the
achievable rates, and ensures that the messages traveling in each of the two
directions fully utilize the relay's power, even under asymmetric channel
conditions. All decoders are lattice decoders and only a single nested lattice
codebook pair is needed. The symmetric rate achieved by the proposed lattice
coding scheme is within 0.5 log 3 bit/Hz/s of the symmetric rate capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1216</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1216</id><created>2012-12-05</created><authors><author><keyname>Brown</keyname><forenames>D. Richard</forenames><suffix>III</suffix></author><author><keyname>Klein</keyname><forenames>Andrew G.</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author></authors><title>Non-Hierarchical Clock Synchronization for Wireless Sensor Networks</title><categories>nlin.AO cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time synchronization is important for a variety of applications in wireless
sensor networks including scheduling communication resources, coordinating
sensor wake/sleep cycles, and aligning signals for distributed
transmission/reception. This paper describes a non-hierarchical approach to
time synchronization in wireless sensor networks that has low overhead and can
be implemented at the physical and/or MAC layers. Unlike most of the prior
approaches, the approach described in this paper allows all nodes to use
exactly the same distributed algorithm and does not require local averaging of
measurements from other nodes. Analytical results show that the
non-hierarchical approach can provide monotonic expected convergence of both
drifts and offsets under broad conditions on the network topology and local
clock update stepsize. Numerical results are also presented verifying the
analysis under two particular network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1223</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1223</id><created>2012-12-05</created><authors><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Shende</keyname><forenames>Nirmal</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author><author><keyname>Ayyagari</keyname><forenames>Arun</forenames></author></authors><title>Throughput Analysis of Primary and Secondary Networks in a Shared IEEE
  802.11 System</title><categories>cs.NI cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><journal-ref>IEEE Transactions on Wireless Communications, ISSN 1536-1276, Vol.
  12, No. 3, pp. 1006-1017, March 2013</journal-ref><doi>10.1109/TWC.2013.012413.112118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the coexistence of a primary and a secondary
(cognitive) network when both networks use the IEEE 802.11 based distributed
coordination function for medium access control. Specifically, we consider the
problem of channel capture by a secondary network that uses spectrum sensing to
determine the availability of the channel, and its impact on the primary
throughput. We integrate the notion of transmission slots in Bianchi's Markov
model with the physical time slots, to derive the transmission probability of
the secondary network as a function of its scan duration. This is used to
obtain analytical expressions for the throughput achievable by the primary and
secondary networks. Our analysis considers both saturated and unsaturated
networks. By performing a numerical search, the secondary network parameters
are selected to maximize its throughput for a given level of protection of the
primary network throughput. The theoretical expressions are validated using
extensive simulations carried out in the Network Simulator 2. Our results
provide critical insights into the performance and robustness of different
schemes for medium access by the secondary network. In particular, we find that
the channel captures by the secondary network does not significantly impact the
primary throughput, and that simply increasing the secondary contention window
size is only marginally inferior to silent-period based methods in terms of its
throughput performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1224</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1224</id><created>2012-12-05</created><authors><author><keyname>Podolsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Random load fluctuations and collapse probability of a power system
  operating near codimension 1 saddle-node bifurcation</title><categories>physics.soc-ph cs.SY stat.AP</categories><comments>5 pages, 1 figure, submission to IEEE PES General Meeting 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a power system operating in the vicinity of the power transfer limit of
its transmission system, effect of stochastic fluctuations of power loads can
become critical as a sufficiently strong such fluctuation may activate voltage
instability and lead to a large scale collapse of the system. Considering the
effect of these stochastic fluctuations near a codimension 1 saddle-node
bifurcation, we explicitly calculate the autocorrelation function of the state
vector and show how its behavior explains the phenomenon of critical
slowing-down often observed for power systems on the threshold of blackout. We
also estimate the collapse probability/mean clearing time for the power system
and construct a new indicator function signaling the proximity to a large scale
collapse. The new indicator function is easy to estimate in real time using PMU
data feeds as well as SCADA information about fluctuations of power load on the
nodes of the power grid. We discuss control strategies leading to the
minimization of the collapse probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1245</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1245</id><created>2012-12-06</created><updated>2013-09-11</updated><authors><author><keyname>Jiang</keyname><forenames>Chunxiao</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>K. J. Ray</forenames></author></authors><title>Distributed Adaptive Networks: A Graphical Evolutionary Game-Theoretic
  View</title><categories>cs.GT cs.LG</categories><comments>Accepted by IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2280444</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed adaptive filtering has been considered as an effective approach
for data processing and estimation over distributed networks. Most existing
distributed adaptive filtering algorithms focus on designing different
information diffusion rules, regardless of the nature evolutionary
characteristic of a distributed network. In this paper, we study the adaptive
network from the game theoretic perspective and formulate the distributed
adaptive filtering problem as a graphical evolutionary game. With the proposed
formulation, the nodes in the network are regarded as players and the local
combiner of estimation information from different neighbors is regarded as
different strategies selection. We show that this graphical evolutionary game
framework is very general and can unify the existing adaptive network
algorithms. Based on this framework, as examples, we further propose two
error-aware adaptive filtering algorithms. Moreover, we use graphical
evolutionary game theory to analyze the information diffusion process over the
adaptive networks and evolutionarily stable strategy of the system. Finally,
simulation results are shown to verify the effectiveness of our analysis and
proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1251</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1251</id><created>2012-12-06</created><updated>2015-07-23</updated><authors><author><keyname>Hahn</keyname><forenames>Ernst Moritz</forenames></author><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author><author><keyname>Wimmer</keyname><forenames>Ralf</forenames></author><author><keyname>Becker</keyname><forenames>Bernd</forenames></author></authors><title>Transient Reward Approximation for Continuous-Time Markov Chains</title><categories>cs.LO cs.NA</categories><comments>Accepted for publication in IEEE Transactions on Reliability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the analysis of very large continuous-time Markov chains
(CTMCs) with many distinct rates. Such models arise naturally in the context of
reliability analysis, e.g., of computer network performability analysis, of
power grids, of computer virus vulnerability, and in the study of crowd
dynamics. We use abstraction techniques together with novel algorithms for the
computation of bounds on the expected final and accumulated rewards in
continuous-time Markov decision processes (CTMDPs). These ingredients are
combined in a partly symbolic and partly explicit (symblicit) analysis
approach. In particular, we circumvent the use of multi-terminal decision
diagrams, because the latter do not work well if facing a large number of
different rates. We demonstrate the practical applicability and efficiency of
the approach on two case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1269</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1269</id><created>2012-12-06</created><authors><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Kunz</keyname><forenames>Konstantin</forenames></author><author><keyname>Kariotoglou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kamgarpour</keyname><forenames>Maryam</forenames></author><author><keyname>Summers</keyname><forenames>Sean</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Approximate Dynamic Programming via Sum of Squares Programming</title><categories>math.OC cs.SY</categories><comments>7 pages, 5 figures. Submitted to the 2013 European Control
  Conference, Zurich, Switzerland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an approximate dynamic programming method for stochastic control
problems on infinite state and input spaces. The optimal value function is
approximated by a linear combination of basis functions with coefficients as
decision variables. By relaxing the Bellman equation to an inequality, one
obtains a linear program in the basis coefficients with an infinite set of
constraints. We show that a recently introduced method, which obtains convex
quadratic value function approximations, can be extended to higher order
polynomial approximations via sum of squares programming techniques. An
approximate value function can then be computed offline by solving a
semidefinite program, without having to sample the infinite constraint. The
policy is evaluated online by solving a polynomial optimization problem, which
also turns out to be convex in some cases. We experimentally validate the
method on an autonomous helicopter testbed using a 10-dimensional helicopter
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1283</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1283</id><created>2012-12-06</created><updated>2013-11-29</updated><authors><author><keyname>Khalid</keyname><forenames>Zubair</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Guo</keyname><forenames>Jing</forenames></author></authors><title>A Tractable Framework for Exact Probability of Node Isolation and
  Minimum Node Degree Distribution in Finite Multi-hop Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>30 pages, accepted to appear in IEEE Transactions on Vehicular
  Technology</comments><journal-ref>IEEE Transactions on Vehicular Technology, vol. 63, no. 6, pp.
  2836-2847, July 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tractable analytical framework for the exact
calculation of probability of node isolation and minimum node degree
distribution when $N$ sensor nodes are independently and uniformly distributed
inside a finite square region. The proposed framework can accurately account
for the boundary effects by partitioning the square into subregions, based on
the transmission range and the node location. We show that for each subregion,
the probability that a random node falls inside a disk centered at an arbitrary
node located in that subregion can be expressed analytically in closed-form.
Using the results for the different subregions, we obtain the exact probability
of node isolation and minimum node degree distribution that serves as an upper
bound for the probability of $k$-connectivity. Our theoretical framework is
validated by comparison with the simulation results and shows that the minimum
node degree distribution serves as a tight upper bound for the probability of
$k$-connectivity. The proposed framework provides a very useful tool to
accurately account for the boundary effects in the design of finite wireless
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1284</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1284</id><created>2012-12-06</created><authors><author><keyname>Hulkury</keyname><forenames>M. N.</forenames></author><author><keyname>Doomun</keyname><forenames>M. R.</forenames></author></authors><title>Integrated Green Cloud Computing Architecture</title><categories>cs.DC</categories><comments>6 pages, International Conference on Advanced Computer Science
  Applications and Technologies, ACSAT 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Arbitrary usage of cloud computing, either private or public, can lead to
uneconomical energy consumption in data processing, storage and communication.
Hence, green cloud computing solutions aim not only to save energy but also
reduce operational costs and carbon footprints on the environment. In this
paper, an Integrated Green Cloud Architecture (IGCA) is proposed that comprises
of a client-oriented Green Cloud Middleware to assist managers in better
overseeing and configuring their overall access to cloud services in the
greenest or most energy-efficient way. Decision making, whether to use local
machine processing, private or public clouds, is smartly handled by the
middleware using predefined system specifications such as service level
agreement (SLA), Quality of service (QoS), equipment specifications and job
description provided by IT department. Analytical model is used to show the
feasibility to achieve efficient energy consumption while choosing between
local, private and public Cloud service provider (CSP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1296</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1296</id><created>2012-12-06</created><authors><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Distributed Model Predictive Consensus via the Alternating Direction
  Method of Multipliers</title><categories>math.OC cs.SY</categories><comments>7 pages, 5 figures, 50th Allerton Conference on Communication,
  Control, and Computing, Monticello, IL, USA, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed optimization method for solving a distributed model
predictive consensus problem. The goal is to design a distributed controller
for a network of dynamical systems to optimize a coupled objective function
while respecting state and input constraints. The distributed optimization
method is an augmented Lagrangian method called the Alternating Direction
Method of Multipliers (ADMM), which was introduced in the 1970s but has seen a
recent resurgence in the context of dramatic increases in computing power and
the development of widely available distributed computing platforms. The method
is applied to position and velocity consensus in a network of double
integrators. We find that a few tens of ADMM iterations yield closed-loop
performance near what is achieved by solving the optimization problem
centrally. Furthermore, the use of recent code generation techniques for
solving local subproblems yields fast overall computation times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1298</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1298</id><created>2012-12-06</created><updated>2012-12-08</updated><authors><author><keyname>Thomas</keyname><forenames>Eldho K.</forenames></author><author><keyname>Markin</keyname><forenames>Nadya</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>On Abelian Group Representability of Finite Groups</title><categories>math.GR cs.IT math.IT</categories><comments>14 pages</comments><msc-class>20D15, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of quasi-uniform random variables $X_1,...,X_n$ may be generated from a
finite group $G$ and $n$ of its subgroups, with the corresponding entropic
vector depending on the subgroup structure of $G$. It is known that the set of
entropic vectors obtained by considering arbitrary finite groups is much richer
than the one provided just by abelian groups. In this paper, we start to
investigate in more detail different families of non-abelian groups with
respect to the entropic vectors they yield. In particular, we address the
question of whether a given non-abelian group $G$ and some fixed subgroups
$G_1,...,G_n$ end up giving the same entropic vector as some abelian group $A$
with subgroups $A_1,...,A_n$, in which case we say that $(A, A_1,..., A_n)$
represents $(G, G_1, ..., G_n)$. If for any choice of subgroups $G_1,...,G_n$,
there exists some abelian group $A$ which represents $G$, we refer to $G$ as
being abelian (group) representable for $n$. We completely characterize
dihedral, quasi-dihedral and dicyclic groups with respect to their abelian
representability, as well as the case when $n=2$, for which we show a group is
abelian representable if and only if it is nilpotent. This problem is motivated
by understanding non-linear coding strategies for network coding, and network
information theory capacity regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1313</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1313</id><created>2012-12-06</created><authors><author><keyname>Banerji</keyname><forenames>Debajyoti</forenames></author><author><keyname>Ray</keyname><forenames>Ranjit</forenames></author><author><keyname>Basu</keyname><forenames>Jhankar</forenames></author><author><keyname>Basak</keyname><forenames>Indrajit</forenames></author></authors><title>Autonomous Navigation by Robust Scan Matching Technique</title><categories>cs.CV cs.AI</categories><comments>7 pages, 9 figures</comments><journal-ref>INTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND CREATIVE
  ENGINEERING (ISSN:2045-8711), VOL.2 NO.10 OCTOBER 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For effective autonomous navigation,estimation of the pose of the robot is
essential at every sampling time. For computing an accurate
estimation,odometric error needs to be reduced with the help of data from
external sensor. In this work, a technique has been developed for accurate pose
estimation of mobile robot by using Laser Range data. The technique is robust
to noisy data, which may contain considerable amount of outliers. A grey image
is formed from laser range data and the key points from this image are
extracted by Harris corner detector. The matching of the key points from
consecutive data sets have been done while outliers have been rejected by
RANSAC method. Robot state is measured by the correspondence between the two
sets of keypoints. Finally, optimal robot state is estimated by Extended Kalman
Filter. The technique has been applied to an operational robot in the
laboratory environment to show the robustness of the technique in presence of
noisy sensor data. The performance of this new technique has been compared with
that of conventional ICP method. Through this method, effective and accurate
navigation has been achieved even in presence of substantial noise in the
sensor data at the cost of a small amount of additional computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1328</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1328</id><created>2012-12-06</created><updated>2013-03-31</updated><authors><author><keyname>Fujita</keyname><forenames>Hiroshi</forenames></author></authors><title>A New Lower Bound for the Ramsey Number R(4, 8)</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lower bound for the classical Ramsey number R(4, 8) is improved from 56
to 58. The author has found a new edge coloring of K_{57} that has no complete
graphs of order 4 in the first color, and no complete graphs of order 8 in the
second color. The coloring was found using a SAT solver which is based on
MiniSat and customized for solving Ramsey problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1329</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1329</id><created>2012-12-06</created><authors><author><keyname>Asha</keyname><forenames>V.</forenames></author><author><keyname>Bhajantri</keyname><forenames>N. U.</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author></authors><title>Automatic Detection of Texture Defects Using Texture-Periodicity and
  Gabor Wavelets</title><categories>cs.CV</categories><comments>06 Pages, 04 Figures, ICIP 2011</comments><msc-class>65D19, 65T60, 11K70, 62H30</msc-class><acm-class>I.0; I.2.10</acm-class><journal-ref>CCIS 157, Computer Networks and Intelligent Computing, Part 9, pp.
  548-553, Springer-Verlag, Berlin Heidelberg, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a machine vision algorithm for automatically
detecting defects in textures belonging to 16 out of 17 wallpaper groups using
texture-periodicity and a family of Gabor wavelets. Input defective images are
subjected to Gabor wavelet transformation in multi-scales and
multi-orientations and a resultant image is obtained in L2 norm. The resultant
image is split into several periodic blocks and energy of each block is used as
a feature space to automatically identify defective and defect-free blocks
using Ward's hierarchical clustering. Experiments on defective fabric images of
three major wallpaper groups, namely, pmm, p2 and p4m, show that the proposed
method is robust in finding fabric defects without human intervention and can
be used for automatic defect detection in fabric industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1340</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1340</id><created>2012-12-06</created><updated>2013-01-16</updated><authors><author><keyname>Rajashekar</keyname><forenames>Rakshith</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Spatial Modulation in Zero-Padded Single Carrier Communication</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Spatial Modulation (SM) system in a frequency
selective channel under single carrier (SC) communication scenario and propose
zero-padding instead of cyclic prefix considered in the existing literature. We
show that the zero-padded single carrier (ZP-SC) SM system offers full
multipath diversity under maximum-likelihood (ML) detection, unlike the cyclic
prefixed SM system. Further, we show that the order of ML decoding complexity
in the proposed ZP-SC SM system is independent of the frame length and depends
only on the number of multipath links between the transmitter and the receiver.
Thus, we show that the zero-padding in the SC SM system has two fold advantage
over cyclic prefixing: 1) gives full multipath diversity, and 2) offers
relatively low ML decoding complexity. Furthermore, we extend the partial
interference cancellation receiver (PIC-R) proposed by Guo and Xia for the
decoding of STBCs in order to convert the ZP-SC system into a set of
flat-fading subsystems. We show that the transmission of any full rank STBC
over these subsystems achieves full transmit, receive as well as multipath
diversity under PIC-R. With the aid of this extended PIC-R, we show that the
ZP-SC SM system achieves receive and multipath diversity with a decoding
complexity same as that of the SM system in flat-fading scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1344</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1344</id><created>2012-12-06</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Holley</keyname><forenames>Julian</forenames></author><author><keyname>Costello</keyname><forenames>Ben De Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Toward Turing's A-type Unorganised Machines in an Unconventional
  Substrate: a Dynamic Representation in Compartmentalised Excitable Chemical
  Media</title><categories>cs.ET</categories><comments>In: Dodig-Crnkovic and Giovagnoli (Eds) Computing Nature. Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turing presented a general representation scheme by which to achieve
artificial intelligence - unorganised machines. Significantly, these were a
form of discrete dynamical system and yet such representations remain
relatively unexplored. Further, at the same time as also suggesting that
natural evolution may provide inspiration for search mechanisms to design
machines, he noted that mechanisms inspired by the social aspects of learning
may prove useful. This paper presents initial results from consideration of
using Turing's dynamical representation within an unconventional substrate -
networks of Belousov-Zhabotinsky vesicles - designed by an imitation-based,
i.e., cultural, approach. Turing's representation scheme is also extended to
include a fuller set of Boolean functions at the nodes of the recurrent
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1346</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1346</id><created>2012-12-06</created><updated>2012-12-11</updated><authors><author><keyname>Lavado</keyname><forenames>Giovanna J.</forenames></author><author><keyname>Pighizzini</keyname><forenames>Giovanni</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Converting Nondeterministic Automata and Context-Free Grammars into
  Parikh Equivalent One-Way and Two-Way Deterministic Automata</title><categories>cs.FL</categories><comments>30 pages, 2 figure. A preliminary version has been presented at DLT
  2012, LNCS 7410, pp. 284-295. Version 2: an example has been added in Section
  3</comments><acm-class>F.1.1; F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the conversion of one-way nondeterministic finite automata and
context-free grammars into Parikh equivalent one-way and two-way deterministic
finite automata, from a descriptional complexity point of view.
  We prove that for each one-way nondeterministic automaton with $n$ states
there exist Parikh equivalent one-way and two-way deterministic automata with
$e^{O(\sqrt{n \ln n})}$ and $p(n)$ states, respectively, where $p(n)$ is a
polynomial. Furthermore, these costs are tight. In contrast, if all the words
accepted by the given automaton contain at least two different letters, then a
Parikh equivalent one-way deterministic automaton with a polynomial number of
states can be found.
  Concerning context-free grammars, we prove that for each grammar in Chomsky
normal form with h variables there exist Parikh equivalent one-way and two-way
deterministic automata with $2^{O(h^2)}$ and $2^{O(h)}$ states, respectively.
Even these bounds are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1360</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1360</id><created>2012-12-06</created><authors><author><keyname>D&#x142;otko</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Specogna</keyname><forenames>Ruben</forenames></author></authors><title>Physics inspired algorithms for (co)homology computation</title><categories>cs.CE math.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of computing (co)homology generators of a cell complex is gaining a
pivotal role in various branches of science. While this issue can be rigorously
solved in polynomial time, it is still overly demanding for large scale
problems. Drawing inspiration from low-frequency electrodynamics, this paper
presents a physics inspired algorithm for first cohomology group computations
on three-dimensional complexes. The algorithm is general and exhibits orders of
magnitude speed up with respect to competing ones, allowing to handle problems
not addressable before. In particular, when generators are employed in the
physical modeling of magneto-quasistatic problems, this algorithm solves one of
the most long-lasting problems in low-frequency computational electromagnetics.
In this case, the effectiveness of the algorithm and its ease of implementation
may be even improved by introducing the novel concept of \textit{lazy
cohomology generators}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1362</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1362</id><created>2012-12-06</created><updated>2013-04-04</updated><authors><author><keyname>Gerlach</keyname><forenames>Martin</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>Stochastic model for the vocabulary growth in natural languages</title><categories>physics.soc-ph cs.CL physics.data-an</categories><comments>corrected typos and errors in reference list; 10 pages text, 15 pages
  supplemental material; to appear in Physical Review X</comments><journal-ref>Phys. Rev. X 3, 021006 (2013)</journal-ref><doi>10.1103/PhysRevX.3.021006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stochastic model for the number of different words in a given
database which incorporates the dependence on the database size and historical
changes. The main feature of our model is the existence of two different
classes of words: (i) a finite number of core-words which have higher frequency
and do not affect the probability of a new word to be used; and (ii) the
remaining virtually infinite number of noncore-words which have lower frequency
and once used reduce the probability of a new word to be used in the future.
Our model relies on a careful analysis of the google-ngram database of books
published in the last centuries and its main consequence is the generalization
of Zipf's and Heaps' law to two scaling regimes. We confirm that these
generalizations yield the best simple description of the data among generic
descriptive models and that the two free parameters depend only on the language
but not on the database. From the point of view of our model the main change on
historical time scales is the composition of the specific words included in the
finite list of core-words, which we observe to decay exponentially in time with
a rate of approximately 30 words per year for English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1368</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1368</id><created>2012-12-06</created><updated>2014-02-04</updated><authors><author><keyname>Ram&#xed;rez</keyname><forenames>Jos&#xe9; L.</forenames></author><author><keyname>Rubiano</keyname><forenames>Gustavo N.</forenames></author><author><keyname>de Castro</keyname><forenames>Rodrigo</forenames></author></authors><title>A Generalization of the Fibonacci Word Fractal and the Fibonacci
  Snowflake</title><categories>cs.DM math.CO</categories><msc-class>05B50, 11B39, 28A80, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a family of infinite words that generalize the
Fibonacci word and we study their combinatorial properties. Moreover, we
associate to this family of words a family of curves, which have fractal
properties, in particular these curves have as attractor the Fibonacci word
fractal. Finally, we describe an infinite family of polyominoes (double
squares) from the generalized Fibonacci words and we study some of their
geometric properties. These last polyominoes generalize the Fibonacci
snowflake.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1406</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1406</id><created>2012-12-05</created><authors><author><keyname>Latorre</keyname><forenames>Fabian</forenames></author></authors><title>The Maxflow problem and a generalization to simplicial complexes</title><categories>math.CO cs.DM math.OC</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The problem of Maxflow is a widely developed subject in modern mathematics.
Efficient algorithms exist to solve this problem, that is why a good
generalization may permit these algorithms to be understood as a particular
instance of solutions in a wider class of problems. In the last section we
suggest a generalization in the context of simplicial complexes, that reduces
to the problem of Maxflow in graphs, when we consider a graph as a simplicial
complex of dimension 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1441</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1441</id><created>2012-12-06</created><updated>2014-01-06</updated><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author></authors><title>A new approach to crushing 3-manifold triangulations</title><categories>math.GT cs.CG</categories><comments>23 pages, 14 figures; v2: many revisions, plus a new section on
  minimal triangulations; v3: minor revisions. This is the full journal version
  of a paper from SCG 2013, and will appear in Discrete &amp; Computational
  Geometry</comments><msc-class>Primary 57N10, Secondary 57Q15, 68W05</msc-class><journal-ref>Discrete and Computational Geometry 52 (2014), no. 1, 116-139</journal-ref><doi>10.1007/s00454-014-9572-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crushing operation of Jaco and Rubinstein is a powerful technique in
algorithmic 3-manifold topology: it enabled the first practical implementations
of 3-sphere recognition and prime decomposition of orientable manifolds, and it
plays a prominent role in state-of-the-art algorithms for unknot recognition
and testing for essential surfaces. Although the crushing operation will always
reduce the size of a triangulation, it might alter its topology, and so it
requires a careful theoretical analysis for the settings in which it is used.
  The aim of this short paper is to make the crushing operation more accessible
to practitioners, and easier to generalise to new settings. When the crushing
operation was first introduced, the analysis was powerful but extremely
complex. Here we give a new treatment that reduces the crushing process to a
sequential combination of three &quot;atomic&quot; operations on a cell decomposition,
all of which are simple to analyse. As an application, we generalise the
crushing operation to the setting of non-orientable 3-manifolds, where we
obtain a new practical and robust algorithm for non-orientable prime
decomposition. We also apply our crushing techniques to the study of
non-orientable minimal triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1449</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1449</id><created>2012-12-06</created><updated>2013-01-02</updated><authors><author><keyname>Laciana</keyname><forenames>Carlos E.</forenames></author><author><keyname>Rovere</keyname><forenames>Santiago L.</forenames></author><author><keyname>Podest&#xe1;</keyname><forenames>Guillermo P.</forenames></author></authors><title>Exploring associations between micro-level models of innovation
  diffusion and emerging macro-level adoption patterns</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, 4 figures and a table of supplementary data. Accepted for
  publication</comments><doi>10.1016/j.physa.2012.12.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A micro-level agent-based model of innovation diffusion was developed that
explicitly combines (a) an individual's perception of the advantages or
relative utility derived from adoption, and (b) social influence from members
of the individual's social network. The micro-model was used to simulate
macro-level diffusion patterns emerging from different configurations of
micro-model parameters. Micro-level simulation results matched very closely the
adoption patterns predicted by the widely-used Bass macro-level model (Bass,
1969). For a portion of the domain, results from micro-simulations were
consistent with aggregate-level adoption patterns reported in the literature.
Induced Bass macro-level parameters and responded to changes in
micro-parameters: (1) increased with the number of innovators and with the rate
at which innovators are introduced; (2) increased with the probability of
rewiring in small-world networks, as the characteristic path length decreases;
and (3) an increase in the overall perceived utility of an innovation caused a
corresponding increase in induced and values. Understanding micro to macro
linkages can inform the design and assessment of marketing interventions on
micro-variables - or processes related to them - to enhance adoption of future
products or technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1464</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1464</id><created>2012-12-06</created><authors><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Structure and Dynamics of Information Pathways in Online Media</title><categories>cs.SI cs.DS cs.IR physics.soc-ph</categories><comments>To Appear at the 6th International Conference on Web Search and Data
  Mining (WSDM '13)</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion of information, spread of rumors and infectious diseases are all
instances of stochastic processes that occur over the edges of an underlying
network. Many times networks over which contagions spread are unobserved, and
such networks are often dynamic and change over time. In this paper, we
investigate the problem of inferring dynamic networks based on information
diffusion data. We assume there is an unobserved dynamic network that changes
over time, while we observe the results of a dynamic process spreading over the
edges of the network. The task then is to infer the edges and the dynamics of
the underlying network.
  We develop an on-line algorithm that relies on stochastic convex optimization
to efficiently solve the dynamic network inference problem. We apply our
algorithm to information diffusion among 3.3 million mainstream media and blog
sites and experiment with more than 179 million different pieces of information
spreading over the network in a one year period. We study the evolution of
information pathways in the online media space and find interesting insights.
Information pathways for general recurrent topics are more stable across time
than for on-going news events. Clusters of news media sites and blogs often
emerge and vanish in matter of days for on-going news events. Major social
movements and events involving civil population, such as the Libyan's civil war
or Syria's uprise, lead to an increased amount of information pathways among
blogs as well as in the overall increase in the network centrality of blogs and
social media sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1469</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1469</id><created>2012-12-06</created><authors><author><keyname>Moreau</keyname><forenames>Marc</forenames></author><author><keyname>Osborn</keyname><forenames>Wendy</forenames></author></authors><title>mqr-tree: A 2-dimensional Spatial Access Method</title><categories>cs.DB</categories><comments>Link to article:
  http://www.scribd.com/doc/113916780/mqr-tree-A-2-dimensional-Spatial-Access-Method;
  Journal of Computer Science and Engineering 15(2), October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the mqr-tree, a two-dimensional spatial access
method that organizes spatial objects in a two-dimensional node and based on
their spatial relationships. Previously proposed spatial access methods that
attempt to maintain spatial relationships between objects in their structures
are limited in their incorporation of existing one-dimensional spatial access
methods, or have lower space utilization in its nodes, and higher tree height,
overcoverage and overlap than is necessary. The mqr-tree utilizes a node
organization, set of spatial relationship rules and insertion strategy in order
to gain significant improvements in overlap and overcoverage. In addition,
other desirable properties are identified as a result of the chosen node
organization and insertion strategies. In particular, zero overlap is achieved
when the mqr-tree is used to index point data. A comparison of the mqr-tree
insertion strategy versus the R-tree shows significant improvements in overlap
and overcoverage, with comparable space utilization. In addition, a comparison
of region searching shows that the mqr-tree achieves a lower number of disk
accesses in many cases
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1471</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1471</id><created>2012-12-06</created><authors><author><keyname>Farnoud</keyname><forenames>Farzad</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author><author><keyname>Touri</keyname><forenames>Behrouz</forenames></author></authors><title>A Novel Distance-Based Approach to Constrained Rank Aggregation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a classical problem in choice theory -- vote aggregation -- using
novel distance measures between permutations that arise in several practical
applications. The distance measures are derived through an axiomatic approach,
taking into account various issues arising in voting with side constraints. The
side constraints of interest include non-uniform relevance of the top and the
bottom of rankings (or equivalently, eliminating negative outliers in votes)
and similarities between candidates (or equivalently, introducing diversity in
the voting process). The proposed distance functions may be seen as weighted
versions of the Kendall $\tau$ distance and weighted versions of the Cayley
distance. In addition to proposing the distance measures and providing the
theoretical underpinnings for their applications, we also consider algorithmic
aspects associated with distance-based aggregation processes. We focus on two
methods. One method is based on approximating weighted distance measures by a
generalized version of Spearman's footrule distance, and it has provable
constant approximation guarantees. The second class of algorithms is based on a
non-uniform Markov chain method inspired by PageRank, for which currently only
heuristic guarantees are known. We illustrate the performance of the proposed
algorithms for a number of distance measures for which the optimal solution may
be easily computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1478</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1478</id><created>2012-12-06</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>The Clustering of Author's Texts of English Fiction in the Vector Space
  of Semantic Fields</title><categories>cs.CL cs.DL cs.IR</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clustering of text documents in the vector space of semantic fields and
in the semantic space with orthogonal basis has been analysed. It is shown that
using the vector space model with the basis of semantic fields is effective in
the cluster analysis algorithms of author's texts in English fiction. The
analysis of the author's texts distribution in cluster structure showed the
presence of the areas of semantic space that represent the author's ideolects
of individual authors. SVD factorization of the semantic fields matrix makes it
possible to reduce significantly the dimension of the semantic space in the
cluster analysis of author's texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1485</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1485</id><created>2012-12-06</created><authors><author><keyname>Bansal</keyname><forenames>Kshitij</forenames><affiliation>New York University, USA</affiliation></author><author><keyname>Demri</keyname><forenames>St&#xe9;phane</forenames><affiliation>New York University, USA</affiliation><affiliation>LSV, CNRS, France</affiliation></author></authors><title>A Note on the Complexity of Model-Checking Bounded Multi-Pushdown
  Systems</title><categories>cs.LO cs.FL</categories><report-no>NYU TR2012-949</report-no><acm-class>F.4.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we provide complexity characterizations of model checking
multi-pushdown systems. Multi-pushdown systems model recursive concurrent
programs in which any sequential process has a finite control. We consider
three standard notions for boundedness: context boundedness, phase boundedness
and stack ordering. The logical formalism is a linear-time temporal logic
extending well-known logic CaRet but dedicated to multi-pushdown systems in
which abstract operators (related to calls and returns) such as those for
next-time and until are parameterized by stacks. We show that the problem is
EXPTIME-complete for context-bounded runs and unary encoding of the number of
context switches; we also prove that the problem is 2EXPTIME-complete for
phase-bounded runs and unary encoding of the number of phase switches. In both
cases, the value k is given as an input (whence it is not a constant of the
model-checking problem), which makes a substantial difference in the
complexity. In certain cases, our results improve previous complexity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1496</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1496</id><created>2012-12-06</created><updated>2013-01-14</updated><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author></authors><title>Excess risk bounds for multitask learning with trace norm regularization</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trace norm regularization is a popular method of multitask learning. We give
excess risk bounds with explicit dependence on the number of tasks, the number
of examples per task and properties of the data distribution. The bounds are
independent of the dimension of the input space, which may be infinite as in
the case of reproducing kernel Hilbert spaces. A byproduct of the proof are
bounds on the expected norm of sums of random positive semidefinite matrices
with subexponential moments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1514</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1514</id><created>2012-12-06</created><updated>2014-05-23</updated><authors><author><keyname>Abunadi</keyname><forenames>Ibrahim</forenames></author><author><keyname>Sanzogni</keyname><forenames>Louis</forenames></author><author><keyname>Sandhu</keyname><forenames>Kuldeep</forenames></author><author><keyname>Woods</keyname><forenames>Peter</forenames></author></authors><title>Success Factors Contributing to eGovernment Adoption in Saudi Arabia:
  G2C approach</title><categories>cs.CY</categories><comments>8 pages, 17 figures, Conference Paper; Saudi International Innovation
  Conference 2008 Proceeding</comments><msc-class>97P70, 68U35</msc-class><acm-class>H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saudi Arabia is predetermined to implement eGovernment and provide
world-class government services to citizens by 2010. However, this initiative
will be meaningless if the people did not adopt these electronic services.
Therefore, the purpose of this study is to determine success factors that will
facilitate the adoption of eGovernment in Saudi Arabia. The results of the
literature review have been deployed into surveys with Saudi eGovernment users.
The discussion of the analysis from results obtained from the practical study
has provided a framework that encompasses the eGovernment adoption success
factors for Saudi Arabia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1521</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1521</id><created>2012-12-06</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>Bounds on mean cycle time in acyclic fork-join queueing networks</title><categories>math.OC cs.SY</categories><comments>The 4th International Workshop on Discrete Event Systems (WODES'98),
  University of Cagliary, Cagliari, Sardinia, Italy, August 26-28, 1998</comments><msc-class>68M20 (Primary) 15A80, 93C65, 90B15, 68U20 (Secondary)</msc-class><journal-ref>Proc. 4th Intern. Workshop on Discrete Event Systems (WODES'98),
  IEE, London, 1998, pp. 469-474</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple lower and upper bounds on mean cycle time in stochastic acyclic
fork-join networks are derived using the $(\max,+)$-algebra approach. The
behaviour of the bounds under various assumptions concerning the service times
in the networks is discussed, and related numerical examples are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1522</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1522</id><created>2012-12-06</created><updated>2014-02-24</updated><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Gkatzelis</keyname><forenames>Vasilis</forenames></author><author><keyname>Goel</keyname><forenames>Gagan</forenames></author></authors><title>Mechanism Design for Fair Division</title><categories>cs.GT cs.DS cs.MA</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.4627</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classic problem of fair division from a mechanism design
perspective, using {\em Proportional Fairness} as a benchmark. In particular,
we aim to allocate a collection of divisible items to a set of agents while
incentivizing the agents to be truthful in reporting their valuations. For the
very large class of homogeneous valuations, we design a truthful mechanism that
provides {\em every agent} with at least a $1/e\approx 0.368$ fraction of her
Proportionally Fair valuation. To complement this result, we show that no
truthful mechanism can guarantee more than a $0.5$ fraction, even for the
restricted class of additive linear valuations. We also propose another
mechanism for additive linear valuations that works really well when every item
is highly demanded. To guarantee truthfulness, our mechanisms discard a
carefully chosen fraction of the allocated resources; we conclude by uncovering
interesting connections between our mechanisms and known mechanisms that use
money instead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1524</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1524</id><created>2012-12-06</created><updated>2013-02-16</updated><authors><author><keyname>Arnold</keyname><forenames>Ludovic</forenames></author><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author></authors><title>Layer-wise learning of deep generative models</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When using deep, multi-layered architectures to build generative models of
data, it is difficult to train all layers at once. We propose a layer-wise
training procedure admitting a performance guarantee compared to the global
optimum. It is based on an optimistic proxy of future performance, the best
latent marginal. We interpret auto-encoders in this setting as generative
models, by showing that they train a lower bound of this criterion. We test the
new learning procedure against a state of the art method (stacked RBMs), and
find it to improve performance. Both theory and experiments highlight the
importance, when training deep architectures, of using an inference model (from
data to hidden variables) richer than the generative model (from hidden
variables to data).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1527</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1527</id><created>2012-12-06</created><updated>2013-09-18</updated><authors><author><keyname>Rabani</keyname><forenames>Yuval</forenames></author><author><keyname>Schulman</keyname><forenames>Leonard</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Learning Mixtures of Arbitrary Distributions over Large Discrete Domains</title><categories>cs.LG cs.DS</categories><comments>Update of previous version with improved aperture and sample-size
  lower bounds</comments><acm-class>F.2.2; G.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for learning a mixture of {\em unstructured}
distributions. This problem arises in various unsupervised learning scenarios,
for example in learning {\em topic models} from a corpus of documents spanning
several topics. We show how to learn the constituents of a mixture of $k$
arbitrary distributions over a large discrete domain $[n]=\{1,2,\dots,n\}$ and
the mixture weights, using $O(n\polylog n)$ samples. (In the topic-model
learning setting, the mixture constituents correspond to the topic
distributions.) This task is information-theoretically impossible for $k&gt;1$
under the usual sampling process from a mixture distribution. However, there
are situations (such as the above-mentioned topic model case) in which each
sample point consists of several observations from the same mixture
constituent. This number of observations, which we call the {\em &quot;sampling
aperture&quot;}, is a crucial parameter of the problem. We obtain the {\em first}
bounds for this mixture-learning problem {\em without imposing any assumptions
on the mixture constituents.} We show that efficient learning is possible
exactly at the information-theoretically least-possible aperture of $2k-1$.
Thus, we achieve near-optimal dependence on $n$ and optimal aperture. While the
sample-size required by our algorithm depends exponentially on $k$, we prove
that such a dependence is {\em unavoidable} when one considers general
mixtures. A sequence of tools contribute to the algorithm, such as
concentration results for random matrices, dimension reduction, moment
estimations, and sensitivity analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1531</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1531</id><created>2012-12-07</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Coward</keyname><forenames>Alexander</forenames></author><author><keyname>Tillmann</keyname><forenames>Stephan</forenames></author></authors><title>Computing closed essential surfaces in knot complements</title><categories>math.GT cs.CG</categories><comments>27 pages, 11 figures</comments><journal-ref>SCG '13: Proceedings of the 29th Annual Symposium on Computational
  Geometry, ACM, 2013, pp. 405-414</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new, practical algorithm to test whether a knot complement
contains a closed essential surface. This property has important theoretical
and algorithmic consequences; however, systematically testing it has until now
been infeasibly slow, and current techniques only apply to specific families of
knots. As a testament to its practicality, we run the algorithm over a
comprehensive body of 2979 knots, including the two 20-crossing dodecahedral
knots, yielding results that were not previously known.
  The algorithm derives from the original Jaco-Oertel framework, involves both
enumeration and optimisation procedures, and combines several techniques from
normal surface theory. This represents substantial progress in the practical
implementation of normal surface theory, in that we can systematically solve a
theoretically double exponential-time problem for significant inputs. Our
methods are relevant for other difficult computational problems in 3-manifold
theory, ranging from testing for Haken-ness to the recognition problem for
knots, links and 3-manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1548</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1548</id><created>2012-12-07</created><authors><author><keyname>Aristiz&#xe1;bal</keyname><forenames>Andr&#xe9;s</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author><author><keyname>Bonchi</keyname><forenames>Filippo</forenames><affiliation>LIP</affiliation></author><author><keyname>Pino</keyname><forenames>Luis</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Valencia</keyname><forenames>Frank D.</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author></authors><title>Partition Refinement for Bisimilarity in CCP</title><categories>cs.LO</categories><comments>27th ACM Symposium On Applied Computing, Trento : Italy (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saraswat's concurrent constraint programming (ccp) is a mature formalism for
modeling processes (or programs) that interact by telling and asking
constraints in a global medium, called the store. Bisimilarity is a standard
behavioural equivalence in concurrency theory, but a well-behaved notion of
bisimilarity for ccp has been proposed only recently. When the state space of a
system is finite, the ordinary notion of bisimilarity can be computed via the
well-known partition refinement algorithm, but unfortunately, this algorithm
does not work for ccp bisimilarity. In this paper, we propose a variation of
the partition refinement algorithm for verifying ccp bisimilarity. To the best
of our knowledge this is the first work providing for the automatic
verification of program equivalence for ccp
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1570</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1570</id><created>2012-12-07</created><authors><author><keyname>Maleki</keyname><forenames>Khashayar Niki</forenames></author><author><keyname>Valipour</keyname><forenames>Mohammad Hadi</forenames></author><author><keyname>Mokari</keyname><forenames>Sadegh</forenames></author><author><keyname>Ashrafi</keyname><forenames>Roohollah Yeylaghi</forenames></author><author><keyname>Jamali</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Lucas</keyname><forenames>Caro</forenames></author></authors><title>A simple method for decision making in robocup soccer simulation 3d
  environment</title><categories>cs.AI cs.RO</categories><comments>8 pages, 10 figures; Revista Avances en Sistemas e Informatica, Vol.
  5, No. 3, December 2008</comments><msc-class>68T15, 68T40, 68T37</msc-class><acm-class>I.2.9; I.2.3; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper new hierarchical hybrid fuzzy-crisp methods for decision making
and action selection of an agent in soccer simulation 3D environment are
presented. First, the skills of an agent are introduced, implemented and
classified in two layers, the basicskills and the highlevel skills. In the
second layer, a twophase mechanism for decision making is introduced. In phase
one, some useful methods are implemented which check the agent's situation for
performing required skills. In the next phase, the team str ategy, team for
mation, agent's role and the agent's positioning system are introduced. A fuzzy
logical approach is employed to recognize the team strategy and further more to
tell the player the best position to move. At last, we comprised our
implemented algor ithm in the Robocup Soccer Simulation 3D environment and
results showed th eefficiency of the introduced methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1603</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1603</id><created>2012-12-07</created><authors><author><keyname>Petersson</keyname><forenames>Daniel</forenames></author><author><keyname>L&#xf6;fberg</keyname><forenames>Johan</forenames></author></authors><title>Model Reduction using a Frequency-Limited H2-Cost</title><categories>cs.SY math.DS</categories><comments>Submitted to Systems and Control Letters</comments><msc-class>93B11, 93A30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for model reduction on a given frequency range, without
the use of input and output filter weights. The method uses a nonlinear
optimization approach to minimize a frequency limited H2 like cost function.
  An important contribution in the paper is the derivation of the gradient of
the proposed cost function. The fact that we have a closed form expression for
the gradient and that considerations have been taken to make the gradient
computationally efficient to compute enables us to efficiently use
off-the-shelf optimization software to solve the optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1609</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1609</id><created>2012-12-07</created><authors><author><keyname>Kolliopoulos</keyname><forenames>Stavros G.</forenames></author><author><keyname>Moysoglou</keyname><forenames>Yannis</forenames></author></authors><title>The 2-valued case of makespan minimization with assignment constraints</title><categories>cs.DS</categories><comments>8 pages, 1 figure, to appear in Information Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following special case of minimizing makespan. A set of jobs
$J$ and a set of machines $M$ are given. Each job $j \in J$ can be scheduled on
a machine from a subset $M_j$ of $M$. The processing time of $j$ is the same on
all machines in $M_j.$ The jobs are of two sizes, namely $b$ (big) and $s$
(small). We present a polynomial-time algorithm that approximates the value of
the optimal makespan within a factor of 1.883 and some further improvements
when every job can be scheduled on at most two machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1611</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1611</id><created>2012-12-07</created><updated>2012-12-10</updated><authors><author><keyname>Yang</keyname><forenames>Liping</forenames></author><author><keyname>Wu</keyname><forenames>Rongjun</forenames></author><author><keyname>Hong</keyname><forenames>Shaofang</forenames></author></authors><title>Nonlinearity of quartic rotation symmetric Boolean functions</title><categories>cs.IT math.CO math.IT</categories><comments>10 pages</comments><journal-ref>Southeast Asian Bull. Math. 37 (2013), 951-961</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinearity of rotation symmetric Boolean functions is an important topic on
cryptography algorithm. Let $e\ge 1$ be any given integer. In this paper, we
investigate the following question: Is the nonlinearity of the quartic rotation
symmetric Boolean function generated by the monomial $x_0x_ex_{2e}x_{3e}$ equal
to its weight? We introduce some new simple sub-functions and develop new
technique to get several recursive formulas. Then we use these recursive
formulas to show that the nonlinearity of the quartic rotation symmetric
Boolean function generated by the monomial $x_0x_ex_{2e}x_{3e}$ is the same as
its weight. So we answer the above question affirmatively. Finally, we
conjecture that if $l\ge 4$ is an integer, then the nonlinearity of the
rotation symmetric Boolean function generated by the monomial
$x_0x_ex_{2e}...x_{le}$ equals its weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1617</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1617</id><created>2012-12-07</created><updated>2013-04-23</updated><authors><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Gheibi</keyname><forenames>Amin</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Sack</keyname><forenames>J&#xf6;rg-R&#xfc;diger</forenames></author><author><keyname>Scheffer</keyname><forenames>Christian</forenames></author></authors><title>Similarity of Polygonal Curves in the Presence of Outliers</title><categories>cs.CG cs.CV cs.GR</categories><comments>Replaces the earlier version of this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fr\'{e}chet distance is a well studied and commonly used measure to
capture the similarity of polygonal curves. Unfortunately, it exhibits a high
sensitivity to the presence of outliers. Since the presence of outliers is a
frequently occurring phenomenon in practice, a robust variant of Fr\'{e}chet
distance is required which absorbs outliers. We study such a variant here. In
this modified variant, our objective is to minimize the length of subcurves of
two polygonal curves that need to be ignored (MinEx problem), or alternately,
maximize the length of subcurves that are preserved (MaxIn problem), to achieve
a given Fr\'{e}chet distance. An exact solution to one problem would imply an
exact solution to the other problem. However, we show that these problems are
not solvable by radicals over $\mathbb{Q}$ and that the degree of the
polynomial equations involved is unbounded in general. This motivates the
search for approximate solutions. We present an algorithm, which approximates,
for a given input parameter $\delta$, optimal solutions for the \MinEx\ and
\MaxIn\ problems up to an additive approximation error $\delta$ times the
length of the input curves. The resulting running time is upper bounded by
$\mathcal{O} \left(\frac{n^3}{\delta} \log \left(\frac{n}{\delta}
\right)\right)$, where $n$ is the complexity of the input polygonal curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1621</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1621</id><created>2012-12-07</created><authors><author><keyname>De Cicco</keyname><forenames>Luca</forenames></author><author><keyname>Mascolo</keyname><forenames>Saverio</forenames></author></authors><title>TCP Congestion Control over HSDPA: an Experimental Evaluation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the experimental evaluation of TCP over the High
Speed Downlink Packet Access (HSDPA), an upgrade of UMTS that is getting
worldwide deployment. Today, this is particularly important in view of the
&quot;liberalization&quot; brought in by the Linux OS which offers several variants of
TCP congestion control. In particular, we consider four TCP variants: 1) TCP
NewReno, which is the only congestion control standardized by the IETF; 2) TCP
BIC, that was, and 3) TCP Cubic that is the default algorithm in the Linux OS;
4) Westwood+ TCP that has been shown to be particularly effective over wireless
links. Main results are that all the TCP variants provide comparable goodputs
but with significant larger round trip times and number of retransmissions and
timeouts in the case of TCP BIC/Cubic, which is a consequence of their more
aggressive probing phases. On the other hand, TCP Westwood+ provides the
shortest round trip delays, which is an effect of its unique way of setting
control windows after congestion episode based on bandwidth measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1625</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1625</id><created>2012-12-07</created><authors><author><keyname>Faria</keyname><forenames>Daniel</forenames></author><author><keyname>Pesquita</keyname><forenames>Catia</forenames></author><author><keyname>Santos</keyname><forenames>Emanuel</forenames></author><author><keyname>Couto</keyname><forenames>Francisco M.</forenames></author><author><keyname>Stroe</keyname><forenames>Cosmin</forenames></author><author><keyname>Cruz</keyname><forenames>Isabel F.</forenames></author></authors><title>Testing the AgreementMaker System in the Anatomy Task of OAEI 2012</title><categories>cs.IR cs.AI</categories><comments>4 pages, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AgreementMaker system was the leading system in the anatomy task of the
Ontology Alignment Evaluation Initiative (OAEI) competition in 2011. While
AgreementMaker did not compete in OAEI 2012, here we report on its performance
in the 2012 anatomy task, using the same configurations of AgreementMaker
submitted to OAEI 2011. Additionally, we also test AgreementMaker using an
updated version of the UBERON ontology as a mediating ontology, and otherwise
identical configurations. AgreementMaker achieved an F-measure of 91.8% with
the 2011 configurations, and an F-measure of 92.2% with the updated UBERON
ontology. Thus, AgreementMaker would have been the second best system had it
competed in the anatomy task of OAEI 2012, and only 0.1% below the F-measure of
the best system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1629</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1629</id><created>2012-12-07</created><authors><author><keyname>Pucci</keyname><forenames>Daniele</forenames></author><author><keyname>Hamel</keyname><forenames>Tarek</forenames></author><author><keyname>Morin</keyname><forenames>Pascal</forenames></author><author><keyname>Samson</keyname><forenames>Claude</forenames></author></authors><title>Modeling for Control of Symmetric Aerial Vehicles Subjected to
  Aerodynamic Forces</title><categories>cs.SY</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper participates in the development of a unified approach to the
control of aerial vehicles with extended flight envelopes. More precisely,
modeling for control purposes of a class of thrust-propelled aerial vehicles
subjected to lift and drag aerodynamic forces is addressed assuming a
rotational symmetry of the vehicle's shape about the thrust force axis. A
condition upon aerodynamic characteristics that allows one to recast the
control problem into the simpler case of a spherical vehicle is pointed out.
Beside showing how to adapt nonlinear controllers developed for this latter
case, the paper extends a previous work by the authors in two directions.
First, the 3D case is addressed whereas only motions in a single vertical plane
was considered. Secondly, the family of models of aerodynamic forces for which
the aforementioned transformation holds is enlarged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1633</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1633</id><created>2012-12-07</created><authors><author><keyname>Wang</keyname><forenames>Cong</forenames></author><author><keyname>Bulatov</keyname><forenames>Andrei A.</forenames></author></authors><title>Inferring Attitude in Online Social Networks Based On Quadratic
  Correlation</title><categories>cs.SI physics.soc-ph</categories><comments>18 pages, 3 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of an online social network in most cases cannot be described
just by links between its members. We study online social networks, in which
members may have certain attitude, positive or negative toward each other, and
so the network consists of a mixture of both positive and negative
relationships. Our goal is to predict the sign of a given relationship based on
the evidences provided in the current snapshot of the network. More precisely,
using machine learning techniques we develop a model that after being trained
on a particular network predicts the sign of an unknown or hidden link. The
model uses relationships and influences from peers as evidences for the guess,
however, the set of peers used is not predefined but rather learned during the
training process. We use quadratic correlation between peer members to train
the predictor. The model is tested on popular online datasets such as Epinions,
Slashdot, and Wikipedia. In many cases it shows almost perfect prediction
accuracy. Moreover, our model can also be efficiently updated as the
underlaying social network evolves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1638</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1638</id><created>2012-12-07</created><updated>2014-03-23</updated><authors><author><keyname>Ji</keyname><forenames>Bo</forenames></author><author><keyname>Gupta</keyname><forenames>Gagan R.</forenames></author><author><keyname>Sharma</keyname><forenames>Manu</forenames></author><author><keyname>Lin</keyname><forenames>Xiaojun</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Achieving Optimal Throughput and Near-Optimal Asymptotic Delay
  Performance in Multi-Channel Wireless Networks with Low Complexity: A
  Practical Greedy Scheduling Policy</title><categories>cs.NI cs.IT cs.PF math.IT</categories><comments>Accepted for publication by the IEEE/ACM Transactions on Networking,
  February 2014. A preliminary version of this work was presented at IEEE
  INFOCOM 2013, Turin, Italy, April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the scheduling problem in multi-channel wireless
networks, e.g., the downlink of a single cell in fourth generation (4G)
OFDM-based cellular networks. Our goal is to design practical scheduling
policies that can achieve provably good performance in terms of both throughput
and delay, at a low complexity. While a class of $O(n^{2.5} \log n)$-complexity
hybrid scheduling policies are recently developed to guarantee both
rate-function delay optimality (in the many-channel many-user asymptotic
regime) and throughput optimality (in the general non-asymptotic setting),
their practical complexity is typically high. To address this issue, we develop
a simple greedy policy called Delay-based Server-Side-Greedy (D-SSG) with a
\lower complexity $2n^2+2n$, and rigorously prove that D-SSG not only achieves
throughput optimality, but also guarantees near-optimal asymptotic delay
performance. Specifically, we show that the rate-function attained by D-SSG for
any delay-violation threshold $b$, is no smaller than the maximum achievable
rate-function by any scheduling policy for threshold $b-1$. Thus, we are able
to achieve a reduction in complexity (from $O(n^{2.5} \log n)$ of the hybrid
policies to $2n^2 + 2n$) with a minimal drop in the delay performance. More
importantly, in practice, D-SSG generally has a substantially lower complexity
than the hybrid policies that typically have a large constant factor hidden in
the $O(\cdot)$ notation. Finally, we conduct numerical simulations to validate
our theoretical results in various scenarios. The simulation results show that
D-SSG not only guarantees a near-optimal rate-function, but also empirically is
virtually indistinguishable from delay-optimal policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1651</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1651</id><created>2012-12-01</created><authors><author><keyname>Glazunov</keyname><forenames>N. M.</forenames></author></authors><title>Foundations of scientific research (Foundations of Research Activities)</title><categories>cs.OH</categories><comments>167 pages</comments><msc-class>68-01, 68T30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During years 2008 to 2011 author gives several courses on Foundations of
Scientific Research at Computer Science Faculty of the National Aviation
University in Kiev. This text presents material to lectures of the courses. It
consists of 18 sections and some ideas of the manual can be seen from their
titles. These include: General notions about scientific research. Ontologies
and upper ontologies. Ontologies of object domains. Examples of Research
Activity. Some Notions of the Theory of Finite and Discrete Sets. Algebraic
Operations and Algebraic Structures. Elements of the Theory of Graphs and Nets.
Scientific activity on the example of Information and its investigation.
Scientific research in Artificial Intelligence. Compilers and compilation.
Objective, Concepts and History of Computer security. Methodological and
categorical apparatus of scientific research. Methodology and methods of
scientific research. Scientific idea and significance of scientific research.
Forms of scientific knowledge organization and principles of scientific
research. Theoretical study, applied study and creativity. Types of scientific
research: theoretical study, applied study. Types of scientific research: forms
of representation of material. Some sections of the text contain enough
material to lectures, but in some cases these are sketchs without references to
Foundations of Research Activities. Really this is the first version of the
manual and author plans to edit, modify and extend the version. Some reasons
impose the author to post it as e-print. . Author compiled material from many
sources and hope that it gives various points of view on Foundations of
Research Activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1682</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1682</id><created>2012-12-07</created><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Panagiotou</keyname><forenames>Konstantinos</forenames></author></authors><title>Going after the k-SAT Threshold</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random $k$-SAT is the single most intensely studied example of a random
constraint satisfaction problem. But despite substantial progress over the past
decade, the threshold for the existence of satisfying assignments is not known
precisely for any $k\geq3$. The best current results, based on the second
moment method, yield upper and lower bounds that differ by an additive $k\cdot
\frac{\ln2}2$, a term that is unbounded in $k$ (Achlioptas, Peres: STOC 2003).
The basic reason for this gap is the inherent asymmetry of the Boolean value
`true' and `false' in contrast to the perfect symmetry, e.g., among the various
colors in a graph coloring problem. Here we develop a new asymmetric second
moment method that allows us to tackle this issue head on for the first time in
the theory of random CSPs. This technique enables us to compute the $k$-SAT
threshold up to an additive $\ln2-\frac12+O(1/k)\approx 0.19$. Independently of
the rigorous work, physicists have developed a sophisticated but non-rigorous
technique called the &quot;cavity method&quot; for the study of random CSPs (M\'ezard,
Parisi, Zecchina: Science 2002). Our result matches the best bound that can be
obtained from the so-called &quot;replica symmetric&quot; version of the cavity method,
and indeed our proof directly harnesses parts of the physics calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1684</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1684</id><created>2012-12-07</created><authors><author><keyname>Gonz&#xe1;lez-Bail&#xf3;n</keyname><forenames>Sandra</forenames></author><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Rivero</keyname><forenames>Alejandro</forenames></author><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Assessing the Bias in Communication Networks Sampled from Twitter</title><categories>physics.soc-ph cs.SI</categories><comments>35 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We collect and analyse messages exchanged in Twitter using two of the
platform's publicly available APIs (the search and stream specifications). We
assess the differences between the two samples, and compare the networks of
communication reconstructed from them. The empirical context is given by
political protests taking place in May 2012: we track online communication
around these protests for the period of one month, and reconstruct the network
of mentions and re-tweets according to the two samples. We find that the search
API over-represents the more central users and does not offer an accurate
picture of peripheral activity; we also find that the bias is greater for the
network of mentions. We discuss the implications of this bias for the study of
diffusion dynamics and collective action in the digital era, and advocate the
need for more uniform sampling procedures in the study of online communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1703</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1703</id><created>2012-12-06</created><authors><author><keyname>Huemer</keyname><forenames>Mario</forenames></author><author><keyname>Hofbauer</keyname><forenames>Christian</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Non-Systematic Complex Number RS Coded OFDM by Unique Word Prefix</title><categories>cs.IT math.IT</categories><comments>IEEE Tranactions on Signal Processing, Vol. 60, No. 1, 2012. arXiv
  admin note: text overlap with arXiv:1202.1398</comments><doi>10.1109/TSP.2011.2168522</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we expand our recently introduced concept of UW-OFDM (unique
word orthogonal frequency division multiplexing). In UW-OFDM the cyclic
prefixes (CPs) are replaced by deterministic sequences, the so-called unique
words (UWs). The UWs are generated by appropriately loading a set of redundant
subcarriers. By that a systematic complex number Reed Solomon (RS) code
construction is introduced in a quite natural way, because an RS code may be
defined as the set of vectors, for which a block of successive zeros occurs in
the other domain w.r.t. a discrete Fourier transform. (For a fixed block
different to zero, i.e., a UW, a coset code of an RS code is generated.) A
remaining problem in the original systematic coded UW-OFDM concept is the fact
that the redundant subcarrier symbols disproportionately contribute to the mean
OFDM symbol energy. In this paper we introduce the concept of non-systematic
coded UW-OFDM, where the redundancy is no longer allocated to dedicated
subcarriers, but distributed over all subcarriers. We derive optimum complex
valued code generator matrices matched to the BLUE (best linear unbiased
estimator) and to the LMMSE (linear minimum mean square error) data estimator,
respectively. With the help of simulations we highlight the advantageous
spectral properties and the superior BER (bit error ratio) performance of
non-systematic coded UW-OFDM compared to systematic coded UW-OFDM as well as to
CP-OFDM in AWGN (additive white Gaussian noise) and in frequency selective
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1707</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1707</id><created>2012-12-07</created><updated>2014-03-28</updated><authors><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author><author><keyname>Sarkar</keyname><forenames>Tuhin</forenames></author><author><keyname>Tatikonda</keyname><forenames>Sekhar</forenames></author></authors><title>Lossy Compression via Sparse Linear Regression: Computationally
  Efficient Encoding and Decoding</title><categories>cs.IT math.IT stat.ML</categories><comments>14 pages, to appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 6, pp.
  3265-3278, June 2014</journal-ref><doi>10.1109/TIT.2014.2314676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose computationally efficient encoders and decoders for lossy
compression using a Sparse Regression Code. The codebook is defined by a design
matrix and codewords are structured linear combinations of columns of this
matrix. The proposed encoding algorithm sequentially chooses columns of the
design matrix to successively approximate the source sequence. It is shown to
achieve the optimal distortion-rate function for i.i.d Gaussian sources under
the squared-error distortion criterion. For a given rate, the parameters of the
design matrix can be varied to trade off distortion performance with encoding
complexity. An example of such a trade-off as a function of the block length n
is the following. With computational resource (space or time) per source sample
of O((n/\log n)^2), for a fixed distortion-level above the Gaussian
distortion-rate function, the probability of excess distortion decays
exponentially in n. The Sparse Regression Code is robust in the following
sense: for any ergodic source, the proposed encoder achieves the optimal
distortion-rate function of an i.i.d Gaussian source with the same variance.
Simulations show that the encoder has good empirical performance, especially at
low and moderate rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1709</identifier>
 <datestamp>2012-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1709</id><created>2012-12-07</created><authors><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Evolution of the most common English words and phrases over the
  centuries</title><categories>physics.soc-ph cs.CL cs.DL</categories><comments>6 two-column pages, 4 figures; accepted for publication in Journal of
  the Royal Society Interface [tables available at
  http://www.matjazperc.com/ngrams]</comments><journal-ref>J. R. Soc. Interface 9 (2012) 3323-3328</journal-ref><doi>10.1098/rsif.2012.0491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By determining which were the most common English words and phrases since the
beginning of the 16th century, we obtain a unique large-scale view of the
evolution of written text. We find that the most common words and phrases in
any given year had a much shorter popularity lifespan in the 16th than they had
in the 20th century. By measuring how their usage propagated across the years,
we show that for the past two centuries the process has been governed by linear
preferential attachment. Along with the steady growth of the English lexicon,
this provides an empirical explanation for the ubiquity of the Zipf's law in
language statistics and confirms that writing, although undoubtedly an
expression of art and skill, is not immune to the same influences of
self-organization that are known to regulate processes as diverse as the making
of new friends and World Wide Web growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1710</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1710</id><created>2012-12-09</created><updated>2014-01-23</updated><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>The information and its observer: external and internal information
  processes, information cooperation, and the origin of the observer intellect</title><categories>nlin.AO cs.IT math.IT</categories><comments>43 pages, 7 figures</comments><msc-class>58J65, 60J65, 93B52, 93E02, 93E15, 93E30</msc-class><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We examine information nature of observing interactive processes during
conversion of observed uncertainty to observer certainty, which leads to
minimax information law of both optimal extraction and consumption of
information: by getting maximum of information from each of its observed
minimum and minimizing the maximum while spending it. The minimax is dual
complimentary variation principle, establishing mathematical information law,
which functionally unifies the following observer regularities. Integral
measuring each observing process under multiple trial actions. Converting the
observed uncertainty to information-certainty by generation of internal
information micro and macrodynamics and verification of trial information.
Enclosing the internal dynamics in information network (IN). Building the IN
dynamic hierarchical logic, which integrates the observer requested information
in the IN code. Enfolding the concurrent information in temporary build the IN
high level logic that requests new information for the running observer IN.
Self-forming the observer inner dynamical and geometrical structures with a
limited boundary, shaped by the IN information geometry during the time-space
cooperative processes. These functional regularities create united information
mechanism, whose integral logic self-operates this mechanism, transforming
multiple interacting uncertainties to physical reality-matter, human
information and cognition, which originate the observer information intellect.
This logic holds invariance of information and physical regularities, following
from the minimax information law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1734</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1734</id><created>2012-12-07</created><authors><author><keyname>Widemann</keyname><forenames>Baltasar Tranc&#xf3;n y</forenames></author></authors><title>Systematic Construction of Temporal Logics for Dynamical Systems via
  Coalgebra</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal logics are an obvious high-level descriptive companion formalism to
dynamical systems which model behavior as deterministic evolution of state over
time. A wide variety of distinct temporal logics applicable to dynamical
systems exists, and each candidate has its own pragmatic justification. Here, a
systematic approach to the construction of temporal logics for dynamical
systems is proposed: Firstly, it is noted that dynamical systems can be seen as
coalgebras in various ways. Secondly, a straightforward standard construction
of modal logics out of coalgebras, namely Moss's coalgebraic logic, is applied.
Lastly, the resulting systems are characterized with respect to the temporal
properties they express.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1735</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1735</id><created>2012-12-07</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards Design of System Hierarchy (research survey)</title><categories>math.OC cs.AI cs.NI cs.SY</categories><comments>36 pages, 41 figures, 9 tables</comments><msc-class>68T20, 90C27, 90C29, 90C59, 90C90</msc-class><acm-class>G.1.6; G.2.1; G.2.3; H.4.2; I.2.8; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses design/building frameworks for some kinds of tree-like
and hierarchical structures of systems. The following approaches are examined:
(1) expert-based procedures, (2) hierarchical clustering; (3) spanning problems
(e.g., minimum spanning tree, minimum Steiner tree, maximum leaf spanning tree
problem; (4) design of organizational 'optimal' hierarchies; (5) design of
multi-layer (e.g., three-layer) k-connected network; (6) modification of
hierarchies or networks: (i) modification of tree via condensing of neighbor
nodes, (ii) hotlink assignment, (iii) transformation of tree into Steiner tree,
(iv) restructuring as modification of an initial structural solution into a
solution that is the most close to a goal solution while taking into account a
cost of the modification. Combinatorial optimization problems are considered as
basic ones (e.g., classification, knapsack problem, multiple choice problem,
assignment problem). Some numerical examples illustrate the suggested problems
and solving frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1740</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1740</id><created>2012-12-07</created><authors><author><keyname>Ferreira</keyname><forenames>Ana S. Rufino</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author></authors><title>A Graph Partitioning Approach to Predict Patterns in Lateral Inhibition
  Systems</title><categories>math.DS cs.SY</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze pattern formation on a network of cells where each cell inhibits
its neighbors through cell-to-cell contact signaling. The network is modeled as
an interconnection of identical dynamical subsystems each of which represents
the signaling reactions in a cell. We search for steady state patterns by
partitioning the graph vertices into disjoint classes, where the cells in the
same class have the same final fate. To prove the existence of steady states
with this structure, we use results from monotone systems theory. Finally, we
analyze the stability of these patterns with a block decomposition based on the
graph partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1744</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1744</id><created>2012-12-07</created><updated>2013-04-20</updated><authors><author><keyname>Snyder</keyname><forenames>David</forenames></author><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author></authors><title>Computational Capabilities of Random Automata Networks for Reservoir
  Computing</title><categories>nlin.AO cond-mat.dis-nn cs.NE</categories><comments>9 pages, 6 figures</comments><journal-ref>Physical Review E, 87(4):042808 (2013)</journal-ref><doi>10.1103/PhysRevE.87.042808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper underscores the conjecture that intrinsic computation is maximal
in systems at the &quot;edge of chaos.&quot; We study the relationship between dynamics
and computational capability in Random Boolean Networks (RBN) for Reservoir
Computing (RC). RC is a computational paradigm in which a trained readout layer
interprets the dynamics of an excitable component (called the reservoir) that
is perturbed by external input. The reservoir is often implemented as a
homogeneous recurrent neural network, but there has been little investigation
into the properties of reservoirs that are discrete and heterogeneous. Random
Boolean networks are generic and heterogeneous dynamical systems and here we
use them as the reservoir. An RBN is typically a closed system; to use it as a
reservoir we extend it with an input layer. As a consequence of perturbation,
the RBN does not necessarily fall into an attractor. Computational capability
in RC arises from a trade-off between separability and fading memory of inputs.
We find the balance of these properties predictive of classification power and
optimal at critical connectivity. These results are relevant to the
construction of devices which exploit the intrinsic dynamics of complex
heterogeneous systems, such as biomolecular substrates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1752</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1752</id><created>2012-12-07</created><authors><author><keyname>Chakraborty</keyname><forenames>Mriganka</forenames></author><author><keyname>Ghosh</keyname><forenames>Arka</forenames></author></authors><title>Hybrid Optimized Back propagation Learning Algorithm For Multi-layer
  Perceptron</title><categories>cs.NE</categories><comments>Accepted for publish in 18th December, 2012,International Journal of
  Computer Applications, Foundation of Computer Science, New York, USA</comments><journal-ref>International Journal of Computer Applications 60(13):1-5, 2012</journal-ref><doi>10.5120/9749-3332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard neural network based on general back propagation learning using
delta method or gradient descent method has some great faults like poor
optimization of error-weight objective function, low learning rate, instability
.This paper introduces a hybrid supervised back propagation learning algorithm
which uses trust-region method of unconstrained optimization of the error
objective function by using quasi-newton method .This optimization leads to
more accurate weight update system for minimizing the learning error during
learning phase of multi-layer perceptron.[13][14][15] In this paper augmented
line search is used for finding points which satisfies Wolfe condition. In this
paper, This hybrid back propagation algorithm has strong global convergence
properties &amp; is robust &amp; efficient in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1754</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1754</id><created>2012-12-07</created><authors><author><keyname>Schalekamp</keyname><forenames>Frans</forenames></author><author><keyname>Sitters</keyname><forenames>Rene</forenames></author><author><keyname>van der Ster</keyname><forenames>Suzanne</forenames></author><author><keyname>Stougie</keyname><forenames>Leen</forenames></author><author><keyname>Verdugo</keyname><forenames>Victor</forenames></author><author><keyname>van Zuylen</keyname><forenames>Anke</forenames></author></authors><title>Split Scheduling with Uniform Setup Times</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a scheduling problem in which jobs may be split into parts, where
the parts of a split job may be processed simultaneously on more than one
machine. Each part of a job requires a setup time, however, on the machine
where the job part is processed. During setup a machine cannot process or set
up any other job. We concentrate on the basic case in which setup times are
job-, machine-, and sequence-independent. Problems of this kind were
encountered when modelling practical problems in planning disaster relief
operations. Our main algorithmic result is a polynomial-time algorithm for
minimising total completion time on two parallel identical machines. We argue
why the same problem with three machines is not an easy extension of the
two-machine case, leaving the complexity of this case as a tantalising open
problem. We give a constant-factor approximation algorithm for the general case
with any number of machines and a polynomial-time approximation scheme for a
fixed number of machines. For the version with objective minimising weighted
total completion time we prove NP-hardness. Finally, we conclude with an
overview of the state of the art for other split scheduling problems with job-,
machine-, and sequence-independent setup times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1762</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1762</id><created>2012-12-08</created><authors><author><keyname>Huyen</keyname><forenames>Phan Thi Thanh</forenames></author><author><keyname>Ochimizu</keyname><forenames>Koichiro</forenames></author></authors><title>A Change Support Model for Distributed Collaborative Work</title><categories>cs.SE</categories><comments>10 pages, 13 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed collaborative software development tends to make artifacts and
decisions inconsistent and uncertain. We try to solve this problem by providing
an information repository to reflect the state of works precisely, by managing
the states of artifacts/products made through collaborative work, and the
states of decisions made through communications. In this paper, we propose
models and a tool to construct the artifact-related part of the information
repository, and explain the way to use the repository to resolve
inconsistencies caused by concurrent changes of artifacts. We first show the
model and the tool to generate the dependency relationships among UML model
elements as content of the information repository. Next, we present the model
and the method to generate change support workflows from the information
repository. These workflows give us the way to efficiently modify the
change-related artifacts for each change request. Finally, we define
inconsistency patterns that enable us to be aware of the possibility of
inconsistency occurrences. By combining this mechanism with version control
systems, we can make changes safely. Our models and tool are useful in the
maintenance phase to perform changes safely and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1763</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1763</id><created>2012-12-08</created><authors><author><keyname>Khanum</keyname><forenames>Mohammadi Akheela</forenames></author><author><keyname>Ketari</keyname><forenames>Lamia Mohammed</forenames></author></authors><title>Trends in Combating Image Spam E-mails</title><categories>cs.CR</categories><journal-ref>ICFIT 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid adoption of Internet as an easy way to communicate, the amount
of unsolicited e-mails, known as spam e-mails, has been growing rapidly. The
major problem of spam e-mails is the loss of productivity and a drain on IT
resources. Today, we receive spam more rapidly than the legitimate e-mails.
Initially, spam e-mails contained only textual messages which were easily
detected by the text-based spam filters. To evade such detection, spammers came
up with a new sophisticated technique called image spam. Image spam consists in
embedding the advertisement text in images rather than in the body of the
e-mail, yet the image contents are not detected by most spam filters. In this
paper, we examine the motivations and the challenges in image spam filtering
research, and we review the recent trends in combating image spam e-mails. The
review indicates that spamming is a business model and spammers are becoming
more sophisticated in their approach to adapt to all challenges, and hence,
defeating the conventional spam filtering technologies. Therefore, image spam
detection techniques should be scalable and adaptable to meet the future
tactics of the spammers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1787</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1787</id><created>2012-12-08</created><authors><author><keyname>Garg</keyname><forenames>Rohan</forenames></author><author><keyname>Sodha</keyname><forenames>Komal</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author></authors><title>A Generic Checkpoint-Restart Mechanism for Virtual Machines</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common today to deploy complex software inside a virtual machine (VM).
Snapshots provide rapid deployment, migration between hosts, dependability
(fault tolerance), and security (insulating a guest VM from the host). Yet, for
each virtual machine, the code for snapshots is laboriously developed on a
per-VM basis. This work demonstrates a generic checkpoint-restart mechanism for
virtual machines. The mechanism is based on a plugin on top of an unmodified
user-space checkpoint-restart package, DMTCP. Checkpoint-restart is
demonstrated for three virtual machines: Lguest, user-space QEMU, and KVM/QEMU.
The plugins for Lguest and KVM/QEMU require just 200 lines of code. The Lguest
kernel driver API is augmented by 40 lines of code. DMTCP checkpoints
user-space QEMU without any new code. KVM/QEMU, user-space QEMU, and DMTCP need
no modification. The design benefits from other DMTCP features and plugins.
Experiments demonstrate checkpoint and restart in 0.2 seconds using forked
checkpointing, mmap-based fast-restart, and incremental Btrfs-based snapshots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1789</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1789</id><created>2012-12-08</created><updated>2013-07-18</updated><authors><author><keyname>Krajicek</keyname><forenames>Jan</forenames></author></authors><title>On the computational complexity of finding hard tautologies</title><categories>math.LO cs.CC</categories><msc-class>03F20, 68Q15</msc-class><doi>10.1112/blms/bdt071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known (cf. K.-Pudl\'ak 1989) that a polynomial time algorithm
finding tautologies hard for a propositional proof system $P$ exists iff $P$ is
not optimal. Such an algorithm takes $1^{(k)}$ and outputs a tautology $\tau_k$
of size at least $k$ such that $P$ is not p-bounded on the set of all
$\tau_k$'s.
  We consider two more general search problems involving finding a hard
formula, {\bf Cert} and {\bf Find}, motivated by two hypothetical situations:
that one can prove that $\np \neq co\np$ and that no optimal proof system
exists. In {\bf Cert} one is asked to find a witness that a given
non-deterministic circuit with $k$ inputs does not define $TAUT \cap \kk$. In
{\bf Find}, given $1^{(k)}$ and a tautology $\alpha$ of size at most $k^{c_0}$,
one should output a size $k$ tautology $\beta$ that has no size $k^{c_1}$
$P$-proof from substitution instances of $\alpha$.
  We shall prove, assuming the existence of an exponentially hard one-way
permutation, that {\bf Cert} cannot be solved by a time $2^{O(k)}$ algorithm.
Using a stronger hypothesis about the proof complexity of Nisan-Wigderson
generator we show that both problems {\bf Cert} and {\bf Find} are actually
only partially defined for infinitely many $k$ (i.e. there are inputs
corresponding to $k$ for which the problem has no solution). The results are
based on interpreting the Nisan-Wigderson generator as a proof system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1790</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1790</id><created>2012-12-08</created><authors><author><keyname>Baig</keyname><forenames>Faisal</forenames></author><author><keyname>Beg</keyname><forenames>Saira</forenames></author><author><keyname>Khan</keyname><forenames>Muhammad Fahad</forenames></author></authors><title>Controlling Home Appliances Remotely through Voice Command</title><categories>cs.OH</categories><comments>4 pages, 4, figures, International Journal of Computer Applications</comments><doi>10.5120/7437-0133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling appliances is a main part of automation. The main object of Home
automation is to provide a wireless communication link of home appliances to
the remote user. The main objective of this work is to make such a system which
controls the home appliances remotely. This paper discusses two methods of
controlling home appliances one is via voice to text SMS and other is to use
the mobile as a remote control, this system will provide a benefit to the
elderly and disable people and also to those who are unaware of typing an SMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1796</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1796</id><created>2012-12-08</created><authors><author><keyname>Kuhn</keyname><forenames>Adrian</forenames></author></authors><title>On Extracting Unit Tests from Interactive Programming Sessions</title><categories>cs.SE</categories><comments>Submitted to ICSE 2012, New Ideas track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software engineering methodologies propose that developers should capture
their efforts in ensuring that programs run correctly in repeatable and
automated artifacts, such as unit tests. However, when looking at developer
activities on a spectrum from exploratory testing to scripted testing we find
that many engineering activities include bursts of exploratory testing. In this
paper we propose to leverage these exploratory testing bursts by automatically
extracting scripted tests from a recording of these sessions. In order to do
so, we wiretap the development environment so we can record all program input,
all user-issued functions calls, and all program output of an exploratory
testing session. We propose to then use machine learning (i.e. clustering) to
extract scripted test cases from these recordings in real-time. We outline two
early-stage prototypes, one for a static and one for a dynamic language. And we
outline how this idea fits into the bigger research direction of programming by
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1798</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1798</id><created>2012-12-08</created><authors><author><keyname>Rokbani</keyname><forenames>Nizar</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M</forenames></author></authors><title>IK-PSO, PSO Inverse Kinematics Solver with Application to Biped Gait
  Generation</title><categories>cs.RO cs.AI</categories><comments>7 pages, 7 figures, &quot;Published with International Journal of Computer
  Applications (IJCA)&quot;</comments><journal-ref>International Journal of Computer applications (IJCA) 58 (22),
  33-39 (2012)</journal-ref><doi>10.5120/9432-3844</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new approach allowing the generation of a simplified
Biped gait. This approach combines a classical dynamic modeling with an inverse
kinematics' solver based on particle swarm optimization, PSO. First, an
inverted pendulum, IP, is used to obtain a simplified dynamic model of the
robot and to compute the target position of a key point in biped locomotion,
the Centre Of Mass, COM. The proposed algorithm, called IK-PSO, Inverse
Kinematics PSO, returns and inverse kinematics solution corresponding to that
COM respecting the joints constraints. In This paper the inertia weight PSO
variant is used to generate a possible solution according to the stability
based fitness function and a set of joints motions constraints. The method is
applied with success to a leg motion generation. Since based on a
pre-calculated COM, that satisfied the biped stability, the proposal allowed
also to plan a walk with application on a small size biped robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1800</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1800</id><created>2012-12-08</created><authors><author><keyname>Rokbani</keyname><forenames>Nizar</forenames></author><author><keyname>Cherif</keyname><forenames>Boudour Ammar</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>Toward Intelligent Biped-Humanoids Gaits Generation</title><categories>cs.RO</categories><comments>15 pages</comments><journal-ref>Nizar Rokbani, Boudour Ammar Cherif and Adel M. Alimi (2009).
  Toward Intelligent Biped-Humanoids Gaits Generation, Humanoid Robots, Ben
  Choi (Ed.), ISBN: 978-953-7619-44-2, InTech</journal-ref><doi>10.5772/6732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we will highlight our experimental studies on natural human
walking analysis and introduce a biologically inspired design for simple
bipedal locomotion system of humanoid robots. Inspiration comes directly from
human walking analysis and human muscles mechanism and control. A hybrid
algorithm for walking gaits generation is then proposed as an innovative
alternative to classically used kinematics and dynamic equations solving, the
gaits include knee, ankle and hip trajectories. The proposed algorithm is an
intelligent evolutionary based on particle swarm optimization paradigm. This
proposal can be used for small size humanoid robots, with a knee an ankle and a
hip and at least six Degrees of Freedom (DOF).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1801</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1801</id><created>2012-12-08</created><updated>2014-10-03</updated><authors><author><keyname>Malloy</keyname><forenames>Matthew L.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>Sequential Testing for Sparse Recovery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies sequential methods for recovery of sparse signals in high
dimensions. When compared to fixed sample size procedures, in the sparse
setting, sequential methods can result in a large reduction in the number of
samples needed for reliable signal support recovery. Starting with a lower
bound, we show any coordinate-wise sequential sampling procedure fails in the
high dimensional limit provided the average number of measurements per
dimension is less then log s/D(P_0||P_1) where s is the level of sparsity and
D(P_0||P_1) the Kullback-Leibler divergence between the underlying
distributions. A series of Sequential Probability Ratio Tests (SPRT) which
require complete knowledge of the underlying distributions is shown to achieve
this bound. Motivated by real world experiments and recent work in adaptive
sensing, we introduce a simple procedure termed Sequential Thresholding which
can be implemented when the underlying testing problem satisfies a monotone
likelihood ratio assumption. Sequential Thresholding guarantees exact support
recovery provided the average number of measurements per dimension grows faster
than log s/ D(P_0||P_1), achieving the lower bound. For comparison, we show any
non-sequential procedure fails provided the number of measurements grows at a
rate less than log n/D(P_1||P_0), where n is the total dimension of the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1819</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1819</id><created>2012-12-08</created><updated>2013-01-10</updated><authors><author><keyname>Carlinet</keyname><forenames>Edwin</forenames></author><author><keyname>G&#xe9;raud</keyname><forenames>Thierry</forenames></author></authors><title>A fair comparison of many max-tree computation algorithms (Extended
  version of the paper submitted to ISMM 2013</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of connected filters for the last decade, many
algorithms have been proposed to compute the max-tree. Max-tree allows to
compute the most advanced connected operators in a simple way. However, no fair
comparison of algorithms has been proposed yet and the choice of an algorithm
over an other depends on many parameters. Since the need of fast algorithms is
obvious for production code, we present an in depth comparison of five
algorithms and some variations of them in a unique framework. Finally, a
decision tree will be proposed to help user in choosing the right algorithm
with respect to their data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1824</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1824</id><created>2012-12-08</created><updated>2012-12-28</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Stochastic Gradient Descent for Non-smooth Optimization: Convergence
  Results and Optimal Averaging Schemes</title><categories>cs.LG math.OC stat.ML</categories><comments>To appear in ICML 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is one of the simplest and most popular
stochastic optimization methods. While it has already been theoretically
studied for decades, the classical analysis usually required non-trivial
smoothness assumptions, which do not apply to many modern applications of SGD
with non-smooth objective functions such as support vector machines. In this
paper, we investigate the performance of SGD without such smoothness
assumptions, as well as a running average scheme to convert the SGD iterates to
a solution with optimal optimization accuracy. In this framework, we prove that
after T rounds, the suboptimality of the last SGD iterate scales as
O(log(T)/\sqrt{T}) for non-smooth convex objective functions, and O(log(T)/T)
in the non-smooth strongly convex case. To the best of our knowledge, these are
the first bounds of this kind, and almost match the minimax-optimal rates
obtainable by appropriate averaging schemes. We also propose a new and simple
averaging scheme, which not only attains optimal rates, but can also be easily
computed on-the-fly (in contrast, the suffix averaging scheme proposed in
Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some
experimental illustrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1831</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1831</id><created>2012-12-08</created><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>A New Regularity Lemma and Faster Approximation Algorithms for Low
  Threshold Rank Graphs</title><categories>cs.DS math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kolla and Tulsiani [KT07,Kolla11} and Arora, Barak and Steurer [ABS10]
introduced the technique of subspace enumeration, which gives approximation
algorithms for graph problems such as unique games and small set expansion; the
running time of such algorithms is exponential in the threshold-rank of the
graph.
  Guruswami and Sinop [GS11,GS12], and Barak, Raghavendra, and Steurer [BRS11]
developed an alternative approach to the design of approximation algorithms for
graphs of bounded threshold-rank, based on semidefinite programming relaxations
in the Lassere hierarchy and on novel rounding techniques. These algorithms are
faster than the ones based on subspace enumeration and work on a broad class of
problems.
  In this paper we develop a third approach to the design of such algorithms.
We show, constructively, that graphs of bounded threshold-rank satisfy a weak
Szemeredi regularity lemma analogous to the one proved by Frieze and Kannan
[FK99] for dense graphs. The existence of efficient approximation algorithms is
then a consequence of the regularity lemma, as shown by Frieze and Kannan.
Applying our method to the Max Cut problem, we devise an algorithm that is
faster than all previous algorithms, and is easier to describe and analyze.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1839</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1839</id><created>2012-12-08</created><authors><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author><author><keyname>Kristalny</keyname><forenames>Maxim</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author></authors><title>On Structured Realizability and Stabilizability of Linear Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the notion of structured realizability for linear systems defined
over graphs. A stabilizable and detectable realization is structured if the
state-space matrices inherit the sparsity pattern of the adjacency matrix of
the associated graph. In this paper, we demonstrate that not every structured
transfer matrix has a structured realization and we reveal the practical
meaning of this fact. We also uncover a close connection between the structured
realizability of a plant and whether the plant can be stabilized by a
structured controller. In particular, we show that a structured stabilizing
controller can only exist when the plant admits a structured realization.
Finally, we give a parameterization of all structured stabilizing controllers
and show that they always have structured realizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1849</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1849</id><created>2012-12-08</created><authors><author><keyname>Rukshan</keyname><forenames>A.</forenames></author><author><keyname>Baravalle</keyname><forenames>A.</forenames></author></authors><title>Automated Usability Testing: Analysing Asia Web Sites</title><categories>cs.HC</categories><comments>13 pages, 6 Tables, and 3 Figures</comments><journal-ref>International Conference on Business and Information 2012 (ICBI
  2012), Faculty of Commerce and Management Studies of University of Kelaniya,
  Sri Lanka</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web usability is continuing to be a pressing problem. For number of years
researchers have been developed tools for doing automatic web usability
testing. This study uses our own PHP, and MySQL based tool AWebHUT: Automated
Web Homepage Usability Tester to evaluate web usability of full Dmoz
(www.dmoz.org) Asia web sites (45126 on time stamp 2011-12-03 04:12:46 GMT).
The tool uses an extensive automated quantitative analysis of XHTML source code
of homepages against seventeen organised web usability guidelines. The
automated quantitative approach is effective on large scale to achieve better
usability. The AWebHUT uses four web usability levels such as N: Neutral, V:
Violate, R: Respect, and E: Error to evaluate web usability. The main objective
of the study is to produce data which is used to answer research questions, (1)
Are there any categories of web sites which have usability problems? Which
ones? and (2) Are there any categories in which the usability is typically
higher? Why? The findings were indicated that all Asia categories have
usability problems. Furthermore, there are four web sites which have highest
web usability problem with violation percentage 71. One step further, the Asia
category: Weather has highest usability problems with 42.2819 as the average of
the violation percentage. The category Weather uses tables and images,
considerable amount of those were not satisfying web usability guidelines which
relates to tables and images. One step further, the Asia wants to get the same
level of usability as North America, Europe, and Australia therefore it is
essential to have an automated web usability evaluation in Asia web sites to
identify web usability problems which are important for improving Asia web
sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1863</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1863</id><created>2012-12-09</created><authors><author><keyname>Sengupta</keyname><forenames>Madhumita</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Self Authentication of image through Daubechies Transform technique
  (SADT)</title><categories>cs.CR cs.CV</categories><comments>4 page paper in 47th Annual National Convention of COMPUTER SOCIETY
  OF INDIA, The First International Conference on Intelligent Infrastructure,
  CSI-2012, held during 1st and 2nd December, 2012 at science city, Kolkata</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a 4 x 4 Daubechies transform based authentication technique
termed as SADT has been proposed to authenticate gray scale images. The cover
image is transformed into the frequency domain using 4 x 4 mask in a row major
order using Daubechies transform technique, resulting four frequency subbands
AF, HF, VF and DF. One byte of every band in a mask is embedding with two or
four bits of secret information. Experimental results are computed and compared
with the existing authentication techniques like Li s method [5], SCDFT [6],
Region-Based method [7] and other similar techniques based on Mean Square Error
(MSE), Peak Signal to Noise Ratio (PSNR) and Image Fidelity (IF), which shows
better performance in SADT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1881</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1881</id><created>2012-12-09</created><updated>2013-08-22</updated><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author></authors><title>Deciding Monotone Duality and Identifying Frequent Itemsets in Quadratic
  Logspace</title><categories>cs.DS cs.AI cs.CC cs.DB</categories><comments>Preprint of a paper which appeared in: Proceedings of the 32nd ACM
  SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS 2013,
  New York, NY,USA, June 22-27,2013, pp.25-36</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The monotone duality problem is defined as follows: Given two monotone
formulas f and g in iredundant DNF, decide whether f and g are dual. This
problem is the same as duality testing for hypergraphs, that is, checking
whether a hypergraph H consists of precisely all minimal transversals of a
simple hypergraph G. By exploiting a recent problem-decomposition method by
Boros and Makino (ICALP 2009), we show that duality testing for hypergraphs,
and thus for monotone DNFs, is feasible in DSPACE[log^2 n], i.e., in quadratic
logspace. As the monotone duality problem is equivalent to a number of problems
in the areas of databases, data mining, and knowledge discovery, the results
presented here yield new complexity results for those problems, too. For
example, it follows from our results that whenever for a Boolean-valued
relation (whose attributes represent items), a number of maximal frequent
itemsets and a number of minimal infrequent itemsets are known, then it can be
decided in quadratic logspace whether there exist additional frequent or
infrequent itemsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1882</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1882</id><created>2012-12-09</created><authors><author><keyname>Elfida</keyname><forenames>Maria</forenames></author><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author></authors><title>Generating Strategic IS: Towards the Winning Strategy</title><categories>cs.OH</categories><comments>5 pages, draft to ICOCSIM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern era, the role of information system in organization has been taken
many discussions. The models of information system are constantly updated.
However, most of them can not face the changing world. This paper discusses an
approach to generating of strategic information system based on features in
organization. We proposed an approach by using disadvantages in some tools of
analysis whereby the lack of analysis appear as behaviour of relation between
organisation and the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1884</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1884</id><created>2012-12-09</created><updated>2012-12-11</updated><authors><author><keyname>Auletta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Ferraioli</keyname><forenames>Diodato</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author><author><keyname>Penna</keyname><forenames>Paolo</forenames></author><author><keyname>Persiano</keyname><forenames>Giuseppe</forenames></author></authors><title>Convergence to Equilibrium of Logit Dynamics for Strategic Games</title><categories>cs.GT cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first general bounds on the mixing time of the Markov chain
associated to the logit dynamics for wide classes of strategic games. The logit
dynamics with inverse noise beta describes the behavior of a complex system
whose individual components act selfishly and keep responding according to some
partial (&quot;noisy&quot;) knowledge of the system, where the capacity of the agent to
know the system and compute her best move is measured by the inverse of the
parameter beta.
  In particular, we prove nearly tight bounds for potential games and games
with dominant strategies. Our results show that, for potential games, the
mixing time is upper and lower bounded by an exponential in the inverse of the
noise and in the maximum potential difference. Instead, for games with dominant
strategies, the mixing time cannot grow arbitrarily with the inverse of the
noise.
  Finally, we refine our analysis for a subclass of potential games called
graphical coordination games, a class of games that have been previously
studied in Physics and, more recently, in Computer Science in the context of
diffusion of new technologies. We give evidence that the mixing time of the
logit dynamics for these games strongly depends on the structure of the
underlying graph. We prove that the mixing time of the logit dynamics for these
games can be upper bounded by a function that is exponential in the cutwidth of
the underlying graph and in the inverse of noise. Moreover, we consider two
specific and popular network topologies, the clique and the ring. For games
played on a clique we prove an almost matching lower bound on the mixing time
of the logit dynamics that is exponential in the inverse of the noise and in
the maximum potential difference, while for games played on a ring we prove
that the time of convergence of the logit dynamics to its stationary
distribution is significantly shorter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1891</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1891</id><created>2012-12-09</created><updated>2015-07-21</updated><authors><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>Natural Proofs Versus Derandomization</title><categories>cs.CC</categories><comments>32 pages, major revision for special issue of STOC'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study connections between Natural Proofs, derandomization, and the problem
of proving &quot;weak&quot; circuit lower bounds such as ${\sf NEXP} \not\subset {\sf
TC^0}$. Natural Proofs have three properties: they are constructive (an
efficient algorithm $A$ is embedded in them), have largeness ($A$ accepts a
large fraction of strings), and are useful ($A$ rejects all strings which are
truth tables of small circuits). Strong circuit lower bounds that are
&quot;naturalizing&quot; would contradict present cryptographic understanding, yet the
vast majority of known circuit lower bound proofs are naturalizing. So it is
imperative to understand how to pursue un-Natural Proofs. Some heuristic
arguments say constructivity should be circumventable: largeness is inherent in
many proof techniques, and it is probably our presently weak techniques that
yield constructivity. We prove:
  $\bullet$ Constructivity is unavoidable, even for $\sf NEXP$ lower bounds.
Informally, we prove for all &quot;typical&quot; non-uniform circuit classes ${\cal C}$,
${\sf NEXP} \not\subset {\cal C}$ if and only if there is a polynomial-time
algorithm distinguishing some function from all functions computable by ${\cal
C}$-circuits. Hence ${\sf NEXP} \not\subset {\cal C}$ is equivalent to
exhibiting a constructive property useful against ${\cal C}$.
  $\bullet$ There are no $\sf P$-natural properties useful against ${\cal C}$
if and only if randomized exponential time can be &quot;derandomized&quot; using truth
tables of circuits from ${\cal C}$ as random seeds. Therefore the task of
proving there are no $\sf P$-natural properties is inherently a derandomization
problem, weaker than but implied by the existence of strong pseudorandom
functions.
  These characterizations are applied to yield several new results, including
improved ${\sf ACC}^0$ lower bounds and new unconditional derandomizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1896</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1896</id><created>2012-12-09</created><updated>2013-01-10</updated><authors><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author></authors><title>Power Consumption Analysis of a Modern Smartphone</title><categories>cs.PF</categories><comments>11 pages, 6 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents observations about power consumption of a latest
smartphone. Modern smartphones are powerful devices with different choices of
data connections and other functional modes. This paper provides analysis of
power utilization for these different operation modes. Also, we present power
consumption by vital operating system (OS) components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1901</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1901</id><created>2012-12-09</created><authors><author><keyname>Alpeev</keyname><forenames>Andrey</forenames></author></authors><title>Kolmogorov Complexity and the Garden of Eden Theorem</title><categories>nlin.CG cs.IT math.IT</categories><comments>11 pages</comments><msc-class>68Q80, 37B15, 68Q30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose $\tau$ is a cellular automaton over an amenable group and a finite
alphabet. Celebrated Garden of Eden theorem states, that pre-injectivity of
$\tau$ is equivalent to non-existence of Garden of Eden configuration. In this
paper we will prove, that imposing some mild restrictions, we could add another
equivalent assertion: non-existence of Garden of Eden configuration is
equivalent to preservation of asymptotic Kolmogorov complexity under the action
of cellular automaton. It yields a characterisation of the cellular automata,
which preserve the asymptotic Kolmogorov complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1909</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1909</id><created>2012-12-09</created><authors><author><keyname>Yu</keyname><forenames>Yun</forenames></author><author><keyname>Nakhleh</keyname><forenames>Luay</forenames></author></authors><title>Fast Algorithms for Reconciliation under Hybridization and Incomplete
  Lineage Sorting</title><categories>q-bio.PE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconciling a gene tree with a species tree is an important task that reveals
much about the evolution of genes, genomes, and species, as well as about the
molecular function of genes. A wide array of computational tools have been
devised for this task under certain evolutionary events such as hybridization,
gene duplication/loss, or incomplete lineage sorting. Work on reconciling gene
tree with species phylogenies under two or more of these events have also begun
to emerge. Our group recently devised both parsimony and probabilistic
frameworks for reconciling a gene tree with a phylogenetic network, thus
allowing for the detection of hybridization in the presence of incomplete
lineage sorting. While the frameworks were general and could handle any
topology, they are computationally intensive, rendering their application to
large datasets infeasible. In this paper, we present two novel approaches to
address the computational challenges of the two frameworks that are based on
the concept of ancestral configurations. Our approaches still compute exact
solutions while improving the computational time by up to five orders of
magnitude. These substantial gains in speed scale the applicability of these
unified reconciliation frameworks to much larger data sets. We discuss how the
topological features of the gene tree and phylogenetic network may affect the
performance of the new algorithms. We have implemented the algorithms in our
PhyloNet software package, which is publicly available in open source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1913</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1913</id><created>2012-12-09</created><updated>2014-08-08</updated><authors><author><keyname>Cohn</keyname><forenames>Henry</forenames></author><author><keyname>Zhao</keyname><forenames>Yufei</forenames></author></authors><title>Energy-minimizing error-correcting codes</title><categories>math.CO cs.IT math.IT</categories><comments>9 pages</comments><proxy>Henry Cohn</proxy><journal-ref>IEEE Trans. Inform. Theory 60 (2014), 7442-7450</journal-ref><doi>10.1109/TIT.2014.2359201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a discrete model of repelling particles, and we show using linear
programming bounds that many familiar families of error-correcting codes
minimize a broad class of potential energies when compared with all other codes
of the same size and block length. Examples of these universally optimal codes
include Hamming, Golay, and Reed-Solomon codes, among many others, and this
helps explain their robustness as the channel model varies. Universal
optimality of these codes is equivalent to minimality of their binomial
moments, which has been proved in many cases by Ashikhmin and Barg. We
highlight connections with mathematical physics and the analogy between these
results and previous work by Cohn and Kumar in the continuous setting, and we
develop a framework for optimizing the linear programming bounds. Furthermore,
we show that if these bounds prove a code is universally optimal, then the code
remains universally optimal even if one codeword is removed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1914</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1914</id><created>2012-12-09</created><authors><author><keyname>Thakur</keyname><forenames>Manoj Rameshchandra</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Heuristic Reputation Based System to Detect Spam activities in a
  Social Networking Platform, HRSSSNP</title><categories>cs.CR</categories><comments>5 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of the social networking platform has drastically affected
the way individuals interact. Even though most of the effects have been
positive, there exist some serious threats associated with the interactions on
a social networking website. A considerable proportion of the crimes that occur
are initiated through a social networking platform [5]. Almost 33% of the
crimes on the internet are initiated through a social networking website [5].
Moreover activities like spam messages create unnecessary traffic and might
affect the user base of a social networking platform. As a result preventing
interactions with malicious intent and spam activities becomes crucial. This
work attempts to detect the same in a social networking platform by considering
a social network as a weighted graph wherein each node, which represents an
individual in the social network, stores activities of other nodes with respect
to itself in an optimized format which is referred to as localized data-set.
The weights associated with the edges in the graph represent the trust
relationship between profiles. The weights of the edges along with the
localized data-set is used to infer whether nodes in the social network are
compromised and are performing spam or malicious activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1915</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1915</id><created>2012-12-09</created><authors><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>Brunner</keyname><forenames>Robert</forenames></author><author><keyname>Burger</keyname><forenames>Dan</forenames></author><author><keyname>DuPrie</keyname><forenames>Kimberly</forenames></author><author><keyname>Hanisch</keyname><forenames>Robert J.</forenames></author><author><keyname>Mann</keyname><forenames>Robert</forenames></author><author><keyname>Mink</keyname><forenames>Jessica</forenames></author><author><keyname>Sandin</keyname><forenames>Christer</forenames></author><author><keyname>Shortridge</keyname><forenames>Keith</forenames></author><author><keyname>Teuben</keyname><forenames>Peter</forenames></author></authors><title>Bring out your codes! Bring out your codes! (Increasing Software
  Visibility and Re-use)</title><categories>astro-ph.IM cs.DL cs.SE</categories><comments>Birds of a Feather session at ADASS XXII (Champaign, IL; November,
  2012) for proceedings; 4 pages. Organized by the Astrophysics Source Code
  Library (ASCL), which is available at ascl.net Unedited notes taken at the
  session are available here: http://asterisk.apod.com/wp/?p=192</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Progress is being made in code discoverability and preservation, but as
discussed at ADASS XXI, many codes still remain hidden from public view. With
the Astrophysics Source Code Library (ASCL) now indexed by the SAO/NASA
Astrophysics Data System (ADS), the introduction of a new journal, Astronomy &amp;
Computing, focused on astrophysics software, and the increasing success of
education efforts such as Software Carpentry and SciCoder, the community has
the opportunity to set a higher standard for its science by encouraging the
release of software for examination and possible reuse. We assembled
representatives of the community to present issues inhibiting code release and
sought suggestions for tackling these factors.
  The session began with brief statements by panelists; the floor was then
opened for discussion and ideas. Comments covered a diverse range of related
topics and points of view, with apparent support for the propositions that
algorithms should be readily available, code used to produce published
scientific results should be made available, and there should be discovery
mechanisms to allow these to be found easily. With increased use of resources
such as GitHub (for code availability), ASCL (for code discovery), and a stated
strong preference from the new journal Astronomy &amp; Computing for code release,
we expect to see additional progress over the next few years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1916</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1916</id><created>2012-12-09</created><authors><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>DuPrie</keyname><forenames>Kimberly</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>Hanisch</keyname><forenames>Robert J.</forenames></author><author><keyname>Mink</keyname><forenames>Jessica</forenames></author><author><keyname>Teuben</keyname><forenames>Peter J.</forenames></author></authors><title>Astrophysics Source Code Library</title><categories>astro-ph.IM cs.DL</categories><comments>Description of ASCL display table at ADASS XXII (Champaign, IL;
  November, 2012) for proceedings; 4 pages, 2 figures. The ASCL, indexed by
  ADS, is available at ascl.net A PowerPoint presentation running at the table
  is available online: http://asterisk.apod.com/wp/?p=194 Some of the resources
  and information made available at the table are listed online:
  http://asterisk.apod.com/wp/?p=216</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Astrophysics Source Code Library (ASCL), founded in 1999, is a free
on-line registry for source codes of interest to astronomers and
astrophysicists. The library is housed on the discussion forum for Astronomy
Picture of the Day (APOD) and can be accessed at http://ascl.net. The ASCL has
a comprehensive listing that covers a significant number of the astrophysics
source codes used to generate results published in or submitted to refereed
journals and continues to grow. The ASCL currently has entries for over 500
codes; its records are citable and are indexed by ADS. The editors of the ASCL
and members of its Advisory Committee were on hand at a demonstration table in
the ADASS poster room to present the ASCL, accept code submissions, show how
the ASCL is starting to be used by the astrophysics community, and take
questions on and suggestions for improving the resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1918</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1918</id><created>2012-12-09</created><authors><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author><author><keyname>Vel&#xe1;zquez-Morales</keyname><forenames>Patricia</forenames></author><author><keyname>Meunier</keyname><forenames>Jean-Guy</forenames></author></authors><title>Condens\'es de textes par des m\'ethodes num\'eriques</title><categories>cs.IR cs.CL</categories><comments>Conf\'erence JADT 2002, Saint-Malo/France. 12 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Since information in electronic form is already a standard, and that the
variety and the quantity of information become increasingly large, the methods
of summarizing or automatic condensation of texts is a critical phase of the
analysis of texts. This article describes CORTEX a system based on numerical
methods, which allows obtaining a condensation of a text, which is independent
of the topic and of the length of the text. The structure of the system enables
it to find the abstracts in French or Spanish in very short times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1927</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1927</id><created>2012-12-09</created><authors><author><keyname>Purohit</keyname><forenames>Hemant</forenames></author><author><keyname>Dow</keyname><forenames>Alex</forenames></author><author><keyname>Alonso</keyname><forenames>Omar</forenames></author><author><keyname>Duan</keyname><forenames>Lei</forenames></author><author><keyname>Haas</keyname><forenames>Kevin</forenames></author></authors><title>User Taglines: Alternative Presentations of Expertise and Interest in
  Social Media</title><categories>cs.SI</categories><comments>First ASE International Conference on Social Informatics,
  Social-Informatics-2012</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web applications are increasingly showing recommended users from social media
along with some descriptions, an attempt to show relevancy - why they are being
shown. For example, Twitter search for a topical keyword shows expert
twitterers on the side for 'whom to follow'. Google+ and Facebook also
recommend users to follow or add to friend circle. Popular Internet newspaper-
The Huffington Post shows Twitter influencers/ experts on the side of an
article for authoritative relevant tweets. The state of the art shows user
profile bios as summary for Twitter experts, but it has issues with length
constraint imposed by user interface (UI) design, missing bio and sometimes
funny profile bio. Alternatively, applications can use human generated user
summary, but it will not scale. Therefore, we study the problem of automatic
generation of informative expertise summary or taglines for Twitter experts in
space constraint imposed by UI design. We propose three methods for expertise
summary generation- Occupation-Pattern based, Link-Triangulation based and
User-Classification based, with use of knowledge-enhanced computing approaches.
We also propose methods for final summary selection for users with multiple
candidates of generated summaries. We evaluate the proposed approaches by
user-study using a number of experiments. Our results show promising quality of
92.8% good summaries with majority agreement in the best case and 70% with
majority agreement in the worst case. Our approaches also outperform the state
of the art up to 88%. This study has implications in the area of expert
profiling, user presentation and application design for engaging user
experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1929</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1929</id><created>2012-12-09</created><authors><author><keyname>Kim</keyname><forenames>MinJi</forenames></author><author><keyname>ParandehGheibi</keyname><forenames>Ali</forenames></author><author><keyname>Urbina</keyname><forenames>Leonardo</forenames></author><author><keyname>Meedard</keyname><forenames>Muriel</forenames></author></authors><title>CTCP: Coded TCP using Multiple Paths</title><categories>cs.NI</categories><comments>Manuscript (written in June 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce CTCP, a novel multi-path transport protocol using network
coding. CTCP is designed to incorporate TCP's good features, such as congestion
control and reliability, while improving on TCP's performance in lossy and/or
dynamic networks. CTCP builds upon the ideas of TCP/NC introduced by
Sundararajan et al. and uses network coding to provide robustness against
losses. We introduce the use of multiple paths to provide robustness against
mobility and network failures. We provide an implementation of CTCP (in
userspace) to demonstrate its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1936</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1936</id><created>2012-12-09</created><authors><author><keyname>Boulanger-Lewandowski</keyname><forenames>Nicolas</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author></authors><title>High-dimensional sequence transduction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of transforming an input sequence into a
high-dimensional output sequence in order to transcribe polyphonic audio music
into symbolic notation. We introduce a probabilistic model based on a recurrent
neural network that is able to learn realistic output distributions given the
input and we devise an efficient algorithm to search for the global mode of
that distribution. The resulting method produces musically plausible
transcriptions even under high levels of noise and drastically outperforms
previous state-of-the-art approaches on five datasets of synthesized sounds and
real recordings, approximately halving the test error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1940</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1940</id><created>2012-12-06</created><authors><author><keyname>Maletic</keyname><forenames>Slobodan</forenames></author><author><keyname>Rajkovic</keyname><forenames>Milan</forenames></author></authors><title>Consensus Formation on Simplicial Complex of Opinions</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric realization of opinion is considered as a simplex and the opinion
space of a group of individuals is a simplicial complex whose topological
features are monitored in the process of opinion formation. The agents are
physically located on the nodes of the scale-free network. Social interactions
include all concepts of social dynamics present in the mainstream models
augmented by four additional interaction mechanisms which depend on the local
properties of opinions and their overlapping properties. The results pertaining
to the formation of consensus are of particular interest. An analogy with
quantum mechanical pure states is established through the application of the
high dimensional combinatorial Laplacian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1941</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1941</id><created>2012-12-09</created><updated>2012-12-11</updated><authors><author><keyname>Nikishkin</keyname><forenames>Vladimir</forenames></author></authors><title>Amortized communication complexity of an equality predicate</title><categories>cs.CC cs.DC</categories><comments>12 pages, beta version</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We study the communication complexity of a direct sum of independent copies
of the equality predicate. We prove that the probabilistic communication
complexity of this problem is equal to O(N); computational complexity of the
proposed protocol is polynomial in size of inputs. Our protocol improves the
result achieved in 1995(Feder, Kushilevitz, Naor, Nisan). Our construction is
based on two techniques: Nisan's pseudorandom generator (1992) and Smith's
string synchronization algorithm (2007).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1942</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1942</id><created>2012-12-09</created><authors><author><keyname>Sumedha</keyname></author><author><keyname>Krishnamurthy</keyname><forenames>Supriya</forenames></author><author><keyname>Sahoo</keyname><forenames>Sharmistha</forenames></author></authors><title>Balanced K-SAT and Biased random K-SAT on trees</title><categories>cond-mat.stat-mech cs.AI cs.CC</categories><comments>22 pages, 7 figures</comments><journal-ref>Phys. Rev. E 87, 042130 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study and solve some variations of the random K-satisfiability problem -
balanced K-SAT and biased random K-SAT - on a regular tree, using techniques we
have developed earlier(arXiv:1110.2065). In both these problems, as well as
variations of these that we have looked at, we find that the SAT-UNSAT
transition obtained on the Bethe lattice matches the exact threshold for the
same model on a random graph for K=2 and is very close to the numerical value
obtained for K=3. For higher K it deviates from the numerical estimates of the
solvability threshold on random graphs, but is very close to the dynamical
1-RSB threshold as obtained from the first non-trivial fixed point of the
survey propagation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1958</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1958</id><created>2012-12-09</created><authors><author><keyname>Bai</keyname><forenames>Shi</forenames></author><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author><author><keyname>Thom&#xe9;</keyname><forenames>Emmanuel</forenames></author></authors><title>Root optimization of polynomials in the number field sieve</title><categories>math.NT cs.DS</categories><comments>16 pages, 18 references</comments><msc-class>11A51 (Primary) 11R09 (Secondary)</msc-class><journal-ref>Mathematics of Computation 84 (2015), 2447-2457</journal-ref><doi>10.1090/S0025-5718-2015-02926-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general number field sieve (GNFS) is the most efficient algorithm known
for factoring large integers. It consists of several stages, the first one
being polynomial selection. The quality of the chosen polynomials in polynomial
selection can be modelled in terms of size and root properties. In this paper,
we describe some algorithms for selecting polynomials with very good root
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1969</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1969</id><created>2012-12-10</created><authors><author><keyname>Al-Dweik</keyname></author><author><keyname>Mirahmadi</keyname><forenames>M.</forenames></author><author><keyname>Shami</keyname><forenames>A.</forenames></author><author><keyname>Ding</keyname><forenames>Z.</forenames></author><author><keyname>Hamila</keyname><forenames>R.</forenames></author></authors><title>Joint Secured and Robust Technique for OFDM Systems</title><categories>cs.IT math.IT</categories><comments>This article has been submitted for possible publication at the IEEE
  ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a novel technique for joint secured and robust
transmission of orthogonal frequency division multiplexing (OFDM) based
communication systems. The proposed system is implemented by developing a new
OFDM symbol structure based on symmetric key cryptography. At the receiver
side, data detection becomes infeasible without the knowledge of the secret
key. For an intruder who tries to detect the data without the knowledge of the
key, the signal will be a noise-like signal. In addition to the system
security, theoretical and simulation results demonstrated that the proposed
system provides time and frequency diversity, which makes the system highly
robust against severe frequency-selective fading as well as other impairments
such as impulsive noise and multiple access interference. For particular
frequency-selective fading channels, the bit error rate (BER) improvements was
about 15 dB at BER of 10E-4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1984</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1984</id><created>2012-12-10</created><updated>2014-02-20</updated><authors><author><keyname>Andr&#xe9;s</keyname><forenames>Miguel E.</forenames></author><author><keyname>Bordenabe</keyname><forenames>Nicol&#xe1;s E.</forenames></author><author><keyname>Chatzikokolakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author></authors><title>Geo-Indistinguishability: Differential Privacy for Location-Based
  Systems</title><categories>cs.CR</categories><comments>15 pages</comments><acm-class>C.2.0; K.4.1</acm-class><journal-ref>Proceedings of the 2013 ACM SIGSAC conference on Computer and
  Communications Security (CCS'13), ACM, pp. 901-914, 2013</journal-ref><doi>10.1145/2508859.2516735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing popularity of location-based systems, allowing unknown/untrusted
servers to easily collect huge amounts of information regarding users'
location, has recently started raising serious privacy concerns. In this paper
we study geo-indistinguishability, a formal notion of privacy for
location-based systems that protects the user's exact location, while allowing
approximate information - typically needed to obtain a certain desired service
- to be released. Our privacy definition formalizes the intuitive notion of
protecting the user's location within a radius r with a level of privacy that
depends on r, and corresponds to a generalized version of the well-known
concept of differential privacy. Furthermore, we present a perturbation
technique for achieving geo-indistinguishability by adding controlled random
noise to the user's location. We demonstrate the applicability of our technique
on a LBS application. Finally, we compare our mechanism with other ones in the
literature. It turns our that our mechanism offers the best privacy guarantees,
for the same utility, among all those which do not depend on the prior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1986</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1986</id><created>2012-12-10</created><authors><author><keyname>Worden</keyname><forenames>Lee</forenames></author></authors><title>WorkingWiki: a MediaWiki-based platform for collaborative research</title><categories>cs.HC cs.SE</categories><comments>11 pages, 3 figures. Presented at ITP 2011 Workshop on Mathematical
  Wikis. Source code archived with revision history at
  &lt;http://lalashan.mcmaster.ca/theobio/projects/index.php/Paper_for_MathWikis-2011&gt;</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  WorkingWiki is a software extension for the popular MediaWiki platform that
makes a wiki into a powerful environment for collaborating on
publication-quality manuscripts and software projects. Developed in Jonathan
Dushoff's theoretical biology lab at McMaster University and available as free
software, it allows wiki users to work together on anything that can be done by
using UNIX commands to transform textual &quot;source code&quot; into output. Researchers
can use it to collaborate on programs written in R, python, C, or any other
language, and there are special features to support easy work on LaTeX
documents. It develops the potential of the wiki medium to serve as a
combination collaborative text editor, development environment, revision
control system, and publishing platform. Its potential uses are open-ended -
its processing is controlled by makefiles that are straightforward to customize
- and its modular design is intended to allow parts of it to be adapted to
other purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.1992</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.1992</id><created>2012-12-10</created><authors><author><keyname>Paszynski</keyname><forenames>Maciej</forenames></author><author><keyname>Calo</keyname><forenames>Victor</forenames></author><author><keyname>Pardo</keyname><forenames>David</forenames></author></authors><title>A direct solver with reutilization of previously-computed LU
  factorizations for h-adaptive finite element grids with point singularities</title><categories>cs.NA math.NA</categories><comments>21 papers, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a direct solver algorithm for a sequence of finite
element meshes that are h-refined towards one or several point singularities.
For such a sequence of grids, the solver delivers linear computational cost
O(N) in terms of CPU time and memory with respect to the number of unknowns N.
The linear computational cost is achieved by utilizing the recursive structure
provided by the sequence of h-adaptive grids with a special construction of the
elimination tree that allows for reutilization of previously computed partial
LU factorizations over the entire unrefined part of the computational mesh. The
reutilization technique reduces the computational cost of the entire sequence
of h-refined grids from O(N^2) down to O(N). Theoretical estimates are
illustrated with numerical results on two- and three-dimensional model problems
exhibiting one or several point singularities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2002</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2002</id><created>2012-12-10</created><updated>2012-12-20</updated><authors><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author></authors><title>A simpler approach to obtaining an O(1/t) convergence rate for the
  projected stochastic subgradient method</title><categories>cs.LG math.OC stat.ML</categories><comments>8 pages, 6 figures. Changes with previous version: Added reference to
  concurrently submitted work arXiv:1212.1824v1; clarifications added; typos
  corrected; title changed to 'subgradient method' as 'subgradient descent' is
  misnomer</comments><msc-class>90C15, 68T05, 65K10</msc-class><acm-class>G.1.6; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present a new averaging technique for the projected
stochastic subgradient method. By using a weighted average with a weight of t+1
for each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)
with both an easy proof and an easy implementation. The new scheme is compared
empirically to existing techniques, with similar performance behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2005</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2005</id><created>2012-12-10</created><authors><author><keyname>Hunsberger</keyname><forenames>Luke</forenames></author><author><keyname>Posenato</keyname><forenames>Roberto</forenames></author><author><keyname>Combi</keyname><forenames>Carlo</forenames></author></authors><title>The Dynamic Controllability of Conditional STNs with Uncertainty</title><categories>cs.AI cs.SY</categories><journal-ref>PlanEX Workshop, ICAPS-2012, pages 21-29, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent attempts to automate business processes and medical-treatment
processes have uncovered the need for a formal framework that can accommodate
not only temporal constraints, but also observations and actions with
uncontrollable durations. To meet this need, this paper defines a Conditional
Simple Temporal Network with Uncertainty (CSTNU) that combines the simple
temporal constraints from a Simple Temporal Network (STN) with the conditional
nodes from a Conditional Simple Temporal Problem (CSTP) and the contingent
links from a Simple Temporal Network with Uncertainty (STNU). A notion of
dynamic controllability for a CSTNU is defined that generalizes the dynamic
consistency of a CTP and the dynamic controllability of an STNU. The paper also
presents some sound constraint-propagation rules for dynamic controllability
that are expected to form the backbone of a dynamic-controllability-checking
algorithm for CSTNUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2006</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2006</id><created>2012-12-10</created><updated>2013-12-27</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Li</keyname><forenames>Sujian</forenames></author></authors><title>A Novel Feature-based Bayesian Model for Query Focused Multi-document
  Summarization</title><categories>cs.CL cs.IR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both supervised learning methods and LDA based topic model have been
successfully applied in the field of query focused multi-document
summarization. In this paper, we propose a novel supervised approach that can
incorporate rich sentence features into Bayesian topic models in a principled
way, thus taking advantages of both topic model and feature based supervised
learning methods. Experiments on TAC2008 and TAC2009 demonstrate the
effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2036</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2036</id><created>2012-12-10</created><updated>2013-12-31</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Li</keyname><forenames>Sujian</forenames></author></authors><title>Query-focused Multi-document Summarization: Combining a Novel Topic
  Model with Graph-based Semi-supervised Learning</title><categories>cs.CL cs.IR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based semi-supervised learning has proven to be an effective approach
for query-focused multi-document summarization. The problem of previous
semi-supervised learning is that sentences are ranked without considering the
higher level information beyond sentence level. Researches on general
summarization illustrated that the addition of topic level can effectively
improve the summary quality. Inspired by previous researches, we propose a
two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised
learning approach. At the same time, we propose a novel topic model which makes
full use of the dependence between sentences and words. Experimental results on
DUC and TAC data sets demonstrate the effectiveness of our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2044</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2044</id><created>2012-12-10</created><updated>2013-09-23</updated><authors><author><keyname>Kronberger</keyname><forenames>Gabriel</forenames></author><author><keyname>Fink</keyname><forenames>Stefan</forenames></author><author><keyname>Kommenda</keyname><forenames>Michael</forenames></author><author><keyname>Affenzeller</keyname><forenames>Michael</forenames></author></authors><title>Macro-Economic Time Series Modeling and Interaction Networks</title><categories>cs.NE stat.AP</categories><comments>The original publication is available at
  http://link.springer.com/chapter/10.1007/978-3-642-20520-0_11</comments><journal-ref>Applications of Evolutionary Computation, LNCS 6625 (Springer
  Berlin Heidelberg), pp. 101-110 (2011)</journal-ref><doi>10.1007/978-3-642-20520-0_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Macro-economic models describe the dynamics of economic quantities. The
estimations and forecasts produced by such models play a substantial role for
financial and political decisions. In this contribution we describe an approach
based on genetic programming and symbolic regression to identify variable
interactions in large datasets. In the proposed approach multiple symbolic
regression runs are executed for each variable of the dataset to find
potentially interesting models. The result is a variable interaction network
that describes which variables are most relevant for the approximation of each
variable of the dataset. This approach is applied to a macro-economic dataset
with monthly observations of important economic indicators in order to identify
potentially interesting dependencies of these indicators. The resulting
interaction network of macro-economic indicators is briefly discussed and two
of the identified models are presented in detail. The two models approximate
the help wanted index and the CPI inflation in the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2054</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2054</id><created>2012-12-10</created><authors><author><keyname>An</keyname><forenames>Dokjun</forenames></author><author><keyname>Ri</keyname><forenames>Myongchol</forenames></author><author><keyname>Choe</keyname><forenames>Changil</forenames></author><author><keyname>Han</keyname><forenames>Sunam</forenames></author><author><keyname>Kim</keyname><forenames>Yongmin</forenames></author></authors><title>SDMS-based Disk Encryption Method</title><categories>cs.CR</categories><report-no>KISU-MATH-2012-E-R-010</report-no><journal-ref>Journal of Mobile,Embeded and Distributed Systems, Vol.4, No.4,
  209-214, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a disk encryption method, called secure disk mixed system (SDMS)
in this paper, for data protection of disk storages such as USB flash memory,
USB hard disk and CD/DVD. It is aimed to solve temporal and spatial limitation
problems of existing disk encryption methods and to control security
performance flexibly according to the security requirement of system. SDMS
stores data by encrypting with different encryption key per sector and updates
sector encryption keys each time data is written. Security performance of SDMS
is analyzed at the end of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2056</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2056</id><created>2012-12-10</created><authors><author><keyname>Monreale</keyname><forenames>Giacoma Valentina</forenames></author><author><keyname>Montanari</keyname><forenames>Ugo</forenames></author><author><keyname>Hoch</keyname><forenames>Nicklas</forenames></author></authors><title>Soft Constraint Logic Programming for Electric Vehicle Travel
  Optimization</title><categories>cs.AI</categories><comments>17 pages; 26th Workshop on Logic Programming - 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft Constraint Logic Programming is a natural and flexible declarative
programming formalism, which allows to model and solve real-life problems
involving constraints of different types.
  In this paper, after providing a slightly more general and elegant
presentation of the framework, we show how we can apply it to the e-mobility
problem of coordinating electric vehicles in order to overcome both energetic
and temporal constraints and so to reduce their running cost. In particular, we
focus on the journey optimization sub-problem, considering sequences of trips
from a user's appointment to another one. Solutions provide the best
alternatives in terms of time and energy consumption, including route sequences
and possible charging events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2058</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2058</id><created>2012-12-10</created><updated>2014-12-26</updated><authors><author><keyname>Pawlik</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Laso&#x144;</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Trotter</keyname><forenames>William T.</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Triangle-free geometric intersection graphs with large chromatic number</title><categories>math.CO cs.CG cs.DM</categories><comments>Small corrections, bibliography update</comments><msc-class>05C62, 05C15</msc-class><journal-ref>Discrete Comput.Geom. 50 (2013) 714-726</journal-ref><doi>10.1007/s00454-013-9534-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several classical constructions illustrate the fact that the chromatic number
of a graph can be arbitrarily large compared to its clique number. However,
until very recently, no such construction was known for intersection graphs of
geometric objects in the plane. We provide a general construction that for any
arc-connected compact set $X$ in $\mathbb{R}^2$ that is not an axis-aligned
rectangle and for any positive integer $k$ produces a family $\mathcal{F}$ of
sets, each obtained by an independent horizontal and vertical scaling and
translation of $X$, such that no three sets in $\mathcal{F}$ pairwise intersect
and $\chi(\mathcal{F})&gt;k$. This provides a negative answer to a question of
Gyarfas and Lehel for L-shapes. With extra conditions, we also show how to
construct a triangle-free family of homothetic (uniformly scaled) copies of a
set with arbitrarily large chromatic number. This applies to many common
shapes, like circles, square boundaries, and equilateral L-shapes.
Additionally, we reveal a surprising connection between coloring geometric
objects in the plane and on-line coloring of intervals on the line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2064</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2064</id><created>2012-12-10</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>An Image Steganography Scheme using Randomized Algorithm and
  Context-Free Grammar</title><categories>cs.CR cs.MM</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</comments><journal-ref>Journal of Advanced Computer Science and Technology (JACST), Vol.
  1, No. 4, pp. 291-305, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, cryptography is in wide use as it is being exploited in various
domains from data confidentiality to data integrity and message authentication.
Basically, cryptography shuffles data so that they become unreadable by
unauthorized parties. However, clearly visible encrypted messages, no matter
how unbreakable, will arouse suspicions. A better approach would be to hide the
very existence of the message using steganography. Fundamentally, steganography
conceals secret data into innocent-looking mediums called carriers which can
then travel from the sender to the receiver safe and unnoticed. This paper
proposes a novel steganography scheme for hiding digital data into uncompressed
image files using a randomized algorithm and a context-free grammar. Besides,
the proposed scheme uses two mediums to deliver the secret data: a carrier
image into which the secret data are hidden into random pixels, and a
well-structured English text that encodes the location of the random carrier
pixels. The English text is generated at runtime using a context-free grammar
coupled with a lexicon of English words. The proposed scheme is stealthy, and
hard to be noticed, detected, and recovered. Experiments conducted showed how
the covering and the uncovering processes of the proposed scheme work. As
future work, a semantic analyzer is to be developed so as to make the English
text medium semantically correct, and consequently safer to be transmitted
without drawing any attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2065</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2065</id><created>2012-12-10</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Survey on Information Retrieval, Text Categorization, and Web Crawling</title><categories>cs.IR</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</comments><journal-ref>Journal of Computer Science &amp; Research (JCSCR), Vol. 1, No. 6, pp.
  1-11, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey discussing Information Retrieval concepts, methods,
and applications. It goes deep into the document and query modelling involved
in IR systems, in addition to pre-processing operations such as removing stop
words and searching by synonym techniques. The paper also tackles text
categorization along with its application in neural networks and machine
learning. Finally, the architecture of web crawlers is to be discussed shedding
the light on how internet spiders index web documents and how they allow users
to search for items on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2067</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2067</id><created>2012-12-10</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Generation-based Text Steganography Method using SQL Queries</title><categories>cs.CR</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</comments><journal-ref>International Journal of Computer Applications, vol. 57, no.12,
  pp.27-31, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography and Steganography are two techniques commonly used to secure and
safely transmit digital data. Nevertheless, they do differ in important ways.
In fact, cryptography scrambles data so that they become unreadable by
eavesdroppers; while, steganography hides the very existence of data so that
they can be transferred unnoticed. Basically, steganography is a technique for
hiding data such as messages into another form of data such as images.
Currently, many types of steganography are in use; however, there is yet no
known steganography application for query languages such as SQL. This paper
proposes a new steganography method for textual data. It encodes input text
messages into SQL carriers made up of SELECT queries. In effect, the output SQL
carrier is dynamically generated out of the input message using a dictionary of
words implemented as a hash table and organized into 65 categories, each of
which represents a particular character in the language. Generally speaking,
every character in the message to hide is mapped to a random word from a
corresponding category in the dictionary. Eventually, all input characters are
transformed into output words which are then put together to form an SQL query.
Experiments conducted, showed how the proposed method can operate on real
examples proving the theory behind it. As future work, other types of SQL
queries are to be researched including INSERT, DELETE, and UPDATE queries,
making the SQL carrier quite puzzling for malicious third parties to recuperate
the secret message that it encodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2071</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2071</id><created>2012-12-10</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Data Warehouse Design for a Typical University Information System</title><categories>cs.DB</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</comments><journal-ref>Journal of Computer Science &amp; Research (JCSCR), vol. 1, no. 6, pp.
  12-17, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presently, large enterprises rely on database systems to manage their data
and information. These databases are useful for conducting daily business
transactions. However, the tight competition in the marketplace has led to the
concept of data mining in which data are analyzed to derive effective business
strategies and discover better ways in carrying out business. In order to
perform data mining, regular databases must be converted into what so called
informational databases also known as data warehouse. This paper presents a
design model for building data warehouse for a typical university information
system. It is based on transforming an operational database into an
informational warehouse useful for decision makers to conduct data analysis,
predication, and forecasting. The proposed model is based on four stages of
data migration: Data extraction, data cleansing, data transforming, and data
indexing and loading. The complete system is implemented under MS Access 2010
and is meant to serve as a repository of data for data mining operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2094</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2094</id><created>2012-12-10</created><authors><author><keyname>Ellings&#xe6;ter</keyname><forenames>Brage</forenames></author></authors><title>Secondary Access to Spectrum with SINR Requirements Through Constraint
  Transformation</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of allocating spectrum among radio
nodes under SINR requirements. This problem is of special interest in dynamic
spectrum access networks where topology and spectral resources differ with time
and location. The problem is to determine the number of radio nodes that can
transmit simultaneously while still achieving their SINR requirements and then
decide which channels these nodes should transmit on. Previous work have shown
how this can be done for a large spectrum pool where nodes allocate multiple
channels from that pool which renders a linear programming approach feasible
when the pool is large enough. In this paper we extend their work by
considering arbitrary individual pool sizes and allow nodes to only transmit on
one channel. Due to the accumulative nature of interference this problem is a
non-convex integer problem which is NP-hard. However, we introduce a constraint
transformation that transforms the problem to a binary quadratic constraint
problem. Although this problem is still NP-hard, well known heuristic
algorithms for solving this problem are known in the literature. We implement a
heuristic algorithm based on Lagrange relaxation which bounds the solution
value of the heuristic to the optimal value of the constraint transformed
problem. Simulation results show that this approach provides solutions within
an average gap of 10% of solutions obtained by a genetic algorithm for the
original non-convex integer problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2125</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2125</id><created>2012-12-10</created><authors><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author><author><keyname>Tatikonda</keyname><forenames>Sekhar</forenames></author></authors><title>Sparse Regression Codes for Multi-terminal Source and Channel Coding</title><categories>cs.IT math.IT</categories><comments>9 pages, appeared in the Proceedings of the 50th Annual Allerton
  Conference on Communication, Control, and Computing - 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new class of codes for Gaussian multi-terminal source and channel
coding. These codes are designed using the statistical framework of
high-dimensional linear regression and are called Sparse Superposition or
Sparse Regression codes. Codewords are linear combinations of subsets of
columns of a design matrix. These codes were recently introduced by Barron and
Joseph and shown to achieve the channel capacity of AWGN channels with
computationally feasible decoding. They have also recently been shown to
achieve the optimal rate-distortion function for Gaussian sources. In this
paper, we demonstrate how to implement random binning and superposition coding
using sparse regression codes. In particular, with minimum-distance
encoding/decoding it is shown that sparse regression codes attain the optimal
information-theoretic limits for a variety of multi-terminal source and channel
coding problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2129</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2129</id><created>2012-12-10</created><updated>2013-05-19</updated><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames></author></authors><title>Online Portfolio Selection: A Survey</title><categories>q-fin.CP cs.AI cs.CE q-fin.PM</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online portfolio selection is a fundamental problem in computational finance,
which has been extensively studied across several research communities,
including finance, statistics, artificial intelligence, machine learning, and
data mining, etc. This article aims to provide a comprehensive survey and a
structural understanding of published online portfolio selection techniques.
From an online machine learning perspective, we first formulate online
portfolio selection as a sequential decision problem, and then survey a variety
of state-of-the-art approaches, which are grouped into several major
categories, including benchmarks, &quot;Follow-the-Winner&quot; approaches,
&quot;Follow-the-Loser&quot; approaches, &quot;Pattern-Matching&quot; based approaches, and
&quot;Meta-Learning Algorithms&quot;. In addition to the problem formulation and related
algorithms, we also discuss the relationship of these algorithms with the
Capital Growth theory in order to better understand the similarities and
differences of their underlying trading ideas. This article aims to provide a
timely and comprehensive survey for both machine learning and data mining
researchers in academia and quantitative portfolio managers in the financial
industry to help them understand the state-of-the-art and facilitate their
research and practical applications. We also discuss some open issues and
evaluate some emerging new trends for future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2136</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2136</id><created>2012-12-10</created><updated>2013-06-18</updated><authors><author><keyname>Flach</keyname><forenames>Boris</forenames></author></authors><title>A class of random fields on complete graphs with tractable partition
  function</title><categories>cs.LG stat.ML</categories><comments>accepted for publication in IEEE TPAMI (short paper)</comments><doi>10.1109/TPAMI.2013.99</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this short note is to draw attention to a method by which the
partition function and marginal probabilities for a certain class of random
fields on complete graphs can be computed in polynomial time. This class
includes Ising models with homogeneous pairwise potentials but arbitrary
(inhomogeneous) unary potentials. Similarly, the partition function and
marginal probabilities can be computed in polynomial time for random fields on
complete bipartite graphs, provided they have homogeneous pairwise potentials.
We expect that these tractable classes of large scale random fields can be very
useful for the evaluation of approximation algorithms by providing exact error
estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2142</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2142</id><created>2012-12-10</created><updated>2013-01-24</updated><authors><author><keyname>Chatterjee</keyname><forenames>Arnab</forenames></author><author><keyname>Mitrovi&#x107;</keyname><forenames>Marija</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Universality in voting behavior: an empirical analysis</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>19 pages, 10 figures, 8 tables. The elections data-sets can be
  downloaded from http://becs.aalto.fi/en/research/complex_systems/elections/</comments><journal-ref>Scientific Reports 3, 1049 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Election data represent a precious source of information to study human
behavior at a large scale. In proportional elections with open lists, the
number of votes received by a candidate, rescaled by the average performance of
all competitors in the same party list, has the same distribution regardless of
the country and the year of the election. Here we provide the first thorough
assessment of this claim. We analyzed election datasets of 15 countries with
proportional systems. We confirm that a class of nations with similar election
rules fulfill the universality claim. Discrepancies from this trend in other
countries with open-lists elections are always associated with peculiar
differences in the election rules, which matter more than differences between
countries and historical periods. Our analysis shows that the role of parties
in the electoral performance of candidates is crucial: alternative scalings not
taking into account party affiliations lead to poor results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2144</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2144</id><created>2012-12-10</created><authors><author><keyname>Velimirovic</keyname><forenames>Lazar</forenames></author><author><keyname>Peric</keyname><forenames>Zoran</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir</forenames></author><author><keyname>Simic</keyname><forenames>Nikola</forenames></author></authors><title>Design of companding quantizer for Laplacian source using the
  approximation of probability density function</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper both piecewise linear and piecewise uniform approximation of
probability density function are performed. For the probability density
function approximated in these ways, a compressor function is formed. On the
basis of compressor function formed in this way, piecewise linear and piecewise
uniform companding quantizer are designed. Design of these companding quantizer
models is performed for the Laplacian source at the entrance of the quantizer.
The performance estimate of the proposed companding quantizer models is done by
determining the values of signal to quantization noise ratio (SQNR) and
approximation error for the both of proposed models and also by their mutual
comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2145</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2145</id><created>2012-12-10</created><authors><author><keyname>Yang</keyname><forenames>Shuang-Hong</forenames></author></authors><title>A Scale-Space Theory for Text</title><categories>cs.IR cs.CL</categories><comments>9 pages, 6 figures; Nature language processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scale-space theory has been established primarily by the computer vision and
signal processing communities as a well-founded and promising framework for
multi-scale processing of signals (e.g., images). By embedding an original
signal into a family of gradually coarsen signals parameterized with a
continuous scale parameter, it provides a formal framework to capture the
structure of a signal at different scales in a consistent way. In this paper,
we present a scale space theory for text by integrating semantic and spatial
filters, and demonstrate how natural language documents can be understood,
processed and analyzed at multiple resolutions, and how this scale-space
representation can be used to facilitate a variety of NLP and text analysis
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2150</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2150</id><created>2012-12-10</created><authors><author><keyname>Yang</keyname><forenames>Shuang-Hong</forenames></author></authors><title>Collaborative Competitive filtering II: Optimal Recommendation and
  Collaborative Games</title><categories>cs.IR</categories><comments>10 pages, 5 figures; Recommender system, Collaborative filtering</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems have emerged as a new weapon to help online firms to
realize many of their strategic goals (e.g., to improve sales, revenue,
customer experience etc.). However, many existing techniques commonly approach
these goals by seeking to recover preference (e.g., estimating ratings) in a
matrix completion framework. This paper aims to bridge this significant gap
between the clearly-defined strategic objectives and the not-so-well-justified
proxy.
  We show it is advantageous to think of a recommender system as an analogy to
a monopoly economic market with the system as the sole seller, users as the
buyers and items as the goods. This new perspective motivates a game-theoretic
formulation for recommendation that enables us to identify the optimal
recommendation policy by explicit optimizing certain strategic goals. In this
paper, we revisit and extend our prior work, the Collaborative-Competitive
Filtering preference model, towards a game-theoretic framework. The proposed
framework consists of two components. First, a conditional preference model
that characterizes how a user would respond to a recommendation action; Second,
knowing in advance how the user would respond, how a recommender system should
act (i.e., recommend) strategically to maximize its goals. We show how
objectives such as click-through rate, sales revenue and consumption diversity
can be optimized explicitly in this framework. Experiments are conducted on a
commercial recommender system and demonstrate promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2153</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2153</id><created>2012-12-10</created><updated>2013-03-04</updated><authors><author><keyname>Cardillo</keyname><forenames>Alessio</forenames></author><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Zanin</keyname><forenames>Massimiliano</forenames></author><author><keyname>Romance</keyname><forenames>Miguel</forenames></author><author><keyname>Papo</keyname><forenames>David</forenames></author><author><keyname>del Pozo</keyname><forenames>Francisco</forenames></author><author><keyname>Boccaletti</keyname><forenames>Stefano</forenames></author></authors><title>Emergence of network features from multiplexity</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 3 figures. Submitted for publication</comments><journal-ref>Scientific Reports 3, 1344 (2013)</journal-ref><doi>10.1038/srep01344</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many biological and man-made networked systems are characterized by the
simultaneous presence of different sub-networks organized in separate layers,
with links and nodes of qualitatively different types. While during the past
few years theoretical studies have examined a variety of structural features of
complex networks, the outstanding question is whether such features are
characterizing all single layers, or rather emerge as a result of
coarse-graining, i.e. when going from the multilayered to the aggregate network
representation. Here we address this issue with the help of real data. We
analyze the structural properties of an intrinsically multilayered real
network, the European Air Transportation Multiplex Network in which each
commercial airline defines a network layer. We examine how several structural
measures evolve as layers are progressively merged together. In particular, we
discuss how the topology of each layer affects the emergence of structural
properties in the aggregate network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2154</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2154</id><created>2012-11-26</created><authors><author><keyname>Li</keyname><forenames>Yongming</forenames></author><author><keyname>Lei</keyname><forenames>Lihui</forenames></author></authors><title>Model-Checking of Linear-Time Properties in Multi-Valued Systems</title><categories>cs.LO cs.FL</categories><comments>50 pages, 9 figures, 2 tables</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study model-checking of linear-time properties in
multi-valued systems. Safety property, invariant property, liveness property,
persistence and dual-persistence properties in multi-valued logic systems are
introduced. Some algorithms related to the above multi-valued linear-time
properties are discussed. The verification of multi-valued regular safety
properties and multi-valued $\omega$-regular properties using lattice-valued
automata are thoroughly studied. Since the law of non-contradiction (i.e.,
$a\wedge \neg a=0$) and the law of excluded-middle (i.e., $a\vee \neg a=1$) do
not hold in multi-valued logic, the linear-time properties introduced in this
paper have the new forms compared to those in classical logic. Compared to
those classical model checking methods, our methods to multi-valued model
checking are more directly accordingly. A new form of multi-valued model
checking with membership degree is also introduced. In particular, we show that
multi-valued model-checking can be reduced to the classical model checking. The
related verification algorithms are also presented. Some illustrative examples
and case study are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2170</identifier>
 <datestamp>2013-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2170</id><created>2012-12-10</created><updated>2013-09-24</updated><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Sirbu</keyname><forenames>Mihai</forenames></author></authors><title>Stochastic Perron's method for Hamilton-Jacobi-Bellman equations</title><categories>math.PR cs.SY math.AP math.OC</categories><comments>Final version. To appear in the SIAM Journal on Control and
  Optimization. Keywords: Perron's method, viscosity solutions, non-smooth
  verification, comparison principle</comments><msc-class>Primary 49L20, 49L25, 60G46, Secondary 60H30, 35Q93, 35D40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the value function of a stochastic control problem is the unique
solution of the associated Hamilton-Jacobi-Bellman (HJB) equation, completely
avoiding the proof of the so-called dynamic programming principle (DPP). Using
Stochastic Perron's method we construct a super-solution lying below the value
function and a sub-solution dominating it. A comparison argument easily closes
the proof. The program has the precise meaning of verification for
viscosity-solutions, obtaining the DPP as a conclusion. It also immediately
follows that the weak and strong formulations of the stochastic control problem
have the same value. Using this method we also capture the possible
face-lifting phenomenon in a straightforward manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2178</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2178</id><created>2012-12-10</created><updated>2014-11-05</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Iglesias</keyname><forenames>Jennifer</forenames></author><author><keyname>Migler</keyname><forenames>Theresa</forenames></author><author><keyname>Ochoa</keyname><forenames>Antonio</forenames></author><author><keyname>Wilfong</keyname><forenames>Gordon</forenames></author><author><keyname>Zhang</keyname><forenames>Lisa</forenames></author></authors><title>Egalitarian Graph Orientations</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph, one can assign directions to each of the edges of
the graph, thus orienting the graph. To be as egalitarian as possible, one may
wish to find an orientation such that no vertex is unfairly hit with too many
arcs directed into it. We discuss how this objective arises in problems
resulting from telecommunications. We give optimal, polynomial-time algorithms
for: finding an orientation that minimizes the lexicographic order of the
indegrees and finding a strongly-connected orientation that minimizes the
maximum indegree. We show that minimizing the lexicographic order of the
indegrees is NP-hard when the resulting orientation is required to be acyclic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2207</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2207</id><created>2012-12-10</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Two Intermediates Audio Steganography Technique</title><categories>cs.CR</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org. arXiv admin note: substantial text overlap with
  arXiv:1212.2064</comments><journal-ref>Journal of Emerging Trends in Computing and Information Sciences
  (CIS), vol. 3, no.11, pp.1459-1465, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the rise of the Internet, digital data became openly public which has
driven IT industries to pay special consideration to data confidentiality. At
present, two main techniques are being used: Cryptography and Steganography. In
effect, cryptography garbles a secret message turning it into a meaningless
form; while, steganography hides the very existence of the message by embedding
it into an intermediate such as a computer file. In fact, in audio
steganography, this computer file is a digital audio file in which secret data
are concealed, predominantly, into the bits that make up its audio samples.
This paper proposes a novel steganography technique for hiding digital data
into uncompressed audio files using a randomized algorithm and a context-free
grammar coupled with a lexicon of words. Furthermore, the proposed technique
uses two intermediates to transmit the secret data between communicating
parties: The first intermediate is an audio file whose audio samples, which are
selected randomly, are used to conceal the secret data; whereas, the second
intermediate is a grammatically correct English text that is generated at
runtime using a context-free grammar and it encodes the location of the random
audio samples in the audio file. The proposed technique is stealthy and
irrecoverable in a sense that it is difficult for unauthorized third parties to
detect the presence of and recover the secret data. Experiments conducted
showed how the covering and the uncovering processes of the proposed technique
work. As future work, a semantic analyzer is to be developed so as to make the
intermediate text not only grammatically correct but also semantically
plausible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2236</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2236</id><created>2012-12-10</created><authors><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Core stability in hedonic coalition formation</title><categories>cs.GT cs.DM cs.DS math.CO</categories><comments>18 pages</comments><msc-class>68R01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many economic, social and political situations individuals carry out
activities in groups (coalitions) rather than alone and on their own. Examples
range from households and sport clubs to research networks, political parties
and trade unions. The underlying game theoretic framework is known as coalition
formation.
  This survey discusses the notion of core stability in hedonic coalition
formation (where each player's happiness only depends on the other members of
his coalition but not on how the remaining players outside his coalition are
grouped). We present the central concepts and algorithmic approaches in the
area, provide many examples, and pose a number of open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2244</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2244</id><created>2012-12-10</created><authors><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Jahan</keyname><forenames>Ishrat</forenames></author><author><keyname>Ahearn</keyname><forenames>Matt</forenames></author><author><keyname>Holley</keyname><forenames>Julian</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Initiation of waves in BZ encapsulated vesicles using light - towards
  design of computing architectures</title><categories>nlin.PS cs.ET nlin.AO physics.chem-ph</categories><comments>20 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A gas free analogue of the Belousov-Zhabotinsky reaction catalysed by ferroin
and encapsulated in phospholipid stabilised vesicles is reported. A reaction
mixture which exhibits spontaneous oscillation and excitation transfer between
vesicles was formulated. By adjusting the reagent concentrations a quiescent
state with fewer spontaneous oscillations was achieved. Using relatively low
power laser sources of specific wavelengths (green 532nm and blue 405nm) it was
shown that waves could be reproducibly initiated within the BZ vesicles.
Furthermore, despite the reduced excitability of the system overall the
initiated waves exhibited vesicle to vesicle transfer. It was possible to
manipulate single vesicles and design simple circuits based on a 2D validation
of collision based circuits. Therefore, we conclude that this BZ system
exhibits promise for computing applications based on 3D networks of vesicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2245</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2245</id><created>2012-12-10</created><authors><author><keyname>Welk</keyname><forenames>Martin</forenames></author><author><keyname>Raudaschl</keyname><forenames>Patrik</forenames></author><author><keyname>Schwarzbauer</keyname><forenames>Thomas</forenames></author><author><keyname>Erler</keyname><forenames>Martin</forenames></author><author><keyname>L&#xe4;uter</keyname><forenames>Martin</forenames></author></authors><title>Fast and Robust Linear Motion Deblurring</title><categories>cs.CV</categories><acm-class>I.4.4; G.1.9</acm-class><doi>10.1007/s11760-013-0563-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate efficient algorithmic realisations for robust deconvolution of
grey-value images with known space-invariant point-spread function, with
emphasis on 1D motion blur scenarios. The goal is to make deconvolution
suitable as preprocessing step in automated image processing environments with
tight time constraints. Candidate deconvolution methods are selected for their
restoration quality, robustness and efficiency. Evaluation of restoration
quality and robustness on synthetic and real-world test images leads us to
focus on a combination of Wiener filtering with few iterations of robust and
regularised Richardson-Lucy deconvolution. We discuss algorithmic optimisations
for specific scenarios. In the case of uniform linear motion blur in coordinate
direction, it is possible to achieve real-time performance (less than 50 ms) in
single-threaded CPU computation on images of $256\times256$ pixels. For more
general space-invariant blur settings, still favourable computation times are
obtained. Exemplary parallel implementations demonstrate that the proposed
method also achieves real-time performance for general 1D motion blurs in a
multi-threaded CPU setting, and for general 2D blurs on a GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2251</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2251</id><created>2012-12-10</created><authors><author><keyname>Davidson</keyname><forenames>Susan B.</forenames></author><author><keyname>Milo</keyname><forenames>Tova</forenames></author><author><keyname>Roy</keyname><forenames>Sudeepa</forenames></author></authors><title>A Propagation Model for Provenance Views of Public/Private Workflows</title><categories>cs.DB</categories><comments>To appear in ICDT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of concealing functionality of a proprietary or private
module when provenance information is shown over repeated executions of a
workflow which contains both `public' and `private' modules. Our approach is to
use `provenance views' to hide carefully chosen subsets of data over all
executions of the workflow to ensure G-privacy: for each private module and
each input x, the module's output f(x) is indistinguishable from G -1 other
possible values given the visible data in the workflow executions. We show that
G-privacy cannot be achieved simply by combining solutions for individual
private modules; data hiding must also be `propagated' through public modules.
We then examine how much additional data must be hidden and when it is safe to
stop propagating data hiding. The answer depends strongly on the workflow
topology as well as the behavior of public modules on the visible data. In
particular, for a class of workflows (which include the common tree and chain
workflows), taking private solutions for each private module, augmented with a
`public closure' that is `upstream-downstream safe', ensures G-privacy. We
define these notions formally and show that the restrictions are necessary. We
also study the related optimization problems of minimizing the amount of hidden
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2257</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2257</id><created>2012-12-10</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author><author><keyname>Zhou</keyname><forenames>Yong</forenames></author></authors><title>A Process Calculus with Logical Operators</title><categories>cs.LO</categories><comments>52 pages</comments><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to combine operational and logical styles of specifications in one
unified framework, the notion of logic labelled transition systems (Logic LTS,
for short) has been presented and explored by L\&quot;{u}ttgen and Vogler in [TCS
373(1-2):19-40; Inform. &amp; Comput. 208:845-867]. In contrast with usual LTS, two
logical constructors $\wedge$ and $\vee$ over Logic LTSs are introduced to
describe logical combinations of specifications. Hitherto such framework has
been dealt with in considerable depth, however, process algebraic style way has
not yet been involved and the axiomatization of constructors over Logic LTSs is
absent. This paper tries to develop L\&quot;{u}ttgen and Vogler's work along this
direction. We will present a process calculus for Logic LTSs (CLL, for short).
The language CLL is explored in detail from two different but equivalent views.
Based on behavioral view, the notion of ready simulation is adopted to
formalize the refinement relation, and the behavioral theory is developed.
Based on proof-theoretic view, a sound and ground-complete axiomatic system for
CLL is provided, which captures operators in CLL through (in)equational laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2262</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2262</id><created>2012-12-10</created><authors><author><keyname>Wang</keyname><forenames>Jin</forenames></author><author><keyname>Liu</keyname><forenames>Ping</forenames></author><author><keyname>She</keyname><forenames>Mary F. H.</forenames></author><author><keyname>Nahavandi</keyname><forenames>Saeid</forenames></author><author><keyname>Kouzani</keyname><forenames>and Abbas</forenames></author></authors><title>Bag-of-Words Representation for Biomedical Time Series Classification</title><categories>cs.LG cs.AI</categories><comments>10 pages, 7 figures. Submitted to IEEE Transaction on Biomedical
  Engineering</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Automatic analysis of biomedical time series such as electroencephalogram
(EEG) and electrocardiographic (ECG) signals has attracted great interest in
the community of biomedical engineering due to its important applications in
medicine. In this work, a simple yet effective bag-of-words representation that
is able to capture both local and global structure similarity information is
proposed for biomedical time series representation. In particular, similar to
the bag-of-words model used in text document domain, the proposed method treats
a time series as a text document and extracts local segments from the time
series as words. The biomedical time series is then represented as a histogram
of codewords, each entry of which is the count of a codeword appeared in the
time series. Although the temporal order of the local segments is ignored, the
bag-of-words representation is able to capture high-level structural
information because both local and global structural information are well
utilized. The performance of the bag-of-words model is validated on three
datasets extracted from real EEG and ECG signals. The experimental results
demonstrate that the proposed method is not only insensitive to parameters of
the bag-of-words model such as local segment length and codebook size, but also
robust to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2264</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2264</id><created>2012-12-10</created><updated>2013-12-03</updated><authors><author><keyname>Jha</keyname><forenames>Madhav</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>A space efficient streaming algorithm for triangle counting using the
  birthday paradox</title><categories>cs.DS cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a space efficient algorithm that approximates the transitivity
(global clustering coefficient) and total triangle count with only a single
pass through a graph given as a stream of edges. Our procedure is based on the
classic probabilistic result, the birthday paradox. When the transitivity is
constant and there are more edges than wedges (common properties for social
networks), we can prove that our algorithm requires $O(\sqrt{n})$ space ($n$ is
the number of vertices) to provide accurate estimates. We run a detailed set of
experiments on a variety of real graphs and demonstrate that the memory
requirement of the algorithm is a tiny fraction of the graph. For example, even
for a graph with 200 million edges, our algorithm stores just 60,000 edges to
give accurate results. Being a single pass streaming algorithm, our procedure
also maintains a real-time estimate of the transitivity/number of triangles of
a graph, by storing a minuscule fraction of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2278</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2278</id><created>2012-12-10</created><updated>2013-05-05</updated><authors><author><keyname>Vondrick</keyname><forenames>Carl</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Malisiewicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Inverting and Visualizing Features for Object Detection</title><categories>cs.CV</categories><comments>This paper is a preprint of our conference paper. We have made it
  available early in the hopes that others find it useful</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce algorithms to visualize feature spaces used by object detectors.
The tools in this paper allow a human to put on `HOG goggles' and perceive the
visual world as a HOG based object detector sees it. We found that these
visualizations allow us to analyze object detection systems in new ways and
gain new insight into the detector's failures. For example, when we visualize
the features for high scoring false alarms, we discovered that, although they
are clearly wrong in image space, they do look deceptively similar to true
positives in feature space. This result suggests that many of these false
alarms are caused by our choice of feature space, and indicates that creating a
better learning algorithm or building bigger datasets is unlikely to correct
these errors. By visualizing feature spaces, we can gain a more intuitive
understanding of our detection systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2281</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2281</id><created>2012-12-10</created><authors><author><keyname>S</keyname><forenames>Ashok kumar. P.</forenames></author><author><keyname>Mahadevan</keyname><forenames>G.</forenames></author><author><keyname>C</keyname><forenames>Gopal Krishna.</forenames></author></authors><title>Recapitulation of Web Services based on Tree Structure</title><categories>cs.SE cs.NI</categories><comments>5pages,3 figures, 2 tables, IJCA conference</comments><journal-ref>ACCTHPCA, June 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Recapitulation of Web service is an approach for the effective
integration of distributed, heterogeneous and autonomous applications to build
more Structured and value added services. Web services selection algorithms are
required to find and select the best services. A QoS is a benchmark to select
the best service for the task of composition. The importance of the web
services selection algorithm is to maximize the QoS of the web services
recapitulation using complex service provider's (CSP) QoS requirement
parameters. This paper have discussed about different Web service
Service-offers. We have classified the CSP's requirements defined on the QoS
and Service-offers based on its structure. We have proposed a tree structure to
represent the CSP's requirement to be defined based on the multiple QoS
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2284</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2284</id><created>2012-12-10</created><updated>2013-08-06</updated><authors><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Williams</keyname><forenames>Tyson</forenames></author></authors><title>The Complexity of Planar Boolean #CSP with Complex Weights</title><categories>cs.CC cs.DS</categories><comments>38 pages, 12 figures, preliminary version appeared at ICALP 2013.
  arXiv admin note: text overlap with arXiv:1207.2354, arXiv:1008.0683 by other
  authors</comments><msc-class>68Q17</msc-class><acm-class>F.1.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a complexity dichotomy theorem for symmetric complex-weighted
Boolean #CSP when the constraint graph of the input must be planar. The
problems that are #P-hard over general graphs but tractable over planar graphs
are precisely those with a holographic reduction to matchgates. This
generalizes a theorem of Cai, Lu, and Xia for the case of real weights. We also
obtain a dichotomy theorem for a symmetric arity 4 signature with complex
weights in the planar Holant framework, which we use in the proof of our #CSP
dichotomy. In particular, we reduce the problem of evaluating the Tutte
polynomial of a planar graph at the point (3,3) to counting the number of
Eulerian orientations over planar 4-regular graphs to show the latter is
#P-hard. This strengthens a theorem by Huang and Lu to the planar setting. Our
proof techniques combine new ideas with refinements and extensions of existing
techniques. These include planar pairings, the recursive unary construction,
the anti-gadget technique, and pinning in the Hadamard basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2287</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2287</id><created>2012-12-10</created><updated>2013-04-26</updated><authors><author><keyname>Asadi</keyname><forenames>Nima</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author><author><keyname>de Vries</keyname><forenames>Arjen P.</forenames></author></authors><title>Runtime Optimizations for Prediction with Tree-Based Models</title><categories>cs.DB cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree-based models have proven to be an effective solution for web ranking as
well as other problems in diverse domains. This paper focuses on optimizing the
runtime performance of applying such models to make predictions, given an
already-trained model. Although exceedingly simple conceptually, most
implementations of tree-based models do not efficiently utilize modern
superscalar processor architectures. By laying out data structures in memory in
a more cache-conscious fashion, removing branches from the execution flow using
a technique called predication, and micro-batching predictions using a
technique called vectorization, we are able to better exploit modern processor
architectures and significantly improve the speed of tree-based models over
hard-coded if-else blocks. Our work contributes to the exploration of
architecture-conscious runtime implementations of machine learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2291</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2291</id><created>2012-12-10</created><updated>2013-04-12</updated><authors><author><keyname>Kim</keyname><forenames>MinJi</forenames></author><author><keyname>Cloud</keyname><forenames>Jason</forenames></author><author><keyname>ParandehGheibi</keyname><forenames>Ali</forenames></author><author><keyname>Urbina</keyname><forenames>Leonardo</forenames></author><author><keyname>Fouli</keyname><forenames>Kerim</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Network Coded TCP (CTCP)</title><categories>cs.NI</categories><comments>12 pages, 12 figures, 1 table, submitted to ACM Mobicom 2013.
  (Revised acknowledgements). arXiv admin note: substantial text overlap with
  arXiv:1212.1929</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce CTCP, a reliable transport protocol using network coding. CTCP
is designed to incorporate TCP features such as congestion control,
reliability, and fairness while significantly improving on TCP's performance in
lossy, interference-limited and/or dynamic networks. A key advantage of
adopting a transport layer over a link layer approach is that it provides
backward compatibility with wireless equipment installed throughout existing
networks. We present a portable userspace implementation of CTCP and
extensively evaluate its performance in both testbed and production wireless
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2303</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2303</id><created>2012-12-11</created><authors><author><keyname>Ezra</keyname><forenames>Esther</forenames></author></authors><title>Small-Size Relative (p,Epsilon)-Approximations for Well-Behaved Range
  Spaces</title><categories>cs.CG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present improved upper bounds for the size of relative
(p,Epsilon)-approximation for range spaces with the following property: For any
(finite) range space projected onto (that is, restricted to) a ground set of
size n and for any parameter 1 &lt;= k &lt;= n, the number of ranges of size at most
k is only nearly-linear in n and polynomial in k. Such range spaces are called
&quot;well behaved&quot;. Our bound is an improvement over the bound O(\log{(1/p)/\eps^2
p) introduced by Li etal. for the general case (where this bound has been shown
to be tight in the worst case), when p &lt;&lt; Epsilon. We also show that such small
size relative (p,Epsilon)-approximations can be constructed in expected
polynomial time.
  Our bound also has an interesting interpretation in the context of &quot;p-nets&quot;:
As observed by Har-Peled and Sharir, p-nets are special cases of relative
(p,Epsilon)-approximations. Specifically, when Epsilon is a constant smaller
than 1, their analysis implies that there are p-nets of size O(\log{(1/p)}/p)
that are \emph{also} relative approximations. In this context our construction
significantly improves this bound for well-behaved range spaces. Despite the
progress in the theory of p-nets and the existence of improved bounds
corresponding to the cases that we study, these bounds do not necessarily
guarantee a bounded relative error.
  Lastly, we present several geometric scenarios of well-behaved range spaces,
and show the resulting bound for each of these cases obtained as a consequence
of our analysis. In particular, when Epsilon is a constant smaller than 1, our
bound for points and axis-parallel boxes in two and three dimensions, as well
as points and &quot;fat&quot; triangles in the plane, matches the optimal bound for
p-nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2308</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2308</id><created>2012-12-11</created><updated>2014-02-20</updated><authors><author><keyname>Sakuma</keyname><forenames>Tadashi</forenames></author></authors><title>On the balanced decomposition number</title><categories>math.CO cs.DM</categories><msc-class>05C15, 05C70, 05D15, 05C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\em balanced coloring} of a graph $G$ means a triple $\{P_1,P_2,X\}$ of
mutually disjoint subsets of the vertex-set $V(G)$ such that $V(G)=P_1 \uplus
P_2 \uplus X$ and $|P_1|=|P_2|$. A {\em balanced decomposition} associated with
the balanced coloring $V(G)=P_1 \uplus P_2 \uplus X$ of $G$ is defined as a
partition of $V(G)=V_1 \uplus \cdots \uplus V_r$ (for some $r$) such that, for
every $i \in \{1,\cdots,r\}$, the subgraph $G[V_i]$ of $G$ is connected and
$|V_i \cap P_1| = |V_i \cap P_2|$. Then the {\em balanced decomposition number}
of a graph $G$ is defined as the minimum integer $s$ such that, for every
balanced coloring $V(G)=P_1 \uplus P_2 \uplus X$ of $G$, there exists a
balanced decomposition $V(G)=V_1 \uplus \cdots \uplus V_r$ whose every element
$V_i (i=1, \cdots, r)$ has at most $s$ vertices. S. Fujita and H. Liu [\/SIAM
J. Discrete Math. 24, (2010), pp. 1597--1616\/] proved a nice theorem which
states that the balanced decomposition number of a graph $G$ is at most $3$ if
and only if $G$ is $\lfloor\frac{|V(G)|}{2}\rfloor$-connected. Unfortunately,
their proof is lengthy (about 10 pages) and complicated. Here we give an
immediate proof of the theorem. This proof makes clear a relationship between
balanced decomposition number and graph matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2309</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2309</id><created>2012-12-11</created><authors><author><keyname>Yuan</keyname><forenames>Ganzhao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenjie</forenames></author><author><keyname>Winslett</keyname><forenames>Marianne</forenames></author><author><keyname>Xiao</keyname><forenames>Xiaokui</forenames></author><author><keyname>Yang</keyname><forenames>Yin</forenames></author><author><keyname>Hao</keyname><forenames>Zhifeng</forenames></author></authors><title>Low Rank Mechanism for Optimizing Batch Queries under Differential
  Privacy</title><categories>cs.DB cs.CR</categories><comments>VLDB 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a promising privacy-preserving paradigm for
statistical query processing over sensitive data. It works by injecting random
noise into each query result, such that it is provably hard for the adversary
to infer the presence or absence of any individual record from the published
noisy results. The main objective in differentially private query processing is
to maximize the accuracy of the query results, while satisfying the privacy
guarantees. Previous work, notably \cite{LHR+10}, has suggested that with an
appropriate strategy, processing a batch of correlated queries as a whole
achieves considerably higher accuracy than answering them individually.
However, to our knowledge there is currently no practical solution to find such
a strategy for an arbitrary query batch; existing methods either return
strategies of poor quality (often worse than naive methods) or require
prohibitively expensive computations for even moderately large domains.
  Motivated by this, we propose the \emph{Low-Rank Mechanism} (LRM), the first
practical differentially private technique for answering batch queries with
high accuracy, based on a \emph{low rank approximation} of the workload matrix.
We prove that the accuracy provided by LRM is close to the theoretical lower
bound for any mechanism to answer a batch of queries under differential
privacy. Extensive experiments using real data demonstrate that LRM
consistently outperforms state-of-the-art query processing solutions under
differential privacy, by large margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2310</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2310</id><created>2012-12-11</created><updated>2014-03-19</updated><authors><author><keyname>Sattari</keyname><forenames>Pegah</forenames></author><author><keyname>Kurant</keyname><forenames>Maciej</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Active Learning of Multiple Source Multiple Destination Topologies</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Signal Processing, Vol. 62, Issue 8, pp.
  1926-1937, April 2014</journal-ref><doi>10.1109/TSP.2014.2304431</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of inferring the topology of a network with $M$
sources and $N$ receivers (hereafter referred to as an $M$-by-$N$ network), by
sending probes between the sources and receivers. Prior work has shown that
this problem can be decomposed into two parts: first, infer smaller subnetwork
components (i.e., $1$-by-$N$'s or $2$-by-$2$'s) and then merge these components
to identify the $M$-by-$N$ topology. In this paper, we focus on the second
part, which had previously received less attention in the literature. In
particular, we assume that a $1$-by-$N$ topology is given and that all
$2$-by-$2$ components can be queried and learned using end-to-end probes. The
problem is which $2$-by-$2$'s to query and how to merge them with the given
$1$-by-$N$, so as to exactly identify the $2$-by-$N$ topology, and optimize a
number of performance metrics, including the number of queries (which directly
translates into measurement bandwidth), time complexity, and memory usage. We
provide a lower bound, $\lceil \frac{N}{2} \rceil$, on the number of
$2$-by-$2$'s required by any active learning algorithm and propose two greedy
algorithms. The first algorithm follows the framework of multiple hypothesis
testing, in particular Generalized Binary Search (GBS), since our problem is
one of active learning, from $2$-by-$2$ queries. The second algorithm is called
the Receiver Elimination Algorithm (REA) and follows a bottom-up approach: at
every step, it selects two receivers, queries the corresponding $2$-by-$2$, and
merges it with the given $1$-by-$N$; it requires exactly $N-1$ steps, which is
much less than all $\binom{N}{2}$ possible $2$-by-$2$'s. Simulation results
over synthetic and realistic topologies demonstrate that both algorithms
correctly identify the $2$-by-$N$ topology and are near-optimal, but REA is
more efficient in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2314</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2314</id><created>2012-12-11</created><authors><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>Tree Projections and Structural Decomposition Methods: Minimality and
  Game-Theoretic Characterization</title><categories>cs.DM cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree projections provide a mathematical framework that encompasses all the
various (purely) structural decomposition methods that have been proposed in
the literature to single out classes of nearly-acyclic (hyper)graphs, such as
the tree decomposition method, which is the most powerful decomposition method
on graphs, and the (generalized) hypertree decomposition method, which is its
natural counterpart on arbitrary hypergraphs. The paper analyzes this
framework, by focusing in particular on &quot;minimal&quot; tree projections, that is, on
tree projections without useless redundancies. First, it is shown that minimal
tree projections enjoy a number of properties that are usually required for
normal form decompositions in various structural decomposition methods. In
particular, they enjoy the same kind of connection properties as (minimal) tree
decompositions of graphs, with the result being tight in the light of the
negative answer that is provided to the open question about whether they enjoy
a slightly stronger notion of connection property, defined to speed-up the
computation of hypertree decompositions. Second, it is shown that tree
projections admit a natural game-theoretic characterization in terms of the
Captain and Robber game. In this game, as for the Robber and Cops game
characterizing tree decompositions, the existence of winning strategies implies
the existence of monotone ones. As a special case, the Captain and Robber game
can be used to characterize the generalized hypertree decomposition method,
where such a game-theoretic characterization was missing and asked for. Besides
their theoretical interest, these results have immediate algorithmic
applications both for the general setting and for structural decomposition
methods that can be recast in terms of tree projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2316</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2316</id><created>2012-12-11</created><updated>2013-01-19</updated><authors><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Asymptotic Optimality of Equal Power Allocation for Linear Estimation of
  WSS Random Processes</title><categories>cs.IT math.IT</categories><report-no>UM-CS-2012-036</report-no><journal-ref>IEEE Wireless Communications Letters 2.3 (2013) 247-250</journal-ref><doi>10.1109/WCL.2013.020513.120908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter establishes the asymptotic optimality of equal power allocation
for measurements of a continuous wide-sense stationary (WSS) random process
with a square-integrable autocorrelation function when linear estimation is
used on equally-spaced measurements with periodicity meeting the Nyquist
criterion and with the variance of the noise on any sample inversely
proportional to the power expended by the user to obtain that measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2334</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2334</id><created>2012-12-11</created><authors><author><keyname>Soudani</keyname><forenames>Adel</forenames><affiliation>EuE</affiliation></author><author><keyname>Divoux</keyname><forenames>Thierry</forenames><affiliation>CRAN</affiliation></author><author><keyname>Tourki</keyname><forenames>Rached</forenames><affiliation>LAB-IT06</affiliation></author></authors><title>Data traffic load balancing and QoS in IEEE 802.11 network: Experimental
  study of the signal strength effect</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>Computers &amp; Electrical Engineering 38, 6 (2012) 1717-1730</journal-ref><doi>10.1016/j.compeleceng.2012.07.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performances of multimedia applications built on wireless systems depend
on bandwidth availability that might heavily affect the quality of service. The
IEEE 802.11 standards do not provide performed mechanism for bandwidth
management through data load distribution among different APs of the network.
Then, an AP can be heavily overloaded causing throughput degradation. Load
Balancing Algorithms (LBAs) was been considered as one of the attractive
solution to share the traffic through the available access points bandwidths.
However, applying the load balancing algorithm and shifting a mobile connection
from an access point to another without considering the received signal
strength indicator of the concerned APs might causes worst communication
performances. This paper is a contribution to check the performance's limits of
the LBA algorithm through experimental evaluation of communication metrics for
MPEG-4 video transmission over IEEE 802.11 network. Then, the paper focuses on
the proposition of a new LBA algorithm structure with the consideration of the
RSS level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2338</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2338</id><created>2012-12-11</created><authors><author><keyname>Martin</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Ahmed-Nacer</keyname><forenames>Mehdi</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Urso</keyname><forenames>Pascal</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Controlled conflict resolution for replicated document</title><categories>cs.DB</categories><comments>Controlled conflict resolution for replicated document (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative working is increasingly popular, but it presents challenges due
to the need for high responsiveness and disconnected work support. To address
these challenges the data is optimistically replicated at the edges of the
network, i.e. personal computers or mobile devices. This replication requires a
merge mechanism that preserves the consistency and structure of the shared data
subject to concurrent modifications. In this paper, we propose a generic design
to ensure eventual consistency (every replica will eventually view the same
data) and to maintain the specific constraints of the replicated data. Our
layered design provides to the application engineer the complete control over
system scalability and behavior of the replicated data in face of concurrent
modifications. We show that our design allows replication of complex data types
with acceptable performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2340</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2340</id><created>2012-12-11</created><authors><author><keyname>Germain</keyname><forenames>Pascal</forenames><affiliation>LAHC</affiliation></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames><affiliation>LAHC</affiliation></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LIF</affiliation></author><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LIF</affiliation></author></authors><title>PAC-Bayesian Learning and Domain Adaptation</title><categories>stat.ML cs.LG</categories><comments>https://sites.google.com/site/multitradeoffs2012/</comments><proxy>ccsd</proxy><journal-ref>Multi-Trade-offs in Machine Learning, NIPS 2012 Workshop, Lake
  Tahoe : United States (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning, Domain Adaptation (DA) arises when the distribution gen-
erating the test (target) data differs from the one generating the learning
(source) data. It is well known that DA is an hard task even under strong
assumptions, among which the covariate-shift where the source and target
distributions diverge only in their marginals, i.e. they have the same labeling
function. Another popular approach is to consider an hypothesis class that
moves closer the two distributions while implying a low-error for both tasks.
This is a VC-dim approach that restricts the complexity of an hypothesis class
in order to get good generalization. Instead, we propose a PAC-Bayesian
approach that seeks for suitable weights to be given to each hypothesis in
order to build a majority vote. We prove a new DA bound in the PAC-Bayesian
context. This leads us to design the first DA-PAC-Bayesian algorithm based on
the minimization of the proposed bound. Doing so, we seek for a \rho-weighted
majority vote that takes into account a trade-off between three quantities. The
first two quantities being, as usual in the PAC-Bayesian approach, (a) the
complexity of the majority vote (measured by a Kullback-Leibler divergence) and
(b) its empirical risk (measured by the \rho-average errors on the source
sample). The third quantity is (c) the capacity of the majority vote to
distinguish some structural difference between the source and target samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2341</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2341</id><created>2012-12-11</created><authors><author><keyname>Ducasse</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Petton</keyname><forenames>Nicolas</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Polito</keyname><forenames>Guillermo</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Semantics and Security Issues in JavaScript</title><categories>cs.PL</categories><comments>Deliverable Resilience FUI 12: 7.3.2.1 Failles de s\'ecurit\'e en
  JavaScript / JavaScript security issues</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a plethora of research articles describing the deep semantics of
JavaScript. Nevertheless, such articles are often difficult to grasp for
readers not familiar with formal semantics. In this report, we propose a digest
of the semantics of JavaScript centered around security concerns. This document
proposes an overview of the JavaScript language and the misleading semantic
points in its design. The first part of the document describes the main
characteristics of the language itself. The second part presents how those
characteristics can lead to problems. It finishes by showing some coding
patterns to avoid certain traps and presents some ECMAScript 5 new features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2342</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2342</id><created>2012-12-11</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Maryline</forenames><affiliation>IETR</affiliation></author><author><keyname>Crussi&#xe8;re</keyname><forenames>Matthieu</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>IETR</affiliation></author></authors><title>Distributed MIMO coding scheme with low decoding complexity for future
  mobile TV broadcasting</title><categories>cs.NI cs.IT math.IT</categories><proxy>ccsd</proxy><journal-ref>IET Electronics Letters 48, 17 (2012) 1079-1081</journal-ref><doi>10.1049/el.2012.1778</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel distributed space-time block code (STBC) for the next generation
mobile TV broadcasting is proposed. The new code provides efficient performance
within a wide range of power imbalance showing strong adaptivity to the single
frequency network (SFN) broadcasting deployments. The new code outperforms
existing STBCs with equivalent decoding complexity and approaches those with
much higher complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2343</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2343</id><created>2012-12-11</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames><affiliation>IETR</affiliation></author><author><keyname>Crussi&#xe8;re</keyname><forenames>Matthieu</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>IETR</affiliation></author></authors><title>Improved Channel Estimation Methods based on PN sequence for TDS-OFDM</title><categories>cs.IT cs.NI math.IT</categories><proxy>ccsd</proxy><journal-ref>19th International Conference on Telecommunications (ICT), Jounieh
  : Lebanon (2012)</journal-ref><doi>10.1109/ICTEL.2012.6221259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate channel estimation is crucial for the novel time domain
synchronous orthogonal frequency-division multiplexing (TDS-OFDM) scheme in
which pseudo noise (PN) sequences serve as both guard intervals (GI) for OFDM
data symbols and training sequences for synchronization/channel estimation.
This paper studies the channel estimation method based on the cross-correlation
of PN sequences. A theoretical analysis of this estimator is conducted and
several improved estimators are then proposed to reduce the estimation error
floor encountered by the PN-correlation-based estimator. It is shown through
mathematical derivations and simulations that the new estimators approach or
even achieve the Cramer-Rao bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2345</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2345</id><created>2012-12-11</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames><affiliation>IETR</affiliation></author><author><keyname>Crussi&#xe8;re</keyname><forenames>Matthieu</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Maryline</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>IETR</affiliation></author><author><keyname>Nasser</keyname><forenames>Youssef</forenames></author></authors><title>Enhanced Mobile Digital Video Broadcasting with Distributed Space-Time
  Coding</title><categories>cs.IT cs.MM math.IT</categories><proxy>ccsd</proxy><journal-ref>ICC Workshop on Telecommunications: From Research to Standards,
  Ottawa : Canada (2012)</journal-ref><doi>10.1109/ICC.2012.6364852</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the distributed space-time (ST) coding proposals for
the future Digital Video Broadcasting--Next Generation Handheld (DVB-NGH)
standard. We first theoretically show that the distributed MIMO scheme is the
best broadcasting scenario in terms of channel capacity. Consequently we
evaluate the performance of several ST coding proposals for DVB-NGH with
practical system specifications and channel conditions. Simulation results
demonstrate that the 3D code is the best ST coding solution for broadcasting in
the distributed MIMO scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2346</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2346</id><created>2012-12-11</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Making Triangles Colorful</title><categories>cs.CG</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for any point set P in the plane, a triangle T, and a positive
integer k, there exists a coloring of P with k colors such that any homothetic
copy of T containing at least ck^8 points of P, for some constant c, contains
at least one of each color. This is the first polynomial bound for range spaces
induced by homothetic polygons. The only previously known bound for this
problem applies to the more general case of octants in R^3, but is doubly
exponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2350</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2350</id><created>2012-12-11</created><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA, LCS</affiliation></author><author><keyname>Ly</keyname><forenames>Kim Quyen</forenames><affiliation>LIAMA, LCS</affiliation></author></authors><title>Automated verification of termination certificates</title><categories>cs.LO cs.MS cs.SE</categories><proxy>ccsd</proxy><journal-ref>15th National Symposium of Selected ICT Problems (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to increase user confidence, many automated theorem provers provide
certificates that can be independently verified. In this paper, we report on
our progress in developing a standalone tool for checking the correctness of
certificates for the termination of term rewrite systems, and formally proving
its correctness in the proof assistant Coq. To this end, we use the extraction
mechanism of Coq and the library on rewriting theory and termination called
CoLoR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2386</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2386</id><created>2012-12-11</created><authors><author><keyname>Jaganathan</keyname><forenames>Kishore</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Reconstruction of Integers from Pairwise Distances</title><categories>cs.DM cs.DS</categories><comments>14 pages, 4 figures, submitted to ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of integers, one can easily construct the set of their pairwise
distances. We consider the inverse problem: given a set of pairwise distances,
find the integer set which realizes the pairwise distance set. This problem
arises in a lot of fields in engineering and applied physics, and has
confounded researchers for over 60 years. It is one of the few fundamental
problems that are neither known to be NP-hard nor solvable by polynomial-time
algorithms. Whether unique recovery is possible also remains an open question.
  In many practical applications where this problem occurs, the integer set is
naturally sparse (i.e., the integers are sufficiently spaced), a property which
has not been explored. In this work, we exploit the sparse nature of the
integer set and develop a polynomial-time algorithm which provably recovers the
set of integers (up to linear shift and reversal) from the set of their
pairwise distances with arbitrarily high probability if the sparsity is
$O(n^{1/2-\eps})$. Numerical simulations verify the effectiveness of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2390</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2390</id><created>2012-12-11</created><authors><author><keyname>Werner</keyname><forenames>Eric</forenames></author></authors><title>On the complexity of learning a language: An improvement of Block's
  algorithm</title><categories>cs.CL cs.LG</categories><comments>7 pages. Key Words: Language learning, rules of language, complexity,
  learning algorithms, evolution of language</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language learning is thought to be a highly complex process. One of the
hurdles in learning a language is to learn the rules of syntax of the language.
Rules of syntax are often ordered in that before one rule can applied one must
apply another. It has been thought that to learn the order of n rules one must
go through all n! permutations. Thus to learn the order of 27 rules would
require 27! steps or 1.08889x10^{28} steps. This number is much greater than
the number of seconds since the beginning of the universe! In an insightful
analysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the
assumption of transitivity this vast number of learning steps reduces to a mere
377 steps. We present a mathematical analysis of the complexity of Block's
algorithm. The algorithm has a complexity of order n^2 given n rules. In
addition, we improve Block's results exponentially, by introducing an algorithm
that has complexity of order less than n log n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2396</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2396</id><created>2012-12-11</created><authors><author><keyname>Timo</keyname><forenames>Roy</forenames></author><author><keyname>Oechtering</keyname><forenames>Tobias J.</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Source Coding Problems with Conditionally Less Noisy Side Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computable expression for the rate-distortion (RD) function proposed by
Heegard and Berger has eluded information theory for nearly three decades.
Heegard and Berger's single-letter achievability bound is well known to be
optimal for \emph{physically degraded} side information; however, it is not
known whether the bound is optimal for arbitrarily correlated side information
(general discrete memoryless sources). In this paper, we consider a new setup
in which the side information at one receiver is \emph{conditionally less
noisy} than the side information at the other. The new setup includes degraded
side information as a special case, and it is motivated by the literature on
degraded and less noisy broadcast channels. Our key contribution is a converse
proving the optimality of Heegard and Berger's achievability bound in a new
setting. The converse rests upon a certain \emph{single-letterization} lemma,
which we prove using an information theoretic telescoping identity {recently
presented by Kramer}. We also generalise the above ideas to two different
successive-refinement problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2398</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2398</id><created>2012-12-11</created><authors><author><keyname>Huijse</keyname><forenames>Pablo</forenames></author><author><keyname>Estevez</keyname><forenames>Pablo A.</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author><author><keyname>Zegers</keyname><forenames>Pablo</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>An Information Theoretic Algorithm for Finding Periodicities in Stellar
  Light Curves</title><categories>astro-ph.IM cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 60, issue 10, pp.
  5135-5145, October 2012</journal-ref><doi>10.1109/TSP.2012.2204260</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new information theoretic metric for finding periodicities in
stellar light curves. Light curves are astronomical time series of brightness
over time, and are characterized as being noisy and unevenly sampled. The
proposed metric combines correntropy (generalized correlation) with a periodic
kernel to measure similarity among samples separated by a given period. The new
metric provides a periodogram, called Correntropy Kernelized Periodogram (CKP),
whose peaks are associated with the fundamental frequencies present in the
data. The CKP does not require any resampling, slotting or folding scheme as it
is computed directly from the available samples. CKP is the main part of a
fully-automated pipeline for periodic light curve discrimination to be used in
astronomical survey databases. We show that the CKP method outperformed the
slotted correntropy, and conventional methods used in astronomy for periodicity
discrimination and period estimation tasks, using a set of light curves drawn
from the MACHO survey. The proposed metric achieved 97.2% of true positives
with 0% of false positives at the confidence level of 99% for the periodicity
discrimination task; and 88% of hits with 11.6% of multiples and 0.4% of misses
in the period estimation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2404</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2404</id><created>2012-12-11</created><authors><author><keyname>Erritali</keyname><forenames>Mohammed</forenames></author><author><keyname>Reda</keyname><forenames>Oussama Mohamed</forenames></author><author><keyname>Ouahidi</keyname><forenames>Bouabid El</forenames></author></authors><title>A beaconing approach whith key exchange in vehicular ad hoc networks</title><categories>cs.CR</categories><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  ,P9-13,Vol.3, No.6, November 2012</journal-ref><doi>10.5121/ijdps.2012.3602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad-Hoc Networks (VANETs) are special forms of Mobile Ad-Hoc
Networks (MANETs) that allows vehicles to communicate together in the absence
of fixed infrastructure.In this type of network beaconing is the means used to
discover the nodes in its eighborhood.For routing protocol successful delivery
of beacons containing speed, direction and position of a car is extremely
important.Otherwise, routing information should not be modified/manipulated
during transmission without detection, in order to ensure the routing
information, messages must be signed and provided with a certificate to attest
valid network participants. In this work we present a beaconing protocol with
key exchange to prepare the generation of a signature to protect the routing
information protocol 'Greedy Perimeter Stateless Routing'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2414</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2414</id><created>2012-12-11</created><authors><author><keyname>Salem</keyname><forenames>Maher</forenames></author><author><keyname>Buehler</keyname><forenames>Ulrich</forenames></author></authors><title>Mining Techniques in Network Security to Enhance Intrusion Detection
  Systems</title><categories>cs.CR cs.LG</categories><comments>16 pages, 7 figures</comments><journal-ref>ISSN: 0975-2307 , e-ISSN: 0974-9330, 2012</journal-ref><doi>10.5121/ijnsa</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In intrusion detection systems, classifiers still suffer from several
drawbacks such as data dimensionality and dominance, different network feature
types, and data impact on the classification. In this paper two significant
enhancements are presented to solve these drawbacks. The first enhancement is
an improved feature selection using sequential backward search and information
gain. This, in turn, extracts valuable features that enhance positively the
detection rate and reduce the false positive rate. The second enhancement is
transferring nominal network features to numeric ones by exploiting the
discrete random variable and the probability mass function to solve the problem
of different feature types, the problem of data dominance, and data impact on
the classification. The latter is combined to known normalization methods to
achieve a significant hybrid normalization approach. Finally, an intensive and
comparative study approves the efficiency of these enhancements and shows
better performance comparing to other proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2415</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2415</id><created>2012-12-11</created><authors><author><keyname>Han</keyname><forenames>Song</forenames></author><author><keyname>Kim</keyname><forenames>Jinsong</forenames></author><author><keyname>Kim</keyname><forenames>Cholhun</forenames></author><author><keyname>Jo</keyname><forenames>Jongchol</forenames></author><author><keyname>Han</keyname><forenames>Sunam</forenames></author></authors><title>Robust Face Recognition using Local Illumination Normalization and
  Discriminant Feature Point Selection</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition systems must be robust to the variation of various factors
such as facial expression, illumination, head pose and aging. Especially, the
robustness against illumination variation is one of the most important problems
to be solved for the practical use of face recognition systems. Gabor wavelet
is widely used in face detection and recognition because it gives the
possibility to simulate the function of human visual system. In this paper, we
propose a method for extracting Gabor wavelet features which is stable under
the variation of local illumination and show experiment results demonstrating
its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2425</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2425</id><created>2012-12-11</created><updated>2012-12-14</updated><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Multi-layered Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>It is the begining of essay for Encyclopedia of Social Network
  Analysis and Mining, Springer 2013, so please cite as: Br\'odka P., Kazienko
  P., Multi-layered Social Networks, Encyclopedia of Social Network Analysis
  and Mining, Springer 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is quite obvious that in the real world, more than one kind of
relationship can exist between two actors and that those ties can be so
intertwined that it is impossible to analyse them separately [Fienberg 85],
[Minor 83], [Szell 10]. Social networks with more than one type of relation are
not a completely new concept [Wasserman 94] but they were analysed mainly at
the small scale, e.g. in [McPherson 01], [Padgett 93], and [Entwisle 07]. Just
like in the case of regular single-layered social network there is no widely
accepted definition or even common name. At the beginning such networks have
been called multiplex network [Haythornthwaite 99], [Monge 03]. The term is
derived from communications theory which defines multiplex as combining
multiple signals into one in such way that it is possible to separate them if
needed [Hamill 06]. Recently, the area of multi-layered social network has
started attracting more and more attention in research conducted within
different domains [Kazienko 11a], [Szell 10], [Rodriguez 07], [Rodriguez 09],
and the meaning of multiplex network has expanded and covers not only social
relationships but any kind of connection, e.g. based on geography, occupation,
kinship, hobbies, etc. [Abraham 12]. This essay aims to summarize existing
knowledge about one concept which has many different names i.e. the concept of
Multi-layered Social Network also known as Layered social network,
Multi-relational social network, Multidimensional social network, Multiplex
social network
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2438</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2438</id><created>2012-12-11</created><authors><author><keyname>Rao</keyname><forenames>Shodhan</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan</forenames></author><author><keyname>van Eunen</keyname><forenames>Karen</forenames></author><author><keyname>Bakker</keyname><forenames>Barbara M.</forenames></author><author><keyname>Jayawardhana</keyname><forenames>Bayu</forenames></author></authors><title>Model-order reduction of biochemical reaction networks</title><categories>cs.SY cs.SE math.DS physics.chem-ph q-bio.MN</categories><comments>7 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1211.6643, arXiv:1110.6078</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a model-order reduction method for chemical reaction
networks governed by general enzyme kinetics, including the mass-action and
Michaelis-Menten kinetics. The model-order reduction method is based on the
Kron reduction of the weighted Laplacian matrix which describes the graph
structure of complexes in the chemical reaction network. We apply our method to
a yeast glycolysis model, where the simulation result shows that the transient
behaviour of a number of key metabolites of the reduced-order model is in good
agreement with those of the full-order model.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="38000" completeListSize="102538">1122234|39001</resumptionToken>
</ListRecords>
</OAI-PMH>
