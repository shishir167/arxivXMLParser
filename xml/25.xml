<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:53:06Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|24001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5027</identifier>
 <datestamp>2011-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5027</id><created>2011-08-25</created><authors><author><keyname>Amblard</keyname><forenames>Maxime</forenames><affiliation>LORIA</affiliation></author></authors><title>Encoding Phases using Commutativity and Non-commutativity in a Logical
  Framework</title><categories>cs.CL cs.AI cs.LO</categories><comments>Logical Aspect of Computational Linguistic, Montpellier : France
  (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an extension of Minimalist Categorial Gram- mars (MCG)
to encode Chomsky's phases. These grammars are based on Par- tially Commutative
Logic (PCL) and encode properties of Minimalist Grammars (MG) of Stabler. The
first implementation of MCG were using both non- commutative properties (to
respect the linear word order in an utterance) and commutative ones (to model
features of different constituents). Here, we pro- pose to adding Chomsky's
phases with the non-commutative tensor product of the logic. Then we could give
account of the PIC just by using logical prop- erties of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5037</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5037</id><created>2011-08-25</created><updated>2011-09-09</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author><author><keyname>Deng</keyname><forenames>Jun</forenames></author><author><keyname>Lu</keyname><forenames>Wenmiao</forenames></author></authors><title>Orthonormal Expansion l1-Minimization Algorithms for Compressed Sensing</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>7 pages, 2 figures, 1 table</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 59, no. 12, pp.
  6285-6290, 2011</journal-ref><doi>10.1109/TSP.2011.2168216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing aims at reconstructing sparse signals from significantly
reduced number of samples, and a popular reconstruction approach is
$\ell_1$-norm minimization. In this correspondence, a method called orthonormal
expansion is presented to reformulate the basis pursuit problem for noiseless
compressed sensing. Two algorithms are proposed based on convex optimization:
one exactly solves the problem and the other is a relaxed version of the first
one. The latter can be considered as a modified iterative soft thresholding
algorithm and is easy to implement. Numerical simulation shows that, in dealing
with noise-free measurements of sparse signals, the relaxed version is
accurate, fast and competitive to the recent state-of-the-art algorithms. Its
practical application is demonstrated in a more general case where signals of
interest are approximately sparse and measurements are contaminated with noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5052</identifier>
 <datestamp>2011-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5052</id><created>2011-08-25</created><authors><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author></authors><title>On the Quality of Wireless Network Connectivity</title><categories>cs.NI cs.IT math.IT</categories><comments>submitted to IEEE INFOCOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite intensive research in the area of network connectivity, there is an
important category of problems that remain unsolved: how to measure the quality
of connectivity of a wireless multi-hop network which has a realistic number of
nodes, not necessarily large enough to warrant the use of asymptotic analysis,
and has unreliable connections, reflecting the inherent unreliable
characteristics of wireless communications? The quality of connectivity
measures how easily and reliably a packet sent by a node can reach another
node. It complements the use of \emph{capacity} to measure the quality of a
network in saturated traffic scenarios and provides a native measure of the
quality of (end-to-end) network connections. In this paper, we explore the use
of probabilistic connectivity matrix as a possible tool to measure the quality
of network connectivity. Some interesting properties of the probabilistic
connectivity matrix and their connections to the quality of connectivity are
demonstrated. We argue that the largest eigenvalue of the probabilistic
connectivity matrix can serve as a good measure of the quality of network
connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5062</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5062</id><created>2011-08-25</created><authors><author><keyname>Beauxis</keyname><forenames>Romain</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Mimram</keyname><forenames>Samuel</forenames><affiliation>CEA LIST</affiliation></author></authors><title>A Non-Standard Semantics for Kahn Networks in Continuous Time</title><categories>cs.PL math.CT</categories><comments>2010</comments><proxy>ccsd</proxy><journal-ref>Computer Science Logic (CSL'11) - 25th International Workshop/20th
  Annual Conference of the EACSL 12 (2011) 35-50</journal-ref><doi>10.4230/LIPIcs.CSL.2011.35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a seminal article, Kahn has introduced the notion of process network and
given a semantics for those using Scott domains whose elements are (possibly
infinite) sequences of values. This model has since then become a standard tool
for studying distributed asynchronous computations. From the beginning, process
networks have been drawn as particular graphs, but this syntax is never
formalized. We take the opportunity to clarify it by giving a precise
definition of these graphs, that we call nets. The resulting category is shown
to be a fixpoint category, i.e. a cartesian category which is traced wrt the
monoidal structure given by the product, and interestingly this structure
characterizes the category: we show that it is the free fixpoint category
containing a given set of morphisms, thus providing a complete axiomatics that
models of process networks should satisfy. We then use these tools to build a
model of networks in which data vary over a continuous time, in order to
elaborate on the idea that process networks should also be able to encompass
computational models such as hybrid systems or electric circuits. We relate
this model to Kahn's semantics by introducing a third model of networks based
on non-standard analysis, whose elements form an internal complete partial
order for which many properties of standard domains can be reformulated. The
use of hyperreals in this model allows it to formally consider the notion of
infinitesimal, and thus to make a bridge between discrete and continuous time:
time is &quot;discrete&quot;, but the duration between two instants is infinitesimal.
Finally, we give some examples of uses of the model by describing some networks
implementing common constructions in analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5084</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5084</id><created>2011-08-25</created><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>On the system F as a glue language for natural-language
  compositional-semantics</title><categories>math.LO cs.LO</categories><comments>(24/08/2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to model in compositional framework some phenomena of lexical
pragmatics and in particular the ones studied by Nicholas Asher several
contributions developed in our team did use the system F of Jean-Yves Girard to
construct logical formulae expressing the meaning of sentences --- while other
authors prefer to use Per Martin-L\&quot;of's type theory In this note we explain
the motivations supporting our preference for system F.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5095</identifier>
 <datestamp>2011-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5095</id><created>2011-08-25</created><authors><author><keyname>Kik</keyname><forenames>Marcin</forenames></author></authors><title>RBO Protocol: Broadcasting Huge Databases for Tiny Receivers</title><categories>cs.DS cs.DB cs.DC cs.DM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a protocol (called RBO) for broadcasting long streams of
single-packet messages over radio channel for tiny, battery powered, receivers.
The messages are labeled by the keys from some linearly ordered set. The sender
repeatedly broadcasts a sequence of many (possibly millions) of messages, while
each receiver is interested in reception of a message with a specified key
within this sequence. The transmission is arranged so that the receiver can
wake up in arbitrary moment and find the nearest transmission of its searched
message. Even if it does not know the position of the message in the sequence,
it needs only to receive a small number of (the headers of) other messages to
locate it properly. Thus it can save energy by keeping the radio switched off
most of the time. We show that bit-reversal permutation has &quot;recursive
bisection properties&quot; and, as a consequence, RBO can be implemented very
efficiently with only constant number of $\log_2 n$-bit variables, where $n$ is
the total number of messages in the sequence. The total number of the required
receptions is at most $2\log_2 n +2$ in the model with perfect synchronization.
The basic procedure of RBO (computation of the time slot for the next required
reception) requires only $O(\log^3 n)$ bit-wise operations. We propose
implementation mechanisms for realistic model (with imperfect synchronization),
for operating systems (such as e.g. TinyOS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5096</identifier>
 <datestamp>2011-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5096</id><created>2011-08-25</created><authors><author><keyname>Amblard</keyname><forenames>Maxime</forenames><affiliation>LORIA</affiliation></author></authors><title>Minimalist Grammars and Minimalist Categorial Grammars, definitions
  toward inclusion of generated languages</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>Logic and Grammar (2011) 1-20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stabler proposes an implementation of the Chomskyan Minimalist Program,
Chomsky 95 with Minimalist Grammars - MG, Stabler 97. This framework inherits a
long linguistic tradition. But the semantic calculus is more easily added if
one uses the Curry-Howard isomorphism. Minimalist Categorial Grammars - MCG,
based on an extension of the Lambek calculus, the mixed logic, were introduced
to provide a theoretically-motivated syntax-semantics interface, Amblard 07. In
this article, we give full definitions of MG with algebraic tree descriptions
and of MCG, and take the first steps towards giving a proof of inclusion of
their generated languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5104</identifier>
 <datestamp>2011-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5104</id><created>2011-08-25</created><authors><author><keyname>Kang</keyname><forenames>Byung Gyun</forenames></author><author><keyname>Kim</keyname><forenames>Hyun Kwang</forenames></author><author><keyname>Toan</keyname><forenames>Phan Thanh</forenames></author></authors><title>Improved Linear Programming Bounds on Sizes of Constant-Weight Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A(n,d,w)$ be the largest possible size of an $(n,d,w)$ constant-weight
binary code. By adding new constraints to Delsarte linear programming, we
obtain twenty three new upper bounds on $A(n,d,w)$ for $n \leq 28$. The used
techniques allow us to give a simple proof of an important theorem of Delsarte
which makes linear programming possible for binary codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5128</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5128</id><created>2011-08-25</created><authors><author><keyname>Di Benedetto</keyname><forenames>M. D.</forenames></author><author><keyname>Di Gennaro</keyname><forenames>S.</forenames></author><author><keyname>D'Innocenzo</keyname><forenames>A.</forenames></author></authors><title>Digital Self Triggered Robust Control of Nonlinear Systems</title><categories>math.OC cs.SY</categories><comments>Proceedings of the 50th IEEE CDC-ECC, Orlando, Florida, USA, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop novel results on self triggering control of
nonlinear systems, subject to perturbations and actuation delays. First,
considering an unperturbed nonlinear system with bounded actuation delays, we
provide conditions that guarantee the existence of a self triggering control
strategy stabilizing the closed--loop system. Then, considering parameter
uncertainties, disturbances, and bounded actuation delays, we provide
conditions guaranteeing the existence of a self triggering strategy, that keeps
the state arbitrarily close to the equilibrium point. In both cases, we provide
a methodology for the computation of the next execution time. We show on an
example the relevant benefits obtained with this approach, in terms of energy
consumption, with respect to control algorithms based on a constant sampling,
with a sensible reduction of the average sampling time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5140</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5140</id><created>2011-08-25</created><updated>2011-11-05</updated><authors><author><keyname>Tchaikovsky</keyname><forenames>Michael M.</forenames></author><author><keyname>Kurdyukov</keyname><forenames>Alexander P.</forenames></author><author><keyname>Timin</keyname><forenames>Victor N.</forenames></author></authors><title>A convex formulation of strict anisotropic norm bounded real lemma</title><categories>cs.SY math.OC</categories><comments>16 pages, 1 fugure, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is aimed at extending the H-infinity Bounded Real Lemma to
stochastic systems under random disturbances with imprecisely known probability
distributions. The statistical uncertainty is measured in entropy theoretic
terms using the mean anisotropy functional. The disturbance attenuation
capabilities of the system are quantified by the anisotropic norm which is a
stochastic counterpart of the H-infinity norm. A state-space sufficient
criterion for the anisotropic norm of a linear discrete time invariant system
to be bounded by a given threshold value is derived. The resulting Strict
Anisotropic Norm Bounded Real Lemma involves an inequality on the determinant
of a positive definite matrix and a linear matrix inequality. It is shown that
slight reformulation of these conditions allows the anisotropic norm of a
system to be efficiently computed via convex optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5147</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5147</id><created>2011-08-25</created><updated>2011-08-29</updated><authors><author><keyname>Zhu</keyname><forenames>Haiyi</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author><author><keyname>Luon</keyname><forenames>Yarun</forenames></author></authors><title>To Switch or Not To Switch: Understanding Social Influence in
  Recommender Systems</title><categories>cs.CY cs.HC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We designed and ran an experiment to test how often people's choices are
reversed by others' recommendations when facing different levels of
confirmation and conformity pressures. In our experiment participants were
first asked to provide their preferences between pairs of items. They were then
asked to make second choices about the same pairs with knowledge of others'
preferences. Our results show that others people's opinions significantly sway
people's own choices. The influence is stronger when people are required to
make their second decision sometime later (22.4%) than immediately (14.1%).
Moreover, people are most likely to reverse their choices when facing a
moderate number of opposing opinions. Finally, the time people spend making the
first decision significantly predicts whether they will reverse their decisions
later on, while demographics such as age and gender do not. These results have
implications for consumer behavior research as well as online marketing
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5148</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5148</id><created>2011-08-25</created><authors><author><keyname>Husain</keyname><forenames>Mohammad Iftekhar</forenames></author><author><keyname>Mahant</keyname><forenames>Suyash</forenames></author><author><keyname>Sridhar</keyname><forenames>Ramalingam</forenames></author></authors><title>CD-PHY: Physical Layer Security in Wireless Networks through
  Constellation Diversity</title><categories>cs.CR cs.CC cs.NI</categories><comments>9 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common approach for introducing security at the physical layer is to rely
on the channel variations of the wireless environment. This type of approach is
not always suitable for wireless networks where the channel remains static for
most of the network lifetime. For these scenarios, a channel independent
physical layer security measure is more appropriate which will rely on a secret
known to the sender and the receiver but not to the eavesdropper. In this
paper, we propose CD-PHY, a physical layer security technique that exploits the
constellation diversity of wireless networks which is independent of the
channel variations. The sender and the receiver use a custom bit sequence to
constellation symbol mapping to secure the physical layer communication which
is not known a priori to the eavesdropper. Through theoretical modeling and
experimental simulation, we show that this information theoretic construct can
achieve Shannon secrecy and any brute force attack from the eavesdropper incurs
high overhead and minuscule probability of success. Our results also show that
the high bit error rate also makes decoding practically infeasible for the
eavesdropper, thus securing the communication between the sender and receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5185</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5185</id><created>2011-08-25</created><authors><author><keyname>Liu</keyname><forenames>Jingwei</forenames></author><author><keyname>Xu</keyname><forenames>Meizhi</forenames></author></authors><title>Function Based Nonlinear Least Squares and Application to
  Jelinski--Moranda Software Reliability Model</title><categories>stat.ME cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function based nonlinear least squares estimation (FNLSE) method is
proposed and investigated in parameter estimation of Jelinski-Moranda software
reliability model. FNLSE extends the potential fitting functions of traditional
least squares estimation (LSE), and takes the logarithm transformed nonlinear
least squares estimation (LogLSE) as a special case. A novel power
transformation function based nonlinear least squares estimation (powLSE) is
proposed and applied to the parameter estimation of Jelinski-Moranda model.
Solved with Newton-Raphson method, Both LogLSE and powLSE of Jelinski-Moranda
models are applied to the mean time between failures (MTBF) predications on six
standard software failure time data sets. The experimental results demonstrate
the effectiveness of powLSE with optimal power index compared to the classical
least--squares estimation (LSE), maximum likelihood estimation (MLE) and LogLSE
in terms of recursively relative error (RE) index and Braun statistic index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5192</identifier>
 <datestamp>2012-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5192</id><created>2011-08-25</created><updated>2012-01-12</updated><authors><author><keyname>Kloumann</keyname><forenames>Isabel M.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Harris</keyname><forenames>Kameron Decker</forenames></author><author><keyname>Bliss</keyname><forenames>Catherine A.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Positivity of the English language</title><categories>physics.soc-ph cs.CL</categories><comments>Manuscript: 9 pages, 3 tables, 5 figures; Supplementary Information:
  12 pages, 3 tables, 8 figures</comments><journal-ref>PLoS ONE, Vol 7, e29484, 2012</journal-ref><doi>10.1371/journal.pone.0029484</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last million years, human language has emerged and evolved as a
fundamental instrument of social communication and semiotic representation.
People use language in part to convey emotional information, leading to the
central and contingent questions: (1) What is the emotional spectrum of natural
language? and (2) Are natural languages neutrally, positively, or negatively
biased? Here, we report that the human-perceived positivity of over 10,000 of
the most frequently used English words exhibits a clear positive bias. More
deeply, we characterize and quantify distributions of word positivity for four
large and distinct corpora, demonstrating that their form is broadly invariant
with respect to frequency of word use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5212</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5212</id><created>2011-08-25</created><authors><author><keyname>Seroussi</keyname><forenames>Gadiel</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author><author><keyname>Weinberger</keyname><forenames>Marcelo J.</forenames></author></authors><title>Deinterleaving Finite Memory Processes via Penalized Maximum Likelihood</title><categories>cs.IT math.IT</categories><report-no>Hewlett-Packard Laboratories Technical Report HPL-2011-136</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of deinterleaving a set of finite-memory (Markov)
processes over disjoint finite alphabets, which have been randomly interleaved
by a finite-memory switch. The deinterleaver has access to a sample of the
resulting interleaved process, but no knowledge of the number or structure of
the component Markov processes, or of the switch. We study conditions for
uniqueness of the interleaved representation of a process, showing that certain
switch configurations, as well as memoryless component processes, can cause
ambiguities in the representation. We show that a deinterleaving scheme based
on minimizing a penalized maximum-likelihood cost function is strongly
consistent, in the sense of reconstructing, almost surely as the observed
sequence length tends to infinity, a set of component and switch Markov
processes compatible with the original interleaved process. Furthermore, under
certain conditions on the structure of the switch (including the special case
of a memoryless switch), we show that the scheme recovers \emph{all} possible
interleaved representations of the original process. Experimental results are
presented demonstrating that the proposed scheme performs well in practice,
even for relatively short input samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5217</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5217</id><created>2011-08-25</created><authors><author><keyname>Sharma</keyname><forenames>Dolly</forenames></author><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author><author><keyname>Dinh</keyname><forenames>Hieu</forenames></author></authors><title>An Experimental Comparison of PMSPrune and Other Algorithms for Motif
  Search</title><categories>q-bio.QM cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting meaningful patterns from voluminous amount of biological data is a
very big challenge. Motifs are biological patterns of great interest to
biologists. Many different versions of the motif finding problem have been
identified by researchers. Examples include the Planted $(l, d)$ Motif version,
those based on position-specific score matrices, etc. A comparative study of
the various motif search algorithms is very important for several reasons. For
example, we could identify the strengths and weaknesses of each. As a result,
we might be able to devise hybrids that will perform better than the individual
components. In this paper we (either directly or indirectly) compare the
performance of PMSprune (an algorithm based on the $(l, d)$ motif model) and
several other algorithms in terms of seven measures and using well established
benchmarks
  In this paper, we (directly or indirectly) compare the quality of motifs
predicted by PMSprune and 14 other algorithms. We have employed several
benchmark datasets including the one used by Tompa, et.al. These comparisons
show that the performance of PMSprune is competitive when compared to the other
14 algorithms tested.
  We have compared (directly or indirectly) the performance of PMSprune and 14
other algorithms using the Benchmark dataset provided by Tompa, et.al. It is
observed that both PMSprune and DME (an algorithm based on position-specific
score matrices) in general perform better than the 13 algorithms reported in
Tompa et. al.. Subsequently we have compared PMSprune and DME on other
benchmark data sets including ChIP-Chip, ChIP-seq, and ABS. Between PMSprune
and DME, PMSprune performs better than DME on six measures. DME performs better
than PMSprune on one measure (namely, specificity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5248</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5248</id><created>2011-08-26</created><updated>2013-04-14</updated><authors><author><keyname>Bachrach</keyname><forenames>Yoram</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Zadimoghaddam</keyname><forenames>Morteza</forenames></author></authors><title>Optimal Coalition Structures in Cooperative Graph Games</title><categories>cs.GT cs.MA</categories><comments>16 pages. A short version of this paper is to appear at AAAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation languages for coalitional games are a key research area in
algorithmic game theory. There is an inherent tradeoff between how general a
language is, allowing it to capture more elaborate games, and how hard it is
computationally to optimize and solve such games. One prominent such language
is the simple yet expressive Weighted Graph Games (WGGs) representation [14],
which maintains knowledge about synergies between agents in the form of an edge
weighted graph.
  We consider the problem of finding the optimal coalition structure in WGGs.
The agents in such games are vertices in a graph, and the value of a coalition
is the sum of the weights of the edges present between coalition members. The
optimal coalition structure is a partition of the agents to coalitions, that
maximizes the sum of utilities obtained by the coalitions. We show that finding
the optimal coalition structure is not only hard for general graphs, but is
also intractable for restricted families such as planar graphs which are
amenable for many other combinatorial problems. We then provide algorithms with
constant factor approximations for planar, minor-free and bounded degree
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5250</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5250</id><created>2011-08-26</created><authors><author><keyname>Mohamed</keyname><forenames>A. K.</forenames></author><author><keyname>Marwala</keyname><forenames>T.</forenames></author><author><keyname>John</keyname><forenames>L. R.</forenames></author></authors><title>Single-trial EEG Discrimination between Wrist and Finger Movement
  Imagery and Execution in a Sensorimotor BCI</title><categories>cs.AI</categories><comments>33rd Annual International IEEE EMBS Conference 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A brain-computer interface (BCI) may be used to control a prosthetic or
orthotic hand using neural activity from the brain. The core of this
sensorimotor BCI lies in the interpretation of the neural information extracted
from electroencephalogram (EEG). It is desired to improve on the interpretation
of EEG to allow people with neuromuscular disorders to perform daily
activities. This paper investigates the possibility of discriminating between
the EEG associated with wrist and finger movements. The EEG was recorded from
test subjects as they executed and imagined five essential hand movements using
both hands. Independent component analysis (ICA) and time-frequency techniques
were used to extract spectral features based on event-related
(de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used
for feature reduction. Mahalanobis distance (MD) clustering and artificial
neural networks (ANN) were used as classifiers and obtained average accuracies
of 65 % and 71 % respectively. This shows that EEG discrimination between wrist
and finger movements is possible. The research introduces a new combination of
motor tasks to BCI research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5253</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5253</id><created>2011-08-26</created><authors><author><keyname>Vo</keyname><forenames>Bay</forenames></author><author><keyname>Le</keyname><forenames>Bac</forenames></author></authors><title>A Frequent Closed Itemsets Lattice-based Approach for Mining Minimal
  Non-Redundant Association Rules</title><categories>cs.DB</categories><comments>11 pages</comments><journal-ref>International Journal of Database Theory and Application, Vol.4,
  No.2, 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are many algorithms developed for improvement the time of mining
frequent itemsets (FI) or frequent closed itemsets (FCI). However, the
algorithms which deal with the time of generating association rules were not
put in deep research. In reality, in case of a database containing many FI/FCI
(from ten thousands up to millions), the time of generating association rules
is much larger than that of mining FI/FCI. Therefore, this paper presents an
application of frequent closed itemsets lattice (FCIL) for mining minimal
non-redundant association rules (MNAR) to reduce a lot of time for generating
rules. Firstly, we use CHARM-L for building FCIL. After that, based on FCIL, an
algorithm for fast generating MNAR will be proposed. Experimental results show
that the proposed algorithm is much faster than frequent itemsets lattice-based
algorithm in the mining time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5277</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5277</id><created>2011-08-26</created><updated>2011-08-30</updated><authors><author><keyname>Monsellato</keyname><forenames>Andrea</forenames></author></authors><title>A stochastic model for distributed real time streaming - Evolution of
  the network structure</title><categories>math.PR cs.NI</categories><journal-ref>Scientifica Acta 4, No. 1, MS 9-15 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a short summary of the main results in the thesis [1]. Based on
the P2P paradigm we construct a stochastic model for a live media streaming
content delivery network. Starting from the behavior of the out degree process
of each node we analyze the evolution of the resulting random graph. For the
out degree process we provide an iterative formula for the distribution
probabilities and calculate an explicit expression for the transient
distribution. Moreover we discuss the steady state distribution and the problem
of local disconnections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5281</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5281</id><created>2011-08-26</created><authors><author><keyname>Adamova</keyname><forenames>Monika</forenames></author><author><keyname>Sefranek</keyname><forenames>Jan</forenames></author></authors><title>Transfer of semantics from argumentation frameworks to logic programming
  A preliminary report</title><categories>cs.LO</categories><comments>WLP 2011 (Workshop on Logic Programming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are various interesting semantics' (extensions) designed for
argumentation frameworks. They enable to assign a meaning, e.g., to odd-length
cycles. Our main motivation is to transfer semantics' proposed by Baroni,
Giacomin and Guida for argumetation frameworks with odd-length cycles to logic
programs with odd-length cycles through default negation. The developed
construction is even stronger. For a given logic program an argumentation
framework is defined. The construction enables to transfer each semantics of
the resulting argumentation framework to a semantics of the given logic
program. Weak points of the construction are discussed and some future
continuations of this approach are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5288</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5288</id><created>2011-08-26</created><updated>2012-06-13</updated><authors><author><keyname>Bulatov</keyname><forenames>Andrei A.</forenames></author><author><keyname>Dyer</keyname><forenames>Martin</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>McQuillan</keyname><forenames>Colin</forenames></author></authors><title>The expressibility of functions on the Boolean domain, with applications
  to Counting CSPs</title><categories>cs.CC</categories><comments>corrected typo in title :-)</comments><msc-class>68Q15, 68Q17</msc-class><journal-ref>JACM Vol 60 Issue 5 Oct 2013 Article 32</journal-ref><doi>10.1145/2528401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important tool in the study of the complexity of Constraint Satisfaction
Problems (CSPs) is the notion of a relational clone, which is the set of all
relations expressible using primitive positive formulas over a particular set
of base relations. Post's lattice gives a complete classification of all
Boolean relational clones, and this has been used to classify the computational
difficulty of CSPs. Motivated by a desire to understand the computational
complexity of (weighted) counting CSPs, we develop an analogous notion of
functional clones and study the landscape of these clones. One of these clones
is the collection of log-supermodular (lsm) functions, which turns out to play
a significant role in classifying counting CSPs. In the conservative case
(where all nonnegative unary functions are available), we show that there are
no functional clones lying strictly between the clone of lsm functions and the
total clone (containing all functions). Thus, any counting CSP that contains a
single nontrivial non-lsm function is computationally as hard to approximate as
any problem in #P. Furthermore, we show that any non-trivial functional clone
(in a sense that will be made precise) contains the binary function &quot;implies&quot;.
As a consequence, in the conservative case, all non-trivial counting CSPs are
as hard as #BIS, the problem of counting independent sets in a bipartite graph.
Given the complexity-theoretic results, it is natural to ask whether the
&quot;implies&quot; clone is equivalent to the clone of lsm functions. We use the Mobius
transform and the Fourier transform to show that these clones coincide
precisely up to arity 3. It is an intriguing open question whether the lsm
clone is finitely generated. Finally, we investigate functional clones in which
only restricted classes of unary functions are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5295</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5295</id><created>2011-08-26</created><authors><author><keyname>Hierons</keyname><forenames>Robert M</forenames></author></authors><title>Checking Finite State Machine Conformance when there are Distributed
  Observations</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns state-based systems that interact with their environment
at physically distributed interfaces, called ports. When such a system is used
a projection of the global trace, called a local trace, is observed at each
port. This leads to the environment having reduced observational power: the set
of local traces observed need not uniquely define the global trace that
occurred. We consider the previously defined implementation relation
$\sqsubseteq_s$ and start by investigating the problem of defining a language
${\mathcal {\tilde L}} (M)$ for a multi-port finite state machine (FSM) $M$
such that $N \sqsubseteq_s M$ if and only if every global trace of $N$ is in
${\mathcal {\tilde L}} (M)$. The motivation is that if we can produce such a
language ${\mathcal {\tilde L}} (M)$ then this can potentially be used to
inform development and testing. We show that ${\mathcal {\tilde L}} (M)$ can be
uniquely defined but need not be regular. We then prove that it is generally
undecidable whether $N \sqsubseteq_s M$, a consequence of this result being
that it is undecidable whether there is a test case that is capable of
distinguishing two states or two multi-port FSM in distributed testing. This
result complements a previous result that it is undecidable whether there is a
test case that is guaranteed to distinguish two states or multi-port FSMs. We
also give some conditions under which $N \sqsubseteq_s M$ is decidable. We then
consider the implementation relation $\sqsubseteq_s^k$ that only concerns input
sequences of length $k$ or less. Naturally, given FSMs $N$ and $M$ it is
decidable whether $N \sqsubseteq_s^k M$ since only a finite set of traces is
relevant. We prove that if we place bounds on $k$ and the number of ports then
we can decide $N \sqsubseteq_s^k M$ in polynomial time but otherwise this
problem is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5316</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5316</id><created>2011-08-26</created><authors><author><keyname>D'Innocenzo</keyname><forenames>Alessandro</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Maria Domenica</forenames></author><author><keyname>Serra</keyname><forenames>Emmanuele</forenames></author></authors><title>Link Failure Detection in Multi-hop Control Networks</title><categories>math.OC cs.NI cs.SY</categories><journal-ref>In Proceedings of the 50th IEEE CDC-ECC, Orlando, Florida, USA.
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Multi-hop Control Network (MCN) consists of a plant where the communication
between sensors, actuators and computational unit is supported by a wireless
multi-hop communication network, and data flow is performed using scheduling
and routing of sensing and actuation data. We characterize the problem of
detecting the failure of links of the radio connectivity graph and provide
necessary and sufficient conditions on the plant dynamics and on the
communication protocol. We also provide a methodology to \emph{explicitly}
design the network topology, scheduling and routing of a communication protocol
in order to satisfy the above conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5355</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5355</id><created>2011-08-24</created><updated>2011-10-11</updated><authors><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Scellato</keyname><forenames>Salvatore</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>A tale of many cities: universal patterns in human urban mobility</title><categories>physics.soc-ph cs.SI</categories><doi>10.1371/journal.pone.0037027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of geographic online social networks such as Foursquare, where
users voluntarily signal their current location, opens the door to powerful
studies on human movement. In particular the fine granularity of the location
data, with GPS accuracy down to 10 meters, and the worldwide scale of
Foursquare adoption are unprecedented. In this paper we study urban mobility
patterns of people in several metropolitan cities around the globe by analyzing
a large set of Foursquare users. Surprisingly, while there are variations in
human movement in different cities, our analysis shows that those are
predominantly due to different distributions of places across different urban
environments. Moreover, a universal law for human mobility is identified, which
isolates as a key component the rank-distance, factoring in the number of
places between origin and destination, rather than pure physical distance, as
considered in some previous works. Building on our findings, we also show how a
rank-based movement model accurately captures real human movements in different
cities. Our results shed new light on the driving factors of urban human
mobility, with potential applications for urban planning, location-based
advertisement and even social studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5356</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5356</id><created>2011-08-26</created><authors><author><keyname>Eschenfeldt</keyname><forenames>Patrick</forenames></author><author><keyname>Gross</keyname><forenames>Ben</forenames></author><author><keyname>Pippenger</keyname><forenames>Nicholas</forenames></author></authors><title>Analysis of an M/M/1 Queue Using Fixed Order of Search for Arrivals and
  Service</title><categories>math.PR cs.PF</categories><comments>i+9 pp</comments><msc-class>60K26, 90B22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze an M/M/1 queue with a service discipline in which customers, upon
arriving when the server is busy, search a sequence of stations for a vacant
station at which to wait, and in which the server, upon becoming free when one
or more customers are waiting, searches the stations in the same order for a
station occupied by a customer to serve. We show how to find complete
asymptotic expansions for all the moments of the waiting time in the heavy
traffic limit. We show in particular that the variance of the waiting time for
this discipline is more similar to that of last-come-first-served (which has a
pole of order three as the arrival rate approaches the service rate) than that
of first-come-first-served (which has pole of order two).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5359</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5359</id><created>2011-08-26</created><updated>2012-05-06</updated><authors><author><keyname>Liu</keyname><forenames>Risheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Wei</keyname><forenames>Siming</forenames></author><author><keyname>Su</keyname><forenames>Zhixun</forenames></author></authors><title>Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering</title><categories>cs.NA cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decades, exactly recovering the intrinsic data structure from
corrupted observations, which is known as robust principal component analysis
(RPCA), has attracted tremendous interests and found many applications in
computer vision. Recently, this problem has been formulated as recovering a
low-rank component and a sparse component from the observed data matrix. It is
proved that under some suitable conditions, this problem can be exactly solved
by principal component pursuit (PCP), i.e., minimizing a combination of nuclear
norm and $l_1$ norm. Most of the existing methods for solving PCP require
singular value decompositions (SVD) of the data matrix, resulting in a high
computational complexity, hence preventing the applications of RPCA to very
large scale computer vision problems. In this paper, we propose a novel
algorithm, called $l_1$ filtering, for \emph{exactly} solving PCP with an
$O(r^2(m+n))$ complexity, where $m\times n$ is the size of data matrix and $r$
is the rank of the matrix to recover, which is supposed to be much smaller than
$m$ and $n$. Moreover, $l_1$ filtering is \emph{highly parallelizable}. It is
the first algorithm that can \emph{exactly} solve a nuclear norm minimization
problem in \emph{linear time} (with respect to the data size). Experiments on
both synthetic data and real applications testify to the great advantage of
$l_1$ filtering in speed over state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5361</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5361</id><created>2011-08-26</created><updated>2013-11-13</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Simons</keyname><forenames>Joseph A.</forenames></author></authors><title>Confluent Hasse diagrams</title><categories>cs.CG cs.DS cs.SE</categories><comments>20 pages, 13 figures</comments><acm-class>D.2.2; G.2.2</acm-class><journal-ref>J. Graph Algorithms &amp; Applications 17(7): 689-710, 2013</journal-ref><doi>10.7155/jgaa.00312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a transitively reduced digraph has a confluent upward drawing if
and only if its reachability relation has order dimension at most two. In this
case, we construct a confluent upward drawing with $O(n^2)$ features, in an
$O(n) \times O(n)$ grid in $O(n^2)$ time. For the digraphs representing
series-parallel partial orders we show how to construct a drawing with $O(n)$
features in an $O(n) \times O(n)$ grid in $O(n)$ time from a series-parallel
decomposition of the partial order. Our drawings are optimal in the number of
confluent junctions they use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5387</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5387</id><created>2011-08-26</created><updated>2011-08-30</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author></authors><title>Un metodo estable para la evaluacion de la complejidad algoritmica de
  cadenas cortas</title><categories>cs.CC cs.IT math.IT</categories><comments>23 pages, 3 figures; Proceedings of the Workshop on Complex Systems
  as Computing Models 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is discussed and surveyed a numerical method proposed before, that
alternative to the usual compression method, provides an approximation to the
algorithmic (Kolmogorov) complexity, particularly useful for short strings for
which compression methods simply fail. The method shows to be stable enough and
useful to conceive and compare patterns in an algorithmic models. (article in
Spanish)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5395</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5395</id><created>2011-08-26</created><authors><author><keyname>Chaux</keyname><forenames>Caroline</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Duval</keyname><forenames>Laurent</forenames></author></authors><title>Noise Covariance Properties in Dual-Tree Wavelet Decompositions</title><categories>math.ST cs.CV stat.TH</categories><journal-ref>IEEE Transactions on Information Theory, December 2007, Volume 53,
  Issue 12, p. 2397 - 2412</journal-ref><doi>10.1109/TIT.2007.909104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual-tree wavelet decompositions have recently gained much popularity, mainly
due to their ability to provide an accurate directional analysis of images
combined with a reduced redundancy. When the decomposition of a random process
is performed -- which occurs in particular when an additive noise is corrupting
the signal to be analyzed -- it is useful to characterize the statistical
properties of the dual-tree wavelet coefficients of this process. As dual-tree
decompositions constitute overcomplete frame expansions, correlation structures
are introduced among the coefficients, even when a white noise is analyzed. In
this paper, we show that it is possible to provide an accurate description of
the covariance properties of the dual-tree coefficients of a wide-sense
stationary process. The expressions of the (cross-)covariance sequences of the
coefficients are derived in the one and two-dimensional cases. Asymptotic
results are also provided, allowing to predict the behaviour of the
second-order moments for large lag values or at coarse resolution. In addition,
the cross-correlations between the primal and dual wavelets, which play a
primary role in our theoretical analysis, are calculated for a number of
classical wavelet families. Simulation results are finally provided to validate
these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5397</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5397</id><created>2011-08-26</created><authors><author><keyname>Bergeron</keyname><forenames>Charles</forenames></author><author><keyname>Hepburn</keyname><forenames>Theresa</forenames></author><author><keyname>Sundling</keyname><forenames>C. Matthew</forenames></author><author><keyname>Krein</keyname><forenames>Michael</forenames></author><author><keyname>Katt</keyname><forenames>Bill</forenames></author><author><keyname>Sukumar</keyname><forenames>Nagamani</forenames></author><author><keyname>Breneman</keyname><forenames>Curt M.</forenames></author><author><keyname>Bennett</keyname><forenames>Kristin P.</forenames></author></authors><title>Prediction of peptide bonding affinity: kernel methods for nonlinear
  modeling</title><categories>stat.ML cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents regression models obtained from a process of blind
prediction of peptide binding affinity from provided descriptors for several
distinct datasets as part of the 2006 Comparative Evaluation of Prediction
Algorithms (COEPRA) contest. This paper finds that kernel partial least
squares, a nonlinear partial least squares (PLS) algorithm, outperforms PLS,
and that the incorporation of transferable atom equivalent features improves
predictive capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5405</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5405</id><created>2011-08-26</created><updated>2012-09-22</updated><authors><author><keyname>Martin</keyname><forenames>H. Jose Antonio</forenames></author></authors><title>Solving Hard Computational Problems Efficiently: Asymptotic Parametric
  Complexity 3-Coloring Algorithm</title><categories>cs.DS cs.CC cs.DM math.CO</categories><comments>Working paper</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many practical problems in almost all scientific and technological
disciplines have been classified as computationally hard (NP-hard or even
NP-complete). In life sciences, combinatorial optimization problems frequently
arise in molecular biology, e.g., genome sequencing; global alignment of
multiple genomes; identifying siblings or discovery of dysregulated pathways.In
almost all of these problems, there is the need for proving a hypothesis about
certain property of an object that can be present only when it adopts some
particular admissible structure (an NP-certificate) or be absent (no admissible
structure), however, none of the standard approaches can discard the hypothesis
when no solution can be found, since none can provide a proof that there is no
admissible structure. This article presents an algorithm that introduces a
novel type of solution method to &quot;efficiently&quot; solve the graph 3-coloring
problem; an NP-complete problem. The proposed method provides certificates
(proofs) in both cases: present or absent, so it is possible to accept or
reject the hypothesis on the basis of a rigorous proof. It provides exact
solutions and is polynomial-time (i.e., efficient) however parametric. The only
requirement is sufficient computational power, which is controlled by the
parameter $\alpha\in\mathbb{N}$. Nevertheless, here it is proved that the
probability of requiring a value of $\alpha&gt;k$ to obtain a solution for a
random graph decreases exponentially: $P(\alpha&gt;k) \leq 2^{-(k+1)}$, making
tractable almost all problem instances. Thorough experimental analyses were
performed. The algorithm was tested on random graphs, planar graphs and
4-regular planar graphs. The obtained experimental results are in accordance
with the theoretical expected results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5422</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5422</id><created>2011-08-27</created><authors><author><keyname>Moosa</keyname><forenames>Tanaeem M.</forenames></author><author><keyname>Nazeen</keyname><forenames>Sumaiya</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Reaz</keyname><forenames>Rezwana</forenames></author></authors><title>Linear Time Inference of Strings from Cover Arrays using a Binary
  Alphabet</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covers being one of the most popular form of regularities in strings, have
drawn much attention over time. In this paper, we focus on the problem of
linear time inference of strings from cover arrays using the least sized
alphabet possible. We present an algorithm that can reconstruct a string $x$
over a two-letter alphabet whenever a valid cover array $C$ is given as an
input. This algorithm uses several interesting combinatorial properties of
cover arrays and an interesting relation between border array and cover array
to achieve this. Our algorithm runs in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5428</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5428</id><created>2011-08-27</created><updated>2011-09-29</updated><authors><author><keyname>Angrishi</keyname><forenames>Kishore</forenames></author><author><keyname>Killat</keyname><forenames>Ulrich</forenames></author></authors><title>Probabilistic Performance Analysis of Networks using an Improved Network
  Service Envelope Approach</title><categories>cs.NI</categories><comments>20 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic network calculus is an evolving theory which accounts for
statistical multiplexing and uses an envelope approach for probabilistic delay
and backlog analysis of networks. One of the key ideas of stochastic network
calculus is the possibility to describe service offered at network node as a
stochastic service envelope, which in turn can be used to describe the
stochastic service available in a network of nodes and determine end-to-end
probabilistic delay and backlog bounds. This paper introduces a new definition
of stochastic service envelopes which yield a simple network service envelope
and tighter end-to-end performance bounds. It is shown for ($\sigma(\theta),
\rho(\theta)$) - constrained traffic model that the end-to-end performance
measures computed using the new stochastic network service envelope are tight
in comparison to the ones obtained using the existing start-of-the-art
definition of statistical network service envelope and are bounded by ${\cal
O}(H \log{H})$, where $H$ is the number of nodes traversed by the arrival
traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5431</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5431</id><created>2011-08-27</created><authors><author><keyname>Dessalles</keyname><forenames>Jean-Louis</forenames></author></authors><title>Providing information can be a stable non-cooperative evolutionary
  strategy</title><categories>q-bio.PE cs.NE</categories><comments>15 pages, 6 figures</comments><report-no>Technical Report Telecom ParisTech 2010D025</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human language is still an embarrassment for evolutionary theory, as the
speaker's benefit remains unclear. The willingness to communicate information
is shown here to be an evolutionary stable strategy (ESS), even if acquiring
original information from the environment involves significant cost and
communicating it provides no material benefit to addressees. In this study,
communication is used to advertise the emitter's ability to obtain novel
information. We found that communication strategies can take two forms,
competitive and uniform, that these two strategies are stable and that they
necessarily coexist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5434</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5434</id><created>2011-08-27</created><authors><author><keyname>Febbraro</keyname><forenames>Onofrio</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Reale</keyname><forenames>Kristian</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>Unit Testing in ASPIDE</title><categories>cs.PL</categories><comments>12 pages, 4 figures, Proceedings of the 25th Workshop on Logic
  Programming (WLP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a declarative logic programming formalism,
which is employed nowadays in both academic and industrial real-world
applications. Although some tools for supporting the development of ASP
programs have been proposed in the last few years, the crucial task of testing
ASP programs received less attention, and is an Achilles' heel of the available
programming environments.
  In this paper we present a language for specifying and running unit tests on
ASP programs. The testing language has been implemented in ASPIDE, a
comprehensive IDE for ASP, which supports the entire life-cycle of ASP
development with a collection of user-friendly graphical tools for program
composition, testing, debugging, profiling, solver execution configuration, and
output-handling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5450</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5450</id><created>2011-08-27</created><authors><author><keyname>Peng</keyname><forenames>Aoyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Lianming</forenames></author></authors><title>Deterministic multidimensional growth model for small-world networks</title><categories>physics.data-an cs.SI</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed a deterministic multidimensional growth model for small-world
networks. The model can characterize the distinguishing properties of many
real-life networks with geometric space structure. Our results show the model
possesses small-world effect: larger clustering coefficient and smaller
characteristic path length. We also obtain some accurate results for its
properties including degree distribution, clustering coefficient and network
diameter and discuss them. It is also worth noting that we get an accurate
analytical expression for calculating the characteristic path length. We verify
numerically and experimentally these main features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5451</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5451</id><created>2011-08-27</created><authors><author><keyname>Behrend</keyname><forenames>Andreas</forenames></author></authors><title>A Uniform Fixpoint Approach to the Implementation of Inference Methods
  for Deductive Databases</title><categories>cs.DB</categories><comments>to appear in the Proceedings of the 19th International Conference on
  Applications of Declarative Programming and Knowledge Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the research area of deductive databases three different database
tasks have been deeply investigated: query evaluation, update propagation and
view updating. Over the last thirty years various inference mechanisms have
been proposed for realizing these main functionalities of a rule-based system.
However, these inference mechanisms have been rarely used in commercial DB
systems until now. One important reason for this is the lack of a uniform
approach well-suited for implementation in an SQL-based system. In this paper,
we present such a uniform approach in form of a new version of the soft
consequence operator. Additionally, we present improved transformation-based
approaches to query optimization and update propagation and view updating which
are all using this operator as underlying evaluation mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5457</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5457</id><created>2011-08-27</created><authors><author><keyname>Gavenciak</keyname><forenames>Tomas</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author></authors><title>Deciding first order logic properties of matroids</title><categories>cs.DS cs.DM cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frick and Grohe [J. ACM 48 (2006), 1184-1206] introduced a notion of graph
classes with locally bounded tree-width and established that every first order
logic property can be decided in almost linear time in such a graph class.
Here, we introduce an analogous notion for matroids (locally bounded
branch-width) and show the existence of a fixed parameter algorithm for first
order logic properties in classes of regular matroids with locally bounded
branch-width. To obtain this result, we show that the problem of deciding the
existence of a circuit of length at most k containing two given elements is
fixed parameter tractable for regular matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5460</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5460</id><created>2011-08-27</created><authors><author><keyname>Jarir</keyname><forenames>Zahi</forenames></author><author><keyname>Quafafou</keyname><forenames>Mohamed</forenames></author><author><keyname>Erradi</keyname><forenames>Mahammed</forenames></author></authors><title>Personalized Web Services for Web Information Extraction</title><categories>cs.IR</categories><journal-ref>International Journal of Web Services Practices, Vol. 5, No.1
  (2010), pp. 22-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of information extraction from the Web emerged with the growth of
the Web and the multiplication of online data sources. This paper is an
analysis of information extraction methods. It presents a service oriented
approach for web information extraction considering both web data management
and extraction services. Then we propose an SOA based architecture to enhance
flexibility and on-the-fly modification of web extraction services. An
implementation of the proposed architecture is proposed on the middleware level
of Java Enterprise Edition (JEE) servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5466</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5466</id><created>2011-08-27</created><authors><author><keyname>Chakrabarty</keyname><forenames>Supriya</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author></authors><title>Quality Evaluation of Conceptual Level Object Multidimensional Data
  Model</title><categories>cs.OH</categories><comments>14 pages - accepted in June 2011 for publication in the International
  Journal of Computer Science &amp; Information Technology (ISSN: 0975-3826)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of technology facilitates explosive growth of mobile usage in
the last decade. Numerous applications have been developed to support its
usage. However, gap in technology exists in obtaining correct and trusted
values for evaluation indexes of the precise amount of usage. The claims of
loss in revenue by the service providers could be more due to unexpected
behaviour of the hardware. A similar mistrust is often observed in the users of
the services. A trustworthy subscription scheme is in demand for consumers
whereas revenue needs to be assured of the service providers. Multiple
Authorizations by Multiple Owners (MAMO) has already been introduced as a
technology to build trust in the third party billing system. In this paper,
MAMO is extended to ensure trustworthiness of the parameters for subscription.
Along with call transaction data are reconciled to assure the proper revenue
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5471</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5471</id><created>2011-08-27</created><authors><author><keyname>Yan</keyname><forenames>Li</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author></authors><title>New Results on the Fault-Tolerant Facility Placement Problem</title><categories>cs.DS</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We studied the Fault-Tolerant Facility Placement problem (FTFP) which
generalizes the uncapacitated facility location problem (UFL). In FTFP, we are
given a set F of sites at which facilities can be built, and a set C of clients
with some demands that need to be satisfied by different facilities. A client
$j$ has demand $r_j$. Building one facility at a site $i$ incurs a cost $f_i$,
and connecting one unit of demand from client $j$ to a facility at site
$i\in\fac$ costs $d_{ij}$. $d_{ij}$'s are assumed to form a metric. A feasible
solution specifies the number of facilities to be built at each site and the
way to connect demands from clients to facilities, with the restriction that
demands from the same client must go to different facilities. Facilities at the
same site are considered different. The goal is to find a solution with minimum
total cost. We gave a 1.7245-approximation algorithm to the FTFP problem. Our
technique is via a reduction to the Fault-Tolerant Facility Location problem,
in which each client has demand $r_j$ but each site can have at most one
facility built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5472</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5472</id><created>2011-08-27</created><updated>2012-10-22</updated><authors><author><keyname>Zhou</keyname><forenames>Shan</forenames></author><author><keyname>Wu</keyname><forenames>Xinzhou</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>Distributed Power Control and Coding-Modulation Adaptation in Wireless
  Networks using Annealed Gibbs Sampling</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless networks, the transmission rate of a link is determined by
received signal strength, interference from simultaneous transmissions, and
available coding-modulation schemes. Rate allocation is a key problem in
wireless network design, but a very challenging problem because: (i) wireless
interference is global, i.e., a transmission interferes all other simultaneous
transmissions, and (ii) the rate-power relation is non-convex and
non-continuous, where the discontinuity is due to limited number of
coding-modulation choices in practical systems. In this paper, we propose a
distributed power control and coding-modulation adaptation algorithm using
annealed Gibbs sampling, which achieves throughput optimality in an arbitrary
network topology. We consider a realistic
Signal-to-Interference-and-Noise-Ratio (SINR) based interference model, and
assume continuous power space and finite rate options (coding-modulation
choices). Our algorithm first decomposes network-wide interference to local
interference by properly choosing a &quot;neighborhood&quot; for each transmitter and
bounding the interference from non-neighbor nodes. The power update policy is
then carefully designed to emulate a Gibbs sampler over a Markov chain with a
continuous state space. We further exploit the technique of simulated annealing
to speed up the convergence of the algorithm to the optimal power and
coding-modulation configuration. Finally, simulation results demonstrate the
superior performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5475</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5475</id><created>2011-08-27</created><authors><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author><author><keyname>Marshall</keyname><forenames>Kyle</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Michael E.</forenames></author></authors><title>The Dimension of Subcode-Subfields of Shortened Generalized Reed Solomon
  Codes</title><categories>cs.IT math.IT</categories><msc-class>14G50, 11T71, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reed-Solomon (RS) codes are among the most ubiquitous codes due to their good
parameters as well as efficient encoding and decoding procedures. However, RS
codes suffer from having a fixed length. In many applications where the length
is static, the appropriate length can be obtained by an RS code by shortening
or puncturing. Generalized Reed-Solomon (GRS) codes are a generalization of RS
codes, whose subfield-subcodes are extensively studied. In this paper we show
that a particular class of GRS codes produces many subfield-subcodes with large
dimension. An algorithm for searching through the codes is presented as well as
a list of new codes obtained from this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5491</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5491</id><created>2011-08-27</created><authors><author><keyname>Melucci</keyname><forenames>Massimo</forenames></author></authors><title>Improving Ranking Using Quantum Probability</title><categories>cs.IR cs.ET cs.LG physics.data-an</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The paper shows that ranking information units by quantum probability differs
from ranking them by classical probability provided the same data used for
parameter estimation. As probability of detection (also known as recall or
power) and probability of false alarm (also known as fallout or size) measure
the quality of ranking, we point out and show that ranking by quantum
probability yields higher probability of detection than ranking by classical
probability provided a given probability of false alarm and the same parameter
estimation data. As quantum probability provided more effective detectors than
classical probability within other domains that data management, we conjecture
that, the system that can implement subspace-based detectors shall be more
effective than a system which implements a set-based detectors, the
effectiveness being calculated as expected recall estimated over the
probability of detection and expected fallout estimated over the probability of
false alarm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5493</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5493</id><created>2011-08-27</created><updated>2011-09-24</updated><authors><author><keyname>Hasan</keyname><forenames>Ziaul</forenames></author><author><keyname>Boostanimehr</keyname><forenames>Hamidreza</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Green Cellular Networks: A Survey, Some Research Issues and Challenges</title><categories>cs.NI</categories><comments>16 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency in cellular networks is a growing concern for cellular
operators to not only maintain profitability, but also to reduce the overall
environment effects. This emerging trend of achieving energy efficiency in
cellular networks is motivating the standardization authorities and network
operators to continuously explore future technologies in order to bring
improvements in the entire network infrastructure. In this article, we present
a brief survey of methods to improve the power efficiency of cellular networks,
explore some research issues and challenges and suggest some techniques to
enable an energy efficient or &quot;green&quot; cellular network. Since base stations
consume a maximum portion of the total energy used in a cellular system, we
will first provide a comprehensive survey on techniques to obtain energy
savings in base stations. Next, we discuss how heterogeneous network deployment
based on micro, pico and femto-cells can be used to achieve this goal. Since
cognitive radio and cooperative relaying are undisputed future technologies in
this regard, we propose a research vision to make these technologies more
energy efficient. Lastly, we explore some broader perspectives in realizing a
&quot;green&quot; cellular network technology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5494</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5494</id><created>2011-08-28</created><authors><author><keyname>Xu</keyname><forenames>Dan</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author></authors><title>Geographic Trough Filling for Internet Datacenters</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To reduce datacenter energy consumption and cost, current practice has
considered demand-proportional resource provisioning schemes, where servers are
turned on/off according to the load of requests.
  Most existing work considers instantaneous (Internet) requests only, which
are explicitly or implicitly assumed to be delay-sensitive. On the other hand,
in datacenters, there exist a vast amount of delay-tolerant jobs, such as
background/maintainance jobs. In this paper, we explicitly differentiate
delay-sensitive jobs and delay tolerant jobs. We focus on the problem of using
delay-tolerant jobs to fill the extra capacity of datacenters, referred to as
trough/valley filling. Giving a higher priority to delay-sensitive jobs, our
schemes complement to most existing demand-proportional resource provisioning
schemes. Our goal is to design intelligent trough filling mechanisms that are
energy efficient and also achieve good delay performance. Specifically, we
propose two joint dynamic speed scaling and traffic shifting schemes, one
subgradient-based and the other queue-based. Our schemes assume little
statistical information of the system, which is usually difficult to obtain in
practice. In both schemes, energy cost saving comes from dynamic speed scaling,
statistical multiplexing, electricity price diversity, and service efficiency
diversity. In addition, good delay performance is achieved in the queue-based
scheme via load shifting and capacity allocation based on queue conditions.
Practical issues that may arise in datacenter networks are considered,
including capacity and bandwidth constraint, service agility constraint, and
load shifting cost. We use both artificial and real datacenter traces to
evaluate the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5497</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5497</id><created>2011-08-28</created><updated>2011-11-08</updated><authors><author><keyname>Jahangir</keyname><forenames>Ifat</forenames></author><author><keyname>Das</keyname><forenames>Anindya</forenames></author><author><keyname>Hasan</keyname><forenames>Masud</forenames></author></authors><title>Formulation and Development of a Novel Quaternary Algebra</title><categories>cs.AR</categories><comments>26 pages, 3 figures, submitted to Journal of Multiple-Valued Logic
  and Soft Computing (on November 9, 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a novel quaternary algebra has been proposed that can be used
to implement any quaternary logic function. Unlike other variants of quaternary
algebra, this algebra is closely related to Boolean algebra and can be used to
convert any binary function into quaternary without any significant
modification. For this purpose, we have defined a set of quaternary operators
and developed two ways to express any quaternary function mathematically.
Finally, we have presented the design of several combinational logic circuits
and compared these designs with several other variants of quaternary logic.
Since a quaternary digit can contain as much information as a pair of binary
digits, this new logic may be quite useful in the fields of communication and
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5499</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5499</id><created>2011-08-28</created><authors><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author><author><keyname>Al-Mushayt</keyname><forenames>Omar Saeed</forenames></author></authors><title>A Note on Separable Nonlinear Least Squares Problem</title><categories>cs.CG</categories><comments>3 pages; IEEE, 2011 International Conference on Future Computer
  Sciences and Application (ICFCSA 2011), Jun. 18- 19, 2011, Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separable nonlinear least squares (SNLS)problem is a special class of
nonlinear least squares (NLS)problems, whose objective function is a mixture of
linear and nonlinear functions. It has many applications in many different
areas, especially in Operations Research and Computer Sciences. They are
difficult to solve with the infinite-norm metric. In this paper, we give a
short note on the separable nonlinear least squares problem, unseparated scheme
for NLS, and propose an algorithm for solving mixed linear-nonlinear
minimization problem, method of which results in solving a series of least
squares separable problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5505</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5505</id><created>2011-08-28</created><authors><author><keyname>Postoyan</keyname><forenames>Romain</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author><author><keyname>Nesic</keyname><forenames>Dragan</forenames></author><author><keyname>Anta</keyname><forenames>Adolfo</forenames></author></authors><title>Event-triggered and self-triggered stabilization of distributed
  networked control systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-triggered and self-triggered control have recently been proposed as
implementation strategies that considerably reduce the resources required for
control. Although most of the work so far has focused on closing a single
control loop, some researchers have started to investigate how these new
implementation strategies can be applied when closing multiple-feedback loops
in the presence of physically distributed sensors and actuators. In this paper,
we consider a scenario where the distributed sensors, actuators, and
controllers communicate via a shared wired channel. We use our recent
prescriptive framework for the event-triggered control of nonlinear systems to
develop novel policies suitable for the considered distributed scenario.
Afterwards, we explain how self-triggering rules can be deduced from the
developed event-triggered strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5514</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5514</id><created>2011-08-29</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Strategic Learning and Robust Protocol Design for Online Communities
  with Selfish Users</title><categories>cs.LG cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on analyzing the free-riding behavior of self-interested
users in online communities. Hence, traditional optimization methods for
communities composed of compliant users such as network utility maximization
cannot be applied here. In our prior work, we show how social reciprocation
protocols can be designed in online communities which have populations
consisting of a continuum of users and are stationary under stochastic
permutations. Under these assumptions, we are able to prove that users
voluntarily comply with the pre-determined social norms and cooperate with
other users in the community by providing their services. In this paper, we
generalize the study by analyzing the interactions of self-interested users in
online communities with finite populations and are not stationary. To optimize
their long-term performance based on their knowledge, users adapt their
strategies to play their best response by solving individual stochastic control
problems. The best-response dynamic introduces a stochastic dynamic process in
the community, in which the strategies of users evolve over time. We then
investigate the long-term evolution of a community, and prove that the
community will converge to stochastically stable equilibria which are stable
against stochastic permutations. Understanding the evolution of a community
provides protocol designers with guidelines for designing social norms in which
no user has incentives to adapt its strategy and deviate from the prescribed
protocol, thereby ensuring that the adopted protocol will enable the community
to achieve the optimal social welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5515</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5515</id><created>2011-08-29</created><authors><author><keyname>Gao</keyname><forenames>Jianxi</forenames></author><author><keyname>Buldyrev</keyname><forenames>S. V.</forenames></author><author><keyname>Havlin</keyname><forenames>S.</forenames></author><author><keyname>Stanley</keyname><forenames>H. E.</forenames></author></authors><title>Robustness of a Tree-like Network of Interdependent Networks</title><categories>physics.data-an cs.SI physics.soc-ph</categories><comments>10 pages, 11 figures</comments><doi>10.1103/PhysRevE.85.066134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In reality, many real-world networks interact with and depend on other
networks. We develop an analytical framework for studying interacting networks
and present an exact percolation law for a network of $n$ interdependent
networks (NON). We present a general framework to study the dynamics of the
cascading failures process at each step caused by an initial failure occurring
in the NON system. We study and compare both $n$ coupled Erd\H{o}s-R\'{e}nyi
(ER) graphs and $n$ coupled random regular (RR) graphs. We found recently [Gao
et. al. arXive:1010.5829] that for an NON composed of $n$ ER networks each of
average degree $k$, the giant component, $P_{\infty}$, is given by
$P_{\infty}=p[1-\exp(-kP_{\infty})]^n$ where $1-p$ is the initial fraction of
removed nodes. Our general result coincides for $n=1$ with the known
Erd\H{o}s-R\'{e}nyi second-order phase transition at a threshold, $p=p_c$, for
a single network. For $n=2$ the general result for $P_{\infty}$ corresponds to
the $n=2$ result [Buldyrev et. al., Nature, 464, (2010)]. Similar to the ER
NON, for $n=1$ the percolation transition at $p_c$, is of second order while
for any $n&gt;1$ it is of first order. The first order percolation transition in
both ER and RR (for $n&gt;1$) is accompanied by cascading failures between the
networks due to their interdependencies. However, we find that the robustness
of $n$ coupled RR networks of degree $k$ is dramatically higher compared to the
$n$ coupled ER networks of average degree $k$. While for ER NON there exists a
critical minimum average degree $k=k_{\min}$, that increases with $n$, below
which the system collapses, there is no such analogous $k_{\min}$ for RR NON
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5520</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5520</id><created>2011-08-29</created><authors><author><keyname>Choy</keyname><forenames>Murphy</forenames></author><author><keyname>Cheong</keyname><forenames>Michelle L. F.</forenames></author><author><keyname>Laik</keyname><forenames>Ma Nang</forenames></author><author><keyname>Shung</keyname><forenames>Koo Ping</forenames></author></authors><title>A sentiment analysis of Singapore Presidential Election 2011 using
  Twitter data with census correction</title><categories>stat.AP cs.CL cs.SI</categories><msc-class>62C05, 62P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis is a new area in text analytics where it focuses on the
analysis and understanding of the emotions from the text patterns. This new
form of analysis has been widely adopted in customer relation management
especially in the context of complaint management. With increasing level of
interest in this technology, more and more companies are adopting it and using
it to champion their marketing efforts. However, sentiment analysis using
twitter has remained extremely difficult to manage due to the sampling bias. In
this paper, we will discuss about the application of using reweighting
techniques in conjunction with online sentiment divisions to predict the vote
percentage that individual candidate will receive. There will be in depth
discussion about the various aspects using sentiment analysis to predict
outcomes as well as the potential pitfalls in the estimation due to the
anonymous nature of the internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5525</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5525</id><created>2011-08-29</created><authors><author><keyname>Gupta</keyname><forenames>Manoj</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author><author><keyname>Sen</keyname><forenames>Sandeep</forenames></author></authors><title>The update complexity of selection and related problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for computing with input data specified by intervals,
representing uncertainty in the values of the input parameters. To compute a
solution, the algorithm can query the input parameters that yield more refined
estimates in form of sub-intervals and the objective is to minimize the number
of queries. The previous approaches address the scenario where every query
returns an exact value. Our framework is more general as it can deal with a
wider variety of inputs and query responses and we establish interesting
relationships between them that have not been investigated previously. Although
some of the approaches of the previous restricted models can be adapted to the
more general model, we require more sophisticated techniques for the analysis
and we also obtain improved algorithms for the previous model.
  We address selection problems in the generalized model and show that there
exist 2-update competitive algorithms that do not depend on the lengths or
distribution of the sub-intervals and hold against the worst case adversary. We
also obtain similar bounds on the competitive ratio for the MST problem in
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5530</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5530</id><created>2011-08-29</created><authors><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Lu</keyname><forenames>Songnian</forenames></author><author><keyname>Zhao</keyname><forenames>Dandan</forenames></author><author><keyname>Zhang</keyname><forenames>Aixin</forenames></author><author><keyname>Li</keyname><forenames>Jianhua</forenames></author></authors><title>An Anti-attack Model Based on Complex Network Theory in P2P networks</title><categories>cs.NI physics.data-an</categories><comments>arXiv admin note: excessive text overlap with arXiv:cond-mat/0504185,
  without attribution</comments><doi>10.1016/j.physa.2011.12.051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex network theory is a useful way to study many real systems. In this
paper, an anti-attack model based on complex network theory is introduced. The
mechanism of this model is based on dynamic compensation process and reverse
percolation process in P2P networks. The main purpose of the paper is: (i) a
dynamic compensation process can turn an attacked P2P network into a power-law
(PL) network with exponential cutoff; (ii) a local healing process can restore
the maximum degree of peers in an attacked P2P network to a normal level; (iii)
a restoring process based on reverse percolation theory connects the
fragmentary peers of an attacked P2P network together into a giant connected
component. In this way, the model based on complex network theory can be
effectively utilized for anti-attack and protection purposes in P2P networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5533</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5533</id><created>2011-08-29</created><updated>2012-09-28</updated><authors><author><keyname>de Castro</keyname><forenames>Yohann</forenames><affiliation>LM-Orsay</affiliation></author></authors><title>A Remark on the Lasso and the Dantzig Selector</title><categories>math.ST cs.IT math.FA math.IT stat.TH</categories><comments>Final Version. This article was written mostly during his Ph.D. at
  the Institut de Math\'ematiques de Toulouse (IMT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates a new parameter for the high-dimensional regression
with noise: the distortion. This latter has attracted a lot of attention
recently with the appearance of new deterministic constructions of
'almost'-Euclidean sections of the L1-ball. It measures how far is the
intersection between the kernel of the design matrix and the unit L1-ball from
an L2-ball. We show that the distortion holds enough information to derive
oracle inequalities (i.e. a comparison to an ideal situation where one knows
the s largest coefficients of the target) for the lasso and the Dantzig
selector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5543</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5543</id><created>2011-08-29</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author><author><keyname>Scholz</keyname><forenames>Oliver</forenames></author><author><keyname>Harada</keyname><forenames>Kanako</forenames></author><author><keyname>Popesku</keyname><forenames>Sergej</forenames></author><author><keyname>Liedke</keyname><forenames>Jens</forenames></author><author><keyname>Raja</keyname><forenames>Humza</forenames></author><author><keyname>Liu</keyname><forenames>Wenguo</forenames></author><author><keyname>Caparrelli</keyname><forenames>Fabio</forenames></author><author><keyname>Jemai</keyname><forenames>Jaouhar</forenames></author><author><keyname>Havlik</keyname><forenames>Jiri</forenames></author><author><keyname>Meister</keyname><forenames>Eugen</forenames></author><author><keyname>Levi</keyname><forenames>Paul</forenames></author></authors><title>Multi-Robot Organisms: State of the Art</title><categories>cs.RO cs.NE cs.SY</categories><journal-ref>ICRA2010, workshop on &quot;Modular Robots: State of the Art&quot;, pp.1-10,
  Anchorage, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper represents the state of the art development on the field of
artificial multi-robot organisms. It briefly considers mechatronic development,
sensor and computational equipment, software framework and introduces one of
the Grand Challenges for swarm and reconfigurable robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5547</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5547</id><created>2011-08-29</created><authors><author><keyname>Stepanov</keyname><forenames>Misha</forenames></author></authors><title>Instantons causing iterative decoding to cycle</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is speculated that the most probable channel noise realizations
(instantons) that cause the iterative decoding of low-density parity-check
codes to fail make the decoding not to converge. A simple example is given of
an instanton that is not a pseudo-codeword and causes iterative decoding to
cycle. A method of finding the instantons for large number of iterations is
presented and tested on Tanner's [155, 64, 20] code and Gaussian channel. The
inherently dynamic instanton with effective distance of 11.475333 is found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5567</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5567</id><created>2011-08-29</created><authors><author><keyname>Lierler</keyname><forenames>Yuliya</forenames><affiliation>University of Kentucky</affiliation></author><author><keyname>Sch&#xfc;ller</keyname><forenames>Peter</forenames><affiliation>Technische Universit&#xe4;t Wien</affiliation></author></authors><title>Parsing Combinatory Categorial Grammar with Answer Set Programming:
  Preliminary Report</title><categories>cs.AI cs.CL</categories><comments>12 pages, 2 figures, Proceedings of the 25th Workshop on Logic
  Programming (WLP 2011)</comments><acm-class>I.2.1; I.2.4; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatory categorial grammar (CCG) is a grammar formalism used for natural
language parsing. CCG assigns structured lexical categories to words and uses a
small set of combinatory rules to combine these categories to parse a sentence.
In this work we propose and implement a new approach to CCG parsing that relies
on a prominent knowledge representation formalism, answer set programming (ASP)
- a declarative programming paradigm. We formulate the task of CCG parsing as a
planning problem and use an ASP computational tool to compute solutions that
correspond to valid parses. Compared to other approaches, there is no need to
implement a specific parsing algorithm using such a declarative method. Our
approach aims at producing all semantically distinct parse trees for a given
sentence. From this goal, normalization and efficiency issues arise, and we
deal with them by combining and extending existing strategies. We have
implemented a CCG parsing tool kit - AspCcgTk - that uses ASP as its main
computational means. The C&amp;C supertagger can be used as a preprocessor within
AspCcgTk, which allows us to achieve wide-coverage natural language parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5574</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5574</id><created>2011-08-29</created><updated>2014-06-26</updated><authors><author><keyname>Berth&#xe9;</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Jolivet</keyname><forenames>Timo</forenames></author><author><keyname>Siegel</keyname><forenames>Anne</forenames></author></authors><title>Substitutive Arnoux-Rauzy sequences have pure discrete spectrum</title><categories>math.DS cs.DM math.CO</categories><comments>19 pages, v2 includes some corrections to match the published
  version, and a mistake in the graph of Fig. 1 has been corrected</comments><journal-ref>Uniform Distribution Theory 7 (2012), no. 1, 173-197</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the symbolic dynamical system generated by a purely
substitutive Arnoux-Rauzy sequence is measurably conjugate to a toral
translation. The proof is based on an explicit construction of a fundamental
domain with fractal boundary (a Rauzy fractal) for this toral translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5575</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5575</id><created>2011-08-29</created><authors><author><keyname>Melucci</keyname><forenames>Massimo</forenames></author></authors><title>Getting Beyond the State of the Art of Information Retrieval with
  Quantum Theory</title><categories>cs.IR cs.LG physics.data-an</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  According to the probability ranking principle, the document set with the
highest values of probability of relevance optimizes information retrieval
effectiveness given the probabilities are estimated as accurately as possible.
The key point of this principle is the separation of the document set into two
subsets with a given level of fallout and with the highest recall. If subsets
of set measures are replaced by subspaces and space measures, we obtain an
alternative theory stemming from Quantum Theory. That theory is named after
vector probability because vectors represent event like sets do in classical
probability. The paper shows that the separation into vector subspaces is more
effective than the separation into subsets with the same available evidence.
The result is proved mathematically and verified experimentally. In general,
the paper suggests that quantum theory is not only a source of rhetoric
inspiration, but is a sufficient condition to improve retrieval effectiveness
in a principled way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5586</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5586</id><created>2011-08-29</created><authors><author><keyname>Schneeweiss</keyname><forenames>Denny</forenames></author><author><keyname>Hofstedt</keyname><forenames>Petra</forenames></author></authors><title>FdConfig: A Constraint-Based Interactive Product Configurator</title><categories>cs.AI</categories><comments>19th International Conference on Applications of Declarative
  Programming and Knowledge Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a constraint-based approach to interactive product configuration.
Our configurator tool FdConfig is based on feature models for the
representation of the product domain. Such models can be directly mapped into
constraint satisfaction problems and dealt with by appropriate constraint
solvers. During the interactive configuration process the user generates new
constraints as a result of his configuration decisions and even may retract
constraints posted earlier. We discuss the configuration process, explain the
underlying techniques and show optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5592</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5592</id><created>2011-08-26</created><authors><author><keyname>Taneja</keyname><forenames>Abhishek</forenames></author><author><keyname>Chauhan</keyname><forenames>R. K.</forenames></author></authors><title>A Performance Study of Data Mining Techniques: Multiple Linear
  Regression vs. Factor Analysis</title><categories>cs.DB</categories><comments>Data mining, Multiple Linear Regression, Factor Analysis, Principal
  Component Regression, Maximum Liklihood Regression, Generalized Least Square
  Regression</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing volume of data usually creates an interesting challenge for the
need of data analysis tools that discover regularities in these data. Data
mining has emerged as disciplines that contribute tools for data analysis,
discovery of hidden knowledge, and autonomous decision making in many
application domains. The purpose of this study is to compare the performance of
two data mining techniques viz., factor analysis and multiple linear regression
for different sample sizes on three unique sets of data. The performance of the
two data mining techniques is compared on following parameters like mean square
error (MSE), R-square, R-Square adjusted, condition number, root mean square
error(RMSE), number of variables included in the prediction model, modified
coefficient of efficiency, F-value, and test of normality. These parameters
have been computed using various data mining tools like SPSS, XLstat, Stata,
and MS-Excel. It is seen that for all the given dataset, factor analysis
outperform multiple linear regression. But the absolute value of prediction
accuracy varied between the three datasets indicating that the data
distribution and data characteristics play a major role in choosing the correct
prediction technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5593</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5593</id><created>2011-08-26</created><authors><author><keyname>Sreekanth</keyname><forenames>S.</forenames></author><author><keyname>Saravana</keyname><forenames>R.</forenames></author><author><keyname>Enkataramana</keyname><forenames>S.</forenames></author><author><keyname>Reddy</keyname><forenames>R. Hemadri</forenames></author></authors><title>Unsteady Hydromagnetic Flow of Viscoelastic Fluid down an Open Inclined
  Channel</title><categories>physics.flu-dyn cs.CE</categories><comments>Walter's B' fluid, open inclined channel, Laplace transform and
  finite Fourier Sine transform technique</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the unsteady hydromagnetic flow of a Walter's fluid
(Model B') down an open inclined channel of width 2a and depth d under gravity,
the walls of the channel being normal to the surface of the bottom under the
influence of a uniform transverse magnetic field. A uniform tangential stress
is applied at the free surface in the direction of flow. We have evaluated the
velocity distribution by using Laplace transform and finite Fourier Sine
transform technique. The velocity distribution has been obtained taking
different form of time dependent pressure gradient g(t), viz., i) constant ii)
exponential decreasing function of time and iii) Cosine function of time. The
effects of magnetic parameter M, Reynolds number R and the viscoelastic
parameter K are discussed on the velocity distribution in three different
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5609</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5609</id><created>2011-08-29</created><authors><author><keyname>Bra&#xdf;el</keyname><forenames>Bernd</forenames></author><author><keyname>Hanus</keyname><forenames>Michael</forenames></author><author><keyname>Peem&#xf6;ller</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Reck</keyname><forenames>Fabian</forenames></author></authors><title>Implementing Equational Constraints in a Functional Language</title><categories>cs.PL</categories><comments>To appear in the Proceedings of the 25th Workshop on Logic
  Programming (WLP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  KiCS2 is a new system to compile functional logic programs of the source
language Curry into purely functional Haskell programs. The implementation is
based on the idea to represent the search space as a data structure and logic
variables as operations that generate their values. This has the advantage that
one can apply various, and in particular, complete search strategies to compute
solutions. However, the generation of all values for logic variables might be
inefficient for applications that exploit constraints on partially known
values. To overcome this drawback, we propose new techniques to implement
equational constraints in this framework. In particular, we show how
unification modulo function evaluation and functional patterns can be added
without sacrificing the efficiency of the kernel implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5619</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5619</id><created>2011-08-26</created><authors><author><keyname>Singh</keyname><forenames>Karanjit</forenames></author><author><keyname>Bhasin</keyname><forenames>Shuchita</forenames></author></authors><title>Modification of GTD from Flat File Format to OLAP for Data Mining</title><categories>cs.DB</categories><comments>GTD, OLAP, Data Mining, Terror Databases</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document is part of original research work by the authors in a bid to
explore new fields for applying Data Mining Techniques. The sample data is part
of a large data set from University of Maryland (UMD) and outlines how more
meaningful patterns can be discovered by preprocessing the data in the form of
OLAP cubes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5622</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5622</id><created>2011-08-29</created><authors><author><keyname>Roozbehani</keyname><forenames>Mardavij</forenames></author><author><keyname>Megretski</keyname><forenames>Alexandre</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author></authors><title>Optimization of Lyapunov Invariants in Verification of Software Systems
  (Extended Version)</title><categories>cs.SY cs.SE math.OC</categories><comments>50 pages, 5 figures. This is the long version with more details.
  Short version available at: http://arxiv.org/abs/1108.0170</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a control-theoretic framework for verification of
numerical software systems, and puts forward software verification as an
important application of control and systems theory. The idea is to transfer
Lyapunov functions and the associated computational techniques from control
systems analysis and convex optimization to verification of various software
safety and performance specifications. These include but are not limited to
absence of overflow, absence of division-by-zero, termination in finite time,
presence of dead-code, and certain user-specified assertions. Central to this
framework are Lyapunov invariants. These are properly constructed functions of
the program variables, and satisfy certain properties-resembling those of
Lyapunov functions-along the execution trace. The search for the invariants can
be formulated as a convex optimization problem. If the associated optimization
problem is feasible, the result is a certificate for the specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5624</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5624</id><created>2011-08-29</created><authors><author><keyname>Sutantyo</keyname><forenames>Donny K.</forenames></author><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author><author><keyname>Nepomnyashchikh</keyname><forenames>Valentin A.</forenames></author><author><keyname>Levi</keyname><forenames>Paul</forenames></author></authors><title>Multi-Robot Searching Algorithm Using Levy Flight and Artificial
  Potential Field</title><categories>cs.RO cs.SY</categories><comments>Eighth IEEE International Workshop on Safety, Security, and Rescue
  Robotics (SSRR-2010), Bremen, Germany, 26-30 July 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient search algorithm is very crucial in robotic area, especially for
exploration missions, where the target availability is unknown and the
condition of the environment is highly unpredictable. In a very large
environment, it is not sufficient to scan an area or volume by a single robot,
multiple robots should be involved to perform the collective exploration. In
this paper, we propose to combine bio-inspired search algorithm called Levy
flight and artificial potential field method to perform an efficient searching
algorithm for multi-robot applications. The main focus of this work is not only
to prove the concept or to measure the efficiency of the algorithm by
experiments, but also to develop an appropriate generic framework to be
implemented both in simulation and on real robotic platforms. Several
experiments, which compare different search algorithms, are also performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5626</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5626</id><created>2011-08-29</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Krennwallner</keyname><forenames>Thomas</forenames></author><author><keyname>Redl</keyname><forenames>Christoph</forenames></author></authors><title>Nested HEX-Programs</title><categories>cs.AI</categories><comments>Proceedings of the 19th International Conference on Applications of
  Declarative Programming and Knowledge Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer-Set Programming (ASP) is an established declarative programming
paradigm. However, classical ASP lacks subprogram calls as in procedural
programming, and access to external computations (like remote procedure calls)
in general. The feature is desired for increasing modularity and---assuming
proper access in place---(meta-)reasoning over subprogram results. While
HEX-programs extend classical ASP with external source access, they do not
support calls of (sub-)programs upfront. We present nested HEX-programs, which
extend HEX-programs to serve the desired feature, in a user-friendly manner.
Notably, the answer sets of called sub-programs can be individually accessed.
This is particularly useful for applications that need to reason over answer
sets like belief set merging, user-defined aggregate functions, or preferences
of answer sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5643</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5643</id><created>2011-08-29</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author><author><keyname>Schmickl</keyname><forenames>Thomas</forenames></author><author><keyname>Timmis</keyname><forenames>Jon</forenames></author></authors><title>Collective Adaptive Systems: Challenges Beyond Evolvability</title><categories>cs.ET cs.CY cs.NE</categories><comments>Workshop &quot;Fundamentals of Collective Adaptive Systems&quot;, European
  Commission, 3-4 November, 2009, Brussels</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This position paper overviews several challenges of collective adaptive
systems, which are beyond the research objectives of current top-projects in
ICT, and especially in FET, initiatives. The attention is paid not only to
challenges and new research topics, but also to their impact and potential
breakthroughs in information and communication technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5648</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5648</id><created>2011-08-29</created><authors><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author></authors><title>On the shoulders of students? The contribution of PhD students to the
  advancement of knowledge</title><categories>physics.soc-ph cs.DL</categories><comments>41 pages, 7 figures, forthcoming in Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the participation in peer reviewed publications of all doctoral
students in Quebec over the 2000-2007 period this paper provides the first
large scale analysis of their research effort. It shows that PhD students
contribute to about a third of the publication output of the province, with
doctoral students in the natural and medical sciences being present in a higher
proportion of papers published than their colleagues of the social sciences and
humanities. Collaboration is an important component of this socialization:
disciplines in which student collaboration is higher are also those in which
doctoral students are the most involved in peer-reviewed publications. In terms
of scientific impact, papers co-signed by doctorate students obtain
significantly lower citation rates than other Quebec papers, except in natural
sciences and engineering. Finally, this paper shows that involving doctoral
students in publications is positively linked with degree completion and
ulterior career in research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5667</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5667</id><created>2011-08-29</created><authors><author><keyname>De Pooter</keyname><forenames>Stef</forenames></author><author><keyname>Wittocx</keyname><forenames>Johan</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>A prototype of a knowledge-based programming environment</title><categories>cs.AI cs.LO</categories><comments>6 pages, appears in the Proceedings of the 19th International
  Conference on Applications of Declarative Programming and Knowledge
  Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a proposal for a knowledge-based programming
environment. In such an environment, declarative background knowledge,
procedures, and concrete data are represented in suitable languages and
combined in a flexible manner. This leads to a highly declarative programming
style. We illustrate our approach on an example and report about our prototype
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5668</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5668</id><created>2011-08-29</created><authors><author><keyname>Dulac-Arnold</keyname><forenames>Gabriel</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Preux</keyname><forenames>Philippe</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Datum-Wise Classification: A Sequential Approach to Sparsity</title><categories>cs.AI cs.LG</categories><comments>ECML2011</comments><journal-ref>Lecture Notes in Computer Science, 2011, Volume 6911/2011, 375-390</journal-ref><doi>10.1007/978-3-642-23780-5_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel classification technique whose aim is to select an
appropriate representation for each datapoint, in contrast to the usual
approach of selecting a representation encompassing the whole dataset. This
datum-wise representation is found by using a sparsity inducing empirical risk,
which is a relaxation of the standard L 0 regularized risk. The classification
problem is modeled as a sequential decision process that sequentially chooses,
for each datapoint, which features to use before classifying. Datum-Wise
Classification extends naturally to multi-class tasks, and we describe a
specific case where our inference has equivalent complexity to a traditional
linear classifier, while still using a variable number of features. We compare
our classifier to classical L 1 regularized linear models (L 1-SVM and LARS) on
a set of common binary and multi-class datasets and show that for an equal
average number of features used we can get improved performance using our
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5669</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5669</id><created>2011-08-29</created><updated>2011-09-02</updated><authors><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Constantin</keyname><forenames>Florin</forenames></author><author><keyname>Iwata</keyname><forenames>Satoru</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author></authors><title>Learning Valuation Functions</title><categories>cs.GT cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the approximate learnability of valuations commonly
used throughout economics and game theory for the quantitative encoding of
agent preferences. We provide upper and lower bounds regarding the learnability
of important subclasses of valuation functions that express
no-complementarities. Our main results concern their approximate learnability
in the distributional learning (PAC-style) setting. We provide nearly tight
lower and upper bounds of $\tilde{\Theta}(n^{1/2})$ on the approximation factor
for learning XOS and subadditive valuations, both widely studied superclasses
of submodular valuations. Interestingly, we show that the
$\tilde{\Omega}(n^{1/2})$ lower bound can be circumvented for XOS functions of
polynomial complexity; we provide an algorithm for learning the class of XOS
valuations with a representation of polynomial size achieving an $O(n^{\eps})$
approximation factor in time $O(n^{1/\eps})$ for any $\eps &gt; 0$. This
highlights the importance of considering the complexity of the target function
for polynomial time learning. We also provide new learning results for
interesting subclasses of submodular functions.
  Our upper bounds for distributional learning leverage novel structural
results for all these valuation classes. We show that many of these results
provide new learnability results in the Goemans et al. model (SODA 2009) of
approximate learning everywhere via value queries.
  We also introduce a new model that is more realistic in economic settings, in
which the learner can set prices and observe purchase decisions at these prices
rather than observing the valuation function directly. In this model, most of
our upper bounds continue to hold despite the fact that the learner receives
less information (both for learning in the distributional setting and with
value queries), while our lower bounds naturally extend.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5673</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5673</id><created>2011-08-29</created><authors><author><keyname>Berger</keyname><forenames>Niklaus</forenames></author></authors><title>Partial wave analysis at BES III harnessing the power of GPUs</title><categories>physics.data-an cs.GR hep-ex</categories><comments>6 pages, 2 figures, prepared for the proceedings of Computing in High
  Energy Physics (CHEP) 2010</comments><doi>10.1063/1.3647201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial wave analysis is a core tool in hadron spectroscopy. With the high
statistics data available at facilities such as the Beijing Spectrometer III,
this procedure becomes computationally very expensive. We have successfully
implemented a framework for performing partial wave analysis on graphics
processors. We discuss the implementation, the parallel computing frameworks
employed and the performance achieved, with a focus on the recent transition to
the OpenCL framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5703</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5703</id><created>2011-08-26</created><authors><author><keyname>E</keyname><forenames>Jeevan H</forenames></author><author><keyname>P</keyname><forenames>Prashanth P</forenames></author><author><keyname>N</keyname><forenames>Punith Kumar S</forenames></author><author><keyname>Hegde</keyname><forenames>Vinay</forenames></author></authors><title>Web Pages Clustering: A New Approach</title><categories>cs.IR</categories><comments>Clustering, concept mining, information retrieval, metasearch engine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of web has resulted in vast volume of information.
Information availability at a rapid speed to the user is vital. English
language (or any for that matter) has lot of ambiguity in the usage of words.
So there is no guarantee that a keyword based search engine will provide the
required results. This paper introduces the use of dictionary (standardised) to
obtain the context with which a keyword is used and in turn cluster the results
based on this context. These ideas can be merged with a metasearch engine to
enhance the search efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5704</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5704</id><created>2011-08-26</created><authors><author><keyname>Boumedyen</keyname></author><author><keyname>Kaneez</keyname></author><author><keyname>Rafael</keyname></author><author><keyname>Victor</keyname></author></authors><title>E- Learning: An effective pedagogical tool for learning</title><categories>cs.CY</categories><comments>E- learning, higher education systems, modernization, decision tree
  algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the info-tech age E-Methods of learning are becoming the most important
vehicle in disseminating knowledge in higher education institutions. This
sector is growing and changing at a rapid speed due to developments in
technologies. But teaching is an art. Can there be fun learning with raw and
dry technology? How can we make the best use of E- Methods, can we make the
required information and data available to the students in a flexible manner,
at ease all the time? What are the advantages of traditional methods of
teaching and learning? Is E-learning a progressive stage incubating all the
benefits of the Manual learning or it is only a window dressing on the face of
advancement? Can we convert the boring, tedious subjects into interactive,
monotony breaking joyous learning? In this paper the researchers have focused
on the modernization of E- Pedagogy vis-a-vis the traditional method of
learning. They have highlighted the effectiveness of using the E- learning
elements and various E- Methods. This work has used the decision tree
algorithms particularly Classifiers.trees.J48 The obtained results show that
using online examination attribute plays major role in increasing the average
grade of the class in higher education. The novelty of this work is that the
researchers have focused on the teaching methodology used by the faculty
members and the tools available in the universities. We believe that this work
will play a constructive role in building higher education system. Our
generated rules/output can be used by the decision makers in the improvement of
higher education system processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5705</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5705</id><created>2011-08-29</created><authors><author><keyname>Ali</keyname><forenames>Usman</forenames><affiliation>LTCI</affiliation></author><author><keyname>Duhamel</keyname><forenames>Pierre</forenames><affiliation>LTCI</affiliation></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames><affiliation>LTCI</affiliation></author></authors><title>Sliding Trellis-Based Frame Synchronization</title><categories>cs.NI</categories><comments>IEEE International Conference on Communication (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frame Synchronization (FS) is required in several communication standards in
order to recover the individual frames that have been aggregated in a burst.
This paper proposes a low-delay and reducedcomplexity Sliding Trellis
(ST)-based FS technique, compared to our previously proposed trellis-based FS
method. Each burst is divided into overlapping windows in which FS is
performed. Useful information is propagated from one window to the next. The
proposed method makes use of soft information provided by the channel, but also
of all sources of redundancy present in the protocol stack. An illustration of
our STbased approach for the WiMAX Media Access Control (MAC) layer is
provided. When FS is performed on bursts transmitted over Rayleigh fading
channel, the ST-based approach reduces the FS latency and complexity at the
cost of a very small performance degradation compared to our full complexity
trellis-based FS and outperforms state-of-the-art FS techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5706</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5706</id><created>2011-08-29</created><authors><author><keyname>Abid</keyname><forenames>Manel</forenames><affiliation>LTCI</affiliation></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames><affiliation>LTCI</affiliation></author><author><keyname>Pesquet-Popescu</keyname><forenames>Beatrice</forenames><affiliation>LTCI, TSI</affiliation></author></authors><title>Consistent Reconstruction of the Input of an Oversampled Filter Bank
  From Noisy Subbands</title><categories>cs.NI</categories><comments>European Signal Processing Conference (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a reconstruction approach for the input signal of an
oversampled filter bank (OFB) when the sub-bands generated at its output are
quantized and transmitted over a noisy channel. This approach exploits the
redundancy introduced by the OFB and the fact that the quantization noise is
bounded. A maximum-likelihood estimate of the input signal is evaluated, which
only considers the vectors of quantization indexes corresponding to subband
signals that could have been generated by the OFB and that are compliant with
the quantization errors. When considering an OFB with an oversampling ratio of
3/2 and a transmission of quantized subbands on an AWGN channel, compared to a
classical decoder, the performance gains are up to 9 dB in terms of SNR for the
reconstructed signal, and 3 dB in terms of channel SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5710</identifier>
 <datestamp>2012-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5710</id><created>2011-08-29</created><authors><author><keyname>Schmidt</keyname><forenames>Mark</forenames><affiliation>INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Alahari</keyname><forenames>Karteek</forenames><affiliation>INRIA Paris - Rocquencourt</affiliation></author></authors><title>Generalized Fast Approximate Energy Minimization via Graph Cuts:
  Alpha-Expansion Beta-Shrink Moves</title><categories>cs.CV cs.AI</categories><comments>Conference on Uncertainty in Artificial Intelligence (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present alpha-expansion beta-shrink moves, a simple generalization of the
widely-used alpha-beta swap and alpha-expansion algorithms for approximate
energy minimization. We show that in a certain sense, these moves dominate both
alpha-beta-swap and alpha-expansion moves, but unlike previous generalizations
the new moves require no additional assumptions and are still solvable in
polynomial-time. We show promising experimental results with the new moves,
which we believe could be used in any context where alpha-expansions are
currently employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5711</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5711</id><created>2011-08-29</created><authors><author><keyname>Angrand</keyname><forenames>Pierre-Yves</forenames><affiliation>LTCI</affiliation></author><author><keyname>Sakarovitch</keyname><forenames>Jacques</forenames><affiliation>LTCI</affiliation></author></authors><title>On the enumerating series of an abstract numeration system</title><categories>cs.DM cs.FL</categories><comments>presented at the Journ\'ees Montoises d'Informatique Th\'eorique
  2010, Sept. 2010, Amiens (France)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that any rational abstract numeration system is faithfully, and
effectively, represented by an N-rational series. A simple proof of this result
is given which yields a representation of this series which in turn allows a
simple computation of the value of words in this system and easy constructions
for the recognition of recognisable sets of numbers. It is also shown that
conversely it is decidable whether an N-rational series corresponds to a
rational abstract numeration system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5717</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5717</id><created>2011-08-29</created><authors><author><keyname>Mihalkova</keyname><forenames>Lilyana</forenames></author><author><keyname>Moustafa</keyname><forenames>Walaa Eldin</forenames></author></authors><title>Structure Selection from Streaming Relational Data</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical relational learning techniques have been successfully applied in
a wide range of relational domains. In most of these applications, the human
designers capitalized on their background knowledge by following a
trial-and-error trajectory, where relational features are manually defined by a
human engineer, parameters are learned for those features on the training data,
the resulting model is validated, and the cycle repeats as the engineer adjusts
the set of features. This paper seeks to streamline application development in
large relational domains by introducing a light-weight approach that
efficiently evaluates relational features on pieces of the relational graph
that are streamed to it one at a time. We evaluate our approach on two social
media tasks and demonstrate that it leads to more accurate models that are
learned faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5719</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5719</id><created>2011-08-29</created><updated>2011-11-13</updated><authors><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author><author><keyname>Gorham</keyname><forenames>Jackson</forenames></author><author><keyname>Kahle</keyname><forenames>Matthew</forenames></author><author><keyname>Mason</keyname><forenames>Jeremy</forenames></author></authors><title>Computational topology for configuration spaces of hard disks</title><categories>math.AT cs.RO math-ph math.MP</categories><comments>version 3: 16 pages, 11 figures; made minor changes requested by
  journal --- changed some color figures to black and white, and reordered
  bibliography to appear in order of citation rather than alphabetical order</comments><doi>10.1103/PhysRevE.85.011303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the topology of configuration spaces of hard disks experimentally,
and show that several changes in the topology can already be observed with a
small number of particles. The results illustrate a theorem of Baryshnikov,
Bubenik, and Kahle that critical points correspond to configurations of disks
with balanced mechanical stresses, and suggest conjectures about the asymptotic
topology as the number of disks tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5720</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5720</id><created>2011-08-29</created><authors><author><keyname>N&#xf6;lle</keyname><forenames>Michael</forenames></author><author><keyname>Suda</keyname><forenames>Martin</forenames></author></authors><title>Conjugate Variables as a Resource in Signal and Image Processing</title><categories>cs.CV physics.data-an quant-ph</categories><comments>22 pages, 2 tables, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a new technique to model joint distributions of
signals. Our technique is based on quantum mechanical conjugate variables. We
show that the transition probability of quantum states leads to a distance
function on the signals. This distance function obeys the triangle inequality
on all quantum states and becomes a metric on pure quantum states. Treating
signals as conjugate variables allows us to create a new approach to segment
them.
  Keywords: Quantum information, transition probability, Euclidean distance,
Fubini-study metric, Bhattacharyya coefficients, conjugate variable,
signal/sensor fusion, signal and image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5724</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5724</id><created>2011-08-29</created><authors><author><keyname>Oliva</keyname><forenames>Gabriele</forenames></author><author><keyname>Panzieri</keyname><forenames>Stefano</forenames></author><author><keyname>Setola</keyname><forenames>Roberto</forenames></author></authors><title>On the Stability of Linear Discrete-Time Fuzzy Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the linear and stationary Discrete-time systems with state
variables and dynamic coefficients represented by fuzzy numbers are studied,
providing some stability criteria, and characterizing the bounds of the set of
solutions in the case of positive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5756</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5756</id><created>2011-08-29</created><authors><author><keyname>Br&#xf6;cker</keyname><forenames>Jochen</forenames></author><author><keyname>Szendro</keyname><forenames>Ivan G.</forenames></author></authors><title>Sensitivity And Out-Of-Sample Error in Continuous Time Data Assimilation</title><categories>physics.ao-ph cs.SY math.OC</categories><comments>submitted to Quarterly Journal of the Royal Meteorological Society</comments><doi>10.1002/qj.940</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data assimilation refers to the problem of finding trajectories of a
prescribed dynamical model in such a way that the output of the model (usually
some function of the model states) follows a given time series of observations.
Typically though, these two requirements cannot both be met at the same
time--tracking the observations is not possible without the trajectory
deviating from the proposed model equations, while adherence to the model
requires deviations from the observations. Thus, data assimilation faces a
trade-off. In this contribution, the sensitivity of the data assimilation with
respect to perturbations in the observations is identified as the parameter
which controls the trade-off. A relation between the sensitivity and the
out-of-sample error is established which allows to calculate the latter under
operational conditions. A minimum out-of-sample error is proposed as a
criterion to set an appropriate sensitivity and to settle the discussed
trade-off. Two approaches to data assimilation are considered, namely
variational data assimilation and Newtonian nudging, aka synchronisation.
Numerical examples demonstrate the feasibility of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5757</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5757</id><created>2011-08-29</created><authors><author><keyname>Camp&#xea;lo</keyname><forenames>Manoel</forenames></author><author><keyname>Corr&#xea;a</keyname><forenames>Ricardo C.</forenames></author><author><keyname>Moura</keyname><forenames>Phablo F. S.</forenames></author><author><keyname>Santos</keyname><forenames>Marcio C.</forenames></author></authors><title>Optimal k-fold colorings of webs and antiwebs</title><categories>cs.DM</categories><comments>A short version of this paper was presented at the Simp\'osio
  Brasileiro de Pesquisa Operacional, Brazil, 2011</comments><journal-ref>Discrete Applied Mathematics, 161(1-2), pages 60-70, 2013</journal-ref><doi>10.1016/j.dam.2012.07.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A k-fold x-coloring of a graph is an assignment of (at least) k distinct
colors from the set {1, 2, ..., x} to each vertex such that any two adjacent
vertices are assigned disjoint sets of colors. The smallest number x such that
G admits a k-fold x-coloring is the k-th chromatic number of G, denoted by
\chi_k(G). We determine the exact value of this parameter when G is a web or an
antiweb. Our results generalize the known corresponding results for odd cycles
and imply necessary and sufficient conditions under which \chi_k(G) attains its
lower and upper bounds based on the clique, the fractional chromatic and the
chromatic numbers. Additionally, we extend the concept of \chi-critical graphs
to \chi_k-critical graphs. We identify the webs and antiwebs having this
property, for every integer k &lt;= 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5766</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5766</id><created>2011-08-29</created><authors><author><keyname>Pinto</keyname><forenames>Alexandre Miguel</forenames></author><author><keyname>Pereira</keyname><forenames>Lu&#x15b; Moniz</forenames></author></authors><title>Each normal logic program has a 2-valued Minimal Hypotheses semantics</title><categories>cs.LO</categories><comments>15 pages Proceedings of the 19th International Conference on
  Applications of Declarative Programming and Knowledge Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore a unifying approach --- that of hypotheses
assumption --- as a means to provide a semantics for all Normal Logic Programs
(NLPs), the Minimal Hypotheses (MH) semantics. This semantics takes a positive
hypotheses assumption approach as a means to guarantee the desirable properties
of model existence, relevance and cumulativity, and of generalizing the Stable
Models in the process. To do so we first introduce the fundamental semantic
concept of minimality of assumed positive hypotheses, define the MH semantics,
and analyze the semantics' properties and applicability. Indeed, abductive
Logic Programming can be conceptually captured by a strategy centered on the
assumption of abducibles (or hypotheses). Likewise, the Argumentation
perspective of Logic Programs also lends itself to an arguments (or hypotheses)
assumption approach. Previous works on Abduction have depicted the atoms of
default negated literals in NLPs as abducibles, i.e., assumable hypotheses. We
take a complementary and more general view than these works to NLP semantics by
employing positive hypotheses instead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5768</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5768</id><created>2011-08-29</created><updated>2011-11-25</updated><authors><author><keyname>Phillips</keyname><forenames>Caleb</forenames></author><author><keyname>Hoenigman</keyname><forenames>Rhonda</forenames></author><author><keyname>Higbee</keyname><forenames>Becky</forenames></author></authors><title>Food Redistribution as Optimization</title><categories>cs.OH</categories><comments>University of Colorado, Computer Science Department, Technical Report</comments><report-no>CU-CS-1085-11</report-no><journal-ref>Phillips C, Hoenigman R, Higbee B, Reed T (2013) Understanding the
  Sustainability of Retail Food Recovery. PLoS ONE 8(10): e75530</journal-ref><doi>10.1371/journal.pone.007553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the simultaneous problems of food waste and hunger in
the context of the possible solution of food (waste) rescue and redistribution.
To this end, we develop an empirical model that can be used in Monte Carlo
simulations to study the dynamics of the underlying problem. Our model's
parameters are derived from a unique data set provided by a large food bank and
food rescue organization in north central Colorado. We find that food supply is
a non-parametric heavy-tailed process that is well-modeled with an extreme
value peaks-over-threshold model. Although the underlying process is
stochastic, the basic approach of food rescue and redistribution appears to be
feasible both at small and large scales. The ultimate efficacy of this model is
intimately tied to the rate at which food expires and hence the ability to
preserve and quickly transport and redistribute food. The cost of the
redistribution is tied to the number and density of participating suppliers,
and costs can be reduced (and supply increased) simply by recruiting additional
donors to participate. Our results show that with sufficient funding and
manpower, a significant amount of food can be rescued from the waste stream and
used to feed the hungry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5781</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5781</id><created>2011-08-29</created><authors><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Phase Transition in Distance-Based Phylogeny Reconstruction</title><categories>math.PR cs.CE cs.DS math.ST q-bio.PE stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new distance-based phylogeny reconstruction technique which
provably achieves, at sufficiently short branch lengths, a logarithmic
sequence-length requirement---improving significantly over previous polynomial
bounds for distance-based methods and matching existing results for general
methods. The technique is based on an averaging procedure that implicitly
reconstructs ancestral sequences.
  In the same token, we extend previous results on phase transitions in
phylogeny reconstruction to general time-reversible models. More precisely, we
show that in the so-called Kesten-Stigum zone (roughly, a region of the
parameter space where ancestral sequences are well approximated by &quot;linear
combinations&quot; of the observed sequences) sequences of length $O(\log n)$
suffice for reconstruction when branch lengths are discretized. Here $n$ is the
number of extant species.
  Our results challenge, to some extent, the conventional wisdom that estimates
of evolutionary distances alone carry significantly less information about
phylogenies than full sequence datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5784</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5784</id><created>2011-08-29</created><authors><author><keyname>Melucci</keyname><forenames>Massimo</forenames></author></authors><title>Probability Ranking in Vector Spaces</title><categories>cs.IR cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Probability Ranking Principle states that the document set with the
highest values of probability of relevance optimizes information retrieval
effectiveness given the probabilities are estimated as accurately as possible.
The key point of the principle is the separation of the document set into two
subsets with a given level of fallout and with the highest recall. The paper
introduces the separation between two vector subspaces and shows that the
separation yields a more effective performance than the optimal separation into
subsets with the same available evidence, the performance being measured with
recall and fallout. The result is proved mathematically and exemplified
experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5793</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5793</id><created>2011-08-29</created><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author><author><keyname>Liu</keyname><forenames>Wanquan</forenames></author></authors><title>The $k$-error linear complexity distribution for $2^n$-periodic binary
  sequences</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The linear complexity and the $k$-error linear complexity of a sequence have
been used as important security measures for key stream sequence strength in
linear feedback shift register design. By studying the linear complexity of
binary sequences with period $2^n$, one could convert the computation of
$k$-error linear complexity into finding error sequences with minimal Hamming
weight. Based on Games-Chan algorithm, the $k$-error linear complexity
distribution of $2^n$-periodic binary sequences is investigated in this paper.
First, for $k=2,3$, the complete counting functions on the $k$-error linear
complexity of $2^n$-periodic balanced binary sequences (with linear complexity
less than $2^n$) are characterized. Second, for $k=3,4$, the complete counting
functions on the $k$-error linear complexity of $2^n$-periodic binary sequences
with linear complexity $2^n$ are presented. Third, as a consequence of these
results, the counting functions for the number of $2^n$-periodic binary
sequences with the $k$-error linear complexity for $k = 2$ and 3 are obtained.
Further more, an important result in a recent paper is proved to be not
completely correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5794</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5794</id><created>2011-08-29</created><authors><author><keyname>Beierle</keyname><forenames>Christoph</forenames></author><author><keyname>Kern-Isberner</keyname><forenames>Gabriele</forenames></author><author><keyname>S&#xf6;dler</keyname><forenames>Karl</forenames></author></authors><title>A Constraint Logic Programming Approach for Computing Ordinal
  Conditional Functions</title><categories>cs.AI</categories><comments>To appear in the Proceedings of the 25th Workshop on Logic
  Programming (WLP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to give appropriate semantics to qualitative conditionals of the
form &quot;if A then normally B&quot;, ordinal conditional functions (OCFs) ranking the
possible worlds according to their degree of plausibility can be used. An OCF
accepting all conditionals of a knowledge base R can be characterized as the
solution of a constraint satisfaction problem. We present a high-level,
declarative approach using constraint logic programming techniques for solving
this constraint satisfaction problem. In particular, the approach developed
here supports the generation of all minimal solutions; these minimal solutions
are of special interest as they provide a basis for model-based inference from
R.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5815</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5815</id><created>2011-08-29</created><updated>2011-12-10</updated><authors><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Barba</keyname><forenames>Lorena A.</forenames></author></authors><title>Hierarchical N-body simulations with auto-tuning for heterogeneous
  systems</title><categories>cs.NA cs.MS</categories><msc-class>70F10</msc-class><acm-class>D.1.2; D.1.3; G.1.0; G.1.2</acm-class><journal-ref>Computing in Science and Engineering, May/June 2012 (vol. 14 no.
  3), pp. 30-39</journal-ref><doi>10.1109/MCSE.2012.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the current hybridization of treecodes and FMMs, combined with
auto-tuning capabilities on heterogeneous architectures, the flexibility of
fast N-body methods has been greatly enhanced. These features are a requirement
to developing a black-box software library for fast N-body algorithms on
heterogeneous systems, which is our immediate goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5822</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5822</id><created>2011-08-29</created><updated>2011-09-12</updated><authors><author><keyname>Irving</keyname><forenames>Geoffrey</forenames></author></authors><title>Banded Householder representation of linear subspaces</title><categories>cs.NA</categories><comments>5 pages, 1 figure, submitted to Linear Algebra and its Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to compactly represent any $n$-dimensional subspace of $R^m$ as a
banded product of Householder reflections using $n(m - n)$ floating point
numbers. This is optimal since these subspaces form a Grassmannian space
$Gr_n(m)$ of dimension $n(m - n)$. The representation is stable and easy to
compute: any matrix can be factored into the product of a banded Householder
matrix and a square matrix using two to three QR decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5825</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5825</id><created>2011-08-30</created><authors><author><keyname>Inoue</keyname><forenames>Katsumi</forenames></author><author><keyname>Sakama</keyname><forenames>Chiaki</forenames></author><author><keyname>Wiese</keyname><forenames>Lena</forenames></author></authors><title>Confidentiality-Preserving Data Publishing for Credulous Users by
  Extended Abduction</title><categories>cs.AI</categories><comments>Paper appears in the Proceedings of the 19th International Conference
  on Applications of Declarative Programming and Knowledge Management (INAP
  2011)</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Publishing private data on external servers incurs the problem of how to
avoid unwanted disclosure of confidential data. We study a problem of
confidentiality in extended disjunctive logic programs and show how it can be
solved by extended abduction. In particular, we analyze how credulous
non-monotonic reasoning affects confidentiality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5837</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5837</id><created>2011-08-30</created><authors><author><keyname>Nguyen</keyname><forenames>Mai</forenames></author><author><keyname>Janhunen</keyname><forenames>Tomi</forenames></author><author><keyname>Niemel&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Translating Answer-Set Programs into Bit-Vector Logic</title><categories>cs.AI cs.LO</categories><comments>12 pages, 1 figure, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer set programming (ASP) is a paradigm for declarative problem solving
where problems are first formalized as rule sets, i.e., answer-set programs, in
a uniform way and then solved by computing answer sets for programs. The
satisfiability modulo theories (SMT) framework follows a similar modelling
philosophy but the syntax is based on extensions of propositional logic rather
than rules. Quite recently, a translation from answer-set programs into
difference logic was provided---enabling the use of particular SMT solvers for
the computation of answer sets. In this paper, the translation is revised for
another SMT fragment, namely that based on fixed-width bit-vector theories.
Thus, even further SMT solvers can be harnessed for the task of computing
answer sets. The results of a preliminary experimental comparison are also
reported. They suggest a level of performance which is similar to that achieved
via difference logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5838</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5838</id><created>2011-08-30</created><updated>2012-09-17</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author></authors><title>Off-grid Direction of Arrival Estimation Using Sparse Bayesian Inference</title><categories>stat.AP cs.IT math.IT stat.ML</categories><comments>To appear in the IEEE Trans. Signal Processing. This is a revised,
  shortened version of version 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direction of arrival (DOA) estimation is a classical problem in signal
processing with many practical applications. Its research has recently been
advanced owing to the development of methods based on sparse signal
reconstruction. While these methods have shown advantages over conventional
ones, there are still difficulties in practical situations where true DOAs are
not on the discretized sampling grid. To deal with such an off-grid DOA
estimation problem, this paper studies an off-grid model that takes into
account effects of the off-grid DOAs and has a smaller modeling error. An
iterative algorithm is developed based on the off-grid model from a Bayesian
perspective while joint sparsity among different snapshots is exploited by
assuming a Laplace prior for signals at all snapshots. The new approach applies
to both single snapshot and multi-snapshot cases. Numerical simulations show
that the proposed algorithm has improved accuracy in terms of mean squared
estimation error. The algorithm can maintain high estimation accuracy even
under a very coarse sampling grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5845</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5845</id><created>2011-08-30</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Opthof</keyname><forenames>Tobias</forenames></author></authors><title>A Rejoinder on Energy versus Impact Indicators</title><categories>cs.DL physics.soc-ph</categories><comments>Scientometrics, in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation distributions are so skewed that using the mean or any other central
tendency measure is ill-advised. Unlike G. Prathap's scalar measures (Energy,
Exergy, and Entropy or EEE), the Integrated Impact Indicator (I3) is based on
non-parametric statistics using the (100) percentiles of the distribution.
Observed values can be tested against expected ones; impact can be qualified at
the article level and then aggregated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5860</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5860</id><created>2011-08-30</created><updated>2012-12-29</updated><authors><author><keyname>Pandolfi</keyname><forenames>Luciano</forenames></author><author><keyname>Priola</keyname><forenames>Enrico</forenames></author><author><keyname>Zabczyk</keyname><forenames>Jerzy</forenames></author></authors><title>Linear Operator Inequality and Null Controllability with Vanishing
  Energy for unbounded control systems</title><categories>math.OC cs.SY math.AP</categories><comments>In this version we have also added a section on examples and
  applications of our main results. This version is similar to the one which
  will be published on &quot;SIAM Journal on Control and Optimization&quot; (SIAM)</comments><msc-class>93C20, 93C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider linear systems on a separable Hilbert space $H$, which are null
controllable at some time $T_0&gt;0$ under the action of a point or boundary
control. Parabolic and hyperbolic control systems usually studied in
applications are special cases. To every initial state $ y_0 \in H$ we
associate the minimal &quot;energy&quot; needed to transfer $ y_0 $ to $ 0 $ in a time $
T \ge T_0$ (&quot;energy&quot; of a control being the square of its $ L^2 $ norm). We
give both necessary and sufficient conditions under which the minimal energy
converges to $ 0 $ for $ T\to+\infty $. This extends to boundary control
systems the concept of null controllability with vanishing energy introduced by
Priola and Zabczyk (Siam J. Control Optim. 42 (2003)) for distributed systems.
The proofs in Priola-Zabczyk paper depend on properties of the associated
Riccati equation, which are not available in the present, general setting. Here
we base our results on new properties of the quadratic regulator problem with
stability and the Linear Operator Inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5864</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5864</id><created>2011-08-30</created><authors><author><keyname>Toubiana</keyname><forenames>Vincent</forenames></author><author><keyname>Verdot</keyname><forenames>Vincent</forenames></author></authors><title>Show Me Your Cookie And I Will Tell You Who You Are</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the success of Web applications, most of our data is now stored on
various third-party servers where they are processed to deliver personalized
services. Naturally we must be authenticated to access this personal
information, but the use of personalized services only restricted by
identification could indirectly and silently leak sensitive data. We analyzed
Google Web Search access mechanisms and found that the current policy applied
to session cookies could be used to retrieve users' personal data. We describe
an attack scheme leveraging the search personalization (based on the same SID
cookie) to retrieve a part of the victim's click history and even some of her
contacts. We implemented a proof of concept of this attack on Firefox and
Chrome Web browsers and conducted an experiment with ten volunteers. Thanks to
this prototype we were able to recover up to 80% of the user's search click
history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5871</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5871</id><created>2011-08-30</created><updated>2012-07-21</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author><author><keyname>Zame</keyname><forenames>William</forenames></author></authors><title>Designing Practical Distributed Exchange for Online Communities</title><categories>cs.GT</categories><comments>55 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many online systems, individuals provide services for each other; the
recipient of the service obtains a benefit but the provider of the service
incurs a cost. If benefit exceeds cost, provision of the service increases
social welfare and should therefore be encouraged -- but the individuals
providing the service gain no (immediate) benefit from providing the service
and hence have an incentive to withhold service. Hence there is scope for
designing a system that improves welfare by encouraging exchange. To operate
successfully within the confines of the online environment, such a system
should be distributed, practicable, and consistent with individual incentives.
This paper proposes and analyzes a simple such system that relies on the
exchange of {\em tokens}; the emphasis is on the design of a protocol (number
of tokens and suggested strategies). We provide estimates for the efficiency of
such protocols and show that choosing the right protocol will lead to almost
full efficiency if agents are sufficiently patient. However, choosing the wrong
protocols may lead to an enormous loss of efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5881</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5881</id><created>2011-08-30</created><updated>2012-10-25</updated><authors><author><keyname>Manganiello</keyname><forenames>Felice</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Spread Decoding in Extension Fields</title><categories>cs.IT math.IT</categories><comments>Submitted for publication to Finite Fields and their Applications
  (Elsevier)</comments><journal-ref>Finite Fields and their Applications, volume 25, pages 94--105,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spread code is a set of vector spaces of a fixed dimension over a finite
field Fq with certain properties used for random network coding. It can be
constructed in different ways which lead to different decoding algorithms. In
this work we present a new representation of spread codes with a minimum
distance decoding algorithm which is efficient when the codewords, the received
space and the error space have small dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5890</identifier>
 <datestamp>2012-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5890</id><created>2011-08-30</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author></authors><title>Coordinating Interfering Transmissions in Cooperative Wireless LANs</title><categories>cs.NI cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2011.091411.102084</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a cooperative medium access control (MAC) protocol
that is designed for a physical layer that can decode interfering transmissions
in distributed wireless networks. The proposed protocol pro-actively enforces
two independent packet transmissions to interfere in a controlled and
cooperative manner. The protocol ensures that when a node desires to transmit a
unicast packet, regardless of the destination, it coordinates with minimal
overhead with relay nodes in order to concurrently transmit over the wireless
channel with a third node. The relay is responsible for allowing packets from
the two selected nodes to interfere only when the desired packets can be
decoded at the appropriate destinations and increase the sum-rate of the
cooperative transmission. In case this is not feasible, classic cooperative or
direct transmission is adopted. To enable distributed, uncoordinated, and
adaptive operation of the protocol, a relay selection mechanism is introduced
so that the optimal relay is selected dynamically and depending on the channel
conditions. The most important advantage of the protocol is that interfering
transmissions can originate from completely independent unicast transmissions
from two senders. We present simulation results that validate the efficacy of
our proposed scheme in terms of throughput and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5893</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5893</id><created>2011-08-30</created><authors><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Trehan</keyname><forenames>Amitabh</forenames></author></authors><title>Edge-preserving self-healing: keeping network backbones densely
  connected</title><categories>cs.DC cs.DS cs.NI</categories><comments>Submitted to IEEE InfoComm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Healing algorithms play a crucial part in distributed P2P networks where
failures occur continuously and frequently. Several self-healing algorithms
have been suggested recently [IPDPS'08, PODC'08, PODC'09, PODC'11] in a line of
work that has yielded gradual improvements in the properties ensured on the
graph. This work motivates a strong general phenomenon of edge-preserving
healing that aims at obtaining self-healing algorithms with the constraint that
all original edges in the graph (not deleted by the adversary), be retained in
every intermediate graph.
  The previous algorithms, in their nascent form, are not explicitly edge
preserving. In this paper, we show they can be suitably modified (We introduce
Xheal+, an edge-preserving version of Xheal[PODC'11]). Towards this end, we
present a general self-healing model that unifies the previous models. The main
contribution of this paper is not in the technical complexity, rather in the
simplicity with which the edge-preserving property can be ensured and the
message that this is a crucial property with several benefits. In particular,
we highlight this by showing that, almost as an immediate corollary, subgraph
densities are preserved or increased. Maintaining density is a notion motivated
by the fact that in certain distributed networks, certain nodes may require and
initially have a larger number of inter-connections. It is vital that a healing
algorithm, even amidst failures, respect these requirements. Our suggested
modifications yield such subgraph density preservation as a by product. In
addition, edge preservation helps maintain any subgraph induced property that
is monotonic. Also, algorithms that are edge-preserving require minimal
alteration of edges which can be an expensive cost in healing - something that
has not been modeled in any of the past work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5904</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5904</id><created>2011-08-30</created><authors><author><keyname>Vaya</keyname><forenames>Shailesh</forenames></author></authors><title>Information Dissemination in Unknown Radio networks with Large Labels</title><categories>cs.DC cs.DM cs.DS</categories><comments>19 pages</comments><acm-class>F.2.m; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problems of deterministic broadcasting and gossiping in
completely unknown ad-hoc radio networks. We assume that nothing is known to
the nodes about the topology or even the size of the network, $n$, except that
$n &gt; 1$. Protocols for vanilla model, when $n$ is known, may be run for
increasingly larger estimates $2^i$ on the size of the network, but one cannot
determine when such a protocol should terminate. Thus, to carry this design
paradigm, successful completion or in-completion of the process should be
detected, and this knowledge circulated in the network. We consider the problem
of deterministic Acknowledged Broadcasting and Gossiping when nodes can take
polynomially large labels.
  For the above setting, we present the following results for strongly
connected networks: (a) A deterministic protocol for acknowledged broadcasting
which takes $NRG(n,n^c)$ rounds, where $NRG(n,n^c)$ is the round complexity of
deterministic gossiping for vanilla model. (b) A deterministic protocol for
acknowledged gossiping, which takes $O(n^2 \lg n)$ rounds when collision
detection mechanism is available. The structure of the transmissions of nodes
in the network, to enable them to infer collisions, and discover existence of
unknown in-neighborhood as a result, is abstracted as a family of integral sets
called Selecting-Colliding family. We prove the existence of
Selecting-Colliding families using the probabilistic method and employ them to
design protocol for acknowledged gossiping when no collision detection
mechanism is available.
  Finally, we present a deterministic protocol for acknowledged broadcasting
for bidirectional networks, with a round complexity of $O(n \lg n)$ rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5934</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5934</id><created>2011-08-30</created><updated>2011-11-02</updated><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>de Oliveira</keyname><forenames>Paulo Murilo Castro</forenames></author></authors><title>The Sznajd model with limited persuasion: competition between
  high-reputation and hesitant agents</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>12 pages, 7 figures, submitted for publication</comments><journal-ref>J. Stat. Mech. P11004 (2011)</journal-ref><doi>10.1088/1742-5468/2011/11/P11004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study a modified version of the two-dimensional Sznajd
sociophysics model. In particular, we consider the effects of agents'
reputations in the persuasion rules. In other words, a high-reputation group
with a common opinion may convince their neighbors with probability $p$, which
induces an increase of the group's reputation. On the other hand, there is
always a probability $q=1-p$ of the neighbors to keep their opinions, which
induces a decrease of the group's reputation. These rules describe a
competition between groups with high reputation and hesitant agents, which
makes the full-consensus states (with all spins pointing in one direction) more
difficult to be reached. As consequences, the usual phase transition does not
occur for $p&lt;p_{c} \sim 0.69$ and the system presents realistic democracy-like
situations, where the majority of spins are aligned in a certain direction, for
a wide range of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5935</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5935</id><created>2011-08-30</created><updated>2013-11-03</updated><authors><author><keyname>Elia</keyname><forenames>Michele</forenames></author><author><keyname>Piva</keyname><forenames>Matteo</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author></authors><title>The Rabin cryptosystem revisited</title><categories>math.NT cs.CR cs.IT math.IT</categories><comments>minor review + introduction of a deterministic scheme using quartic
  reciprocity that works for primes congruent 5 modulo 8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rabin public-key cryptosystem is revisited with a focus on the problem of
identifying the encrypted message unambiguously for any pair of primes. In
particular, a deterministic scheme using quartic reciprocity is described that
works for primes congruent 5 modulo 8, a case that was still open. Both
theoretical and practical solutions are presented. The Rabin signature is also
reconsidered and a deterministic padding mechanism is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5943</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5943</id><created>2011-08-30</created><updated>2011-09-22</updated><authors><author><keyname>Zhao</keyname><forenames>Xishun</forenames></author><author><keyname>Shen</keyname><forenames>Yuping</forenames></author></authors><title>Proof System for Plan Verification under 0-Approximation Semantics</title><categories>cs.AI cs.LO</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a proof system is developed for plan verification problems
$\{X\}c\{Y\}$ and $\{X\}c\{KW p\}$ under 0-approximation semantics for
${\mathcal A}_K$. Here, for a plan $c$, two sets $X,Y$ of fluent literals, and
a literal $p$, $\{X\}c\{Y\}$ (resp. $\{X\}c\{KW p\}$) means that all literals
of $Y$ become true (resp. $p$ becomes known) after executing $c$ in any initial
state in which all literals in $X$ are true.Then, soundness and completeness
are proved. The proof system allows verifying plans and generating plans as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5964</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5964</id><created>2011-08-30</created><authors><author><keyname>Elsholtz</keyname><forenames>Christian</forenames></author><author><keyname>Heuberger</keyname><forenames>Clemens</forenames></author><author><keyname>Prodinger</keyname><forenames>Helmut</forenames></author></authors><title>The number of Huffman codes, compact trees, and sums of unit fractions</title><categories>math.CO cs.DM math.NT</categories><msc-class>05A16, 05A15, 05C05, 05C30, 11D68, 68P30, 68R10, 94A10</msc-class><journal-ref>IEEE Trans. Inf. Theory 59 (2013), 1065-1075</journal-ref><doi>10.1109/TIT.2012.2226560</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of &quot;nonequivalent&quot; Huffman codes of length r over an alphabet of
size t has been studied frequently. Equivalently, the number of &quot;nonequivalent&quot;
complete t-ary trees has been examined. We first survey the literature,
unifying several independent approaches to the problem. Then, improving on
earlier work we prove a very precise asymptotic result on the counting
function, consisting of two main terms and an error term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5974</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5974</id><created>2011-08-30</created><authors><author><keyname>Wero&#x144;ski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Sienkiewicz</keyname><forenames>Julian</forenames></author><author><keyname>Paltoglou</keyname><forenames>Georgios</forenames></author><author><keyname>Buckley</keyname><forenames>Kevan</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Ho&#x142;yst</keyname><forenames>Janusz A.</forenames></author></authors><title>Emotional Analysis of Blogs and Forums Data</title><categories>cs.CL physics.data-an physics.soc-ph</categories><comments>REVTEX format, 5 pages, 6 figures, 2 tables, accepted to Acta Physica
  Polonica A</comments><journal-ref>Acta Physica Polonica A 121, B-128 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform a statistical analysis of emotionally annotated comments in two
large online datasets, examining chains of consecutive posts in the
discussions. Using comparisons with randomised data we show that there is a
high level of correlation for the emotional content of messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.5985</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.5985</id><created>2011-08-30</created><updated>2013-04-25</updated><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Fisikopoulos</keyname><forenames>Vissarion</forenames></author><author><keyname>Konaxis</keyname><forenames>Christos</forenames></author><author><keyname>Pe&#xf1;aranda</keyname><forenames>Luis</forenames></author></authors><title>An Oracle-based, Output-sensitive Algorithm for Projections of Resultant
  Polytopes</title><categories>cs.SC cs.CG</categories><comments>27 pages, 7 figures, 4 tables. In IJCGA (invited papers from SoCG
  '12)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We design an algorithm to compute the Newton polytope of the resultant, known
as resultant polytope, or its orthogonal projection along a given direction.
The resultant is fundamental in algebraic elimination, optimization, and
geometric modeling. Our algorithm exactly computes vertex- and
halfspace-representations of the polytope using an oracle producing resultant
vertices in a given direction, thus avoiding walking on the polytope whose
dimension is alpha-n-1, where the input consists of alpha points in Z^n. Our
approach is output-sensitive as it makes one oracle call per vertex and facet.
It extends to any polytope whose oracle-based definition is advantageous, such
as the secondary and discriminant polytopes. Our publicly available
implementation uses the experimental CGAL package triangulation. Our method
computes 5-, 6- and 7-dimensional polytopes with 35K, 23K and 500 vertices,
respectively, within 2hrs, and the Newton polytopes of many important surface
equations encountered in geometric modeling in &lt;1sec, whereas the corresponding
secondary polytopes are intractable. It is faster than tropical geometry
software up to dimension 5 or 6. Hashing determinantal predicates accelerates
execution up to 100 times. One variant computes inner and outer approximations
with, respectively, 90% and 105% of the true volume, up to 25 times faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6003</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6003</id><created>2011-08-29</created><updated>2011-09-12</updated><authors><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Zanin</keyname><forenames>Massimiliano</forenames></author><author><keyname>Herrera</keyname><forenames>Perfecto</forenames></author><author><keyname>Serra</keyname><forenames>Xavier</forenames></author></authors><title>Characterization and exploitation of community structure in cover song
  networks</title><categories>cs.IR cs.MM cs.SI physics.data-an stat.ML</categories><journal-ref>Pattern Recognition Letters 33(9): 1032-1041, 2012</journal-ref><doi>10.1016/j.patrec.2012.02.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of community detection algorithms is explored within the framework of
cover song identification, i.e. the automatic detection of different audio
renditions of the same underlying musical piece. Until now, this task has been
posed as a typical query-by-example task, where one submits a query song and
the system retrieves a list of possible matches ranked by their similarity to
the query. In this work, we propose a new approach which uses song communities
to provide more relevant answers to a given query. Starting from the output of
a state-of-the-art system, songs are embedded in a complex weighted network
whose links represent similarity (related musical content). Communities inside
the network are then recognized as groups of covers and this information is
used to enhance the results of the system. In particular, we show that this
approach increases both the coherence and the accuracy of the system.
Furthermore, we provide insight into the internal organization of individual
cover song communities, showing that there is a tendency for the original song
to be central within the community. We postulate that the methods and results
presented here could be relevant to other query-by-example tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6007</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6007</id><created>2011-08-30</created><authors><author><keyname>Triska</keyname><forenames>Markus</forenames></author></authors><title>Domain-specific Languages in a Finite Domain Constraint Programming
  System</title><categories>cs.AI</categories><comments>Proceedings of the 19th International Conference on Applications of
  Declarative Programming and Knowledge Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present domain-specific languages (DSLs) that we devised
for their use in the implementation of a finite domain constraint programming
system, available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs
are used in propagator selection and constraint reification. In these areas,
they lead to concise specifications that are easy to read and reason about. At
compilation time, these specifications are translated to Prolog code, reducing
interpretative run-time overheads. The devised languages can be used in the
implementation of other finite domain constraint solvers as well and may
contribute to their correctness, conciseness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6010</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6010</id><created>2011-08-30</created><authors><author><keyname>Deb</keyname><forenames>Novarun</forenames></author><author><keyname>Chakraborty</keyname><forenames>Manali</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author></authors><title>A State-of-the-art Survey on IDS for Mobile Ad-Hoc Networks and Wireless
  Mesh Networks</title><categories>cs.CR</categories><comments>Accepted for publication in PDCTA 2011 to be held in Chennair during
  September 25-27, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Intrusion Detection System (IDS) detects malicious and selfish nodes in a
network. Ad hoc networks are often secured by using either intrusion detection
or by secure routing. Designing efficient IDS for wireless ad-hoc networks that
would not affect the performance of the network significantly is indeed a
challenging task. Arguably, the most common thing in a review paper in the
domain of wireless networks is to compare the performances of different
solutions using simulation results. However, variance in multiple configuration
aspects including that due to different underlying routing protocols, makes the
task of simulation based comparative evaluation of IDS solutions somewhat
unrealistic. In stead, the authors have followed an analytic approach to
identify the gaps in the existing IDS solutions for MANETs and wireless mesh
networks. The paper aims to ease the job of a new researcher by exposing him to
the state of the art research issues on IDS. Nearly 80% of the works cited in
this paper are published with in last 3 to 4 years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6016</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6016</id><created>2011-08-30</created><authors><author><keyname>Gemmell</keyname><forenames>Jim</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Chandra</keyname><forenames>Ashok K.</forenames></author></authors><title>Improving Entity Resolution with Global Constraints</title><categories>cs.DB cs.IR</categories><comments>10 pages, 13 figures</comments><report-no>MSR-TR-2011-100</report-no><acm-class>H.2; H.3.3; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some of the greatest advances in web search have come from leveraging
socio-economic properties of online user behavior. Past advances include
PageRank, anchor text, hubs-authorities, and TF-IDF. In this paper, we
investigate another socio-economic property that, to our knowledge, has not yet
been exploited: sites that create lists of entities, such as IMDB and Netflix,
have an incentive to avoid gratuitous duplicates. We leverage this property to
resolve entities across the different web sites, and find that we can obtain
substantial improvements in resolution accuracy. This improvement in accuracy
also translates into robustness, which often reduces the amount of training
data that must be labeled for comparing entities across many sites.
Furthermore, the technique provides robustness when resolving sites that have
some duplicates, even without first removing these duplicates. We present
algorithms with very strong precision and recall, and show that max weight
matching, while appearing to be a natural choice turns out to have poor
performance in some situations. The presented techniques are now being used in
the back-end entity resolution system at a major Internet search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6022</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6022</id><created>2011-08-30</created><authors><author><keyname>Solomon</keyname><forenames>Shay</forenames></author><author><keyname>Elkin</keyname><forenames>Michael</forenames></author></authors><title>Balancing Degree, Diameter and Weight in Euclidean Spanners</title><categories>cs.CG cs.DS</categories><comments>27 pages, 7 figures; a preliminary version of this paper appeared in
  ESA'10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we devise a novel \emph{unified} construction of Euclidean
spanners that trades between the maximum degree, diameter and weight
gracefully. For a positive integer k, our construction provides a
(1+eps)-spanner with maximum degree O(k), diameter O(log_k n + alpha(k)),
weight O(k \cdot log_k n \cdot log n) \cdot w(MST(S)), and O(n) edges. Note
that for k= n^{1/alpha(n)} this gives rise to diameter O(alpha(n)), weight
O(n^{1/alpha(n)} \cdot log n \cdot alpha(n)) \cdot w(MST(S)) and maximum degree
O(n^{1/alpha(n)}), which improves upon a classical result of Arya et al.
\cite{ADMSS95}; in the corresponding result from \cite{ADMSS95} the spanner has
the same number of edges and diameter, but its weight and degree may be
arbitrarily large. Also, for k = O(1) this gives rise to maximum degree O(1),
diameter O(log n) and weight O(log^2 n) \cdot w(MST(S)), which reproves another
classical result of Arya et al. \cite{ADMSS95}. Our bound of O(log_k n +
alpha(k)) on the diameter is optimal under the constraints that the maximum
degree is O(k) and the number of edges is O(n). Our bound on the weight is
optimal up to a factor of log n. Our construction also provides a similar
tradeoff in the complementary range of parameters, i.e., when the weight should
be smaller than log^2 n, but the diameter is allowed to grow beyond log n.
  For random point sets in the d-dimensional unit cube, we &quot;shave&quot; a factor of
log n from the weight bound. Specifically, in this case our construction
achieves maximum degree O(k), diameter O(log_k n + alpha(k)) and weight that is
with high probability O(k \cdot log_k n) \cdot w(MST(S)).
  Finally, en route to these results we devise optimal constructions of
1-spanners for general tree metrics, which are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6031</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6031</id><created>2011-08-30</created><authors><author><keyname>Lee</keyname><forenames>Taeyoung</forenames></author></authors><title>Robust Adaptive Geometric Tracking Controls on SO(3) with an Application
  to the Attitude Dynamics of a Quadrotor UAV</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides new results for a robust adaptive tracking control of the
attitude dynamics of a rigid body. Both of the attitude dynamics and the
proposed control system are globally expressed on the special orthogonal group,
to avoid complexities and ambiguities associated with other attitude
representations such as Euler angles or quaternions. By designing an adaptive
law for the inertia matrix of a rigid body, the proposed control system can
asymptotically follow an attitude command without the knowledge of the inertia
matrix, and it is extended to guarantee boundedness of tracking errors in the
presence of unstructured disturbances. These are illustrated by numerical
examples and experiments for the attitude dynamics of a quadrotor UAV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6033</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6033</id><created>2011-08-30</created><updated>2011-11-10</updated><authors><author><keyname>Baldeschi</keyname><forenames>Riccardo Colini</forenames></author><author><keyname>Leonardi</keyname><forenames>Stefano</forenames></author><author><keyname>Schaefer</keyname><forenames>Guido</forenames></author></authors><title>Multi-Unit Auction with Diminishing Marginal Valuations and Capacities</title><categories>cs.GT</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6046</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6046</id><created>2011-08-30</created><authors><author><keyname>Milosavljevic</keyname><forenames>Nebojsa</forenames></author><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Optimal Deterministic Polynomial-Time Data Exchange for Omniscience</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing a deterministic polynomial time
algorithm that achieves omniscience, in a rate-optimal manner, among a set of
users that are interested in a common file but each has only partial knowledge
about it as side-information. Assuming that the collective information among
all the users is sufficient to allow the reconstruction of the entire file, the
goal is to minimize the (possibly weighted) amount of bits that these users
need to exchange over a noiseless public channel in order for all of them to
learn the entire file. Using established connections to the multi-terminal
secrecy problem, our algorithm also implies a polynomial-time method for
constructing a maximum size secret shared key in the presence of an
eavesdropper. We consider the following types of side-information settings: (i)
side information in the form of uncoded fragments/packets of the file, where
the users' side-information consists of subsets of the file; (ii) side
information in the form of linearly correlated packets, where the users have
access to linear combinations of the file packets; and (iii) the general
setting where the the users' side-information has an arbitrary (i.i.d.)
correlation structure. Building on results from combinatorial optimization, we
provide a polynomial-time algorithm (in the number of users) that, first finds
the optimal rate allocations among these users, then determines an explicit
transmission scheme (i.e., a description of which user should transmit what
information) for cases (i) and (ii).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6087</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6087</id><created>2011-08-30</created><authors><author><keyname>Navaravong</keyname><forenames>Leenhapat</forenames></author><author><keyname>Shea</keyname><forenames>John M.</forenames></author><author><keyname>Pasiliao</keyname><forenames>Eduardo L.</forenames><suffix>Jr</suffix></author><author><keyname>Barnette</keyname><forenames>Gregory L.</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Optimizing Network Topology to Reduce Aggregate Traffic in Systems of
  Mobile Robots</title><categories>cs.NI</categories><comments>Submitted to Infocom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems of networked mobile robots, such as unmanned aerial or ground
vehicles, will play important roles in future military and commercial
applications. The communications for such systems will typically be over
wireless links and may require that the robots form an ad hoc network and
communicate on a peer-to-peer basis. In this paper, we consider the problem of
optimizing the network topology to minimize the total traffic in a network
required to support a given set of data flows under constraints on the amount
of movement possible at each mobile robot. In this paper, we consider a
subclass of this problem in which the initial and final topologies are trees,
and the movement restrictions are given in terms of the number of edges in the
graph that must be traversed. We develop algorithms to optimize the network
topology while maintaining network connectivity during the topology
reconfiguration process. Our topology reconfiguration algorithm uses the
concept of prefix labelling and routing to move nodes through the network while
maintaining network connectivity. We develop two algorithms to determine the
final network topology: an optimal, but computationally complex algorithm, and
a greedy suboptimal algorithm that has much lower complexity. We present
simulation results to compare the performance of these algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6088</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6088</id><created>2011-08-30</created><authors><author><keyname>Foster</keyname><forenames>Dean</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author></authors><title>No Internal Regret via Neighborhood Watch</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which attains O(\sqrt{T}) internal (and thus
external) regret for finite games with partial monitoring under the local
observability condition. Recently, this condition has been shown by (Bartok,
Pal, and Szepesvari, 2011) to imply the O(\sqrt{T}) rate for partial monitoring
games against an i.i.d. opponent, and the authors conjectured that the same
holds for non-stochastic adversaries. Our result is in the affirmative, and it
completes the characterization of possible rates for finite partial-monitoring
games, an open question stated by (Cesa-Bianchi, Lugosi, and Stoltz, 2006). Our
regret guarantees also hold for the more general model of partial monitoring
with random signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6097</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6097</id><created>2011-08-30</created><authors><author><keyname>Drescher</keyname><forenames>Conrad</forenames></author><author><keyname>Lynce</keyname><forenames>Ines</forenames></author><author><keyname>Treinen</keyname><forenames>Ralf</forenames></author></authors><title>Proceedings Second Workshop on Logics for Component Configuration</title><categories>cs.LO cs.SE</categories><comments>EPTCS 65, 2011</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.65</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the second international
workshop on Logics for Component Configuration (LoCoCo 2011) which was
associated with the International Conference on Principles and Practice of
Constraint Programming (CP 2011) and which took place on September 12, 2011 in
Perugia, Italy. Representing and solving configuration problems is a hot topic
of great importance for many application domains. For example, modern software
distributions are based on the notion of components, which denote units of
independent development and deployment. Components provide the necessary
flexibility when organizing a complex software distribution, but also are a
challenge when it comes to selecting components from a large repository of
possible choices, and configuring these components according to user needs,
resource constraints, and interdependencies with other components. Some
well-known examples of complex systems of components in the world of Free and
Open Source software are the different distributions for GNU/Linux, BSD, or
Eclipse plugins. The LoCoCo workshop focus on logic-based methods for
specifying and solving complex configuration problems. The goal of the workshop
was to bring together both researchers and practitioners active in the area of
component configuration of systems, using different modeling and solving
techniques, such as constraint and logic programming, description logics,
satisfiability and its extensions. The workshop was an opportunity to discuss
common and complementary solutions for solving component configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6113</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6113</id><created>2011-08-30</created><updated>2011-11-05</updated><authors><author><keyname>Albert</keyname><forenames>Reka</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Gitter</keyname><forenames>Anthony</forenames></author><author><keyname>Gursoy</keyname><forenames>Gamze</forenames></author><author><keyname>Hegde</keyname><forenames>Rashmi</forenames></author><author><keyname>Paul</keyname><forenames>Pradyut</forenames></author><author><keyname>Sivanathan</keyname><forenames>Gowri Sangeetha</forenames></author><author><keyname>Sontag</keyname><forenames>Eduardo</forenames></author></authors><title>A New Computationally Efficient Measure of Topological Redundancy of
  Biological and Social Networks</title><categories>physics.soc-ph cs.DM cs.SI math.DS q-bio.MN</categories><comments>18 pages</comments><msc-class>68R10, 05C85, 05C82, 05C20, 05C40, 91D30, 92C42, 93C15, 37N25</msc-class><acm-class>J.3; J.4; F.2.2; G.2.1; G.2.2; G.2.3</acm-class><journal-ref>Physical Review E, 84 (3), 036117, 2011</journal-ref><doi>10.1103/PhysRevE.84.036117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that biological and social interaction networks have a
varying degree of redundancy, though a consensus of the precise cause of this
is so far lacking. In this paper, we introduce a topological redundancy measure
for labeled directed networks that is formal, computationally efficient and
applicable to a variety of directed networks such as cellular signaling,
metabolic and social interaction networks. We demonstrate the computational
efficiency of our measure by computing its value and statistical significance
on a number of biological and social networks with up to several thousands of
nodes and edges. Our results suggest a number of interesting observations: (1)
social networks are more redundant that their biological counterparts, (2)
transcriptional networks are less redundant than signaling networks, (3) the
topological redundancy of the C. elegans metabolic network is largely due to
its inclusion of currency metabolites, and (4) the redundancy of signaling
networks is highly (negatively) correlated with the monotonicity of their
dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6114</identifier>
 <datestamp>2011-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6114</id><created>2011-08-30</created><updated>2011-12-14</updated><authors><author><keyname>Sarabia</keyname><forenames>Manuel Gonzalez</forenames></author><author><keyname>Marquez</keyname><forenames>Carlos Renteria</forenames></author><author><keyname>Rosales</keyname><forenames>Eliseo Sarmiento</forenames></author></authors><title>Projective Parameterized Linear Codes Arising from some Matrices and
  their Main Parameters</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we will estimate the main parameters of some evaluation codes
which are known as projective parameterized codes. We will find the length of
these codes and we will give a formula for the dimension in terms of the
Hilbert function associated to two ideals, one of them being the vanishing
ideal of the projective torus. Also we will find an upper bound for the minimum
distance and, in some cases, we will give some lower bounds for the regularity
index and the minimum distance. These lower bounds work in several cases,
particularly for any projective parameterized code associated to the incidence
matrix of uniform clutters and then they work in the case of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6119</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6119</id><created>2011-08-30</created><authors><author><keyname>Ling</keyname><forenames>Xiang</forenames></author><author><keyname>Lau</keyname><forenames>Henry Y. K.</forenames></author><author><keyname>Jiang</keyname><forenames>Rui</forenames></author><author><keyname>Hu</keyname><forenames>Mao-Bin</forenames></author></authors><title>Pheromone Static Routing Strategy for Complex Networks</title><categories>physics.data-an cond-mat.stat-mech cs.NI</categories><comments>10 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we adopt the concept of pheromone to generate a set of static
paths that can reach the performance of global dynamic routing strategy [Phys.
Rev. E 81, 016113(2010)]. In the test stage, pheromone is dropped to the nodes
by packets forwarded by the global dynamic routing strategy. After that, static
paths are generated according to the density of pheromone. The output paths can
greatly improve traffic systems' overall capacity on different network
structures, including scale-free networks, small-world networks and random
graphs. Because the paths are static, the system needs much less computational
resource than the global dynamic routing strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6121</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6121</id><created>2011-08-30</created><authors><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author></authors><title>The Value of Feedback in Decentralized Detection</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the decentralized binary hypothesis testing problem in networks
with feedback, where some or all of the sensors have access to compressed
summaries of other sensors' observations. We study certain two-message feedback
architectures, in which every sensor sends two messages to a fusion center,
with the second message based on full or partial knowledge of the first
messages of the other sensors. We also study one-message feedback
architectures, in which each sensor sends one message to a fusion center, with
a group of sensors having full or partial knowledge of the messages from the
sensors not in that group. Under either a Neyman-Pearson or a Bayesian
formulation, we show that the asymptotically optimal (in the limit of a large
number of sensors) detection performance (as quantified by error exponents)
does not benefit from the feedback messages, if the fusion center remembers all
sensor messages. However, feedback can improve the Bayesian detection
performance in the one-message feedback architecture if the fusion center has
limited memory; for that case, we determine the corresponding optimal error
exponents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6123</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6123</id><created>2011-08-30</created><updated>2012-03-02</updated><authors><author><keyname>Bolot</keyname><forenames>Jean</forenames></author><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Taft</keyname><forenames>Nina</forenames></author></authors><title>Private Decayed Sum Estimation under Continual Observation</title><categories>cs.DS cs.CR</categories><doi>10.1145/2448496.2448530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In monitoring applications, recent data is more important than distant data.
How does this affect privacy of data analysis? We study a general class of data
analyses - computing predicate sums - with privacy. Formally, we study the
problem of estimating predicate sums {\em privately}, for sliding windows (and
other well-known decay models of data, i.e. exponential and polynomial decay).
We extend the recently proposed continual privacy model of Dwork et al.
  We present algorithms for decayed sum which are $\eps$-differentially
private, and are accurate. For window and exponential decay sums, our
algorithms are accurate up to additive $1/\eps$ and polylog terms in the range
of the computed function; for polynomial decay sums which are technically more
challenging because partial solutions do not compose easily, our algorithms
incur additional relative error. Further, we show lower bounds, tight within
polylog factors and tight with respect to the dependence on the probability of
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6132</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6132</id><created>2011-08-31</created><updated>2013-05-11</updated><authors><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Song</keyname><forenames>Qingyang</forenames></author><author><keyname>Wang</keyname><forenames>Xingwei</forenames></author><author><keyname>Jamalipour</keyname><forenames>Abbas</forenames></author></authors><title>Distributed MAC Protocol Supporting Physical-Layer Network Coding</title><categories>cs.NI cs.DC cs.IT math.IT</categories><comments>Final version</comments><journal-ref>IEEE Transactions on Mobile Computing, vol. 12, no. 5, pp.
  1023-1036, May 2013</journal-ref><doi>10.1109/TMC.2012.69</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical-layer network coding (PNC) is a promising approach for wireless
networks. It allows nodes to transmit simultaneously. Due to the difficulties
of scheduling simultaneous transmissions, existing works on PNC are based on
simplified medium access control (MAC) protocols, which are not applicable to
general multi-hop wireless networks, to the best of our knowledge. In this
paper, we propose a distributed MAC protocol that supports PNC in multi-hop
wireless networks. The proposed MAC protocol is based on the carrier sense
multiple access (CSMA) strategy and can be regarded as an extension to the IEEE
802.11 MAC protocol. In the proposed protocol, each node collects information
on the queue status of its neighboring nodes. When a node finds that there is
an opportunity for some of its neighbors to perform PNC, it notifies its
corresponding neighboring nodes and initiates the process of packet exchange
using PNC, with the node itself as a relay. During the packet exchange process,
the relay also works as a coordinator which coordinates the transmission of
source nodes. Meanwhile, the proposed protocol is compatible with conventional
network coding and conventional transmission schemes. Simulation results show
that the proposed protocol is advantageous in various scenarios of wireless
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6146</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6146</id><created>2011-08-31</created><authors><author><keyname>Linkov</keyname><forenames>Alexander M.</forenames><affiliation>Institute for Problems of Mechanical Engineering, Saint Petersburg, Russia, Presently at Rzeszow University of Technology, Poland</affiliation></author></authors><title>Use of a speed equation for numerical simulation of hydraulic fractures</title><categories>physics.flu-dyn cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper treats the propagation of a hydraulically driven crack. We
explicitly write the local speed equation, which facilitates using the theory
of propagating interfaces. It is shown that when neglecting the lag between the
liquid front and the crack tip, the lubrication PDE yields that a solution
satisfies the speed equation identically. This implies that for zero or small
lag, the boundary value problem appears ill-posed when solved numerically. We
suggest e - regularization, which consists in employing the speed equation
together with a prescribed BC on the front to obtain a new BC formulated at a
small distance behind the front rather than on the front itself. It is shown
that - regularization provides accurate and stable results with reasonable time
expense. It is also shown that the speed equation gives a key to proper choice
of unknown functions when solving a hydraulic fracture problem numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6150</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6150</id><created>2011-08-31</created><updated>2012-10-05</updated><authors><author><keyname>Unser</keyname><forenames>Michael</forenames></author><author><keyname>Tafti</keyname><forenames>Pouya D.</forenames></author><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author></authors><title>A unified formulation of Gaussian vs. sparse stochastic processes - Part
  I: Continuous-domain theory</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general distributional framework that results in a unifying
description and characterization of a rich variety of continuous-time
stochastic processes. The cornerstone of our approach is an innovation model
that is driven by some generalized white noise process, which may be Gaussian
or not (e.g., Laplace, impulsive Poisson or alpha stable). This allows for a
conceptual decoupling between the correlation properties of the process, which
are imposed by the whitening operator L, and its sparsity pattern which is
determined by the type of noise excitation. The latter is fully specified by a
Levy measure. We show that the range of admissible innovation behavior varies
between the purely Gaussian and super-sparse extremes. We prove that the
corresponding generalized stochastic processes are well-defined mathematically
provided that the (adjoint) inverse of the whitening operator satisfies some Lp
bound for p&gt;=1. We present a novel operator-based method that yields an
explicit characterization of all Levy-driven processes that are solutions of
constant-coefficient stochastic differential equations. When the underlying
system is stable, we recover the family of stationary CARMA processes,
including the Gaussian ones. The approach remains valid when the system is
unstable and leads to the identification of potentially useful generalizations
of the Levy processes, which are sparse and non-stationary. Finally, we show
how we can apply finite difference operators to obtain a stationary
characterization of these processes that is maximally decoupled and stable,
irrespective of the location of the poles in the complex plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6152</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6152</id><created>2011-08-31</created><updated>2012-10-05</updated><authors><author><keyname>Unser</keyname><forenames>Michael</forenames></author><author><keyname>Tafti</keyname><forenames>Pouya D.</forenames></author><author><keyname>Amini</keyname><forenames>Arash</forenames></author><author><keyname>Kirshner</keyname><forenames>Hagai</forenames></author></authors><title>A unified formulation of Gaussian vs. sparse stochastic processes - Part
  II: Discrete-domain theory</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to the characterization of an extended family of CARMA
(continuous-time autoregressive moving average) processes that are solutions of
stochastic differential equations driven by white Levy innovations. These are
completely specified by: (1) a set of poles and zeros that fixes their
correlation structure, and (2) a canonical infinitely-divisible probability
distribution that controls their degree of sparsity (with the Gaussian model
corresponding to the least sparse scenario). The generalized CARMA processes
are either stationary or non-stationary, depending on the location of the poles
in the complex plane. The most basic non-stationary representatives (with a
single pole at the origin) are the Levy processes, which are the non-Gaussian
counterparts of Brownian motion. We focus on the general analog-to-discrete
conversion problem and introduce a novel spline-based formalism that greatly
simplifies the derivation of the correlation properties and joint probability
distributions of the discrete versions of these processes. We also rely on the
concept of generalized increment process, which suppresses all long range
dependencies, to specify an equivalent discrete-domain innovation model. A
crucial ingredient is the existence of a minimally-supported function
associated with the whitening operator L; this B-spline, which is fundamental
to our formulation, appears in most of our formulas, both at the level of the
correlation and the characteristic function. We make use of these
discrete-domain results to numerically generate illustrative examples of sparse
signals that are consistent with the continuous-domain model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6160</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6160</id><created>2011-08-31</created><authors><author><keyname>Altarelli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Ramezanpour</keyname><forenames>Abolfazl</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Stochastic optimization by message passing</title><categories>cond-mat.stat-mech cs.DC cs.DS</categories><comments>31 pages, 8 figures</comments><journal-ref>J. Stat. Mech. (2011) P11009</journal-ref><doi>10.1088/1742-5468/2011/11/P11009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most optimization problems in applied sciences realistically involve
uncertainty in the parameters defining the cost function, of which only
statistical information is known beforehand. In a recent work we introduced a
message passing algorithm based on the cavity method of statistical physics to
solve the two-stage matching problem with independently distributed stochastic
parameters. In this paper we provide an in-depth explanation of the general
method and caveats, show the details of the derivation and resulting algorithm
for the matching problem and apply it to a stochastic version of the
independent set problem, which is a computationally hard and relevant problem
in communication networks. We compare the results with some greedy algorithms
and briefly discuss the extension to more complicated stochastic multi-stage
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6175</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6175</id><created>2011-08-31</created><authors><author><keyname>Meister</keyname><forenames>Eugen</forenames></author><author><keyname>Stepanenko</keyname><forenames>Sergej</forenames></author><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Adaptive Locomotion of Multibody Snake-like Robot</title><categories>cs.RO cs.SY</categories><comments>Multibody Dynamics 2011, ECCOMAS Thematic Conference, J.C. Samin, P.
  Fisette (eds.) Brussels, Belgium, 4-7 July, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper represents an adaptive rhythmic control for a snake-like robot
with 25 degrees of freedom. The adaptive gait control is implemented in
algorithmic way in simulation and on a real robot. We investigated behavioral
and energetic properties of this control and a dynamics of different body
segments. It turned out that despite using homogeneous generators, physical
constraints have an inhomogeneous impact on neighbor body segments. By
analytical modeling of such dynamics, it may result in heterogeneous coupling
of oscillators for a rhythmic control and impact scalability and
synchronization effects of gait pattern generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6185</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6185</id><created>2011-08-31</created><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Thomsen</keyname><forenames>Casper</forenames></author></authors><title>Weighted Reed-Muller codes revisited</title><categories>cs.IT math.IT</categories><comments>29 pages, 2 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider weighted Reed-Muller codes over point ensemble $S_1
\times...\times S_m$ where $S_i$ needs not be of the same size as $S_j$. For $m
= 2$ we determine optimal weights and analyze in detail what is the impact of
the ratio $|S_1|/|S_2|$ on the minimum distance. In conclusion the weighted
Reed-Muller code construction is much better than its reputation. For a class
of affine variety codes that contains the weighted Reed-Muller codes we then
present two list decoding algorithms. With a small modification one of these
algorithms is able to correct up to 31 errors of the [49, 11, 28] Joyner code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6195</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6195</id><created>2011-08-31</created><authors><author><keyname>Tuli</keyname><forenames>Ruchi</forenames></author><author><keyname>Kumar</keyname><forenames>Parveen</forenames></author></authors><title>Analysis of Recent Checkpointing Techniques for Mobile Computing Systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovery from transient failures is one of the prime issues in the context of
distributed systems. These systems demand to have transparent yet efficient
techniques to achieve the same. Checkpoint is defined as a designated place in
a program where normal processing of a system is interrupted to preserve the
status information. Checkpointing is a process of saving status information.
Mobile computing systems often suffer from high failure rates that are
transient and independent in nature. To add reliability and high availability
to such distributed systems, checkpoint based rollback recovery is one of the
widely used techniques for applications such as scientific computing, database,
telecommunication applications and mission critical applications. This paper
surveys the algorithms which have been reported in the literature for
checkpointing in Mobile Computing Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6197</identifier>
 <datestamp>2012-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6197</id><created>2011-08-31</created><updated>2012-01-14</updated><authors><author><keyname>Rochanakul</keyname><forenames>Penying</forenames></author></authors><title>Two-Level Fingerprinting Codes: Non-Trivial Constructions</title><categories>cs.IT math.IT</categories><msc-class>94A62, 94B60, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the concept of two-level fingerprinting codes, introduced by
Anthapadmanabhan and Barg (2009) in context of traceability (TA) codes, to
other types of fingerprinting codes, namely identifiable parent property (IPP)
codes, secure-frameproof (SFP) codes, and frameproof (FP) codes. We define and
propose the first explicit non-trivial construction for two-level IPP, SFP and
FP codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6198</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6198</id><created>2011-08-31</created><authors><author><keyname>Rao</keyname><forenames>G. Koteswara</forenames></author><author><keyname>Dey</keyname><forenames>Shubhamoy</forenames></author></authors><title>Decision Support for e-Governance: A Text Mining Approach</title><categories>cs.DB cs.IR</categories><comments>19 Pages, 7 Figures</comments><journal-ref>International Journal of Managing Information Technology (IJMIT)
  Vol.3, No.3, 2011, 73-91</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Information and communication technology has the capability to improve the
process by which governments involve citizens in formulating public policy and
public projects. Even though much of government regulations may now be in
digital form (and often available online), due to their complexity and
diversity, identifying the ones relevant to a particular context is a
non-trivial task. Similarly, with the advent of a number of electronic online
forums, social networking sites and blogs, the opportunity of gathering
citizens' petitions and stakeholders' views on government policy and proposals
has increased greatly, but the volume and the complexity of analyzing
unstructured data makes this difficult. On the other hand, text mining has come
a long way from simple keyword search, and matured into a discipline capable of
dealing with much more complex tasks. In this paper we discuss how text-mining
techniques can help in retrieval of information and relationships from textual
data sources, thereby assisting policy makers in discovering associations
between policies and citizens' opinions expressed in electronic public forums
and blogs etc. We also present here, an integrated text mining based
architecture for e-governance decision support along with a discussion on the
Indian scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6208</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6208</id><created>2011-08-31</created><authors><author><keyname>Manthey</keyname><forenames>Norbert</forenames></author></authors><title>Coprocessor - a Standalone SAT Preprocessor</title><categories>cs.AI</categories><comments>system description, short paper, WLP 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work a stand-alone preprocessor for SAT is presented that is able to
perform most of the known preprocessing techniques. Preprocessing a formula in
SAT is important for performance since redundancy can be removed. The
preprocessor is part of the SAT solver riss and is called Coprocessor. Not only
riss, but also MiniSat 2.2 benefit from it, because the SatELite preprocessor
of MiniSat does not implement recent techniques. By using more advanced
techniques, Coprocessor is able to reduce the redundancy in a formula further
and improves the overall solving performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6210</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6210</id><created>2011-08-31</created><updated>2012-01-10</updated><authors><author><keyname>Delestre</keyname><forenames>Olivier</forenames><affiliation>JAD</affiliation></author><author><keyname>Lagr&#xe9;e</keyname><forenames>Pierre-Yves</forenames><affiliation>IJLRA</affiliation></author></authors><title>A well-balanced finite volume scheme for 1D hemodynamic simulations</title><categories>math.NA cs.NA</categories><comments>6 pages. R\'esum\'e en fran\c{c}ais : Nous nous int\'eressons \`a la
  simulation d'\'ecoulements sanguins dans des art\`eres dont les parois sont
  \`a \'elasticit\'e variable. Ceci est mod\'elis\'e \`a l'aide d'un mod\`ele
  unidimensionnel. Nous pr\'esentons un sch\'ema &quot;volume fini \'equilibr\'e&quot;
  bas\'e sur les d\'eveloppements r\'ecents effectu\'es pour la r\'esolution du
  syst\`eme de Saint-Venant. Ainsi, nous obtenons un sch\'ema qui pr\'eserve le
  volume de fluide ainsi que les \'equilibres au repos: Q=0. Le sch\'ema
  introduit est test\'e sur des solutions analytiques</comments><proxy>ccsd</proxy><doi>10.1051/proc/201235018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in simulating blood flow in arteries with variable
elasticity with a one dimensional model. We present a well-balanced finite
volume scheme based on the recent developments in shallow water equations
context. We thus get a mass conservative scheme which also preserves equilibria
of Q=0. This numerical method is tested on analytical tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6211</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6211</id><created>2011-08-31</created><updated>2011-09-01</updated><authors><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Restelli</keyname><forenames>Marcello</forenames></author></authors><title>Transfer from Multiple MDPs</title><categories>cs.AI cs.LG</categories><comments>2011</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer reinforcement learning (RL) methods leverage on the experience
collected on a set of source tasks to speed-up RL algorithms. A simple and
effective approach is to transfer samples from source tasks and include them
into the training set used to solve a given target task. In this paper, we
investigate the theoretical properties of this transfer method and we introduce
novel algorithms adapting the transfer process on the basis of the similarity
between source and target tasks. Finally, we report illustrative experimental
results in a continuous chain problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6214</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6214</id><created>2011-08-31</created><updated>2012-08-01</updated><authors><author><keyname>Hlinka</keyname><forenames>Ondrej</forenames></author><author><keyname>Sluciak</keyname><forenames>Ondrej</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author><author><keyname>Djuric</keyname><forenames>Petar M.</forenames></author><author><keyname>Rupp</keyname><forenames>Markus</forenames></author></authors><title>Likelihood Consensus and Its Application to Distributed Particle
  Filtering</title><categories>stat.AP cs.IT math.IT</categories><doi>10.1109/TSP.2012.2196697</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed state estimation in a wireless sensor network without
a fusion center. Each sensor performs a global estimation task---based on the
past and current measurements of all sensors---using only local processing and
local communications with its neighbors. In this estimation task, the joint
(all-sensors) likelihood function (JLF) plays a central role as it epitomizes
the measurements of all sensors. We propose a distributed method for computing,
at each sensor, an approximation of the JLF by means of consensus algorithms.
This &quot;likelihood consensus&quot; method is applicable if the local likelihood
functions of the various sensors (viewed as conditional probability density
functions of the local measurements) belong to the exponential family of
distributions. We then use the likelihood consensus method to implement a
distributed particle filter and a distributed Gaussian particle filter. Each
sensor runs a local particle filter, or a local Gaussian particle filter, that
computes a global state estimate. The weight update in each local (Gaussian)
particle filter employs the JLF, which is obtained through the likelihood
consensus scheme. For the distributed Gaussian particle filter, the number of
particles can be significantly reduced by means of an additional consensus
scheme. Simulation results are presented to assess the performance of the
proposed distributed particle filters for a multiple target tracking problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6223</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6223</id><created>2011-08-31</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards Configuration of applied Web-based information system</title><categories>cs.SE cs.AI cs.DM cs.NI cs.SY math.OC</categories><comments>13 pages, 9 tables, 17 figures</comments><msc-class>94C30, 90B50, 90C27, 90C29, 90C59, 68M07, 68M07</msc-class><acm-class>D.2.11; H.4; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, combinatorial synthesis of structure for applied Web-based
systems is described. The problem is considered as a combination of selected
design alternatives for system parts/components into a resultant composite
decision (i.e., system configuration design). The solving framework is based on
Hierarchical Morphological Multicriteria Design (HMMD) approach: (i)
multicriteria selection of alternatives for system parts, (ii) composing the
selected alternatives into a resultant combination (while taking into account
ordinal quality of the alternatives above and their compatibility). A
lattice-based discrete space is used to evaluate (to integrate) quality of the
resultant combinations (i.e., composite system decisions or system
configurations). In addition, a simplified solving framework based on
multicriteria multiple choice problem is considered. A multistage design
process to obtain a system trajectory is described as well. The basic applied
example is targeted to an applied Web-based system for a communication service
provider. Two other applications are briefly described (corporate system and
information system for academic application).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6239</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6239</id><created>2011-08-31</created><updated>2011-10-13</updated><authors><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Kayhan</keyname><forenames>Farbod</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Efficient data compression from statistical physics of codes over finite
  fields</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>10 pages, 4 figures</comments><journal-ref>Phys. Rev. E 84, 051111 (2011)</journal-ref><doi>10.1103/PhysRevE.84.051111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss a novel data compression technique for binary
symmetric sources based on the cavity method over a Galois Field of order q
(GF(q)). We present a scheme of low complexity and near optimal empirical
performance. The compression step is based on a reduction of sparse low density
parity check codes over GF(q) and is done through the so called reinforced
belief-propagation equations. These reduced codes appear to have a non-trivial
geometrical modification of the space of codewords which makes such compression
computationally feasible. The computational complexity is O(d.n.q.log(q)) per
iteration, where d is the average degree of the check nodes and n is the number
of bits. For our code ensemble, decompression can be done in a time linear in
the code's length by a simple leaf-removal algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6240</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6240</id><created>2011-08-31</created><updated>2013-10-11</updated><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>Blending margins: The modal logic K has nullary unification type</title><categories>cs.LO math.LO</categories><comments>12 pages, 1 figure</comments><msc-class>03B45 (Primary), 68T15 (Secondary)</msc-class><journal-ref>Journal of Logic and Computation 25 (2015), no. 5, pp. 1231--1240</journal-ref><doi>10.1093/logcom/ext055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate properties of the formula $p \to \Box p$ in the basic modal
logic K. We show that K satisfies an infinitary weaker variant of the rule of
margins $\phi \to \Box\phi / \phi, \neg\phi$, and as a consequence, we obtain
various negative results about admissibility and unification in K. We describe
a complete set of unifiers (i.e., substitutions making the formula provable) of
$p \to \Box p$, and use it to establish that K has the worst possible
unification type: nullary. In well-behaved transitive modal logics,
admissibility and unification can be analyzed in terms of projective formulas,
introduced by Ghilardi; in particular, projective formulas coincide for these
logics with formulas that are admissibly saturated (i.e., derive all their
multiple-conclusion admissible consequences) or exact (i.e., axiomatize a
theory of a substitution). In contrast, we show that in K, the formula $p \to
\Box p$ is admissibly saturated, but neither projective nor exact. All our
results for K also apply to the basic description logic ALC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6254</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6254</id><created>2011-08-31</created><authors><author><keyname>Hassan</keyname><forenames>Md. Mahedi</forenames></author><author><keyname>Hoong</keyname><forenames>Poo Kuan</forenames></author></authors><title>Integrated Solution Scheme for Handover Latency Diminution in Proxy
  Mobile IPv6</title><categories>cs.NI</categories><comments>20 pages, 15 figures, 3 tables; Lee, H., Han, Y. &amp; Min, S. (2010)
  &quot;Network Mobility Support Scheme on PMIPv6 Networks&quot;, International Journal
  of Computer Networks &amp; Communications (IJCNC), Vol.2, No.5</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent trends show that there are swift developments and fast convergence of
wireless and mobile communication networks with internet services to provide
the quality of ubiquitous access to network users. Most of the wireless
networks and mobile cellular networks are moving to be all IP based. These
networks are connected through the private IP core networks using the TCP/IP
protocol or through the Internet. As such, there is room to improve the
mobility support through the Internet and support ubiquitous network access by
providing seamless handover. This is especially true with the invention of
portable mobile and laptop devices that can be connected almost everywhere at
any time. However, the recent explosion on the usage of mobile and laptop
devices has also generated several issues in terms of performance and quality
of service. Nowadays, mobile users demand high quality performance, best
quality of services and seamless connections that support real-time application
such as audio and video streaming. The goal of this paper is to study the
impact and evaluate the mobility management protocols under micro mobility
domain on link layer and network layer handover performance. Therefore, this
paper proposes an integration solution of network-based mobility management
framework, based on Proxy Mobile IPv6, to alleviate handover latency, packet
loss and increase throughput and the performance of video transmission when
mobile host moves to new network during handover on high speed mobility.
Simulations are conducted to analyze the relationship between the network
performances with the moving speed of mobile host over mobility protocols.
Based on simulation results, we presented and analyzed the results of mobility
protocols under intra-domain traffics in micro mobility domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6260</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6260</id><created>2011-08-31</created><updated>2014-01-11</updated><authors><author><keyname>Nair</keyname><forenames>Girish N.</forenames></author></authors><title>Structural Routability of n-Pairs Information Networks</title><categories>cs.IT cs.NI cs.SI math.IT</categories><comments>The final publication is available at link.springer.com
  http://link.springer.com/chapter/10.1007/978-3-319-02150-8_7</comments><msc-class>94</msc-class><journal-ref>Information and Control in Networks (2014) 215-239</journal-ref><doi>10.1007/978-3-319-02150-8_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information does not generally behave like a conservative fluid flow in
communication networks with multiple sources and sinks. However, it is often
conceptually and practically useful to be able to associate separate data
streams with each source-sink pair, with only routing and no coding performed
at the network nodes. This raises the question of whether there is a nontrivial
class of network topologies for which achievability is always equivalent to
routability, for any combination of source signals and positive channel
capacities. This chapter considers possibly cyclic, directed, errorless
networks with n source-sink pairs and mutually independent source signals. The
concept of downward dominance is introduced and it is shown that, if the
network topology is downward dominated, then the achievability of a given
combination of source signals and channel capacities implies the existence of a
feasible multicommodity flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6261</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6261</id><created>2011-08-31</created><updated>2012-02-28</updated><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>The complexity of admissible rules of {\L}ukasiewicz logic</title><categories>cs.LO cs.CC math.LO</categories><comments>14 pages, 2 figures; to appear in Journal of Logic and Computation</comments><msc-class>03B52 (Primary), 68Q17, 08B20 (Secondary)</msc-class><journal-ref>Journal of Logic and Computation 23 (2013), no. 3, pp. 693--705</journal-ref><doi>10.1093/logcom/exs007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational complexity of admissibility of inference
rules in infinite-valued {\L}ukasiewicz propositional logic (\L). It was shown
in [13] that admissibility in {\L} is checkable in PSPACE. We establish that
this result is optimal, i.e., admissible rules of {\L} are PSPACE-complete. In
contrast, derivable rules of {\L} are known to be coNP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6263</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6263</id><created>2011-08-31</created><updated>2012-07-31</updated><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>The ubiquity of conservative translations</title><categories>math.LO cs.LO</categories><comments>15 pages; to appear in Review of Symbolic Logic</comments><msc-class>03B22 (Primary) 03B47, 03F25 (Secondary)</msc-class><journal-ref>Review of Symbolic Logic 5 (2012), no. 4, pp. 666--678</journal-ref><doi>10.1017/S1755020312000226</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the notion of conservative translation between logics introduced by
Feitosa and D'Ottaviano. We show that classical propositional logic (CPC) is
universal in the sense that every finitary consequence relation over a
countable set of formulas can be conservatively translated into CPC. The
translation is computable if the consequence relation is decidable. More
generally, we show that one can take instead of CPC a broad class of logics
(extensions of a certain fragment of full Lambek calculus FL) including most
nonclassical logics studied in the literature, hence in a sense, (almost) any
two reasonable deductive systems can be conservatively translated into each
other. We also provide some counterexamples, in particular the paraconsistent
logic LP is not universal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6274</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6274</id><created>2011-08-31</created><authors><author><keyname>L&#xfc;decke</keyname><forenames>Rainer</forenames></author></authors><title>Every Formula-Based Logic Program Has a Least Infinite-Valued Model</title><categories>cs.LO cs.AI</categories><comments>This paper appears in the Proceedings of the 19th International
  Conference on Applications of Declarative Programming and Knowledge
  Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every definite logic program has as its meaning a least Herbrand model with
respect to the program-independent ordering &quot;set-inclusion&quot;. In the case of
normal logic programs there do not exist least models in general. However,
according to a recent approach by Rondogiannis and Wadge, who consider
infinite-valued models, every normal logic program does have a least model with
respect to a program-independent ordering. We show that this approach can be
extended to formula-based logic programs (i.e., finite sets of rules of the
form A\leftarrowF where A is an atom and F an arbitrary first-order formula).
We construct for a given program P an interpretation M_P and show that it is
the least of all models of P. Keywords: Logic programming, semantics of
programs, negation-as-failure, infinite-valued logics, set theory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6290</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6290</id><created>2011-08-31</created><updated>2011-09-01</updated><authors><author><keyname>Li</keyname><forenames>Chunxi</forenames></author><author><keyname>Chen</keyname><forenames>Changjia</forenames></author><author><keyname>Chiu</keyname><forenames>DahMing</forenames></author></authors><title>Compression and Quantitative Analysis of Buffer Map Message in P2P
  Streaming System</title><categories>cs.MM cs.IT math.IT</categories><comments>13pages,12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BM compression is a straightforward and operable way to reduce buffer message
length as well as to improve system performance. In this paper, we thoroughly
discuss the principles and protocol progress of different compression schemes,
and for the first time present an original compression scheme which can nearly
remove all redundant information from buffer message. Theoretical limit of
compression rates are deduced in the theory of information. Through the
analysis of information content and simulation with our measured BM trace of
UUSee, the validity and superiority of our compression scheme are validated in
term of compression ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6293</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6293</id><created>2011-08-31</created><updated>2011-09-28</updated><authors><author><keyname>Li</keyname><forenames>Chunxi</forenames></author><author><keyname>Chen</keyname><forenames>Changjia</forenames></author><author><keyname>Chiu</keyname><forenames>DahMing</forenames></author></authors><title>Buffer Map Message Compression Based on Relevant Window in P2P Streaming
  Media System</title><categories>cs.MM cs.IT math.IT</categories><comments>12 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular peer to peer streaming media systems such as PPLive and UUSee rely on
periodic buffer-map exchange between peers for proper operation. The buffer-map
exchange contains redundant information which causes non-negligible overhead.
In this paper we present a theoretical framework to study how the overhead can
be lowered. Differentiating from the traditional data compression approach, we
do not treat each buffer-map as an isolated data block, but consider the
correlations between the sequentially exchanged buffer-maps. Under this
framework, two buffer-map compression schemes are proposed and the correctness
of the schemes is proved mathematically. Moreover, we derive the theoretical
limit of compression gain based on probability theory and information theory.
Based on the system parameters of UUSee (a popular P2P streaming platform), our
simulations show that the buffer-map sizes are reduced by 86% and 90% (from 456
bits down to only 66 bits and 46 bits) respectively after applying our schemes.
Furthermore, by combining with the traditional compression methods (on
individual blocks), the sizes are decreased by 91% and 95% (to 42 bits and 24
bits) respectively. Our study provides a guideline for developing practical
compression algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6294</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6294</id><created>2011-08-31</created><authors><author><keyname>Sudha</keyname><forenames>L. R</forenames></author><author><keyname>Bhavani</keyname><forenames>Dr. R</forenames></author></authors><title>Biometric Authorization System using Gait Biometry</title><categories>cs.CV</categories><comments>12 pages, 4 figures, 6 tables; International Journal of Computer
  Science, Engineering and Applications (IJCSEA) Vol.1, No.4, August 2011</comments><doi>10.5121/ijcsea.2011.1401</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Human gait, which is a new biometric aimed to recognize individuals by the
way they walk have come to play an increasingly important role in visual
surveillance applications. In this paper a novel hybrid holistic approach is
proposed to show how behavioural walking characteristics can be used to
recognize unauthorized and suspicious persons when they enter a surveillance
area. Initially background is modelled from the input video captured from
cameras deployed for security and the foreground moving object in the
individual frames are segmented using the background subtraction algorithm.
Then gait representing spatial, temporal and wavelet components are extracted
and fused for training and testing multi class support vector machine models
(SVM). The proposed system is evaluated using side view videos of NLPR
database. The experimental results demonstrate that the proposed system
achieves a pleasing recognition rate and also the results indicate that the
classification ability of SVM with Radial Basis Function (RBF) is better than
with other kernel functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6296</identifier>
 <datestamp>2012-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6296</id><created>2011-08-31</created><updated>2012-01-14</updated><authors><author><keyname>Xu</keyname><forenames>Zenglin</forenames><affiliation>Alan</affiliation></author><author><keyname>Yan</keyname><forenames>Feng</forenames><affiliation>Alan</affiliation></author><author><keyname>Yuan</keyname><affiliation>Alan</affiliation></author><author><keyname>Qi</keyname></author></authors><title>Infinite Tucker Decomposition: Nonparametric Bayesian Models for
  Multiway Data Analysis</title><categories>cs.LG cs.NA</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Tensor decomposition is a powerful computational tool for multiway data
analysis. Many popular tensor decomposition approaches---such as the Tucker
decomposition and CANDECOMP/PARAFAC (CP)---amount to multi-linear
factorization. They are insufficient to model (i) complex interactions between
data entities, (ii) various data types (e.g. missing data and binary data), and
(iii) noisy observations and outliers. To address these issues, we propose
tensor-variate latent nonparametric Bayesian models, coupled with efficient
inference methods, for multiway data analysis. We name these models InfTucker.
Using these InfTucker, we conduct Tucker decomposition in an infinite feature
space. Unlike classical tensor decomposition models, our new approaches handle
both continuous and binary data in a probabilistic framework. Unlike previous
Bayesian models on matrices and tensors, our models are based on latent
Gaussian or $t$ processes with nonlinear covariance functions. To efficiently
learn the InfTucker from data, we develop a variational inference technique on
tensors. Compared with classical implementation, the new technique reduces both
time and space complexities by several orders of magnitude. Our experimental
results on chemometrics and social network datasets demonstrate that our new
models achieved significantly higher prediction accuracy than the most
state-of-art tensor decomposition
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6302</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6302</id><created>2011-08-31</created><authors><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author></authors><title>Dynamic MDS Matrices for Substantial Cryptographic Strength</title><categories>cs.CR</categories><comments>Short paper at WISA'10, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ciphers get their strength from the mathematical functions of confusion and
diffusion, also known as substitution and permutation. These were the basics of
classical cryptography and they are still the basic part of modern ciphers. In
block ciphers diffusion is achieved by the use of Maximum Distance Separable
(MDS) matrices. In this paper we present some methods for constructing dynamic
(and random) MDS matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6304</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6304</id><created>2011-08-31</created><authors><author><keyname>Marinho</keyname><forenames>Eraldo Pereira</forenames></author><author><keyname>Andreazza</keyname><forenames>Carmen Maria</forenames></author></authors><title>Anisotropic k-Nearest Neighbor Search Using Covariance Quadtree</title><categories>cs.CV cs.CG cs.DS</categories><comments>Work presented at the Minisymposia of Computational Geometry in the
  joint events IX Argentinian Congress on Computational Mechanics, XXXI
  Iberian-Latin-American Congress on Computational Methods in Engineering, II
  South American Congress on Computational Mechanics, held in Buenos Aires in
  15-18 November 2010; Mec\'anica Computacional (Computational Mechanics) Vol.
  XXIX, 2010, ISSN 1666-6070</comments><acm-class>I.4.5; I.4.6; I.4.7; H.3.3</acm-class><journal-ref>2010, Mec\'anica Computacional, Volume XXIX. Number 60.
  Computational Geometry (A), pp 6045-6064</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a variant of the hyper-quadtree that divides a multidimensional
space according to the hyperplanes associated to the principal components of
the data in each hyperquadrant. Each of the $2^\lambda$ hyper-quadrants is a
data partition in a $\lambda$-dimension subspace, whose intrinsic
dimensionality $\lambda\leq d$ is reduced from the root dimensionality $d$ by
the principal components analysis, which discards the irrelevant eigenvalues of
the local covariance matrix. In the present method a component is irrelevant if
its length is smaller than, or comparable to, the local inter-data spacing.
Thus, the covariance hyper-quadtree is fully adaptive to the local
dimensionality. The proposed data-structure is used to compute the anisotropic
K nearest neighbors (kNN), supported by the Mahalanobis metric. As an
application, we used the present k nearest neighbors method to perform density
estimation over a noisy data distribution. Such estimation method can be
further incorporated to the smoothed particle hydrodynamics, allowing computer
simulations of anisotropic fluid flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6312</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6312</id><created>2011-08-31</created><updated>2013-02-04</updated><authors><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Nazer</keyname><forenames>Bobak</forenames></author><author><keyname>Whiting</keyname><forenames>Phil</forenames></author></authors><title>Computation Alignment: Capacity Approximation without Noise Accumulation</title><categories>cs.IT math.IT</categories><comments>36 pages, to appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 59, pp. 3811 - 3832,
  June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider several source nodes communicating across a wireless network to a
destination node with the help of several layers of relay nodes. Recent work by
Avestimehr et al. has approximated the capacity of this network up to an
additive gap. The communication scheme achieving this capacity approximation is
based on compress-and-forward, resulting in noise accumulation as the messages
traverse the network. As a consequence, the approximation gap increases
linearly with the network depth.
  This paper develops a computation alignment strategy that can approach the
capacity of a class of layered, time-varying wireless relay networks up to an
approximation gap that is independent of the network depth. This strategy is
based on the compute-and-forward framework, which enables relays to decode
deterministic functions of the transmitted messages. Alone, compute-and-forward
is insufficient to approach the capacity as it incurs a penalty for
approximating the wireless channel with complex-valued coefficients by a
channel with integer coefficients. Here, this penalty is circumvented by
carefully matching channel realizations across time slots to create
integer-valued effective channels that are well-suited to compute-and-forward.
Unlike prior constant gap results, the approximation gap obtained in this paper
also depends closely on the fading statistics, which are assumed to be i.i.d.
Rayleigh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6313</identifier>
 <datestamp>2011-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6313</id><created>2011-08-31</created><authors><author><keyname>Damgaard</keyname><forenames>Ivan</forenames></author><author><keyname>Funder</keyname><forenames>Jakob</forenames></author><author><keyname>Nielsen</keyname><forenames>Jesper Buus</forenames></author><author><keyname>Salvail</keyname><forenames>Louis</forenames></author></authors><title>Superposition Attacks on Cryptographic Protocols</title><categories>quant-ph cs.CR</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attacks on classical cryptographic protocols are usually modeled by allowing
an adversary to ask queries from an oracle. Security is then defined by
requiring that as long as the queries satisfy some constraint, there is some
problem the adversary cannot solve, such as compute a certain piece of
information. In this paper, we introduce a fundamentally new model of quantum
attacks on classical cryptographic protocols, where the adversary is allowed to
ask several classical queries in quantum superposition. This is a strictly
stronger attack than the standard one, and we consider the security of several
primitives in this model. We show that a secret-sharing scheme that is secure
with threshold $t$ in the standard model is secure against superposition
attacks if and only if the threshold is lowered to $t/2$. We use this result to
give zero-knowledge proofs for all of NP in the common reference string model.
While our protocol is classical, it is sound against a cheating unbounded
quantum prover and computational zero-knowledge even if the verifier is allowed
a superposition attack. Finally, we consider multiparty computation and show
that for the most general type of attack, simulation based security is not
possible. However, putting a natural constraint on the adversary, we show a
non-trivial example of a protocol that can indeed be simulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1108.6328</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1108.6328</id><created>2011-08-31</created><updated>2012-04-20</updated><authors><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author><author><keyname>Freytag</keyname><forenames>Johann-Christoph</forenames></author></authors><title>Foundations of Traversal Based Query Execution over Linked Data
  (Extended Version)</title><categories>cs.DB</categories><comments>v5: aligned with the final version published in HT'2012; v4: new
  Sec.3.2, added the concept of 'LD machine decidability', added the concept of
  'seed identifiers', 20 pages; v3: new Sec.6.5, rewritten Abstract and intro,
  proofs in single column format, 19 pages; v2: completed missing proof, 16
  pages; v1: 15 pages</comments><acm-class>H.3.3; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query execution over the Web of Linked Data has attracted much attention
recently. A particularly interesting approach is link traversal based query
execution which proposes to integrate the traversal of data links into the
construction of query results. Hence -in contrast to traditional query
execution paradigms- this approach does not assume a fixed set of relevant data
sources beforehand; instead, it discovers data on the fly and, thus, enables
applications to tap the full potential of the Web.
  While several authors study possibilities to implement the idea of link
traversal based query execution and to optimize query execution in this
context, no work exists that discusses the theoretical foundations of the
approach in general. Our paper fills this gap.
  We introduce a well-defined semantics for queries that may be executed using
the link traversal based approach. Based on this semantics we formally analyze
properties of such queries. In particular, we study the computability of
queries as well as the implications of querying a potentially infinite Web of
Linked Data. Our results show that query computation in general is not
guaranteed to terminate and that for any given query it is undecidable whether
the execution terminates. Furthermore, we define an abstract execution model
that captures the integration of link traversal into the query execution
process. Based on this model we prove the soundness and completeness of link
traversal based query execution and analyze an existing implementation
approach..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0001</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0001</id><created>2011-08-31</created><updated>2013-02-18</updated><authors><author><keyname>Endo</keyname><forenames>Toshiki</forenames></author><author><keyname>Suzuki</keyname><forenames>Yuki</forenames></author></authors><title>Vertex unfoldings of tight polyhedra</title><categories>math.CO cs.CG</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unfolding of a polyhedron along its edges is called a vertex unfolding if
adjacent faces are allowed to be connected at not only an edge but also a
vertex. Demaine et al showed that every triangulated polyhedron has a vertex
unfolding. We extend this result to a tight polyhedron, where a polyhedron is
tight if all non-triangular faces are mutually non-adjacent parallelograms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0003</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0003</id><created>2011-08-31</created><updated>2011-09-02</updated><authors><author><keyname>Riebe</keyname><forenames>Kristin</forenames></author><author><keyname>Partl</keyname><forenames>Adrian M.</forenames></author><author><keyname>Enke</keyname><forenames>Harry</forenames></author><author><keyname>Forero-Romero</keyname><forenames>Jaime</forenames></author><author><keyname>Gottloeber</keyname><forenames>Stefan</forenames></author><author><keyname>Klypin</keyname><forenames>Anatoly</forenames></author><author><keyname>Lemson</keyname><forenames>Gerard</forenames></author><author><keyname>Prada</keyname><forenames>Francisco</forenames></author><author><keyname>Primack</keyname><forenames>Joel R.</forenames></author><author><keyname>Steinmetz</keyname><forenames>Matthias</forenames></author><author><keyname>Turchaninov</keyname><forenames>Victor</forenames></author></authors><title>The MultiDark Database: Release of the Bolshoi and MultiDark
  Cosmological Simulations</title><categories>astro-ph.CO astro-ph.IM cs.DB</categories><comments>28 pages, 9 figures, submitted to New Astronomy</comments><doi>10.1002/asna.201211900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the online MultiDark Database -- a Virtual Observatory-oriented,
relational database for hosting various cosmological simulations. The data is
accessible via an SQL (Structured Query Language) query interface, which also
allows users to directly pose scientific questions, as shown in a number of
examples in this paper. Further examples for the usage of the database are
given in its extensive online documentation (www.multidark.org). The database
is based on the same technology as the Millennium Database, a fact that will
greatly facilitate the usage of both suites of cosmological simulations. The
first release of the MultiDark Database hosts two 8.6 billion particle
cosmological N-body simulations: the Bolshoi (250/h Mpc simulation box, 1/h kpc
resolution) and MultiDark Run1 simulation (MDR1, or BigBolshoi, 1000/h Mpc
simulation box, 7/h kpc resolution). The extraction methods for halos/subhalos
from the raw simulation data, and how this data is structured in the database
are explained in this paper. With the first data release, users get full access
to halo/subhalo catalogs, various profiles of the halos at redshifts z=0-15,
and raw dark matter data for one time-step of the Bolshoi and four time-steps
of the MultiDark simulation. Later releases will also include galaxy mock
catalogs and additional merging trees for both simulations as well as new large
volume simulations with high resolution. This project is further proof of the
viability to store and present complex data using relational database
technology. We encourage other simulators to publish their results in a similar
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0032</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0032</id><created>2011-08-31</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>Semantic Integration in the IFF</title><categories>cs.DL cs.LO math.CT</categories><comments>Presented at the Semantic Integration Workshop of the 2nd
  International Semantic Web Conference (ISWC2003), Sanibel Island, Florida,
  October 20, 2003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE P1600.1 Standard Upper Ontology (SUO) project aims to specify an
upper ontology that will provide a structure and a set of general concepts upon
which domain ontologies could be constructed. The Information Flow Framework
(IFF), which is being developed under the auspices of the SUO Working Group,
represents the structural aspect of the SUO. The IFF is based on category
theory. Semantic integration of object-level ontologies in the IFF is
represented with its fusion construction. The IFF maintains ontologies using
powerful composition primitives, which includes the fusion construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0035</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0035</id><created>2011-08-31</created><authors><author><keyname>Savaidis</keyname><forenames>Stylianos P.</forenames></author><author><keyname>Miridakis</keyname><forenames>Nikolaos I.</forenames></author></authors><title>Statistical Model of Downlink Power Consumption in Cellular CDMA
  Networks</title><categories>cs.SY</categories><comments>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol. 3,
  No. 4, August 2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Present work proposes a theoretical statistical model of the downlink power
consumption in cellular CDMA networks. The proposed model employs a simple but
popular propagation model, which breaks down path losses into a distance
dependent and a log-normal shadowing loss term. Based on the aforementioned
path loss formalism, closed-form expressions for the first and the second
moment of power consumption are obtained taking into account conditions placed
by cell selection and handoff algorithms. Numerical results for various radio
propagation environments and cell selection as well as handoff schemes are
provided and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0038</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0038</id><created>2011-08-31</created><authors><author><keyname>Asosheh</keyname><forenames>Abbas</forenames></author><author><keyname>Karimi</keyname><forenames>Nafiseh</forenames></author><author><keyname>Khodkari</keyname><forenames>Hourieh</forenames></author></authors><title>Provide a Model for Handover Technology in Wireless Networks</title><categories>cs.NI</categories><comments>15 pages, 7 figures, 9 tables, journal published IJWMN August 2011;
  a8rcc.org; AIRCC August 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Handovers for the MIPv6 (FMIPv6) has been proposed to reduce the
Handover latency, in the IETF. It could not find the acceptable reduction, so
led to more efforts to improve it and however the creation of multiple Handover
methods in the literature. A stable connection is very important in mobile
services so the mobility of device would not cause any interruption in network
services and thus mobility management plays a very important role. Mobile IPv6
has become a general solution for supporting mobility between different
networks on the internet which a flawless connection needs to be managed
properly. In order to select the appropriate method, in this paper, all the
proposed methods have been classified according to the identified performance
metrics. Call blocking probability, Handover blocking probability, Probability
of an unnecessary handover, Duration of interruption and delay, as the most
important Handover algorithm performance metrics are introduced. The AHP method
will be deployed to weight the metrics in a sample topology according to the
selected sound application. Then the TOPSIS method will be employed to find the
appropriate Handover algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0059</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0059</id><created>2011-08-31</created><authors><author><keyname>Gandica</keyname><forenames>Y&#xe9;rali</forenames></author><author><keyname>Charmell</keyname><forenames>A.</forenames></author><author><keyname>Villegas-Febres</keyname><forenames>J.</forenames></author><author><keyname>Bonalde</keyname><forenames>I.</forenames></author></authors><title>Cluster size entropy in the Axelrod model of social influence:
  small-world networks and mass media</title><categories>physics.soc-ph cs.SI</categories><comments>21 pages, 7 figures</comments><doi>10.1103/PhysRevE.84.046109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Axelrod's cultural adaptation model using the concept of cluster
size entropy, $S_{c}$ that gives information on the variability of the cultural
cluster size present in the system. Using networks of different topologies,
from regular to random, we find that the critical point of the well-known
nonequilibrium monocultural-multicultural (order-disorder) transition of the
Axelrod model is unambiguously given by the maximum of the $S_{c}(q)$
distributions. The width of the cluster entropy distributions can be used to
qualitatively determine whether the transition is first- or second-order. By
scaling the cluster entropy distributions we were able to obtain a relationship
between the critical cultural trait $q_c$ and the number $F$ of cultural
features in regular networks. We also analyze the effect of the mass media
(external field) on social systems within the Axelrod model in a square
network. We find a new partially ordered phase whose largest cultural cluster
is not aligned with the external field, in contrast with a recent suggestion
that this type of phase cannot be formed in regular networks. We draw a new
$q-B$ phase diagram for the Axelrod model in regular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0069</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0069</id><created>2011-08-31</created><updated>2014-04-20</updated><authors><author><keyname>Lahiri</keyname><forenames>Shibamouli</forenames></author><author><keyname>Lu</keyname><forenames>Xiaofei</forenames></author></authors><title>Inter-rater Agreement on Sentence Formality</title><categories>cs.CL</categories><comments>5 pages, 1 figure, 1 table</comments><acm-class>H.3.1; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formality is one of the most important dimensions of writing style variation.
In this study we conducted an inter-rater reliability experiment for assessing
sentence formality on a five-point Likert scale, and obtained good agreement
results as well as different rating distributions for different sentence
categories. We also performed a difficulty analysis to identify the bottlenecks
of our rating procedure. Our main objective is to design an automatic scoring
mechanism for sentence-level formality, and this study is important for that
purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0077</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0077</id><created>2011-08-31</created><authors><author><keyname>Mostafa</keyname><forenames>Sheikh Shanawaz</forenames></author><author><keyname>Hossian</keyname><forenames>Md. Mahbub</forenames></author><author><keyname>Reza</keyname><forenames>Khondker Jahid</forenames></author><author><keyname>Rashid</keyname><forenames>Gazi Maniur</forenames></author></authors><title>A Radio Based Intelligent Railway Grade Crossing System to Avoid
  Collision</title><categories>cs.SY</categories><comments>5 pages,4 figures, International Journal of Computer Science Issues,
  Vol. 7, Issue 6, November 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Railway grade crossing is become the major headache for the transportation
system. This paper describes an intelligent railway crossing control system for
multiple tracks that features a controller which receives messages from
incoming and outgoing trains by sensors. These messages contain detail
information including the direction and identity of a train. Depending on those
messages the controller device decides whenever the railroad crossing gate will
close or open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0085</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0085</id><created>2011-09-01</created><authors><author><keyname>Jassadapakorn</keyname><forenames>Chaiwat</forenames></author><author><keyname>Chongstitvatana</keyname><forenames>Prabhas</forenames></author></authors><title>Self-Adaptation Mechanism to Control the Diversity of the Population in
  Genetic Algorithm</title><categories>cs.NE</categories><comments>17 pages, 12 figures</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT), Vol 3, No 4, August 2011</journal-ref><doi>10.5121/ijcsit.2011.3409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the problems in applying Genetic Algorithm is that there is some
situation where the evolutionary process converges too fast to a solution which
causes it to be trapped in local optima. To overcome this problem, a proper
diversity in the candidate solutions must be determined. Most existing
diversity-maintenance mechanisms require a problem specific knowledge to setup
parameters properly. This work proposes a method to control diversity of the
population without explicit parameter setting. A self-adaptation mechanism is
proposed based on the competition of preference characteristic in mating. It
can adapt the population toward proper diversity for the problems. The
experiments are carried out to measure the effectiveness of the proposed method
based on nine well-known test problems. The performance of the adaptive method
is comparable to traditional Genetic Algorithm with the best parameter setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0086</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0086</id><created>2011-09-01</created><updated>2012-03-28</updated><authors><author><keyname>Zeng</keyname><forenames>Qiang</forenames></author><author><keyname>Zhuge</keyname><forenames>Hai</forenames></author></authors><title>Comments on &quot;Stack-based Algorithms for Pattern Matching on DAGs&quot;</title><categories>cs.DB</categories><comments>This paper has been withdrawn by the author, since this is an
  obsolete version. Please refer to the version published in PVLDB Volume 5
  Issue 7</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper &quot;Stack-based Algorithms for Pattern Matching on DAGs&quot; generalizes
the classical holistic twig join algorithms and proposes PathStackD, TwigStackD
and DagStackD to respectively evaluate path, twig and DAG pattern queries on
directed acyclic graphs. In this paper, we investigate the major results of
that paper, pointing out several discrepancies and proposing solutions to
resolving them. We show that the original algorithms do not find particular
types of query solutions that are common in practice. We also analyze the
effect of an underlying assumption on the correctness of the algorithms and
discuss the pre-filtering process that the original work proposes to prune
redundant nodes. Our experimental study on both real and synthetic data
substantiates our conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0090</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0090</id><created>2011-09-01</created><authors><author><keyname>Pal</keyname><forenames>Arup Kumar</forenames></author><author><keyname>Sar</keyname><forenames>Anup</forenames></author></authors><title>An Efficient Codebook Initialization Approach for LBG Algorithm</title><categories>cs.CV</categories><doi>10.5121/ijcsea.2011.1407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In VQ based image compression technique has three major steps namely (i)
Codebook Design, (ii) VQ Encoding Process and (iii) VQ Decoding Process. The
performance of VQ based image compression technique depends upon the
constructed codebook. A widely used technique for VQ codebook design is the
Linde-Buzo-Gray (LBG) algorithm. However the performance of the standard LBG
algorithm is highly dependent on the choice of the initial codebook. In this
paper, we have proposed a simple and very effective approach for codebook
initialization for LBG algorithm. The simulation results show that the proposed
scheme is computationally efficient and gives expected performance as compared
to the standard LBG algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0093</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0093</id><created>2011-09-01</created><updated>2012-12-10</updated><authors><author><keyname>Roux</keyname><forenames>Nicolas Le</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>Local Component Analysis</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel density estimation, a.k.a. Parzen windows, is a popular density
estimation method, which can be used for outlier detection or clustering. With
multivariate data, its performance is heavily reliant on the metric used within
the kernel. Most earlier work has focused on learning only the bandwidth of the
kernel (i.e., a scalar multiplicative factor). In this paper, we propose to
learn a full Euclidean metric through an expectation-minimization (EM)
procedure, which can be seen as an unsupervised counterpart to neighbourhood
component analysis (NCA). In order to avoid overfitting with a fully
nonparametric density estimator in high dimensions, we also consider a
semi-parametric Gaussian-Parzen density model, where some of the variables are
modelled through a jointly Gaussian density, while others are modelled through
Parzen windows. For these two models, EM leads to simple closed-form updates
based on matrix inversions and eigenvalue decompositions. We show empirically
that our method leads to density estimators with higher test-likelihoods than
natural competing methods, and that the metrics may be used within most
unsupervised learning techniques that rely on such metrics, such as spectral
clustering or manifold learning methods. Finally, we present a stochastic
approximation scheme which allows for the use of this method in a large-scale
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0094</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0094</id><created>2011-09-01</created><authors><author><keyname>Afify</keyname><forenames>Heba</forenames></author><author><keyname>Islam</keyname><forenames>Muhammad</forenames></author><author><keyname>Wahed</keyname><forenames>Manal Abdel</forenames></author></authors><title>DNA Lossless Differential Compression Algorithm based on Similarity of
  Genomic Sequence Database</title><categories>cs.DS cs.CE cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern biological science produces vast amounts of genomic sequence data.
This is fuelling the need for efficient algorithms for sequence compression and
analysis. Data compression and the associated techniques coming from
information theory are often perceived as being of interest for data
communication and storage. In recent years, a substantial effort has been made
for the application of textual data compression techniques to various
computational biology tasks, ranging from storage and indexing of large
datasets to comparison of genomic databases. This paper presents a differential
compression algorithm that is based on production of difference sequences
according to op-code table in order to optimize the compression of homologous
sequences in dataset. Therefore, the stored data are composed of reference
sequence, the set of differences, and differences locations, instead of storing
each sequence individually. This algorithm does not require a priori knowledge
about the statistics of the sequence set. The algorithm was applied to three
different datasets of genomic sequences, it achieved up to 195-fold compression
rate corresponding to 99.4% space saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0097</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0097</id><created>2011-09-01</created><authors><author><keyname>Gong</keyname><forenames>Xun</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author><author><keyname>Schear</keyname><forenames>Nab&#xed;l</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Website Detection Using Remote Traffic Analysis</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent work in traffic analysis has shown that traffic patterns leaked
through side channels can be used to recover important semantic information.
For instance, attackers can find out which website, or which page on a website,
a user is accessing simply by monitoring the packet size distribution. We show
that traffic analysis is even a greater threat to privacy than previously
thought by introducing a new attack that can be carried out remotely. In
particular, we show that, to perform traffic analysis, adversaries do not need
to directly observe the traffic patterns. Instead, they can gain sufficient
information by sending probes from a far-off vantage point that exploits a
queuing side channel in routers. To demonstrate the threat of such remote
traffic analysis, we study a remote website detection attack that works against
home broadband users. Because the remotely observed traffic patterns are more
noisy than those obtained using previous schemes based on direct local traffic
monitoring, we take a dynamic time warping (DTW) based approach to detecting
fingerprints from the same website. As a new twist on website fingerprinting,
we consider a website detection attack, where the attacker aims to find out
whether a user browses a particular web site, and its privacy implications. We
show experimentally that, although the success of the attack is highly
variable, depending on the target site, for some sites very low error rates. We
also show how such website detection can be used to deanonymize message board
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0098</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0098</id><created>2011-09-01</created><authors><author><keyname>Gall</keyname><forenames>Michael</forenames></author><author><keyname>Grechenig</keyname><forenames>Thomas</forenames></author><author><keyname>Bjerre</keyname><forenames>Mogens</forenames></author></authors><title>Assessing the Feasibility of Developing a Federated ERP System</title><categories>cs.SE</categories><journal-ref>International Journal of Managing Information Technology (IJMIT)
  Vol.3, No.3, August 2011, 16-26</journal-ref><doi>10.5121/ijmit.2011.3302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In past years ERP Systems have become one of the main components within the
corporate IT structure. Several problems exist around implementing and
operating these systems within companies. In the literature one can find
several studies about the problems arising during the implementation of an ERP
system. The main problem areas are around the complexity of ERP systems. One
vision to overcome some of these problems is federated ERP. Federated ERP
systems are built of components from different vendors, which are distributed
within a network. All components act as one single ERP system from the user
perspective. The decreased complexity of such a system would require lower
installation and maintenance cost. Additional, only the components which are
needed to cover the company's business processes would be used. Several
theories around this concept exist, but a feasibility assessment of developing
a federated ERP system has not been done yet. Based on a literary analysis of
existing methods for feasibility studies, this paper is applying strategic
planning concepts and referential data from the traditional ERP development to
provide a first assessment of the overall feasibility of developing a platform
for federated ERP systems. An analytical hierarchical approach is used to
define effort and effect related criteria and their domain values. The
assessment as the criteria is done in comparison to the development of a
classical ERP system. Using the developed criteria, a net present value
calculation is done. The calculation of the net present value is done on an
overall, not company specific level. In order to estimate the weighted average
cost of capital, the values from successful software companies are used as a
baseline. Additional potential risks and obstacles are identified for further
clarification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0105</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0105</id><created>2011-09-01</created><updated>2011-09-16</updated><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author><author><keyname>Thakurta</keyname><forenames>Abhradeep</forenames></author></authors><title>Differentially Private Online Learning</title><categories>cs.LG cs.CR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of preserving privacy in the online
learning setting. We study the problem in the online convex programming (OCP)
framework---a popular online learning setting with several interesting
theoretical and practical implications---while using differential privacy as
the formal privacy measure. For this problem, we distill two critical
attributes that a private OCP algorithm should have in order to provide
reasonable privacy as well as utility guarantees: 1) linearly decreasing
sensitivity, i.e., as new data points arrive their effect on the learning model
decreases, 2) sub-linear regret bound---regret bound is a popular
goodness/utility measure of an online learning algorithm.
  Given an OCP algorithm that satisfies these two conditions, we provide a
general framework to convert the given algorithm into a privacy preserving OCP
algorithm with good (sub-linear) regret. We then illustrate our approach by
converting two popular online learning algorithms into their differentially
private variants while guaranteeing sub-linear regret ($O(\sqrt{T})$). Next, we
consider the special case of online linear regression problems, a practically
important class of online learning problems, for which we generalize an
approach by Dwork et al. to provide a differentially private algorithm with
just $O(\log^{1.5} T)$ regret. Finally, we show that our online learning
framework can be used to provide differentially private algorithms for offline
learning as well. For the offline learning problem, our approach obtains better
error bounds as well as can handle larger class of problems than the existing
state-of-the-art methods Chaudhuri et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0113</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0113</id><created>2011-09-01</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames><affiliation>University of Potsdam</affiliation></author><author><keyname>Kaminski</keyname><forenames>Roland</forenames><affiliation>University of Potsdam</affiliation></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames><affiliation>University of Potsdam</affiliation></author></authors><title>aspcud: A Linux Package Configuration Tool Based on Answer Set
  Programming</title><categories>cs.AI cs.LO</categories><comments>In Proceedings LoCoCo 2011, arXiv:1108.6097</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 65, 2011, pp. 12-25</journal-ref><doi>10.4204/EPTCS.65.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Linux package configuration tool aspcud based on Answer Set
Programming. In particular, we detail aspcud's preprocessor turning a CUDF
specification into a set of logical facts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0114</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0114</id><created>2011-09-01</created><authors><author><keyname>Friedrich</keyname><forenames>Gerhard</forenames><affiliation>Alpen-Adria Universit&#xe4;t</affiliation></author><author><keyname>Ryabokon</keyname><forenames>Anna</forenames><affiliation>Alpen-Adria Universit&#xe4;t</affiliation></author><author><keyname>Falkner</keyname><forenames>Andreas A.</forenames><affiliation>Siemens AG &#xd6;sterreich</affiliation></author><author><keyname>Haselb&#xf6;ck</keyname><forenames>Alois</forenames><affiliation>Siemens AG &#xd6;sterreich</affiliation></author><author><keyname>Schenner</keyname><forenames>Gottfried</forenames><affiliation>Siemens AG &#xd6;sterreich</affiliation></author><author><keyname>Schreiner</keyname><forenames>Herwig</forenames><affiliation>Siemens AG &#xd6;sterreich</affiliation></author></authors><title>(Re)configuration based on model generation</title><categories>cs.AI cs.LO</categories><comments>In Proceedings LoCoCo 2011, arXiv:1108.6097</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 65, 2011, pp. 26-35</journal-ref><doi>10.4204/EPTCS.65.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconfiguration is an important activity for companies selling configurable
products or services which have a long life time. However, identification of a
set of required changes in a legacy configuration is a hard problem, since even
small changes in the requirements might imply significant modifications. In
this paper we show a solution based on answer set programming, which is a
logic-based knowledge representation formalism well suited for a compact
description of (re)configuration problems. Its applicability is demonstrated on
simple abstractions of several real-world scenarios. The evaluation of our
solution on a set of benchmark instances derived from commercial
(re)configuration problems shows its practical applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0115</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0115</id><created>2011-09-01</created><authors><author><keyname>Aschinger</keyname><forenames>Markus</forenames></author><author><keyname>Drescher</keyname><forenames>Conrad</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author></authors><title>Introducing LoCo, a Logic for Configuration Problems</title><categories>cs.LO</categories><comments>In Proceedings LoCoCo 2011, arXiv:1108.6097</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 65, 2011, pp. 36-45</journal-ref><doi>10.4204/EPTCS.65.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the core of LoCo, a logic-based high-level
representation language for expressing configuration problems. LoCo shall allow
to model these problems in an intuitive and declarative way, the dynamic
aspects of configuration notwithstanding. Our logic enforces that
configurations contain only finitely many components and reasoning can be
reduced to the task of model construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0129</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0129</id><created>2011-09-01</created><authors><author><keyname>Chen</keyname><forenames>Sheng-Gwo</forenames></author><author><keyname>Wu</keyname><forenames>Jyh-Yang</forenames></author></authors><title>Discrete Conservation Law on Curved Surfaces</title><categories>cs.CG math.NA physics.comp-ph</categories><comments>18 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we shall introduce a simple, effective numerical method for
finding differential operators for scalar and vector-valued functions on
surfaces. The key idea of our algorithm is to develop an intrinsic and unified
way to compute directly the partial derivatives of functions defined on
triangular meshes which are the discretization of regular surfaces under
consideration. Most importantly, the divergence theorem and conservation laws
on triangular meshes are fulfilled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0132</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0132</id><created>2011-09-01</created><authors><author><keyname>Yalamanchili</keyname><forenames>Sushma</forenames></author><author><keyname>Rao</keyname><forenames>M. Kameswara</forenames></author></authors><title>A Framework for Devanagari Script-based Captcha</title><categories>cs.CR cs.HC</categories><comments>10 pages, 8 Figures, CCSEA 2011 - First International Conference,
  Chennai, July 15-17, 2011</comments><journal-ref>International Journal of Advanced Information Technology, Vol. 1,
  No. 4, August, pp. 47-57, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human Interactive Proofs (HIPs) are automatic reverse Turing tests designed
to distinguish between various groups of users. Completely Automatic Public
Turing test to tell Computers and Humans Apart (CAPTCHA) is a HIP system that
distinguish between humans and malicious computer programs. Many CAPTCHAs have
been proposed in the literature that text-graphical based, audio-based,
puzzle-based and mathematical questions-based. The design and implementation of
CAPTCHAs fall in the realm of Artificial Intelligence. We aim to utilize
CAPTCHAs as a tool to improve the security of Internet based applications. In
this paper we present a framework for a text-based CAPTCHA based on Devanagari
script which can exploit the difference in the reading proficiency between
humans and computer programs. Our selection of Devanagari script-based CAPTCHA
is based on the fact that it is used by a large number of Indian languages
including Hindi which is the third most spoken language. There is potential for
an exponential rise in the applications that are likely to be developed in that
script thereby making it easy to secure Indian language based applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0137</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0137</id><created>2011-09-01</created><authors><author><keyname>Makarenko</keyname><forenames>A. V.</forenames></author><author><keyname>Pravdivtsev</keyname><forenames>A. V.</forenames></author></authors><title>Architectural solutions of conformal network-centric staring-sensor
  systems with spherical field of view</title><categories>cs.SY math.PR physics.optics</categories><comments>16 pages, 2 figures, 2 tables, Report accepted for conference: SPIE
  Security+Defence 2011, Conferences &quot;Electro-Optical and Infrared Systems:
  Technology and Applications&quot;, 19-22 September 2011, Prague, Czech Republic,
  Paper 8185-18</comments><msc-class>93A14, 93A15, 68T05, 93E10</msc-class><acm-class>I.2.8; I.2.9; I.2.10; I.2.11; J.7</acm-class><journal-ref>Proc. SPIE 8185, 81850I (2011)</journal-ref><doi>10.1117/12.897804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents the concept of network-centric conformal electro-optical
systems construction with spherical field of view. It discusses abstract
passive distributed electro-optical systems with focal array detectors based on
a group of moving objects distributed in space. The system performs conformal
processing of information from sensor matrix in a single event coordinate-time
field. Unequivocally the construction of the systems which satisfy the
different criteria of optimality is very complicated and requires special
approaches to their development and design. The paper briefly touches upon key
questions (in the authors' opinion) in the synthesis of such systems that meet
different criteria of optimality. The synthesis of such systems is discussed by
authors with the systematic and synergy approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0138</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0138</id><created>2011-09-01</created><authors><author><keyname>Boujelben</keyname><forenames>Atef</forenames></author><author><keyname>Tmar</keyname><forenames>Hedi</forenames></author><author><keyname>Mnif</keyname><forenames>Jameleddine</forenames></author><author><keyname>Abid</keyname><forenames>Mohamed</forenames></author></authors><title>Automatic Application Level Set Approach in Detection Calcifications in
  Mammographic Image</title><categories>cs.CV</categories><comments>14 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast cancer is considered as one of a major health problem that constitutes
the strongest cause behind mortality among women in the world. So, in this
decade, breast cancer is the second most common type of cancer, in term of
appearance frequency, and the fifth most common cause of cancer related death.
In order to reduce the workload on radiologists, a variety of CAD systems;
Computer-Aided Diagnosis (CADi) and Computer-Aided Detection (CADe) have been
proposed. In this paper, we interested on CADe tool to help radiologist to
detect cancer. The proposed CADe is based on a three-step work flow; namely,
detection, analysis and classification. This paper deals with the problem of
automatic detection of Region Of Interest (ROI) based on Level Set approach
depended on edge and region criteria. This approach gives good visual
information from the radiologist. After that, the features extraction using
textures characteristics and the vector classification using Multilayer
Perception (MLP) and k-Nearest Neighbours (KNN) are adopted to distinguish
different ACR (American College of Radiology) classification. Moreover, we use
the Digital Database for Screening Mammography (DDSM) for experiments and these
results in term of accuracy varied between 60 % and 70% are acceptable and must
be ameliorated to aid radiologist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0166</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0166</id><created>2011-09-01</created><authors><author><keyname>Amini</keyname><forenames>Bahram</forenames></author><author><keyname>Ibrahim</keyname><forenames>Roliana</forenames></author><author><keyname>Othman</keyname><forenames>Mohd Shahizan</forenames></author></authors><title>Discovering the Impact of Knowledge in Recommender Systems: A
  Comparative Study</title><categories>cs.IR</categories><comments>14 pages, 3 tables; International Journal of Computer Science &amp;
  Engineering Survey (IJCSES) Vol.2, No.3, August 2011</comments><doi>10.5121/ijcses.2011.2301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems engage user profiles and appropriate filtering techniques
to assist users in finding more relevant information over the large volume of
information. User profiles play an important role in the success of
recommendation process since they model and represent the actual user needs.
However, a comprehensive literature review of recommender systems has
demonstrated no concrete study on the role and impact of knowledge in user
profiling and filtering approache. In this paper, we review the most prominent
recommender systems in the literature and examine the impression of knowledge
extracted from different sources. We then come up with this finding that
semantic information from the user context has substantial impact on the
performance of knowledge based recommender systems. Finally, some new clues for
improvement the knowledge-based profiles have been proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0172</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0172</id><created>2011-09-01</created><authors><author><keyname>Ravins</keyname></author><author><keyname>Singh</keyname><forenames>R. K. Brojen</forenames></author></authors><title>Effect of diffusion of elements on network topology and
  self-organization</title><categories>physics.comp-ph cs.SI physics.soc-ph</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the influence of elements diffusing in and out of a network to the
topological changes of the network and characterize it by investigating the
behavior of probability of degree distribution ($\Gamma(k)$) with degree $k$.
The local memory of the incoming element and its interaction with the elements
already present in the network during the growing process significantly affect
the network stability which in turn reorganize the network properties. We found
that the properties of $\Gamma(k)$ of this network are deviated from scale free
type, where the power law behavior contains a exponentially decay factor
supporting earlier reported results of Amaral et.al. \cite{ama} and Newman
\cite{new1} and recent statistical analysis results on degree distribution data
of some scale free network [11]. Our numerical results also support the
behavior of this $\Gamma(k)$. However, we found numerically the contribution
from exponential factor to the $\Gamma(k)$ to be very weak as compared to the
scale free factor showing that the network as a whole carries the scale free
properties approximately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0181</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0181</id><created>2011-09-01</created><authors><author><keyname>Umbrich</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Hogan</keyname><forenames>Aidan</forenames></author><author><keyname>Polleres</keyname><forenames>Axel</forenames></author></authors><title>Improving the recall of decentralised linked data querying through
  implicit knowledge</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aside from crawling, indexing, and querying RDF data centrally, Linked Data
principles allow for processing SPARQL queries on-the-fly by dereferencing
URIs. Proposed link-traversal query approaches for Linked Data have the
benefits of up-to-date results and decentralised (i.e., client-side) execution,
but operate on incomplete knowledge available in dereferenced documents, thus
affecting recall. In this paper, we investigate how implicit knowledge -
specifically that found through owl:sameAs and RDFS reasoning - can improve the
recall in this setting. We start with an empirical analysis of a large crawl
featuring 4 m Linked Data sources and 1.1 g quadruples: we (1) measure expected
recall by only considering dereferenceable information, (2) measure the
improvement in recall given by considering rdfs:seeAlso links as previous
proposals did. We further propose and measure the impact of additionally
considering (3) owl:sameAs links, and (4) applying lightweight RDFS reasoning
(specifically {\rho}DF) for finding more results, relying on static schema
information. We evaluate our methods for live queries over our crawl.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0213</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0213</id><created>2011-08-05</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Lin</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Shizhen</forenames></author></authors><title>A Novel VSWR-Protected and Controllable CMOS Class E Power Amplifier for
  Bluetooth Applications</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design of a differential class-E PA for Bluetooth
applications in 0.18um CMOS technology with load mismatch protection and power
control features. The breakdown induced by load mismatch can be avoided by
attenuating the RF power to the final stage during over voltage conditions.
Power control is realized by means of &quot;open loop&quot; techniques to regulate the
power supply voltage, and a novel controllable bias network with temperature
compensated is proposed, which allows a moderate power control slope (dB/V) to
be achieved. Post-layout Simulation results show that the level of output power
can be controlled in 2dBm steps; especially the output power in every step is
quite insensitive to temperature variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0214</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0214</id><created>2011-08-31</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Complexity</title><categories>cs.OH</categories><comments>Draft entry for the Encyclopedia of Philosophy and Social Sciences,
  Sage, 2013</comments><report-no>C3 2011.05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term complexity derives etymologically from the Latin plexus, which means
interwoven. Intuitively, this implies that something complex is composed by
elements that are difficult to separate. This difficulty arises from the
relevant interactions that take place between components. This lack of
separability is at odds with the classical scientific method - which has been
used since the times of Galileo, Newton, Descartes, and Laplace - and has also
influenced philosophy and engineering. In recent decades, the scientific study
of complexity and complex systems has proposed a paradigm shift in science and
philosophy, proposing novel methods that take into account relevant
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0216</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0216</id><created>2011-08-31</created><authors><author><keyname>Shahbahrami</keyname><forenames>Asadollah</forenames></author><author><keyname>Bahrampour</keyname><forenames>Ramin</forenames></author><author><keyname>Rostami</keyname><forenames>Mobin Sabbaghi</forenames></author><author><keyname>Mobarhan</keyname><forenames>Mostafa Ayoubi</forenames></author></authors><title>Evaluation of Huffman and Arithmetic Algorithms for Multimedia
  Compression Standards</title><categories>cs.IT cs.MM math.IT</categories><comments>11 pages; http://airccse.org/journal/ijcsea/current.html
  International Journal of Computer Science, Engineering and Applications
  (IJCSEA) August 2011, Volume 1, Number 4</comments><msc-class>68Uxx</msc-class><acm-class>H.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression is a technique to reduce the quantity of data without excessively
reducing the quality of the multimedia data. The transition and storing of
compressed multimedia data is much faster and more efficient than original
uncompressed multimedia data. There are various techniques and standards for
multimedia data compression, especially for image compression such as the JPEG
and JPEG2000 standards. These standards consist of different functions such as
color space conversion and entropy coding. Arithmetic and Huffman coding are
normally used in the entropy coding phase. In this paper we try to answer the
following question. Which entropy coding, arithmetic or Huffman, is more
suitable compared to other from the compression ratio, performance, and
implementation points of view? We have implemented and tested Huffman and
arithmetic algorithms. Our implemented results show that compression ratio of
arithmetic coding is better than Huffman coding, while the performance of the
Huffman coding is higher than Arithmetic coding. In addition, implementation of
Huffman coding is much easier than the Arithmetic coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0217</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0217</id><created>2011-08-13</created><authors><author><keyname>Cai</keyname><forenames>Xiaohao</forenames></author><author><keyname>Chan</keyname><forenames>Raymond</forenames></author><author><keyname>Morigi</keyname><forenames>Serena</forenames></author><author><keyname>Sgallari</keyname><forenames>Fiorella</forenames></author></authors><title>Vessel Segmentation in Medical Imaging Using a Tight-Frame Based
  Algorithm</title><categories>math.NA cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Tight-frame, a generalization of orthogonal wavelets, has been used
successfully in various problems in image processing, including inpainting,
impulse noise removal, super-resolution image restoration, etc. Segmentation is
the process of identifying object outlines within images. There are quite a few
efficient algorithms for segmentation that depend on the variational approach
and the partial differential equation (PDE) modeling.
  In this paper, we propose to apply the tight-frame approach to automatically
identify tube-like structures such as blood vessels in Magnetic Resonance
Angiography (MRA) images. Our method iteratively refines a region that encloses
the possible boundary or surface of the vessels. In each iteration, we apply
the tight-frame algorithm to denoise and smooth the possible boundary and
sharpen the region. We prove the convergence of our algorithm. Numerical
experiments on real 2D/3D MRA images demonstrate that our method is very
efficient with convergence usually within a few iterations, and it outperforms
existing PDE and variational methods as it can extract more tubular objects and
fine details in the images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0246</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0246</id><created>2011-09-01</created><authors><author><keyname>Farooq</keyname><forenames>Umer</forenames></author><author><keyname>Iqbal</keyname><forenames>M. Aqeel</forenames></author><author><keyname>Shabbir</keyname><forenames>Usman</forenames></author><author><keyname>Nazir</keyname><forenames>Sohail</forenames></author></authors><title>Critical Aspects of Modern Open Source SoftwareTechnology to Support
  Emerging Demands</title><categories>cs.SE</categories><comments>7 pages, 3 tables</comments><journal-ref>International Journal of Computer Applications Volume 27 - No.9,
  2011, 13-19</journal-ref><doi>10.5120/3330-4580</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software has gained immense importance in our everyday lifeand is handling
each and every aspect of today's technologicalworld. The idea of software at
initial phase was implemented bya very precise minority of individual and now
it's everywherewhether one's personal life or an organization
.Financiallystrong organization and people who can purchase this bounty
oftechnological era can fulfill their desires efficiently. For sure it's not a
generalized case that one is financially strong and caneasily afford the
desired software. There are numerous userswho cannot do so. Open source
software has a way out for theseusers it provides them the same facilities and
functionalities asin their equivalent software irrespective of any
financialpressure. So the financially constrained personals ororganization can
make use of open source software forachievement of their desired tasks. In this
research paper ananalysis of open source software has been presented
byproviding a brief comparison of Ubuntu as an emerging highquality open source
modern operating system with well knownMicrosoft windows operating system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0257</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0257</id><created>2011-08-05</created><authors><author><keyname>Bhattacharya</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Khandelwal</keyname><forenames>Ronak</forenames></author><author><keyname>Gera</keyname><forenames>Rishita</forenames></author><author><keyname>Agarwal</keyname><forenames>Anjali</forenames></author></authors><title>Smart Radio Spectrum Management for Cognitive Radio</title><categories>cs.NI</categories><comments>13 pages, 11 figures</comments><journal-ref>International Journal of Parallel and Distributed Systems, Vol. 2,
  NO 4, July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's wireless networks are characterized by fixed spectrum assignment
policy. The limited available spectrum and the inefficiency in the spectrum
usage necessitate a new communication paradigm to exploit the existing wireless
spectrum opportunistically. Cognitive radio is a paradigm for wireless
communication in which either a network or a wireless node changes its
transmission or reception parameters to communicate efficiently avoiding
interference with licensed or unlicensed users. In this work, a fuzzy logic
based system for spectrum management is proposed where the radio can share
unused spectrum depending on some parameters like distance, signal strength,
node velocity and availability of unused spectrum. The system is simulated and
is found to give satisfactory results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0262</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0262</id><created>2011-09-01</created><updated>2012-03-15</updated><authors><author><keyname>Potter</keyname><forenames>Gail E.</forenames></author><author><keyname>Handcock</keyname><forenames>Mark S.</forenames></author><author><keyname>Longini,</keyname><forenames>Ira M.</forenames><suffix>Jr.</suffix></author><author><keyname>Halloran</keyname><forenames>M. Elizabeth</forenames></author></authors><title>Estimating within-school contact networks to understand influenza
  transmission</title><categories>stat.ME cs.SI physics.soc-ph stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOAS505 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS505</report-no><journal-ref>Annals of Applied Statistics 2012, Vol. 6, No. 1, 1-26</journal-ref><doi>10.1214/11-AOAS505</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many epidemic models approximate social contact behavior by assuming random
mixing within mixing groups (e.g., homes, schools and workplaces). The effect
of more realistic social network structure on estimates of epidemic parameters
is an open area of exploration. We develop a detailed statistical model to
estimate the social contact network within a high school using friendship
network data and a survey of contact behavior. Our contact network model
includes classroom structure, longer durations of contacts to friends than
nonfriends and more frequent contacts with friends, based on reports in the
contact survey. We performed simulation studies to explore which network
structures are relevant to influenza transmission. These studies yield two key
findings. First, we found that the friendship network structure important to
the transmission process can be adequately represented by a dyad-independent
exponential random graph model (ERGM). This means that individual-level sampled
data is sufficient to characterize the entire friendship network. Second, we
found that contact behavior was adequately represented by a static rather than
dynamic contact network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0264</identifier>
 <datestamp>2011-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0264</id><created>2011-09-01</created><authors><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Luo</keyname><forenames>Jianqiang</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Huang</keyname><forenames>Cheng</forenames></author><author><keyname>Li</keyname><forenames>Jin</forenames></author></authors><title>Simple Regenerating Codes: Network Coding for Cloud Storage</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>9 pages, 10 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network codes designed specifically for distributed storage systems have the
potential to provide dramatically higher storage efficiency for the same
availability. One main challenge in the design of such codes is the exact
repair problem: if a node storing encoded information fails, in order to
maintain the same level of reliability we need to create encoded information at
a new node. One of the main open problems in this emerging area has been the
design of simple coding schemes that allow exact and low cost repair of failed
nodes and have high data rates. In particular, all prior known explicit
constructions have data rates bounded by 1/2.
  In this paper we introduce the first family of distributed storage codes that
have simple look-up repair and can achieve arbitrarily high rates. Our
constructions are very simple to implement and perform exact repair by simple
XORing of packets. We experimentally evaluate the proposed codes in a realistic
cloud storage simulator and show significant benefits in both performance and
reliability compared to replication and standard Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0303</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0303</id><created>2011-09-01</created><authors><author><keyname>Mukkamala</keyname><forenames>V S Padmini</forenames></author></authors><title>Obstacles, Slopes, and Tic-Tac-Toe: An excursion in discrete geometry
  and combinatorial game theory</title><categories>math.CO cs.DM</categories><comments>This is Padmini Mukkamala's PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A drawing of a graph is said to be a {\em straight-line drawing} if the
vertices of $G$ are represented by distinct points in the plane and every edge
is represented by a straight-line segment connecting the corresponding pair of
vertices and not passing through any other vertex of $G$. The minimum number of
slopes in a straight-line drawing of $G$ is called the slope number of $G$. We
show that every cubic graph can be drawn in the plane with straight-line edges
using only the four basic slopes $\{0,\pi/4,\pi/2,-\pi/4\}$. We also prove that
four slopes have this property if and only if we can draw $K_4$ with them.
  Given a graph $G$, an {\em obstacle representation} of $G$ is a set of points
in the plane representing the vertices of $G$, together with a set of obstacles
(connected polygons) such that two vertices of $G$ are joined by an edge if and
only if the corresponding points can be connected by a segment which avoids all
obstacles. The {\em obstacle number} of $G$ is the minimum number of obstacles
in an obstacle representation of $G$. We show that there are graphs on $n$
vertices with obstacle number $\Omega({n}/{\log n})$.
  We show that there is an $m=2n+o(n)$, such that, in the Maker-Breaker game
played on $\Z^d$ where Maker needs to put at least $m$ of his marks
consecutively in one of $n$ given winning directions, Breaker can force a draw
using a pairing strategy. This improves the result of Kruczek and Sundberg who
showed that such a pairing strategy exits if $m\ge 3n$. A simple argument shows
that $m$ has to be at least $2n+1$ if Breaker is only allowed to use a pairing
strategy, thus the main term of our bound is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0312</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0312</id><created>2011-09-01</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Simons</keyname><forenames>Joseph A.</forenames></author></authors><title>Fully Retroactive Approximate Range and Nearest Neighbor Searching</title><categories>cs.CG cs.DS</categories><comments>24 pages, 4 figures. To appear at the 22nd International Symposium on
  Algorithms and Computation (ISAAC 2011)</comments><acm-class>E.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe fully retroactive dynamic data structures for approximate range
reporting and approximate nearest neighbor reporting. We show how to maintain,
for any positive constant $d$, a set of $n$ points in $\R^d$ indexed by time
such that we can perform insertions or deletions at any point in the timeline
in $O(\log n)$ amortized time. We support, for any small constant $\epsilon&gt;0$,
$(1+\epsilon)$-approximate range reporting queries at any point in the timeline
in $O(\log n + k)$ time, where $k$ is the output size. We also show how to
answer $(1+\epsilon)$-approximate nearest neighbor queries for any point in the
past or present in $O(\log n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0318</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0318</id><created>2011-09-01</created><authors><author><keyname>Mantzel</keyname><forenames>William</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author><author><keyname>Sabra</keyname><forenames>Karim</forenames></author></authors><title>Compressive Matched-Field Processing</title><categories>cs.IT math.IT</categories><doi>10.1121/1.4728224</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source localization by matched-field processing (MFP) generally involves
solving a number of computationally intensive partial differential equations.
This paper introduces a technique that mitigates this computational workload by
&quot;compressing&quot; these computations. Drawing on key concepts from the recently
developed field of compressed sensing, it shows how a low-dimensional proxy for
the Green's function can be constructed by backpropagating a small set of
random receiver vectors. Then, the source can be located by performing a number
of &quot;short&quot; correlations between this proxy and the projection of the recorded
acoustic data in the compressed space. Numerical experiments in a Pekeris ocean
waveguide are presented which demonstrate that this compressed version of MFP
is as effective as traditional MFP even when the compression is significant.
The results are particularly promising in the broadband regime where using as
few as two random backpropagations per frequency performs almost as well as the
traditional broadband MFP, but with the added benefit of generic applicability.
That is, the computationally intensive backpropagations may be computed offline
independently from the received signals, and may be reused to locate any source
within the search grid area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0323</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0323</id><created>2011-09-01</created><authors><author><keyname>Danvy</keyname><forenames>Olivier</forenames></author><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames></author></authors><title>Proceedings IFIP Working Conference on Domain-Specific Languages</title><categories>cs.PL</categories><comments>This volume is dedicated to the memory of Anne-Fran\c{c}oise Le Meur
  (1972--2011)</comments><proxy>EPTCS</proxy><acm-class>D.3.2</acm-class><journal-ref>EPTCS 66, 2011</journal-ref><doi>10.4204/EPTCS.66</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume is the proceedings of the second IFIP Working Conference on
Domain-Specific Languages (DSL 2011). It contains 2 abstracts of invited
presentations, 7 peer-reviewed articles selected by the program committee from
14 submissions, and 6 lecture notes for the distilled tutorials that we
solicited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0324</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0324</id><created>2011-09-01</created><authors><author><keyname>Yessad</keyname><forenames>Lamia</forenames></author><author><keyname>Boufaida</keyname><forenames>Zizette</forenames></author></authors><title>A qos ontology-based component selection</title><categories>cs.SE</categories><comments>15 pages, 5 figures, 6 tables</comments><journal-ref>International Journal on Soft Computing ( IJSC ), Vol.2, No.3,
  August 2011</journal-ref><doi>10.5121/ijsc.2011.2302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the component-based software development, the selection step is very
important. It consists of searching and selecting appropriate software
components from a set of candidate components in order to satisfy the
developer-specific requirements. In the selection process, both functional and
non-functional requirements are generally considered. In this paper, we focus
only on the QoS, a subset of non-functional characteristics, in order to
determine the best components for selection. The component selection based on
the QoS is a hard task due to the QoS descriptions heterogeneity. Thus, we
propose a QoS ontology which provides a formal, a common and an explicit
description of the software components QoS. We use this ontology in order to
semantically select relevant components based on the QoS specified by the
developer. Our selection process is performed in two steps: (1) a QoS matching
process that uses the relations between QoS concepts to pre-select candidate
components. Each candidate component is matched against the developer's request
and (2) a component ranking process that uses the QoS values to determine the
best components for selection from the pre-selected components. The algorithms
of QoS matching and component ranking are then presented and experimented in
the domain of multimedia components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0325</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0325</id><created>2011-09-01</created><authors><author><keyname>Pudenz</keyname><forenames>Kristen L.</forenames></author><author><keyname>Lidar</keyname><forenames>Daniel A.</forenames></author></authors><title>Quantum adiabatic machine learning</title><categories>quant-ph cs.LG</categories><comments>21 pages, 9 figures</comments><doi>10.1007/s11128-012-0506-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an approach to machine learning and anomaly detection via quantum
adiabatic evolution. In the training phase we identify an optimal set of weak
classifiers, to form a single strong classifier. In the testing phase we
adiabatically evolve one or more strong classifiers on a superposition of
inputs in order to find certain anomalous elements in the classification space.
Both the training and testing phases are executed via quantum adiabatic
evolution. We apply and illustrate this approach in detail to the problem of
software verification and validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0333</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0333</id><created>2011-09-01</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>A KIF Formalization for the IFF Category Theory Ontology</title><categories>cs.LO cs.AI math.CT</categories><comments>Paper presented at the Standard Upper Ontology workshop of the 17th
  International Joint Conference on Artificial Intelligence (IJCAI-01), August,
  2001, Seattle, Washington</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper begins the discussion of how the Information Flow Framework can be
used to provide a principled foundation for the metalevel (or structural level)
of the Standard Upper Ontology (SUO). This SUO structural level can be used as
a logical framework for manipulating collections of ontologies in the object
level of the SUO or other middle level or domain ontologies. From the
Information Flow perspective, the SUO structural level resolves into several
metalevel ontologies. This paper discusses a KIF formalization for one of those
metalevel categories, the Category Theory Ontology. In particular, it discusses
its category and colimit sub-namespaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0337</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0337</id><created>2011-09-01</created><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author></authors><title>On discrete cosine transform</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The discrete cosine transform (DCT), introduced by Ahmed, Natarajan and Rao,
has been used in many applications of digital signal processing, data
compression and information hiding. There are four types of the discrete cosine
transform. In simulating the discrete cosine transform, we propose a
generalized discrete cosine transform with three parameters, and prove its
orthogonality for some new cases. A new type of discrete cosine transform is
proposed and its orthogonality is proved. Finally, we propose a generalized
discrete W transform with three parameters, and prove its orthogonality for
some new cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0345</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0345</id><created>2011-09-01</created><authors><author><keyname>Duncan</keyname><forenames>Christian A.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author></authors><title>Planar and Poly-Arc Lombardi Drawings</title><categories>cs.CG cs.DM</categories><comments>Expanded version of paper appearing in the 19th International
  Symposium on Graph Drawing (GD 2011). 16 pages, 8 figures</comments><msc-class>05C10, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Lombardi drawings of graphs, edges are represented as circular arcs, and
the edges incident on vertices have perfect angular resolution. However, not
every graph has a Lombardi drawing, and not every planar graph has a planar
Lombardi drawing. We introduce k-Lombardi drawings, in which each edge may be
drawn with k circular arcs, noting that every graph has a smooth 2-Lombardi
drawing. We show that every planar graph has a smooth planar 3-Lombardi drawing
and further investigate topics connecting planarity and Lombardi drawings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0351</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0351</id><created>2011-09-01</created><updated>2012-10-31</updated><authors><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author></authors><title>Directed Information, Causal Estimation, and Communication in Continuous
  Time</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A notion of directed information between two continuous-time processes is
proposed. A key component in the definition is taking an infimum over all
possible partitions of the time interval, which plays a role no less
significant than the supremum over &quot;space&quot; partitions inherent in the
definition of mutual information. Properties and operational interpretations in
estimation and communication are then established for the proposed notion of
directed information. For the continuous-time additive white Gaussian noise
channel, it is shown that Duncan's classical relationship between causal
estimation and information continues to hold in the presence of feedback upon
replacing mutual information by directed information. A parallel result is
established for the Poisson channel. The utility of this relationship is then
demonstrated in computing the directed information rate between the input and
output processes of a continuous-time Poisson channel with feedback, where the
channel input process is constrained to be constant between events at the
channel output. Finally, the capacity of a wide class of continuous-time
channels with feedback is established via directed information, characterizing
the fundamental limit on reliable communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0359</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0359</id><created>2011-09-02</created><authors><author><keyname>Srinath</keyname><forenames>T. R.</forenames></author><author><keyname>Singh</keyname><forenames>Mahendra Pratap</forenames></author><author><keyname>Pais</keyname><forenames>Alwyn Roshan</forenames></author></authors><title>Anonymity and verifiability in multi-attribute reverse auction</title><categories>cs.CR</categories><comments>9 pages, 1 figure and 2 tables</comments><journal-ref>International Journal of Information Technology Convergence and
  Services (IJITCS) Vol.1, No.4, August 2011</journal-ref><doi>10.5121/ijitcs.2011.1401</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The use of e-Auction services has been increasing in recent years. Security
requirements in conducting e-Auctions are mainly bid privacy, anonymity and
public verifiability. Most of the secure protocols concentrate on privacy and
anonymity, which are achieved through bidder-resolved multi-party computation,
assuming two or more trusted third parties, either through numerous auctioneers
or with asymmetric models in which the commercial entity of an auction issuer
or registration manager is assumed in addition to the auctioneer.
Multi-attribute reverse auctions promise higher market efficiency and effective
information exchange. This work extends and uses the existing schemes. This
scheme uses scoring function, winner determination in multi-attribute auctions
to implement public verifiability. Anonymity is achieved through bidder side
pseudonym generation. By results and analysis we say this is very simple and
effective scheme. This scheme ensures public verifiability and anonymity in
multi-attribute auctions without revelation of the bids received, third parties
and complex communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0372</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0372</id><created>2011-09-02</created><updated>2012-03-14</updated><authors><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author></authors><title>Quantum Money with Classical Verification</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and construct a quantum money scheme that allows verification
through classical communication with a bank. This is the first demonstration
that a secure quantum money scheme exists that does not require quantum
communication for coin verification.
  Our scheme is secure against adaptive adversaries - this property is not
directly related to the possibility of classical verification, nevertheless
none of the earlier quantum money constructions is known to possess it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0375</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0375</id><created>2011-09-02</created><authors><author><keyname>Sefranek</keyname><forenames>Jan</forenames></author><author><keyname>Simko</keyname><forenames>Alexander</forenames></author></authors><title>Warranted Derivations of Preferred Answer</title><categories>cs.LO</categories><comments>wlp 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are aiming at a semantics of logic programs with preferences defined on
rules, which always selects a preferred answer set, if there is a non-empty set
of (standard) answer sets of the given program. It is shown in a seminal paper
by Brewka and Eiter that the goal mentioned above is incompatible with their
second principle and it is not satisfied in their semantics of prioritized
logic programs. Similarly, also according to other established semantics, based
on a prescriptive approach, there are programs with standard answer sets, but
without preferred answer sets. According to the standard prescriptive approach
no rule can be fired before a more preferred rule, unless the more preferred
rule is blocked. This is a rather imperative approach, in its spirit.
  In our approach, rules can be blocked by more preferred rules, but the rules
which are not blocked are handled in a more declarative style, their execution
does not depend on the given preference relation on the rules. An argumentation
framework (different from the Dung's framework) is proposed in this paper.
Argu- mentation structures are derived from the rules of a given program. An
attack relation on argumentation structures is defined, which is derived from
attacks of more preferred rules against the less preferred rules. Preferred
answer sets correspond to complete argumentation structures, which are not
blocked by other complete argumentation structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0389</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0389</id><created>2011-09-02</created><authors><author><keyname>Tsakalidis</keyname><forenames>Athanasios</forenames></author><author><keyname>Tsichlas</keyname><forenames>Kostas</forenames></author></authors><title>A Space-Optimal Hidden Surface Removal Algorithm for Iso-Oriented
  Rectangles</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of finding the visible pieces of a scene of
objects from a specified viewpoint. In particular, we are interested in the
design of an efficient hidden surface removal algorithm for a scene comprised
of iso-oriented rectangles. We propose an algorithm where given a set of $n$
iso-oriented rectangles we report all visible surfaces in $O((n+k)\log n)$ time
and linear space, where $k$ is the number of surfaces reported. The previous
best result by Bern, has the same time complexity but uses $O(n\log n)$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0392</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0392</id><created>2011-09-02</created><updated>2011-09-14</updated><authors><author><keyname>Dumont</keyname><forenames>Thierry</forenames></author></authors><title>Context Tree Estimation in Variable Length Hidden Markov Models</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of context tree estimation in variable length hidden
Markov models. We propose an estimator of the context tree of the hidden Markov
process which needs no prior upper bound on the depth of the context tree. We
prove that the estimator is strongly consistent. This uses
information-theoretic mixture inequalities in the spirit of Finesso and
Lorenzo(Consistent estimation of the order for Markov and hidden Markov
chains(1990)) and E.Gassiat and S.Boucheron (Optimal error exponents in hidden
Markov model order estimation(2003)). We propose an algorithm to efficiently
compute the estimator and provide simulation studies to support our result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0397</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0397</id><created>2011-09-02</created><updated>2014-07-30</updated><authors><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author></authors><title>Auction-Based Resource Allocation in Digital Ecosystems</title><categories>cs.DC</categories><comments>Proceedings of the 6th International Conference on MOBILe Wireless
  MiddleWARE, Operating Systems, and Applications (MobilWare 2013). Bologna,
  Italy, November 11-12, 2013</comments><acm-class>C.2.4; H.m</acm-class><doi>10.1109/Mobilware.2013.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of portable devices (PDAs, smartphones, digital multimedia
players, and so forth) allows mobile users to carry around a pool of computing,
storage and communication resources. Sharing these resources with other users
(&quot;Digital Organisms&quot; -- DOs) opens the door to novel interesting scenarios,
where people trade resources to allow the execution, anytime and anywhere, of
applications that require a mix of capabilities. In this paper we present a
fully distributed approach for resource sharing among multiple devices owned by
different mobile users. Our scheme enables DOs to trade computing/networking
facilities through an auction-based mechanism, without the need of a central
control. We use a set of numerical experiments to compare our approach with an
optimal (centralized) allocation strategy that, given the set of resource
demands and offers, maximizes the number of matches. Results confirm the
effectiveness of our approach since it produces a fair allocation of resources
with low computational cost, providing DOs with the means to form an altruistic
digital ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0414</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0414</id><created>2011-09-02</created><authors><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Anti-Structure Problems</title><categories>cs.IT math.IT</categories><comments>a short note, following the Banff meeting on Algebraic structure in
  network information theroy, Aug. 14-19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent success of structured solutions for a class of
information-theoretic network problems, calls for exploring their limits. We
show that sum-product channels resist a solution by structured (as well as
random) codes. We conclude that the structured approach fails whenever the
channel operations do not commute (or for general functional channels, when the
channel function is non decomposable).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0418</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0418</id><created>2011-09-02</created><authors><author><keyname>Manathara</keyname><forenames>Joel George</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Ghose</keyname><forenames>Dabasish</forenames></author></authors><title>Tropical Algebraic approach to Consensus over Networks</title><categories>math.OC cs.DM cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the convergence of the max-consensus protocol.
Tropical algebra is used to formulate the problem. Necessary and sufficient
conditions for convergence of the max-consensus protocol over fixed as well as
switching topology networks are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0420</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0420</id><created>2011-09-02</created><authors><author><keyname>Ni</keyname><forenames>Yizhao</forenames></author><author><keyname>Mcvicar</keyname><forenames>Matt</forenames></author><author><keyname>Santos-Rodriguez</keyname><forenames>Raul</forenames></author><author><keyname>De Bie</keyname><forenames>Tijl</forenames></author></authors><title>Meta-song evaluation for chord recognition</title><categories>cs.IR</categories><comments>technique report and preparation for conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to evaluate chord recognition systems on songs
which do not have full annotations. The principle is to use online chord
databases to generate high accurate &quot;pseudo annotations&quot; for these songs and
compute &quot;pseudo accuracies&quot; of test systems. Statistical models that model the
relationship between &quot;pseudo accuracy&quot; and real performance are then applied to
estimate test systems' performance. The approach goes beyond the existing
evaluation metrics, allowing us to carry out extensive analysis on chord
recognition systems, such as their generalizations to different genres. In the
experiments we applied this method to evaluate three state-of-the-art chord
recognition systems, of which the results verified its reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0428</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0428</id><created>2011-09-02</created><authors><author><keyname>Leghmizi</keyname><forenames>Said</forenames></author><author><keyname>Liu</keyname><forenames>Sheng</forenames></author></authors><title>A survey of fuzzy control for stabilized platforms</title><categories>cs.SY</categories><journal-ref>International Journal of Computer Science &amp; Engineering Survey
  (IJCSES) Vol.2, No.3, August 2011</journal-ref><doi>10.5121/ijcses.2011.2304</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper focusses on the application of fuzzy control techniques (fuzzy
type-1 and type-2) and their hybrid forms (Hybrid adaptive fuzzy controller and
fuzzy-PID controller) in the area of stabilized platforms. It represents an
attempt to cover the basic principles and concepts of fuzzy control in
stabilization and position control, with an outline of a number of recent
applications used in advanced control of stabilized platform. Overall, in this
survey we will make some comparisons with the classical control techniques such
us PID control to demonstrate the advantages and disadvantages of the
application of fuzzy control techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0455</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0455</id><created>2011-09-02</created><authors><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author><author><keyname>Leng</keyname><forenames>Chenlei</forenames></author></authors><title>Gradient-based kernel dimension reduction for supervised learning</title><categories>stat.ML cs.LG</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel kernel approach to linear dimension reduction for
supervised learning. The purpose of the dimension reduction is to find
directions in the input space to explain the output as effectively as possible.
The proposed method uses an estimator for the gradient of regression function,
based on the covariance operators on reproducing kernel Hilbert spaces. In
comparison with other existing methods, the proposed one has wide applicability
without strong assumptions on the distributions or the type of variables, and
uses computationally simple eigendecomposition. Experimental results show that
the proposed method successfully finds the effective directions with efficient
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0456</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0456</id><created>2011-09-01</created><authors><author><keyname>Di Cosmo</keyname><forenames>Roberto</forenames><affiliation>Univ Paris Diderot, Sorbonne Paris Cite, and INRIA Rocquencourt, Paris, France</affiliation></author><author><keyname>Lhomme</keyname><forenames>Olivier</forenames><affiliation>IBM France, Sophia Antipolis, France</affiliation></author><author><keyname>Michel</keyname><forenames>Claude</forenames><affiliation>I3S</affiliation></author></authors><title>Aligning component upgrades</title><categories>cs.SE cs.LO</categories><comments>In Proceedings LoCoCo 2011, arXiv:1108.6097</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 65, 2011, pp. 1-11</journal-ref><doi>10.4204/EPTCS.65.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern software systems, like GNU/Linux distributions or Eclipse-based
development environment, are often deployed by selecting components out of
large component repositories. Maintaining such software systems by performing
component upgrades is a complex task, and the users need to have an expressive
preferences language at their disposal to specify the kind of upgrades they are
interested in. Recent research has shown that it is possible to develop solvers
that handle preferences expressed as a combination of a few basic criteria used
in the MISC competition, ranging from the number of new components to the
freshness of the final configuration. In this work we introduce a set of new
criteria that allow the users to specify their preferences for solutions with
components aligned to the same upstream sources, provide an efficient encoding
and report on the experimental results that prove that optimising these
alignment criteria is a tractable problem in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0486</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0486</id><created>2011-09-02</created><updated>2012-11-12</updated><authors><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Vicen&#xe7;</forenames></author></authors><title>The Variational Garrote</title><categories>stat.ME cs.LG</categories><comments>26 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new variational method for sparse regression
using $L_0$ regularization. The variational parameters appear in the
approximate model in a way that is similar to Breiman's Garrote model. We refer
to this method as the variational Garrote (VG). We show that the combination of
the variational approximation and $L_0$ regularization has the effect of making
the problem effectively of maximal rank even when the number of samples is
small compared to the number of variables. The VG is compared numerically with
the Lasso method, ridge regression and the recently introduced paired mean
field method (PMF) (M. Titsias &amp; M. L\'azaro-Gredilla., NIPS 2012). Numerical
results show that the VG and PMF yield more accurate predictions and more
accurately reconstruct the true model than the other methods. It is shown that
the VG finds correct solutions when the Lasso solution is inconsistent due to
large input correlations. Globally, VG is significantly faster than PMF and
tends to perform better as the problems become denser and in problems with
strongly correlated inputs. The naive implementation of the VG scales cubic
with the number of features. By introducing Lagrange multipliers we obtain a
dual formulation of the problem that scales cubic in the number of samples, but
close to linear in the number of features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0507</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0507</id><created>2011-09-02</created><authors><author><keyname>Barth</keyname><forenames>Adam</forenames></author><author><keyname>Li</keyname><forenames>Saung</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>How Open Should Open Source Be?</title><categories>cs.CR cs.LG</categories><comments>19 pages, 27 figures</comments><report-no>UCB/EECS-2011-98</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many open-source projects land security fixes in public repositories before
shipping these patches to users. This paper presents attacks on such projects -
taking Firefox as a case-study - that exploit patch metadata to efficiently
search for security patches prior to shipping. Using access-restricted bug
reports linked from patch descriptions, security patches can be immediately
identified for 260 out of 300 days of Firefox 3 development. In response to
Mozilla obfuscating descriptions, we show that machine learning can exploit
metadata such as patch author to search for security patches, extending the
total window of vulnerability by 5 months in an 8 month period when examining
up to two patches daily. Finally we present strong evidence that further
metadata obfuscation is unlikely to prevent information leaks, and we argue
that open-source projects instead ought to keep security patches secret until
they are ready to be released.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0522</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0522</id><created>2011-09-02</created><authors><author><keyname>Cooper</keyname><forenames>Joshua</forenames></author><author><keyname>Kay</keyname><forenames>Bill</forenames></author></authors><title>Graham's Tree Reconstruction Conjecture and a Waring-Type Problem on
  Partitions</title><categories>math.CO cs.DM</categories><comments>18 pages, 1 figure</comments><msc-class>05C76 (Primary) 05C05, 05C60, 11P05, 11P81 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose G is a tree. Graham's &quot;Tree Reconstruction Conjecture&quot; states that G
is uniquely determined by the integer sequence |G|, |L(G)|, |L(L(G))|,
|L(L(L(G)))|, ..., where L(H) denotes the line graph of the graph H. Little is
known about this question apart from a few simple observations. We show that
the number of trees on n vertices which can be distinguished by their
associated integer sequences is at least exp(c(log n)^(3/2)). The proof
strategy involves constructing a large collection of caterpillar graphs using
partitions arising from the Prouhet-Tarry-Escott problem. We identify, but only
partially resolve, an interesting question about representations of integers as
sums of k-th powers of the parts of integer partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0530</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0530</id><created>2011-09-02</created><authors><author><keyname>Ackerman</keyname><forenames>Margareta</forenames></author><author><keyname>Loker</keyname><forenames>David</forenames></author><author><keyname>Lopez-Ortiz</keyname><forenames>Alejandro</forenames></author></authors><title>Orthogonal Query Expansion</title><categories>cs.IR</categories><comments>15 pages</comments><msc-class>68P20, 68M11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last fifteen years, web searching has seen tremendous improvements.
Starting from a nearly random collection of matching pages in 1995, today,
search engines tend to satisfy the user's informational need on well-formulated
queries. One of the main remaining challenges is to satisfy the users' needs
when they provide a poorly formulated query. When the pages matching the user's
original keywords are judged to be unsatisfactory, query expansion techniques
are used to alter the result set. These techniques find keywords that are
similar to the keywords given by the user, which are then appended to the
original query leading to a perturbation of the result set. However, when the
original query is sufficiently ill-posed, the user's informational need is best
met using entirely different keywords, and a small perturbation of the original
result set is bound to fail.
  We propose a novel approach that is not based on the keywords of the original
query. We intentionally seek out orthogonal queries, which are related queries
that have low similarity to the user's query. The result sets of orthogonal
queries intersect with the result set of the original query on a small number
of pages. An orthogonal query can access the user's informational need while
consisting of entirely different terms than the original query. We illustrate
the effectiveness of our approach by proposing a query expansion method derived
from these observations that improves upon results obtained using the Yahoo
BOSS infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0545</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0545</id><created>2011-09-02</created><authors><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author><author><keyname>Yoffe</keyname><forenames>Genady</forenames></author></authors><title>Quality Up in Polynomial Homotopy Continuation by Multithreaded Path
  Tracking</title><categories>cs.DC cs.NA cs.SC math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speedup measures how much faster we can solve the same problem using many
cores. If we can afford to keep the execution time fixed, then quality up
measures how much better the solution will be computed using many cores. In
this paper we describe our multithreaded implementation to track one solution
path defined by a polynomial homotopy. Limiting quality to accuracy and
confusing accuracy with precision, we strive to offset the cost of
multiprecision arithmetic running multithreaded code on many cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0556</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0556</id><created>2011-09-02</created><authors><author><keyname>Ree</keyname><forenames>Suhan</forenames></author></authors><title>Effects of long-range links on metastable states in a dynamic
  interaction network</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>4 pages, 5 figures</comments><journal-ref>Phys. Rev. E. 85, 045101(R) (2012)</journal-ref><doi>10.1103/PhysRevE.85.045101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for random-walking nodes on a periodic lattice, where
the dynamic interaction network is defined from local interactions and E
randomly-added long-range links. With periodic states for nodes and an
interaction rule of repeated averaging, we numerically find two types of
metastable states at low- and high-E limits, respectively, along with consensus
states. If we apply this model to opinion dynamics, metastable states can be
interpreted as sustainable diversities in our societies, and our result then
implies that, while diversities decrease and eventually disappear with more
long-range connections, another type of states of diversities can appear when
networks are almost fully-connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0562</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0562</id><created>2011-09-02</created><authors><author><keyname>Malekesmaeili</keyname><forenames>Mehrnoush</forenames></author><author><keyname>Chauve</keyname><forenames>Cedric</forenames></author><author><keyname>Stephen</keyname><forenames>Tamon</forenames></author></authors><title>A tight bound on the length of odd cycles in the incompatibility graph
  of a non-C1P matrix</title><categories>cs.DS</categories><comments>7 pages</comments><msc-class>68W40, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary matrix has the consecutive ones property (C1P) if it is possible to
order the columns so that all 1s are consecutive in every row. In [McConnell,
SODA 2004 768-777] the notion of incompatibility graph of a binary matrix was
introduced and it was shown that odd cycles of this graph provide a certificate
that a matrix does not have the consecutive ones property. A bound of (k+2) was
claimed for the smallest odd cycle of a non-C1P matrix with k columns. In this
note we show that this result can be obtained simply and directly via Tucker
patterns, and that the correct bound is (k+2) when k is even, but (k+3) when k
is odd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0573</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0573</id><created>2011-09-02</created><updated>2011-09-20</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author><author><keyname>Voroninski</keyname><forenames>Vlad</forenames></author></authors><title>Phase Retrieval via Matrix Completion</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel framework for phase retrieval, a problem which
arises in X-ray crystallography, diffraction imaging, astronomical imaging and
many other applications. Our approach combines multiple structured
illuminations together with ideas from convex programming to recover the phase
from intensity measurements, typically from the modulus of the diffracted wave.
We demonstrate empirically that any complex-valued object can be recovered from
the knowledge of the magnitude of just a few diffracted patterns by solving a
simple convex optimization problem inspired by the recent literature on matrix
completion. More importantly, we also demonstrate that our noise-aware
algorithms are stable in the sense that the reconstruction degrades gracefully
as the signal-to-noise ratio decreases. Finally, we introduce some theory
showing that one can design very simple structured illumination patterns such
that three diffracted figures uniquely determine the phase of the object we
wish to recover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0583</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0583</id><created>2011-09-02</created><authors><author><keyname>Tasharrofi</keyname><forenames>Shahab</forenames><affiliation>Newman</affiliation></author><author><keyname>Xiongnan</keyname><affiliation>Newman</affiliation></author><author><keyname>Wu</keyname></author><author><keyname>Ternovska</keyname><forenames>Eugenia</forenames></author></authors><title>Solving Modular Model Expansion Tasks</title><categories>cs.LO</categories><comments>15 pages, 3 figures, 2 algorithms. This paper appears in the
  Proceedings of the 25th Workshop on Logic Programming (WLP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work we describe here is a part of a research program of developing
foundations of declarative solving of search problems. We consider the model
expansion task as the task representing the essence of search problems where we
are given an instance of a problem and are searching for a solution satisfying
certain properties. Such tasks are common in artificial intelligence, formal
verification, computational biology. Recently, the model expansion framework
was extended to deal with multiple modules. In the current paper, inspired by
practical combined solvers, we introduce an algorithm to solve model expansion
tasks for modular systems. We show that our algorithm closely corresponds to
what is done in practice in different areas such as Satisfiability Modulo
Theories (SMT), Integer Linear Programming (ILP), Answer Set Programming (ASP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0596</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0596</id><created>2011-09-03</created><authors><author><keyname>Zhang</keyname><forenames>Jia-Ning</forenames></author><author><keyname>Fang</keyname><forenames>Lei</forenames></author><author><keyname>Ge</keyname><forenames>Mo-Lin</forenames></author></authors><title>Discrete Wigner Function Reconstruction and Compressed Sensing</title><categories>quant-ph cond-mat.other cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A new reconstruction method for Wigner function is reported for quantum
tomography based on compressed sensing. By analogy with computed tomography,
Wigner functions for some quantum states can be reconstructed with less
measurements utilizing this compressed sensing based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0597</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0597</id><created>2011-09-03</created><updated>2011-11-22</updated><authors><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Khurshid</keyname><forenames>Ahmed</forenames></author><author><keyname>Juen</keyname><forenames>Joshua</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Stealthy Traffic Analysis of Low-Latency Anonymous Communication Using
  Throughput Fingerprinting</title><categories>cs.CR cs.NI</categories><comments>Accepted for publication in ACM CCS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymity systems such as Tor aim to enable users to communicate in a manner
that is untraceable by adversaries that control a small number of machines. To
provide efficient service to users, these anonymity systems make full use of
forwarding capacity when sending traffic between intermediate relays. In this
paper, we show that doing this leaks information about the set of Tor relays in
a circuit (path). We present attacks that, with high confidence and based
solely on throughput information, can (a) reduce the attacker's uncertainty
about the bottleneck relay of any Tor circuit whose throughput can be observed,
(b) exactly identify the guard relay(s) of a Tor user when circuit throughput
can be observed over multiple connections, and (c) identify whether two
concurrent TCP connections belong to the same Tor user, breaking unlinkability.
Our attacks are stealthy, and cannot be readily detected by a user or by Tor
relays. We validate our attacks using experiments over the live Tor network. We
find that the attacker can substantially reduce the entropy of a bottleneck
relay distribution of a Tor circuit whose throughput can be observed-the
entropy gets reduced by a factor of 2 in the median case. Such information
leaks from a single Tor circuit can be combined over multiple connections to
exactly identify a user's guard relay(s). Finally, we are also able to link two
connections from the same initiator with a crossover error rate of less than
1.5% in under 5 minutes. Our attacks are also more accurate and require fewer
resources than previous attacks on Tor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0601</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0601</id><created>2011-09-03</created><authors><author><keyname>Kornienko</keyname><forenames>S.</forenames></author><author><keyname>Kornienko</keyname><forenames>O.</forenames></author><author><keyname>Levi</keyname><forenames>P.</forenames></author></authors><title>Application of distributed constraint satisfaction problem to the
  agent-based planning in manufacturing systems</title><categories>cs.MA</categories><journal-ref>Proceedings of the International Scientific Congress &quot;Intelligent
  Systems (IEEE AIS'03)&quot; and &quot;Intelligent CAD's (CAD-2003)&quot;, p.124-140,
  Divnomorsk, Russia, 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, a globalization of national markets requires developing flexible
and demand-driven production systems. Agent-based technology, being
distributed, flexible and autonomous is expected to provide a short-time
reaction to disturbances and sudden changes of environment and allows
satisfying the mentioned requirements. The distributed constraint satisfaction
approach underlying the suggested method is described by a modified Petri
network providing both the conceptual notions and main details of
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0604</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0604</id><created>2011-09-03</created><updated>2011-11-07</updated><authors><author><keyname>Li</keyname><forenames>Liang</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Approximate Counting via Correlation Decay in Spin Systems</title><categories>cs.DS math.PR</categories><comments>29 pages, 1 figure, to appear in SODA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first deterministic fully polynomial-time approximation scheme
(FPTAS) for computing the partition function of a two-state spin system on an
arbitrary graph, when the parameters of the system satisfy the uniqueness
condition on infinite regular trees. This condition is of physical significance
and is believed to be the right boundary between approximable and
inapproximable.
  The FPTAS is based on the correlation decay technique introduced by
Bandyopadhyay and Gamarnik [SODA 06] and Weitz [STOC 06]. The classic
correlation decay is defined with respect to graph distance. Although this
definition has natural physical meanings, it does not directly support an FPTAS
for systems on arbitrary graphs, because for graphs with unbounded degrees, the
local computation that provides a desirable precision by correlation decay may
take super-polynomial time. We introduce a notion of computationally efficient
correlation decay, in which the correlation decay is measured in a refined
metric instead of graph distance. We use a potential method to analyze the
amortized behavior of this correlation decay and establish a correlation decay
that guarantees an inverse-polynomial precision by polynomial-time local
computation. This gives us an FPTAS for spin systems on arbitrary graphs. This
new notion of correlation decay properly reflects the algorithmic aspect of the
spin systems, and may be used for designing FPTAS for other counting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0614</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0614</id><created>2011-09-03</created><updated>2011-12-08</updated><authors><author><keyname>Rao</keyname><forenames>G. Koteswara</forenames></author><author><keyname>Kumar</keyname><forenames>Roshan</forenames></author></authors><title>Framework to Integrate Business Intelligence and Knowledge Management in
  Banking Industry</title><categories>cs.OH</categories><journal-ref>Review of Business and Technology Research, Vol.4, No.1,
  July-2011, ISSN 1941-9406</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this digital age organizations depend upon the technologies to provide
customer-centric solutions by understanding well about their customers'
behaviour and continuously improving business process of the organization.
Business intelligence (BI) applications will play a vital role at this stage by
discovering the knowledge hidden in internal as well as external sources. On
the other hand, Knowledge Management (KM) will enhance the organisations
performance by providing collaborative tools to learn, create and share the
knowledge among the employees. The main intention of the BI is to enhance the
employees' knowledge with information that allows them to make decisions to
achieve its organisational strategies. However only twenty percent of data
exist in structured form, majority of banks knowledge is in unstructured or
minds of its employees. Organizations are needed to integrate KM with Knowledge
which is discovered from data and information. The purpose of this paper is to
discuss the need of business insiders in the process of knowledge discovery and
distribution, to make BI more relevant to business of the bank. We have also
discussed about the BI/KM applications in banking industry and provided a
framework to integrate BI and KM in banking industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0616</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0616</id><created>2011-09-03</created><updated>2012-10-09</updated><authors><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Rudnicki</keyname><forenames>Piotr</forenames></author><author><keyname>Sutcliffe</keyname><forenames>Geoff</forenames></author></authors><title>ATP and Presentation Service for Mizar Formalizations</title><categories>cs.DL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the Automated Reasoning for Mizar (MizAR) service, which
integrates several automated reasoning, artificial intelligence, and
presentation tools with Mizar and its authoring environment. The service
provides ATP assistance to Mizar authors in finding and explaining proofs, and
offers generation of Mizar problems as challenges to ATP systems. The service
is based on a sound translation from the Mizar language to that of first-order
ATP systems, and relies on the recent progress in application of ATP systems in
large theories containing tens of thousands of available facts. We present the
main features of MizAR services, followed by an account of initial experiments
in finding proofs with the ATP assistance. Our initial experience indicates
that the tool offers substantial help in exploring the Mizar library and in
preparing new Mizar articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0617</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0617</id><created>2011-09-03</created><authors><author><keyname>Komalavalli</keyname><forenames>C.</forenames><affiliation>Jagan Institute of Management Studies, Rohini, New Delhi</affiliation></author><author><keyname>Laroiya</keyname><forenames>Chetna</forenames><affiliation>Jagan Insitute of Management Studies, Rohini, New Delhi</affiliation></author></authors><title>Metadata Challenge for Query Processing Over Heterogeneous Wireless
  Sensor Network</title><categories>cs.DB</categories><comments>15 Pages</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  3, No. 4, August 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks become integral part of our life. These networks can
be used for monitoring the data in various domain due to their flexibility and
functionality. Query processing and optimization in the WSN is a very
challenging task because of their energy and memory constraint. In this paper,
first our focus is to review the different approaches that have significant
impacts on the development of query processing techniques for WSN. Finally, we
aim to illustrate the existing approach in popular query processing engines
with future research challenges in query optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0621</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0621</id><created>2011-09-03</created><authors><author><keyname>Kluza</keyname><forenames>Krzysztof</forenames></author><author><keyname>Nalepa</keyname><forenames>Grzegorz J.</forenames></author><author><keyname>&#x141;ysik</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Visual Inference Specification Methods for Modularized Rulebases.
  Overview and Integration Proposal</title><categories>cs.AI cs.SE</categories><comments>from the KESE6 workshop at the 33rd German AI Conference KI-2010 in
  Karlsruhe (see: http://ai.ia.agh.edu.pl/wiki/kese:kese6)</comments><acm-class>I.6.5; I.2.4; H.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper concerns selected rule modularization techniques. Three visual
methods for inference specification for modularized rule- bases are described:
Drools Flow, BPMN and XTT2. Drools Flow is a popular technology for workflow or
process modeling, BPMN is an OMG standard for modeling business processes, and
XTT2 is a hierarchical tab- ular system specification method. Because of some
limitations of these solutions, several proposals of their integration are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0624</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0624</id><created>2011-09-03</created><authors><author><keyname>Graja</keyname><forenames>Marwa</forenames></author><author><keyname>Jaoua</keyname><forenames>Maher</forenames></author><author><keyname>Belguith</keyname><forenames>Lamia Hadrich</forenames></author></authors><title>Building Ontologies to Understand Spoken Tunisian Dialect</title><categories>cs.CL</categories><comments>10 pages, 3 figures</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.1, No.4, August 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to understand spoken Tunisian dialect based on
lexical semantic. This method takes into account the specificity of the
Tunisian dialect which has no linguistic processing tools. This method is
ontology-based which allows exploiting the ontological concepts for semantic
annotation and ontological relations for speech interpretation. This
combination increases the rate of comprehension and limits the dependence on
linguistic resources. This paper also details the process of building the
ontology used for annotation and interpretation of Tunisian dialect in the
context of speech understanding in dialogue systems for restricted domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0628</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0628</id><created>2011-09-03</created><authors><author><keyname>Wang</keyname><forenames>Baocheng</forenames></author><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author><author><keyname>Xu</keyname><forenames>Maozhi</forenames></author></authors><title>The Weight Distributions of Cyclic Codes and Elliptic Curves</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes with two zeros and their dual codes as a practically and
theoretically interesting class of linear codes, have been studied for many
years. However, the weight distributions of cyclic codes are difficult to
determine. From elliptic curves, this paper determines the weight distributions
of dual codes of cyclic codes with two zeros for a few more cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0631</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0631</id><created>2011-09-03</created><updated>2012-04-17</updated><authors><author><keyname>Silva</keyname><forenames>Rosemberg</forenames></author><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Dahab</keyname><forenames>Ricardo</forenames></author></authors><title>LWE-based Identification Schemes</title><categories>cs.CR cs.IT math.IT</categories><comments>5 pages. Presented at the 2012 Information Theory Workshop (ITW)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some hard problems from lattices, like LWE (Learning with Errors), are
particularly suitable for application in Cryptography due to the possibility of
using worst-case to average-case reductions as evidence of strong security
properties. In this work, we show two LWE-based constructions of zero-knowledge
identification schemes and discuss their performance and security. We also
highlight the design choices that make our solution of both theoretical and
practical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0633</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0633</id><created>2011-09-03</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Eliciting implicit assumptions of proofs in the MIZAR Mathematical
  Library by property omission</title><categories>cs.LO cs.AI math.LO</categories><comments>11 pages, 3 tables. Preliminary version presented at the 3rd Workshop
  on Modules and Libraries for Proof Assistants (MLPA-11), affiliated with the
  2nd Conference on Interactive Theorem Proving (ITP-2011), Nijmegen, the
  Netherlands</comments><msc-class>03B35, 03A05</msc-class><acm-class>H.3.3</acm-class><doi>10.1007/s10817-012-9264-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When formalizing proofs with interactive theorem provers, it often happens
that extra background knowledge (declarative or procedural) about mathematical
concepts is employed without the formalizer explicitly invoking it, to help the
formalizer focus on the relevant details of the proof. In the contexts of
producing and studying a formalized mathematical argument, such mechanisms are
clearly valuable. But we may not always wish to suppress background knowledge.
For certain purposes, it is important to know, as far as possible, precisely
what background knowledge was implicitly employed in a formal proof. In this
note we describe an experiment conducted on the MIZAR Mathematical Library of
formal mathematical proofs to elicit one such class of implicitly employed
background knowledge: properties of functions and relations (e.g.,
commutativity, asymmetry, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0638</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0638</id><created>2011-09-03</created><authors><author><keyname>Umeda</keyname><forenames>Masanobu</forenames></author><author><keyname>Naruse</keyname><forenames>Ryoto</forenames></author><author><keyname>Sone</keyname><forenames>Hiroaki</forenames></author><author><keyname>Katamine</keyname><forenames>Keiichi</forenames></author></authors><title>Translating Nondeterministic Functional Language based on Attribute
  Grammars into Java</title><categories>cs.PL</categories><comments>13 pages, 8 figures, 2 tables, 19th International Conference on
  Applications of Declarative Programming and Knowledge Management (INAP2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge-based systems are suitable for realizing advanced functions that
require domain-specific expert knowledge, while knowledge representation
languages and their supporting environments are essential for realizing such
systems. Although Prolog is useful and effective in realizing such a supporting
environment, the language interoperability with other implementation languages,
such as Java, is often an important issue in practical application development.
This paper describes the techniques for translating a knowledge representation
language that is a nondeterministic functional language based on attribute
grammars into Java. The translation is based on binarization and the techniques
proposed for Prolog to Java translation although the semantics are different
from those of Prolog. A continuation unit is introduced to handle continuation
efficiently, while the variable and register management on backtracking is
simplified by using the single and unidirectional assignment features of
variables. An experimental translator written in the language itself
successfully generates Java code, while experimental results show that the
generated code is over 25 times faster than that of Prolog Cafe for
nondeterministic programs, and over 2 times faster for deterministic programs.
The generated code is also over 2 times faster than B-Prolog for
nondeterministic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0651</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0651</id><created>2011-09-03</created><authors><author><keyname>Bardhan</keyname><forenames>Jaydeep P.</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author></authors><title>Mathematical Analysis of the BIBEE Approximation for Molecular
  Solvation: Exact Results for Spherical Inclusions</title><categories>cs.CE physics.chem-ph physics.comp-ph</categories><comments>33 pages, 5 figures</comments><journal-ref>Journal of Chemical Physics, 135(12):124107-124117, 2011</journal-ref><doi>10.1063/1.3641485</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the mathematically rigorous BIBEE (boundary-integral based
electrostatics estimation) approximation of the mixed-dielectric continuum
model of molecular electrostatics, using the analytically solvable case of a
spherical solute containing an arbitrary charge distribution. Our analysis,
which builds on Kirkwood's solution using spherical harmonics, clarifies
important aspects of the approximation and its relationship to Generalized Born
models. First, our results suggest a new perspective for analyzing fast
electrostatic models: the separation of variables between material properties
(the dielectric constants) and geometry (the solute dielectric boundary and
charge distribution). Second, we find that the eigenfunctions of the
reaction-potential operator are exactly preserved in the BIBEE model for the
sphere, which supports the use of this approximation for analyzing
charge-charge interactions in molecular binding. Third, a comparison of BIBEE
to the recent GB$\epsilon$ theory suggests a modified BIBEE model capable of
predicting electrostatic solvation free energies to within 4% of a full
numerical Poisson calculation. This modified model leads to a
projection-framework understanding of BIBEE and suggests opportunities for
future improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0660</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0660</id><created>2011-09-03</created><updated>2011-09-16</updated><authors><author><keyname>Fannjiang</keyname><forenames>Albert</forenames></author><author><keyname>Liao</keyname><forenames>Wenjing</forenames></author></authors><title>Mismatch and resolution in compressive imaging</title><categories>cs.IT math.IT math.NA</categories><comments>Figure 5 revised</comments><doi>10.1117/12.892434</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly coherent sensing matrices arise in discretization of continuum
problems such as radar and medical imaging when the grid spacing is below the
Rayleigh threshold as well as in using highly coherent, redundant dictionaries
as sparsifying operators. Algorithms (BOMP, BLOOMP) based on techniques of band
exclusion and local optimization are proposed to enhance Orthogonal Matching
Pursuit (OMP) and deal with such coherent sensing matrices. BOMP and BLOOMP
have provably performance guarantee of reconstructing sparse, widely separated
objects {\em independent} of the redundancy and have a sparsity constraint and
computational cost similar to OMP's. Numerical study demonstrates the
effectiveness of BLOOMP for compressed sensing with highly coherent, redundant
sensing matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0675</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0675</id><created>2011-09-04</created><authors><author><keyname>Garimella</keyname><forenames>Rama Murthy</forenames></author><author><keyname>Abhijeet</keyname><forenames>Samdarshi</forenames></author><author><keyname>Singhal</keyname><forenames>Deepti</forenames></author></authors><title>Doubly Optimal Secure Multicasting: Hierarchical Hybrid Communication
  Network : Disaster Relief</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the world has witnessed the increasing occurrence of disasters,
some of natural origin and others caused by man. The intensity of the
phenomenon that cause such disasters, the frequency in which they occur, the
number of people affected and the material damage caused by them have been
growing substantially. Disasters are defined as natural, technological, and
human-initiated events that disrupt the normal functioning of the economy and
society on a large scale. Areas where disasters have occurred bring many
dangers to rescue teams and the communication network infrastructure is usually
destroyed. To manage these hazards, different wireless technologies can be
launched in the area of disaster. This paper discusses the innovative wireless
technologies for Disaster Management. Specifically, issues related to the
design of Hierarchical Hybrid Communication Network (arising in the
communication network for disaster relief) are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0680</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0680</id><created>2011-09-04</created><authors><author><keyname>Tsiavos</keyname><forenames>Prodromos</forenames><affiliation>London School of Economics, UK</affiliation></author><author><keyname>Stefaneas</keyname><forenames>Petros</forenames><affiliation>National Technical University of Athens, Greece</affiliation></author></authors><title>Beyond the Boundaries of Open, Closed and Pirate Archives: Lessons from
  a Hybrid Approach</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The creation of open archives i.e. archives where access is regulated by open
licensing models (content, source, data), should be seen as part of a broader
socio-economic phenomenon that finds legal expression in specific
organizational and technical formats.This paper examines the origins and main
characteristics of the open archives phenomenon. We investigate the extent to
which different models of production of economic or social value can be
expressed in different forms of licensing in the context of open archives.
Through this process, we assess the extent to which the digital archive is
moving towards providing access that is deeper (meaning, that offers more
access rights) and wider (in the sense that most of the information given is in
open content licensing) or face a gradual stratification and polarization of
the content. Such stratification entails the emergence of two types of content:
content to which access is extremely limited and content to which access
remains completely open. This differentiation between classes of content is the
result of multiple factors: from purely legislative, administrative and
contractual restrictions (e.g. data protection and confidentiality
restrictions) to information economics (e.g. peer production) or social
(minimum universal access).
  We claim that with respect to the access management model, most of the
current archiving processes include elements of openness. Usually, this is the
result of economic necessity expressed in licensing instruments or
organisational arrangements. The viability and the socio-economic importance of
the digital archives also contributes to the use of open archiving practices.
In such a context, although pure forms of open digital archives may remain an
ideal, the reality of hybrid open digital archives is a necessity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0681</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0681</id><created>2011-09-04</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Generic Optimization of Linear Precoding in Multibeam Satellite Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multibeam satellite systems have been employed to provide interactive
broadband services to geographical areas under-served by terrestrial
infrastructure. In this context, this paper studies joint multiuser linear
precoding design in the forward link of fixed multibeam satellite systems. We
provide a generic optimization framework for linear precoding design to handle
any objective functions of data rate with general linear and nonlinear power
constraints. To achieve this, an iterative algorithm which optimizes the
precoding vectors and power allocation alternatingly is proposed and most
importantly, the proposed algorithm is proved to always converge. The proposed
optimization algorithm is also applicable to nonlinear dirty paper coding. In
addition, the aforementioned problems and algorithms are extended to the case
that each terminal has multiple co-polarization or dual-polarization antennas.
Simulation results demonstrate substantial performance improvement of the
proposed schemes over conventional multibeam satellite systems, zero-forcing
and regularized zero-forcing precoding schemes in terms of meeting the traffic
demand. The performance of the proposed linear precoding scheme is also shown
to be very close to the dirty paper coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0686</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0686</id><created>2011-09-04</created><updated>2014-05-15</updated><authors><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Yao</keyname><forenames>Yong</forenames></author></authors><title>A Majorization Order on Monomials and Termination of a Successive
  Difference Substitution Algorithm</title><categories>cs.SC math.AC</categories><comments>6 pages</comments><msc-class>68T15 26D05</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce a majorization order on monomials. With the help of this order,
we derive a necessary condition on the positive termination of a general
successive difference substitution algorithm (KSDS) for an input form $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0687</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0687</id><created>2011-09-04</created><updated>2012-10-31</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Performance of distributed mechanisms for flow admission in wireless
  adhoc networks</title><categories>cs.IT cs.DC math.IT</categories><comments>21 pages, submitted. Journal version of arXiv:0906.3782</comments><journal-ref>Wireless Networks, vol. 20, pp. 1321-1334, August 2014</journal-ref><doi>10.1007/s11276-013-0680-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a wireless network where some pairs of communication links interfere
with each other, we study sufficient conditions for determining whether a given
set of minimum bandwidth quality-of-service (QoS) requirements can be
satisfied. We are especially interested in algorithms which have low
communication overhead and low processing complexity. The interference in the
network is modeled using a conflict graph whose vertices correspond to the
communication links in the network. Two links are adjacent in this graph if and
only if they interfere with each other due to being in the same vicinity and
hence cannot be simultaneously active. The problem of scheduling the
transmission of the various links is then essentially a fractional, weighted
vertex coloring problem, for which upper bounds on the fractional chromatic
number are sought using only localized information. We recall some distributed
algorithms for this problem, and then assess their worst-case performance. Our
results on this fundamental problem imply that for some well known classes of
networks and interference models, the performance of these distributed
algorithms is within a bounded factor away from that of an optimal, centralized
algorithm. The performance bounds are simple expressions in terms of graph
invariants. It is seen that the induced star number of a network plays an
important role in the design and performance of such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0689</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0689</id><created>2011-09-04</created><authors><author><keyname>Singh</keyname><forenames>Sandeep Pratap</forenames></author><author><keyname>Shukla</keyname><forenames>Shiv Shankar P.</forenames></author><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author><author><keyname>Tyagi</keyname><forenames>Vipin</forenames></author></authors><title>Problem Reduction in Online Payment System Using Hybrid Model</title><categories>cs.CR</categories><journal-ref>International Journal of Managing Information Technology(IJMIT)
  August 2011, Volume 3, Number 3 ISSN : 0975-5586 (Online) ;0975-5926 (Print)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0693</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0693</id><created>2011-09-04</created><authors><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Xie</keyname><forenames>Yan-Bo</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Transportation dynamics on networks of mobile agents</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 83, 016102 (2011)</journal-ref><doi>10.1103/PhysRevE.83.016102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing works on transportation dynamics focus on networks of a fixed
structure, but networks whose nodes are mobile have become widespread, such as
cell-phone networks. We introduce a model to explore the basic physics of
transportation on mobile networks. Of particular interest are the dependence of
the throughput on the speed of agent movement and communication range. Our
computations reveal a hierarchical dependence for the former while, for the
latter, we find an algebraic power law between the throughput and the
communication range with an exponent determined by the speed. We develop a
physical theory based on the Fokker-Planck equation to explain these phenomena.
Our findings provide insights into complex transportation dynamics arising
commonly in natural and engineering systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0696</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0696</id><created>2011-09-04</created><authors><author><keyname>Villard</keyname><forenames>Joffrey</forenames><affiliation>Shitz</affiliation></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Hybrid Digital/Analog Schemes for Secure Transmission with Side
  Information</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, 1 table. To be presented at ITW 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results on source-channel coding for secure transmission show that
separation holds in several cases under some less-noisy conditions. However, it
has also been proved through a simple counterexample that pure analog schemes
can be optimal and hence outperform digital ones. According to these
observations and assuming matched-bandwidth, we present a novel hybrid
digital/analog scheme that aims to gather the advantages of both digital and
analog ones. In the quadratic Gaussian setup when side information is only
present at the eavesdropper, this strategy is proved to be optimal.
Furthermore, it outperforms both digital and analog schemes and cannot be
achieved via time-sharing. An application example to binary symmetric sources
with side information is also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0697</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0697</id><created>2011-09-04</created><authors><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Wu</keyname><forenames>Zhi-Xi</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Traffic dynamics in scale-free networks with limited packet-delivering
  capacity</title><categories>cs.NI physics.soc-ph</categories><journal-ref>Physica A 387 (2008) 6857-6862</journal-ref><doi>10.1016/j.physa.2008.09.016</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a limited packet-delivering capacity model for traffic dynamics in
scale-free networks. In this model, the total node's packet-delivering capacity
is fixed, and the allocation of packet-delivering capacity on node $i$ is
proportional to $k_{i}^{\phi}$, where $k_{i}$ is the degree of node $i$ and
$\phi$ is a adjustable parameter. We have applied this model on the shortest
path routing strategy as well as the local routing strategy, and found that
there exists an optimal value of parameter $\phi$ leading to the maximal
network capacity under both routing strategies. We provide some explanations
for the emergence of optimal $\phi$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0708</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0708</id><created>2011-09-04</created><authors><author><keyname>Cherian</keyname><forenames>Annmol</forenames></author><author><keyname>Augustine</keyname><forenames>Ajay</forenames></author><author><keyname>Jose</keyname><forenames>Jemy</forenames></author><author><keyname>Pangracious</keyname><forenames>Vinod</forenames></author></authors><title>A Novel Methodology for Thermal Analysis &amp; 3-Dimensional Memory
  Integration</title><categories>cs.AR</categories><journal-ref>International Journal of Advanced Information Technology (IJAIT)
  Vol. 1, No. 4, August 2011</journal-ref><doi>10.5121/ijait.2011.1403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semiconductor industry is reaching a fascinating confluence in several
evolutionary trends that will likely lead to a number of revolutionary changes
in the design, implementation, scaling, and the use of computer systems.
However, recently Moore's law has come to a stand-still since device scaling
beyond 65 nm is not practical. 2D integration has problems like memory latency,
power dissipation, and large foot-print. 3D technology comes as a solution to
the problems posed by 2D integration. The utilization of 3D is limited by the
problem of temperature crisis. It is important to develop an accurate power
profile extraction methodology to design 3D structure. In this paper, design of
3D integration of memory is considered and hence the static power dissipation
of the memory cell is analysed in transistor level and is used to accurately
model the inter-layer thermal effects for 3D memory stack. Subsequently,
packaging of the chip is considered and modelled using an architecture level
simulator. This modelling is intended to analyse the thermal effects of 3D
memory, its reliability and lifetime of the chip, with greater accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0724</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0724</id><created>2011-09-04</created><updated>2012-05-18</updated><authors><author><keyname>Huang</keyname><forenames>Chuan</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Throughput Maximization for the Gaussian Relay Channel with Energy
  Harvesting Constraints</title><categories>cs.IT math.IT</categories><comments>appear in IEEE Journal on Selected Areas in Communications, special
  issue on theories and methods for advanced wireless relays</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the use of energy harvesters, instead of conventional
time-invariant energy sources, in wireless cooperative communication. For the
purpose of exposition, we study the classic three-node Gaussian relay channel
with decode-and-forward (DF) relaying, in which the source and relay nodes
transmit with power drawn from energy-harvesting (EH) sources. Assuming a
deterministic EH model under which the energy arrival time and the harvested
amount are known prior to transmission, the throughput maximization problem
over a finite horizon of $N$ transmission blocks is investigated. In
particular, two types of data traffic with different delay constraints are
considered: delay-constrained (DC) traffic (for which only one-block decoding
delay is allowed at the destination) and no-delay-constrained (NDC) traffic
(for which arbitrary decoding delay up to $N$ blocks is allowed). For the DC
case, we show that the joint source and relay power allocation over time is
necessary to achieve the maximum throughput, and propose an efficient algorithm
to compute the optimal power profiles. For the NDC case, although the
throughput maximization problem is non-convex, we prove the optimality of a
separation principle for the source and relay power allocation problems, based
upon which a two-stage power allocation algorithm is developed to obtain the
optimal source and relay power profiles separately. Furthermore, we compare the
DC and NDC cases, and obtain the sufficient and necessary conditions under
which the NDC case performs strictly better than the DC case. It is shown that
NDC transmission is able to exploit a new form of diversity arising from the
independent source and relay energy availability over time in cooperative
communication, termed &quot;energy diversity&quot;, even with time-invariant channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0732</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0732</id><created>2011-09-04</created><updated>2011-10-26</updated><authors><author><keyname>Lin</keyname><forenames>Feiyu</forenames></author><author><keyname>Krizhanovsky</keyname><forenames>Andrew</forenames></author></authors><title>Multilingual ontology matching based on Wiktionary data accessible via
  SPARQL endpoint</title><categories>cs.IR</categories><comments>8 pages, 3 tables, 4 figures, In: Proceedings of the 13th Russian
  Conference on Digital Libraries RCDL'2011. October 19-22, Voronezh, Russia. -
  pp. 19-26. (preprint)</comments><msc-class>68W25, 90C35</msc-class><acm-class>I.7.2; I.7.3; I.7.5; H.3.1; H.3.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Interoperability is a feature required by the Semantic Web. It is provided by
the ontology matching methods and algorithms. But now ontologies are presented
not only in English, but in other languages as well. It is important to use an
automatic translation for obtaining correct matching pairs in multilingual
ontology matching. The translation into many languages could be based on the
Google Translate API, the Wiktionary database, etc. From the point of view of
the balance of presence of many languages, of manually crafted translations, of
a huge size of a dictionary, the most promising resource is the Wiktionary. It
is a collaborative project working on the same principles as the Wikipedia. The
parser of the Wiktionary was developed and the machine-readable dictionary was
designed. The data of the machine-readable Wiktionary are stored in a
relational database, but with the help of D2R server the database is presented
as an RDF store. Thus, it is possible to get lexicographic information
(definitions, translations, synonyms) from web service using SPARQL requests.
In the case study, the problem entity is a task of multilingual ontology
matching based on Wiktionary data accessible via SPARQL endpoint. Ontology
matching results obtained using Wiktionary were compared with results based on
Google Translate API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0736</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0736</id><created>2011-09-04</created><authors><author><keyname>Kimura</keyname><forenames>Hideaki</forenames><affiliation>Brown University</affiliation></author><author><keyname>Narasayya</keyname><forenames>Vivek</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Syamala</keyname><forenames>Manoj</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>Compression Aware Physical Database Design</title><categories>cs.DB</categories><comments>VLDB2011</comments><proxy>uroehm</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern RDBMSs support the ability to compress data using methods such as null
suppression and dictionary encoding. Data compression offers the promise of
significantly reducing storage requirements and improving I/O performance for
decision support queries. However, compression can also slow down update and
query performance due to the CPU costs of compression and decompression. In
this paper, we study how data compression affects choice of appropriate
physical database design, such as indexes, for a given workload. We observe
that approaches that decouple the decision of whether or not to choose an index
from whether or not to compress the index can result in poor solutions. Thus,
we focus on the novel problem of integrating compression into physical database
design in a scalable manner. We have implemented our techniques by modifying
Microsoft SQL Server and the Database Engine Tuning Advisor (DTA) physical
design tool. Our techniques are general and are potentially applicable to DBMSs
that support other compression methods. Our experimental results on real world
as well as TPC-H benchmark workloads demonstrate the effectiveness of our
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0742</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0742</id><created>2011-09-04</created><authors><author><keyname>Cloud</keyname><forenames>Robert Louis</forenames></author></authors><title>Problems in Modern High Performance Parallel I/O Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past couple of decades, the computational abilities of supercomput-
ers have increased tremendously. Leadership scale supercomputers now are
capable of petaflops. Likewise, the problem size targeted by applications
running on such computers has also scaled. These large applications have I/O
throughput requirements on the order of tens of gigabytes per second. For a
variety of reasons, the I/O subsystems of such computers have not kept pace
with the computational increases, and the time required for I/O in an
application has become one of the dominant bottlenecks. Also troublesome is the
fact that scientific applications do not attain near the peak theoretical
bandwidth of the I/O subsystems. In addressing the two prior issues, one must
also question the nature of the data itself; one can ask whether contem- porary
practices of data dumping and analysis are optimal and whether they will
continue to be applicable as computers continue to scale. These three topics,
the I/O subsystem, the nature of scientific data output, and future possible
optimizations are discussed in this report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0752</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0752</id><created>2011-09-04</created><authors><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Gu</keyname><forenames>Huaxi</forenames></author><author><keyname>Yang</keyname><forenames>Yintang</forenames></author></authors><title>An improved distributed routing algorithm for Benes based optical NoC</title><categories>cs.AR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrated optical interconnect is believed to be one of the main
technologies to replace electrical wires. Optical Network-on-Chip (ONoC) has
attracted more attentions nowadays. Benes topology is a good choice for ONoC
for its rearrangeable non-blocking character, multistage feature and easy
scalability. Routing algorithm plays an important role in determining the
performance of ONoC. But traditional routing algorithms for Benes network are
not suitable for ONoC communication, we developed a new distributed routing
algorithm for Benes ONoC in this paper. Our algorithm selected the routing path
dynamically according to network condition and enables more path choices for
the message traveling in the network. We used OPNET to evaluate the performance
of our routing algorithm and also compared it with a well-known bit-controlled
routing algorithm. ETE delay and throughput were showed under different packet
length and network sizes. Simulation results show that our routing algorithm
can provide better performance for ONoC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0753</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0753</id><created>2011-09-04</created><authors><author><keyname>Shah</keyname><forenames>Kinjal</forenames></author><author><keyname>Dua</keyname><forenames>Gagan</forenames></author><author><keyname>Sharma</keyname><forenames>Dharmendar</forenames></author><author><keyname>Mishra</keyname><forenames>Priyanka</forenames></author><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author></authors><title>Transmission of Successful Route Error Message(RERR) in Routing Aware
  Multiple Description Video Coding over Mobile Ad-Hoc Network</title><categories>cs.MM</categories><comments>9 pages,2 figures, 1 table for algorithm</comments><doi>10.5121/ijma.2011.3305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video transmission over mobile ad-hoc networks is becoming important as these
networks become more widely used in the wireless networks. We propose a
routing-aware multiple description video coding approach to support video
transmission over mobile ad-hoc networks with single and multiple path
transport. We build a model to estimate the packet loss probability of each
packet transmitted over the network based on the standard ad-hoc routing
messages and network parameters without losing the RERR message. We then
calculate the frame loss probability in order to eliminate error without any
loss of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0755</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0755</id><created>2011-09-04</created><authors><author><keyname>Xie</keyname><forenames>Peibo</forenames></author><author><keyname>Gu</keyname><forenames>Huaxi</forenames></author></authors><title>Intelligent Bees for QoS Routing in Networks-on-Chip</title><categories>cs.AR</categories><comments>4 pages, 4 figures, IEEE 2010 Second Pacific-Asia Conference on
  Circuits, Communications and System (PACCS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks-on-Chip (NoCs) for future many-core processor platforms integrate
more and more heterogeneous components of different types and many real-time
and latency-sensitive applications can run on a single chip concurrently. The
reconfigurable FPGA and reconfigurable NoCs have emerged for the purpose of
reusability. Those types' traffics within NoCs exhibit diverse, burst, and
unpredictable communication patterns. QoS guaranteed mechanisms are necessary
to provide guaranteed throughput (GT) or guaranteed bandwidth (GB) performance
for NoCs. In this paper, we propose a QoS routing algorithm inspired by bees'
foraging behaviors to provide guaranteed bandwidth performance. Virtual
circuits and Spatial Division Multiplexing are employed to maintain available
paths for different type's traffics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0758</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0758</id><created>2011-09-04</created><authors><author><keyname>Ye</keyname><forenames>Mao</forenames></author><author><keyname>Liu</keyname><forenames>Xingjie</forenames></author><author><keyname>Lee</keyname><forenames>Wang-Chien</forenames></author></authors><title>Exploring Social Influence for Recommendation - A Probabilistic
  Generative Model Approach</title><categories>cs.SI cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a probabilistic generative model, called unified
model, which naturally unifies the ideas of social influence, collaborative
filtering and content-based methods for item recommendation. To address the
issue of hidden social influence, we devise new algorithms to learn the model
parameters of our proposal based on expectation maximization (EM). In addition
to a single-machine version of our EM algorithm, we further devise a
parallelized implementation on the Map-Reduce framework to process two
large-scale datasets we collect. Moreover, we show that the social influence
obtained from our generative models can be used for group recommendation.
Finally, we conduct comprehensive experiments using the datasets crawled from
last.fm and whrrl.com to validate our ideas. Experimental results show that the
generative models with social influence significantly outperform those without
incorporating social influence. The unified generative model proposed in this
paper obtains the best performance. Moreover, our study on social influence
finds that users in whrrl.com are more likely to get influenced by friends than
those in last.fm. The experimental results also confirm that our social
influence based group recommendation algorithm outperforms the state-of-the-art
algorithms for group recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0762</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0762</id><created>2011-09-04</created><authors><author><keyname>Ni</keyname><forenames>Nan</forenames></author></authors><title>Tunable Dual-band IFA Antenna using LC Resonators</title><categories>cs.IT math.IT</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tunable dual-band inverted F antenna (IFA) is presented in this paper. By
placing a LC resonator on the radiating arm, dual-band characteristic is
achieved. Especially, the capacitor in the resonator is a tunable thin-film BST
capacitor, which has a 3.3:1 tuning ratio. The capacitance of the BST
capacitors can be tuned by an external DC bias voltage. By varying the
capacitance, both the lower band and the upper band of the IFA antenna can be
tuned. And the total bandwidth can cover six systems, i.e., GSM-850, GSM-900,
GPS, DCS, PCS, and UMTS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0766</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0766</id><created>2011-09-04</created><updated>2011-09-06</updated><authors><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Xu</keyname><forenames>Kaihe</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author></authors><title>Cooperative Secret Key Generation from Phase Estimation in Narrowband
  Fading Channels</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By exploiting multipath fading channels as a source of common randomness,
physical layer (PHY) based key generation protocols allow two terminals with
correlated observations to generate secret keys with information-theoretical
security. The state of the art, however, still suffers from major limitations,
e.g., low key generation rate, lower entropy of key bits and a high reliance on
node mobility. In this paper, a novel cooperative key generation protocol is
developed to facilitate high-rate key generation in narrowband fading channels,
where two keying nodes extract the phase randomness of the fading channel with
the aid of relay node(s). For the first time, we explicitly consider the effect
of estimation methods on the extraction of secret key bits from the underlying
fading channels and focus on a popular statistical method--maximum likelihood
estimation (MLE). The performance of the cooperative key generation scheme is
extensively evaluated theoretically. We successfully establish both a
theoretical upper bound on the maximum secret key rate from mutual information
of correlated random sources and a more practical upper bound from Cramer-Rao
bound (CRB) in estimation theory. Numerical examples and simulation studies are
also presented to demonstrate the performance of the cooperative key generation
system. The results show that the key rate can be improved by a couple of
orders of magnitude compared to the existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0774</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0774</id><created>2011-09-04</created><authors><author><keyname>Bauer</keyname><forenames>Tim</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Erwig</keyname><forenames>Martin</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Fern</keyname><forenames>Alan</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Pinto</keyname><forenames>Jervis</forenames><affiliation>Oregon State University</affiliation></author></authors><title>Adaptation-Based Programming in Haskell</title><categories>cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><acm-class>D.3.3</acm-class><journal-ref>EPTCS 66, 2011, pp. 1-23</journal-ref><doi>10.4204/EPTCS.66.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an embedded DSL to support adaptation-based programming (ABP) in
Haskell. ABP is an abstract model for defining adaptive values, called
adaptives, which adapt in response to some associated feedback. We show how our
design choices in Haskell motivate higher-level combinators and constructs and
help us derive more complicated compositional adaptives.
  We also show an important specialization of ABP is in support of
reinforcement learning constructs, which optimize adaptive values based on a
programmer-specified objective function. This permits ABP users to easily
define adaptive values that express uncertainty anywhere in their programs.
Over repeated executions, these adaptive values adjust to more efficient ones
and enable the user's programs to self optimize.
  The design of our DSL depends significantly on the use of type classes. We
will illustrate, along with presenting our DSL, how the use of type classes can
support the gradual evolution of DSLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0775</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0775</id><created>2011-09-04</created><authors><author><keyname>Bestavros</keyname><forenames>Azer</forenames><affiliation>Boston University</affiliation></author><author><keyname>Kfoury</keyname><forenames>Assaf</forenames><affiliation>Boston University</affiliation></author></authors><title>A Domain-Specific Language for Incremental and Modular Design of
  Large-Scale Verifiably-Safe Flow Networks (Preliminary Report)</title><categories>cs.PL cs.DC cs.LO cs.SE</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 24-47</journal-ref><doi>10.4204/EPTCS.66.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a domain-specific language (DSL) to inductively assemble flow
networks from small networks or modules to produce arbitrarily large ones, with
interchangeable functionally-equivalent parts. Our small networks or modules
are &quot;small&quot; only as the building blocks in this inductive definition (there is
no limit on their size). Associated with our DSL is a type theory, a system of
formal annotations to express desirable properties of flow networks together
with rules that enforce them as invariants across their interfaces, i.e, the
rules guarantee the properties are preserved as we build larger networks from
smaller ones. A prerequisite for a type theory is a formal semantics, i.e, a
rigorous definition of the entities that qualify as feasible flows through the
networks, possibly restricted to satisfy additional efficiency or safety
requirements. This can be carried out in one of two ways, as a denotational
semantics or as an operational (or reduction) semantics; we choose the first in
preference to the second, partly to avoid exponential-growth rewriting in the
operational approach. We set up a typing system and prove its soundness for our
DSL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0776</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0776</id><created>2011-09-04</created><authors><author><keyname>Beyak</keyname><forenames>Lucas</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Carette</keyname><forenames>Jacques</forenames><affiliation>McMaster University</affiliation></author></authors><title>SAGA: A DSL for Story Management</title><categories>cs.PL cs.MM cs.SE</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 48-67</journal-ref><doi>10.4204/EPTCS.66.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video game development is currently a very labour-intensive endeavour.
Furthermore it involves multi-disciplinary teams of artistic content creators
and programmers, whose typical working patterns are not easily meshed. SAGA is
our first effort at augmenting the productivity of such teams.
  Already convinced of the benefits of DSLs, we set out to analyze the domains
present in games in order to find out which would be most amenable to the DSL
approach. Based on previous work, we thus sought those sub-parts that already
had a partially established vocabulary and at the same time could be well
modeled using classical computer science structures. We settled on the 'story'
aspect of video games as the best candidate domain, which can be modeled using
state transition systems.
  As we are working with a specific company as the ultimate customer for this
work, an additional requirement was that our DSL should produce code that can
be used within a pre-existing framework. We developed a full system (SAGA)
comprised of a parser for a human-friendly language for 'story events', an
internal representation of design patterns for implementing object-oriented
state-transitions systems, an instantiator for these patterns for a specific
'story', and three renderers (for C++, C# and Java) for the instantiated
abstract code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0777</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0777</id><created>2011-09-04</created><authors><author><keyname>Orchard</keyname><forenames>Dominic</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author><author><keyname>Mycroft</keyname><forenames>Alan</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>Efficient and Correct Stencil Computation via Pattern Matching and
  Static Typing</title><categories>cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 68-92</journal-ref><doi>10.4204/EPTCS.66.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stencil computations, involving operations over the elements of an array, are
a common programming pattern in scientific computing, games, and image
processing. As a programming pattern, stencil computations are highly regular
and amenable to optimisation and parallelisation. However, general-purpose
languages obscure this regular pattern from the compiler, and even the
programmer, preventing optimisation and obfuscating (in)correctness. This paper
furthers our work on the Ypnos domain-specific language for stencil
computations embedded in Haskell. Ypnos allows declarative, abstract
specification of stencil computations, exposing the structure of a problem to
the compiler and to the programmer via specialised syntax. In this paper we
show the decidable safety guarantee that well-formed, well-typed Ypnos programs
cannot index outside of array boundaries. Thus indexing in Ypnos is safe and
run-time bounds checking can be eliminated. Program information is encoded as
types, using the advanced type-system features of the Glasgow Haskell Compiler,
with the safe-indexing invariant enforced at compile time via type checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0778</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0778</id><created>2011-09-04</created><authors><author><keyname>Rompf</keyname><forenames>Tiark</forenames><affiliation>EPFL</affiliation></author><author><keyname>Sujeeth</keyname><forenames>Arvind K.</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Lee</keyname><forenames>HyoukJoong</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Brown</keyname><forenames>Kevin J.</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Chafi</keyname><forenames>Hassan</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Odersky</keyname><forenames>Martin</forenames><affiliation>EPFL</affiliation></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames><affiliation>Stanford University</affiliation></author></authors><title>Building-Blocks for Performance Oriented DSLs</title><categories>cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 93-117</journal-ref><doi>10.4204/EPTCS.66.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain-specific languages raise the level of abstraction in software
development. While it is evident that programmers can more easily reason about
very high-level programs, the same holds for compilers only if the compiler has
an accurate model of the application domain and the underlying target platform.
Since mapping high-level, general-purpose languages to modern, heterogeneous
hardware is becoming increasingly difficult, DSLs are an attractive way to
capitalize on improved hardware performance, precisely by making the compiler
reason on a higher level. Implementing efficient DSL compilers is a daunting
task however, and support for building performance-oriented DSLs is urgently
needed. To this end, we present the Delite Framework, an extensible toolkit
that drastically simplifies building embedded DSLs and compiling DSL programs
for execution on heterogeneous hardware. We discuss several building blocks in
some detail and present experimental results for the OptiML machine-learning
DSL implemented on top of Delite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0779</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0779</id><created>2011-09-04</created><authors><author><keyname>Starynkevitch</keyname><forenames>Basile</forenames><affiliation>CEA, LIST</affiliation></author></authors><title>MELT - a Translated Domain Specific Language Embedded in the GCC
  Compiler</title><categories>cs.PL cs.SE</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><acm-class>D.3.4; D.1.1; D.2.4; D.3.2</acm-class><journal-ref>EPTCS 66, 2011, pp. 118-142</journal-ref><doi>10.4204/EPTCS.66.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GCC free compiler is a very large software, compiling source in several
languages for many targets on various systems. It can be extended by plugins,
which may take advantage of its power to provide extra specific functionality
(warnings, optimizations, source refactoring or navigation) by processing
various GCC internal representations (Gimple, Tree, ...). Writing plugins in C
is a complex and time-consuming task, but customizing GCC by using an existing
scripting language inside is impractical. We describe MELT, a specific
Lisp-like DSL which fits well into existing GCC technology and offers
high-level features (functional, object or reflexive programming, pattern
matching). MELT is translated to C fitted for GCC internals and provides
various features to facilitate this. This work shows that even huge, legacy,
software can be a posteriori extended by specifically tailored and translated
high-level DSLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0780</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0780</id><created>2011-09-04</created><authors><author><keyname>Walkingshaw</keyname><forenames>Eric</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Erwig</keyname><forenames>Martin</forenames><affiliation>Oregon State University</affiliation></author></authors><title>A DSEL for Studying and Explaining Causation</title><categories>cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 143-167</journal-ref><doi>10.4204/EPTCS.66.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a domain-specific embedded language (DSEL) in Haskell that
supports the philosophical study and practical explanation of causation. The
language provides constructs for modeling situations comprised of events and
functions for reliably determining the complex causal relationships that emerge
between these events. It enables the creation of visual explanations of these
causal relationships and a means to systematically generate alternative,
related scenarios, along with corresponding outcomes and causes. The DSEL is
based on neuron diagrams, a visual notation that is well established in
practice and has been successfully employed for causation explanation and
research. In addition to its immediate applicability by users of neuron
diagrams, the DSEL is extensible, allowing causation experts to extend the
notation to introduce special-purpose causation constructs. The DSEL also
extends the notation of neuron diagrams to operate over non-boolean values,
improving its expressiveness and offering new possibilities for causation
research and its applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0781</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0781</id><created>2011-09-04</created><authors><author><keyname>Cook</keyname><forenames>William R.</forenames><affiliation>University of Texas at Austin</affiliation></author><author><keyname>L&#xe4;mmel</keyname><forenames>Ralf</forenames><affiliation>University of Koblenz-Landau</affiliation></author></authors><title>Tutorial on Online Partial Evaluation</title><categories>cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 168-180</journal-ref><doi>10.4204/EPTCS.66.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a short tutorial introduction to online partial evaluation. We
show how to write a simple online partial evaluator for a simple, pure,
first-order, functional programming language. In particular, we show that the
partial evaluator can be derived as a variation on a compositionally defined
interpreter. We demonstrate the use of the resulting partial evaluator for
program optimization in the context of model-driven development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0782</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0782</id><created>2011-09-04</created><updated>2011-11-18</updated><authors><author><keyname>Gibbons</keyname><forenames>Jeremy</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Maximum Segment Sum, Monadically (distilled tutorial, with solutions)</title><categories>cs.DS cs.DM cs.PL</categories><comments>Revision of the article in Proceedings DSL 2011, EPTCS 66,
  arXiv:1109.0323, to provide solutions to the exercises</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 181-194</journal-ref><doi>10.4204/EPTCS.66.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum segment sum problem is to compute, given a list of integers, the
largest of the sums of the contiguous segments of that list. This problem
specification maps directly onto a cubic-time algorithm; however, there is a
very elegant linear-time solution too. The problem is a classic exercise in the
mathematics of program construction, illustrating important principles such as
calculational development, pointfree reasoning, algebraic structure, and
datatype-genericity. Here, we take a sideways look at the datatype-generic
version of the problem in terms of monadic functional programming, instead of
the traditional relational approach; the presentation is tutorial in style, and
leavened with exercises for the reader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0783</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0783</id><created>2011-09-04</created><authors><author><keyname>Karczmarczuk</keyname><forenames>Jerzy</forenames><affiliation>University of Caen, France</affiliation></author></authors><title>Specific &quot;scientific&quot; data structures, and their processing</title><categories>cs.DS cs.MS cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 195-209</journal-ref><doi>10.4204/EPTCS.66.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming physicists use, as all programmers, arrays, lists, tuples,
records, etc., and this requires some change in their thought patterns while
converting their formulae into some code, since the &quot;data structures&quot; operated
upon, while elaborating some theory and its consequences, are rather: power
series and Pad\'e approximants, differential forms and other instances of
differential algebras, functionals (for the variational calculus), trajectories
(solutions of differential equations), Young diagrams and Feynman graphs, etc.
Such data is often used in a [semi-]numerical setting, not necessarily
&quot;symbolic&quot;, appropriate for the computer algebra packages. Modules adapted to
such data may be &quot;just libraries&quot;, but often they become specific, embedded
sub-languages, typically mapped into object-oriented frameworks, with
overloaded mathematical operations. Here we present a functional approach to
this philosophy. We show how the usage of Haskell datatypes and - fundamental
for our tutorial - the application of lazy evaluation makes it possible to
operate upon such data (in particular: the &quot;infinite&quot; sequences) in a natural
and comfortable manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0784</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0784</id><created>2011-09-04</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author></authors><title>Implementing Explicit and Finding Implicit Sharing in Embedded DSLs</title><categories>cs.PL cs.DS</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 210-225</journal-ref><doi>10.4204/EPTCS.66.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aliasing, or sharing, is prominent in many domains, denoting that two
differently-named objects are in fact identical: a change in one object (memory
cell, circuit terminal, disk block) is instantly reflected in the other.
Languages for modelling such domains should let the programmer explicitly
define the sharing among objects or expressions. A DSL compiler may find other
identical expressions and share them, implicitly. Such common subexpression
elimination is crucial to the efficient implementation of DSLs. Sharing is
tricky in embedded DSL, since host aliasing may correspond to copying of the
underlying objects rather than their sharing.
  This tutorial summarizes discussions of implementing sharing in Haskell DSLs
for automotive embedded systems and hardware description languages. The
technique has since been used in a Haskell SAT solver and the DSL for music
synthesis. We demonstrate the embedding in pure Haskell of a simple DSL with a
language form for explicit sharing. The DSL also has implicit sharing,
implemented via hash-consing. Explicit sharing greatly speeds up hash-consing.
The seemingly imperative nature of hash-consing is hidden beneath a simple
combinator language. The overall implementation remains pure functional and
easy to reason about.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0785</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0785</id><created>2011-09-04</created><authors><author><keyname>Nakata</keyname><forenames>Keiko</forenames><affiliation>Institute of Cybernetics at Tallinn University of Technology</affiliation></author></authors><title>Resumption-based big-step and small-step interpreters for While with
  interactive I/O</title><categories>cs.PL</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 226-235</journal-ref><doi>10.4204/EPTCS.66.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial, we program big-step and small-step total interpreters for
the While language extended with input and output primitives. While is a simple
imperative language consisting of skip, assignment, sequence, conditional and
loop. We first develop trace-based interpreters for While. Traces are
potentially infinite nonempty sequences of states. The interpreters assign
traces to While programs: for us, traces are denotations of While programs. The
trace is finite if the program is terminating and infinite if the program is
non-terminating. However, we cannot decide (i.e., write a program to
determine), for any given program, whether its trace is finite or infinite,
which amounts to deciding the halting problem. We then extend While with
interactive input/output primitives. Accordingly, we extend the interpreters by
generalizing traces to resumptions.
  The tutorial is based on our previous work with T. Uustalu on reasoning about
interactive programs in the setting of constructive type theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0786</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0786</id><created>2011-09-04</created><authors><author><keyname>Taha</keyname><forenames>Walid</forenames><affiliation>Halmstad University</affiliation></author><author><keyname>Gaspes</keyname><forenames>Veronica</forenames><affiliation>Halmstad University</affiliation></author><author><keyname>Page</keyname><forenames>Rex</forenames><affiliation>University of Oklahoma</affiliation></author></authors><title>Accurate Programming: Thinking about programs in terms of properties</title><categories>cs.PL cs.SE</categories><comments>In Proceedings DSL 2011, arXiv:1109.0323</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 66, 2011, pp. 236-260</journal-ref><doi>10.4204/EPTCS.66.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate programming is a practical approach to producing high quality
programs. It combines ideas from test-automation, test-driven development,
agile programming, and other state of the art software development methods. In
addition to building on approaches that have proven effective in practice, it
emphasizes concepts that help programmers sharpen their understanding of both
the problems they are solving and the solutions they come up with. This is
achieved by encouraging programmers to think about programs in terms of
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0787</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0787</id><created>2011-09-04</created><authors><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Tanigawa</keyname><forenames>Shin-ichi</forenames></author></authors><title>Rooted-tree Decompositions with Matroid Constraints and the
  Infinitesimal Rigidity of Frameworks with Boundaries</title><categories>math.CO cs.DM math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an extension of a classical tree-partition problem, we consider
decompositions of graphs into edge-disjoint (rooted-)trees with an additional
matroid constraint. Specifically, suppose we are given a graph $G=(V,E)$, a
multiset $R=\{r1,..., r_t\}$ of vertices in $V$, and a matroid ${\cal M}$ on
$R$. We prove a necessary and sufficient condition for $G$ to be decomposed
into $t$ edge-disjoint subgraphs $G_1=(V_1,T_1),..., G_t=(V_t,T_t)$ such that
(i) for each $i$, $G_i$ is a tree with $r_i\in V_i$, and (ii) for each $v\in
V$, the multiset $\{r_i\in R\mid v\in V_i\}$ is a base of ${\cal M}$. If ${\cal
M}$ is a free matroid, this is a decomposition into $t$ edge-disjoint spanning
trees; thus, our result is a proper extension of Nash-Williams' tree-partition
theorem.
  Such a matroid constraint is motivated by combinatorial rigidity theory. As a
direct application of our decomposition theorem, we present characterizations
of the infinitesimal rigidity of frameworks with non-generic &quot;boundary&quot;, which
extend classical Laman's theorem for generic 2-rigidity of bar-joint frameworks
and Tay's theorem for generic $d$-rigidity of body-bar frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0792</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0792</id><created>2011-09-04</created><authors><author><keyname>Tam</keyname><forenames>Adrian Sai-wah</forenames></author><author><keyname>Xi</keyname><forenames>Kang</forenames></author><author><keyname>Chao</keyname><forenames>H. Jonathan</forenames></author></authors><title>Trimming the Multipath for Efficient Dynamic Routing</title><categories>cs.NI math.OC</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipath routing is a trivial way to exploit the path diversity to leverage
the network throughput. Technologies such as OSPF ECMP use all the available
paths in the network to forward traffic, however, we argue that is not
necessary to do so to load balance the network. In this paper, we consider
multipath routing with only a limited number of end-to-end paths for each
source and destination, and found that this can still load balance the traffic.
We devised an algorithm to select a few paths for each source-destination pair
so that when all traffic are forwarded over these paths, we can achieve a
balanced load in the sense that the maximum link utilization is comparable to
that of ECMP forwarding. When the constraint of only shortest paths (i.e. equal
paths) are relaxed, we can even outperform ECMP in certain cases. As a result,
we can use a few end-to-end tunnels between each source and destination nodes
to achieve the load balancing of traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0800</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0800</id><created>2011-09-05</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Quantized Compute and Forward: A Low-Complexity Architecture for
  Distributed Antenna Systems</title><categories>cs.IT math.IT</categories><comments>To appear in 2011 IEEE Information Theory Workshop (ITW 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a low-complexity version of the Compute and Forward scheme that
involves only scaling, offset (dithering removal) and scalar quantization at
the relays. The proposed scheme is suited for the uplink of a distributed
antenna system where the antenna elements must be very simple and are connected
to a oint processor via orthogonal perfect links of given rate R0. We consider
the design of non-binary LDPC codes naturally matched to the proposed scheme.
Each antenna element performs individual (decentralized) Belief Propagation
decoding of its own quantized signal, and sends a linear combination of the
users' information messages via the noiseless link to the joint processor,
which retrieves the users' messages by Gaussian elimination. The complexity of
this scheme is linear in the coding block length and polynomial in the system
size (number of relays).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0802</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0802</id><created>2011-09-05</created><authors><author><keyname>Sen</keyname><forenames>Pranab</forenames></author></authors><title>Achieving the Han-Kobayashi inner bound for the quantum interference
  channel by sequential decoding</title><categories>quant-ph cs.IT math.IT</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the power of sequential decoding strategies for
several channels with classical input and quantum output. In our sequential
decoding strategies, the receiver loops through all candidate messages trying
to project the received state onto a `typical' subspace for the candidate
message under consideration, stopping if the projection succeeds for a message,
which is then declared as the guess of the receiver for the sent message. We
show that even such a conceptually simple strategy can be used to achieve rates
up to the mutual information for a single sender single receiver channel called
cq-channel henceforth, as well as the standard inner bound for a two sender
single receiver multiple access channel, called ccq-MAC in this paper. Our
decoding scheme for the ccq-MAC uses a new kind of conditionally typical
projector which is constructed using a geometric result about how two subspaces
interact structurally.
  As the main application of our methods, we construct an encoding and decoding
scheme achieving the Chong-Motani-Garg inner bound for a two sender two
receiver interference channel with classical input and quantum output, called
ccqq-IC henceforth. This matches the best known inner bound for the
interference channel in the classical setting. Achieving the Chong-Motani-Garg
inner bound, which is known to be equivalent to the Han-Kobayashi inner bound,
answers an open question raised recently by Fawzi et al. (arxiv:1102.2624). Our
encoding scheme is the same as that of Chong-Motani-Garg, and our decoding
scheme is sequential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0807</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0807</id><created>2011-09-05</created><updated>2013-05-21</updated><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>Schober</keyname><forenames>Steffen</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author></authors><title>Harmonic Analysis of Boolean Networks: Determinative Power and
  Perturbations</title><categories>cs.IT cond-mat.dis-nn math.IT q-bio.MN</categories><journal-ref>EURASIP Journal on Bioinformatics and Systems Biology 2013, 2013:6</journal-ref><doi>10.1186/1687-4153-2013-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a large Boolean network with a feed forward structure. Given a
probability distribution on the inputs, can one find, possibly small,
collections of input nodes that determine the states of most other nodes in the
network? To answer this question, a notion that quantifies the determinative
power of an input over the states of the nodes in the network is needed. We
argue that the mutual information (MI) between a given subset of the inputs X =
{X_1, ..., X_n} of some node i and its associated function f_i(X) quantifies
the determinative power of this set of inputs over node i. We compare the
determinative power of a set of inputs to the sensitivity to perturbations to
these inputs, and find that, maybe surprisingly, an input that has large
sensitivity to perturbations does not necessarily have large determinative
power. However, for unate functions, which play an important role in genetic
regulatory networks, we find a direct relation between MI and sensitivity to
perturbations. As an application of our results, we analyze the large-scale
regulatory network of Escherichia coli. We identify the most determinative
nodes and show that a small subset of those reduces the overall uncertainty of
the network state significantly. Furthermore, the network is found to be
tolerant to perturbations of its inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0820</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0820</id><created>2011-09-05</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Wexler</keyname><forenames>Yonatan</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>ShareBoost: Efficient Multiclass Learning with Feature Sharing</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiclass prediction is the problem of classifying an object into a relevant
target class. We consider the problem of learning a multiclass predictor that
uses only few features, and in particular, the number of used features should
increase sub-linearly with the number of possible classes. This implies that
features should be shared by several classes. We describe and analyze the
ShareBoost algorithm for learning a multiclass predictor that uses few shared
features. We prove that ShareBoost efficiently finds a predictor that uses few
shared features (if such a predictor exists) and that it has a small
generalization error. We also describe how to use ShareBoost for learning a
non-linear predictor that has a fast evaluation time. In a series of
experiments with natural data sets we demonstrate the benefits of ShareBoost
and evaluate its success relatively to other state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0825</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0825</id><created>2011-09-05</created><updated>2012-07-02</updated><authors><author><keyname>Formenti</keyname><forenames>E.</forenames></author><author><keyname>Pham</keyname><forenames>V. T.</forenames></author><author><keyname>Phan</keyname><forenames>H. D.</forenames></author><author><keyname>Tran</keyname><forenames>T. T. H.</forenames></author></authors><title>Fixed point forms of the parallel symmetric sandpile model</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a generalization of the sandpile model, called the
parallel symmetric sandpile model, which inherits the rules of the symmetric
sandpile model and implements them in parallel. In this new model, at each step
the collapsing of the collapsible columns happens at the same time and one
collapsible column is able to collapse on the left or on the right but not
both. We prove that the set of forms of fixed points of the symmetric sandpile
model is the same as the one of that model using parallel update scheme by
constructing explicitly the way (in the parallel update scheme) to reach the
form of an arbitrary fixed point of the sequential model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0827</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0827</id><created>2011-09-05</created><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>A Trellis Coded Modulation Scheme for the Fading Relay Channel</title><categories>cs.IT math.IT</categories><comments>18 pages, 17 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A decode and forward protocol based Trellis Coded Modulation (TCM) scheme for
the half-duplex relay channel, in a Rayleigh fading environment, is presented.
The proposed scheme can achieve any spectral efficiency greater than or equal
to one bit per channel use (bpcu). A near-ML decoder for the suggested TCM
scheme is proposed. It is shown that the high SNR performance of this near-ML
decoder approaches the performance of the optimal ML decoder. The high SNR
performance of this near-ML decoder is independent of the strength of the
Source-Relay link and approaches the performance of the optimal ML decoder with
an ideal Source-Relay link. Based on the derived Pair-wise Error Probability
(PEP) bounds, design criteria to maximize the diversity and coding gains are
obtained. Simulation results show a large gain in SNR for the proposed TCM
scheme over uncoded communication as well as the direct transmission without
the relay. Also, it is shown that even for the uncoded transmission scheme, the
choice of the labelling scheme (mapping from bits to complex symbols) used at
the source and the relay significantly impacts the BER vs SNR performance. We
provide a good labelling scheme for $2^l$-PSK signal set, where $l\geq 2$ is an
integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0839</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0839</id><created>2011-09-05</created><authors><author><keyname>Agliari</keyname><forenames>Elena</forenames></author><author><keyname>Cioli</keyname><forenames>Claudia</forenames></author><author><keyname>Guadagnini</keyname><forenames>Enore</forenames></author></authors><title>Percolation on correlated random networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>11 pages, 9 figure; to appear in Phys. Rev. E</comments><doi>10.1103/PhysRevE.84.031120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of random, weighted networks, obtained through a
redefinition of patterns in an Hopfield-like model and, by performing
percolation processes, we get information about topology and resilience
properties of the networks themselves. Given the weighted nature of the graphs,
different kinds of bond percolation can be studied: stochastic (deleting links
randomly) and deterministic (deleting links based on rank weights), each
mimicking a different physical process. The evolution of the network is
accordingly different, as evidenced by the behavior of the largest component
size and of the distribution of cluster sizes. In particular, we can derive
that weak ties are crucial in order to maintain the graph connected and that,
when they are the most prone to failure, the giant component typically shrinks
without abruptly breaking apart; these results have been recently evidenced in
several kinds of social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0847</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0847</id><created>2011-09-05</created><updated>2014-12-04</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author></authors><title>Robust Transceiver with Tomlinson-Harashima Precoding for
  Amplify-and-Forward MIMO Relaying Systems</title><categories>cs.IT math.IT</categories><comments>IEEE Journal on Selected Areas in Communications - Special Issue on
  Theories and Methods for Advanced Wireless Relays The final version and
  several typos have been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, robust transceiver design with Tomlinson-Harashima precoding
(THP) for multi-hop amplify-and-forward (AF) multiple-input multiple-output
(MIMO) relaying systems is investigated. At source node, THP is adopted to
mitigate the spatial intersymbol interference. However, due to its nonlinear
nature, THP is very sensitive to channel estimation errors. In order to reduce
the effects of channel estimation errors, a joint Bayesian robust design of THP
at source, linear forwarding matrices at relays and linear equalizer at
destination is proposed. With novel applications of elegant characteristics of
multiplicative convexity and matrix-monotone functions, the optimal structure
of the nonlinear transceiver is first derived. Based on the derived structure,
the transceiver design problem reduces to a much simpler one with only scalar
variables which can be efficiently solved. Finally, the performance advantage
of the proposed robust design over non-robust design is demonstrated by
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0882</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0882</id><created>2011-09-05</created><updated>2012-06-22</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Yu</keyname><forenames>Weichuan</forenames></author></authors><title>Moving Object Detection by Detecting Contiguous Outliers in the Low-Rank
  Representation</title><categories>cs.CV</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is a fundamental step for automated video analysis in many
vision applications. Object detection in a video is usually performed by object
detectors or background subtraction techniques. Often, an object detector
requires manually labeled examples to train a binary classifier, while
background subtraction needs a training sequence that contains no objects to
build a background model. To automate the analysis, object detection without a
separate training phase becomes a critical task. People have tried to tackle
this task by using motion information. But existing motion-based methods are
usually limited when coping with complex scenarios such as nonrigid motion and
dynamic background. In this paper, we show that above challenges can be
addressed in a unified framework named DEtecting Contiguous Outliers in the
LOw-rank Representation (DECOLOR). This formulation integrates object detection
and background learning into a single process of optimization, which can be
solved by an alternating algorithm efficiently. We explain the relations
between DECOLOR and other sparsity-based methods. Experiments on both simulated
data and real sequences demonstrate that DECOLOR outperforms the
state-of-the-art approaches and it can work effectively on a wide range of
complex scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0895</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0895</id><created>2011-08-31</created><authors><author><keyname>Charrada</keyname><forenames>Anis</forenames></author><author><keyname>Samet</keyname><forenames>Abdelaziz</forenames></author></authors><title>Nonlinear Channel Estimation for OFDM System by Complex LS-SVM under
  High Mobility Conditions</title><categories>cs.LG stat.ML</categories><comments>11 pages</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  3, No. 4, August 2011</journal-ref><doi>10.5121/ijwmn.2011.3412</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A nonlinear channel estimator using complex Least Square Support Vector
Machines (LS-SVM) is proposed for pilot-aided OFDM system and applied to Long
Term Evolution (LTE) downlink under high mobility conditions. The estimation
algorithm makes use of the reference signals to estimate the total frequency
response of the highly selective multipath channel in the presence of
non-Gaussian impulse noise interfering with pilot signals. Thus, the algorithm
maps trained data into a high dimensional feature space and uses the structural
risk minimization (SRM) principle to carry out the regression estimation for
the frequency response function of the highly selective channel. The
simulations show the effectiveness of the proposed method which has good
performance and high precision to track the variations of the fading channels
compared to the conventional LS method and it is robust at high speed mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0908</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0908</id><created>2011-09-05</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Increasing Physical Layer Security through Scrambled Codes and ARQ</title><categories>cs.IT cs.CR math.IT</categories><comments>5 pages, 4 figures; Proc. IEEE ICC 2011, Kyoto, Japan, 5-9 June 2011</comments><doi>10.1109/iccw.2011.5963540</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the proposal of non-systematic channel codes on the AWGN wire-tap
channel. Such coding technique, based on scrambling, achieves high transmission
security with a small degradation of the eavesdropper's channel with respect to
the legitimate receiver's channel. In this paper, we show that, by implementing
scrambling and descrambling on blocks of concatenated frames, rather than on
single frames, the channel degradation needed is further reduced. The usage of
concatenated scrambling allows to achieve security also when both receivers
experience the same channel quality. However, in this case, the introduction of
an ARQ protocol with authentication is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0910</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0910</id><created>2011-09-05</created><updated>2011-09-06</updated><authors><author><keyname>Mendenhall</keyname><forenames>Marcus H.</forenames></author><author><keyname>Weller</keyname><forenames>Robert A.</forenames></author></authors><title>A probability-conserving cross-section biasing mechanism for variance
  reduction in Monte Carlo particle transport calculations</title><categories>physics.comp-ph cs.NA</categories><doi>10.1016/j.nima.2011.11.084</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Monte Carlo particle transport codes, it is often important to adjust
reaction cross sections to reduce the variance of calculations of relatively
rare events, in a technique known as non-analogous Monte Carlo. We present the
theory and sample code for a Geant4 process which allows the cross section of a
G4VDiscreteProcess to be scaled, while adjusting track weights so as to
mitigate the effects of altered primary beam depletion induced by the cross
section change. This makes it possible to increase the cross section of nuclear
reactions by factors exceeding 10^4 (in appropriate cases), without distorting
the results of energy deposition calculations or coincidence rates. The
procedure is also valid for bias factors less than unity, which is useful, for
example, in problems that involve computation of particle penetration deep into
a target, such as occurs in atmospheric showers or in shielding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0915</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0915</id><created>2011-09-05</created><authors><author><keyname>Mundici</keyname><forenames>Daniele</forenames></author><author><keyname>Picardi</keyname><forenames>Claudia</forenames></author></authors><title>Drawing Sound Conclusions from Unsound Premises</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given sets $\Phi_1=\{\phi_{11},...,\phi_{1u(1)}\},
...,\Phi_{z}=\{\phi_{z1},...,\phi_{zu(z)}\}$ of boolean formulas, a formula
$\omega$ follows from the conjunction $\bigwedge\Phi_i= \bigwedge \phi_{ij}$
iff $\neg \omega\wedge \bigwedge_{i=1}^z \Phi_i$ is unsatisfiable.
  Now assume that, given integers $0\leq e_i &lt; u(i)$, we must check if $\neg
\omega\wedge \bigwedge_{i=1}^z \Phi'_i$ remains unsatisfiable, where
$\Phi'_i\subseteq \Phi_i$ is obtained by deleting $\,\,e_{i}$ arbitrarily
chosen formulas of $\Phi_i$, for each $i=1,...,z.$
  Intuitively, does $\omega$ {\it stably} follow, after removing $e_i$ random
formulas from each $\Phi_i$?
  We construct a quadratic reduction of this problem to the consequence problem
in infinite-valued \luk\ logic \L$_\infty$. In this way we obtain a
self-contained proof that the \L$_\infty$-consequence problem is coNP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0916</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0916</id><created>2011-09-05</created><authors><author><keyname>Lewandowski</keyname><forenames>Dirk</forenames></author><author><keyname>Spree</keyname><forenames>Ulrike</forenames></author></authors><title>Ranking of Wikipedia articles in search engines revisited: Fair ranking
  for reasonable quality?</title><categories>cs.IR</categories><journal-ref>Journal of the American Society for Information Science and
  Technology 62(2011)1, 117-132</journal-ref><doi>10.1002/asi.21423</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to review the fiercely discussed question of whether the
ranking of Wikipedia articles in search engines is justified by the quality of
the articles. After an overview of current research on information quality in
Wikipedia, a summary of the extended discussion on the quality of encyclopedic
entries in general is given. On this basis, a heuristic method for evaluating
Wikipedia entries is developed and applied to Wikipedia articles that scored
highly in a search engine retrieval effectiveness test and compared with the
relevance judgment of jurors. In all search engines tested, Wikipedia results
are unanimously judged better by the jurors than other results on the
corresponding results position. Relevance judgments often roughly correspond
with the results from the heuristic evaluation. Cases in which high relevance
judgments are not in accordance with the comparatively low score from the
heuristic evaluation are interpreted as an indicator of a high degree of trust
in Wikipedia. One of the systemic shortcomings of Wikipedia lies in its
necessarily incoherent user model. A further tuning of the suggested criteria
catalogue, for instance the different weighing of the supplied criteria, could
serve as a starting point for a user model differentiated evaluation of
Wikipedia articles. Approved methods of quality evaluation of reference works
are applied to Wikipedia articles and integrated with the question of search
engine evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0918</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0918</id><created>2011-09-05</created><authors><author><keyname>Bechmann</keyname><forenames>Matthias</forenames></author><author><keyname>Sebald</keyname><forenames>Angelika</forenames></author><author><keyname>Stepney</keyname><forenames>Susan</forenames></author></authors><title>Boolean logic gate design principles in unconventional computers: an NMR
  case study</title><categories>cs.ET cond-mat.other</categories><journal-ref>International Journal of Unconventional Computing, 2012, 8,
  139-159</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a general method for analysing novel computational substrates to
determine which of their parameters can be manipulated to exhibit the complete
set of 2-input boolean logical operations. We demonstrate this approach with an
NMR-based case study, showing which NMR parameters can be used to perform
boolean logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0923</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0923</id><created>2011-09-05</created><authors><author><keyname>Kelly</keyname><forenames>Benjamin G.</forenames></author><author><keyname>Wagner</keyname><forenames>Aaron B.</forenames></author></authors><title>Reliability in Source Coding with Side Information</title><categories>cs.IT math.IT</categories><comments>55 pages, 5 figures. Extended manuscript with all proofs. Shorter
  article submitted to IEEE Trans. Inf. Theory in September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study error exponents for source coding with side information. Both
achievable exponents and converse bounds are obtained for the following two
cases: lossless source coding with coded information (SCCSI) and lossy source
coding with full side information (Wyner-Ziv). These results recover and extend
several existing results on source-coding error exponents and are tight in some
circumstances. Our bounds have a natural interpretation as a two-player game
between nature and the code designer, with nature seeking to minimize the
exponent and the code designer seeking to maximize it. In the Wyner-Ziv problem
our analysis exposes a tension in the choice of test channel with the optimal
test channel balancing two competing error events. The Gaussian and
binary-erasure cases are examined in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0931</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0931</id><created>2011-09-05</created><updated>2011-09-29</updated><authors><author><keyname>Morshed</keyname><forenames>Md. Monzur</forenames></author><author><keyname>Rahman</keyname><forenames>Meftah Ur</forenames></author><author><keyname>Islam</keyname><forenames>Md. Rafiqul</forenames></author></authors><title>An Empirical Study on variants of TCP over AODV routing protocol in
  MANET</title><categories>cs.NI cs.PF</categories><comments>(6 pages, 9 figures, 3 tables) This is a joint research collaboration
  of AIUB &amp; TigerHATS Research Team. http://www.aiub.edu,
  http://www.tigerhats.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cardinal concept of TCP development was to carry data within the network
where network congestion plays a vital role to cause packet loss. On the other
hand, there are several other reasons to lose packets in Mobile Ad Hoc Networks
due to fading, interfaces, multi-path routing, malicious node, and black hole.
Along with throughput, fairness of TCP protocols is important to establish a
good communication. In this paper, an empirical study has been done by
simulation and analysis of TCP variations under AODV routing protocol. In our
simulation, we studied multiple variations of TCP, such as Reno, New-Reno,
Vegas, and Tahoe. The simulation work has been done in NS2 environment. Based
on the analysis simulation result of we carried out our observations with
respect to the behavior of AODV routing protocol for different TCP packets
under several QoS metrics such as drop, throughput, delay, and jitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0937</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0937</id><created>2011-09-05</created><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>Impact of the Evolution of Smart Phones in Education Technology and its
  Application in Technical and Professional Studies: Indian Perspective</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The greatness of any nation depends largely on the system of education that
is used to nurture its talent from within. With the digital era taking the
spotlight, and the world rapidly reforming into a global village, it is now
quintessential that a spirit of healthy competitiveness be inculcated in the
budding minds of this country. While trying to remodel and upgrade the
education system, a key issue is that of quality of education processes in the
country. Needs and expectations of the society are changing very fast and the
quality of higher education requires to be sustained at the desired level. The
use of internet for educational purposes has increased many folds among Indian
youths. Online video lectures and e-books are the emerging trends among
learners. The birth of high speed internet access and its availability on
recently evolved smart phones has opened several new avenues for learning. The
growing popularity of these smart phones among the youth can potentially
revolutionize the way we learn. The introduction of 3G technology is already
being pinned as the next big thing in the mobile internet revolution. This
paper discusses the use of Smart Phones in Education Technology and its
application in Technical &amp; Professional studies in India. We intend to put
forward some challenges and advices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0964</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0964</id><created>2011-09-05</created><authors><author><keyname>Pereszl&#xe9;nyi</keyname><forenames>Attila</forenames></author></authors><title>On quantum interactive proofs with short messages</title><categories>cs.CC quant-ph</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves one of the open problem posed by Beigi et al. in
arXiv:1004.0411v2. We consider quantum interactive proof systems where in the
beginning the verifier and prover send messages to each other with the combined
length of all messages being at most logarithmic (in the input length); and at
the end the prover sends a polynomial-length message to the verifier. We show
that this class has the same expressive power as QMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0971</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0971</id><created>2011-09-05</created><authors><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>X-Vine: Secure and Pseudonymous Routing Using Social Networks</title><categories>cs.CR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed hash tables suffer from several security and privacy
vulnerabilities, including the problem of Sybil attacks. Existing social
network-based solutions to mitigate the Sybil attacks in DHT routing have a
high state requirement and do not provide an adequate level of privacy. For
instance, such techniques require a user to reveal their social network
contacts. We design X-Vine, a protection mechanism for distributed hash tables
that operates entirely by communicating over social network links. As with
traditional peer-to-peer systems, X-Vine provides robustness, scalability, and
a platform for innovation. The use of social network links for communication
helps protect participant privacy and adds a new dimension of trust absent from
previous designs. X-Vine is resilient to denial of service via Sybil attacks,
and in fact is the first Sybil defense that requires only a logarithmic amount
of state per node, making it suitable for large-scale and dynamic settings.
X-Vine also helps protect the privacy of users social network contacts and
keeps their IP addresses hidden from those outside of their social circle,
providing a basis for pseudonymous communication. We first evaluate our design
with analysis and simulations, using several real world large-scale social
networking topologies. We show that the constraints of X-Vine allow the
insertion of only a logarithmic number of Sybil identities per attack edge; we
show this mitigates the impact of malicious attacks while not affecting the
performance of honest nodes. Moreover, our algorithms are efficient, maintain
low stretch, and avoid hot spots in the network. We validate our design with a
PlanetLab implementation and a Facebook plugin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.0983</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.0983</id><created>2011-09-05</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>The Information Flow Framework: New architecture</title><categories>cs.DL cs.LO math.CT</categories><comments>Presented at the International Category Theory Conference (CT 2006)
  June 25 - July 1, 2006 at White Point, Nova Scotia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This presentation discusses a new, modular, more mature architecture for the
Information Flow Framework (IFF). The IFF uses institution theory as a
foundation for the semantic integration of ontologies. It represents metalogic,
and as such operates at the structural level of ontologies. The content, form
and experience of the IFF could contribute to the development of a standard
ontology for category theory. The foundational aspect of the IFF helps to
explain the relationship between the fundamental concepts of set theory and
category theory. The development of the IFF follows two design principles:
conceptual warrant and categorical design. Both are limitations of the logical
expression. Conceptual warrant limits the content of logical expression, by
requiring us to justify the introduction of new terminology (and attendant
axiomatizations). Categorical design limits the form of logical expression (of
all mathematical concepts and constraints) to atomic expressions: declarations,
equations or relational expressions. The IFF is a descriptive category
metatheory. It is descriptive, since it follows the principle of conceptual
warrant; it is categorical, since it follows the principle of categorical
design; and it is a metatheory, since it provides a framework for all theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1015</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1015</id><created>2011-09-05</created><authors><author><keyname>Stehl&#xe9;</keyname><forenames>J.</forenames></author><author><keyname>Voirin</keyname><forenames>N.</forenames></author><author><keyname>Barrat</keyname><forenames>A.</forenames></author><author><keyname>Cattuto</keyname><forenames>C.</forenames></author><author><keyname>Isella</keyname><forenames>L.</forenames></author><author><keyname>Pinton</keyname><forenames>J. -F.</forenames></author><author><keyname>Quaggiotto</keyname><forenames>M.</forenames></author><author><keyname>Broeck</keyname><forenames>W. Van den</forenames></author><author><keyname>R&#xe9;gis</keyname><forenames>C.</forenames></author><author><keyname>Lina</keyname><forenames>B.</forenames></author><author><keyname>Vanhems</keyname><forenames>P.</forenames></author></authors><title>High-resolution measurements of face-to-face contact patterns in a
  primary school</title><categories>physics.soc-ph cs.SI q-bio.QM</categories><journal-ref>PLoS ONE 6(8):e23176 (2011)</journal-ref><doi>10.1371/journal.pone.0023176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Little quantitative information is available on the mixing patterns of
children in school environments. Describing and understanding contacts between
children at school would help quantify the transmission opportunities of
respiratory infections and identify situations within schools where the risk of
transmission is higher. We report on measurements carried out in a French
school (6-12 years children), where we collected data on the time-resolved
face-to-face proximity of children and teachers using a proximity-sensing
infrastructure based on radio frequency identification devices.
  Data on face-to-face interactions were collected on October 1st and 2nd,
2009. We recorded 77,602 contact events between 242 individuals. Each child has
on average 323 contacts per day with 47 other children, leading to an average
daily interaction time of 176 minutes. Most contacts are brief, but long
contacts are also observed. Contacts occur mostly within each class, and each
child spends on average three times more time in contact with classmates than
with children of other classes. We describe the temporal evolution of the
contact network and the trajectories followed by the children in the school,
which constrain the contact patterns. We determine an exposure matrix aimed at
informing mathematical models. This matrix exhibits a class and age structure
which is very different from the homogeneous mixing hypothesis.
  The observed properties of the contact patterns between school children are
relevant for modeling the propagation of diseases and for evaluating control
measures. We discuss public health implications related to the management of
schools in case of epidemics and pandemics. Our results can help define a
prioritization of control measures based on preventive measures, case
isolation, classes and school closures, that could reduce the disruption to
education during epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1021</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1021</id><created>2011-09-05</created><authors><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Min</keyname><forenames>Alexander W.</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Shin</keyname><forenames>Kang G.</forenames></author></authors><title>Attack Prevention for Collaborative Spectrum Sensing in Cognitive Radio
  Networks</title><categories>cs.CR</categories><comments>37 pages including 7 figures and 2 tables; IEEE Journal on Selected
  Areas in Communications with special issue in Cooperative Networking -
  Challenges and Applications (2012 expected)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative spectrum sensing can significantly improve the detection
performance of secondary unlicensed users (SUs). However, the performance of
collaborative sensing is vulnerable to sensing data falsification attacks,
where malicious SUs (attackers) submit manipulated sensing reports to mislead
the fusion center's decision on spectrum occupancy. Moreover, attackers may not
follow the fusion center's decision regarding their spectrum access. This paper
considers a challenging attack scenario where multiple rational attackers
overhear all honest SUs' sensing reports and cooperatively maximize attackers'
aggregate spectrum utilization. We show that, without attack-prevention
mechanisms, honest SUs are unable to transmit over the licensed spectrum, and
they may further be penalized by the primary user for collisions due to
attackers' aggressive transmissions. To prevent such attacks, we propose two
novel attack-prevention mechanisms with direct and indirect punishments. The
key idea is to identify collisions to the primary user that should not happen
if all SUs follow the fusion center's decision. Unlike prior work, the proposed
simple mechanisms do not require the fusion center to identify and exclude
attackers. The direct punishment can effectively prevent all attackers from
behaving maliciously. The indirect punishment is easier to implement and can
prevent attacks when the attackers care enough about their long-term reward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1027</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1027</id><created>2011-09-05</created><updated>2013-03-08</updated><authors><author><keyname>Caplan</keyname><forenames>R. M.</forenames></author><author><keyname>Carretero</keyname><forenames>R.</forenames></author></authors><title>A Two-Step High-Order Compact Scheme for the Laplacian Operator and its
  Implementation in an Explicit Method for Integrating the Nonlinear
  Schr\&quot;odinger Equation</title><categories>cs.NA</categories><comments>18 pages, 3 figures</comments><msc-class>65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and test an easy-to-implement two-step high-order compact (2SHOC)
scheme for the Laplacian operator and its implementation into an explicit
finite-difference scheme for simulating the nonlinear Schr\&quot;odinger equation
(NLSE). Our method relies on a compact `double-differencing' which is shown to
be computationally equivalent to standard fourth-order non-compact schemes.
Through numerical simulations of the NLSE using fourth-order Runge-Kutta, we
confirm that our scheme shows the desired fourth-order accuracy. A computation
and storage requirement comparison is made between the 2SHOC scheme and the
non-compact equivalent scheme for both the Laplacian operator alone, as well as
when implemented in the NLSE simulations. Stability bounds are also shown in
order to get maximum e?cffiency out of the method. We conclude that the modest
increase in storage and computation of the 2SHOC schemes are well worth the
advantages of having the schemes compact, and their ease of implementation
makes their use very useful for practical implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1032</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1032</id><created>2011-09-05</created><authors><author><keyname>Coviello</keyname><forenames>Emanuele</forenames></author><author><keyname>Chan</keyname><forenames>Antoni B.</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert R. G.</forenames></author></authors><title>Tech Report A Variational HEM Algorithm for Clustering Hidden Markov
  Models</title><categories>cs.AI stat.ML</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hidden Markov model (HMM) is a generative model that treats sequential
data under the assumption that each observation is conditioned on the state of
a discrete hidden variable that evolves in time as a Markov chain. In this
paper, we derive a novel algorithm to cluster HMMs through their probability
distributions. We propose a hierarchical EM algorithm that i) clusters a given
collection of HMMs into groups of HMMs that are similar, in terms of the
distributions they represent, and ii) characterizes each group by a &quot;cluster
center&quot;, i.e., a novel HMM that is representative for the group. We present
several empirical studies that illustrate the benefits of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1041</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1041</id><created>2011-09-05</created><updated>2013-02-03</updated><authors><author><keyname>Liu</keyname><forenames>Jianquan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Alternative Awaiting and Broadcast for Two-Way Relay Fading Channels</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Vehicular Technology, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a two-way relay (TWR) fading channel where two source nodes
wish to exchange information with the help of a relay node. Given traditional
TWR protocols, transmission rates in both directions are known to be limited by
the hop with lower capacity, i.e., the min operations between uplink and
downlink. In this paper, we propose a new transmission protocol, named as
alternative awaiting and broadcast (AAB), to cancel the min operations in the
TWR fading channels. The operational principles, new upper bound on ergodic
sum-capacity (ESC) and convergence behavior of average delay of signal
transmission (ST) (in relay buffer) for the proposed AAB protocol are analyzed.
Moreover, we propose a suboptimal encoding/decoding solution for the AAB
protocol and derive an achievable ergodic sum-rate (ESR) with corresponding
average delay of ST. Numerical results show that 1) the proposed AAB protocol
significantly improves the achievable ESR compared to the traditional TWR
protocols, 2) considering the average delay of system service (SS) (in source
buffer), the average delay of ST induced by the proposed AAB protocol is very
small and negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1044</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1044</id><created>2011-09-05</created><updated>2011-09-08</updated><authors><author><keyname>Petre</keyname><forenames>Ion</forenames><affiliation>&#xc5;bo Akademi University</affiliation></author><author><keyname>de Vink</keyname><forenames>Erik</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Proceedings Third International Workshop on Computational Models for
  Cell Processes</title><categories>cs.CE q-bio.CB</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 67, 2011</journal-ref><doi>10.4204/EPTCS.67</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the final versions of the papers presented at the 3rd
International Workshop on Computational Models for Cell Processes (CompMod
2011). The workshop took place on September 10, 2011 at the University of
Aachen, Germany, in conjunction with CONCUR 2011. The first edition of the
workshop (2008) took place in Turku, Finland, in conjunction with Formal
Methods 2008 and the second edition (2009) took place in Eindhoven, the
Netherlands, as well in conjunction with Formal Methods 2009. The goal of the
CompMod workshop series is to bring together researchers in Computer Science
(especially in Formal Methods) and Mathematics (both discrete and continuous),
interested in the opportunities and the challenges of Systems Biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1045</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1045</id><created>2011-09-05</created><authors><author><keyname>Zeng</keyname><forenames>Weiliang</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author><author><keyname>Wang</keyname><forenames>Mingxi</forenames></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>On the Linear Precoder Design for MIMO Channels with Finite-Alphabet
  Inputs and Statistical CSI</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, accepted by IEEE Global Communications Conference
  (GLOBECOM) 2011, Houston, TX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the linear precoder design that maximizes the average
mutual information of multiple-input multiple-output channels with
finite-alphabet inputs and statistical channel state information known at the
transmitter. This linear precoder design is an important open problem and is
extremely difficult to solve: First, average mutual information lacks
closed-form expression and involves complicated computations; Second, the
optimization problem over precoder is nonconcave. This study explores the
solution to this problem and provides the following contributions: 1) A
closed-form lower bound of average mutual information is derived. It achieves
asymptotic optimality at low and high signal-to-noise ratio regions and, with a
constant shift, offers an accurate approximation to the average mutual
information; 2) The optimal structure of the precoder is revealed, and a
unified two-step iterative algorithm is proposed to solve this problem.
Numerical examples show the convergence and the efficacy of the proposed
algorithm. Compared to its conventional counterparts, the proposed linear
precoding method provides a significant performance gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1053</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1053</id><created>2011-09-06</created><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author><author><keyname>Yan</keyname><forenames>Qiqi</forenames></author></authors><title>An approximately truthful-in-expectation mechanism for combinatorial
  auctions using value queries</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript presents an alternative implementation of the
truthful-in-expectation mechanism of Dughmi, Roughgarden and Yan for
combinatorial auctions with weighted-matroid-rank-sum valuations. The new
implementation uses only value queries and is approximately
truthful-in-expectation, in the sense that by reporting truthfully each agent
maximizes his utility within a multiplicative 1-o(1) factor. It still provides
an optimal (1-1/e-o(1))-approximation in social welfare. We achieve this by
first presenting an approximately maximal-in-distributional-range allocation
rule and then showing a black-box transformation to an approximately
truthful-in-expectation mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1055</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1055</id><created>2011-09-06</created><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>Limitations of randomized mechanisms for combinatorial auctions</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a randomized mechanism has been discovered [Dughmi, Roughgarden and
Yan; STOC'11] for combinatorial auctions that is truthful in expectation and
guarantees a (1-1/e)-approximation to the optimal social welfare when players
have coverage valuations. This approximation ratio is the best possible even
for non-truthful algorithms, assuming $P \neq NP$. Given the recent sequence of
negative results for combinatorial auctions under more restrictive notions of
incentive compatibility, this development raises a natural question: Are
truthful-in-expectation mechanisms compatible with polynomial-time
approximation in a way that deterministic or universally truthful mechanisms
are not? In particular, can polynomial-time truthful-in-expectation mechanisms
guarantee a near-optimal approximation ratio for more general variants of
combinatorial auctions?
  We prove that this is not the case. Specifically, the result of Dughmi,
Roughgarden and Yan cannot be extended to combinatorial auctions with
submodular valuations in the value oracle model. (Absent strategic
considerations, a (1-1/e)-approximation is still achievable in this setting.)
More precisely, we prove that there is a constant \gamma&gt;0 such that there is
no randomized mechanism that is truthful-in-expectation--- or even
approximately truthful-in-expectation --- and guarantees an
m^{-\gamma}-approximation to the optimal social welfare for combinatorial
auctions with submodular valuations in the value oracle model. We also prove an
analogous result for the flexible combinatorial public projects (CPP) problem.
Both our results present an unexpected separation between coverage functions
and submodular functions, which does not occur for these problems without
strategic considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1057</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1057</id><created>2011-09-06</created><authors><author><keyname>Liu</keyname><forenames>Risheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Kewei</forenames></author><author><keyname>Su</keyname><forenames>Zhixun</forenames></author></authors><title>Toward Designing Intelligent PDEs for Computer Vision: An Optimal
  Control Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision and image processing problems can be posed as solving
partial differential equations (PDEs). However, designing PDE system usually
requires high mathematical skills and good insight into the problems. In this
paper, we consider designing PDEs for various problems arising in computer
vision and image processing in a lazy manner: \emph{learning PDEs from real
data via data-based optimal control}. We first propose a general intelligent
PDE system which holds the basic translational and rotational invariance rule
for most vision problems. By introducing a PDE-constrained optimal control
framework, it is possible to use the training data resulting from multiple ways
(ground truth, results from other methods, and manual results from humans) to
learn PDEs for different computer vision tasks. The proposed optimal control
based training framework aims at learning a PDE-based regressor to approximate
the unknown (and usually nonlinear) mapping of different vision tasks. The
experimental results show that the learnt PDEs can solve different vision
problems reasonably well. In particular, we can obtain PDEs not only for
problems that traditional PDEs work well but also for problems that PDE-based
methods have never been tried before, due to the difficulty in describing those
problems in a mathematical way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1059</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1059</id><created>2011-09-06</created><authors><author><keyname>Yoon</keyname><forenames>Seok-Ho</forenames></author><author><keyname>Kim</keyname><forenames>Sang-Wook</forenames></author><author><keyname>Park</keyname><forenames>Sunju</forenames></author></authors><title>C-Rank: A Link-based Similarity Measure for Scientific Literature
  Databases</title><categories>cs.DL cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of people who use scientific literature databases grows, the
demand for literature retrieval services has been steadily increased. One of
the most popular retrieval services is to find a set of papers similar to the
paper under consideration, which requires a measure that computes similarities
between papers. Scientific literature databases exhibit two interesting
characteristics that are different from general databases. First, the papers
cited by old papers are often not included in the database due to technical and
economic reasons. Second, since a paper references the papers published before
it, few papers cite recently-published papers. These two characteristics cause
all existing similarity measures to fail in at least one of the following
cases: (1) measuring the similarity between old, but similar papers, (2)
measuring the similarity between recent, but similar papers, and (3) measuring
the similarity between two similar papers: one old, the other recent. In this
paper, we propose a new link-based similarity measure called C-Rank, which uses
both in-link and out-link by disregarding the direction of references. In
addition, we discuss the most suitable normalization method for scientific
literature databases and propose an evaluation method for measuring the
accuracy of similarity measures. We have used a database with real-world papers
from DBLP and their reference information crawled from Libra for experiments
and compared the performance of C-Rank with those of existing similarity
measures. Experimental results show that C-Rank achieves a higher accuracy than
existing similarity measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1062</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1062</id><created>2011-09-06</created><authors><author><keyname>George</keyname><forenames>G. Victo Sudha</forenames></author><author><keyname>Raj</keyname><forenames>V. Cyril</forenames></author></authors><title>Review on Feature Selection Techniques and the Impact of SVM for Cancer
  Classification using Gene Expression Profile</title><categories>cs.CE cs.ET cs.LG q-bio.QM</categories><comments>12 pages</comments><journal-ref>International Journal of Computer Science &amp; Engineering Survey
  (IJCSES) Vol.2, No.3, International Journal of Computer Science &amp; Engineering
  Survey (IJCSES) Vol.2, No.3, August 2011</journal-ref><doi>10.5121/ijcses.2011.2302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The DNA microarray technology has modernized the approach of biology research
in such a way that scientists can now measure the expression levels of
thousands of genes simultaneously in a single experiment. Gene expression
profiles, which represent the state of a cell at a molecular level, have great
potential as a medical diagnosis tool. But compared to the number of genes
involved, available training data sets generally have a fairly small sample
size for classification. These training data limitations constitute a challenge
to certain classification methodologies. Feature selection techniques can be
used to extract the marker genes which influence the classification accuracy
effectively by eliminating the un wanted noisy and redundant genes This paper
presents a review of feature selection techniques that have been employed in
micro array data based cancer classification and also the predominant role of
SVM for cancer classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1063</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1063</id><created>2011-09-06</created><authors><author><keyname>Yoon</keyname><forenames>Seok-Ho</forenames></author><author><keyname>Kim</keyname><forenames>Ki-Nam</forenames></author><author><keyname>Kim</keyname><forenames>Sang-Wook</forenames></author><author><keyname>Park</keyname><forenames>Sunju</forenames></author></authors><title>A Community-Based Sampling Method Using DPL for Online Social Network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new graph sampling method for online social
networks that achieves the following. First, a sample graph should reflect the
ratio between the number of nodes and the number of edges of the original
graph. Second, a sample graph should reflect the topology of the original
graph. Third, sample graphs should be consistent with each other when they are
sampled from the same original graph. The proposed method employs two
techniques: hierarchical community extraction and densification power law. The
proposed method partitions the original graph into a set of communities to
preserve the topology of the original graph. It also uses the densification
power law which captures the ratio between the number of nodes and the number
of edges in online social networks. In experiments, we use several real-world
online social networks, create sample graphs using the existing methods and
ours, and analyze the differences between the sample graph by each sampling
method and the original graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1067</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1067</id><created>2011-09-06</created><authors><author><keyname>Padma</keyname><forenames>A.</forenames></author><author><keyname>Sukanesh</keyname><forenames>Dr. R.</forenames></author></authors><title>Automatic Diagnosis of Abnormal Tumor Region from Brain Computed
  Tomography Images Using Wavelet Based Statistical Texture Features</title><categories>cs.CV</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The research work presented in this paper is to achieve the tissue
classification and automatically diagnosis the abnormal tumor region present in
Computed Tomography (CT) images using the wavelet based statistical texture
analysis method. Comparative studies of texture analysis method are performed
for the proposed wavelet based texture analysis method and Spatial Gray Level
Dependence Method (SGLDM). Our proposed system consists of four phases i)
Discrete Wavelet Decomposition (ii) Feature extraction (iii) Feature selection
(iv) Analysis of extracted texture features by classifier. A wavelet based
statistical texture feature set is derived from normal and tumor regions.
Genetic Algorithm (GA) is used to select the optimal texture features from the
set of extracted texture features. We construct the Support Vector Machine
(SVM) based classifier and evaluate the performance of classifier by comparing
the classification results of the SVM based classifier with the Back
Propagation Neural network classifier(BPN). The results of Support Vector
Machine (SVM), BPN classifiers for the texture analysis methods are evaluated
using Receiver Operating Characteristic (ROC) analysis. Experimental results
show that the classification accuracy of SVM is 96% for 10 fold cross
validation method. The system has been tested with a number of real Computed
Tomography brain images and has achieved satisfactory results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1068</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1068</id><created>2011-09-06</created><authors><author><keyname>Pavan</keyname><forenames>K. Karteeka</forenames></author><author><keyname>Rao</keyname><forenames>Allam Appa</forenames></author><author><keyname>Rao</keyname><forenames>A. V. Dattatreya</forenames></author></authors><title>An Automatic Clustering Technique for Optimal Clusters</title><categories>cs.CV</categories><comments>12 pages, 5 figures, 2 tables</comments><msc-class>62H30</msc-class><acm-class>I.5.3</acm-class><journal-ref>International journal of Computer Sciene Engineering and
  Applications, Vol., No.4, 2011, pp 133-144</journal-ref><doi>10.5121/ijcsea.2011.1412</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple, automatic and efficient clustering algorithm,
namely, Automatic Merging for Optimal Clusters (AMOC) which aims to generate
nearly optimal clusters for the given datasets automatically. The AMOC is an
extension to standard k-means with a two phase iterative procedure combining
certain validation techniques in order to find optimal clusters with automation
of merging of clusters. Experiments on both synthetic and real data have proved
that the proposed algorithm finds nearly optimal clustering structures in terms
of number of clusters, compactness and separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1074</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1074</id><created>2011-09-06</created><authors><author><keyname>Martin</keyname><forenames>A.</forenames></author><author><keyname>Anutthamaa</keyname><forenames>Na. Ba.</forenames></author><author><keyname>Sathyavathy</keyname><forenames>M.</forenames></author><author><keyname>Francois</keyname><forenames>Marie Manjari Saint</forenames></author><author><keyname>Venkatesan</keyname><forenames>Dr. V. Prasanna</forenames></author></authors><title>A Framework for Predicting Phishing Websites using Neural Networks</title><categories>cs.NE</categories><comments>Phishing, Neural Networks, Classification,Learning, Phishing
  Detection</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 2, 2011, 330-336</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1077</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1077</id><created>2011-09-06</created><updated>2013-11-16</updated><authors><author><keyname>Sarkar</keyname><forenames>Purnamrita</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Deepayan</forenames></author><author><keyname>Jordan</keyname><forenames>Michael</forenames></author></authors><title>Nonparametric Link Prediction in Large Scale Dynamic Networks</title><categories>stat.ML cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a nonparametric approach to link prediction in large-scale dynamic
networks. Our model uses graph-based features of pairs of nodes as well as
those of their local neighborhoods to predict whether those nodes will be
linked at each time step. The model allows for different types of evolution in
different parts of the graph (e.g, growing or shrinking communities). We focus
on large-scale graphs and present an implementation of our model that makes use
of locality-sensitive hashing to allow it to be scaled to large problems.
Experiments with simulated data as well as five real-world dynamic graphs show
that we outperform the state of the art, especially when sharp fluctuations or
nonlinearities are present. We also establish theoretical properties of our
estimator, in particular consistency and weak convergence, the latter making
use of an elaboration of Stein's method for dependency graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1082</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1082</id><created>2011-09-06</created><authors><author><keyname>Mason</keyname><forenames>Sarah</forenames></author><author><keyname>Parsley</keyname><forenames>Jason</forenames></author></authors><title>A geometric and combinatorial view of weighted voting</title><categories>math.CO cs.GT</categories><comments>24 pages, 3 figures</comments><msc-class>Primary: 91A65, Secondary: 52B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A natural partial ordering exists on all weighted games and, more broadly, on
all linear games. We describe several properties of the partially ordered sets
formed by these games and utilize this perspective to enumerate proper linear
games with one generator. We introduce a geometric approach to weighted voting
by considering the convex polytope of all possible realizations of a weighted
game and connect this geometric perspective to the weighted games poset in
several ways. In particular, we prove that generic vertical lines in $C_n$, the
union of all weighted $n$-voter polytopes, correspond to maximal saturated
chains in the poset of weighted games, i.e., the poset is a blueprint for how
the polytopes fit together to form $C_n$. Finally, we describe the facets of
each polytope, from which we develop a method for determining the weightedness
of any linear game that covers or is covered by a weighted game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1087</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1087</id><created>2011-09-06</created><authors><author><keyname>Martin</keyname><forenames>A.</forenames></author><author><keyname>Manjula</keyname><forenames>M.</forenames></author><author><keyname>Venkatesan</keyname><forenames>Dr. V. Prasanna</forenames></author></authors><title>A Business Intelligence Model to Predict Bankruptcy using Financial
  Domain Ontology with Association Rule Mining Algorithm</title><categories>cs.DB</categories><comments>Bankruptcy, Financial domain Ontology, Data Mining, Z-Score Model,
  Business Intelligence, Altman Bankruptcy model</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 2, May 2011 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today in every organization financial analysis provides the basis for
understanding and evaluating the results of business operations and delivering
how well a business is doing. This means that the organizations can control the
operational activities primarily related to corporate finance. One way that
doing this is by analysis of bankruptcy prediction. This paper develops an
ontological model from financial information of an organization by analyzing
the Semantics of the financial statement of a business. One of the best
bankruptcy prediction models is Altman Z-score model. Altman Z-score method
uses financial rations to predict bankruptcy. From the financial ontological
model the relation between financial data is discovered by using data mining
algorithm. By combining financial domain ontological model with association
rule mining algorithm and Zscore model a new business intelligence model is
developed to predict the bankruptcy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1088</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1088</id><created>2011-09-06</created><authors><author><keyname>Martin</keyname><forenames>A.</forenames></author><author><keyname>Maladhy</keyname><forenames>D.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author></authors><title>A Framework for Business Intelligence Application using Ontological
  Classification</title><categories>cs.IR</categories><comments>Classification, Ontology, Business Intelligence, Datamining, Inverted
  Index, Ontology Tree Index</comments><journal-ref>International Journal of Engineering Science and Technology
  (IJEST) Vol. 3 No. 2, (2011) 1213-1221</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every business needs knowledge about their competitors to survive better. One
of the information repositories is web. Retrieving Specific information from
the web is challenging. An Ontological model is developed to capture specific
information by using web semantics. From the Ontology model, the relations
between the data are mined using decision tree. From all these a new framework
is developed for Business Intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1093</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1093</id><created>2011-09-06</created><authors><author><keyname>Martin</keyname><forenames>A.</forenames></author><author><keyname>Lakshmi</keyname><forenames>T. Miranda</forenames></author><author><keyname>Madhusudanan</keyname><forenames>J.</forenames></author></authors><title>Multi Agent Communication System for Online Auction with Decision
  Support System by JADE and TRACE</title><categories>cs.MA</categories><comments>Multi-Agents, Agents communication, intelligent auction agents,
  Decision support system, JADE, TRACE</comments><journal-ref>Journal of Convergence Information Technology Volume 4, Number 2,
  June 2009, 154-163</journal-ref><doi>10.4156/jcit.vol4.issue2.martin</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of online auctions has given buyers access to greater product
diversity with potentially lower prices. It has provided sellers with access to
large numbers of potential buyers and reduced transaction costs by enabling
auctions to take place without regard to time or place. However it is difficult
to spend more time period with system and closely monitor the auction until
auction participant wins the bid or closing of the auction. Determining which
items to bid on or what may be the recommended bid and when to bid it are
difficult questions to answer for online auction participants. The multi agent
auction advisor system JADE and TRACE, which is connected with decision support
system, gives the recommended bid to buyers for online auctions. The auction
advisor system relies on intelligent agents both for the retrieval of relevant
auction data and for the processing of that data to enable meaningful
recommendations, statistical reports and market prediction report to be made to
auction participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1102</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1102</id><created>2011-09-06</created><updated>2011-12-20</updated><authors><author><keyname>Dai</keyname><forenames>Xiongping</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Xiao</keyname><forenames>Mingqing</forenames></author></authors><title>Stability of time-varying nonlinear switching systems under
  perturbations</title><categories>cs.SY math.OC</categories><comments>8 pages, submitted</comments><msc-class>93C15, 34H05, 93D20, 93D09, 93E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a Liao-type exponent, we study the stability of a time-varying
nonlinear switching system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1105</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1105</id><created>2011-09-06</created><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author></authors><title>Embedding Constructions of Tail-Biting Trellises for Linear Block Codes</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, embedding construction of tail-biting trellises for linear
block codes is presented. With the new approach of constructing tail-biting
trellises, most of the study of tail-biting trellises can be converted into the
study of conventional trellises. It is proved that any minimal tail-biting
trellis can be constructed by the recursive process of embedding constructions
from the well-known Bahl-Cocke-Jelinek-Raviv (BCJR) constructed conventional
trellises. Furthermore, several properties of embedding constructions of
tail-biting trellises are discussed. Finally, we give four sufficient
conditions to reduce the maximum state-complexity of a trellis with one peak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1133</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1133</id><created>2011-09-06</created><authors><author><keyname>Ershad</keyname><forenames>Shervan Fekri</forenames></author></authors><title>Color Texture Classification Approach Based on Combination of Primitive
  Pattern Units and Statistical Features</title><categories>cs.CV cs.AI</categories><comments>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.3, August 2011</comments><doi>10.5121/ijma.2011.3301</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Texture classification became one of the problems which has been paid much
attention on by image processing scientists since late 80s. Consequently, since
now many different methods have been proposed to solve this problem. In most of
these methods the researchers attempted to describe and discriminate textures
based on linear and non-linear patterns. The linear and non-linear patterns on
any window are based on formation of Grain Components in a particular order.
Grain component is a primitive unit of morphology that most meaningful
information often appears in the form of occurrence of that. The approach which
is proposed in this paper could analyze the texture based on its grain
components and then by making grain components histogram and extracting
statistical features from that would classify the textures. Finally, to
increase the accuracy of classification, proposed approach is expanded to color
images to utilize the ability of approach in analyzing each RGB channels,
individually. Although, this approach is a general one and it could be used in
different applications, the method has been tested on the stone texture and the
results can prove the quality of approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1134</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1134</id><created>2011-09-06</created><authors><author><keyname>Ismail</keyname><forenames>Anis</forenames></author><author><keyname>Barbar</keyname><forenames>Aziz</forenames></author><author><keyname>Ismail</keyname><forenames>Ziad</forenames></author></authors><title>P2P Simulator for Queries Routing using Data Mining</title><categories>cs.NI</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is used to extract hidden information from large databases. In
Peer-to-Peer context, a challenging problem is how to find the appropriate Peer
to deal with a given query without overly consuming bandwidth. Different
methods proposed routing strategies of queries taking into account the P2P
network at hand. An unstructured P2P system based on an organization of Peers
around Super-Peers that are connected to Super-Super-Peer according to their
semantic domains is considered. This paper integrates Decision Trees in P2P
architectures for predicting Query-Suitable Super-Peers representing a
community of Peers, where one among them is able to answer the given query. In
fact, by analyzing the queries' log file, a predictive model that avoids
flooding queries in the P2P networks constructed by predicting the appropriate
Super-Peer, and hence the Peer to answer the query. The proposed architecture
is based on a Decision Tree (Base-Knowledge - BK). The efficiency of these
architectures is discussed considering architecture without knowledge
(Baseline) using only the flooding queries method to answer queries. The
advantage of this knowledge based model is the robustness in Queries routing
mechanism and scalability in P2P Network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1144</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1144</id><created>2011-09-06</created><authors><author><keyname>Venkatesan</keyname><forenames>M.</forenames></author><author><keyname>Thangavelu</keyname><forenames>Arunkumar</forenames></author><author><keyname>Prabhavathy</keyname><forenames>P.</forenames></author></authors><title>Event Centric Modeling Approach in Colocation Pattern Snalysis from
  Spatial Data</title><categories>cs.DB</categories><comments>9 pages</comments><doi>10.5121/ijdms.2011.3311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial co-location patterns are the subsets of Boolean spatial features
whose instances are often located in close geographic proximity. Co-location
rules can be identified by spatial statistics or data mining approaches. In
data mining method, Association rule-based approaches can be used which are
further divided into transaction-based approaches and distance-based
approaches. Transaction-based approaches focus on defining transactions over
space so that an Apriori algorithm can be used. The natural notion of
transactions is absent in spatial data sets which are embedded in continuous
geographic space. A new distance -based approach is developed to mine
co-location patterns from spatial data by using the concept of proximity
neighborhood. A new interest measure, a participation index, is used for
spatial co-location patterns as it possesses an anti-monotone property. An
algorithm to discover co-location patterns are designed which generates
candidate locations and their table instances. Finally the co-location rules
are generated to identify the patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1145</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1145</id><created>2011-09-06</created><authors><author><keyname>Kamde</keyname><forenames>Pravin M.</forenames></author><author><keyname>Algur</keyname><forenames>Dr. Siddu. P.</forenames></author></authors><title>A Survey on Web Multimedia Mining</title><categories>cs.MM</categories><comments>13 Pages; The International Journal of Multimedia &amp; Its Applications
  (IJMA) Vol.3, No.3, August 2011</comments><acm-class>A.1</acm-class><doi>10.5121/ijma</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern developments in digital media technologies has made transmitting and
storing large amounts of multi/rich media data (e.g. text, images, music, video
and their combination) more feasible and affordable than ever before. However,
the state of the art techniques to process, mining and manage those rich media
are still in their infancy. Advances developments in multimedia acquisition and
storage technology the rapid progress has led to the fast growing incredible
amount of data stored in databases. Useful information to users can be revealed
if these multimedia files are analyzed. Multimedia mining deals with the
extraction of implicit knowledge, multimedia data relationships, or other
patterns not explicitly stored in multimedia files. Also in retrieval, indexing
and classification of multimedia data with efficient information fusion of the
different modalities is essential for the system's overall performance. The
purpose of this paper is to provide a systematic overview of multimedia mining.
This article is also represents the issues in the application process component
for multimedia mining followed by the multimedia mining models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1146</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1146</id><created>2011-09-06</created><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author><author><keyname>Hlavac</keyname><forenames>Vaclav</forenames></author></authors><title>A Distributed Mincut/Maxflow Algorithm Combining Path Augmentation and
  Push-Relabel</title><categories>cs.DC cs.DM cs.DS</categories><comments>40 pages, 15 figures</comments><report-no>K333-43/11, CTU-CMP-2011-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel distributed algorithm for the minimum cut problem. We
primarily aim at solving large sparse problems. Assuming vertices of the graph
are partitioned into several regions, the algorithm performs path augmentations
inside the regions and updates of the push-relabel style between the regions.
The interaction between regions is considered expensive (regions are loaded
into the memory one-by-one or located on separate machines in a network). The
algorithm works in sweeps - passes over all regions. Let $B$ be the set of
vertices incident to inter-region edges of the graph. We present a sequential
and parallel versions of the algorithm which terminate in at most $2|B|^2+1$
sweeps. The competing algorithm by Delong and Boykov uses push-relabel updates
inside regions. In the case of a fixed partition we prove that this algorithm
has a tight $O(n^2)$ bound on the number of sweeps, where $n$ is the number of
vertices. We tested sequential versions of the algorithms on instances of
maxflow problems in computer vision. Experimentally, the number of sweeps
required by the new algorithm is much lower than for the Delong and Boykov's
variant. Large problems (up to $10^8$ vertices and $6\cdot 10^8$ edges) are
solved using under 1GB of memory in about 10 sweeps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1147</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1147</id><created>2011-09-06</created><authors><author><keyname>Ismail</keyname><forenames>Anis</forenames></author><author><keyname>Barbar</keyname><forenames>Aziz</forenames></author></authors><title>P2P Domain Classification using Decision Tree</title><categories>cs.NI</categories><comments>20 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Peer-to-Peer context, a challenging problem is how to find the appropriate
peer to deal with a given query without overly consuming bandwidth? Different
methods proposed routing strategies of queries taking into account the P2P
network at hand. This paper considers an unstructured P2P system based on an
organization of peers around Super-Peers that are connected to Super-Super-
Peer according to their semantic domains; By analyzing the queries log file, a
predictive model that avoids flooding queries in the P2P network is constructed
after predicting the appropriate Super-Peer, and hence the peer to answer the
query. A challenging problem in a schema-based Peer-to-Peer (P2P) system is how
to locate peers that are relevant to a given query. In this paper,
architecture, based on (Super-)Peers is proposed, focusing on query routing.
The approach to be implemented, groups together (Super-)Peers that have similar
interests for an efficient query routing method. In such groups, called
Super-Super-Peers (SSP), Super-Peers submit queries that are often processed by
members of this group. A SSP is a specific Super-Peer which contains knowledge
about: 1. its Super-Peers and 2. The other SSP. Knowledge is extracted by using
data mining techniques (e.g. Decision Tree algorithms) starting from queries of
peers that transit on the network. The advantage of this distributed knowledge
is that, it avoids making semantic mapping between heterogeneous data sources
owned by (Super-)Peers, each time the system decides to route query to other
(Super-) Peers. The set of SSP improves the robustness in queries routing
mechanism, and the scalability in P2P Network. Compared with a baseline
approach,the proposal architecture shows the effect of the data mining with
better performance in respect to response time and precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1149</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1149</id><created>2011-09-06</created><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author><author><keyname>Hlavac</keyname><forenames>Vaclav</forenames></author></authors><title>On Partial Opimality by Auxiliary Submodular Problems</title><categories>cs.DM cs.CV math.OC</categories><comments>9 pages, 0 figures; Control Systems and Computers #2/2011, Special
  issue: &quot;Optimal Labeling Problem in Structural Pattern Recognition&quot;, pp.
  71-78, issn 0130-5395</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we prove several relations between three different energy
minimization techniques. A recently proposed methods for determining a provably
optimal partial assignment of variables by Ivan Kovtun (IK), the linear
programming relaxation approach (LP) and the popular expansion move algorithm
by Yuri Boykov. We propose a novel sufficient condition of optimal partial
assignment, which is based on LP relaxation and called LP-autarky. We show that
methods of Kovtun, which build auxiliary submodular problems, fulfill this
sufficient condition. The following link is thus established: LP relaxation
cannot be tightened by IK. For non-submodular problems this is a non-trivial
result. In the case of two labels, LP relaxation provides optimal partial
assignment, known as persistency, which, as we show, dominates IK. Relating IK
with expansion move, we show that the set of fixed points of expansion move
with any &quot;truncation&quot; rule for the initial problem and the problem restricted
by one-vs-all method of IK would coincide -- i.e. expansion move cannot be
improved by this method. In the case of two labels, expansion move with a
particular truncation rule coincide with one-vs-all method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1151</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1151</id><created>2011-09-06</created><updated>2011-09-07</updated><authors><author><keyname>Tebbi</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Attari</keyname><forenames>Mahmoud Ahmadian</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>An Achievable Rate Region for a Two-Relay Network with
  Receiver-Transmitter Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a relay network with two relays and a feedback link from the
receiver to the sender. To obtain the achievability result, we use
compress-and-forward and random binning techniques combined with deterministic
binning and restricted decoding. Moreover, we use joint decoding technique to
decode the relays' compressed information to achieve a higher rate in the
receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1152</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1152</id><created>2011-09-06</created><updated>2012-10-11</updated><authors><author><keyname>Fournier</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Vigneron</keyname><forenames>Antoine</forenames></author></authors><title>A deterministic algorithm for fitting a step function to a weighted
  point-set</title><categories>cs.DS cs.CG</categories><comments>5 pages, 2 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of n points in the plane, each point having a positive weight,
and an integer k&gt;0, we present an optimal O(n \log n)-time deterministic
algorithm to compute a step function with k steps that minimizes the maximum
weighted vertical distance to the input points. It matches the expected time
bound of the best known randomized algorithm for this problem. Our approach
relies on Cole's improved parametric searching technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1168</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1168</id><created>2011-09-06</created><authors><author><keyname>Rajaei</keyname><forenames>Arezoo</forenames></author><author><keyname>Dastjerdi</keyname><forenames>Ahmad Baraani</forenames></author><author><keyname>Aghaee</keyname><forenames>Nasser Ghasem</forenames></author></authors><title>An Extension of Semantic Proximity for Fuzzy Multivalued Dependencies in
  Fuzzy Relational Database</title><categories>cs.DB cs.IR</categories><comments>13 pages, 2 tables, Journal</comments><journal-ref>International Journal of Database Management Systems (IJDMS),
  Vol.3, No.3, August 2011, 157-169</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the development of fuzzy logic theory by Lotfi Zadeh, its
applications were investigated by researchers in different fields. Presenting
and working with uncertain data is a complex problem. To solve for such a
complex problem, the structure of relationships and operators dependent on such
relationships must be repaired. The fuzzy database has integrity limitations
including data dependencies. In this paper, first fuzzy multivalued dependency
based semantic proximity and its problems are studied. To solve these problems,
the semantic proximity's formula is modified, and fuzzy multivalued dependency
based on the concept of extension of semantic proximity with \alpha degree is
defined in fuzzy relational database which includes Crisp, NULL and fuzzy
values, and also inference rules for this dependency are defined, and their
completeness is proved. Finally, we will show that fuzzy functional dependency
based on this concept is a special case of fuzzy multivalued dependency in
fuzzy relational database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1173</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1173</id><created>2011-09-06</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Which cities' paper output and citation impact are above expectation in
  information science? Some improvements of our previous mapping approaches</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bornmann and Leydesdorff (in press) proposed methods based on Web-of-Science
data to identify field-specific excellence in cities where highly-cited papers
were published more frequently than can be expected. Top performers in output
are cities in which authors are located who publish a number of highly-cited
papers that is statistically significantly higher than can be expected for
these cities. Using papers published between 1989 and 2009 in information
science improvements to the methods of Bornmann and Leydesdorff (in press) are
presented and an alternative mapping approach based on the indicator I3 is
introduced here. The I3 indicator was introduced by Leydesdorff and Bornmann
(in press).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1175</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1175</id><created>2011-09-06</created><updated>2012-03-16</updated><authors><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author><author><keyname>Shu</keyname><forenames>Chang</forenames></author></authors><title>Estimating 3D Human Shapes from Measurements</title><categories>cs.CV cs.GR</categories><comments>Added more experiments</comments><journal-ref>Machine Vision and Applications, 24(6):1133-1147, 2013</journal-ref><doi>10.1007/s00138-012-0472-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent advances in 3-D imaging technologies give rise to databases of
human shapes, from which statistical shape models can be built. These
statistical models represent prior knowledge of the human shape and enable us
to solve shape reconstruction problems from partial information. Generating
human shape from traditional anthropometric measurements is such a problem,
since these 1-D measurements encode 3-D shape information. Combined with a
statistical shape model, these easy-to-obtain measurements can be leveraged to
create 3D human shapes. However, existing methods limit the creation of the
shapes to the space spanned by the database and thus require a large amount of
training data. In this paper, we introduce a technique that extrapolates the
statistically inferred shape to fit the measurement data using nonlinear
optimization. This method ensures that the generated shape is both human-like
and satisfies the measurement conditions. We demonstrate the effectiveness of
the method and compare it to existing approaches through extensive experiments,
using both synthetic data and real human measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1186</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1186</id><created>2011-09-06</created><updated>2011-10-12</updated><authors><author><keyname>Zhou</keyname><forenames>Yan-Bo</forenames></author><author><keyname>L&#xfc;</keyname><forenames>Linyuan</forenames></author><author><keyname>Li</keyname><forenames>Menghui</forenames></author></authors><title>Quantifying the influence of scientists and their publications:
  Distinguish prestige from popularity</title><categories>cs.DL physics.soc-ph</categories><comments>9 pages, 10 figures, 2 tables</comments><journal-ref>New J Phys 14 (2012) 033033</journal-ref><doi>10.1088/1367-2630/14/3/033033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of citations is a widely used metric to evaluate the scientific
credit of papers, scientists and journals. However, it does happen that a paper
with fewer citations from prestigious scientists is of higher influence than
papers with more citations. In this paper, we argue that from whom the paper is
being cited is of higher significance than merely the number of received
citations. Accordingly, we propose an interactive model on author-paper
bipartite networks as well as an iterative algorithm to get better rankings for
scientists and their publications. The main advantage of this method is
twofold: (i) it is a parameter-free algorithm; (ii) it considers the
relationship between the prestige of scientists and the quality of their
publications. We conducted real experiments on publications in econophysics,
and applied this method to evaluate the influences of related scientific
journals. The comparisons between the rankings by our method and simple
citation counts suggest that our method is effective to distinguish prestige
from popularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1202</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1202</id><created>2011-09-06</created><authors><author><keyname>Raorane</keyname><forenames>Abhijit</forenames></author><author><keyname>Kulkarni</keyname><forenames>R. V.</forenames></author></authors><title>Data Mining Techniques: A Source for Consumer Behavior Analysis</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various studies on consumer purchasing behaviors have been presented and used
in real problems. Data mining techniques are expected to be a more effective
tool for analyzing consumer behaviors. However, the data mining method has
disadvantages as well as advantages. Therefore, it is important to select
appropriate techniques to mine databases. The objective of this paper is to
know consumer behavior, his psychological condition at the time of purchase and
how suitable data mining method apply to improve conventional method. Moreover,
in an experiment, association rule is employed to mine rules for trusted
customers using sales data in a super market industry
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1211</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1211</id><created>2011-09-06</created><authors><author><keyname>Ramya</keyname><forenames>C.</forenames></author><author><keyname>Kavitha</keyname><forenames>G.</forenames></author></authors><title>An Efficient Preprocessing Methodology for Discovering Patterns and
  Clustering of Web Users using a Dynamic ART1 Neural Network</title><categories>cs.NE</categories><comments>6 pages; International Conference on Information Processing,
  august-2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a complete preprocessing methodology for discovering patterns
in web usage mining process to improve the quality of data by reducing the
quantity of data has been proposed. A dynamic ART1 neural network clustering
algorithm to group users according to their Web access patterns with its neat
architecture is also proposed. Several experiments are conducted and the
results show the proposed methodology reduces the size of Web log files down to
73-82% of the initial size and the proposed ART1 algorithm is dynamic and
learns relatively stable quality clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1214</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1214</id><created>2011-09-06</created><updated>2011-09-07</updated><authors><author><keyname>Doan</keyname><forenames>Minh Dang</forenames></author><author><keyname>Keviczky</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>De Schutter</keyname><forenames>Bart</forenames></author></authors><title>A distributed optimization-based approach for hierarchical model
  predictive control of large-scale systems with coupled dynamics and
  constraints</title><categories>math.OC cs.MA cs.SY</categories><comments>This is the extended version of our paper at the 50th IEEE Conference
  on Decision and Control and European Control Conference, Orlando, Florida,
  Dec. 2011. In this version the proofs are provided</comments><report-no>Tech. rep. 11-039, Delft Center for Systems and Control, Delft
  University of Technology, Delft, The Netherlands</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a hierarchical model predictive control approach for large-scale
systems based on dual decomposition. The proposed scheme allows coupling in
both dynamics and constraints between the subsystems and generates a primal
feasible solution within a finite number of iterations, using primal averaging
and a constraint tightening approach. The primal update is performed in a
distributed way and does not require exact solutions, while the dual problem
uses an approximate subgradient method. Stability of the scheme is established
using bounded suboptimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1231</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1231</id><created>2011-09-06</created><authors><author><keyname>Cambazard</keyname><forenames>Hadrien</forenames></author><author><keyname>Mehta</keyname><forenames>Deepak</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Ruffini</keyname><forenames>Marco</forenames></author><author><keyname>Payne</keyname><forenames>David</forenames></author><author><keyname>Doyle</keyname><forenames>Linda</forenames></author></authors><title>A Combinatorial Optimisation Approach to Designing Dual-Parented
  Long-Reach Passive Optical Networks</title><categories>cs.AI</categories><comments>University of Ulster, Intelligent System Research Centre, technical
  report series. ISSN 2041-6407</comments><journal-ref>Proceedings of the 22nd Irish Conference on Artificial
  Intelligence and Cognitive Science (AICS 2011), pp. 26-35, Derry, UK</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an application focused on the design of resilient long-reach
passive optical networks. We specifically consider dual-parented networks
whereby each customer must be connected to two metro sites via local exchange
sites. An important property of such a placement is resilience to single metro
node failure. The objective of the application is to determine the optimal
position of a set of metro nodes such that the total optical fibre length is
minimized. We prove that this problem is NP-Complete. We present two
alternative combinatorial optimisation approaches to finding an optimal metro
node placement using: a mixed integer linear programming (MIP) formulation of
the problem; and, a hybrid approach that uses clustering as a preprocessing
step. We consider a detailed case-study based on a network for Ireland. The
hybrid approach scales well and finds solutions that are close to optimal, with
a runtime that is two orders-of-magnitude better than the MIP model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1247</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1247</id><created>2011-09-06</created><authors><author><keyname>Dongre</keyname><forenames>Vikas J</forenames></author><author><keyname>Mankar</keyname><forenames>Vijay H</forenames></author></authors><title>Devnagari document segmentation using histogram approach</title><categories>cs.CV</categories><comments>8 pages; 4 figures; 8 tables; journal paper: International Journal of
  Computer Science, Engineering and Information Technology (IJCSEIT), Vol.1,
  No.3, August 2011</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.1, No.3, 2011, 46-53</journal-ref><doi>10.5121/ijcseit.2011.1305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document segmentation is one of the critical phases in machine recognition of
any language. Correct segmentation of individual symbols decides the accuracy
of character recognition technique. It is used to decompose image of a sequence
of characters into sub images of individual symbols by segmenting lines and
words. Devnagari is the most popular script in India. It is used for writing
Hindi, Marathi, Sanskrit and Nepali languages. Moreover, Hindi is the third
most popular language in the world. Devnagari documents consist of vowels,
consonants and various modifiers. Hence proper segmentation of Devnagari word
is challenging. A simple histogram based approach to segment Devnagari
documents is proposed in this paper. Various challenges in segmentation of
Devnagari script are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1248</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1248</id><created>2011-09-06</created><authors><author><keyname>Cattafi</keyname><forenames>Massimiliano</forenames></author><author><keyname>Gavanelli</keyname><forenames>Marco</forenames></author><author><keyname>Nonato</keyname><forenames>Maddalena</forenames></author><author><keyname>Alvisi</keyname><forenames>Stefano</forenames></author><author><keyname>Franchini</keyname><forenames>Marco</forenames></author></authors><title>Optimal Placement of Valves in a Water Distribution Network with CLP(FD)</title><categories>cs.LO</categories><comments>Best paper award at the 27th International Conference on Logic
  Programming - ICLP 2011; Theory and Practice of Logic Programming, (ICLP'11)
  Special Issue, volume 11, issue 4-5, 2011</comments><acm-class>D.1.6; G.1.6</acm-class><doi>10.1017/S1471068411000275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new application of logic programming to a real-life
problem in hydraulic engineering. The work is developed as a collaboration of
computer scientists and hydraulic engineers, and applies Constraint Logic
Programming to solve a hard combinatorial problem. This application deals with
one aspect of the design of a water distribution network, i.e., the valve
isolation system design.
  We take the formulation of the problem by Giustolisi and Savic (2008) and
show how, thanks to constraint propagation, we can get better solutions than
the best solution known in the literature for the Apulian distribution network.
  We believe that the area of the so-called hydroinformatics can benefit from
the techniques developed in Constraint Logic Programming and possibly from
other areas of logic programming, such as Answer Set Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1251</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1251</id><created>2011-09-06</created><authors><author><keyname>Chen</keyname><forenames>Yushan</forenames></author><author><keyname>Ding</keyname><forenames>Xu Chu</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Synthesis of Distributed Control and Communication Schemes from Global
  LTL Specifications</title><categories>cs.RO cs.SY math.OC</categories><comments>Technical Report accompanying an accepted paper for CDC2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a technique for synthesis of control and communication
strategies for a team of agents from a global task specification given as a
Linear Temporal Logic (LTL) formula over a set of properties that can be
satisfied by the agents. We consider a purely discrete scenario, in which the
dynamics of each agent is modeled as a finite transition system. The proposed
computational framework consists of two main steps. First, we extend results
from concurrency theory to check whether the specification is distributable
among the agents. Second, we generate individual control and communication
strategies by using ideas from LTL model checking. We apply the method to
automatically deploy a team of miniature cars in our Robotic Urban-Like
Environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1255</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1255</id><created>2011-09-06</created><authors><author><keyname>Aldridge</keyname><forenames>Matthew</forenames></author></authors><title>Interference Mitigation in Large Random Wireless Networks</title><categories>cs.IT math.IT</categories><comments>PhD thesis, University of Bristol, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in the operation of large wireless networks is how to deal
with interference -- the unwanted signals being sent by transmitters that a
receiver is not interested in. This thesis looks at ways of combating such
interference.
  In Chapters 1 and 2, we outline the necessary information and communication
theory background, including the concept of capacity. We also include an
overview of a new set of schemes for dealing with interference known as
interference alignment, paying special attention to a channel-state-based
strategy called ergodic interference alignment.
  In Chapter 3, we consider the operation of large regular and random networks
by treating interference as background noise. We consider the local performance
of a single node, and the global performance of a very large network.
  In Chapter 4, we use ergodic interference alignment to derive the asymptotic
sum-capacity of large random dense networks. These networks are derived from a
physical model of node placement where signal strength decays over the distance
between transmitters and receivers. (See also arXiv:1002.0235 and
arXiv:0907.5165.)
  In Chapter 5, we look at methods of reducing the long time delays incurred by
ergodic interference alignment. We analyse the tradeoff between reducing delay
and lowering the communication rate. (See also arXiv:1004.0208.)
  In Chapter 6, we outline a problem that is equivalent to the problem of
pooled group testing for defective items. We then present some new work that
uses information theoretic techniques to attack group testing. We introduce for
the first time the concept of the group testing channel, which allows for
modelling of a wide range of statistical error models for testing. We derive
new results on the number of tests required to accurately detect defective
items, including when using sequential `adaptive' tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1264</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1264</id><created>2011-09-06</created><authors><author><keyname>Progsch</keyname><forenames>J.</forenames></author><author><keyname>Ineichen</keyname><forenames>Y.</forenames></author><author><keyname>Adelmann</keyname><forenames>A.</forenames></author></authors><title>A New Vectorization Technique for Expression Templates in C++</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector operations play an important role in high performance computing and
are typically provided by highly optimized libraries that implement the BLAS
(Basic Linear Algebra Subprograms) interface. In C++ templates and operator
overloading allow the implementation of these vector operations as expression
templates which construct custom loops at compile time and providing a more
abstract interface. Unfortunately existing expression template libraries lack
the performance of fast BLAS(Basic Linear Algebra Subprograms) implementations.
This paper presents a new approach - Statically Accelerated Loop Templates
(SALT) - to close this performance gap by combining expression templates with
an aggressive loop unrolling technique. Benchmarks were conducted using the
Intel C++ compiler and GNU Compiler Collection to assess the performance of our
library relative to Intel's Math Kernel Library as well as the Eigen template
library. The results show that the approach is able to provide optimization
comparable to the fastest available BLAS implementations, while retaining the
convenience and flexibility of a template library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1265</identifier>
 <datestamp>2012-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1265</id><created>2011-09-06</created><updated>2012-02-15</updated><authors><author><keyname>Costa</keyname><forenames>Rui A.</forenames></author><author><keyname>Ferreira</keyname><forenames>Diogo</forenames></author><author><keyname>Barros</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>FEBER: Feedback Based Erasure Recovery for Real-Time Multicast over
  802.11 Networks</title><categories>cs.IT cs.MM cs.NI math.IT</categories><comments>This paper has been withdrawn due to changes in the results obtained
  in a different testbed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of broadcasting data streams over a wireless network
for multiple receivers with reliability and timely delivery guarantees. In our
framework, we consider packets that need to be delivered within a given time
interval, after which the packet is no longer useful at the application layer.
We set the notion of critical packet and, based on periodic feedback from the
receivers, we propose a retransmission scheme that will guarantee timely
delivery of such packets, as well as packets that are innovative for other
receivers. Our solution provides a trade-off between packet delivery ratio and
bandwidth use, which contrasts with existing approaches such as FEC and ARQ,
where the focus is on ensuring reliability first, offering no guarantees of
timely delivery of data. We evaluate the performance of our proposal in a
802.11 wireless network testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1275</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1275</id><created>2011-09-06</created><authors><author><keyname>Yordanov</keyname><forenames>Boyan</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>A Formal Verification Approach to the Design of Synthetic Gene Networks</title><categories>cs.SY math.OC q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of genetic networks with specific functions is one of the major
goals of synthetic biology. However, constructing biological devices that work
&quot;as required&quot; remains challenging, while the cost of uncovering flawed designs
experimentally is large. To address this issue, we propose a fully automated
framework that allows the correctness of synthetic gene networks to be formally
verified in silico from rich, high level functional specifications.
  Given a device, we automatically construct a mathematical model from
experimental data characterizing the parts it is composed of. The specific
model structure guarantees that all experimental observations are captured and
allows us to construct finite abstractions through polyhedral operations. The
correctness of the model with respect to temporal logic specifications can then
be verified automatically using methods inspired by model checking.
  Overall, our procedure is conservative but it can filter through a large
number of potential device designs and select few that satisfy the
specification to be implemented and tested further experimentally. Illustrative
examples of the application of our methods to the design of simple synthetic
gene networks are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1276</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1276</id><created>2011-09-06</created><authors><author><keyname>Agrawal</keyname><forenames>Rohan</forenames></author></authors><title>Application of the Modified 2-opt and Jumping Gene Operators in
  Multi-Objective Genetic Algorithm to solve MOTSP</title><categories>cs.AI cs.NE</categories><comments>4 pages, 5 figures Selected in ICNCI 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary Multi-Objective Optimization is becoming a hot research area and
quite a few papers regarding these algorithms have been published. However the
role of local search techniques has not been expanded adequately. This paper
studies the role of a local search technique called 2-opt for the
Multi-Objective Travelling Salesman Problem (MOTSP). A new mutation operator
called Jumping Gene (JG) is also used. Since 2-opt operator was intended for
the single objective TSP, its domain has been expanded to MOTSP in this paper.
This new technique is applied to the list of KroAB100 cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1293</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1293</id><created>2011-09-06</created><updated>2012-07-17</updated><authors><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author></authors><title>Source Coding When the Side Information May Be Delayed</title><categories>cs.IT math.IT</categories><comments>revised July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For memoryless sources, delayed side information at the decoder does not
improve the rate-distortion function. However, this is not the case for more
general sources with memory, as demonstrated by a number of works focusing on
the special case of (delayed) feedforward. In this paper, a setting is studied
in which the encoder is potentially uncertain about the delay with which
measurements of the side information are acquired at the decoder. Assuming a
hidden Markov model for the sources, at first, a single-letter characterization
is given for the set-up where the side information delay is arbitrary and known
at the encoder, and the reconstruction at the destination is required to be
(near) lossless. Then, with delay equal to zero or one source symbol, a
single-letter characterization is given of the rate-distortion region for the
case where side information may be delayed or not, unbeknownst to the encoder.
The characterization is further extended to allow for additional information to
be sent when the side information is not delayed. Finally, examples for binary
and Gaussian sources are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1302</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1302</id><created>2011-09-06</created><authors><author><keyname>Paci</keyname><forenames>Hakik</forenames></author><author><keyname>Kajo</keyname><forenames>Elinda</forenames></author><author><keyname>Tafa</keyname><forenames>Igli</forenames></author><author><keyname>Xhuvani</keyname><forenames>Aleksander</forenames></author></authors><title>Adding a new site in an existing Oracle Multimaster replication without
  quiescing the replication</title><categories>cs.DB</categories><comments>9 pages, 4 figures, in International Journal of Database Management
  Systems (IJDMS) (2011)</comments><journal-ref>International Journal of Database Management Systems (IJDMS),
  Vol.3, No.3, (2011) 58-67</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new solution, which adds a new database server on an
existing Oracle Multimaster Data replication system with Online Instantiation
method. During this time the system is down, because we cannot execute DML
statements on replication objects but we can only make queries. The time for
adding the new database server depends on the number of objects, on the
replication group and on the network conditions. We propose to add a new layer
between replication objects and the database sessions, which contain DML
statements. The layer eliminates the system down time exploiting our developed
packages. The packages will be active only during the addition of a new site
process and will modify all DML statements and queries based on replication
objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1314</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1314</id><created>2011-09-06</created><authors><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>Togelius</keyname><forenames>Julian</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Measuring Intelligence through Games</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial general intelligence (AGI) refers to research aimed at tackling
the full problem of artificial intelligence, that is, create truly intelligent
agents. This sets it apart from most AI research which aims at solving
relatively narrow domains, such as character recognition, motion planning, or
increasing player satisfaction in games. But how do we know when an agent is
truly intelligent? A common point of reference in the AGI community is Legg and
Hutter's formal definition of universal intelligence, which has the appeal of
simplicity and generality but is unfortunately incomputable. Games of various
kinds are commonly used as benchmarks for &quot;narrow&quot; AI research, as they are
considered to have many important properties. We argue that many of these
properties carry over to the testing of general intelligence as well. We then
sketch how such testing could practically be carried out. The central part of
this sketch is an extension of universal intelligence to deal with finite time,
and the use of sampling of the space of games expressed in a suitably biased
game description language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1317</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1317</id><created>2011-09-06</created><authors><author><keyname>Vaezipoor</keyname><forenames>Pashootan</forenames></author><author><keyname>Mitchell</keyname><forenames>David</forenames></author><author><keyname>Mari&#xeb;n</keyname><forenames>Maarten</forenames></author></authors><title>Lifted Unit Propagation for Effective Grounding</title><categories>cs.LO cs.AI</categories><comments>Appears in the Proceedings of the 19th International Conference on
  Applications of Declarative Programming and Knowledge Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A grounding of a formula $\phi$ over a given finite domain is a ground
formula which is equivalent to $\phi$ on that domain. Very effective
propositional solvers have made grounding-based methods for problem solving
increasingly important, however for realistic problem domains and instances,
the size of groundings is often problematic. A key technique in ground (e.g.,
SAT) solvers is unit propagation, which often significantly reduces ground
formula size even before search begins. We define a &quot;lifted&quot; version of unit
propagation which may be carried out prior to grounding, and describe
integration of the resulting technique into grounding algorithms. We describe
an implementation of the method in a bottom-up grounder, and an experimental
study of its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1325</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1325</id><created>2011-09-06</created><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author></authors><title>Get the Most out of Your Sample: Optimal Unbiased Estimators using
  Partial Information</title><categories>cs.DB cs.DS cs.NI math.ST stat.TH</categories><comments>This is a full version of a PODS 2011 paper</comments><acm-class>G.3; E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random sampling is an essential tool in the processing and transmission of
data. It is used to summarize data too large to store or manipulate and meet
resource constraints on bandwidth or battery power. Estimators that are applied
to the sample facilitate fast approximate processing of queries posed over the
original data and the value of the sample hinges on the quality of these
estimators.
  Our work targets data sets such as request and traffic logs and sensor
measurements, where data is repeatedly collected over multiple {\em instances}:
time periods, locations, or snapshots.
  We are interested in queries that span multiple instances, such as distinct
counts and distance measures over selected records. These queries are used for
applications ranging from planning to anomaly and change detection.
  Unbiased low-variance estimators are particularly effective as the relative
error decreases with the number of selected record keys.
  The Horvitz-Thompson estimator, known to minimize variance for sampling with
&quot;all or nothing&quot; outcomes (which reveals exacts value or no information on
estimated quantity), is not optimal for multi-instance operations for which an
outcome may provide partial information.
  We present a general principled methodology for the derivation of (Pareto)
optimal unbiased estimators over sampled instances and aim to understand its
potential. We demonstrate significant improvement in estimate accuracy of
fundamental queries for common sampling schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1342</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1342</id><created>2011-09-06</created><authors><author><keyname>Shi</keyname><forenames>Ziqiang</forenames></author><author><keyname>Zheng</keyname><forenames>Tieran</forenames></author><author><keyname>Han</keyname><forenames>Jiqing</forenames></author></authors><title>Trace Norm Regularized Tensor Classification and Its Online Learning
  Approaches</title><categories>cs.NA math.OC</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an algorithm to classify tensor data. Our
methodology is built on recent studies about matrix classification with the
trace norm constrained weight matrix and the tensor trace norm. Similar to
matrix classification, the tensor classification is formulated as a convex
optimization problem which can be solved by using the off-the-shelf accelerated
proximal gradient (APG) method. However, there are no analytic solutions as the
matrix case for the updating of the weight tensors via the proximal gradient.
To tackle this problem, the Douglas-Rachford splitting technique and the
alternating direction method of multipliers (ADM) used in tensor completion are
adapted to update the weight tensors. Further more, due to the demand of real
applications, we also propose its online learning approaches. Experiments
demonstrate the efficiency of the methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1355</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1355</id><created>2011-09-07</created><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Localization on low-order eigenvectors of data matrices</title><categories>cs.DM cs.CE cs.LG</categories><comments>21 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eigenvector localization refers to the situation when most of the components
of an eigenvector are zero or near-zero. This phenomenon has been observed on
eigenvectors associated with extremal eigenvalues, and in many of those cases
it can be meaningfully interpreted in terms of &quot;structural heterogeneities&quot; in
the data. For example, the largest eigenvectors of adjacency matrices of large
complex networks often have most of their mass localized on high-degree nodes;
and the smallest eigenvectors of the Laplacians of such networks are often
localized on small but meaningful community-like sets of nodes. Here, we
describe localization associated with low-order eigenvectors, i.e.,
eigenvectors corresponding to eigenvalues that are not extremal but that are
&quot;buried&quot; further down in the spectrum. Although we have observed it in several
unrelated applications, this phenomenon of low-order eigenvector localization
defies common intuitions and simple explanations, and it creates serious
difficulties for the applicability of popular eigenvector-based machine
learning and data analysis tools. After describing two examples where low-order
eigenvector localization arises, we present a very simple model that
qualitatively reproduces several of the empirically-observed results. This
model suggests certain coarse structural similarities among the
seemingly-unrelated applications where we have observed low-order eigenvector
localization, and it may be used as a diagnostic tool to help extract insight
from data graphs when such low-order eigenvector localization is present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1359</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1359</id><created>2011-09-07</created><authors><author><keyname>Pratondo</keyname><forenames>Agus</forenames></author></authors><title>Representation for alphanumeric data type based on space and speed case
  study: Student ID of X university</title><categories>cs.DB</categories><doi>10.5121/ijdms.2011.3303</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  ID is derived from the word identity, derived from the first two characters
in the word. ID is used to distinguish between an entity to another entity.
Student ID (SID) is the key differentiator between a student with other
students. On the concept of database, the differentiator is unique. SID can be
numbers, letters, or a combination of both (alphanumeric). Viewed from the
daily context, it is not important to determine which a SID belongs to the type
of data. However, when reviewed on database design, determining the type of
data, including SID in this case, is important. Problems arise because there is
a contradiction between the data type viewed from the data characteristic and
practical needs. Type of data for SID is a string, if it is evaluated from the
basic concepts and its characteristic. It is acceptable because SID consists of
a set of numbers which will not be meaningful if applied arithmetic operations
like addition, subtraction, multiplication and division. But in terms of
computer organization, data representation type will determine how much data
space requirements, speed of access, and speed of operation. By considering the
constraints of space and speed on the experiments conducted, SID is better
expressed as an integer rather than a set of characters.
  KEYWORDS aphanumeric,representation, string, integer, space, speed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1363</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1363</id><created>2011-09-07</created><authors><author><keyname>Calcagno</keyname><forenames>Cristina</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Coppo</keyname><forenames>Mario</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Damiani</keyname><forenames>Ferruccio</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Drocco</keyname><forenames>Maurizio</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Sciacca</keyname><forenames>Eva</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Spinella</keyname><forenames>Salvatore</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Troina</keyname><forenames>Angelo</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author></authors><title>Modelling Spatial Interactions in the Arbuscular Mycorrhizal Symbiosis
  using the Calculus of Wrapped Compartments</title><categories>cs.LO cs.CE</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><acm-class>F.4.2; I.6; J.3</acm-class><journal-ref>EPTCS 67, 2011, pp. 3-18</journal-ref><doi>10.4204/EPTCS.67.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arbuscular mycorrhiza (AM) is the most wide-spread plant-fungus symbiosis on
earth. Investigating this kind of symbiosis is considered one of the most
promising ways to develop methods to nurture plants in more natural manners,
avoiding the complex chemical productions used nowadays to produce artificial
fertilizers. In previous work we used the Calculus of Wrapped Compartments
(CWC) to investigate different phases of the AM symbiosis. In this paper, we
continue this line of research by modelling the colonisation of the plant root
cells by the fungal hyphae spreading in the soil. This study requires the
description of some spatial interaction. Although CWC has no explicit feature
modelling a spatial geometry, the compartment labelling feature can be
effectively exploited to define a discrete surface topology outlining the
relevant sectors which determine the spatial properties of the system under
consideration. Different situations and interesting spatial properties can be
modelled and analysed in such a lightweight framework (which has not an
explicit notion of geometry with coordinates and spatial metrics), thus
exploiting the existing CWC simulation tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1364</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1364</id><created>2011-09-07</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames><affiliation>Dept. of Mathematics and Informatics, University of Trieste, Italy.</affiliation></author><author><keyname>Policriti</keyname><forenames>Alberto</forenames><affiliation>Dept. of Mathematics and Informatics, University of Udine, Italy. Institute of Applied Genomics, Udine, Italy.</affiliation></author></authors><title>Programmable models of growth and mutation of cancer-cell populations</title><categories>cs.CE q-bio.CB</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><acm-class>J.3; I.6.5; I.6.2</acm-class><journal-ref>EPTCS 67, 2011, pp. 19-33</journal-ref><doi>10.4204/EPTCS.67.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a systematic approach to construct mathematical
models describing populations of cancer-cells at different stages of disease
development. The methodology we propose is based on stochastic Concurrent
Constraint Programming, a flexible stochastic modelling language. The
methodology is tested on (and partially motivated by) the study of prostate
cancer. In particular, we prove how our method is suitable to systematically
reconstruct different mathematical models of prostate cancer growth - together
with interactions with different kinds of hormone therapy - at different levels
of refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1365</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1365</id><created>2011-09-07</created><authors><author><keyname>Galpin</keyname><forenames>Vashti</forenames></author><author><keyname>Hillston</keyname><forenames>Jane</forenames></author><author><keyname>Ciocchetta</keyname><forenames>Federica</forenames></author></authors><title>A semi-quantitative equivalence for abstracting from fast reactions</title><categories>cs.CE q-bio.QM</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 67, 2011, pp. 34-49</journal-ref><doi>10.4204/EPTCS.67.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic equivalences are used in process algebra to capture the notion of
similar behaviour, and this paper proposes a semi-quantitative equivalence for
a stochastic process algebra developed for biological modelling. We consider
abstracting away from fast reactions as suggested by the Quasi-Steady-State
Assumption. We define a fast-slow bisimilarity based on this idea. We also show
congruence under an appropriate condition for the cooperation operator of
Bio-PEPA. The condition requires that there is no synchronisation over fast
actions, and this distinguishes fast-slow bisimilarity from weak bisimilarity.
We also show congruence for an operator which extends the reactions available
for a species. We characterise models for which it is only necessary to
consider the matching of slow transitions and we illustrate the equivalence on
two models of competitive inhibition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1366</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1366</id><created>2011-09-07</created><authors><author><keyname>Bioglio</keyname><forenames>Livio</forenames></author></authors><title>A Minimal OO Calculus for Modelling Biological Systems</title><categories>cs.CE cs.LO</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 67, 2011, pp. 50-64</journal-ref><doi>10.4204/EPTCS.67.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a minimal object oriented core calculus for
modelling the biological notion of type that arises from biological ontologies
in formalisms based on term rewriting. This calculus implements encapsulation,
method invocation, subtyping and a simple formof overriding inheritance, and it
is applicable to models designed in the most popular term-rewriting formalisms.
The classes implemented in a formalism can be used in several models, like
programming libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1367</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1367</id><created>2011-09-07</created><authors><author><keyname>Yuan</keyname><forenames>Qixia</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Pang</keyname><forenames>Jun</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Mauw</keyname><forenames>Sjouke</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Trairatphisan</keyname><forenames>Panuwat</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Wiesinger</keyname><forenames>Monique</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Sauter</keyname><forenames>Thomas</forenames><affiliation>University of Luxembourg</affiliation></author></authors><title>A Study of the PDGF Signaling Pathway with PRISM</title><categories>cs.CE cs.LO q-bio.QM</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 67, 2011, pp. 65-81</journal-ref><doi>10.4204/EPTCS.67.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply the probabilistic model checker PRISM to the analysis
of a biological system -- the Platelet-Derived Growth Factor (PDGF) signaling
pathway, demonstrating in detail how this pathway can be analyzed in PRISM. We
show that quantitative verification can yield a better understanding of the
PDGF signaling pathway.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1368</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1368</id><created>2011-09-07</created><authors><author><keyname>Li&#xf2;</keyname><forenames>Pietro</forenames><affiliation>Computer Laboratory. University of Cambridge. United Kingdom</affiliation></author><author><keyname>Merelli</keyname><forenames>Emanuela</forenames><affiliation>School of Science and Technology. University of Camerino. Italy</affiliation></author><author><keyname>Paoletti</keyname><forenames>Nicola</forenames><affiliation>School of Science and Technology. University of Camerino. Italy</affiliation></author></authors><title>Multiple verification in computational modeling of bone pathologies</title><categories>cs.LO cs.CE cs.SY math.OC q-bio.TO</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><acm-class>D.2.4; I.6; J.3</acm-class><journal-ref>EPTCS 67, 2011, pp. 82-96</journal-ref><doi>10.4204/EPTCS.67.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model checking approach to diagnose the emerging of bone
pathologies. The implementation of a new model of bone remodeling in PRISM has
led to an interesting characterization of osteoporosis as a defective bone
remodeling dynamics with respect to other bone pathologies. Our approach allows
to derive three types of model checking-based diagnostic estimators. The first
diagnostic measure focuses on the level of bone mineral density, which is
currently used in medical practice. In addition, we have introduced a novel
diagnostic estimator which uses the full patient clinical record, here
simulated using the modeling framework. This estimator detects rapid (months)
negative changes in bone mineral density. Independently of the actual bone
mineral density, when the decrease occurs rapidly it is important to alarm the
patient and monitor him/her more closely to detect insurgence of other bone
co-morbidities. A third estimator takes into account the variance of the bone
density, which could address the investigation of metabolic syndromes, diabetes
and cancer. Our implementation could make use of different logical combinations
of these statistical estimators and could incorporate other biomarkers for
other systemic co-morbidities (for example diabetes and thalassemia). We are
delighted to report that the combination of stochastic modeling with formal
methods motivate new diagnostic framework for complex pathologies. In
particular our approach takes into consideration important properties of
biosystems such as multiscale and self-adaptiveness. The multi-diagnosis could
be further expanded, inching towards the complexity of human diseases. Finally,
we briefly introduce self-adaptiveness in formal methods which is a key
property in the regulative mechanisms of biological systems and well known in
other mathematical and engineering areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1396</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1396</id><created>2011-09-07</created><updated>2012-06-06</updated><authors><author><keyname>Orm&#xe1;ndi</keyname><forenames>R&#xf3;bert</forenames></author><author><keyname>Heged&#xfc;s</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Jelasity</keyname><forenames>M&#xe1;rk</forenames></author></authors><title>Gossip Learning with Linear Models on Fully Distributed Data</title><categories>cs.LG cs.DC</categories><comments>The paper was published in the journal Concurrency and Computation:
  Practice and Experience
  http://onlinelibrary.wiley.com/journal/10.1002/%28ISSN%291532-0634 (DOI:
  http://dx.doi.org/10.1002/cpe.2858). The modifications are based on the
  suggestions from the reviewers</comments><doi>10.1002/cpe.2858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning over fully distributed data poses an important problem in
peer-to-peer (P2P) applications. In this model we have one data record at each
network node, but without the possibility to move raw data due to privacy
considerations. For example, user profiles, ratings, history, or sensor
readings can represent this case. This problem is difficult, because there is
no possibility to learn local models, the system model offers almost no
guarantees for reliability, yet the communication cost needs to be kept low.
Here we propose gossip learning, a generic approach that is based on multiple
models taking random walks over the network in parallel, while applying an
online learning algorithm to improve themselves, and getting combined via
ensemble learning methods. We present an instantiation of this approach for the
case of classification with linear models. Our main contribution is an ensemble
learning method which---through the continuous combination of the models in the
network---implements a virtual weighted voting mechanism over an exponential
number of models at practically no extra cost as compared to independent random
walks. We prove the convergence of the method theoretically, and perform
extensive experiments on benchmark datasets. Our experimental analysis
demonstrates the performance and robustness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1409</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1409</id><created>2011-09-07</created><updated>2012-04-27</updated><authors><author><keyname>Pons-Pons</keyname><forenames>M.</forenames></author><author><keyname>Johnson</keyname><forenames>P. A.</forenames></author><author><keyname>Rosas-Casals</keyname><forenames>M.</forenames></author><author><keyname>Sureda</keyname><forenames>B.</forenames></author><author><keyname>Jover</keyname><forenames>E.</forenames></author></authors><title>A georeferenced Agent-Based Model to analyze the climate change impacts
  on the Andorra winter tourism</title><categories>cs.MA</categories><comments>24 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a georeferenced agent-based model to analyze the climate
change impacts on the ski industry in Andorra and the effect of snowmaking as
future adaptation strategy. The present study is the first attempt to analyze
the ski industry in the Pyrenees region and will contribute to a better
understanding of the vulnerability of Andorran ski resorts and the suitability
of snowmaking as potential adaptation strategy to climate change. The resulting
model can be used as a planning support tool to help local stakeholders
understand the vulnerability and potential impacts of climate change. This
model can be used in the decision-making process of designing and developing
appropriate sustainable adaptation strategies to future climate variability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1420</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1420</id><created>2011-09-07</created><authors><author><keyname>Bone</keyname><forenames>Paul</forenames></author><author><keyname>Somogyi</keyname><forenames>Zoltan</forenames></author><author><keyname>Schachte</keyname><forenames>Peter</forenames></author></authors><title>Estimating the overlap between dependent computations for automatic
  parallelization</title><categories>cs.PL cs.DC</categories><journal-ref>Theory and Practice of Logic Programming, 27th Int'l. Conference
  on Logic Programming (ICLP'11) Special Issue, volume 11, issue 4-5, pages
  575-587. July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers working on the automatic parallelization of programs have long
known that too much parallelism can be even worse for performance than too
little, because spawning a task to be run on another CPU incurs overheads.
Autoparallelizing compilers have therefore long tried to use granularity
analysis to ensure that they only spawn off computations whose cost will
probably exceed the spawn-off cost by a comfortable margin. However, this is
not enough to yield good results, because data dependencies may \emph{also}
limit the usefulness of running computations in parallel. If one computation
blocks almost immediately and can resume only after another has completed its
work, then the cost of parallelization again exceeds the benefit.
  We present a set of algorithms for recognizing places in a program where it
is worthwhile to execute two or more computations in parallel that pay
attention to the second of these issues as well as the first. Our system uses
profiling information to compute the times at which a procedure call consumes
the values of its input arguments and the times at which it produces the values
of its output arguments. Given two calls that may be executed in parallel, our
system uses the times of production and consumption of the variables they share
to determine how much their executions would overlap if they were run in
parallel, and therefore whether executing them in parallel is a good idea or
not.
  We have implemented this technique for Mercury in the form of a tool that
uses profiling data to generate recommendations about what to parallelize, for
the Mercury compiler to apply on the next compilation of the program. We
present preliminary results that show that this technique can yield useful
parallelization speedups, while requiring nothing more from the programmer than
representative input data for the profiling run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1421</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1421</id><created>2011-09-07</created><authors><author><keyname>Bone</keyname><forenames>Paul</forenames></author><author><keyname>Somogyi</keyname><forenames>Zoltan</forenames></author></authors><title>Profiling parallel Mercury programs with ThreadScope</title><categories>cs.PL cs.DC cs.PF</categories><comments>21st Workshop on Logic-based methods in Programming Environments.
  Lexington, Kentucky, July 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behavior of parallel programs is even harder to understand than the
behavior of sequential programs. Parallel programs may suffer from any of the
performance problems affecting sequential programs, as well as from several
problems unique to parallel systems. Many of these problems are quite hard (or
even practically impossible) to diagnose without help from specialized tools.
We present a proposal for a tool for profiling the parallel execution of
Mercury programs, a proposal whose implementation we have already started. This
tool is an adaptation and extension of the ThreadScope profiler that was first
built to help programmers visualize the execution of parallel Haskell programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1454</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1454</id><created>2011-09-06</created><authors><author><keyname>Ismail</keyname><forenames>Anis</forenames></author><author><keyname>Hajjar</keyname><forenames>Abd El Salam AL</forenames></author><author><keyname>Hajjar</keyname><forenames>Mohammad</forenames></author></authors><title>A Prototype System for Controlling a Computer by Head Movements and
  Voice Commands</title><categories>cs.HC</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new prototype system for controlling a PC by head
movements and also with voice commands. Our system is a multimodal interface
concerned with controlling the computer. The selected modes of interaction are
speech and gestures. We are seeing the revolutionary of computers and
information technologies into daily practice. Healthy people use keyboard,
mouse, trackball, or touchpad for controlling the PC. However these peripheries
are usually not suitable for handicapped people. They may have problems using
these standard peripheries, for example when they suffer from myopathy, or
cannot move their hands after an injury. Our system has been developed to
provide computer access for people with severe disabilities. This system tracks
the computer user's Head movements with a video camera and translates them into
the movements of the mouse pointer on the screen and the voice as button
presses. Therefore we are coming with a proposal system that can be used with
handicapped people to control the PC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1460</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1460</id><created>2011-09-07</created><updated>2011-12-06</updated><authors><author><keyname>Aleksenko</keyname><forenames>A.</forenames></author><author><keyname>Lakshtanov</keyname><forenames>E.</forenames></author></authors><title>Finiteness of the playing time in 'Beggar-my-neighbour' card game</title><categories>math.PR cs.IT math.DS math.IT</categories><comments>The paper has been submitted to the Problems of Information
  Transmission. It will appear in English in the English version of the journal</comments><journal-ref>Problems of Information Transmission, Volume 49, Issue 2, pp
  163-166, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that in card games similar to 'Beggar-my-neighbour' the
mathematical expectation of the playing time is finite, provided that the
player who starts the round is determined randomly and the deck is shuffled
when the trick is added. The result holds for the generic setting of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1465</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1465</id><created>2011-09-07</created><authors><author><keyname>Bachmaier</keyname><forenames>Christian</forenames></author><author><keyname>Brandenburg</keyname><forenames>Franz J.</forenames></author><author><keyname>Effinger</keyname><forenames>Philip</forenames></author><author><keyname>Gutwenger</keyname><forenames>Carsten</forenames></author><author><keyname>Katajainen</keyname><forenames>Jyrki</forenames></author><author><keyname>Klein</keyname><forenames>Karsten</forenames></author><author><keyname>Sp&#xf6;nemann</keyname><forenames>Miro</forenames></author><author><keyname>Stegmaier</keyname><forenames>Matthias</forenames></author><author><keyname>Wybrow</keyname><forenames>Michael</forenames></author></authors><title>The Open Graph Archive: A Community-Driven Effort</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to evaluate, compare, and tune graph algorithms, experiments on well
designed benchmark sets have to be performed. Together with the goal of
reproducibility of experimental results, this creates a demand for a public
archive to gather and store graph instances. Such an archive would ideally
allow annotation of instances or sets of graphs with additional information
like graph properties and references to the respective experiments and results.
Here we examine the requirements, and introduce a new community project with
the aim of producing an easily accessible library of graphs. Through successful
community involvement, it is expected that the archive will contain a
representative selection of both real-world and generated graph instances,
covering significant application areas as well as interesting classes of
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1480</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1480</id><created>2011-09-07</created><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Rother</keyname><forenames>Carsten</forenames></author></authors><title>Curvature Prior for MRF-based Segmentation and Shape Inpainting</title><categories>cs.CV</categories><comments>17 pages, 16 figures</comments><report-no>CTU--CMP--2011--11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image labeling problems such as segmentation and image reconstruction
are fundamentally ill-posed and suffer from ambiguities and noise. Higher order
image priors encode high level structural dependencies between pixels and are
key to overcoming these problems. However, these priors in general lead to
computationally intractable models. This paper addresses the problem of
discovering compact representations of higher order priors which allow
efficient inference. We propose a framework for solving this problem which uses
a recently proposed representation of higher order functions where they are
encoded as lower envelopes of linear functions. Maximum a Posterior inference
on our learned models reduces to minimizing a pairwise function of discrete
variables, which can be done approximately using standard methods. Although
this is a primarily theoretical paper, we also demonstrate the practical
effectiveness of our framework on the problem of learning a shape prior for
image segmentation and reconstruction. We show that our framework can learn a
compact representation that approximates a prior that encourages low curvature
shapes. We evaluate the approximation accuracy, discuss properties of the
trained model, and show various results for shape inpainting and image
segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1488</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1488</id><created>2011-09-07</created><updated>2012-05-21</updated><authors><author><keyname>I&#xf1;iguez</keyname><forenames>Gerardo</forenames></author><author><keyname>Tag&#xfc;e&#xf1;a-Mart&#xed;nez</keyname><forenames>Julia</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo K.</forenames></author><author><keyname>Barrio</keyname><forenames>R. A.</forenames></author></authors><title>Are Opinions Based on Science: Modelling Social Response to Scientific
  Facts</title><categories>physics.soc-ph cs.CY cs.SI nlin.AO</categories><comments>21 pages, 5 figures. Submitted to PLoS ONE</comments><journal-ref>PLoS ONE 7(8), e42122 (2012)</journal-ref><doi>10.1371/journal.pone.0042122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As scientists we like to think that modern societies and their members base
their views, opinions and behaviour on scientific facts. This is not
necessarily the case, even though we are all (over-) exposed to information
flow through various channels of media, i.e. newspapers, television, radio,
internet, and web. It is thought that this is mainly due to the conflicting
information on the mass media and to the individual attitude (formed by
cultural, educational and environmental factors), that is, one external factor
and another personal factor. In this paper we will investigate the dynamical
development of opinion in a small population of agents by means of a
computational model of opinion formation in a co-evolving network of socially
linked agents. The personal and external factors are taken into account by
assigning an individual attitude parameter to each agent, and by subjecting all
to an external but homogeneous field to simulate the effect of the media. We
then adjust the field strength in the model by using actual data on scientific
perception surveys carried out in two different populations, which allow us to
compare two different societies. We interpret the model findings with the aid
of simple mean field calculations. Our results suggest that scientifically
sound concepts are more difficult to acquire than concepts not validated by
science, since opposing individuals organize themselves in close communities
that prevent opinion consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1494</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1494</id><created>2011-09-07</created><updated>2011-10-28</updated><authors><author><keyname>Butman</keyname><forenames>Ayelet</forenames></author><author><keyname>Clifford</keyname><forenames>Peter</forenames></author><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Lewenstein</keyname><forenames>Noa</forenames></author><author><keyname>Porat</keyname><forenames>Benny</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author></authors><title>Pattern Matching under Polynomial Transformation</title><categories>cs.DS</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of pattern matching problems where a normalising
transformation is applied at every alignment. Normalised pattern matching plays
a key role in fields as diverse as image processing and musical information
processing where application specific transformations are often applied to the
input. By considering the class of polynomial transformations of the input, we
provide fast algorithms and the first lower bounds for both new and old
problems. Given a pattern of length m and a longer text of length n where both
are assumed to contain integer values only, we first show O(n log m) time
algorithms for pattern matching under linear transformations even when wildcard
symbols can occur in the input. We then show how to extend the technique to
polynomial transformations of arbitrary degree. Next we consider the problem of
finding the minimum Hamming distance under polynomial transformation. We show
that, for any epsilon&gt;0, there cannot exist an O(n m^(1-epsilon)) time
algorithm for additive and linear transformations conditional on the hardness
of the classic 3SUM problem. Finally, we consider a version of the Hamming
distance problem under additive transformations with a bound k on the maximum
distance that need be reported. We give a deterministic O(nk log k) time
solution which we then improve by careful use of randomisation to O(n sqrt(k
log k) log n) time for sufficiently small k. Our randomised solution outputs
the correct answer at every position with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1498</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1498</id><created>2011-06-30</created><authors><author><keyname>Di Sciascio</keyname><forenames>E.</forenames></author><author><keyname>Donini</keyname><forenames>F. M.</forenames></author><author><keyname>Mongiello</keyname><forenames>M.</forenames></author></authors><title>Structured Knowledge Representation for Image Retrieval</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  209-257, 2002</journal-ref><doi>10.1613/jair.902</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a structured approach to the problem of retrieval of images by
content and present a description logic that has been devised for the semantic
indexing and retrieval of images containing complex objects. As other
approaches do, we start from low-level features extracted with image analysis
to detect and characterize regions in an image. However, in contrast with
feature-based approaches, we provide a syntax to describe segmented regions as
basic objects and complex objects as compositions of basic ones. Then we
introduce a companion extensional semantics for defining reasoning services,
such as retrieval, classification, and subsumption. These services can be used
for both exact and approximate matching, using similarity measures. Using our
logical approach as a formal specification, we implemented a complete
client-server image retrieval system, which allows a user to pose both queries
by sketch and queries by example. A set of experiments has been carried out on
a testbed of images to assess the retrieval capabilities of the system in
comparison with expert users ranking. Results are presented adopting a
well-established measure of quality borrowed from textual information
retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1504</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1504</id><created>2011-09-07</created><updated>2011-11-16</updated><authors><author><keyname>Sudholt</keyname><forenames>Dirk</forenames></author></authors><title>A New Method for Lower Bounds on the Running Time of Evolutionary
  Algorithms</title><categories>cs.NE</categories><comments>A preliminary version with parts of the results appeared at PPSN
  2010. The results therein were restricted to mutation rate 1/n</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for proving lower bounds on the expected running time
of evolutionary algorithms. It is based on fitness-level partitions and an
additional condition on transition probabilities between fitness levels. The
method is versatile, intuitive, elegant, and very powerful. It yields exact or
near-exact lower bounds for LO, OneMax, long k-paths, and all functions with a
unique optimum. Most lower bounds are very general: they hold for all
evolutionary algorithms that only use bit-flip mutation as variation
operator---i.e. for all selection operators and population models. The lower
bounds are stated with their dependence on the mutation rate.
  These results have very strong implications. They allow to determine the
optimal mutation-based algorithm for LO and OneMax, i.e., which algorithm
minimizes the expected number of fitness evaluations. This includes the choice
of the optimal mutation rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1507</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1507</id><created>2011-09-07</created><updated>2012-12-21</updated><authors><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Mohajer</keyname><forenames>Soheil</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On the Symmetric Feedback Capacity of the K-user Cyclic Z-Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K-user cyclic Z-interference channel models a situation in which the kth
transmitter causes interference only to the (k-1)th receiver in a cyclic
manner, e.g., the first transmitter causes interference only to the Kth
receiver. The impact of noiseless feedback on the capacity of this channel is
studied by focusing on the Gaussian cyclic Z-interference channel. To this end,
the symmetric feedback capacity of the linear shift deterministic cyclic
Z-interference channel (LD-CZIC) is completely characterized for all
interference regimes. Using insights from the linear deterministic channel
model, the symmetric feedback capacity of the Gaussian cyclic Z-interference
channel is characterized up to within a constant number of bits. As a byproduct
of the constant gap result, the symmetric generalized degrees of freedom with
feedback for the Gaussian cyclic Z-interference channel are also characterized.
These results highlight that the symmetric feedback capacities for both linear
and Gaussian channel models are in general functions of K, the number of users.
Furthermore, the capacity gain obtained due to feedback decreases as K
increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1517</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1517</id><created>2011-09-07</created><authors><author><keyname>Burr</keyname><forenames>Michael A.</forenames></author><author><keyname>Rafalin</keyname><forenames>Eynat</forenames></author><author><keyname>Souvaine</keyname><forenames>Diane L.</forenames></author></authors><title>Dynamic Maintenance of Half-Space Depth for Points and Contours</title><categories>cs.CG math.ST stat.TH</categories><comments>31 pages</comments><msc-class>68U05</msc-class><acm-class>G.3; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Half-space depth (also called Tukey depth or location depth) is one of the
most commonly studied data depth measures because it possesses many desirable
properties for data depth functions. The data depth contours bound regions of
increasing depth. For the sample case, there are two competing definitions of
contours: the rank-based contours and the cover-based contours.
  In this paper, we present three dynamic algorithms for maintaining the
half-space depth of points and contours: The first maintains the half-space
depth of a single point in a data set in $O(\log n)$ time per update
(insertion/deletion) and overall linear space. By maintaining such a data
structure for each data point, we present an algorithm for dynamically
maintaining the rank-based contours in $O(n\cdot\log n)$ time per update and
overall quadratic space. The third dynamic algorithm maintains the cover-based
contours in $O(n\cdot \log^2 n)$ time per update and overall quadratic space.
  We also augment our first algorithm to maintain the local cover-based
contours at data points while maintaining the same complexities. A corollary of
this discussion is a strong structural result of independent interest
describing the behavior of dynamic cover-based contours near data points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1525</identifier>
 <datestamp>2011-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1525</id><created>2011-09-07</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>Conceptual Knowledge Markup Language: The central core</title><categories>cs.DL cs.AI</categories><comments>Presented at the Twelfth Workshop on Knowledge Acquisition, Modeling
  and Management (KAW'99), 1999</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conceptual knowledge framework OML/CKML needs several components for a
successful design. One important, but previously overlooked, component is the
central core of OML/CKML. The central core provides a theoretical link between
the ontological specification in OML and the conceptual knowledge
representation in CKML. This paper discusses the formal semantics and syntactic
styles of the central core, and also the important role it plays in defining
interoperability between OML/CKML, RDF/S and Ontolingua.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1528</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1528</id><created>2011-09-07</created><updated>2012-03-01</updated><authors><author><keyname>Kianercy</keyname><forenames>Ardeshir</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Dynamics of Boltzmann Q-Learning in Two-Player Two-Action Games</title><categories>cs.GT cs.LG cs.MA nlin.AO q-bio.PE</categories><comments>10 pages, 12 figures. Version 2: added more extensive discussion of
  asymmetric equilibria; clarified conditions for continuous/discontinuous
  bifurcations in coordination/anti-coordination games</comments><journal-ref>Physical Review E, vol.85, 4, 041145, 2012</journal-ref><doi>10.1103/PhysRevE.85.041145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dynamics of Q-learning in two-player two-action games with a
Boltzmann exploration mechanism. For any non-zero exploration rate the dynamics
is dissipative, which guarantees that agent strategies converge to rest points
that are generally different from the game's Nash Equlibria (NE). We provide a
comprehensive characterization of the rest point structure for different games,
and examine the sensitivity of this structure with respect to the noise due to
exploration. Our results indicate that for a class of games with multiple NE
the asymptotic behavior of learning dynamics can undergo drastic changes at
critical exploration rates. Furthermore, we demonstrate that for certain games
with a single NE, it is possible to have additional rest points (not
corresponding to any NE) that persist for a finite range of the exploration
rates and disappear when the exploration rates of both players tend to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1530</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1530</id><created>2011-09-07</created><authors><author><keyname>Byers</keyname><forenames>John W.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Zervas</keyname><forenames>Georgios</forenames></author></authors><title>Daily Deals: Prediction, Social Diffusion, and Reputational
  Ramifications</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 9 tables, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Daily deal sites have become the latest Internet sensation, providing
discounted offers to customers for restaurants, ticketed events, services, and
other items. We begin by undertaking a study of the economics of daily deals on
the web, based on a dataset we compiled by monitoring Groupon and LivingSocial
sales in 20 large cities over several months. We use this dataset to
characterize deal purchases; glean insights about operational strategies of
these firms; and evaluate customers' sensitivity to factors such as price, deal
scheduling, and limited inventory. We then marry our daily deals dataset with
additional datasets we compiled from Facebook and Yelp users to study the
interplay between social networks and daily deal sites. First, by studying user
activity on Facebook while a deal is running, we provide evidence that daily
deal sites benefit from significant word-of-mouth effects during sales events,
consistent with results predicted by cascade models. Second, we consider the
effects of daily deals on the longer-term reputation of merchants, based on
their Yelp reviews before and after they run a daily deal. Our analysis shows
that while the number of reviews increases significantly due to daily deals,
average rating scores from reviewers who mention daily deals are 10% lower than
scores of their peers on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1533</identifier>
 <datestamp>2011-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1533</id><created>2011-09-07</created><authors><author><keyname>Dai</keyname><forenames>Wenhan</forenames></author><author><keyname>Gai</keyname><forenames>Yi</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>The Non-Bayesian Restless Multi-Armed Bandit: A Case of Near-Logarithmic
  Strict Regret</title><categories>math.OC cs.LG cs.NI cs.SY math.PR</categories><comments>arXiv admin note: significant text overlap with arXiv:1011.4752</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classic Bayesian restless multi-armed bandit (RMAB) problem, there are
$N$ arms, with rewards on all arms evolving at each time as Markov chains with
known parameters. A player seeks to activate $K \geq 1$ arms at each time in
order to maximize the expected total reward obtained over multiple plays. RMAB
is a challenging problem that is known to be PSPACE-hard in general. We
consider in this work the even harder non-Bayesian RMAB, in which the
parameters of the Markov chain are assumed to be unknown \emph{a priori}. We
develop an original approach to this problem that is applicable when the
corresponding Bayesian problem has the structure that, depending on the known
parameter values, the optimal solution is one of a prescribed finite set of
policies. In such settings, we propose to learn the optimal policy for the
non-Bayesian RMAB by employing a suitable meta-policy which treats each policy
from this finite set as an arm in a different non-Bayesian multi-armed bandit
problem for which a single-arm selection policy is optimal. We demonstrate this
approach by developing a novel sensing policy for opportunistic spectrum access
over unknown dynamic channels. We prove that our policy achieves
near-logarithmic regret (the difference in expected reward compared to a
model-aware genie), which leads to the same average reward that can be achieved
by the optimal policy under a known model. This is the first such result in the
literature for a non-Bayesian RMAB. For our proof, we also develop a novel
generalization of the Chernoff-Hoeffding bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1552</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1552</id><created>2011-09-07</created><authors><author><keyname>Dai</keyname><forenames>Wenhan</forenames></author><author><keyname>Gai</keyname><forenames>Yi</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Efficient Online Learning for Opportunistic Spectrum Access</title><categories>cs.LG cs.NI cs.SY math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of opportunistic spectrum access in cognitive radio networks has
been recently formulated as a non-Bayesian restless multi-armed bandit problem.
In this problem, there are N arms (corresponding to channels) and one player
(corresponding to a secondary user). The state of each arm evolves as a
finite-state Markov chain with unknown parameters. At each time slot, the
player can select K &lt; N arms to play and receives state-dependent rewards
(corresponding to the throughput obtained given the activity of primary users).
The objective is to maximize the expected total rewards (i.e., total
throughput) obtained over multiple plays. The performance of an algorithm for
such a multi-armed bandit problem is measured in terms of regret, defined as
the difference in expected reward compared to a model-aware genie who always
plays the best K arms. In this paper, we propose a new continuous exploration
and exploitation (CEE) algorithm for this problem. When no information is
available about the dynamics of the arms, CEE is the first algorithm to
guarantee near-logarithmic regret uniformly over time. When some bounds
corresponding to the stationary state distributions and the state-dependent
rewards are known, we show that CEE can be easily modified to achieve
logarithmic regret over time. In contrast, prior algorithms require additional
information concerning bounds on the second eigenvalues of the transition
matrices in order to guarantee logarithmic regret. Finally, we show through
numerical simulations that CEE is more efficient than prior algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1579</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1579</id><created>2011-09-07</created><authors><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Im</keyname><forenames>Sungjin</forenames></author><author><keyname>Moseley</keyname><forenames>Benjamin</forenames></author></authors><title>Fast Clustering using MapReduce</title><categories>cs.DC cs.DS</categories><comments>Accepted to KDD 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering problems have numerous applications and are becoming more
challenging as the size of the data increases. In this paper, we consider
designing clustering algorithms that can be used in MapReduce, the most popular
programming environment for processing large datasets. We focus on the
practical and popular clustering problems, $k$-center and $k$-median. We
develop fast clustering algorithms with constant factor approximation
guarantees. From a theoretical perspective, we give the first analysis that
shows several clustering algorithms are in $\mathcal{MRC}^0$, a theoretical
MapReduce class introduced by Karloff et al. \cite{KarloffSV10}. Our algorithms
use sampling to decrease the data size and they run a time consuming clustering
algorithm such as local search or Lloyd's algorithm on the resulting data set.
Our algorithms have sufficient flexibility to be used in practice since they
run in a constant number of MapReduce rounds. We complement these results by
performing experiments using our algorithms. We compare the empirical
performance of our algorithms to several sequential and parallel algorithms for
the $k$-median problem. The experiments show that our algorithms' solutions are
similar to or better than the other algorithms' solutions. Furthermore, on data
sets that are sufficiently large, our algorithms are faster than the other
parallel algorithms that we tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1583</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1583</id><created>2011-09-07</created><authors><author><keyname>Quoc</keyname><forenames>Trong Duong</forenames></author><author><keyname>Perkuhn</keyname><forenames>Heiko</forenames></author><author><keyname>Catrein</keyname><forenames>Daniel</forenames></author><author><keyname>Naumann</keyname><forenames>Uwe</forenames></author><author><keyname>Anwar</keyname><forenames>Toni</forenames></author></authors><title>Optimization and Evaluation of a Multimedia Streaming Service on Hybrid
  Telco cloud</title><categories>cs.DC cs.MM</categories><comments>20 pages; International Journal on Cloud Computing: Services and
  Architecture (IJCCSA), 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent developments in cloud computing, a paradigm shift from rather
static deployment of resources to more dynamic, on-demand practices means more
flexibility and better utilization of resources. This demands new ways to
efficiently configure networks.
  In this paper, we will characterize a class of competitive cloud services
that telecom operators could provide based on the characteristics of telecom
infrastructure through an applicable streaming service architecture. Then, we
will model this architecture as a cost-based mathematic model. This model
provides a tool to evaluate and compare the cost of software services for
different telecom network topologies and deployment strategies. Additionally,
with each topology it acts as a means to characterize the deployment solution
that yields the lowest resource usage over the entire network. These
applications are illustrated through numerical analysis. Finally, a
proof-of-concept prototype is deployed to shows dynamic properties of the
service in the architecture and the model above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1587</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1587</id><created>2011-09-07</created><authors><author><keyname>Comini</keyname><forenames>Marco</forenames></author><author><keyname>Titolo</keyname><forenames>Laura</forenames></author><author><keyname>Villanueva</keyname><forenames>Alicia</forenames></author></authors><title>Abstract Diagnosis for Timed Concurrent Constraint programs</title><categories>cs.PL cs.LO</categories><comments>16 pages</comments><msc-class>68-06</msc-class><acm-class>D.3.3; D.3.4; F.3.1; F.3.2; D.3.1</acm-class><journal-ref>Theory and Practice of Logic Programming 2011, 11(4-5): 487-502
  (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Timed Concurrent Constraint Language (tccp in short) is a concurrent
logic language based on the simple but powerful concurrent constraint paradigm
of Saraswat. In this paradigm, the notion of store-as-value is replaced by the
notion of store-as-constraint, which introduces some differences w.r.t. other
approaches to concurrency. In this paper, we provide a general framework for
the debugging of tccp programs. To this end, we first present a new compact,
bottom-up semantics for the language that is well suited for debugging and
verification purposes in the context of reactive systems. We also provide an
abstract semantics that allows us to effectively implement debugging algorithms
based on abstract interpretation. Given a tccp program and a behavior
specification, our debugging approach automatically detects whether the program
satisfies the specification. This differs from other semiautomatic approaches
to debugging and avoids the need to provide symptoms in advance. We show the
efficacy of our approach by introducing two illustrative examples. We choose a
specific abstract domain and show how we can detect that a program is
erroneous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1602</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1602</id><created>2011-09-07</created><authors><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Nordhaus-Gaddum for Treewidth</title><categories>math.CO cs.DM</categories><journal-ref>European Journal of Combinatorics, 33.4:488-490, 2012</journal-ref><doi>10.1016/j.ejc.2011.10.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every graph $G$ with $n$ vertices, the treewidth of $G$
plus the treewidth of the complement of $G$ is at least $n-2$. This bound is
tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1604</identifier>
 <datestamp>2012-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1604</id><created>2011-09-07</created><updated>2012-02-21</updated><authors><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Annapureddy</keyname><forenames>V. Sreekanth</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Degrees of Freedom (DoF) of Locally Connected Interference Channels with
  Coordinated Multi-Point (CoMP) Transmission</title><categories>cs.IT math.IT</categories><comments>In Proc. IEEE International Conference on Communications (ICC),
  Ottawa, Jun. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) available for communication provides an
analytically tractable way to characterize the information-theoretic capacity
of interference channels. In this paper, the DoF of a K-user interference
channel is studied under the assumption that the transmitters can cooperate via
coordinated multi-point (CoMP) transmission. In [1], the authors considered the
linear asymmetric model of Wyner, where each transmitter is connected to its
own receiver and its successor, and is aware of its own message as well as M-1
preceding messages. The per user DoF was shown to go to M/(M+1) as the number
of users increases to infinity. In this work, the same model of channel
connectivity is considered, with a relaxed cooperation constraint that bounds
the maximum number of transmitters at which each message can be available, by a
cooperation order M. We show that the relaxation of the cooperation constraint,
while maintaining the same load imposed on a backhaul link needed to distribute
the messages, results in a gain in the DoF. In particular, the asymptotic limit
of the per user DoF under the cooperation order constraint is (2M)/(2M+1) .
Moreover, the optimal transmit set selection satisfies a local cooperation
constraint. i.e., each message needs only to be available at neighboring
transmitters. [1] A. Lapidoth, S. Shamai (Shitz) and M. A. Wigger, &quot;A linear
interference network with local Side-Information,&quot; in Proc. IEEE International
Symposium on Information Theory (ISIT), Nice, Jun. 2007.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1605</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1605</id><created>2011-09-07</created><authors><author><keyname>Rocklin</keyname><forenames>Matthew</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>On Clustering on Graphs with Multiple Edge Types</title><categories>cs.SI cs.LG physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study clustering on graphs with multiple edge types. Our main motivation
is that similarities between objects can be measured in many different metrics.
For instance similarity between two papers can be based on common authors,
where they are published, keyword similarity, citations, etc. As such, graphs
with multiple edges is a more accurate model to describe similarities between
objects. Each edge/metric provides only partial information about the data;
recovering full information requires aggregation of all the similarity metrics.
Clustering becomes much more challenging in this context, since in addition to
the difficulties of the traditional clustering problem, we have to deal with a
space of clusterings. We generalize the concept of clustering in single-edge
graphs to multi-edged graphs and investigate problems such as: Can we find a
clustering that remains good, even if we change the relative weights of
metrics? How can we describe the space of clusterings efficiently? Can we find
unexpected clusterings (a good clustering that is distant from all given
clusterings)? If given the ground-truth clustering, can we recover how the
weights for edge types were aggregated? %In this paper, we discuss these
problems and the underlying algorithmic challenges and propose some solutions.
We also present two case studies: one based on papers on Arxiv and one based on
CIA World Factbook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1606</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1606</id><created>2011-09-07</created><authors><author><keyname>Gai</keyname><forenames>Yi</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>Online Learning for Combinatorial Network Optimization with Restless
  Markovian Rewards</title><categories>cs.LG cs.NI math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial network optimization algorithms that compute optimal structures
taking into account edge weights form the foundation for many network
protocols. Examples include shortest path routing, minimal spanning tree
computation, maximum weighted matching on bipartite graphs, etc. We present
CLRMR, the first online learning algorithm that efficiently solves the
stochastic version of these problems where the underlying edge weights vary as
independent Markov chains with unknown dynamics.
  The performance of an online learning algorithm is characterized in terms of
regret, defined as the cumulative difference in rewards between a
suitably-defined genie, and that obtained by the given algorithm. We prove
that, compared to a genie that knows the Markov transition matrices and uses
the single-best structure at all times, CLRMR yields regret that is polynomial
in the number of edges and nearly-logarithmic in time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1618</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1618</id><created>2011-09-07</created><authors><author><keyname>Doan</keyname><forenames>Son</forenames></author><author><keyname>Vo</keyname><forenames>Bao-Khanh Ho</forenames></author><author><keyname>Collier</keyname><forenames>Nigel</forenames></author></authors><title>An analysis of Twitter messages in the 2011 Tohoku Earthquake</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>9 pages, 4 figures, eHealth 2011 conference, Malaga (Spain)
  (accepted)</comments><journal-ref>Lecture Notes of the Institute for Computer Sciences, Social
  Informatics and Telecommunications Engineering, 2012, Volume 91, Part 4,
  58-66</journal-ref><doi>10.1007/978-3-642-29262-0_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media such as Facebook and Twitter have proven to be a useful resource
to understand public opinion towards real world events. In this paper, we
investigate over 1.5 million Twitter messages (tweets) for the period 9th March
2011 to 31st May 2011 in order to track awareness and anxiety levels in the
Tokyo metropolitan district to the 2011 Tohoku Earthquake and subsequent
tsunami and nuclear emergencies. These three events were tracked using both
English and Japanese tweets. Preliminary results indicated: 1) close
correspondence between Twitter data and earthquake events, 2) strong
correlation between English and Japanese tweets on the same events, 3) tweets
in the native language play an important roles in early warning, 4) tweets
showed how quickly Japanese people's anxiety returned to normal levels after
the earthquake event. Several distinctions between English and Japanese tweets
on earthquake events are also discussed. The results suggest that Twitter data
can be used as a useful resource for tracking the public mood of populations
affected by natural disasters as well as an early warning system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1643</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1643</id><created>2011-09-08</created><updated>2012-12-11</updated><authors><author><keyname>Zeeshan</keyname><forenames>Muhammad</forenames></author><author><keyname>Khan</keyname><forenames>Shoab A</forenames></author><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author></authors><title>An Efficient Hybrid Power Control Algorithm for Capacity Improvement of
  CDMA-based Fixed Wireless Applications</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to some mistakes in
  simulations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Fixed Wireless Applications (FWA), the Code Division Multiple Access
(CDMA) is the most promising candidate for wideband data access. The reason is
the soft limit on the number of active mobile devices. Many Fixed Wireless
Applications impose an upper bound on the BER performance which restricts the
increase in number of mobile users. The number of active mobile users or
Capacity is further reduced in Multipath Fading Environment (MFE). This paper
presents an effective method of improving the capacity of CDMA based Fixed
Wireless Networks by using a hybrid power control algorithm. The proposed
scheme improves the capacity two times as compared to the conventional CDMA
based networks. Simulation results have been presented to demonstrate the
effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1646</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1646</id><created>2011-09-08</created><updated>2014-03-30</updated><authors><author><keyname>Liu</keyname><forenames>Guangcan</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Exact Subspace Segmentation and Outlier Detection by Low-Rank
  Representation</title><categories>cs.IT cs.CV math.IT</categories><comments>Proceedings of the Fifteenth International Conference on Artificial
  Intelligence and Statistics, AISTATS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the following matrix recovery problem: suppose we
are given a set of data points containing two parts, one part consists of
samples drawn from a union of multiple subspaces and the other part consists of
outliers. We do not know which data points are outliers, or how many outliers
there are. The rank and number of the subspaces are unknown either. Can we
detect the outliers and segment the samples into their right subspaces,
efficiently and exactly? We utilize a so-called {\em Low-Rank Representation}
(LRR) method to solve this problem, and prove that under mild technical
conditions, any solution to LRR exactly recovers the row space of the samples
and detect the outliers as well. Since the subspace membership is provably
determined by the row space, this further implies that LRR can perform exact
subspace segmentation and outlier detection, in an efficient way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1648</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1648</id><created>2011-09-08</created><authors><author><keyname>Mandal</keyname><forenames>Ardhendu</forenames></author></authors><title>BRIDGE: A Model for Modern Software Development Process to Cater the
  Present Software Crisis</title><categories>cs.SE</categories><comments>7 Pages; 2009 IEEE International Advance Computing Conference (IACC
  2009, also available at IEEE Xplore)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As hardware components are becoming cheaper and powerful day by day, the
expected services from modern software are increasing like any thing.
Developing such software has become extremely challenging. Not only the
complexity, but also the developing of such software within the time
constraints and budget has become the real challenge. Quality concern and
maintainability are added flavour to the challenge. On stream, the requirements
of the clients are changing so frequently that it has become extremely tough to
manage these changes. More often, the clients are unhappy with the end product.
Large, complex software projects are notoriously late to market, often exhibit
quality problems, and don't always deliver on promised functionality. None of
the existing models are helpful to cater the modern software crisis. Hence, a
better modern software development process model to handle with the present
software crisis is badly needed. This paper suggests a new software development
process model, BRIDGE, to tackle present software crisis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1649</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1649</id><created>2011-09-08</created><authors><author><keyname>Brim</keyname><forenames>L.</forenames><affiliation>Masaryk University</affiliation></author><author><keyname>Fabrikov&#xe1;</keyname><forenames>J.</forenames><affiliation>Masaryk University</affiliation></author><author><keyname>Dra&#x17e;an</keyname><forenames>S.</forenames><affiliation>Masaryk University</affiliation></author><author><keyname>&#x160;afr&#xe1;nek</keyname><forenames>D.</forenames><affiliation>Masaryk University</affiliation></author></authors><title>Reachability in Biochemical Dynamical Systems by Quantitative Discrete
  Approximation (extended abstract)</title><categories>cs.SY cs.CE math.OC</categories><comments>In Proceedings CompMod 2011, arXiv:1109.1044</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 67, 2011, pp. 97-112</journal-ref><doi>10.4204/EPTCS.67.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel computational technique for finite discrete
approximation of continuous dynamical systems suitable for a significant class
of biochemical dynamical systems is introduced. The method is parameterized in
order to affect the imposed level of approximation provided that with
increasing parameter value the approximation converges to the original
continuous system. By employing this approximation technique, we present
algorithms solving the reachability problem for biochemical dynamical systems.
The presented method and algorithms are evaluated on several exemplary
biological models and on a real case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1650</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1650</id><created>2011-09-08</created><authors><author><keyname>Mandal</keyname><forenames>Ardhendu</forenames></author><author><keyname>Pal</keyname><forenames>Subhas Chandra</forenames></author></authors><title>An Empirical Study and Analysis of the Dynamic Load Balancing Techniques
  Used in Parallel Computing Systems</title><categories>cs.DC</categories><comments>6 Pages</comments><journal-ref>Proceedings of ICCS-2010, 19-20 Nov, 2010</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A parallel computer system is a collection of processing elements that
communicate and cooperate to solve large computational problems efficiently. To
achieve this, at first the large computational problem is partitioned into
several tasks with different work-loads and then are assigned to the different
processing elements for computation. Distribution of the work load is known as
Load Balancing. An appropriate distribution of work-loads across the various
processing elements is very important as disproportional workloads can
eliminate the performance benefit of parallelizing the job. Hence, load
balancing on parallel systems is a critical and challenging activity. Load
balancing algorithms can be broadly categorized as static or dynamic. Static
load balancing algorithms distribute the tasks to processing elements at
compile time, while dynamic algorithms bind tasks to processing elements at run
time. This paper explains only the different dynamic load balancing techniques
in brief used in parallel systems and concluding with the comparative
performance analysis result of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1651</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1651</id><created>2011-09-08</created><authors><author><keyname>Mandal</keyname><forenames>Ardhendu</forenames></author></authors><title>SRS BUILDER 1.0: An Upper Type CASE Tool For Requirement Specification</title><categories>cs.SE</categories><comments>6 Pages; Proceedings of the 4th National Conference; INDIACom-2010</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Software (SW) development is a labor intensive activity. Modern software
projects generally have to deal with producing and managing large and complex
software products. Developing such software has become an extremely challenging
job not only because of inherent complexity, but also mainly for economic
constraints unlike time, quality, maintainability concerns. Hence, developing
modern software within the budget still remains as one of the main software
crisis. The most significant way to reduce the software development cost is to
use the Computer-Aided Software Engineering (CASE) tools over the entire
Software Development Life Cycle (SDLC) process as substitute to expensive human
labor cost. We think that automation of software development methods is a
valuable support for the software engineers in coping with this complexity and
for improving quality too. This paper demonstrates the newly developed CASE
tools name &quot;SRS Builder 1.0&quot; for software requirement specification developed
at our university laboratory, University of North Bengal, India. This paper
discusses our new developed product with its functionalities and usages. We
believe the tool has the potential to play an important role in the software
development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1653</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1653</id><created>2011-09-08</created><authors><author><keyname>Sarkar</keyname><forenames>Tamal</forenames></author><author><keyname>Das</keyname><forenames>Samir Chandra</forenames></author><author><keyname>Mandal</keyname><forenames>Ardhendu</forenames></author></authors><title>A Study of Computer-Based Simulations for Nano-Systems and their types</title><categories>cs.ET physics.comp-ph</categories><comments>6 Pages; Proceedings of 2nd National Conference on Nano-materials and
  Nanotechnology (21-23 December, 2009), ISBN: 978-93-80043-61-6</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In most of the cases, the experimental study of Nanotechnology involves high
cost for Laboratory set-up and the experimentation processes were also slow.
So, one cannot rely on experimental nanotechnology alone. As such, the
Computer-Based molecular simulations and modeling are one of the foundations of
computational nanotechnology. The computer based modeling and simulations were
also referred as computational experimentations. In real experiments, the
investigator doesn't have full control over the experiment. But, in
Computational experimentation the investigator have full control over the
experiment. The accuracy of such Computational nano-technology based experiment
generally depends on the accuracy of the following things: Intermolecular
interaction, Numerical models and Simulation schemes used. Once the accuracy of
the Computational Scheme is guaranteed one can use that to investigate various
nonlinear interactions whose results are completely unexpected and unforeseen.
Apart from it, numerical modeling and computer based simulations also help to
understand the theoretical part of the nano-science involved in the
nano-system. They allow us to develop useful analytic and predictive models. In
this paper, a brief study of Computer-Based- Simulation techniques as well as
some Experimental result obtained using it were given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1657</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1657</id><created>2011-09-08</created><authors><author><keyname>Hu</keyname><forenames>Fu-Tao</forenames></author><author><keyname>Xu</keyname><forenames>Jun-Ming</forenames></author></authors><title>Complexity of Bondage and Reinforcement</title><categories>math.CO cs.CC</categories><comments>16 pages with 3 figures</comments><msc-class>05C69</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a graph. A subset $D\subseteq V$ is a dominating set if
every vertex not in $D$ is adjacent to a vertex in $D$. A dominating set $D$ is
called a total dominating set if every vertex in $D$ is adjacent to a vertex in
$D$. The domination (resp. total domination) number of $G$ is the smallest
cardinality of a dominating (resp. total dominating) set of $G$. The bondage
(resp. total bondage) number of a nonempty graph $G$ is the smallest number of
edges whose removal from $G$ results in a graph with larger domination (resp.
total domination) number of $G$. The reinforcement number of $G$ is the
smallest number of edges whose addition to $G$ results in a graph with smaller
domination number. This paper shows that the decision problems for bondage,
total bondage and reinforcement are all NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1664</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1664</id><created>2011-09-08</created><authors><author><keyname>Roggen</keyname><forenames>Daniel</forenames></author><author><keyname>Wirz</keyname><forenames>Martin</forenames></author><author><keyname>Tr&#xf6;ster</keyname><forenames>Gerhard</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Recognition of Crowd Behavior from Mobile Sensors with Pattern Analysis
  and Graph Clustering Methods</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Networks and Heterogenous Media, 6(3), 2011, pages 521-544</journal-ref><doi>10.3934/nhm.2011.6.521</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile on-body sensing has distinct advantages for the analysis and
understanding of crowd dynamics: sensing is not geographically restricted to a
specific instrumented area, mobile phones offer on-body sensing and they are
already deployed on a large scale, and the rich sets of sensors they contain
allows one to characterize the behavior of users through pattern recognition
techniques.
  In this paper we present a methodological framework for the machine
recognition of crowd behavior from on-body sensors, such as those in mobile
phones. The recognition of crowd behaviors opens the way to the acquisition of
large-scale datasets for the analysis and understanding of crowd dynamics. It
has also practical safety applications by providing improved crowd situational
awareness in cases of emergency.
  The framework comprises: behavioral recognition with the user's mobile
device, pairwise analyses of the activity relatedness of two users, and graph
clustering in order to uncover globally, which users participate in a given
crowd behavior. We illustrate this framework for the identification of groups
of persons walking, using empirically collected data.
  We discuss the challenges and research avenues for theoretical and applied
mathematics arising from the mobile sensing of crowd behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1670</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1670</id><created>2011-09-08</created><authors><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>A New Rate Region for General Interference Channel (Improved HK Region)</title><categories>cs.IT math.IT</categories><comments>23 pages, new and revised version, sumitted to IEEE Trans. on
  Information Thory, Sept.2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper (a) after detailed investigation of the previous equivalent
rate regions for general interference channel, i.e., the Han-Kobayashi (HK) and
the Chong-Motani-Garg (CMG) regions, we define modified CMG region the
equivalency of which with the HK region is readily seen; (b) we make two novel
changes in the HK coding. First, we allow the input auxiliary random variables
to be correlated and, second, exploit the powerful technique of random binning
instead of the HK -CMG superposition coding, thereby establishing a new rate
region for general interference channel, as an improved version of the HK
region; (c) we make a novel change in the CMG coding by allowing the message
variables to be correlated and obtain an equivalent form for our new region in
(b), as an improved version of the CMG region. Then, (d) in order to exactly
demarcate the regions, by considering their different easily comparable
versions, we compare our region to the HK and CMG regions. Specifically, using
a simple dependency structure for the correlated auxiliary random variables,
based on the Wyner and Gacs-Korner common information between dependent
variables, we show that the HK and the CMG regions are special cases of our new
region. Keywords. Interference channel, Correlated auxiliary random variables,
Common information, Superposition coding, Binning scheme
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1674</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1674</id><created>2011-09-08</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>A Linear-Optical Proof that the Permanent is #P-Hard</title><categories>quant-ph cs.CC</categories><comments>12 pages, 2 figures, to appear in Proceedings of the Royal Society A.
  doi: 10.1098/rspa.2011.0232</comments><doi>10.1098/rspa.2011.0232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the crown jewels of complexity theory is Valiant's 1979 theorem that
computing the permanent of an n*n matrix is #P-hard. Here we show that, by
using the model of linear-optical quantum computing---and in particular, a
universality theorem due to Knill, Laflamme, and Milburn---one can give a
different and arguably more intuitive proof of this theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1680</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1680</id><created>2011-09-08</created><updated>2011-11-30</updated><authors><author><keyname>Nebe</keyname><forenames>Gabriele</forenames></author></authors><title>An extremal [72,36,16] binary code has no automorphism group containing
  Z2xZ4, Q_8, or Z_{10}</title><categories>cs.IT math.IT</categories><comments>This is the final version to appear in Finite fields and their
  applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $C$ be an extremal self-dual binary code of length 72 and $g\in \Aut(C) $
be an automorphism of order 2. We show that $C$ is a free $\F_2&lt;g&gt;$ module and
use this to exclude certain subgroups of order 8 of $\Aut (C)$. We also show
that $\Aut(C)$ does not contain an element of order 10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1681</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1681</id><created>2011-09-08</created><authors><author><keyname>Nebe</keyname><forenames>Gabriele</forenames></author></authors><title>On extremal self-dual ternary codes of length 48</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All extremal ternary codes of length 48 that have some automorphism of prime
order $p\geq 5$ are equivalent to one of the two known codes, the Pless code or
the extended quadratic residue code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1691</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1691</id><created>2011-09-08</created><updated>2014-05-23</updated><authors><author><keyname>Karandikar</keyname><forenames>Prateek</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames></author></authors><title>Generalized Post Embedding Problems</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Regular Post Embedding Problem extended with partial (co)directness is
shown decidable. This extends to universal and/or counting versions. It is also
shown that combining directness and codirectness in Post Embedding problems
leads to undecidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1693</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1693</id><created>2011-09-08</created><authors><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Demmel</keyname><forenames>James</forenames></author><author><keyname>Holtz</keyname><forenames>Olga</forenames></author><author><keyname>Schwartz</keyname><forenames>Oded</forenames></author></authors><title>Graph Expansion and Communication Costs of Fast Matrix Multiplication</title><categories>cs.DS cs.CC cs.DC cs.NA math.CO math.NA</categories><report-no>UCB/EECS-2011-40</report-no><acm-class>F.2.1</acm-class><journal-ref>Proceedings of the 23rd annual symposium on parallelism in
  algorithms and architectures. ACM, 1-12. 2011 (a shorter conference version)</journal-ref><doi>10.1145/1989493.1989495</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication cost of algorithms (also known as I/O-complexity) is shown
to be closely related to the expansion properties of the corresponding
computation graphs. We demonstrate this on Strassen's and other fast matrix
multiplication algorithms, and obtain first lower bounds on their communication
costs.
  In the sequential case, where the processor has a fast memory of size $M$,
too small to store three $n$-by-$n$ matrices, the lower bound on the number of
words moved between fast and slow memory is, for many of the matrix
multiplication algorithms, $\Omega((\frac{n}{\sqrt M})^{\omega_0}\cdot M)$,
where $\omega_0$ is the exponent in the arithmetic count (e.g., $\omega_0 = \lg
7$ for Strassen, and $\omega_0 = 3$ for conventional matrix multiplication).
With $p$ parallel processors, each with fast memory of size $M$, the lower
bound is $p$ times smaller.
  These bounds are attainable both for sequential and for parallel algorithms
and hence optimal. These bounds can also be attained by many fast algorithms in
linear algebra (e.g., algorithms for LU, QR, and solving the Sylvester
equation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1702</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1702</id><created>2011-09-08</created><updated>2011-09-26</updated><authors><author><keyname>Awodey</keyname><forenames>Steve</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Rabe</keyname><forenames>Florian</forenames><affiliation>Jabos University Bremen</affiliation></author></authors><title>Kripke Semantics for Martin-L\&quot;of's Extensional Type Theory</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  27, 2011) lmcs:1184</journal-ref><doi>10.2168/LMCS-7(3:18)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that simple type theory is complete with respect to
non-standard set-valued models. Completeness for standard models only holds
with respect to certain extended classes of models, e.g., the class of
cartesian closed categories. Similarly, dependent type theory is complete for
locally cartesian closed categories. However, it is usually difficult to
establish the coherence of interpretations of dependent type theory, i.e., to
show that the interpretations of equal expressions are indeed equal. Several
classes of models have been used to remedy this problem. We contribute to this
investigation by giving a semantics that is standard, coherent, and
sufficiently general for completeness while remaining relatively easy to
compute with. Our models interpret types of Martin-L\&quot;of's extensional
dependent type theory as sets indexed over posets or, equivalently, as
fibrations over posets. This semantics can be seen as a generalization to
dependent type theory of the interpretation of intuitionistic first-order logic
in Kripke models. This yields a simple coherent model theory, with respect to
which simple and dependent type theory are sound and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1705</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1705</id><created>2011-09-08</created><authors><author><keyname>Halupczok</keyname><forenames>Immanuel</forenames></author><author><keyname>Schulz</keyname><forenames>Andre</forenames></author></authors><title>Pinning Balloons with Perfect Angles and Optimal Area</title><categories>cs.CG</categories><comments>Full version of the Graph Drawing 2011 conference version, 16 pages,
  9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of arranging a set of $n$ disks with prescribed radii on
$n$ rays emanating from the origin such that two neighboring rays are separated
by an angle of $2\pi/n$. The center of the disks have to lie on the rays, and
no two disk centers are allowed to lie on the same ray. We require that the
disks have disjoint interiors, and that for every ray the segment between the
origin and the boundary of its associated disk avoids the interior of the
disks. Let $\r$ be the sum of the disk radii. We introduce a greedy strategy
that constructs such a disk arrangement that can be covered with a disk
centered at the origin whose radius is at most $2\r$, which is best possible.
The greedy strategy needs O(n) arithmetic operations.
  As an application of our result we present an algorithm for embedding
unordered trees with straight lines and perfect angular resolution such that it
can be covered with a disk of radius $n^{3.0367}$, while having no edge of
length smaller than 1. The tree drawing algorithm is an enhancement of a recent
result by Duncan et al. [Symp. of Graph Drawing, 2010] that exploits the
heavy-edge tree decomposition technique to construct a drawing of the tree that
can be covered with a disk of radius $2 n^4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1706</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1706</id><created>2011-09-08</created><authors><author><keyname>Ghosh</keyname><forenames>Esha</forenames></author><author><keyname>Ghosh</keyname><forenames>Subhas K.</forenames></author><author><keyname>Rangan</keyname><forenames>C. Pandu</forenames></author></authors><title>On the Fault Tolerance and Hamiltonicity of the Optical Transpose
  Interconnection System of Non-Hamiltonian Base Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hamiltonicity is an important property in parallel and distributed
computation. Existence of Hamiltonian cycle allows efficient emulation of
distributed algorithms on a network wherever such algorithm exists for
linear-array and ring, and can ensure deadlock freedom in some routing
algorithms in hierarchical interconnection networks. Hamiltonicity can also be
used for construction of independent spanning tree and leads to designing fault
tolerant protocols. Optical Transpose Interconnection Systems or OTIS (also
referred to as two-level swapped network) is a widely studied interconnection
network topology which is popular due to high degree of scalability,
regularity, modularity and package ability. Surprisingly, to our knowledge,
only one strong result is known regarding Hamiltonicity of OTIS - showing that
OTIS graph built of Hamiltonian base graphs are Hamiltonian. In this work we
consider Hamiltonicity of OTIS networks, built on Non-Hamiltonian base and
answer some important questions. First, we prove that Hamiltonicity of base
graph is not a necessary condition for the OTIS to be Hamiltonian. We present
an infinite family of Hamiltonian OTIS graphs composed on Non-Hamiltonian base
graphs. We further show that, it is not sufficient for the base graph to have
Hamiltonian path for the OTIS constructed on it to be Hamiltonian. We give
constructive proof of Hamiltonicity for a large family of Butterfly-OTIS. This
proof leads to an alternate efficient algorithm for independent spanning trees
construction on this class of OTIS graphs. Our algorithm is linear in the
number of vertices as opposed to the generalized algorithm, which is linear in
the number of edges of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1724</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1724</id><created>2011-09-08</created><updated>2013-03-20</updated><authors><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author></authors><title>The Complexity of Approximating a Bethe Equilibrium</title><categories>cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper resolves a common complexity issue in the Bethe approximation of
statistical physics and the Belief Propagation (BP) algorithm of artificial
intelligence. The Bethe approximation and the BP algorithm are heuristic
methods for estimating the partition function and marginal probabilities in
graphical models, respectively. The computational complexity of the Bethe
approximation is decided by the number of operations required to solve a set of
non-linear equations, the so-called Bethe equation. Although the BP algorithm
was inspired and developed independently, Yedidia, Freeman and Weiss (2004)
showed that the BP algorithm solves the Bethe equation if it converges
(however, it often does not). This naturally motivates the following question
to understand limitations and empirical successes of the Bethe and BP methods:
is the Bethe equation computationally easy to solve? We present a
message-passing algorithm solving the Bethe equation in a polynomial number of
operations for general binary graphical models of n variables where the maximum
degree in the underlying graph is O(log n). Our algorithm can be used as an
alternative to BP fixing its convergence issue and is the first fully
polynomial-time approximation scheme for the BP fixed-point computation in such
a large class of graphical models, while the approximate fixed-point
computation is known to be (PPAD-)hard in general. We believe that our
technique is of broader interest to understand the computational complexity of
the cavity method in statistical physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1729</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1729</id><created>2011-09-08</created><authors><author><keyname>Wang</keyname><forenames>Nan</forenames></author><author><keyname>Han</keyname><forenames>Jizhong</forenames></author><author><keyname>Fang</keyname><forenames>Jinyun</forenames></author></authors><title>Anomaly Sequences Detection from Logs Based on Compression</title><categories>cs.LG cs.DS</categories><comments>7 pages, 5 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining information from logs is an old and still active research topic. In
recent years, with the rapid emerging of cloud computing, log mining becomes
increasingly important to industry. This paper focus on one major mission of
log mining: anomaly detection, and proposes a novel method for mining abnormal
sequences from large logs. Different from previous anomaly detection systems
which based on statistics, probabilities and Markov assumption, our approach
measures the strangeness of a sequence using compression. It first trains a
grammar about normal behaviors using grammar-based compression, then measures
the information quantities and densities of questionable sequences according to
incrementation of grammar length. We have applied our approach on mining some
real bugs from fine grained execution logs. We have also tested its ability on
intrusion detection using some publicity available system call traces. The
experiments show that our method successfully selects the strange sequences
which related to bugs or attacking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1746</identifier>
 <datestamp>2012-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1746</id><created>2011-09-08</created><updated>2011-11-28</updated><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Sumi</keyname><forenames>R&#xf3;bert</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Circadian patterns of Wikipedia editorial activity: A demographic
  analysis</title><categories>physics.soc-ph cs.SI stat.AP</categories><journal-ref>PLoS ONE 7(1): e30091 (2012)</journal-ref><doi>10.1371/journal.pone.0030091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia (WP) as a collaborative, dynamical system of humans is an
appropriate subject of social studies. Each single action of the members of
this society, i.e. editors, is well recorded and accessible. Using the
cumulative data of 34 Wikipedias in different languages, we try to characterize
and find the universalities and differences in temporal activity patterns of
editors. Based on this data, we estimate the geographical distribution of
editors for each WP in the globe. Furthermore we also clarify the differences
among different groups of WPs, which originate in the variance of cultural and
social features of the communities of editors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1754</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1754</id><created>2011-09-08</created><updated>2011-09-09</updated><authors><author><keyname>Mau&#xe1;</keyname><forenames>Denis Deratani</forenames></author><author><keyname>de Campos</keyname><forenames>Cassio Polpo</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>Solving Limited Memory Influence Diagrams</title><categories>cs.AI cs.CC stat.ML</categories><comments>43 pages, 8 figures</comments><msc-class>68T37</msc-class><acm-class>I.2.1; I.2.8; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for exactly solving decision making problems
represented as influence diagrams. We do not require the usual assumptions of
no forgetting and regularity; this allows us to solve problems with
simultaneous decisions and limited information. The algorithm is empirically
shown to outperform a state-of-the-art algorithm on randomly generated problems
of up to 150 variables and $10^{64}$ solutions. We show that the problem is
NP-hard even if the underlying graph structure of the problem has small
treewidth and the variables take on a bounded number of states, but that a
fully polynomial time approximation scheme exists for these cases. Moreover, we
show that the bound on the number of states is a necessary condition for any
efficient approximation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1766</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1766</id><created>2011-09-08</created><authors><author><keyname>L&#xe4;ssig</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Sudholt</keyname><forenames>Dirk</forenames></author></authors><title>Analysis of Speedups in Parallel Evolutionary Algorithms for
  Combinatorial Optimization</title><categories>cs.NE</categories><comments>An extended abstract will appear in the proceedings of the 22nd
  International Symposium on Algorithms and Computation (ISAAC 2011). Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms are popular heuristics for solving various
combinatorial problems as they are easy to apply and often produce good
results. Island models parallelize evolution by using different populations,
called islands, which are connected by a graph structure as communication
topology. Each island periodically communicates copies of good solutions to
neighboring islands in a process called migration.
  We consider the speedup gained by island models in terms of the parallel
running time for problems from combinatorial optimization: sorting (as
maximization of sortedness), shortest paths, and Eulerian cycles. Different
search operators are considered. The results show in which settings and up to
what degree evolutionary algorithms can be parallelized efficiently. Along the
way, we also investigate how island models deal with plateaus. In particular,
we show that natural settings lead to exponential vs. logarithmic speedups,
depending on the frequency of migration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1774</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1774</id><created>2011-09-08</created><authors><author><keyname>Akgun</keyname><forenames>Ozgur</forenames></author><author><keyname>Frisch</keyname><forenames>Alan M.</forenames></author><author><keyname>Hnich</keyname><forenames>Brahim</forenames></author><author><keyname>Jefferson</keyname><forenames>Chris</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author></authors><title>Conjure Revisited: Towards Automated Constraint Modelling</title><categories>cs.AI cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automating the constraint modelling process is one of the key challenges
facing the constraints field, and one of the principal obstacles preventing
widespread adoption of constraint solving. This paper focuses on the
refinement-based approach to automated modelling, where a user specifies a
problem in an abstract constraint specification language and it is then
automatically refined into a constraint model. In particular, we revisit the
Conjure system that first appeared in prototype form in 2005 and present a new
implementation with a much greater coverage of the specification language
Essence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1801</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1801</id><created>2011-09-08</created><authors><author><keyname>Chen</keyname><forenames>Richard</forenames></author><author><keyname>Cohn</keyname><forenames>Amy</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>An Implicit Optimization Approach for Survivable Network Design</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing a network of minimum cost while
satisfying a prescribed survivability criterion. The survivability criterion
requires that a feasible flow must still exists (i.e. all demands can be
satisfied without violating arc capacities) even after the disruption of a
subset of the network's arcs. Specifically, we consider the case in which a
disruption (random or malicious) can destroy a subset of the arcs, with the
cost of the disruption not to exceed a disruption budget. This problem takes
the form of a tri-level, two-player game, in which the network operator designs
(or augments) the network, then the attacker launches a disruption that
destroys a subset of arcs, and then the network operator attempts to find a
feasible flow over the residual network. We first show how this can be modeled
as a two-stage stochastic program from the network operator's perspective, with
each of the exponential number of potential attacks considered as a disruption
scenario. We then reformulate this problem, via a Benders decomposition, to
consider the recourse decisions implicitly, greatly reducing the number of
variables but at the expense of an exponential increase in the number of
constraints. We next develop a cut-generation based algorithm. Rather than
\emph{explicitly} considering each disruption scenario to identify these
Benders cuts, however, we develop a bi-level program and corresponding
separation algorithm that enables us to \emph{implicitly} evaluate the
exponential set of disruption scenarios. Our computational results demonstrate
the efficacy of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1808</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1808</id><created>2011-09-08</created><authors><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Mayernik</keyname><forenames>Matthew S.</forenames></author></authors><title>The use of microblogging for field-based scientific research</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>Proceedings of the 45th Hawaii International Conference on System
  Science (HICSS-45 2012)</comments><doi>10.1109/HICSS.2012.589</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Documenting the context in which data are collected is an integral part of
the scientific research lifecycle. In field-based research, contextual
information provides a detailed description of scientific practices and thus
enables data interpretation and reuse. For field data, losing contextual
information often means losing the data altogether. Yet, documenting the
context of distributed, collaborative, field-based research can be a
significant challenge due to the unpredictable nature of real-world settings
and to the high degree of variability in data collection methods and scientific
practices of different researchers. In this article, we propose the use of
microblogging as a mechanism to support collection, ingestion, and publication
of contextual information about the variegated digital artifacts that are
produced in field research. We perform interviews with scholars involved in
field-based environmental and urban sensing research, to determine the extent
of adoption of Twitter and similar microblogging platforms and their potential
use for field-specific research applications. Based on the results of these
interviews as well as participant observation of field activities, we present
the design, development, and pilot evaluation of a microblogging application
integrated with an existing data collection platform on a handheld device. We
investigate whether microblogging accommodates the variable and unpredictable
nature of highly mobile research and whether it represents a suitable mechanism
to document the context of field research data early in the scientific
information lifecycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1841</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1841</id><created>2011-09-08</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author><author><keyname>Bowman</keyname><forenames>C. Mic</forenames></author></authors><title>Digital Libraries, Conceptual Knowledge Systems, and the Nebula
  Interface</title><categories>cs.DL cs.AI</categories><comments>Technical report, Transarc Corporation, Pittsburgh, Pennsylvania,
  April 1995</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept Analysis provides a principled approach to effective management of
wide area information systems, such as the Nebula File System and Interface.
This not only offers evidence to support the assertion that a digital library
is a bounded collection of incommensurate information sources in a logical
space, but also sheds light on techniques for collaboration through coordinated
access to the shared organization of knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1843</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1843</id><created>2011-09-08</created><authors><author><keyname>Phillips</keyname><forenames>Caleb</forenames></author><author><keyname>Sicker</keyname><forenames>Douglas</forenames></author><author><keyname>Grunwald</keyname><forenames>Dirk</forenames></author></authors><title>The Stability of The Longley-Rice Irregular Terrain Model for Typical
  Problems</title><categories>cs.NI</categories><comments>University of Colorado Technical Report</comments><report-no>CU-CS-1086-11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the numerical stability of the popular Longley-Rice
Irregular Terrain Model (ITM). This model is widely used to plan wireless
networks and in simulation-validated research and hence its stability is of
fundamental importance to the correctness of a large amount of work. We take a
systematic approach by first porting the reference ITM implementation to a
multiprecision framework and then generating loss predictions along many random
paths using real terrain data. We find that the ITM is not unstable for common
numerical precisions and practical prediction scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1844</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1844</id><created>2011-09-08</created><authors><author><keyname>Ackerman</keyname><forenames>Margareta</forenames></author><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author><author><keyname>Branzei</keyname><forenames>Simina</forenames></author><author><keyname>Loker</keyname><forenames>David</forenames></author></authors><title>Weighted Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate clustering in the weighted setting, in which
every data point is assigned a real valued weight. We conduct a theoretical
analysis on the influence of weighted data on standard clustering algorithms in
each of the partitional and hierarchical settings, characterising the precise
conditions under which such algorithms react to weights, and classifying
clustering methods into three broad categories: weight-responsive,
weight-considering, and weight-robust. Our analysis raises several interesting
questions and can be directly mapped to the classical unweighted setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1852</identifier>
 <datestamp>2011-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1852</id><created>2011-09-08</created><updated>2011-12-20</updated><authors><author><keyname>Wang</keyname><forenames>Chunyan</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Long Trend Dynamics in Social Media</title><categories>physics.soc-ph cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A main characteristic of social media is that its diverse content, copiously
generated by both standard outlets and general users, constantly competes for
the scarce attention of large audiences. Out of this flood of information some
topics manage to get enough attention to become the most popular ones and thus
to be prominently displayed as trends. Equally important, some of these trends
persist long enough so as to shape part of the social agenda. How this happens
is the focus of this paper. By introducing a stochastic dynamical model that
takes into account the user's repeated involvement with given topics, we can
predict the distribution of trend durations as well as the thresholds in
popularity that lead to their emergence within social media. Detailed
measurements of datasets from Twitter confirm the validity of the model and its
predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1865</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1865</id><created>2011-09-08</created><authors><author><keyname>Pandharkar</keyname><forenames>Rohit</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author></authors><title>Progressive versus Random Projections for Compressive Capture of Images,
  Lightfields and Higher Dimensional Visual Signals</title><categories>cs.CV</categories><comments>Draft of working paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational photography involves sophisticated capture methods. A new trend
is to capture projection of higher dimensional visual signals such as videos,
multi-spectral data and lightfields on lower dimensional sensors. Carefully
designed capture methods exploit the sparsity of the underlying signal in a
transformed domain to reduce the number of measurements and use an appropriate
reconstruction method. Traditional progressive methods may capture successively
more detail using a sequence of simple projection basis, such as DCT or
wavelets and employ straightforward backprojection for reconstruction.
Randomized projection methods do not use any specific sequence and use L0
minimization for reconstruction. In this paper, we analyze the statistical
properties of natural images, videos, multi-spectral data and light-fields and
compare the effectiveness of progressive and random projections. We define
effectiveness by plotting reconstruction SNR against compression factor. The
key idea is a procedure to measure best-case effectiveness that is fast,
independent of specific hardware and independent of the reconstruction
procedure. We believe this is the first empirical study to compare different
lossy capture strategies without the complication of hardware or reconstruction
ambiguity. The scope is limited to linear non-adaptive sensing. The results
show that random projections produce significant advantages over other
projections only for higher dimensional signals, and suggest more research to
nascent adaptive and non-linear projection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1874</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1874</id><created>2011-09-08</created><updated>2012-12-11</updated><authors><author><keyname>Zeeshan</keyname><forenames>Muhammad</forenames></author><author><keyname>Khan</keyname><forenames>Shoab A</forenames></author><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author></authors><title>A Capacity Improvement Method for CDMA based Mesh Networks in SUI
  Multipath Fading Channels</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a some mistakes in
  simulations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code Division Multiple Access (CDMA) is the most promising candidate for
wideband data access. This is due to the advantage of soft limit on the number
of active mobile devices. Many wireless mesh systems impose an upper bound on
the BER performance which restricts the increase in number of mobile users.
Capacity is further reduced in Multipath Fading Environment (MFE). This paper
presents an effective method of improving the capacity of a CDMA based mesh
network by managing the transmitted powers of the mobile devices and using MMSE
based Multiuser Detection (MUD). The proposed scheme improves the capacity two
times as compared to the conventional CDMA based mesh network. Simulation
results have been presented to demonstrate the effectiveness of the proposed
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1877</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1877</id><created>2011-09-08</created><authors><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author></authors><title>Efficient Implementation of Elliptic Curve Cryptography Using Low-power
  Digital Signal Processor</title><categories>cs.CR</categories><comments>IEEE ICACT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RSA(Rivest, Shamir and Adleman)is being used as a public key exchange and key
agreement tool for many years. Due to large numbers involved in RSA, there is
need for more efficient methods in implementation for public key cryptosystems.
Elliptic Curve Cryptography(ECC) is based on elliptic curves defined over a
finite field. Elliptic curve cryptosystems(ECC) were discovered by Victor
Miller and Neal Koblitz in 1985.This paper comprises of five sections. Section
I is introduction to ECC and its components. Section II describes advantages of
ECC schemes and its comparison with RSA. Section III is about some of the
applications of ECC. Section IV gives some embedded implementations of ECC.
Section V contains ECC implementation on fixed point Digital Signal
Processor(TMS320VC5416). ECC was implemented using general purpose
microcontrollers and Field Programmable Gate Arrays (FPGA) before this work.
DSP is more powerful than microcontrollers and much economical than FPGA. So
this implementation can be efficiently utilized in low-power applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1879</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1879</id><created>2011-09-08</created><authors><author><keyname>Qinghui</keyname><forenames>Tae</forenames></author><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author><author><keyname>Hong</keyname><forenames>Youngjee</forenames></author><author><keyname>Park</keyname><forenames>Jinwoo</forenames></author></authors><title>A Real-time Localization System Using RFID for Visually Impaired</title><categories>cs.MA cs.NI</categories><comments>9 pages, 5 figures, Presented at IEOM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gadgets helping the disabled, especially blind that are in least
accessibility of information, use acoustic methods that can cause stress to ear
and infringe user's privacy. Even if some project uses embedded Radio Frequency
Identification (RFID) into the sidewalk for blind's free walking, the tag
memory design is not specified for buildings and road conditions. This paper
suggested allocation scheme of RFID tag referring to EPCglobal SGLN, tactile
method for conveying information, and use of lithium battery as power source
with solar cells as an alternative. Results have shown independent mobility,
accidents prevention, stress relief and satisfied factors in terms of cost and
human usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1891</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1891</id><created>2011-09-09</created><authors><author><keyname>Dube</keyname><forenames>Mahesh R.</forenames></author><author><keyname>Dixit</keyname><forenames>Shantanu K.</forenames></author></authors><title>Comprehensive measurement framework for enterprise architectures</title><categories>cs.SE</categories><comments>22 Pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 3, No 4, August 2011</journal-ref><doi>10.5121/ijcsit.2011.3406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise Architecture defines the overall form and function of systems
across an enterprise involving the stakeholders and providing a framework,
standards and guidelines for project-specific architectures. Project-specific
Architecture defines the form and function of the systems in a project or
program, within the context of the enterprise as a whole with broad scope and
business alignments. Application-specific Architecture defines the form and
function of the applications that will be developed to realize functionality of
the system with narrow scope and technical alignments. Because of the magnitude
and complexity of any enterprise integration project, a major engineering and
operations planning effort must be accomplished prior to any actual integration
work. As the needs and the requirements vary depending on their volume, the
entire enterprise problem can be broken into chunks of manageable pieces. These
pieces can be implemented and tested individually with high integration effort.
Therefore it becomes essential to analyze the economic and technical
feasibility of realizable enterprise solution. It is difficult to migrate from
one technological and business aspect to other as the enterprise evolves. The
existing process models in system engineering emphasize on life-cycle
management and low-level activity coordination with milestone verification.
Many organizations are developing enterprise architecture to provide a clear
vision of how systems will support and enable their business. The paper
proposes an approach for selection of suitable enterprise architecture
depending on the measurement framework. The framework consists of unique
combination of higher order goals, non-functional requirement support and
inputs-outcomes pair evaluation. The earlier efforts in this regard were
concerned about only custom scales indicating the availability of a parameter
in a range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1895</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1895</id><created>2011-09-09</created><authors><author><keyname>Jin</keyname><forenames>Yuzhe</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Support Recovery of Sparse Signals in the Presence of Multiple
  Measurement Vectors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of support recovery of sparse signals based on
multiple measurement vectors (MMV). The MMV support recovery problem is
connected to the problem of decoding messages in a Single-Input Multiple-Output
(SIMO) multiple access channel (MAC), thereby enabling an information theoretic
framework for analyzing performance limits in recovering the support of sparse
signals. Sharp sufficient and necessary conditions for successful support
recovery are derived in terms of the number of measurements per measurement
vector, the number of nonzero rows, the measurement noise level, and especially
the number of measurement vectors. Through the interpretations of the results,
in particular the connection to the multiple output communication system, the
benefit of having MMV for sparse signal recovery is illustrated providing a
theoretical foundation to the performance improvement enabled by MMV as
observed in many existing simulation results. In particular, it is shown that
the structure (rank) of the matrix formed by the nonzero entries plays an
important role on the performance limits of support recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1900</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1900</id><created>2011-09-09</created><updated>2013-03-31</updated><authors><author><keyname>Boussaid</keyname><forenames>Nabile</forenames><affiliation>LM-Besan&#xe7;on</affiliation></author><author><keyname>Caponigro</keyname><forenames>Marco</forenames><affiliation>IECN, INRIA Nancy - Grand Est / IECN / LMAM</affiliation></author><author><keyname>Chambrion</keyname><forenames>Thomas</forenames><affiliation>IECN, INRIA Nancy - Grand Est / IECN / LMAM</affiliation></author></authors><title>Weakly-coupled systems in quantum control</title><categories>math.AP cs.SY math-ph math.MP math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides rigorous definitions and analysis of the dynamics of
weakly-coupled systems and gives sufficient conditions for an infinite
dimensional quantum control system to be weakly-coupled. As an illustration we
provide examples chosen among common physical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1905</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1905</id><created>2011-09-09</created><authors><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Bodin</keyname><forenames>Martin</forenames><affiliation>VERIMAG - IMAG, DI</affiliation></author></authors><title>Modular Abstractions of Reactive Nodes using Disjunctive Invariants</title><categories>cs.PL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We wish to abstract nodes in a reactive programming language, such as Lustre,
into nodes with a simpler control structure, with a bound on the number of
control states. In order to do so, we compute disjunctive invariants in
predicate abstraction, with a bounded number of disjuncts, then we abstract the
node, each disjunct representing an abstract state. The computation of the
disjunctive invariant is performed by a form of quantifier elimination
expressed using SMT-solving. The same method can also be used to obtain
disjunctive loop invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1913</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1913</id><created>2011-09-09</created><updated>2012-03-01</updated><authors><author><keyname>Junnila</keyname><forenames>Ville</forenames><affiliation>IF</affiliation></author><author><keyname>Laihonen</keyname><forenames>Tero</forenames><affiliation>IF</affiliation></author><author><keyname>Parreau</keyname><forenames>Aline</forenames><affiliation>IF</affiliation></author></authors><title>Tolerant identification with Euclidean balls</title><categories>cs.IT cs.DM math.CO math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of identifying codes was introduced by Karpovsky, Chakrabarty and
Levitin in 1998. The identifying codes can be applied, for example, to sensor
networks. In this paper, we consider as sensors the set Z^2 where one sensor
can check its neighbours within Euclidean distance r. We construct tolerant
identifying codes in this network that are robust against some changes in the
neighbourhood monitored by each sensor. We give bounds for the smallest density
of a tolerant identifying code for general values of r and Delta. We also
provide infinite families of values (r,Delta) with optimal such codes and study
the case of small values of r.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1914</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1914</id><created>2011-09-09</created><updated>2011-10-24</updated><authors><author><keyname>Thiery</keyname><forenames>Jean-Marc</forenames><affiliation>LTCI</affiliation></author><author><keyname>Tierny</keyname><forenames>Julien</forenames><affiliation>LTCI</affiliation></author><author><keyname>Boubekeur</keyname><forenames>Tamy</forenames><affiliation>LTCI</affiliation></author></authors><title>Jacobians and Hessians of Mean Value Coordinates for Closed Triangular
  Meshes</title><categories>cs.GR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical note, we present the formulae of the derivatives of the
Mean Value Coordinates based transformations, using an enclosing triangle mesh,
acting as a cage for the deformation of an interior object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1922</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1922</id><created>2011-09-09</created><authors><author><keyname>Vladislavleva</keyname><forenames>Katya</forenames></author><author><keyname>Friedrich</keyname><forenames>Tobias</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author><author><keyname>Wagner</keyname><forenames>Markus</forenames></author></authors><title>Predicting the Energy Output of Wind Farms Based on Weather Data:
  Important Variables and their Correlation</title><categories>cs.AI</categories><comments>13 pages, 11 figures, 2 tables</comments><acm-class>I.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wind energy plays an increasing role in the supply of energy world-wide. The
energy output of a wind farm is highly dependent on the weather condition
present at the wind farm. If the output can be predicted more accurately,
energy suppliers can coordinate the collaborative production of different
energy sources more efficiently to avoid costly overproductions.
  With this paper, we take a computer science perspective on energy prediction
based on weather data and analyze the important parameters as well as their
correlation on the energy output. To deal with the interaction of the different
parameters we use symbolic regression based on the genetic programming tool
DataModeler.
  Our studies are carried out on publicly available weather and energy data for
a wind farm in Australia. We reveal the correlation of the different variables
for the energy output. The model obtained for energy prediction gives a very
reliable prediction of the energy output for newly given weather data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1949</identifier>
 <datestamp>2012-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1949</id><created>2011-09-09</created><updated>2012-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Jianquan</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Alternative Awaiting and Broadcast for Two-Way Relay Fading Channels</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author as it is a duplicate of
  arXiv:1109.1041</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a two-way relay (TWR) fading channel based on
store-and-forward (SF), where two source nodes wish to exchange information
with the help of a relay node. A new upper bound on the ergodic sum-capacity
for the TWR fading system is derived when delay tends to infinity.We further
propose two alternative awaiting and broadcast (AAB) schemes: pure partial
decoding (PPD) with SF-I and combinatorial decoding (CBD) with SF-II, which
approach the new upper bound at high SNR with unbounded and bounded delay
respectively. Numerical results show that the proposed AAB schemes
significantly outperform the traditional physical layer network coding (PLNC)
methods without delay. Compared to the traditional TWR schemes without delay,
the proposed CBD with SF-II method significantly improves the maximum sum-rate
with an average delay of only some dozen seconds in the relay buffer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1951</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1951</id><created>2011-09-09</created><updated>2013-01-14</updated><authors><author><keyname>Bruner</keyname><forenames>Marie-Louise</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author></authors><title>A W[1]-Completeness Result for Generalized Permutation Pattern Matching</title><categories>cs.CC math.CO</categories><comments>The contents of this paper have been integrated in the more
  comprehensive paper &quot;The computational landscape of permutation patterns&quot;,
  arXiv:1301.0340</comments><msc-class>68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-complete Permutation Pattern Matching problem asks whether a
permutation P (the pattern) can be matched into a permutation T (the text). A
matching is an order-preserving embedding of P into T. In the Generalized
Permutation Pattern Matching problem one can additionally enforce that certain
adjacent elements in the pattern must be mapped to adjacent elements in the
text. This paper studies the parameterized complexity of this more general
problem. We show W[1]-completeness with respect to the length of the pattern P.
Under standard complexity theoretic assumptions this implies that no
fixed-parameter tractable algorithm can be found for any parameter depending
solely on P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1963</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1963</id><created>2011-09-09</created><updated>2013-06-17</updated><authors><author><keyname>Fritz</keyname><forenames>Tobias</forenames></author></authors><title>Velocity Polytopes of Periodic Graphs and a No-Go Theorem for Digital
  Physics</title><categories>math-ph cs.DM math.CO math.MG math.MP</categories><comments>18 pages, 1 figure. See also http://pirsa.org/12100100/. Corrigendum
  in v3: most mathematical results were obtained earlier by other authors,
  references have been included</comments><msc-class>Primary: 05C38, 05C22, Secondary: 52C07, 68R10</msc-class><journal-ref>Discrete Mathematics 313 (2013) pp. 1289-1301</journal-ref><doi>10.1016/j.disc.2013.02.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A periodic graph in dimension $d$ is a directed graph with a free action of
$\Z^d$ with only finitely many orbits. It can conveniently be represented in
terms of an associated finite graph with weights in $\Z^d$, corresponding to a
$\Z^d$-bundle with connection. Here we use the weight sums along cycles in this
associated graph to construct a certain polytope in $\R^d$, which we regard as
a geometrical invariant associated to the periodic graph. It is the unit ball
of a norm on $\R^d$ describing the large-scale geometry of the graph. It has a
physical interpretation as the set of attainable velocities of a particle on
the graph which can hop along one edge per timestep. Since a polytope
necessarily has distinguished directions, there is no periodic graph for which
this velocity set is isotropic. In the context of classical physics, this can
be viewed as a no-go theorem for the emergence of an isotropic space from a
discrete structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1966</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1966</id><created>2011-09-09</created><updated>2012-06-20</updated><authors><author><keyname>Hunter</keyname><forenames>Timothy</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre</forenames></author></authors><title>The path inference filter: model-based low-latency map matching of probe
  vehicle data</title><categories>cs.AI</categories><comments>Preprint, 23 pages and 23 figures</comments><doi>10.1016/j.trb.2013.03.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing vehicle trajectories from sparse
sequences of GPS points, for which the sampling interval is between 10 seconds
and 2 minutes. We introduce a new class of algorithms, called altogether path
inference filter (PIF), that maps GPS data in real time, for a variety of
trade-offs and scenarios, and with a high throughput. Numerous prior approaches
in map-matching can be shown to be special cases of the path inference filter
presented in this article. We present an efficient procedure for automatically
training the filter on new data, with or without ground truth observations. The
framework is evaluated on a large San Francisco taxi dataset and is shown to
improve upon the current state of the art. This filter also provides insights
about driving patterns of drivers. The path inference filter has been deployed
at an industrial scale inside the Mobile Millennium traffic information system,
and is used to map fleets of data in San Francisco, Sacramento, Stockholm and
Porto.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1989</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1989</id><created>2011-09-09</created><authors><author><keyname>Grace</keyname><forenames>L. K. Joshila</forenames></author><author><keyname>Maheswari</keyname><forenames>V.</forenames></author><author><keyname>Nagamalai</keyname><forenames>Dhinaharan</forenames></author></authors><title>Efficient Personalized Web Mining: Utilizing The Most Utilized Data</title><categories>cs.IR</categories><comments>conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looking into the growth of information in the web it is a very tedious
process of getting the exact information the user is looking for. Many search
engines generate user profile related data listing. This paper involves one
such process where the rating is given to the link that the user is clicking
on. Rather than avoiding the uninterested links both interested links and the
uninterested links are listed. But sorted according to the weightings given to
each link by the number of visit made by the particular user and the amount of
time spent on the particular link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1990</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1990</id><created>2011-09-09</created><authors><author><keyname>Grave</keyname><forenames>Edouard</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Trace Lasso: a trace norm regularization for correlated designs</title><categories>cs.LG stat.ML</categories><proxy>ccsd</proxy><journal-ref>Neural Information Processing Systems, Spain (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the $\ell_1$-norm to regularize the estimation of the parameter vector
of a linear model leads to an unstable estimator when covariates are highly
correlated. In this paper, we introduce a new penalty function which takes into
account the correlation of the design matrix to stabilize the estimation. This
norm, called the trace Lasso, uses the trace norm, which is a convex surrogate
of the rank, of the selected covariates as the criterion of model complexity.
We analyze the properties of our norm, describe an optimization algorithm based
on reweighted least-squares, and illustrate the behavior of this norm on
synthetic data, showing that it is more adapted to strong correlations than
competing methods such as the elastic net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1991</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1991</id><created>2011-09-09</created><authors><author><keyname>Grace</keyname><forenames>L. K. Joshila</forenames></author><author><keyname>Maheswari</keyname><forenames>V.</forenames></author><author><keyname>Nagamalai</keyname><forenames>Dhinaharan</forenames></author></authors><title>Effective Personalized Web Mining by Utilizing The Most Utilized Data</title><categories>cs.IR</categories><comments>9 pages, journal paper</comments><journal-ref>International Journal of Database Management Systems ( IJDMS ),
  Vol.3, No.3, August 2011</journal-ref><doi>10.5121/ijdms.2011.3309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looking into the growth of information in the web it is a very tedious
process of getting the exact information the user is looking for. Many search
engines generate user profile related data listing. This paper involves one
such process where the rating is given to the link that the user is clicking
on. Rather than avoiding the uninterested links both interested links and the
uninterested links are listed. But sorted according to the weightings given to
each link by the number of visit made by the particular user and the amount of
time spent on the particular link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.1994</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.1994</id><created>2011-09-09</created><updated>2011-10-09</updated><authors><author><keyname>Friggeri</keyname><forenames>Adrien</forenames><affiliation>ENS / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme / INRIA Grenoble Rh&#xf4;ne-Alpes, IXXI</affiliation></author><author><keyname>Fleury</keyname><forenames>Eric</forenames><affiliation>ENS / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme / INRIA Grenoble Rh&#xf4;ne-Alpes, IXXI</affiliation></author></authors><title>Maximizing the Cohesion is NP-hard</title><categories>cs.NI cs.CC</categories><comments>No. RR-7734 (2011)</comments><proxy>ccsd</proxy><report-no>RR-7734</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem of finding a set with maximum cohesion in an
undirected network is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2015</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2015</id><created>2011-09-09</created><authors><author><keyname>Hallerstede</keyname><forenames>Stefan</forenames></author><author><keyname>Leuschel</keyname><forenames>Michael</forenames></author></authors><title>Constraint-Based Deadlock Checking of High-Level Specifications</title><categories>cs.LO cs.PL cs.SE</categories><journal-ref>Theory and Practice of Logic Programming 11(4--5): 767--782, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Establishing the absence of deadlocks is important in many applications of
formal methods. The use of model checking for finding deadlocks in formal
models is often limited. In this paper we propose a constraint-based approach
to finding deadlocks employing the ProB constraint solver. We present the
general technique, as well as various improvements that had to be performed on
ProB's Prolog kernel, such as reification of membership and arithmetic
constraints. This work was guided by an industrial case study, where a team
from Bosch was modeling a cruise control system. Within this case study, ProB
was able to quickly find counter examples to very large deadlock-freedom
constraints. In the paper, we also present other successful applications of
this new technique. Experiments using SAT and SMT solvers on these constraints
were thus far unsuccessful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2028</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2028</id><created>2011-09-09</created><authors><author><keyname>Javaid</keyname><forenames>Nadeem</forenames></author><author><keyname>Bibi</keyname><forenames>Ayesha</forenames></author><author><keyname>Javaid</keyname><forenames>Akmal</forenames></author><author><keyname>Malik</keyname><forenames>Shahzad A.</forenames></author></authors><title>Modeling Routing Overhead Generated by Wireless Proactive Routing
  Protocols</title><categories>cs.NI</categories><journal-ref>54th IEEE Globecom2011 (SaCoNAS Workshop)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a detailed framework consisting of modeling of
routing overhead generated by three widely used proactive routing protocols;
Destination-Sequenced Distance Vector (DSDV), Fish-eye State Routing (FSR) and
Optimized Link State Routing (OLSR). The questions like, how these protocols
differ from each other on the basis of implementing different routing
strategies, how neighbor estimation errors affect broadcast of route requests,
how reduction of broadcast overhead achieves bandwidth, how to cope with the
problem of mobility and density, etc, are attempted to respond. In all of the
above mentioned situations, routing overhead and delay generated by the chosen
protocols can exactly be calculated from our modeled equations. Finally, we
analyze the performance of selected routing protocols using our proposed
framework in NS-2 by considering different performance parameters; Route
REQuest (RREQ) packet generation, End-to-End Delay (E2ED) and Normalized
Routing Load (NRL) with respect to varying rates of mobility and density of
nodes in the underlying wireless network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2034</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2034</id><created>2011-09-09</created><updated>2013-08-22</updated><authors><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Osendorfer</keyname><forenames>Christian</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>Learning Sequence Neighbourhood Metrics</title><categories>cs.NE cs.LG</categories><comments>Artificial Neural Networks and Machine Learning ICANN 2012 Springer
  Berlin Heidelberg 2012. 531-538</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNNs) in combination with a pooling operator and
the neighbourhood components analysis (NCA) objective function are able to
detect the characterizing dynamics of sequences and embed them into a
fixed-length vector space of arbitrary dimensionality. Subsequently, the
resulting features are meaningful and can be used for visualization or nearest
neighbour classification in linear time. This kind of metric learning for
sequential data enables the use of algorithms tailored towards fixed length
vector spaces such as R^n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2044</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2044</id><created>2011-09-09</created><updated>2013-03-12</updated><authors><author><keyname>Migda&#x142;</keyname><forenames>Piotr</forenames></author><author><keyname>Denkiewicz</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Rcaczaszek-Leonardi</keyname><forenames>Joanna</forenames></author><author><keyname>Plewczynski</keyname><forenames>Dariusz</forenames></author></authors><title>Information-sharing and aggregation models for interacting minds</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>22 pages, 4 figures, 2 tables; after the final revision</comments><journal-ref>Journal of Mathematical Psychology 56 (2012) 417-426</journal-ref><doi>10.1016/j.jmp.2013.01.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study mathematical models of the collaborative solving of a two-choice
discrimination task. We estimate the difference between the shared performance
for a group of n observers over a single person performance. Our paper is a
theoretical extension of the recent work of Bahrami et al. (2010) from a dyad
(a pair) to a group of n interacting minds. We analyze several models of
communication, decision-making and hierarchical information-aggregation.
  The maximal slope of psychometric function (closely related to the percentage
of right answers vs. easiness of the task) is a convenient parameter
characterizing performance. For every model we investigated, the group
performance turns out to be a product of two numbers: a scaling factor
depending of the group size and an average performance. The scaling factor is a
power function of the group size (with the exponent ranging from 0 to 1),
whereas the average is arithmetic mean, quadratic mean, or maximum of the
individual slopes. Moreover, voting can be almost as efficient as more
elaborate communication models, given the participants have similar individual
performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2047</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2047</id><created>2011-09-09</created><authors><author><keyname>Chawla</keyname><forenames>N. V.</forenames></author><author><keyname>Karakoulas</keyname><forenames>Grigoris</forenames></author></authors><title>Learning From Labeled And Unlabeled Data: An Empirical Study Across
  Techniques And Domains</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  331-366, 2005</journal-ref><doi>10.1613/jair.1509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been increased interest in devising learning techniques that
combine unlabeled data with labeled data ? i.e. semi-supervised learning.
However, to the best of our knowledge, no study has been performed across
various techniques and different types and amounts of labeled and unlabeled
data. Moreover, most of the published work on semi-supervised learning
techniques assumes that the labeled and unlabeled data come from the same
distribution. It is possible for the labeling process to be associated with a
selection bias such that the distributions of data points in the labeled and
unlabeled sets are different. Not correcting for such bias can result in biased
function approximation with potentially poor performance. In this paper, we
present an empirical study of various semi-supervised learning techniques on a
variety of datasets. We attempt to answer various questions such as the effect
of independence or relevance amongst features, the effect of the size of the
labeled and unlabeled sets and the effect of noise. We also investigate the
impact of sample-selection bias on the semi-supervised learning techniques
under study and implement a bivariate probit technique particularly designed to
correct for such bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2048</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2048</id><created>2011-09-09</created><authors><author><keyname>Barish</keyname><forenames>G.</forenames></author><author><keyname>Knoblock</keyname><forenames>C. A.</forenames></author></authors><title>An Expressive Language and Efficient Execution System for Software
  Agents</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  625-666, 2005</journal-ref><doi>10.1613/jair.1548</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software agents can be used to automate many of the tedious, time-consuming
information processing tasks that humans currently have to complete manually.
However, to do so, agent plans must be capable of representing the myriad of
actions and control flows required to perform those tasks. In addition, since
these tasks can require integrating multiple sources of remote information ?
typically, a slow, I/O-bound process ? it is desirable to make execution as
efficient as possible. To address both of these needs, we present a flexible
software agent plan language and a highly parallel execution system that enable
the efficient execution of expressive agent plans. The plan language allows
complex tasks to be more easily expressed by providing a variety of operators
for flexibly processing the data as well as supporting subplans (for
modularity) and recursion (for indeterminate looping). The executor is based on
a streaming dataflow model of execution to maximize the amount of operator and
data parallelism possible at runtime. We have implemented both the language and
executor in a system called THESEUS. Our results from testing THESEUS show that
streaming dataflow execution can yield significant speedups over both
traditional serial (von Neumann) as well as non-streaming dataflow-style
execution that existing software and robot agent execution systems currently
support. In addition, we show how plans written in the language we present can
represent certain types of subtasks that cannot be accomplished using the
languages supported by network query engines. Finally, we demonstrate that the
increased expressivity of our plan language does not hamper performance;
specifically, we show how data can be integrated from multiple remote sources
just as efficiently using our architecture as is possible with a
state-of-the-art streaming-dataflow network query engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2049</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2049</id><created>2011-09-09</created><authors><author><keyname>Belov</keyname><forenames>Anton</forenames></author><author><keyname>J&#xe4;rvisalo</keyname><forenames>Matti</forenames></author></authors><title>Structure-Based Local Search Heuristics for Circuit-Level Boolean
  Satisfiability</title><categories>cs.AI</categories><comments>15 pages</comments><journal-ref>Presented at 8th International Workshop on Local Search Techniques
  in Constraint Satisfaction (LSCS 2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on improving state-of-the-art in stochastic local search
(SLS) for solving Boolean satisfiability (SAT) instances arising from
real-world industrial SAT application domains. The recently introduced SLS
method CRSat has been shown to noticeably improve on previously suggested SLS
techniques in solving such real-world instances by combining
justification-based local search with limited Boolean constraint propagation on
the non-clausal formula representation form of Boolean circuits. In this work,
we study possibilities of further improving the performance of CRSat by
exploiting circuit-level structural knowledge for developing new search
heuristics for CRSat. To this end, we introduce and experimentally evaluate a
variety of search heuristics, many of which are motivated by circuit-level
heuristics originally developed in completely different contexts, e.g., for
electronic design automation applications. To the best of our knowledge, most
of the heuristics are novel in the context of SLS for SAT and, more generally,
SLS for constraint satisfaction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2058</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2058</id><created>2011-09-09</created><authors><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>Text mining and visualization using VOSviewer</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VOSviewer is a computer program for creating, visualizing, and exploring
bibliometric maps of science. In this report, the new text mining functionality
of VOSviewer is presented. A number of examples are given of applications in
which VOSviewer is used for analyzing large amounts of text data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2066</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2066</id><created>2011-09-09</created><authors><author><keyname>Arapinis</keyname><forenames>Myrto</forenames></author><author><keyname>Mancini</keyname><forenames>Loretta Ilaria</forenames></author><author><keyname>Ritter</keyname><forenames>Eike</forenames></author><author><keyname>Ryan</keyname><forenames>Mark</forenames></author></authors><title>Formal Analysis of UMTS Privacy</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ubiquitous presence of mobile communication devices and the continuous
development of mo- bile data applications, which results in high level of
mobile devices' activity and exchanged data, often transparent to the user,
makes privacy preservation an important feature of mobile telephony systems. We
present a formal analysis of the UMTS Authentication and Key Agreement
protocol, using the applied pi-calculus and the ProVerif tool. We formally
verify the model with respect to privacy properties. We show a linkability
attack which makes it possible, for individuals with low-cost equipment, to
trace UMTS subscribers. The attack exploits information leaked by poorly
designed error messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2067</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2067</id><created>2011-09-09</created><authors><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>On the Impossibility of Black-Box Transformations in Mechanism Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of converting an arbitrary approximation algorithm
for a single-parameter optimization problem into a computationally efficient
truthful mechanism. We ask for reductions that are black-box, meaning that they
require only oracle access to the given algorithm and in particular do not
require explicit knowledge of the problem constraints. Such a reduction is
known to be possible, for example, for the social welfare objective when the
goal is to achieve Bayesian truthfulness and preserve social welfare in
expectation. We show that a black-box reduction for the social welfare
objective is not possible if the resulting mechanism is required to be truthful
in expectation and to preserve the worst-case approximation ratio of the
algorithm to within a subpolynomial factor. Further, we prove that for other
objectives such as makespan, no black-box reduction is possible even if we only
require Bayesian truthfulness and an average-case performance guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2075</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2075</id><created>2011-09-09</created><authors><author><keyname>Lima</keyname><forenames>Arlindo</forenames></author><author><keyname>Goul&#xe3;o</keyname><forenames>Miguel</forenames></author><author><keyname>Monteiro</keyname><forenames>Miguel Pessoa</forenames></author></authors><title>Evidence-Based Comparison of Modularity Support Between Java and Object
  Teams</title><categories>cs.SE</categories><comments>Proceedings of Empirical Evaluation of Software Composition
  Techniques (ESCOT 2010), at 9th International Conference on Aspect-Oriented
  Software Development (AOSD'2010), Rennes and Saint Malo, France, 2010</comments><acm-class>D.2.2; D.2.8</acm-class><journal-ref>Proceedings of Empirical Evaluation of Software Composition
  Techniques (ESCOT 2010), at 9th International Conference on Aspect-Oriented
  Software Development (AOSD'2010), Rennes and Saint Malo, France, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Aspect-oriented programming (AOP) is an emerging programming
paradigm whose focus is about improving modularity, with an emphasis on the
modularization of crosscutting concerns.
  Objective: The goal of this paper is to assess the extent to which an AOP
language -ObjectTeams/Java (OT/J) -improves the modularity of a software
system. This improvement has been claimed but, to the best of our knowledge,
this paper is the first attempting to present quantitative evidence of it.
  Method: We compare functionally-equivalent implementations of the
Gang-of-Four design patterns, developed in Java and OT/J, using software
metrics.
  Results: The results of our comparison support the modularity improvement
claims made in the literature. For six of the seven metrics used, the OT/J
versions of the patterns obtained significantly better results.
  Limitations: This work uses a set of metrics originally defined for
object-oriented (OO) systems. It may be the case that the metrics are biased,
in that they were created in the context of OO programming (OOP), before the
advent of AOP. We consider this comparison a stepping stone as, ultimately, we
plan to assess the modularity improvements with paradigm independent metrics,
which will conceivably eliminate the bias. Each individual example from the
sample used in this paper is small. In future, we plan to replicate this
experiment using larger systems, where the benefits of AOP may be more
noticeable.
  Conclusion: This work contributes with evidence to fill gaps in the body of
quantitative results supporting alleged benefits to software modularity brought
by AOP languages, namely OT/J.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2088</identifier>
 <datestamp>2011-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2088</id><created>2011-09-09</created><authors><author><keyname>Gai</keyname><forenames>Yi</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Online Learning Algorithms for Stochastic Water-Filling</title><categories>cs.LG cs.NI cs.SY math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Water-filling is the term for the classic solution to the problem of
allocating constrained power to a set of parallel channels to maximize the
total data-rate. It is used widely in practice, for example, for power
allocation to sub-carriers in multi-user OFDM systems such as WiMax. The
classic water-filling algorithm is deterministic and requires perfect knowledge
of the channel gain to noise ratios. In this paper we consider how to do power
allocation over stochastically time-varying (i.i.d.) channels with unknown gain
to noise ratio distributions. We adopt an online learning framework based on
stochastic multi-armed bandits. We consider two variations of the problem, one
in which the goal is to find a power allocation to maximize $\sum\limits_i
\mathbb{E}[\log(1 + SNR_i)]$, and another in which the goal is to find a power
allocation to maximize $\sum\limits_i \log(1 + \mathbb{E}[SNR_i])$. For the
first problem, we propose a \emph{cognitive water-filling} algorithm that we
call CWF1. We show that CWF1 obtains a regret (defined as the cumulative gap
over time between the sum-rate obtained by a distribution-aware genie and this
policy) that grows polynomially in the number of channels and logarithmically
in time, implying that it asymptotically achieves the optimal time-averaged
rate that can be obtained when the gain distributions are known. For the second
problem, we present an algorithm called CWF2, which is, to our knowledge, the
first algorithm in the literature on stochastic multi-armed bandits to exploit
non-linear dependencies between the arms. We prove that the number of times
CWF2 picks the incorrect power allocation is bounded by a function that is
polynomial in the number of channels and logarithmic in time, implying that its
frequency of incorrect allocation tends to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2112</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2112</id><created>2011-09-09</created><updated>2011-11-28</updated><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>Plumettaz</keyname><forenames>Matthieu</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author></authors><title>A local strengthening of Reed's {\omega}, \Delta, {\chi} conjecture for
  quasi-line graphs</title><categories>cs.DM math.CO</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reed's $\omega$, $\Delta$, $\chi$ conjecture proposes that every graph
satisfies $\chi\leq \lceil\frac 12(\Delta+1+\omega)\rceil$; it is known to hold
for all claw-free graphs. In this paper we consider a local strengthening of
this conjecture. We prove the local strengthening for line graphs, then note
that previous results immediately tell us that the local strengthening holds
for all quasi-line graphs. Our proofs lead to polytime algorithms for
constructing colourings that achieve our bounds: $O(n^2)$ for line graphs and
$O(n^3m^2)$ for quasi-line graphs. For line graphs, this is faster than the
best known algorithm for constructing a colouring that achieves the bound of
Reed's original conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2114</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2114</id><created>2011-09-09</created><authors><author><keyname>Kharitonov</keyname><forenames>Daniel</forenames></author></authors><title>Net-Centric World: Lifestyle of the 21st Century</title><categories>cs.CY cs.HC</categories><comments>Preprint for ITU-T Kaleidoscope 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we research the potential of information communication
technologies (ICTs) for changing our society from a commute-centric to a
network-centric environment. We propose to formalize the key attributes of
ICT-based telecommuting experiences from both economic and human interactivity
perspective. We introduce the notion of network-eligible transactions and
disclose the link between degree of network centricity and worker settlement
radius, postulating that media-rich network services have a strong potential to
increase the physical distance between work and home locations. We also
highlight notable technology challenges and opportunities of migration from
location-based to mobile living, signifying the needs for new services and
standards development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2127</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2127</id><created>2011-09-09</created><authors><author><keyname>Bayer-Zubek</keyname><forenames>V.</forenames></author><author><keyname>Dietterich</keyname><forenames>T. G.</forenames></author></authors><title>Integrating Learning from Examples into the Search for Diagnostic
  Policies</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  263-303, 2005</journal-ref><doi>10.1613/jair.1512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of learning diagnostic policies from training
examples. A diagnostic policy is a complete description of the decision-making
actions of a diagnostician (i.e., tests followed by a diagnostic decision) for
all possible combinations of test results. An optimal diagnostic policy is one
that minimizes the expected total cost, which is the sum of measurement costs
and misdiagnosis costs. In most diagnostic settings, there is a tradeoff
between these two kinds of costs. This paper formalizes diagnostic decision
making as a Markov Decision Process (MDP). The paper introduces a new family of
systematic search algorithms based on the AO* algorithm to solve this MDP. To
make AO* efficient, the paper describes an admissible heuristic that enables
AO* to prune large parts of the search space. The paper also introduces several
greedy algorithms including some improvements over previously-published
methods. The paper then addresses the question of learning diagnostic policies
from examples. When the probabilities of diseases and test results are computed
from training data, there is a great danger of overfitting. To reduce
overfitting, regularizers are integrated into the search algorithms. Finally,
the paper compares the proposed methods on five benchmark diagnostic data sets.
The studies show that in most cases the systematic search methods produce
better diagnostic policies than the greedy methods. In addition, the studies
show that for training sets of realistic size, the systematic search algorithms
are practical on todays desktop computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2128</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2128</id><created>2011-09-09</created><updated>2011-09-26</updated><authors><author><keyname>Erkan</keyname><forenames>Gunes</forenames></author><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames></author></authors><title>LexRank: Graph-based Lexical Centrality as Salience in Text
  Summarization</title><categories>cs.CL</categories><proxy>Daniel Bryce</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  457-479, 2004</journal-ref><doi>10.1613/jair.1523</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a stochastic graph-based method for computing relative
importance of textual units for Natural Language Processing. We test the
technique on the problem of Text Summarization (TS). Extractive TS relies on
the concept of sentence salience to identify the most important sentences in a
document or set of documents. Salience is typically defined in terms of the
presence of particular important words or in terms of similarity to a centroid
pseudo-sentence. We consider a new approach, LexRank, for computing sentence
importance based on the concept of eigenvector centrality in a graph
representation of sentences. In this model, a connectivity matrix based on
intra-sentence cosine similarity is used as the adjacency matrix of the graph
representation of sentences. Our system, based on LexRank ranked in first place
in more than one task in the recent DUC 2004 evaluation. In this paper we
present a detailed analysis of our approach and apply it to a larger data set
including data from earlier DUC evaluations. We discuss several methods to
compute centrality using the similarity graph. The results show that
degree-based methods (including LexRank) outperform both centroid-based methods
and other systems participating in DUC in most of the cases. Furthermore, the
LexRank with threshold method outperforms the other degree-based techniques
including continuous LexRank. We also show that our approach is quite
insensitive to the noise in the data that may result from an imperfect topical
clustering of documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2129</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2129</id><created>2011-09-09</created><authors><author><keyname>Dunne</keyname><forenames>P. E.</forenames></author></authors><title>Extremal Behaviour in Multiagent Contract Negotiation</title><categories>cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  41-78, 2005</journal-ref><doi>10.1613/jair.1526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine properties of a model of resource allocation in which several
agents exchange resources in order to optimise their individual holdings. The
schemes discussed relate to well-known negotiation protocols proposed in
earlier work and we consider a number of alternative notions of rationality
covering both quantitative measures, e.g. cooperative and individual
rationality and more qualitative forms, e.g. Pigou-Dalton transfers. While it
is known that imposing particular rationality and structural restrictions may
result in some reallocations of the resource set becoming unrealisable, in this
paper we address the issue of the number of restricted rational deals that may
be required to implement a particular reallocation when it is possible to do
so. We construct examples showing that this number may be exponential (in the
number of resources m), even when all of the agent utility functions are
monotonic. We further show that k agents may achieve in a single deal a
reallocation requiring exponentially many rational deals if at most k-1 agents
can participate, this same reallocation being unrealisable by any sequences of
rational deals in which at most k-2 agents are involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2130</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2130</id><created>2011-09-09</created><authors><author><keyname>Montoyo</keyname><forenames>A.</forenames></author><author><keyname>Palomar</keyname><forenames>M.</forenames></author><author><keyname>Rigau</keyname><forenames>G.</forenames></author><author><keyname>Suarez</keyname><forenames>A.</forenames></author></authors><title>Combining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  299-330, 2005</journal-ref><doi>10.1613/jair.1529</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we concentrate on the resolution of the lexical ambiguity that
arises when a given word has several different meanings. This specific task is
commonly referred to as word sense disambiguation (WSD). The task of WSD
consists of assigning the correct sense to words using an electronic dictionary
as the source of word definitions. We present two WSD methods based on two main
methodological approaches in this research area: a knowledge-based method and a
corpus-based method. Our hypothesis is that word-sense disambiguation requires
several knowledge sources in order to solve the semantic ambiguity of the
words. These sources can be of different kinds--- for example, syntagmatic,
paradigmatic or statistical information. Our approach combines various sources
of knowledge, through combinations of the two WSD methods mentioned above.
Mainly, the paper concentrates on how to combine these methods and sources of
information in order to achieve good results in the disambiguation. Finally,
this paper presents a comprehensive study and experimental work on evaluation
of the methods and their combinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2131</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2131</id><created>2011-09-09</created><authors><author><keyname>Larrosa</keyname><forenames>J.</forenames></author><author><keyname>Morancho</keyname><forenames>E.</forenames></author><author><keyname>Niso</keyname><forenames>D.</forenames></author></authors><title>On the Practical use of Variable Elimination in Constraint Optimization
  Problems: 'Still-life' as a Case Study</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  421-440, 2005</journal-ref><doi>10.1613/jair.1541</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable elimination is a general technique for constraint processing. It is
often discarded because of its high space complexity. However, it can be
extremely useful when combined with other techniques. In this paper we study
the applicability of variable elimination to the challenging problem of finding
still-lifes. We illustrate several alternatives: variable elimination as a
stand-alone algorithm, interleaved with search, and as a source of good quality
lower bounds. We show that these techniques are the best known option both
theoretically and empirically. In our experiments we have been able to solve
the n=20 instance, which is far beyond reach with alternative approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2132</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2132</id><created>2011-09-09</created><authors><author><keyname>Nair</keyname><forenames>R.</forenames></author><author><keyname>Tambe</keyname><forenames>M.</forenames></author></authors><title>Hybrid BDI-POMDP Framework for Multiagent Teaming</title><categories>cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  367-420, 2005</journal-ref><doi>10.1613/jair.1549</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many current large-scale multiagent team implementations can be characterized
as following the belief-desire-intention (BDI) paradigm, with explicit
representation of team plans. Despite their promise, current BDI team
approaches lack tools for quantitative performance analysis under uncertainty.
Distributed partially observable Markov decision problems (POMDPs) are well
suited for such analysis, but the complexity of finding optimal policies in
such models is highly intractable. The key contribution of this article is a
hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP
tractability and POMDP analysis improves BDI team plan performance. Concretely,
we focus on role allocation, a fundamental problem in BDI teams: which agents
to allocate to the different roles in the team. The article provides three key
contributions. First, we describe a role allocation technique that takes into
account future uncertainties in the domain; prior work in multiagent role
allocation has failed to address such uncertainties. To that end, we introduce
RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model
for analysis of role allocations. Our technique gains in tractability by
significantly curtailing RMTDP policy search; in particular, BDI team plans
provide incomplete RMTDP policies, and the RMTDP policy search fills the gaps
in such incomplete policies by searching for the best role allocation. Our
second key contribution is a novel decomposition technique to further improve
RMTDP policy search efficiency. Even though limited to searching role
allocations, there are still combinatorially many role allocations, and
evaluating each in RMTDP to identify the best is extremely difficult. Our
decomposition technique exploits the structure in the BDI team plans to
significantly prune the search space of role allocations. Our third key
contribution is a significantly faster policy evaluation algorithm suited for
our BDI-POMDP hybrid approach. Finally, we also present experimental results
from two domains: mission rehearsal simulation and RoboCupRescue disaster
rescue simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2134</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2134</id><created>2011-09-09</created><authors><author><keyname>Dixon</keyname><forenames>H. E.</forenames></author><author><keyname>Ginsberg</keyname><forenames>M. L.</forenames></author><author><keyname>Luks</keyname><forenames>E. M.</forenames></author><author><keyname>Parkes</keyname><forenames>A. J.</forenames></author></authors><title>Generalizing Boolean Satisfiability II: Theory</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  481-534, 2004</journal-ref><doi>10.1613/jair.1555</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the second of three planned papers describing ZAP, a satisfiability
engine that substantially generalizes existing tools while retaining the
performance characteristics of modern high performance solvers. The fundamental
idea underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal is to define a representation in which this structure is apparent and can
easily be exploited to improve computational performance. This paper presents
the theoretical basis for the ideas underlying ZAP, arguing that existing ideas
in this area exploit a single, recurring structure in that multiple database
axioms can be obtained by operating on a single axiom using a subgroup of the
group of permutations on the literals in the problem. We argue that the group
structure precisely captures the general structure at which earlier approaches
hinted, and give numerous examples of its use. We go on to extend the
Davis-Putnam-Logemann-Loveland inference procedure to this broader setting, and
show that earlier computational improvements are either subsumed or left intact
by the new method. The third paper in this series discusses ZAPs implementation
and presents experimental performance results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2135</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2135</id><created>2011-09-09</created><authors><author><keyname>Doshi</keyname><forenames>P.</forenames></author><author><keyname>Gmytrasiewicz</keyname><forenames>P. J.</forenames></author></authors><title>A Framework for Sequential Planning in Multi-Agent Settings</title><categories>cs.AI cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  49-79, 2005</journal-ref><doi>10.1613/jair.1579</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the framework of partially observable Markov decision
processes (POMDPs) to multi-agent settings by incorporating the notion of agent
models into the state space. Agents maintain beliefs over physical states of
the environment and over models of other agents, and they use Bayesian updates
to maintain their beliefs over time. The solutions map belief states to
actions. Models of other agents may include their belief states and are related
to agent types considered in games of incomplete information. We express the
agents autonomy by postulating that their models are not directly manipulable
or observable by other agents. We show that important properties of POMDPs,
such as convergence of value iteration, the rate of convergence, and piece-wise
linearity and convexity of the value functions carry over to our framework. Our
approach complements a more traditional approach to interactive settings which
uses Nash equilibria as a solution paradigm. We seek to avoid some of the
drawbacks of equilibria which may be non-unique and do not capture
off-equilibrium behaviors. We do so at the cost of having to represent, process
and continuously revise models of other agents. Since the agents beliefs may be
arbitrarily nested, the optimal solutions to decision making problems are only
asymptotically computable. However, approximate belief updates and
approximately optimal plans are computable. We illustrate our framework using a
simple application domain, and we show examples of belief updates and value
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2136</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2136</id><created>2011-09-09</created><authors><author><keyname>Jordan</keyname><forenames>P. W.</forenames></author><author><keyname>Walker</keyname><forenames>M. A.</forenames></author></authors><title>Learning Content Selection Rules for Generating Object Descriptions in
  Dialogue</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  157-194, 2005</journal-ref><doi>10.1613/jair.1591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental requirement of any task-oriented dialogue system is the ability
to generate object descriptions that refer to objects in the task domain. The
subproblem of content selection for object descriptions in task-oriented
dialogue has been the focus of much previous work and a large number of models
have been proposed. In this paper, we use the annotated COCONUT corpus of
task-oriented design dialogues to develop feature sets based on Dale and
Reiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact
model, and Jordans (2000b) intentional influences model, and use these feature
sets in a machine learning experiment to automatically learn a model of content
selection for object descriptions. Since Dale and Reiters model requires a
representation of discourse structure, the corpus annotations are used to
derive a representation based on Grosz and Sidners (1986) theory of the
intentional structure of discourse, as well as two very simple representations
of discourse structure based purely on recency. We then apply the
rule-induction program RIPPER to train and test the content selection component
of an object description generator on a set of 393 object descriptions from the
corpus. To our knowledge, this is the first reported experiment of a trainable
content selection component for object description generation in dialogue.
Three separate content selection models that are based on the three theoretical
models, all independently achieve accuracies significantly above the majority
class baseline (17%) on unseen test data, with the intentional influences model
(42.4%) performing significantly better than either the incremental model
(30.4%) or the conceptual pact model (28.9%). But the best performing models
combine all the feature sets, achieving accuracies near 60%. Surprisingly, a
simple recency-based representation of discourse structure does as well as one
based on intentional structure. To our knowledge, this is also the first
empirical comparison of a representation of Grosz and Sidners model of
discourse structure with a simpler model for any generation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2137</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2137</id><created>2011-09-09</created><authors><author><keyname>Domingos</keyname><forenames>P.</forenames></author><author><keyname>Sanghai</keyname><forenames>S.</forenames></author><author><keyname>Weld</keyname><forenames>D.</forenames></author></authors><title>Relational Dynamic Bayesian Networks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  759-797, 2005</journal-ref><doi>10.1613/jair.1625</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic processes that involve the creation of objects and relations over
time are widespread, but relatively poorly studied. For example, accurate fault
diagnosis in factory assembly processes requires inferring the probabilities of
erroneous assembly operations, but doing this efficiently and accurately is
difficult. Modeled as dynamic Bayesian networks, these processes have discrete
variables with very large domains and extremely high dimensionality. In this
paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an
extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a
generalization of dynamic probabilistic relational models (DPRMs), which we had
proposed in our previous work to model dynamic uncertain domains. We first
extend the Rao-Blackwellised particle filtering described in our earlier work
to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in
RDBNs and propose two new forms of particle filtering. The first one uses
abstraction hierarchies over the predicates to smooth the particle filters
estimates. The second employs kernel density estimation with a kernel function
specifically designed for relational domains. Experiments show these two
methods greatly outperform standard particle filtering on the task of assembly
plan execution monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2138</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2138</id><created>2011-09-09</created><authors><author><keyname>Foo</keyname><forenames>N. Y.</forenames></author><author><keyname>Vo</keyname><forenames>Q. B.</forenames></author></authors><title>Reasoning about Action: An Argumentation - Theoretic Approach</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  465-518, 2005</journal-ref><doi>10.1613/jair.1602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a uniform non-monotonic solution to the problems of reasoning
about action on the basis of an argumentation-theoretic approach. Our theory is
provably correct relative to a sensible minimisation policy introduced on top
of a temporal propositional logic. Sophisticated problem domains can be
formalised in our framework. As much attention of researchers in the field has
been paid to the traditional and basic problems in reasoning about actions such
as the frame, the qualification and the ramification problems, approaches to
these problems within our formalisation lie at heart of the expositions
presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2139</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2139</id><created>2011-09-09</created><authors><author><keyname>Hawkins</keyname><forenames>P. J.</forenames></author><author><keyname>Lagoon</keyname><forenames>V.</forenames></author><author><keyname>Stuckey</keyname><forenames>P. J.</forenames></author></authors><title>Solving Set Constraint Satisfaction Problems using ROBDDs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  109-156, 2005</journal-ref><doi>10.1613/jair.1638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach to modeling finite set domain
constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We
show that it is possible to construct an efficient set domain propagator which
compactly represents many set domains and set constraints using ROBDDs. We
demonstrate that the ROBDD-based approach provides unprecedented flexibility in
modeling constraint satisfaction problems, leading to performance improvements.
We also show that the ROBDD-based modeling approach can be extended to the
modeling of integer and multiset constraint problems in a straightforward
manner. Since domain propagation is not always practical, we also show how to
incorporate less strict consistency notions into the ROBDD framework, such as
set bounds, cardinality bounds and lexicographic bounds consistency. Finally,
we present experimental results that demonstrate the ROBDD-based solver
performs better than various more conventional constraint solvers on several
standard set constraint problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2140</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2140</id><created>2011-09-09</created><authors><author><keyname>Cimiano</keyname><forenames>P.</forenames></author><author><keyname>Hotho</keyname><forenames>A.</forenames></author><author><keyname>Staab</keyname><forenames>S.</forenames></author></authors><title>Learning Concept Hierarchies from Text Corpora using Formal Concept
  Analysis</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  305-339, 2005</journal-ref><doi>10.1613/jair.1648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to the automatic acquisition of taxonomies or
concept hierarchies from a text corpus. The approach is based on Formal Concept
Analysis (FCA), a method mainly used for the analysis of data, i.e. for
investigating and processing explicitly given information. We follow Harris
distributional hypothesis and model the context of a certain term as a vector
representing syntactic dependencies which are automatically acquired from the
text corpus with a linguistic parser. On the basis of this context information,
FCA produces a lattice that we convert into a special kind of partial order
constituting a concept hierarchy. The approach is evaluated by comparing the
resulting concept hierarchies with hand-crafted taxonomies for two domains:
tourism and finance. We also directly compare our approach with hierarchical
agglomerative clustering as well as with Bi-Section-KMeans as an instance of a
divisive clustering algorithm. Furthermore, we investigate the impact of using
different measures weighting the contribution of each attribute as well as of
applying a particular smoothing technique to cope with data sparseness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2141</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2141</id><created>2011-09-09</created><authors><author><keyname>Khardon</keyname><forenames>R.</forenames></author><author><keyname>Roth</keyname><forenames>D.</forenames></author><author><keyname>Servedio</keyname><forenames>R. A.</forenames></author></authors><title>Efficiency versus Convergence of Boolean Kernels for On-Line Learning
  Algorithms</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  341-356, 2005</journal-ref><doi>10.1613/jair.1655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies machine learning problems where each example is described
using a set of Boolean features and where hypotheses are represented by linear
threshold elements. One method of increasing the expressiveness of learned
hypotheses in this context is to expand the feature set to include conjunctions
of basic features. This can be done explicitly or where possible by using a
kernel function. Focusing on the well known Perceptron and Winnow algorithms,
the paper demonstrates a tradeoff between the computational efficiency with
which the algorithm can be run over the expanded feature space and the
generalization ability of the corresponding learning algorithm. We first
describe several kernel functions which capture either limited forms of
conjunctions or all conjunctions. We show that these kernels can be used to
efficiently run the Perceptron algorithm over a feature space of exponentially
many conjunctions; however we also show that using such kernels, the Perceptron
algorithm can provably make an exponential number of mistakes even when
learning simple functions. We then consider the question of whether kernel
functions can analogously be used to run the multiplicative-update Winnow
algorithm over an expanded feature space of exponentially many conjunctions.
Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal
Form (DNF) formulae with a polynomial mistake bound in this setting. However,
we prove that it is computationally hard to simulate Winnows behavior for
learning DNF over such a feature set. This implies that the kernel functions
which correspond to running Winnow for this problem are not efficiently
computable, and that there is no general construction that can run Winnow with
kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2142</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2142</id><created>2011-09-09</created><authors><author><keyname>Dixon</keyname><forenames>H. E.</forenames></author><author><keyname>Ginsberg</keyname><forenames>M. L.</forenames></author><author><keyname>Hofer</keyname><forenames>D.</forenames></author><author><keyname>Luks</keyname><forenames>E. M.</forenames></author><author><keyname>Parkes</keyname><forenames>A. J.</forenames></author></authors><title>Generalizing Boolean Satisfiability III: Implementation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  441-531, 2005</journal-ref><doi>10.1613/jair.1656</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the third of three papers describing ZAP, a satisfiability engine
that substantially generalizes existing tools while retaining the performance
characteristics of modern high-performance solvers. The fundamental idea
underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal has been to define a representation in which this structure is apparent
and can be exploited to improve computational performance. The first paper
surveyed existing work that (knowingly or not) exploited problem structure to
improve the performance of satisfiability engines, and the second paper showed
that this structure could be understood in terms of groups of permutations
acting on individual clauses in any particular Boolean theory. We conclude the
series by discussing the techniques needed to implement our ideas, and by
reporting on their performance on a variety of problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2143</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2143</id><created>2011-09-09</created><authors><author><keyname>Jaeger</keyname><forenames>M.</forenames></author></authors><title>Ignorability in Statistical and Probabilistic Inference</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  889-917, 2005</journal-ref><doi>10.1613/jair.1657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with incomplete data in statistical learning, or incomplete
observations in probabilistic inference, one needs to distinguish the fact that
a certain event is observed from the fact that the observed event has happened.
Since the modeling and computational complexities entailed by maintaining this
proper distinction are often prohibitive, one asks for conditions under which
it can be safely ignored. Such conditions are given by the missing at random
(mar) and coarsened at random (car) assumptions. In this paper we provide an
in-depth analysis of several questions relating to mar/car assumptions. Main
purpose of our study is to provide criteria by which one may evaluate whether a
car assumption is reasonable for a particular data collecting or observational
process. This question is complicated by the fact that several distinct
versions of mar/car assumptions exist. We therefore first provide an overview
over these different versions, in which we highlight the distinction between
distributional and coarsening variable induced versions. We show that
distributional versions are less restrictive and sufficient for most
applications. We then address from two different perspectives the question of
when the mar/car assumption is warranted. First we provide a static analysis
that characterizes the admissibility of the car assumption in terms of the
support structure of the joint probability distribution of complete data and
incomplete observations. Here we obtain an equivalence characterization that
improves and extends a recent result by Grunwald and Halpern. We then turn to a
procedural analysis that characterizes the admissibility of the car assumption
in terms of procedural models for the actual data (or observation) generating
process. The main result of this analysis is that the stronger coarsened
completely at random (ccar) condition is arguably the most reasonable
assumption, as it alone corresponds to data coarsening procedures that satisfy
a natural robustness property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2145</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2145</id><created>2011-09-09</created><authors><author><keyname>Spaan</keyname><forenames>M. T. J.</forenames></author><author><keyname>Vlassis</keyname><forenames>N.</forenames></author></authors><title>Perseus: Randomized Point-based Value Iteration for POMDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  195-220, 2005</journal-ref><doi>10.1613/jair.1659</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially observable Markov decision processes (POMDPs) form an attractive
and principled framework for agent planning under uncertainty. Point-based
approximate techniques for POMDPs compute a policy based on a finite set of
points collected in advance from the agents belief space. We present a
randomized point-based value iteration algorithm called Perseus. The algorithm
performs approximate value backup stages, ensuring that in each backup stage
the value of each point in the belief set is improved; the key observation is
that a single backup may improve the value of many belief points. Contrary to
other point-based methods, Perseus backs up only a (randomly selected) subset
of points in the belief set, sufficient for improving the value of each belief
point in the set. We show how the same idea can be extended to dealing with
continuous action spaces. Experimental results show the potential of Perseus in
large scale POMDP problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2146</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2146</id><created>2011-09-09</created><authors><author><keyname>Garc&#xed;a-Pedrajas</keyname><forenames>N.</forenames></author><author><keyname>Herv&#xe1;s-Mart&#xed;nez</keyname><forenames>C.</forenames></author><author><keyname>Ortiz-Boyer</keyname><forenames>D.</forenames></author></authors><title>CIXL2: A Crossover Operator for Evolutionary Algorithms Based on
  Population Features</title><categories>cs.NE</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  1-48, 2005</journal-ref><doi>10.1613/jair.1660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a crossover operator for evolutionary algorithms
with real values that is based on the statistical theory of population
distributions. The operator is based on the theoretical distribution of the
values of the genes of the best individuals in the population. The proposed
operator takes into account the localization and dispersion features of the
best individuals of the population with the objective that these features would
be inherited by the offspring. Our aim is the optimization of the balance
between exploration and exploitation in the search process. In order to test
the efficiency and robustness of this crossover, we have used a set of
functions to be optimized with regard to different criteria, such as,
multimodality, separability, regularity and epistasis. With this set of
functions we can extract conclusions in function of the problem at hand. We
analyze the results using ANOVA and multiple comparison statistical tests. As
an example of how our crossover can be used to solve artificial intelligence
problems, we have applied the proposed model to the problem of obtaining the
weight of each network in a ensemble of neural networks. The results obtained
are above the performance of standard methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2147</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2147</id><created>2011-09-09</created><authors><author><keyname>Geibel</keyname><forenames>P.</forenames></author><author><keyname>Wysotzki</keyname><forenames>F.</forenames></author></authors><title>Risk-Sensitive Reinforcement Learning Applied to Control under
  Constraints</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  81-108, 2005</journal-ref><doi>10.1613/jair.1666</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider Markov Decision Processes (MDPs) with error
states. Error states are those states entering which is undesirable or
dangerous. We define the risk with respect to a policy as the probability of
entering such a state when the policy is pursued. We consider the problem of
finding good policies whose risk is smaller than some user-specified threshold,
and formalize it as a constrained MDP with two criteria. The first criterion
corresponds to the value function originally given. We will show that the risk
can be formulated as a second criterion function based on a cumulative return,
whose definition is independent of the original value function. We present a
model free, heuristic reinforcement learning algorithm that aims at finding
good deterministic policies. It is based on weighting the original value
function and the risk. The weight parameter is adapted in order to find a
feasible solution for the constrained problem that has a good performance with
respect to the value function. The algorithm was successfully applied to the
control of a feed tank with stochastic inflows that lies upstream of a
distillation column. This control task was originally formulated as an optimal
control problem with chance constraints, and it was solved under certain
assumptions on the model to obtain an optimal solution. The power of our
learning algorithm is that it can be used even when some of these restrictive
assumptions are relaxed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2148</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2148</id><created>2011-09-09</created><authors><author><keyname>De Raedt</keyname><forenames>L.</forenames></author><author><keyname>Kersting</keyname><forenames>K.</forenames></author><author><keyname>Raiko</keyname><forenames>T.</forenames></author></authors><title>Logical Hidden Markov Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  425-456, 2006</journal-ref><doi>10.1613/jair.1675</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov
models to deal with sequences of structured symbols in the form of logical
atoms, rather than flat characters.
  This note formally introduces LOHMMs and presents solutions to the three
central inference problems for LOHMMs: evaluation, most likely hidden state
sequence and parameter estimation. The resulting representation and algorithms
are experimentally evaluated on problems from the domain of bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2152</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2152</id><created>2011-09-09</created><authors><author><keyname>Gottlob</keyname><forenames>G.</forenames></author><author><keyname>Greco</keyname><forenames>G.</forenames></author><author><keyname>Scarcello</keyname><forenames>F.</forenames></author></authors><title>Pure Nash Equilibria: Hard and Easy Games</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  357-406, 2005</journal-ref><doi>10.1613/jair.1683</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate complexity issues related to pure Nash equilibria of strategic
games. We show that, even in very restrictive settings, determining whether a
game has a pure Nash Equilibrium is NP-hard, while deciding whether a game has
a strong Nash equilibrium is SigmaP2-complete. We then study practically
relevant restrictions that lower the complexity. In particular, we are
interested in quantitative and qualitative restrictions of the way each players
payoff depends on moves of other players. We say that a game has small
neighborhood if the utility function for each player depends only on (the
actions of) a logarithmically small number of other players. The dependency
structure of a game G can be expressed by a graph DG(G) or by a hypergraph
H(G). By relating Nash equilibrium problems to constraint satisfaction problems
(CSPs), we show that if G has small neighborhood and if H(G) has bounded
hypertree width (or if DG(G) has bounded treewidth), then finding pure Nash and
Pareto equilibria is feasible in polynomial time. If the game is graphical,
then these problems are LOGCFL-complete and thus in the class NC2 of highly
parallelizable problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2153</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2153</id><created>2011-09-09</created><authors><author><keyname>Bonet</keyname><forenames>B.</forenames></author><author><keyname>Geffner</keyname><forenames>H.</forenames></author></authors><title>mGPT: A Probabilistic Planner Based on Heuristic Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  933-944, 2005</journal-ref><doi>10.1613/jair.1688</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the version of the GPT planner used in the probabilistic track of
the 4th International Planning Competition (IPC-4). This version, called mGPT,
solves Markov Decision Processes specified in the PPDDL language by extracting
and using different classes of lower bounds along with various heuristic-search
algorithms. The lower bounds are extracted from deterministic relaxations where
the alternative probabilistic effects of an action are mapped into different,
independent, deterministic actions. The heuristic-search algorithms use these
lower bounds for focusing the updates and delivering a consistent value
function over all states reachable from the initial state and the greedy
policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2154</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2154</id><created>2011-09-09</created><authors><author><keyname>Botea</keyname><forenames>A.</forenames></author><author><keyname>Enzenberger</keyname><forenames>M.</forenames></author><author><keyname>Mueller</keyname><forenames>M.</forenames></author><author><keyname>Schaeffer</keyname><forenames>J.</forenames></author></authors><title>Macro-FF: Improving AI Planning with Automatically Learned
  Macro-Operators</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  581-621, 2005</journal-ref><doi>10.1613/jair.1696</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite recent progress in AI planning, many benchmarks remain challenging
for current planners. In many domains, the performance of a planner can greatly
be improved by discovering and exploiting information about the domain
structure that is not explicitly encoded in the initial PDDL formulation. In
this paper we present and compare two automated methods that learn relevant
information from previous experience in a domain and use it to solve new
problem instances. Our methods share a common four-step strategy. First, a
domain is analyzed and structural information is extracted, then
macro-operators are generated based on the previously discovered structure. A
filtering and ranking procedure selects the most useful macro-operators.
Finally, the selected macros are used to speed up future searches. We have
successfully used such an approach in the fourth international planning
competition IPC-4. Our system, Macro-FF, extends Hoffmanns state-of-the-art
planner FF 2.3 with support for two kinds of macro-operators, and with
engineering enhancements. We demonstrate the effectiveness of our ideas on
benchmarks from international planning competitions. Our results indicate a
large reduction in search effort in those complex domains where structural
information can be inferred.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2155</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2155</id><created>2011-09-09</created><authors><author><keyname>Kambhampati</keyname><forenames>S.</forenames></author><author><keyname>Briel</keyname><forenames>M. H. L. van den</forenames></author></authors><title>Optiplan: Unifying IP-based and Graph-based Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  919-931, 2005</journal-ref><doi>10.1613/jair.1698</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Optiplan planning system is the first integer programming-based planner
that successfully participated in the international planning competition. This
engineering note describes the architecture of Optiplan and provides the
integer programming formulation that enabled it to perform reasonably well in
the competition. We also touch upon some recent developments that make integer
programming encodings significantly more competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2156</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2156</id><created>2011-09-09</created><authors><author><keyname>Fern</keyname><forenames>A.</forenames></author><author><keyname>Givan</keyname><forenames>R.</forenames></author><author><keyname>Yoon</keyname><forenames>S.</forenames></author></authors><title>Approximate Policy Iteration with a Policy Language Bias: Solving
  Relational Markov Decision Processes</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  75-118, 2006</journal-ref><doi>10.1613/jair.1700</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an approach to policy selection for large relational Markov Decision
Processes (MDPs). We consider a variant of approximate policy iteration (API)
that replaces the usual value-function learning step with a learning step in
policy space. This is advantageous in domains where good policies are easier to
represent and learn than the corresponding value functions, which is often the
case for the relational MDPs we are interested in. In order to apply API to
such problems, we introduce a relational policy language and corresponding
learner. In addition, we introduce a new bootstrapping routine for goal-based
planning domains, based on random walks. Such bootstrapping is necessary for
many large relational MDPs, where reward is extremely sparse, as API is
ineffective in such domains when initialized with an uninformed policy. Our
experiments show that the resulting system is able to find good policies for a
number of classical planning domains and their stochastic variants by solving
them as extremely large relational MDPs. The experiments also point to some
limitations of our approach, suggesting future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2158</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2158</id><created>2011-09-09</created><authors><author><keyname>Berberich</keyname><forenames>Eric</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Pogalnikova</keyname><forenames>Roza</forenames></author></authors><title>Deconstructing Approximate Offsets</title><categories>cs.CG cs.DS</categories><comments>18 pages, 11 figures, previous version accepted at SoCG 2011,
  submitted to DCG</comments><msc-class>68U05</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the offset-deconstruction problem: Given a polygonal shape Q with
n vertices, can it be expressed, up to a tolerance \eps in Hausdorff distance,
as the Minkowski sum of another polygonal shape P with a disk of fixed radius?
If it does, we also seek a preferably simple-looking solution P; then, P's
offset constitutes an accurate, vertex-reduced, and smoothened approximation of
Q. We give an O(n log n)-time exact decision algorithm that handles any
polygonal shape, assuming the real-RAM model of computation. A variant of the
algorithm, which we have implemented using CGAL, is based on rational
arithmetic and answers the same deconstruction problem up to an uncertainty
parameter \delta; its running time additionally depends on \delta. If the input
shape is found to be approximable, this algorithm also computes an approximate
solution for the problem. It also allows us to solve parameter-optimization
problems induced by the offset-deconstruction problem. For convex shapes, the
complexity of the exact decision algorithm drops to O(n), which is also the
time required to compute a solution P with at most one more vertex than a
vertex-minimal one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2162</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2162</id><created>2011-09-09</created><authors><author><keyname>McGrae</keyname><forenames>Andrew R. A.</forenames></author><author><keyname>Zito</keyname><forenames>Michele</forenames></author></authors><title>The Complexity of the Empire Colouring Problem</title><categories>cs.CC</categories><comments>23 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational complexity of the empire colouring problem
(as defined by Percy Heawood in 1890) for maps containing empires formed by
exactly $r &gt; 1$ countries each. We prove that the problem can be solved in
polynomial time using $s$ colours on maps whose underlying adjacency graph has
no induced subgraph of average degree larger than $s/r$. However, if $s \geq
3$, the problem is NP-hard even if the graph is a forest of paths of arbitrary
lengths (for any $r \geq 2$, provided $s &lt; 2r - \sqrt{2r + 1/4+ 3/2).
Furthermore we obtain a complete characterization of the problem's complexity
for the case when the input graph is a tree, whereas our result for arbitrary
planar graphs fall just short of a similar dichotomy. Specifically, we prove
that the empire colouring problem is NP-hard for trees, for any $r \geq 2$, if
$3 \leq s \leq 2r-1$ (and polynomial time solvable otherwise). For arbitrary
planar graphs we prove NP-hardness if $s&lt;7$ for $r=2$, and $s &lt; 6r-3$, for $r
\geq 3$. The result for planar graphs also proves the NP-hardness of colouring
with less than 7 colours graphs of thickness two and less than $6r-3$ colours
graphs of thickness $r \geq 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2169</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2169</id><created>2011-09-09</created><authors><author><keyname>Frackiewicz</keyname><forenames>Piotr</forenames></author></authors><title>Quantum information approach to the ultimatum game</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to quantization of extensive games with the use of both
the Marinatto-Weber and the Eisert-Wilkens-Lewenstein concept of quantum game.
We revise the current conception of quantum ultimatum game and we show why the
proposal is unacceptable. To support our comment, we present the new idea of
the quantum ultimatum game. Our scheme also makes a point of departure for a
protocol to quantize extensive games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2174</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2174</id><created>2011-09-09</created><authors><author><keyname>Choudhary</keyname><forenames>K.</forenames></author><author><keyname>Margulies</keyname><forenames>S.</forenames></author><author><keyname>Hicks</keyname><forenames>I. V.</forenames></author></authors><title>A Note on Total and Paired Domination of Cartesian Product Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dominating set $D$ for a graph $G$ is a subset of $V(G)$ such that any
vertex not in $D$ has at least one neighbor in $D$. The domination number
$\gamma(G)$ is the size of a minimum dominating set in $G$. Vizing's conjecture
from 1968 states that for the Cartesian product of graphs $G$ and $H$,
$\gamma(G) \gamma(H) \leq \gamma(G \Box H)$, and Clark and Suen (2000) proved
that $\gamma(G) \gamma(H) \leq 2\gamma(G \Box H)$. In this paper, we modify the
approach of Clark and Suen to prove a variety of similar bounds related to
total and paired domination, and also extend these bounds to the $n$-Cartesian
product of graphs $A^1$ through $A^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2176</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2176</id><created>2011-09-09</created><authors><author><keyname>Khot</keyname><forenames>Subhash</forenames></author><author><keyname>Popat</keyname><forenames>Preyas</forenames></author><author><keyname>Vishnoi</keyname><forenames>Nisheeth K.</forenames></author></authors><title>$2^{\log^{1-\eps} n}$ Hardness for Closest Vector Problem with
  Preprocessing</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for an arbitrarily small constant $\eps&gt;0,$ assuming NP$\not
\subseteq$DTIME$(2^{{\log^{O(1/\eps)} n}})$, the preprocessing versions of the
closest vector problem and the nearest codeword problem are hard to approximate
within a factor better than $2^{\log ^{1-\eps}n}.$ This improves upon the
previous hardness factor of $(\log n)^\delta$ for some $\delta &gt; 0$ due to
\cite{AKKV05}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2215</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2215</id><created>2011-09-10</created><updated>2011-11-20</updated><authors><author><keyname>Yan</keyname><forenames>Bowen</forenames></author><author><keyname>Gregory</keyname><forenames>Steve</forenames></author></authors><title>Finding missing edges and communities in incomplete networks</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>15 pages, 9 figures</comments><journal-ref>J. Phys. A: Math. Theor. 44 (2011) 495102</journal-ref><doi>10.1088/1751-8113/44/49/495102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms have been proposed for predicting missing edges in networks,
but they do not usually take account of which edges are missing. We focus on
networks which have missing edges of the form that is likely to occur in real
networks, and compare algorithms that find these missing edges. We also
investigate the effect of this kind of missing data on community detection
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2222</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2222</id><created>2011-09-10</created><authors><author><keyname>Wortel</keyname><forenames>Lars</forenames></author></authors><title>Side Effects in Steering Fragments</title><categories>cs.LO cs.PL</categories><comments>Master's thesis - Master of Logic - University of Amsterdam</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis I will give a formal definition of side effects. I will do so
by modifying a system for modelling program instructions and program states,
Quantified Dynamic Logic, to a system called DLAf (for Dynamic Logic with
Assignments as Formulas), which in contrast to QDL allows assignments in
formulas and makes use of short-circuit evaluation. I will show the underlying
logic in those formulas to be a variant of short-circuit logic called
repetition-proof short-circuit logic.
  Using DLAf I will define the actual and the expected evaluation of a single
instruction. The side effects are then defined to be the difference between the
two. I will give rules for composing those side effects in single instructions,
thus scaling up our definition of side effects to a definition of side effects
in deterministic \dlaf-programs. Using this definition I will give a
classification of side effects, introducing as most important class that of
marginal side effects. Finally, I will show how to use our system for
calculating the side effects in a real system such as Program Algebra (PGA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2227</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2227</id><created>2011-09-10</created><updated>2011-09-28</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal Narayan</forenames></author></authors><title>A radial version of the Central Limit Theorem</title><categories>cs.IT cs.CV math.IT math.PR</categories><comments>3 pages, 2 figures; minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we give a probabilistic interpretation of the Central Limit
Theorem used for approximating isotropic Gaussians in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2229</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2229</id><created>2011-09-10</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>A Learning Theory Approach to Non-Interactive Database Privacy</title><categories>cs.DS cs.CR cs.LG</categories><comments>Full Version. Extended Abstract appeared in STOC 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate that, ignoring computational constraints, it is
possible to privately release synthetic databases that are useful for large
classes of queries -- much larger in size than the database itself.
Specifically, we give a mechanism that privately releases synthetic data for a
class of queries over a discrete domain with error that grows as a function of
the size of the smallest net approximately representing the answers to that
class of queries. We show that this in particular implies a mechanism for
counting queries that gives error guarantees that grow only with the
VC-dimension of the class of queries, which itself grows only logarithmically
with the size of the query class.
  We also show that it is not possible to privately release even simple classes
of queries (such as intervals and their generalizations) over continuous
domains. Despite this, we give a privacy-preserving polynomial time algorithm
that releases information useful for all halfspace queries, given a slight
relaxation of the utility guarantee. This algorithm does not release synthetic
data, but instead another data structure capable of representing an answer for
each query. We also give an efficient algorithm for releasing synthetic data
for the class of interval queries and axis-aligned rectangles of constant
dimension.
  Finally, inspired by learning theory, we introduce a new notion of data
privacy, which we call distributional privacy, and show that it is strictly
stronger than the prevailing privacy notion, differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2231</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2231</id><created>2011-09-10</created><authors><author><keyname>Mohanty</keyname><forenames>Rakesh</forenames></author><author><keyname>Sharma</keyname><forenames>Burle</forenames></author><author><keyname>Tripathy</keyname><forenames>Sasmita</forenames></author></authors><title>Characterization of Request Sequences for List Accessing Problem and New
  Theoretical Results for MTF Algorithm</title><categories>cs.DS</categories><comments>06 pages, 1 figure</comments><journal-ref>International Journal of Computer Applications, Volume 22-- No.8,
  May 2011</journal-ref><doi>10.5120/2601-3627</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  List Accessing Problem is a well studied research problem in the context of
linear search. Input to the list accessing problem is an unsorted linear list
of distinct elements along with a sequence of requests, where each request is
an access operation on an element of the list. A list accessing algorithm
reorganizes the list while processing a request sequence on the list in order
to minimize the access cost. Move-To-Front algorithm has been proved to be the
best performing list accessing online algorithm till date in the literature.
Characterization of the input request sequences corresponding to practical real
life situations is a big challenge for the list accessing problem. As far as
our knowledge is concerned, no characterization for the request sequences has
been done in the literature till date for the list accessing problem. In this
paper, we have characterized the request sequences for the list accessing
problem based on several factors such as size of the list, size of the request
sequence, ordering of elements and frequency of occurrence of elements in the
request sequence. We have made a comprehensive study of MTF list accessing
algorithm and obtained new theoretical results for our characterized special
class of request sequences. Our characterization will open up a new direction
of research for empirical analysis of list accessing algorithms for real life
inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2232</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2232</id><created>2011-09-10</created><authors><author><keyname>Mohanty</keyname><forenames>Rakesh</forenames></author><author><keyname>Bhoi</keyname><forenames>Seetaya</forenames></author><author><keyname>Tripathy</keyname><forenames>Sasmita</forenames></author></authors><title>A New Proposed Cost Model for List Accessing Problem using Buffering</title><categories>cs.DS</categories><comments>05 Pages, 2 figures</comments><journal-ref>International Journal of Computer Applications, Volume 22-- No.8,
  May 2011</journal-ref><doi>10.5120/2604-3631</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many existing well known cost models for the list accessing
problem. The standard cost model developed by Sleator and Tarjan is most widely
used. In this paper, we have made a comprehensive study of the existing cost
models and proposed a new cost model for the list accessing problem. In our
proposed cost model, for calculating the processing cost of request sequence
using a singly linked list, we consider the access cost, matching cost and
replacement cost. The cost of processing a request sequence is the sum of
access cost, matching cost and replacement cost. We have proposed a novel
method for processing the request sequence which does not consider the
rearrangement of the list and uses the concept of buffering, matching, look
ahead and flag bit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2237</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2237</id><created>2011-09-10</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>The World is Either Algorithmic or Mostly Random</title><categories>cs.IT math.IT physics.data-an physics.pop-ph</categories><comments>Third Prize Winning Essay -- 2011 Foundational Questions Institute
  (FQXi) Contest &quot;Is Reality Digital or Analog?&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I will propose the notion that the universe is digital, not as a claim about
what the universe is made of but rather about the way it unfolds. Central to
the argument will be the concepts of symmetry breaking and algorithmic
probability, which will be used as tools to compare the way patterns are
distributed in our world to the way patterns are distributed in a simulated
digital one. These concepts will provide a framework for a discussion of the
informational nature of reality. I will argue that if the universe were analog,
then the world would likely be random, making it largely incomprehensible. The
digital model has, however, an inherent beauty in its imposition of an upper
limit and in the convergence in computational power to a maximal level of
sophistication. Even if deterministic, that it is digital doesn't mean that the
world is trivial or predictable, but rather that it is built up from operations
that at the lowest scale are very simple but that at a higher scale look
complex and even random, though only in appearance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2247</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2247</id><created>2011-09-10</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>The Standard Aspect of Dialectical Logic</title><categories>cs.LO cs.PL math.CT</categories><comments>An abstracted version of this paper, entitled &quot;Dialectical Program
  Semantics&quot;, was accepted for presentation at the 1st International Conference
  on Algebraic Methodology and Software Technology (AMAST'89), University of
  Iowa, Iowa City, Iowa, 1989</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialectical logic is the logic of dialectical processes. The goal of
dialectical logic is to introduce dynamic notions into logical computational
systems. The fundamental notions of proposition and truth-value in standard
logic are subsumed by the notions of process and flow in dialectical logic.
Dialectical logic has a standard aspect, which can be defined in terms of the
&quot;local cartesian closure&quot; of subtypes. The standard aspect of dialectical logic
provides a natural program semantics which incorporates Hoare's
precondition/postcondition semantics and extends the standard Kripke semantics
of dynamic logic. The goal of the standard aspect of dialectical logic is to
unify the logic of small-scale and large-scale programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2271</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2271</id><created>2011-09-10</created><updated>2011-12-28</updated><authors><author><keyname>Chen</keyname><forenames>Tianqi</forenames></author><author><keyname>Zheng</keyname><forenames>Zhao</forenames></author><author><keyname>Lu</keyname><forenames>Qiuxia</forenames></author><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author></authors><title>Feature-Based Matrix Factorization</title><categories>cs.AI cs.IR</categories><comments>Minor update, add some related works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender system has been more and more popular and widely used in many
applications recently. The increasing information available, not only in
quantities but also in types, leads to a big challenge for recommender system
that how to leverage these rich information to get a better performance. Most
traditional approaches try to design a specific model for each scenario, which
demands great efforts in developing and modifying models. In this technical
report, we describe our implementation of feature-based matrix factorization.
This model is an abstract of many variants of matrix factorization models, and
new types of information can be utilized by simply defining new features,
without modifying any lines of code. Using the toolkit, we built the best
single model reported on track 1 of KDDCup'11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2275</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2275</id><created>2011-09-10</created><updated>2011-10-30</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>On Phase Transition of Compressed Sensing in the Complex Domain</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><journal-ref>IEEE Signal Processing Letters, vol. 19, no. 1, pp. 47--50, 2012</journal-ref><doi>10.1109/LSP.2011.2177496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phase transition is a performance measure of the sparsity-undersampling
tradeoff in compressed sensing (CS). This letter reports our first observation
and evaluation of an empirical phase transition of the $\ell_1$ minimization
approach to the complex valued CS (CVCS), which is positioned well above the
known phase transition of the real valued CS in the phase plane. This result
can be considered as an extension of the existing phase transition theory of
the block-sparse CS (BSCS) based on the universality argument, since the CVCS
problem does not meet the condition required by the phase transition theory of
BSCS but its observed phase transition coincides with that of BSCS. Our result
is obtained by applying the recently developed ONE-L1 algorithms to the
empirical evaluation of the phase transition of CVCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2282</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2282</id><created>2011-09-11</created><authors><author><keyname>Balakrishnan</keyname><forenames>Bhargav.</forenames></author></authors><title>Efficiency of Biometric integration with Salt Value at an Enterprise
  Level and Data Centres</title><categories>cs.CR</categories><comments>26 Pages 9 Figures Intech Open access publishers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter is going to deal with enhancing the efficiency of Biometric by
integrating it with Salt Value (randomly generated value of varying length).
Normally at an enterprise level or data centres, the servers are maintained
with complex passwords and they are known only to the system administrators.
Even after applying lot of securities at an expert level, the hackers are able
to penetrate through the network and break the passwords easily. Here how the
biometric can play a vital role and that too with the inclusion of Salt value
can prevent the hacker from stealing the confidential data's of an
organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2285</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2285</id><created>2011-09-11</created><authors><author><keyname>Balakrishnan</keyname><forenames>Bhargav.</forenames></author></authors><title>Case study and analysis of WAN Optimization pre-requirements</title><categories>cs.NI</categories><comments>5 Pages 3 Figures IEEE German section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with HOW to analyze the requirements for setting up the WAN
Optimizer. The criteria's that needs to be taken into account, the steps
involved in the analysis of WAN optimization requirement. These entire analyses
will give a complete framework for setting up a WAN optimizer within an
organization and the organization will have a clear record on the analysis made
before setting up this WAN Optimizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2288</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2288</id><created>2011-09-11</created><authors><author><keyname>Kernbach</keyname><forenames>S.</forenames></author><author><keyname>Schlachter</keyname><forenames>F.</forenames></author><author><keyname>Humza</keyname><forenames>R.</forenames></author><author><keyname>Liedke</keyname><forenames>J.</forenames></author><author><keyname>Popesku</keyname><forenames>S.</forenames></author><author><keyname>Russo</keyname><forenames>S.</forenames></author><author><keyname>Ranzani</keyname><forenames>T.</forenames></author><author><keyname>Manfredi</keyname><forenames>L.</forenames></author><author><keyname>Stefanini</keyname><forenames>C.</forenames></author><author><keyname>Matthias</keyname><forenames>R.</forenames></author><author><keyname>Schwarzer</keyname><forenames>Ch.</forenames></author><author><keyname>Girault</keyname><forenames>B.</forenames></author><author><keyname>Alschbach</keyname><forenames>P.</forenames></author><author><keyname>Meister</keyname><forenames>E.</forenames></author><author><keyname>Scholz</keyname><forenames>O.</forenames></author></authors><title>Heterogeneity for Increasing Performance and Reliability of
  Self-Reconfigurable Multi-Robot Organisms</title><categories>cs.RO cs.SY</categories><journal-ref>IROS11, workshop on &quot;Reconfigurable Modular Robotics&quot;, San
  Francisco, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homogeneity and heterogeneity represent a well-known trade-off in the design
of modular robot systems. This work addresses the heterogeneity concept, its
rationales, design choices and performance evaluation. We introduce challenges
for self-reconfigurable systems, show advances of mechatronic and software
design of heterogeneous platforms and discuss experiments, which intend to
demonstrate usability and performance of this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2291</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2291</id><created>2011-09-11</created><authors><author><keyname>Ananth</keyname><forenames>Prabhanjan</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>An Algebraic Characterization of Rainbow Connectivity</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of algebraic techniques to solve combinatorial problems is studied in
this paper. We formulate the rainbow connectivity problem as a system of
polynomial equations. We first consider the case of two colors for which the
problem is known to be hard and we then extend the approach to the general
case. We also give a formulation of the rainbow connectivity problem as an
ideal membership problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2293</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2293</id><created>2011-09-11</created><authors><author><keyname>Balakrishnan</keyname><forenames>Bhargav.</forenames></author></authors><title>Efficient management of IT Infrastructure implementation and support at
  enterprise level</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with how to manage effectively in the design, implementation
and support of an IT infrastructure at an enterprise level. This particular
management is lacking in today's IT infrastructure scenario. Just
implementation is not sufficient for an NON-IT industry, they need a proper
support in the infrastructure like documentation, support work flow, ticketing
systems (used for IT related issue either hardware or software) etc... Many
organizations spend a lot of money for this support and they expect a lot from
the provider. Many providers sign in the SLA that they will provide them with
an excellent support, but 80-90% it doesn't happen. Many times they don't meet
the expectations of their client. So how to make these expectations being met
100% for the client? That is what is going to be discussed in this paper with
respect to ITIL framework and other technical terminologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2296</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2296</id><created>2011-09-11</created><authors><author><keyname>Di Castro</keyname><forenames>Dotan</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Bandits with an Edge</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bandit problem over a graph where the rewards are not directly
observed. Instead, the decision maker can compare two nodes and receive
(stochastic) information pertaining to the difference in their value. The graph
structure describes the set of possible comparisons. Consequently, comparing
between two nodes that are relatively far requires estimating the difference
between every pair of nodes on the path between them. We analyze this problem
from the perspective of sample complexity: How many queries are needed to find
an approximately optimal node with probability more than $1-\delta$ in the PAC
setup? We show that the topology of the graph plays a crucial in defining the
sample complexity: graphs with a low diameter have a much better sample
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2297</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2297</id><created>2011-09-11</created><authors><author><keyname>Mostafa</keyname><forenames>Sheikh Shanawaz</forenames></author><author><keyname>Reza</keyname><forenames>Khondker Jahid</forenames></author><author><keyname>Rashid</keyname><forenames>Gazi Maniur</forenames></author><author><keyname>Moinuddin</keyname><forenames>Muhammad</forenames></author><author><keyname>Amin</keyname><forenames>Md. Ziaul</forenames></author><author><keyname>Nahid</keyname><forenames>Abdullah Al</forenames></author></authors><title>An Efficient Paging Algorithm for Multi-Carrier CDMA System</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Science Issues,pp 421-426, Vol.
  8, Issue 3, No. 2, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To cope with the increasing demand of wireless communication services
multi-carrier systems are being used. Radio resources are very limited and
efficient usages of these resources are inevitable to get optimum performance
of the system. Paging channel is a low-bandwidth channel and one of the most
important channels on which system performance depends significantly. Therefore
it is vulnerable to even moderate overloads. In this paper, an efficient paging
algorithm, Concurrent Search, is proposed for efficient use of paging channel
in Multi- carrier CDMA system instead of existing sequential searching
algorithm. It is shown by the simulation that the paging performance in
proposed algorithm is far better than the existing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2304</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2304</id><created>2011-09-11</created><authors><author><keyname>Ramalingam</keyname><forenames>Srikumar</forenames></author><author><keyname>Russell</keyname><forenames>Chris</forenames></author><author><keyname>Ladicky</keyname><forenames>Lubor</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>Efficient Minimization of Higher Order Submodular Functions using
  Monotonic Boolean Functions</title><categories>cs.DS cs.CV cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular function minimization is a key problem in a wide variety of
applications in machine learning, economics, game theory, computer vision and
many others. The general solver has a complexity of $O(n^6+n^5L)$ where $L$ is
the time required to evaluate the function and $n$ is the number of variables
\cite{orlin09}. On the other hand, many useful applications in computer vision
and machine learning applications are defined over a special subclasses of
submodular functions in which that can be written as the sum of many submodular
cost functions defined over cliques containing few variables. In such
functions, the pseudo-Boolean (or polynomial) representation \cite{BorosH02} of
these subclasses are of degree (or order, or clique size) $k$ where $k&lt;&lt;n$. In
this work, we develop efficient algorithms for the minimization of this useful
subclass of submodular functions. To do this, we define novel mapping that
transform submodular functions of order $k$ into quadratic ones, which can be
efficiently minimized in $O(n^3)$ time using a max-flow algorithm. The
underlying idea is to use auxiliary variables to model the higher order terms
and the transformation is found using a carefully constructed linear program.
In particular, we model the auxiliary variables as monotonic Boolean functions,
allowing us to obtain a compact transformation using as few auxiliary variables
as possible. Specifically, we show that our approach for fourth order function
requires only 2 auxiliary variables in contrast to 30 or more variables used in
existing approaches. In the general case, we give an upper bound for the number
or auxiliary variables required to transform a function of order $k$ using
Dedekind number, which is substantially lower than the existing bound of
$2^{2^k}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2306</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2306</id><created>2011-09-11</created><updated>2012-04-07</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>An Evaluation of Impacts in &quot;Nanoscience &amp; nanotechnology:&quot; Steps
  towards standards for citation analysis</title><categories>cs.DL</categories><comments>Scientometrics (in press). Paper to be presented at the 2011 Atlanta
  Conference on Science, Technology, and Innovation Policy, September 15-17,
  Atlanta GA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One is inclined to conceptualize impact in terms of citations per
publication, and thus as an average. However, citation distributions are
skewed, and the average has the disadvantage that the number of publications is
used in the denominator. Using hundred percentiles, one can integrate the
normalized citation curve and develop an indicator that can be compared across
document sets because percentile ranks are defined at the article level. I
apply this indicator to the set of 58 journals in the ISI Subject Category of
&quot;Nanoscience &amp; nanotechnology,&quot; and rank journals, countries, cities, and
institutes using non-parametric statistics. The significance levels of results
can thus be indicated. The results are first compared with the ISI-Impact
Factors, but this Integrated Impact Indicator (I3) can be used with any set
downloaded from the (Social) Science Citation Index. The software is made
publicly available at the Internet. Visualization techniques are also specified
for evaluation by positioning institutes on Google Map overlays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2313</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2313</id><created>2011-09-11</created><authors><author><keyname>Chen</keyname><forenames>Junting</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Convergence Analysis of Saddle Point Problems in Time Varying Wireless
  Systems - Control Theoretical Approach</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 60, no. 1, pp.
  443-452, 2012</journal-ref><doi>10.1109/ICASSP.2012.6288569</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saddle point problems arise from many wireless applications, and primal-dual
iterative algorithms are widely applied to find the saddle points. In the
existing literature, the convergence results of such algorithms are established
assuming the problem specific parameters remain unchanged during the
iterations. However, this assumption is unrealistic in time varying wireless
systems, as explicit message passing is usually involved in the iterations and
the channel state information (CSI) may change in a time scale comparable to
the algorithm update period. This paper investigates the convergence behavior
and the tracking error of primal-dual iterative algorithms under time varying
CSI. The convergence results are established by studying the stability of an
equivalent virtual dynamic system derived in the paper, and the Lyapunov theory
is applied for the stability analysis. We show that the average tracking error
is proportional to the time variation rate of the CSI. Based on these analyses,
we also derive an adaptive primal-dual algorithm by introducing a compensation
term to reduce the tracking error under the time varying CSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2317</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2317</id><created>2011-09-11</created><updated>2013-01-29</updated><authors><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author><author><keyname>Oggier</keyname><forenames>Frederique</forenames></author></authors><title>An Overview of Codes Tailor-made for Better Repairability in Networked
  Distributed Storage Systems</title><categories>cs.DC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continuously increasing amount of digital data generated by today's
society asks for better storage solutions. This survey looks at a new
generation of coding techniques designed specifically for the needs of
distributed networked storage systems, trying to reach the best compromise
among storage space efficiency, fault tolerance, and maintenance overheads.
Four families of codes tailor-made for distributed settings, namely - pyramid,
hierarchical, regenerating and self-repairing codes - are presented at a high
level, emphasizing the main ideas behind each of these codes, and discussing
their pros and cons, before concluding with a quantitative comparison among
them. This survey deliberately excluded technical details for the codes, nor
does it provide an exhaustive summary of the numerous works. Instead, it
provides an overview of the major code families in a manner easily accessible
to a broad audience, by presenting the big picture of advances in coding
techniques for distributed storage solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2321</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2321</id><created>2011-09-11</created><authors><author><keyname>Deborah</keyname><forenames>L. Jegatha</forenames></author><author><keyname>Baskaran</keyname><forenames>R.</forenames></author><author><keyname>Kannan</keyname><forenames>A.</forenames></author></authors><title>Visualizing Domain Ontology using Enhanced Anaphora Resolution Algorithm</title><categories>cs.IR</categories><comments>13 pages in total, 11 figures, 2 tables, Older version of the Paper
  published in the International Workshop on Database Management Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enormous explosion in the number of the World Wide Web pages occur every day
and since the efficiency of most of the information processing systems is found
to be less, the potential of the Internet applications is often underutilized.
Efficient utilization of the web can be exploited when similar web pages are
rigorously, exhaustively organized and clustered based on some domain knowledge
(semantic-based) .Ontology which is a formal representation of domain knowledge
aids in such efficient utilization. The performance of almost all the
semantic-based clustering techniques depends on the constructed ontology,
describing the domain knowledge . The proposed methodology provides an enhanced
pronominal anaphora resolution, one of the key aspects of semantic analysis in
Natural Language Processing for obtaining cross references within a web page
providing better ontology construction. The experimental data sets exhibits
better efficiency of the proposed method compared to earlier traditional
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2323</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2323</id><created>2011-09-11</created><authors><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author></authors><title>An inventory of three-dimensional Hilbert space-filling curves</title><categories>cs.CG</categories><comments>25 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hilbert's two-dimensional space-filling curve is appreciated for its good
locality properties for many applications. However, it is not clear what is the
best way to generalize this curve to filling higher-dimensional spaces. We
argue that the properties that make Hilbert's curve unique in two dimensions,
are shared by 10694807 structurally different space-filling curves in three
dimensions. These include several curves that have, in some sense, better
locality properties than any generalized Hilbert curve that has been considered
in the literature before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2325</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2325</id><created>2011-09-11</created><authors><author><keyname>Gunjal</keyname><forenames>Baisa L.</forenames></author><author><keyname>Mali</keyname><forenames>Suresh N.</forenames></author></authors><title>Secured color image watermarking technique in DWT-DCT domain</title><categories>cs.MM</categories><comments>9 pages; International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.1, No. 3, August 2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The multilayer secured DWT-DCT and YIQ color space based image watermarking
technique with robustness and better correlation is presented here. The
security levels are increased by using multiple pn sequences, Arnold
scrambling, DWT domain, DCT domain and color space conversions. Peak signal to
noise ratio and Normalized correlations are used as measurement metrics. The
512x512 sized color images with different histograms are used for testing and
watermark of size 64x64 is embedded in HL region of DWT and 4x4 DCT is used.
'Haar' wavelet is used for decomposition and direct flexing factor is used. We
got PSNR value is 63.9988 for flexing factor k=1 for Lena image and the maximum
NC 0.9781 for flexing factor k=4 in Q color space. The comparative performance
in Y, I and Q color space is presented. The technique is robust for different
attacks like scaling, compression, rotation etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2341</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2341</id><created>2011-09-11</created><updated>2012-05-20</updated><authors><author><keyname>Jenrich</keyname><forenames>Thomas</forenames></author></authors><title>Guaranteed successful strategies for a square achievement game on an n
  by n grid</title><categories>cs.DM math.CO</categories><comments>5 pages, 2 additional files (plain text version and program source);
  updated because the initially provided program SQRGAME turned out to be
  incorrect</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At some places (see the references) Martin Erickson describes a certain game:
  &quot;Two players alternately write O's (first player) and X's (second player) in
the unoccupied cells of an n x n grid. The first player (if any) to occupy four
cells at the vertices of a square with horizontal and vertical sides is the
winner.&quot;
  Then he asks &quot;What is the outcome of the game given optimal play?&quot; or
  &quot;What is the smallest n such that the first player has a winning strategy?&quot;
  For n lower than 3 a win is obviously impossible.
  The aim of this article and the additionally (in the source package) provided
computer program SQRGAME2 is to give and prove sure strategies for the second
player not to lose if n is 3 or 4, and for the first player to win if n is 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2346</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2346</id><created>2011-09-11</created><authors><author><keyname>Howe</keyname><forenames>A. E.</forenames></author><author><keyname>Watson</keyname><forenames>J. P.</forenames></author><author><keyname>Whitley</keyname><forenames>L. D.</forenames></author></authors><title>Linking Search Space Structure, Run-Time Dynamics, and Problem
  Difficulty: A Step Toward Demystifying Tabu Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  221-261, 2005</journal-ref><doi>10.1613/jair.1576</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tabu search is one of the most effective heuristics for locating high-quality
solutions to a diverse array of NP-hard combinatorial optimization problems.
Despite the widespread success of tabu search, researchers have a poor
understanding of many key theoretical aspects of this algorithm, including
models of the high-level run-time dynamics and identification of those search
space features that influence problem difficulty. We consider these questions
in the context of the job-shop scheduling problem (JSP), a domain where tabu
search algorithms have been shown to be remarkably effective. Previously, we
demonstrated that the mean distance between random local optima and the nearest
optimal solution is highly correlated with problem difficulty for a well-known
tabu search algorithm for the JSP introduced by Taillard. In this paper, we
discuss various shortcomings of this measure and develop a new model of problem
difficulty that corrects these deficiencies. We show that Taillards algorithm
can be modeled with high fidelity as a simple variant of a straightforward
random walk. The random walk model accounts for nearly all of the variability
in the cost required to locate both optimal and sub-optimal solutions to random
JSPs, and provides an explanation for differences in the difficulty of random
versus structured JSPs. Finally, we discuss and empirically substantiate two
novel predictions regarding tabu search algorithm behavior. First, the method
for constructing the initial solution is highly unlikely to impact the
performance of tabu search. Second, tabu tenure should be selected to be as
small as possible while simultaneously avoiding search stagnation; values
larger than necessary lead to significant degradations in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2347</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2347</id><created>2011-09-11</created><authors><author><keyname>Aloul</keyname><forenames>F. A.</forenames></author><author><keyname>Markov</keyname><forenames>I. L.</forenames></author><author><keyname>Ramani</keyname><forenames>A.</forenames></author><author><keyname>Sakallah</keyname><forenames>K. A.</forenames></author></authors><title>Breaking Instance-Independent Symmetries In Exact Graph Coloring</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  289-322, 2006</journal-ref><doi>10.1613/jair.1637</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code optimization and high level synthesis can be posed as constraint
satisfaction and optimization problems, such as graph coloring used in register
allocation. Graph coloring is also used to model more traditional CSPs relevant
to AI, such as planning, time-tabling and scheduling. Provably optimal
solutions may be desirable for commercial and defense applications.
Additionally, for applications such as register allocation and code
optimization, naturally-occurring instances of graph coloring are often small
and can be solved optimally. A recent wave of improvements in algorithms for
Boolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests
generic problem-reduction methods, rather than problem-specific heuristics,
because (1) heuristics may be upset by new constraints, (2) heuristics tend to
ignore structure, and (3) many relevant problems are provably inapproximable.
  Problem reductions often lead to highly symmetric SAT instances, and
symmetries are known to slow down SAT solvers. In this work, we compare several
avenues for symmetry breaking, in particular when certain kinds of symmetry are
present in all generated instances. Our focus on reducing CSPs to SAT allows us
to leverage recent dramatic improvement in SAT solvers and automatically
benefit from future progress. We can use a variety of black-box SAT solvers
without modifying their source code because our symmetry-breaking techniques
are static, i.e., we detect symmetries and add symmetry breaking predicates
(SBPs) during pre-processing.
  An important result of our work is that among the types of
instance-independent SBPs we studied and their combinations, the simplest and
least complete constructions are the most effective. Our experiments also
clearly indicate that instance-independent symmetries should mostly be
processed together with instance-specific symmetries rather than at the
specification level, contrary to what has been suggested in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2348</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2348</id><created>2011-09-11</created><authors><author><keyname>Eirola</keyname><forenames>Axel</forenames></author></authors><title>Lossless data compression on GPGPU architectures</title><categories>cs.DS</categories><comments>Aalto University special course on data compression course
  assignment. (http://www.cs.hut.fi/~travis/compression/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern graphics processors provide exceptional computa- tional power, but
only for certain computational models. While they have revolutionized
computation in many fields, compression has been largely unnaffected. This
paper aims to explain the current issues and possibili- ties in GPGPU
compression. This is done by a high level overview of the GPGPU computational
model in the context of compression algorithms; along with a more in-depth
analysis of how one would implement bzip2 on a GPGPU architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2355</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2355</id><created>2011-09-11</created><authors><author><keyname>Gretton</keyname><forenames>C.</forenames></author><author><keyname>Kabanza</keyname><forenames>F.</forenames></author><author><keyname>Price</keyname><forenames>D.</forenames></author><author><keyname>Slaney</keyname><forenames>J.</forenames></author><author><keyname>Thiebaux</keyname><forenames>S.</forenames></author></authors><title>Decision-Theoretic Planning with non-Markovian Rewards</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  17-74, 2006</journal-ref><doi>10.1613/jair.1676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A decision process in which rewards depend on history rather than merely on
the current state is called a decision process with non-Markovian rewards
(NMRDP). In decision-theoretic planning, where many desirable behaviours are
more naturally expressed as properties of execution sequences rather than as
properties of states, NMRDPs form a more natural model than the commonly
adopted fully Markovian decision process (MDP) model. While the more tractable
solution methods developed for MDPs do not directly apply in the presence of
non-Markovian rewards, a number of solution methods for NMRDPs have been
proposed in the literature. These all exploit a compact specification of the
non-Markovian reward function in temporal logic, to automatically translate the
NMRDP into an equivalent MDP which is solved using efficient MDP solution
methods. This paper presents NMRDPP (Non-Markovian Reward Decision Process
Planner), a software platform for the development and experimentation of
methods for decision-theoretic planning with non-Markovian rewards. The current
version of NMRDPP implements, under a single interface, a family of methods
based on existing as well as new approaches which we describe in detail. These
include dynamic programming, heuristic search, and structured methods. Using
NMRDPP, we compare the methods and identify certain problem features that
affect their performance. NMRDPPs treatment of non-Markovian rewards is
inspired by the treatment of domain-specific search control knowledge in the
TLPlan planner, which it incorporates as a special case. In the First
International Probabilistic Planning Competition, NMRDPP was able to compete
and perform well in both the domain-independent and hand-coded tracks, using
search control knowledge in the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2361</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2361</id><created>2011-09-11</created><authors><author><keyname>Petkovic</keyname><forenames>Marko D.</forenames></author><author><keyname>Pokrajac</keyname><forenames>Dragoljub</forenames></author><author><keyname>Latecki</keyname><forenames>Longin Jan</forenames></author></authors><title>Spherical coverage verification</title><categories>cs.CG cs.CC math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of covering hypersphere by a set of spherical
hypercaps. This sort of problem has numerous practical applications such as
error correcting codes and reverse k-nearest neighbor problem. Using the
reduction of non degenerated concave quadratic programming (QP) problem, we
demonstrate that spherical coverage verification is NP hard. We propose a
recursive algorithm based on reducing the problem to several lower dimension
subproblems. We test the performance of the proposed algorithm on a number of
generated constellations. We demonstrate that the proposed algorithm, in spite
of its exponential worst-case complexity, is applicable in practice. In
contrast, our results indicate that spherical coverage verification using QP
solvers that utilize heuristics, due to numerical instability, may produce
false positives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2363</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2363</id><created>2011-09-11</created><authors><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author><author><keyname>Cochran</keyname><forenames>Douglas</forenames></author></authors><title>Sensor Management: Past, Present, and Future</title><categories>stat.AP cs.RO cs.SY math.OC</categories><comments>15 pages, 112 references</comments><journal-ref>IEEE Sensors Journal, vol. 11, issue 12, pp. 3064-3075, December
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor systems typically operate under resource constraints that prevent the
simultaneous use of all resources all of the time. Sensor management becomes
relevant when the sensing system has the capability of actively managing these
resources; i.e., changing its operating configuration during deployment in
reaction to previous measurements. Examples of systems in which sensor
management is currently used or is likely to be used in the near future include
autonomous robots, surveillance and reconnaissance networks, and waveform-agile
radars. This paper provides an overview of the theory, algorithms, and
applications of sensor management as it has developed over the past decades and
as it stands today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2378</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2378</id><created>2011-09-12</created><authors><author><keyname>M&#xfc;llner</keyname><forenames>Daniel</forenames></author></authors><title>Modern hierarchical, agglomerative clustering algorithms</title><categories>stat.ML cs.DS</categories><comments>29 pages</comments><msc-class>62H30</msc-class><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents algorithms for hierarchical, agglomerative clustering
which perform most efficiently in the general-purpose setup that is given in
modern standard software. Requirements are: (1) the input data is given by
pairwise dissimilarities between data points, but extensions to vector data are
also discussed (2) the output is a &quot;stepwise dendrogram&quot;, a data structure
which is shared by all implementations in current standard software. We present
algorithms (old and new) which perform clustering in this setting efficiently,
both in an asymptotic worst-case analysis and from a practical point of view.
The main contributions of this paper are: (1) We present a new algorithm which
is suitable for any distance update scheme and performs significantly better
than the existing algorithms. (2) We prove the correctness of two algorithms by
Rohlf and Murtagh, which is necessary in each case for different reasons. (3)
We give well-founded recommendations for the best current algorithms for the
various agglomerative clustering schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2388</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2388</id><created>2011-09-12</created><authors><author><keyname>Akbas</keyname><forenames>Emre</forenames></author><author><keyname>Ghanem</keyname><forenames>Bernard</forenames></author><author><keyname>Ahuja</keyname><forenames>Narendra</forenames></author></authors><title>MIS-Boost: Multiple Instance Selection Boosting</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new multiple instance learning (MIL) method,
called MIS-Boost, which learns discriminative instance prototypes by explicit
instance selection in a boosting framework. Unlike previous instance selection
based MIL methods, we do not restrict the prototypes to a discrete set of
training instances but allow them to take arbitrary values in the instance
feature space. We also do not restrict the total number of prototypes and the
number of selected-instances per bag; these quantities are completely
data-driven. We show that MIS-Boost outperforms state-of-the-art MIL methods on
a number of benchmark datasets. We also apply MIS-Boost to large-scale image
classification, where we show that the automatically selected prototypes map to
visually meaningful image regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2389</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2389</id><created>2011-09-12</created><authors><author><keyname>Ghanem</keyname><forenames>Bernard</forenames></author><author><keyname>Ahuja</keyname><forenames>Narendra</forenames></author></authors><title>A Probabilistic Framework for Discriminative Dictionary Learning</title><categories>cs.CV cs.LG</categories><comments>10 pages, 4 figures, conference, dictionary learning, sparse coding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of discriminative dictionary learning
(DDL), where sparse linear representation and classification are combined in a
probabilistic framework. As such, a single discriminative dictionary and linear
binary classifiers are learned jointly. By encoding sparse representation and
discriminative classification models in a MAP setting, we propose a general
optimization framework that allows for a data-driven tradeoff between faithful
representation and accurate classification. As opposed to previous work, our
learning methodology is capable of incorporating a diverse family of
classification cost functions (including those used in popular boosting
methods), while avoiding the need for involved optimization techniques. We show
that DDL can be solved by a sequence of updates that make use of well-known and
well-studied sparse coding and dictionary learning algorithms from the
literature. To validate our DDL framework, we apply it to digit classification
and face recognition and test it on standard benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2397</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2397</id><created>2011-09-12</created><updated>2012-04-20</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Structured sparsity through convex optimization</title><categories>cs.LG stat.ML</categories><comments>Statistical Science (2012) To appear</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse estimation methods are aimed at using or obtaining parsimonious
representations of data or models. While naturally cast as a combinatorial
optimization problem, variable or feature selection admits a convex relaxation
through the regularization by the $\ell_1$-norm. In this paper, we consider
situations where we are not only interested in sparsity, but where some
structural prior knowledge is available as well. We show that the $\ell_1$-norm
can then be extended to structured norms built on either disjoint or
overlapping groups of variables, leading to a flexible framework that can deal
with various structures. We present applications to unsupervised learning, for
structured sparse principal component analysis and hierarchical dictionary
learning, and to supervised learning in the context of non-linear variable
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2399</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2399</id><created>2011-09-12</created><authors><author><keyname>Pimentel</keyname><forenames>Ernesto</forenames></author><author><keyname>Valero</keyname><forenames>Valent&#xed;n</forenames></author></authors><title>Proceedings Fifth Workshop on Formal Languages and Analysis of
  Contract-Oriented Software</title><categories>cs.LO</categories><comments>EPTCS 68, 2011</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.68</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume consists of the proceedings of the 5th Workshop on Formal
Languages and Analysis of Contract-Oriented Software (FLACOS'11). The FLACOS
Workshops serve as annual meeting places to bring together researchers and
practitioners working on language-based solutions to contract-oriented software
development. High-level models of contracts are needed as a tool to negotiate
contracts and provide services conforming to them. This Workshop provides
language-based solutions to the above issues through formalization of
contracts, design of appropriate abstraction mechanisms, and formal analysis of
contract languages and software. The program of this edition consists of 5
regular papers and 3 invited presentations.
  Detailed information about the FLACOS 2011 Workshop can be found at
http://flacos2011.lcc.uma.es/. The 5th edition of the FLACOS Workshop was
organized by the University of M\'alaga. It took place in M\'alaga, Spain,
during September 22-23, 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2405</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2405</id><created>2011-09-12</created><authors><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Guen</keyname><forenames>Julien Le</forenames><affiliation>VERIMAG - IMAG, ST Microelectronics</affiliation></author></authors><title>Stratified Static Analysis Based on Variable Dependencies</title><categories>cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In static analysis by abstract interpretation, one often uses widening
operators in order to enforce convergence within finite time to an inductive
invariant. Certain widening operators, including the classical one over finite
polyhedra, exhibit an unintuitive behavior: analyzing the program over a subset
of its variables may lead a more precise result than analyzing the original
program! In this article, we present simple workarounds for such behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2415</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2415</id><created>2011-09-12</created><updated>2011-12-01</updated><authors><author><keyname>Schmidt</keyname><forenames>Mark</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Roux</keyname><forenames>Nicolas Le</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>Convergence Rates of Inexact Proximal-Gradient Methods for Convex
  Optimization</title><categories>cs.LG math.OC</categories><comments>Neural Information Processing Systems (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimizing the sum of a smooth convex function and
a non-smooth convex function using proximal-gradient methods, where an error is
present in the calculation of the gradient of the smooth term or in the
proximity operator with respect to the non-smooth term. We show that both the
basic proximal-gradient method and the accelerated proximal-gradient method
achieve the same convergence rate as in the error-free case, provided that the
errors decrease at appropriate rates.Using these rates, we perform as well as
or better than a carefully chosen fixed error level on a set of structured
sparsity problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2417</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2417</id><created>2011-09-12</created><authors><author><keyname>Emruli</keyname><forenames>Sali</forenames></author><author><keyname>Baca</keyname><forenames>Miroslav</forenames></author></authors><title>Internet and political communication - Macedonian case</title><categories>cs.SI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 1, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis how to use Internet influence to the process of political
communication, marketing and the management of public relations, what kind of
online communication methods are used by political parties, and to assess
satisfaction, means of communication and the services they provide to their
partys voters (people) and other interest groups and whether social networks
can affect the political and economic changes in the state, and the political
power of one party.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2418</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2418</id><created>2011-09-12</created><authors><author><keyname>Emruli</keyname><forenames>Sali</forenames></author><author><keyname>Zejneli</keyname><forenames>Tahir</forenames></author><author><keyname>Agai</keyname><forenames>Florin</forenames></author></authors><title>Facebook and political communication -- Macedonian case</title><categories>cs.SI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, July 2011, 451-459</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis how to use Internet influence to the process of political
communication, marketing and the management of public relations, what kind of
online communication methods are used by political parties, and to assess
satisfaction, means of communication and the services they provide to their
partys voters (people) and other interest groups and whether social networks
can affect the political and economic changes in the state, and the political
power of one party.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2425</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2425</id><created>2011-09-12</created><authors><author><keyname>Meghini</keyname><forenames>Carlo</forenames></author><author><keyname>Tzitzikas</keyname><forenames>Yannis</forenames></author><author><keyname>Coltella</keyname><forenames>Veronica</forenames></author><author><keyname>Analyti</keyname><forenames>Anastasia</forenames></author></authors><title>Query processing in distributed, taxonomy-based information sources</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of answering queries over a distributed information
system, storing objects indexed by terms organized in a taxonomy. The taxonomy
consists of subsumption relationships between negation-free DNF formulas on
terms and negation-free conjunctions of terms. In the first part of the paper,
we consider the centralized case, deriving a hypergraph-based algorithm that is
efficient in data complexity. In the second part of the paper, we consider the
distributed case, presenting alternative ways implementing the centralized
algorithm. These ways descend from two basic criteria: direct vs. query
re-writing evaluation, and centralized vs. distributed data or taxonomy
allocation. Combinations of these criteria allow to cover a wide spectrum of
architectures, ranging from client-server to peer-to-peer. We evaluate the
performance of the various architectures by simulation on a network with
O(10^4) nodes, and derive final results. An extensive review of the relevant
literature is finally included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2427</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2427</id><created>2011-09-12</created><authors><author><keyname>Rajalakshmi</keyname><forenames>M.</forenames></author><author><keyname>Purusothaman</keyname><forenames>Dr. T.</forenames></author><author><keyname>Nedunchezhian</keyname><forenames>Dr. R.</forenames></author></authors><title>Maximal frequent itemset generation using segmentation approach</title><categories>cs.DB</categories><comments>14 pages</comments><doi>10.5121/ijdms.2011.3302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding frequent itemsets in a data source is a fundamental operation behind
Association Rule Mining. Generally, many algorithms use either the bottom-up or
top-down approaches for finding these frequent itemsets. When the length of
frequent itemsets to be found is large, the traditional algorithms find all the
frequent itemsets from 1-length to n-length, which is a difficult process. This
problem can be solved by mining only the Maximal Frequent Itemsets (MFS).
Maximal Frequent Itemsets are frequent itemsets which have no proper frequent
superset. Thus, the generation of only maximal frequent itemsets reduces the
number of itemsets and also time needed for the generation of all frequent
itemsets as each maximal itemset of length m implies the presence of 2m-2
frequent itemsets. Furthermore, mining only maximal frequent itemset is
sufficient in many data mining applications like minimal key discovery and
theory extraction. In this paper, we suggest a novel method for finding the
maximal frequent itemset from huge data sources using the concept of
segmentation of data source and prioritization of segments. Empirical
evaluation shows that this method outperforms various other known methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2430</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2430</id><created>2011-09-12</created><authors><author><keyname>Banerjee</keyname><forenames>Indrajit</forenames></author><author><keyname>Chanak</keyname><forenames>Prasenjit</forenames></author><author><keyname>Rahaman</keyname><forenames>Hafizur</forenames></author></authors><title>CCABC: Cyclic Cellular Automata Based Clustering For Energy Conservation
  in Sensor Networks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sensor network has been recognized as the most significant technology for
next century. Despites of its potential application, wireless sensor network
encounters resource restriction such as low power, reduced bandwidth and
specially limited power sources. This work proposes an efficient technique for
the conservation of energy in a wireless sensor network (WSN) by forming an
effective cluster of the network nodes distributed over a wide range of
geographical area. The clustering scheme is developed around a specified class
of cellular automata (CA) referred to as the modified cyclic cellular automata
(mCCA). It sets a number of nodes in stand-by mode at an instance of time
without compromising the area of network coverage and thereby conserves the
battery power. The proposed scheme also determines an effective cluster size
where the inter-cluster and intra-cluster communication cost is minimum. The
simulation results establish that the cyclic cellular automata based clustering
for energy conservation in sensor networks (CCABC) is more reliable than the
existing schemes where clustering and CA based energy saving technique is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2434</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2434</id><created>2011-09-12</created><authors><author><keyname>Bauters</keyname><forenames>Kim</forenames></author><author><keyname>Janssen</keyname><forenames>Jeroen</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author><author><keyname>Vermeir</keyname><forenames>Dirk</forenames></author><author><keyname>De Cock</keyname><forenames>Martine</forenames></author></authors><title>Expressiveness of Communication in Answer Set Programming</title><categories>cs.LO cs.PL</categories><comments>35 pages. This article has been accepted for publication in Theory
  and Practice of Logic Programming, Copyright Cambridge University Press</comments><acm-class>D.1.6; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer set programming (ASP) is a form of declarative programming that allows
to succinctly formulate and efficiently solve complex problems. An intuitive
extension of this formalism is communicating ASP, in which multiple ASP
programs collaborate to solve the problem at hand. However, the expressiveness
of communicating ASP has not been thoroughly studied. In this paper, we present
a systematic study of the additional expressiveness offered by allowing ASP
programs to communicate. First, we consider a simple form of communication
where programs are only allowed to ask questions to each other. For the most
part, we deliberately only consider simple programs, i.e. programs for which
computing the answer sets is in P. We find that the problem of deciding whether
a literal is in some answer set of a communicating ASP program using simple
communication is NP-hard. In other words: we move up a step in the polynomial
hierarchy due to the ability of these simple ASP programs to communicate and
collaborate. Second, we modify the communication mechanism to also allow us to
focus on a sequence of communicating programs, where each program in the
sequence may successively remove some of the remaining models. This mimics a
network of leaders, where the first leader has the first say and may remove
models that he or she finds unsatisfactory. Using this particular communication
mechanism allows us to capture the entire polynomial hierarchy. This means, in
particular, that communicating ASP could be used to solve problems that are
above the second level of the polynomial hierarchy, such as some forms of
abductive reasoning as well as PSPACE-complete problems such as STRIPS
planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2445</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2445</id><created>2011-09-12</created><updated>2011-10-17</updated><authors><author><keyname>Watanabe</keyname><forenames>Yusuke</forenames></author></authors><title>A conjecture on independent sets and graph covers</title><categories>math.CO cs.DM</categories><comments>This is a technical report; minor corrections are made in v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, I present a conjecture on the number of independent sets on
graph covers. I also show that the conjecture implies that the partition
function of a binary pairwise attractive model is greater than that of the
Bethe approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2449</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2449</id><created>2011-09-12</created><updated>2011-09-17</updated><authors><author><keyname>Funke</keyname><forenames>Jan</forenames></author><author><keyname>Andres</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Hamprecht</keyname><forenames>Fred</forenames></author><author><keyname>Cardona</keyname><forenames>Albert</forenames></author><author><keyname>Cook</keyname><forenames>Matthew</forenames></author></authors><title>Multi-Hypothesis CRF-Segmentation of Neural Tissue in Anisotropic EM
  Volumes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for the joint segmentation and grouping of similar
components in anisotropic 3D image data and use it to segment neural tissue in
serial sections electron microscopy (EM) images.
  We first construct a nested set of neuron segmentation hypotheses for each
slice. A conditional random field (CRF) then allows us to evaluate both the
compatibility of a specific segmentation and a specific inter-slice assignment
of neuron candidates with the underlying observations. The model is solved
optimally for an entire image stack simultaneously using integer linear
programming (ILP), which yields the maximum a posteriori solution in amortized
linear time in the number of slices.
  We evaluate the performance of our approach on an annotated sample of the
Drosophila larva neuropil and show that the consideration of different
segmentation hypotheses in each slice leads to a significant improvement in the
segmentation and assignment accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2475</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2475</id><created>2011-09-12</created><authors><author><keyname>Stauffer</keyname><forenames>Dietrich</forenames></author></authors><title>Statistical Physics for Humanities: A Tutorial</title><categories>physics.pop-ph cond-mat.stat-mech cs.SI physics.ed-ph physics.soc-ph</categories><comments>17 pages for conference &quot;Science Matters&quot;, Nov. 2011, Lisbon
  (Portugal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The image of physics is connected with simple &quot;mechanical&quot; deterministic
events: that an apple always falls down, that force equals mass times
acceleleration. Indeed, applications of such concept to social or historical
problems go back two centuries (population growth and stabilisation, by Malthus
and by Verhulst) and use &quot;differential equations&quot;, as recently revierwed by
Vitanov and Ausloos [2011]. However, since even today's computers cannot follow
the motion of all air molecules within one cubic centimeter, the probabilistic
approach has become fashionable since Ludwig Boltzmann invented Statistical
Physics in the 19th century. Computer simulations in Statistical Physics deal
with single particles, a method called agent-based modelling in fields which
adopted it later. Particularly simple are binary models where each particle has
only two choices, called spin up and spin down by physicists, bit zero and bit
one by computer scientists, and voters for the Republicans or for the Democrats
in American politics (where one human is simulated as one particle).
Neighbouring particles may influence each other, and the Ising model of 1925 is
the best-studied example of such models. This text will explain to the reader
how to program the Ising model on a square lattice (in Fortran language);
starting from there the readers can build their own computer programs. Some
applications of Statistical Physics outside the natural sciences will be
listed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2477</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2477</id><created>2011-09-12</created><updated>2011-09-29</updated><authors><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author></authors><title>A O(1/eps^2)^n Time Sieving Algorithm for Approximate Integer
  Programming</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Integer Programming Problem (IP) for a polytope P \subseteq R^n is to
find an integer point in P or decide that P is integer free. We give an
algorithm for an approximate version of this problem, which correctly decides
whether P contains an integer point or whether a (1+\eps) scaling of P around
its barycenter is integer free in time O(1/\eps^2)^n. We reduce this
approximate IP question to an approximate Closest Vector Problem (CVP) in a
&quot;near-symmetric&quot; semi-norm, which we solve via a sieving technique first
developed by Ajtai, Kumar, and Sivakumar (STOC 2001). Our main technical
contribution is an extension of the AKS sieving technique which works for any
near-symmetric semi-norm. Our results also extend to general convex bodies and
lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2489</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2489</id><created>2011-09-09</created><updated>2011-09-14</updated><authors><author><keyname>Weber</keyname><forenames>Anina</forenames></author><author><keyname>Taglioni</keyname><forenames>Geo</forenames></author></authors><title>Swiss Elections to the National Council: First trials with e-voting in
  elections at federal level</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On October 23rd 2011, around 22'000 voters will be authorized to cast their
votes electronically in occasion of the elections to the National Council.
These are the first trials ever with e-voting in elections at federal level in
Switzerland. Four cantons are going to conduct trials with this new channel.
Only Swiss voters living abroad will be authorized to participate. The Swiss
Confederation pursues the long term goal of the introduction of e-voting as a
third, complementary voting method - in addition to voting in person at the
polling station and postal voting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2499</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2499</id><created>2011-09-12</created><authors><author><keyname>Dhanjal</keyname><forenames>Charanpal</forenames></author><author><keyname>Clemencon</keyname><forenames>Stephan</forenames></author><author><keyname>De Arazoza</keyname><forenames>Hector</forenames></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames></author><author><keyname>Tran</keyname><forenames>Viet Chi</forenames></author></authors><title>The Evolution of the Cuban HIV/AIDS Network</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An individual detected as HIV positive in Cuba is asked to provide a list of
his/her sexual contacts for the previous 2 years. This allows one to gather
detailed information on the spread of the HIV epidemic. Here we study the
evolution of the sexual contact graph of detected individuals and also the
directed graph of HIV infections. The study covers the Cuban HIV epidemic
between the years 1986 and 2004 inclusive and is motivated by an earlier study
on the static properties of the network at the end of 2004. We use a variety of
advanced graph algorithms to paint a picture of the growth of the epidemic,
including an examination of diameters, geodesic distances, community structure
and centrality amongst others characteristics. The analysis contrasts the HIV
network with other real networks, and graphs generated using the configuration
model. We find that the early epidemic starts in the heterosexual population
and then grows mainly through MSM (Men having Sex with Men) contact. The
epidemic exhibits a giant component which is shown to have degenerate chains of
vertices and after 1989, diameters are larger than that expected by the
equivalent configuration model graphs. In 1997 there is an significant increase
in the detection rate from 73 to 256 detections/year covering mainly MSMs which
results in a rapid increase of distances and diameters in the giant component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2536</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2536</id><created>2011-09-12</created><updated>2011-09-28</updated><authors><author><keyname>Chadha</keyname><forenames>Rohit</forenames><affiliation>LSV, ENS Cachan</affiliation></author><author><keyname>Sistla</keyname><forenames>A. Prasad</forenames><affiliation>Univ of Illinois, Chicago</affiliation></author><author><keyname>Viswanathan</keyname><forenames>Mahesh</forenames><affiliation>Univ of Illinois, Urbana Campaign</affiliation></author></authors><title>Power of Randomization in Automata on Infinite Strings</title><categories>cs.FL cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.3,D.2.4,F.1.1,F.1.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  29, 2011) lmcs:948</journal-ref><doi>10.2168/LMCS-7(3:22)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic B\&quot;uchi Automata (PBA) are randomized, finite state automata
that process input strings of infinite length. Based on the threshold chosen
for the acceptance probability, different classes of languages can be defined.
In this paper, we present a number of results that clarify the power of such
machines and properties of the languages they define. The broad themes we focus
on are as follows. We present results on the decidability and precise
complexity of the emptiness, universality and language containment problems for
such machines, thus answering questions central to the use of these models in
formal verification. Next, we characterize the languages recognized by PBAs
topologically, demonstrating that though general PBAs can recognize languages
that are not regular, topologically the languages are as simple as
\omega-regular languages. Finally, we introduce Hierarchical PBAs, which are
syntactically restricted forms of PBAs that are tractable and capture exactly
the class of \omega-regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2543</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2543</id><created>2011-09-12</created><authors><author><keyname>Zhang</keyname><forenames>Guoqiang</forenames></author><author><keyname>Klejsa</keyname><forenames>Janusz</forenames></author><author><keyname>Kleijn</keyname><forenames>W. Bastiaan</forenames></author></authors><title>Optimal Index Assignment for Multiple Description Scalar Quantization</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures, submitted to IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a method for designing an optimal index assignment for scalar
K-description coding. The method stems from a construction of translated scalar
lattices, which provides a performance advantage by exploiting a so-called
staggered gain. Interestingly, generation of the optimal index assignment is
based on a lattice in K-1 dimensional space. The use of the K-1 dimensional
lattice facilitates analytic insight into the performance and eliminates the
need for a greedy optimization of the index assignment. It is shown that that
the optimal index assignment is not unique. This is illustrated for the
two-description case, where a periodic index assignment is selected from
possible optimal assignments and described in detail. The new index assignment
is applied to design of a K-description quantizer, which is found to outperform
a reference K-description quantizer at high rates. The performance advantage
due to the staggered gain increases with increasing redundancy among the
descriptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2548</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2548</id><created>2011-09-12</created><authors><author><keyname>Kriener</keyname><forenames>Jael</forenames></author><author><keyname>King</keyname><forenames>Andy</forenames></author></authors><title>RedAlert: Determinacy Inference for Prolog</title><categories>cs.PL cs.LO</categories><comments>Theory and Practice of Logic Programming, 2011, 27th Int'l.
  Conference on Logic Programming (ICLP'11) Special Issue, volume 11, issue 4-5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper revisits the problem of determinacy inference addressing the
problem of how to uniformly handle cut. To this end a new semantics is
introduced for cut, which is abstracted to systematically derive a backward
analysis that derives conditions sufficient for a goal to succeed at most once.
The method is conceptionally simpler and easier to implement than existing
techniques, whilst improving the latter's handling of cut. Formal arguments
substantiate correctness and experimental work, and a tool called 'RedAlert'
demonstrates the method's generality and applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2563</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2563</id><created>2011-09-12</created><updated>2013-01-10</updated><authors><author><keyname>Buhrman</keyname><forenames>Harry</forenames></author><author><keyname>Fehr</keyname><forenames>Serge</forenames></author><author><keyname>Schaffner</keyname><forenames>Christian</forenames></author><author><keyname>Speelman</keyname><forenames>Florian</forenames></author></authors><title>The Garden-Hose Model</title><categories>quant-ph cs.CR</categories><comments>19 pages, 1 figure, accepted at QCRYPT 2011. v2: fixed problem with
  missing references, no changes in content, v3: equivalent to final ITCS 2013
  proceedings version. Substantial updates: re-ordering of subjects,
  introduction of randomized and quantum garden-hose models. Previous Section 3
  regarding the optimality of a particular attack is removed but can be found
  in arxiv:1210.4353</comments><journal-ref>ITCS 2013 - Proceedings of the 4th conference on Innovations in
  Theoretical Computer Science, Pages 145-158</journal-ref><doi>10.1145/2422436.2422455</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new model of communication complexity, called the garden-hose
model. Informally, the garden-hose complexity of a function f:{0,1}^n x {0,1}^n
to {0,1} is given by the minimal number of water pipes that need to be shared
between two parties, Alice and Bob, in order for them to compute the function f
as follows: Alice connects her ends of the pipes in a way that is determined
solely by her input x \in {0,1}^n and, similarly, Bob connects his ends of the
pipes in a way that is determined solely by his input y \in {0,1}^n. Alice
turns on the water tap that she also connected to one of the pipes. Then, the
water comes out on Alice's or Bob's side depending on the function value
f(x,y).
  We prove almost-linear lower bounds on the garden-hose complexity for
concrete functions like inner product, majority, and equality, and we show the
existence of functions with exponential garden-hose complexity. Furthermore, we
show a connection to classical complexity theory by proving that all functions
computable in log-space have polynomial garden-hose complexity.
  We consider a randomized variant of the garden-hose complexity, where Alice
and Bob hold pre-shared randomness, and a quantum variant, where Alice and Bob
hold pre-shared quantum entanglement, and we show that the randomized
garden-hose complexity is within a polynomial factor of the deterministic
garden-hose complexity. Examples of (partial) functions are given where the
quantum garden-hose complexity is logarithmic in n while the classical
garden-hose complexity can be lower bounded by n^c for constant c&gt;0.
  Finally, we show an interesting connection between the garden-hose model and
the (in)security of a certain class of quantum position-verification schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2567</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2567</id><created>2011-09-12</created><authors><author><keyname>Rhim</keyname><forenames>Joong Bum</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Quantization of Prior Probabilities for Collaborative Distributed
  Hypothesis Testing</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><journal-ref>IEEE Trans. on Signal Processing, vol. 60, no. 9, pp. 4537-4550,
  September 2012</journal-ref><doi>10.1109/TSP.2012.2200890</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the quantization of prior probabilities, drawn from an
ensemble, for distributed detection and data fusion. Design and performance
equivalences between a team of N agents tied by a fixed fusion rule and a more
powerful single agent are obtained. Effects of identical quantization and
diverse quantization are compared. Consideration of perceived common risk
enables agents using diverse quantizers to collaborate in hypothesis testing,
and it is proven that the minimum mean Bayes risk error is achieved by diverse
quantization. The comparison shows that optimal diverse quantization with K
cells per quantizer performs as well as optimal identical quantization with
N(K-1)+1 cells per quantizer. Similar results are obtained for maximum Bayes
risk error as the distortion criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2573</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2573</id><created>2011-09-12</created><updated>2014-04-24</updated><authors><author><keyname>Nicolaescu</keyname><forenames>Liviu I.</forenames></author><author><keyname>Rowekamp</keyname><forenames>Brandon</forenames></author></authors><title>Pixelations of planar semialgebraic sets and shape recognition</title><categories>math.DG cs.CG math.GT</categories><comments>36 pages, 13 figures, to appear in Algebraic &amp; Geometric Topology</comments><msc-class>53A04, 53C65, 58A25</msc-class><journal-ref>Algebr. Geom. Topol. 14 (2014) 3345-3394</journal-ref><doi>10.2140/agt.2014.14.3345</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm that associates to each positive real number $r$ and
each finite collection $C_r$ of planar pixels of size $r$ a planar piecewise
linear set $S_r$ with the following additional property: if $C_r$ is the
collection of pixels of size $r$ that touch a given compact semialgebraic set
$S$, then the normal cycle of $S_r$ converges to the normal cycle of $S$ in the
sense of currents. In particular, in the limit we can recover the homotopy type
of $S$ and its geometric invariants such as area, perimeter and curvature
measures. At its core, this algorithm is a discretization of stratified Morse
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2577</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2577</id><created>2011-09-12</created><updated>2011-09-14</updated><authors><author><keyname>Pajevic</keyname><forenames>Sinisa</forenames></author><author><keyname>Plenz</keyname><forenames>Dietmar</forenames></author></authors><title>The Organization of Strong Links in Complex Networks</title><categories>physics.soc-ph cs.SI q-bio.NC</categories><comments>21 pages, 9 figures, 1 table in the main text + Supplementary
  Material</comments><doi>10.1038/nphys2257</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A small-world topology characterizes many complex systems including the
structural and functional organization of brain networks. The topology allows
simultaneously for local and global efficiency in the interaction of the system
constituents. However, it ignores the gradations of interactions commonly
quantified by the link weight, w. Here, we identify an integrative weight
organization for brain, gene, social, and language networks, in which strong
links preferentially occur between nodes with overlapping neighbourhoods and
the small-world properties are robust to removal of a large fraction of the
weakest links. We also determine local learning rules that dynamically
establish such weight organization in response to past activity and capacity
demands, while preserving efficient local and global communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2583</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2583</id><created>2011-09-12</created><updated>2012-07-14</updated><authors><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Liu</keyname><forenames>Yanpei</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author></authors><title>Optimal Backpressure Scheduling in Wireless Networks using Mutual
  Information Accumulation</title><categories>cs.IT cs.NI math.IT</categories><comments>submitted to IEEE Trans. on Information Theory, September 2011;
  revised, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop scheduling policies that maximize the stability
region of a wireless network under the assumption that mutual information
accumulation is implemented at the physical layer. When the link quality
between nodes is not sufficiently high that a packet can be decoded within a
single slot, the system can accumulate information across multiple slots,
eventually decoding the packet. The result is an expanded stability region. The
accumulation process over weak links is temporally coupled and therefore does
not satisfy the independent and identically distributed (i.i.d) assumption that
underlies many previous analysis in this area. Therefore the problem setting
also poses new analytic challenges. We propose two dynamic scheduling
algorithms to cope with the non-i.i.d nature of the decoding. The first
performs scheduling every $T$ slots, and approaches the boundary of the
stability region as $T$ gets large, but at the cost of increased average delay.
The second introduces virtual queues for each link and constructs a virtual
system wherein two virtual nodes are introduced for each link. The constructed
virtual system is shown to have the same stability region as the original
system. Through controlling the virtual queues in the constructed system, we
avoid the non-i.i.d analysis difficulty and attain the full stability region.
We derive performance bounds for both algorithms and compare them through
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2591</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2591</id><created>2011-09-12</created><updated>2012-07-27</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author></authors><title>Polar codes for classical-quantum channels</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, 3 figures; v2 in IEEE format with minor changes; v3 final
  version accepted for publication in the IEEE Transactions on Information
  Theory</comments><journal-ref>IEEE Transactions on Information Theory vol. 59, no. 2, pages
  1175-1187 (February 2013)</journal-ref><doi>10.1109/TIT.2012.2218792</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Holevo, Schumacher, and Westmoreland's coding theorem guarantees the
existence of codes that are capacity-achieving for the task of sending
classical data over a channel with classical inputs and quantum outputs.
Although they demonstrated the existence of such codes, their proof does not
provide an explicit construction of codes for this task. The aim of the present
paper is to fill this gap by constructing near-explicit &quot;polar&quot; codes that are
capacity-achieving. The codes exploit the channel polarization phenomenon
observed by Arikan for the case of classical channels. Channel polarization is
an effect in which one can synthesize a set of channels, by &quot;channel combining&quot;
and &quot;channel splitting,&quot; in which a fraction of the synthesized channels are
perfect for data transmission while the other fraction are completely useless
for data transmission, with the good fraction equal to the capacity of the
channel. The channel polarization effect then leads to a simple scheme for data
transmission: send the information bits through the perfect channels and
&quot;frozen&quot; bits through the useless ones. The main technical contributions of the
present paper are threefold. First, we leverage several known results from the
quantum information literature to demonstrate that the channel polarization
effect occurs for channels with classical inputs and quantum outputs. We then
construct linear polar codes based on this effect, and the encoding complexity
is O(N log N), where N is the blocklength of the code. We also demonstrate that
a quantum successive cancellation decoder works well, in the sense that the
word error rate decays exponentially with the blocklength of the code. For this
last result, we exploit Sen's recent &quot;non-commutative union bound&quot; that holds
for a sequence of projectors applied to a quantum state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2613</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2613</id><created>2011-09-12</created><updated>2012-06-01</updated><authors><author><keyname>Shi</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Lucani</keyname><forenames>Daniel E.</forenames></author></authors><title>Whether and Where to Code in the Wireless Relay Channel</title><categories>cs.NI</categories><comments>11 pages, 12 figures, to be published in the IEEE JSAC Special Issue
  on Theories and Methods for Advanced Wireless Relays</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The throughput benefits of random linear network codes have been studied
extensively for wirelined and wireless erasure networks. It is often assumed
that all nodes within a network perform coding operations. In
energy-constrained systems, however, coding subgraphs should be chosen to
control the number of coding nodes while maintaining throughput. In this paper,
we explore the strategic use of network coding in the wireless packet erasure
relay channel according to both throughput and energy metrics. In the relay
channel, a single source communicates to a single sink through the aid of a
half-duplex relay. The fluid flow model is used to describe the case where both
the source and the relay are coding, and Markov chain models are proposed to
describe packet evolution if only the source or only the relay is coding. In
addition to transmission energy, we take into account coding and reception
energies. We show that coding at the relay alone while operating in a rateless
fashion is neither throughput nor energy efficient. Given a set of system
parameters, our analysis determines the optimal amount of time the relay should
participate in the transmission, and where coding should be performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2637</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2637</id><created>2011-09-12</created><authors><author><keyname>Hoepman</keyname><forenames>Jaap-Henk</forenames></author></authors><title>In Things We Trust? Towards trustability in the Internet of Things</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This essay discusses the main privacy, security and trustability issues with
the Internet of Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2638</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2638</id><created>2011-09-12</created><authors><author><keyname>Garg</keyname><forenames>Nitin</forenames></author><author><keyname>Zhu</keyname><forenames>Ed</forenames></author><author><keyname>Botelho</keyname><forenames>Fabiano C.</forenames></author></authors><title>Light-weight Locks</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new approach to building synchronization
primitives, dubbed &quot;lwlocks&quot; (short for light-weight locks). The primitives are
optimized for small memory footprint while maintaining efficient performance in
low contention scenarios. A read-write lwlock occupies 4 bytes, a mutex
occupies 4 bytes (2 if deadlock detection is not required), and a condition
variable occupies 4 bytes. The corresponding primitives of the popular pthread
library occupy 56 bytes, 40 bytes and 48 bytes respectively on the x86-64
platform. The API for lwlocks is similar to that of the pthread library but
covering only the most common use cases. Lwlocks allow explicit control of
queuing and scheduling decisions in contention situations and support
&quot;asynchronous&quot; or &quot;deferred blocking&quot; acquisition of locks. Asynchronous
locking helps in working around the constraints of lock-ordering which
otherwise limits concurrency. The small footprint of lwlocks enables the
construction of data structures with very fine-grained locking, which in turn
is crucial for lowering contention and supporting highly concurrent access to a
data structure. Currently, the Data Domain File System uses lwlocks for its
in-memory inode cache as well as in a generic doubly-linked concurrent list
which forms the building block for more sophisticated structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2641</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2641</id><created>2011-09-12</created><updated>2011-10-28</updated><authors><author><keyname>Sommer</keyname><forenames>Christian</forenames></author></authors><title>More Compact Oracles for Approximate Distances in Planar Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance oracles are data structures that provide fast (possibly approximate)
answers to shortest-path and distance queries in graphs. The tradeoff between
the space requirements and the query time of distance oracles is of particular
interest and the main focus of this paper.
  In FOCS'01, Thorup introduced approximate distance oracles for planar graphs.
He proved that, for any eps&gt;0 and for any planar graph on n nodes, there exists
a (1+eps)-approximate distance oracle using space O(n eps^{-1} log n) such that
approximate distance queries can be answered in time O(1/eps).
  Ten years later, we give the first improvements on the space-querytime
tradeoff for planar graphs.
  * We give the first oracle having a space-time product with subquadratic
dependency on 1/eps. For space ~O(n log n) we obtain query time ~O(1/eps)
(assuming polynomial edge weights). The space shows a doubly logarithmic
dependency on 1/eps only. We believe that the dependency on eps may be almost
optimal.
  * For the case of moderate edge weights (average bounded by polylog(n), which
appears to be the case for many real-world road networks), we hit a &quot;sweet
spot,&quot; improving upon Thorup's oracle both in terms of eps and n. Our oracle
uses space ~O(n log log n) and it has query time ~O(log log log n + 1/eps).
  (Asymptotic notation in this abstract hides low-degree polynomials in
log(1/eps) and log*(n).)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2654</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2654</id><created>2011-09-12</created><authors><author><keyname>Mart&#xed;nez</keyname><forenames>Enrique</forenames><affiliation>University of Castilla-La Mancha</affiliation></author><author><keyname>Cambronero</keyname><forenames>M. Emilia</forenames><affiliation>University of Castilla-La Mancha</affiliation></author><author><keyname>D&#xed;az</keyname><forenames>Gregorio</forenames><affiliation>University of Castilla-La Mancha</affiliation></author><author><keyname>Schneider</keyname><forenames>Gerardo</forenames><affiliation>Chalmers | University of Gothenburg</affiliation></author></authors><title>Timed Automata Semantics for Visual e-Contracts</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FLACOS 2011, arXiv:1109.2399</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 68, 2011, pp. 7-21</journal-ref><doi>10.4204/EPTCS.68.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  C-O Diagrams have been introduced as a means to have a more visual
representation of electronic contracts, where it is possible to represent the
obligations, permissions and prohibitions of the different signatories, as well
as what are the penalties in case of not fulfillment of their obligations and
prohibitions. In such diagrams we are also able to represent absolute and
relative timing constraints. In this paper we present a formal semantics for
C-O Diagrams based on timed automata extended with an ordering of states and
edges in order to represent different deontic modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2655</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2655</id><created>2011-09-12</created><authors><author><keyname>D</keyname><forenames>Adrian Francalanza Ph.</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Sc</keyname><forenames>Andrew Gauci M.</forenames><affiliation>University of Malta</affiliation></author><author><keyname>D</keyname><forenames>Gordon Pace Ph.</forenames><affiliation>University of Malta</affiliation></author></authors><title>Distributed System Contract Monitoring</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FLACOS 2011, arXiv:1109.2399</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 68, 2011, pp. 23-37</journal-ref><doi>10.4204/EPTCS.68.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of behavioural contracts, to specify, regulate and verify systems, is
particularly relevant to runtime monitoring of distributed systems. System
distribution poses major challenges to contract monitoring, from
monitoring-induced information leaks to computation load balancing,
communication overheads and fault-tolerance. We present mDPi, a location-aware
process calculus, for reasoning about monitoring of distributed systems. We
define a family of Labelled Transition Systems for this calculus, which allow
formal reasoning about different monitoring strategies at different levels of
abstractions. We also illustrate the expressivity of the calculus by showing
how contracts in a simple contract language can be synthesised into different
mDPi monitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2656</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2656</id><created>2011-09-12</created><authors><author><keyname>Hantry</keyname><forenames>Francois</forenames><affiliation>UCBLyon France</affiliation></author><author><keyname>Hacid</keyname><forenames>Mohand-Said</forenames><affiliation>UCBLyon France</affiliation></author></authors><title>Handling Conflicts in Depth-First Search for LTL Tableau to Debug
  Compliance Based Languages</title><categories>cs.LO</categories><comments>In Proceedings FLACOS 2011, arXiv:1109.2399</comments><proxy>EPTCS</proxy><acm-class>F.3.1</acm-class><journal-ref>EPTCS 68, 2011, pp. 39-53</journal-ref><doi>10.4204/EPTCS.68.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing adequate tools to tackle the problem of inconsistent compliance
rules is a critical research topic. This problem is of paramount importance to
achieve automatic support for early declarative design and to support evolution
of rules in contract-based or service-based systems. In this paper we
investigate the problem of extracting temporal unsatisfiable cores in order to
detect the inconsistent part of a specification. We extend conflict-driven
SAT-solver to provide a new conflict-driven depth-first-search solver for
temporal logic. We use this solver to compute LTL unsatisfiable cores without
re-exploring the history of the solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2657</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2657</id><created>2011-09-12</created><authors><author><keyname>Montazeri</keyname><forenames>Seyed M.</forenames><affiliation>University of Gothenburg, Sweden</affiliation></author><author><keyname>Roy</keyname><forenames>Nivir K. S.</forenames><affiliation>University of Gothenburg, Sweden</affiliation></author><author><keyname>Schneider</keyname><forenames>Gerardo</forenames><affiliation>Chalmers | University of Gothenburg, Sweden</affiliation></author></authors><title>From Contracts in Structured English to CL Specifications</title><categories>cs.CL cs.FL cs.LO</categories><comments>In Proceedings FLACOS 2011, arXiv:1109.2399</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 68, 2011, pp. 55-69</journal-ref><doi>10.4204/EPTCS.68.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a framework to analyze conflicts of contracts
written in structured English. A contract that has manually been rewritten in a
structured English is automatically translated into a formal language using the
Grammatical Framework (GF). In particular we use the contract language CL as a
target formal language for this translation. In our framework CL specifications
could then be input into the tool CLAN to detect the presence of conflicts
(whether there are contradictory obligations, permissions, and prohibitions. We
also use GF to get a version in (restricted) English of CL formulae. We discuss
the implementation of such a framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2658</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2658</id><created>2011-09-12</created><authors><author><keyname>Gor&#xed;n</keyname><forenames>Daniel</forenames><affiliation>Dpto. Computaci&#xf3;n, FCEyN, UBA, Buenos Aires, Argentina</affiliation></author><author><keyname>Mera</keyname><forenames>Sergio</forenames><affiliation>Dpto. Computaci&#xf3;n, FCEyN, UBA, Buenos Aires, Argentina</affiliation></author><author><keyname>Schapachnik</keyname><forenames>Fernando</forenames><affiliation>Dpto. Computaci&#xf3;n, FCEyN, UBA, Buenos Aires, Argentina</affiliation></author></authors><title>A Software Tool for Legal Drafting</title><categories>cs.CY cs.LO cs.SE</categories><comments>In Proceedings FLACOS 2011, arXiv:1109.2399</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 68, 2011, pp. 71-86</journal-ref><doi>10.4204/EPTCS.68.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many attempts at automated aids for legal drafting have been made,
they were based on the construction of a new tool, completely from scratch.
This is at least curious, considering that a strong parallelism can be
established between a normative document and a software specification: both
describe what an entity should or should not do, can or cannot do.
  In this article we compare normative documents and software specifications to
find out their similarities and differences. The comparison shows that there
are distinctive particularities, but they are restricted to a very specific
subclass of normative propositions. The rest, we postulate, can be dealt with
software tools. For such an enterprise the \FormaLex tool set was devised: an
LTL-based language and companion tools that utilize model checking to find out
normative incoherences in regulations, contracts and other legal documents. A
feature-rich case study is analyzed with the presented tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2676</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2676</id><created>2011-09-13</created><updated>2013-07-02</updated><authors><author><keyname>Bayat</keyname><forenames>Siavash</forenames></author><author><keyname>Louie</keyname><forenames>Raymond H. Y.</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author></authors><title>Dynamic Decentralized Algorithms for Cognitive Radio Relay Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed spectrum access algorithm for cognitive radio relay
networks with multiple primary users (PU) and multiple secondary users (SU).
The key idea behind the proposed algorithm is that the PUs negotiate with the
SUs on both the amount of monetary compensation, and the amount of time the SUs
are either (i) allowed spectrum access, or (ii) cooperatively relaying the PU's
data, such that both the PUs' and the SUs' minimum rate requirement are
satisfied. The proposed algorithm is shown to be flexible in prioritizing
either the primary or the secondary users. We prove that the proposed algorithm
will result in the best possible stable matching and is weak Pareto optimal.
Numerical analysis also reveal that the distributed algorithm can achieve a
performance comparable to an optimal centralized solution, but with
significantly less overhead and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2684</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2684</id><created>2011-09-13</created><authors><author><keyname>Emruli</keyname><forenames>Sali</forenames></author><author><keyname>Zejneli</keyname><forenames>Tahir</forenames></author><author><keyname>Agai</keyname><forenames>Florin</forenames></author></authors><title>YouTube and political communication -- Macedonian case</title><categories>cs.SI</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 8, Issue
  4, No 1, July 2011 ISSN (Online): 1694-0814 http://www.IJCSI.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis how to use Internet influence to the process of political
communication, marketing and the management of public relations, what kind of
online communication methods are used by political parties, and to assess
satisfaction, means of communication and the services they provide to their
party's voters (people) and other interest groups and whether social networks
can affect the political and economic changes in the state, and the political
power of one party.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2696</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2696</id><created>2011-09-13</created><updated>2011-09-16</updated><authors><author><keyname>Gavoille</keyname><forenames>Cyril</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest, IUF</affiliation></author><author><keyname>Godfroy</keyname><forenames>Quentin</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Viennot</keyname><forenames>Laurent</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Node-Disjoint Multipath Spanners and their Relationship with
  Fault-Tolerant Spanners</title><categories>cs.NI cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by multipath routing, we introduce a multi-connected variant of
spanners. For that purpose we introduce the $p$-multipath cost between two
nodes $u$ and $v$ as the minimum weight of a collection of $p$ internally
vertex-disjoint paths between $u$ and $v$. Given a weighted graph $G$, a
subgraph $H$ is a $p$-multipath $s$-spanner if for all $u,v$, the $p$-multipath
cost between $u$ and $v$ in $H$ is at most $s$ times the $p$-multipath cost in
$G$. The $s$ factor is called the stretch. Building upon recent results on
fault-tolerant spanners, we show how to build $p$-multipath spanners of
constant stretch and of $\tO(n^{1+1/k})$ edges, for fixed parameters $p$ and
$k$, $n$ being the number of nodes of the graph. Such spanners can be
constructed by a distributed algorithm running in $O(k)$ rounds. Additionally,
we give an improved construction for the case $p=k=2$. Our spanner $H$ has
$O(n^{3/2})$ edges and the $p$-multipath cost in $H$ between any two node is at
most twice the corresponding one in $G$ plus $O(W)$, $W$ being the maximum edge
weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2697</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2697</id><created>2011-09-13</created><authors><author><keyname>Ling</keyname><forenames>Amy Poh Ai</forenames></author><author><keyname>Masao</keyname><forenames>Mukaidono</forenames></author></authors><title>Selection of Model in Developing Information Security Criteria for Smart
  Grid Security System</title><categories>cs.CR cs.SY</categories><journal-ref>Smart Grid Security and Communications, The Ninth International
  Symposium on Parallel and Distributed Processing with Applications (ISPA),
  No. 108, May 2011, Korea, pp.91-98; Journal of Convergence, Vol.2, No.1,
  2011-6, pp.39-46</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At present, the &quot;Smart Grid&quot; has emerged as one of the best advanced energy
supply chains. This paper looks into the security system of smart grid via the
smart planet system. The scope focused on information security criteria that
impact on consumer trust and satisfaction. The importance of information
security criteria is perceived as the main aspect to impact on customer trust
throughout the entire smart grid system. On one hand, this paper also focuses
on the selection of the model for developing information security criteria on a
smart grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2720</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2720</id><created>2011-09-13</created><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Morgenshtern</keyname><forenames>Veniamin I.</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author></authors><title>Capacity Pre-Log of SIMO Correlated Block-Fading Channels</title><categories>cs.IT math.IT</categories><comments>Accepted by ISWCS11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish an upper bound on the noncoherent capacity pre-log of temporally
correlated block-fading single-input multiple-output (SIMO) channels. The upper
bound matches the lower bound recently reported in Riegler et al. (2011), and,
hence, yields a complete characterization of the SIMO noncoherent capacity
pre-log, provided that the channel covariance matrix satisfies a mild technical
condition. This result allows one to determine the optimal number of receive
antennas to be used to maximize the capacity pre-log for a given block-length
and a given rank of the channel covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2724</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2724</id><created>2011-09-13</created><updated>2012-01-11</updated><authors><author><keyname>Tembine</keyname><forenames>H.</forenames></author><author><keyname>Boudec</keyname><forenames>J. -Y. Le</forenames></author><author><keyname>El-Azouzi</keyname><forenames>R.</forenames></author><author><keyname>Altman</keyname><forenames>E.</forenames></author></authors><title>Mean Field Asymptotics of Markov Decision Evolutionary Games and Teams</title><categories>math.OC cs.GT</categories><comments>23 pages. This paper has been presented at the First International
  Conference on Game Theory for Networks, Gamenets 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Mean Field Markov games with $N$ players, in which each
individual in a large population interacts with other randomly selected
players. The states and actions of each player in an interaction together
determine the instantaneous payoff for all involved players. They also
determine the transition probabilities to move to the next state. Each
individual wishes to maximize the total expected discounted payoff over an
infinite horizon. We provide a rigorous derivation of the asymptotic behavior
of this system as the size of the population grows to infinity. Under
indistinguishability per type assumption, we show that under any Markov
strategy, the random process consisting of one specific player and the
remaining population converges weakly to a jump process driven by the solution
of a system of differential equations. We characterize the solutions to the
team and to the game problems at the limit of infinite population and use these
to construct near optimal strategies for the case of a finite, but large,
number of players. We show that the large population asymptotic of the
microscopic model is equivalent to a (macroscopic) mean field stochastic game
in which a local interaction is described by a single player against a
population profile (the mean field limit). We illustrate our model to derive
the equations for a dynamic evolutionary Hawk and Dove game with energy level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2752</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2752</id><created>2011-09-13</created><authors><author><keyname>Morgado</keyname><forenames>Antonio</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>On Validating Boolean Optimizers</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean optimization finds a wide range of application domains, that
motivated a number of different organizations of Boolean optimizers since the
mid 90s. Some of the most successful approaches are based on iterative calls to
an NP oracle, using either linear search, binary search or the identification
of unsatisfiable sub-formulas. The increasing use of Boolean optimizers in
practical settings raises the question of confidence in computed results. For
example, the issue of confidence is paramount in safety critical settings. One
way of increasing the confidence of the results computed by Boolean optimizers
is to develop techniques for validating the results. Recent work studied the
validation of Boolean optimizers based on branch-and-bound search. This paper
complements existing work, and develops methods for validating Boolean
optimizers that are based on iterative calls to an NP oracle. This entails
implementing solutions for validating both satisfiable and unsatisfiable
answers from the NP oracle. The work described in this paper can be applied to
a wide range of Boolean optimizers, that find application in Pseudo-Boolean
Optimization and in Maximum Satisfiability. Preliminary experimental results
indicate that the impact of the proposed method in overall performance is
negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2763</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2763</id><created>2011-09-13</created><authors><author><keyname>Katchalski</keyname><forenames>Meir</forenames></author><author><keyname>Levy</keyname><forenames>Eliahu</forenames></author></authors><title>No O(N) queries for checking if N intervals cover everything or for
  piercing N pairs of intervals. An O(N log N)-steps algorithm for piercing</title><categories>math.CO cs.CG math.MG</categories><comments>8 pages, 2 figures</comments><msc-class>52C45, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of two related geometrical (indeed, combinatorial) problems is
considered, measured by the number of queries needed to determine the solution.
It is proved that one cannot check in a linear in N number of queries whether N
intervals cover a whole interval, or whether for N pairs of intervals on two
lines there is a pair of points intersecting each of these pairs of intervals
(&quot;piercing all pairs of intervals&quot;). The proofs are related to examples which
show that there is no &quot;Helly property&quot; here - the whole set of N may cover the
whole interval (resp. may have no pair of points piercing all pairs of
intervals) while any proper subset does not. Also, for the piercing problem we
outline an algorithm, taking O(N log N) steps, to check whether there is a pair
of points piercing all pairs of intervals and if there is, to find it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2766</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2766</id><created>2011-09-13</created><updated>2011-09-25</updated><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author></authors><title>Secure Broadcasting With Side-Information</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, submitted to International Conference on
  Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive information-theoretic performance limits for secure
and reliable communications over the general two-user discrete memoryless
broadcast channel with side-information at the transmitter. The sender wishes
to broadcast two independent messages to two receivers, under the constraint
that each message should be kept confidential from the unintended receiver.
Furthermore, the encoder has side-information - for example, fading in the
wireless medium, interference caused by neighboring nodes in the network, etc.
- provided to it in a noncausal manner, i.e., before the process of
transmission. We derive an inner bound on the capacity region of this channel,
by employing an extension of Marton's coding technique used for the classical
two-user broadcast channel, in conjunction with a stochastic encoder to satisfy
confidentiality constraints. Based on previously known results, we discuss a
procedure to present a schematic of the achievable rate region. The
rate-penalties for dealing with side-information and confidentiality
constraints make the achievable region for this channel strictly smaller than
the rate regions of those channels where one or both of these constraints are
relaxed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2777</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2777</id><created>2011-09-13</created><authors><author><keyname>Bras</keyname><forenames>Remco</forenames></author></authors><title>Connectivity Structure of Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider to what degree the structure of a linear system is
determined by the system's input/output behavior. The structure of a linear
system is a directed graph where the vertices represent the variables in the
system and an edge (x,y) exists if x directly influences y. In a number of
studies, researchers have attempted to identify such structures using
input/output data. Thus, our main aim is to consider to what degree the results
of such studies are valid. We begin by showing that in many cases, applying a
linear transformation to a system will change the system's graph. Furthermore,
we show that even the graph's components and their interactions are not
determined by input/output behavior. From these results, we conclude that
without further assumptions, very few aspects, if any, of a system's structure
are determined by its input/output relation. We consider a number of such
assumptions. First, we show that for a number of parameterizations, we can
characterize when two systems have the same structure. Second, in many
applications, we can use domain knowledge to exclude certain interactions. In
these cases, we can assume that a certain variable x does not influence another
variable y. We show that these assumptions cannot be sufficient to identify a
system's parameters using input/output data. We conclude that identifying a
system's structure from input/output data may not be possible given only
assumptions of the form x does not influence y.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2782</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2782</id><created>2011-09-13</created><updated>2011-10-08</updated><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author></authors><title>Two Classes of Broadcast Channels With Side-Information: Capacity Outer
  Bounds</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to International Conference on Communications,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive outer bounds on the capacity region of two classes
of the general two-user discrete memoryless broadcast channels with
side-information at the transmitter. The first class comprises the classical
broadcast channel where a sender transmits two independent messages to two
receivers. A constraint that each message must be kept confidential from the
unintended receiver constitutes the second class. For both classes, the
conditional distribution characterizing the channel depends on a state process
and the encoder has side-information provided to it in a noncausal manner. For
the first class of channels, an outer bound is derived employing techniques
used to prove the converse theorem for the Gel'fand-Pinsker's channel with
random parameters; the bounds are tight for individual rate constraints, but
can be improved upon for the sum rate. The technique for deriving outer bounds
for the second class of channels hinges on the confidentiality requirements; we
also derive a genie-aided outer bound, where a hypothetical genie gives the
unintended message to a receiver which treats it as side-information during
equivocation computation. For both classes of channels, Csisz\'{a}r's sum
identity plays a central role in establishing the capacity outer bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2785</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2785</id><created>2011-09-13</created><authors><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author><author><keyname>Schruefer</keyname><forenames>Eberhard</forenames></author><author><keyname>Webster</keyname><forenames>Kenneth</forenames></author></authors><title>Solving large linear algebraic systems in the context of integrable
  non-abelian Laurent ODEs</title><categories>cs.SC nlin.SI</categories><comments>15 pages, talk given at AMMCS 2011, submitted for publication in
  Programming and Computer Software</comments><msc-class>34M55, 37J35, 37K10</msc-class><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper reports on a computer algebra program LSSS (Linear Selective
Systems Solver) for solving linear algebraic systems with rational
coefficients. The program is especially efficient for very large sparse systems
that have a solution in which many variables take the value zero. The program
is applied to the symmetry investigation of a non-abelian Laurent ODE
introduced recently by M. Kontsevich. The computed symmetries confirmed that a
Lax pair found for this system earlier generates all first integrals of degree
at least up to 14.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2788</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2788</id><created>2011-09-13</created><authors><author><keyname>Stromatias</keyname><forenames>Evangelos</forenames></author></authors><title>Developing a supervised training algorithm for limited precision
  feed-forward spiking neural networks</title><categories>cs.NE</categories><comments>107 pages, MSc thesis Microelectronic Systems, 2011, University of
  Liverpool, supervised by John Marsland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neural networks have been referred to as the third generation of
artificial neural networks where the information is coded as time of the
spikes. There are a number of different spiking neuron models available and
they are categorized based on their level of abstraction. In addition, there
are two known learning methods, unsupervised and supervised learning. This
thesis focuses on supervised learning where a new algorithm is proposed, based
on genetic algorithms. The proposed algorithm is able to train both synaptic
weights and delays and also allow each neuron to emit multiple spikes thus
taking full advantage of the spatial-temporal coding power of the spiking
neurons. In addition, limited synaptic precision is applied; only six bits are
used to describe and train a synapse, three bits for the weights and three bits
for the delays. Two limited precision schemes are investigated. The proposed
algorithm is tested on the XOR classification problem where it produces better
results for even smaller network architectures than the proposed ones.
Furthermore, the algorithm is benchmarked on the Fisher iris classification
problem where it produces higher classification accuracies compared to
SpikeProp, QuickProp and Rprop. Finally, a hardware implementation on a
microcontroller is done for the XOR problem as a proof of concept. Keywords:
Spiking neural networks, supervised learning, limited synaptic precision,
genetic algorithms, hardware implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2793</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2793</id><created>2011-09-13</created><updated>2012-05-15</updated><authors><author><keyname>Yan</keyname><forenames>Bowen</forenames></author><author><keyname>Gregory</keyname><forenames>Steve</forenames></author></authors><title>Finding missing edges in networks based on their community structure</title><categories>cs.IR cs.SI physics.data-an physics.soc-ph</categories><comments>7 pages, 6 figures</comments><journal-ref>Phys. Rev. E 85, 056112 (2012)</journal-ref><doi>10.1103/PhysRevE.85.056112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many edge prediction methods have been proposed, based on various local or
global properties of the structure of an incomplete network. Community
structure is another significant feature of networks: Vertices in a community
are more densely connected than average. It is often true that vertices in the
same community have &quot;similar&quot; properties, which suggests that missing edges are
more likely to be found within communities than elsewhere. We use this insight
to propose a strategy for edge prediction that combines existing edge
prediction methods with community detection. We show that this method gives
better prediction accuracy than existing edge prediction methods alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2806</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2806</id><created>2011-09-13</created><authors><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>HPI</affiliation></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames><affiliation>GREYC, UMMISCO</affiliation></author><author><keyname>Koch</keyname><forenames>Pierrick</forenames><affiliation>GREYC</affiliation></author></authors><title>Using the DiaSpec design language and compiler to develop robotics
  systems</title><categories>cs.RO cs.SE</categories><comments>DSLRob'11: Domain-Specific Languages and models for ROBotic systems
  (2011)</comments><proxy>ccsd</proxy><report-no>DSLRob/2011/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Sense/Compute/Control (SCC) application is one that interacts with the
physical environment. Such applications are pervasive in domains such as
building automation, assisted living, and autonomic computing. Developing an
SCC application is complex because: (1) the implementation must address both
the interaction with the environment and the application logic; (2) any
evolution in the environment must be reflected in the implementation of the
application; (3) correctness is essential, as effects on the physical
environment can have irreversible consequences. The SCC architectural pattern
and the DiaSpec domain-specific design language propose a framework to guide
the design of such applications. From a design description in DiaSpec, the
DiaSpec compiler is capable of generating a programming framework that guides
the developer in implementing the design and that provides runtime support. In
this paper, we report on an experiment using DiaSpec (both the design language
and compiler) to develop a standard robotics application. We discuss the
benefits and problems of using DiaSpec in a robotics setting and present some
changes that would make DiaSpec a better framework in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2807</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2807</id><created>2011-09-13</created><authors><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Balland</keyname><forenames>Emilie</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Consel</keyname><forenames>Charles</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, ENSEIRB</affiliation></author><author><keyname>Lawall</keyname><forenames>Julia</forenames><affiliation>DIKU, LIP6</affiliation></author></authors><title>Leveraging Software Architectures to Guide and Verify the Development of
  Sense/Compute/Control Applications</title><categories>cs.PL cs.SE</categories><proxy>ccsd</proxy><journal-ref>ICSE'11: Proceedings of the 33rd International Conference on
  Software Engineering (2011) 431-440</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A software architecture describes the structure of a computing system by
specifying software components and their interactions. Mapping a software
architecture to an implementation is a well known challenge. A key element of
this mapping is the architecture's description of the data and control-flow
interactions between components. The characterization of these interactions can
be rather abstract or very concrete, providing more or less implementation
guidance, programming support, and static verification. In this paper, we
explore one point in the design space between abstract and concrete component
interaction specifications. We introduce a notion of behavioral contract that
expresses the set of allowed interactions between components, describing both
data and control-flow constraints. This declaration is part of the architecture
description, allows generation of extensive programming support, and enables
various verifications. We instantiate our approach in an architecture
description language for the domain of Sense/Compute/Control applications, and
describe associated compilation and verification strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2809</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2809</id><created>2011-09-13</created><updated>2011-11-29</updated><authors><author><keyname>Chyzak</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Davenport</keyname><forenames>James</forenames><affiliation>RISC</affiliation></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames><affiliation>RISC</affiliation></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>On Kahan's Rules for Determining Branch Cuts</title><categories>cs.SC</categories><comments>SYNASC 2011. 13th International Symposium on Symbolic and Numeric
  Algorithms for Scientific Computing. (2011)</comments><proxy>ccsd</proxy><doi>10.1109/SYNASC.2011.51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer algebra there are different ways of approaching the mathematical
concept of functions, one of which is by defining them as solutions of
differential equations. We compare different such approaches and discuss the
occurring problems. The main focus is on the question of determining possible
branch cuts. We explore the extent to which the treatment of branch cuts can be
rendered (more) algorithmic, by adapting Kahan's rules to the differential
equation setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2816</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2816</id><created>2011-09-13</created><authors><author><keyname>Hartley</keyname><forenames>Edward N.</forenames></author><author><keyname>Maciejowski</keyname><forenames>Jan M.</forenames></author></authors><title>Designing MPC controllers by reverse-engineering existing LTI
  controllers</title><categories>math.OC cs.SY</categories><report-no>CUED/F-INFENG/TR.671</report-no><msc-class>93C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report presents a method for designing a constrained
output-feedback model predictive controller (MPC) that behaves in the same way
as an existing baseline stabilising linear time invariant output-feedback
controller when constraints are inactive. The baseline controller is cast into
an observer-compensator form and an inverse-optimal cost function is used as
the basis of the MPC controller. The available degrees of design freedom are
explored, and some guidelines provided for the selection of an appropriate
observer-compensator realisation that will best allow exploitation of the
constraint-handling and redundancy management capabilities of MPC.
Consideration is given to output setpoint tracking, and the method is
demonstrated with three different multivariable plants of varying complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2843</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2843</id><created>2011-09-13</created><authors><author><keyname>Jaafar</keyname><forenames>Wael</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author><author><keyname>Haccoun</keyname><forenames>David</forenames></author></authors><title>A Novel Relay-Aided Transmission Scheme in Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>In Proc. IEEE Global Communications Conference (IEEE Globecom 2011),
  Houston, TX, USA, 5-9 Dec. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In underlay cognitive radio networks, unlicensed secondary users are allowed
to share the spectrum with licensed primary users when the interference induced
on the primary transmission is limited. In this paper, we propose a new
cooperative transmission scheme for cognitive radio networks where a relay node
is able to help both the primary and secondary transmissions. We derive exact
closed-form and upper bound expressions of the conditional primary and
secondary outage probabilities over Rayleigh fading channels. Furthermore, we
proposed a simple power allocation algorithm. Finally, using numerical
evaluation and simulation results we show the potential of our cooperative
transmission scheme in improving the secondary outage probability without
harming the primary one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2844</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2844</id><created>2011-09-13</created><updated>2013-07-25</updated><authors><author><keyname>Kunz-Jacques</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Jouguet</keyname><forenames>Paul</forenames></author></authors><title>Using Hash-Based Signatures to Bootstrap Quantum Key Distribution</title><categories>quant-ph cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum Key Distribution is a secret distribution technique that requires an
authenticated channel. This channel is usually created on top of an
un-authenticated communication medium using unconditionally secure Message
Authentication Codes (MAC) and an initial common secret. We examine the
consequences of replacing this MAC algorithm by a cryptographic hash-based
signature algorithm, like the Lamport algorithm. We show that provided one-way
functions exist, the Lamport algorithm or its variants can be instantiated in a
secure way in the Universally Composable sense, and can therefore be plugged
into any QKD protocol with a composable security proof in a secure manner. This
association, while relying on short-term computational hardness assumptions,
results in an increase of the practical security of QKD and eases its
deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2873</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2873</id><created>2011-09-13</created><authors><author><keyname>Rahmouni</keyname><forenames>M'hamed</forenames></author></authors><title>MDA-based ATL transformation to generate MVC 2 web models</title><categories>cs.SE</categories><comments>International Journal of Computer Science &amp; Information
  Technology-2011</comments><doi>10.5121/ijcsit.2011.3405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development and maintenance of Web application is still a complex and
error-prone process. We need integrated techniques and tool support for
automated generation of Web systems and a ready prescription for easy
maintenance. The MDA approach proposes an architecture taking into account the
development and maintenance of large and complex software. In this paper, we
apply MDA approach for generating PSM from UML design to MVC 2Web
implementation. That is why we have developed two meta-models handling UML
class diagrams and MVC 2 Web applications, then we have to set up
transformation rules. These last are expressed in ATL language. To specify the
transformation rules (especially CRUD methods) we used a UML profiles. To
clearly illustrate the result generated by this transformation, we converted
the XMI file generated in an EMF (Eclipse Modeling Framework) model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2885</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2885</id><created>2011-09-13</created><updated>2012-04-25</updated><authors><author><keyname>Golin</keyname><forenames>Mordecai J.</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>Krizanc</keyname><forenames>Danny</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author><author><keyname>Rao</keyname><forenames>S. Srinivasa</forenames></author><author><keyname>Shende</keyname><forenames>Sunil</forenames></author></authors><title>Encoding 2-D Range Maximum Queries</title><categories>cs.DS</categories><comments>Full version of ISAAC 2011 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the \emph{two-dimensional range maximum query (2D-RMQ)} problem:
given an array $A$ of ordered values, to pre-process it so that we can find the
position of the smallest element in the sub-matrix defined by a
(user-specified) range of rows and range of columns. We focus on determining
the \emph{effective} entropy of 2D-RMQ, i.e., how many bits are needed to
encode $A$ so that 2D-RMQ queries can be answered \emph{without} access to $A$.
We give tight upper and lower bounds on the expected effective entropy for the
case when $A$ contains independent identically-distributed random values, and
new upper and lower bounds for arbitrary $A$, for the case when $A$ contains
few rows. The latter results improve upon previous upper and lower bounds by
Brodal et al. (ESA 2010). In some cases we also give data structures whose
space usage is close to the effective entropy and answer 2D-RMQ queries
rapidly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2891</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2891</id><created>2011-09-13</created><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Kan</keyname><forenames>Haibin</forenames></author></authors><title>On the nonexistence of $[\binom{2m}{m-1}, 2m, \binom{2m-1}{m-1}]$, $m$
  odd, complex orthogonal design</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex orthogonal designs (CODs) are used to construct space-time block
codes. COD $\mathcal{O}_z$ with parameter $[p, n, k]$ is a $p\times n$ matrix,
where nonzero entries are filled by $\pm z_i$ or $\pm z^*_i$, $i = 1, 2,...,
k$, such that $\mathcal{O}^H_z \mathcal{O}_z =
(|z_1|^2+|z_2|^2+...+|z_k|^2)I_{n \times n}$. Adams et al. in &quot;The final case
of the decoding delay problem for maximum rate complex orthogonal designs,&quot;
IEEE Trans. Inf. Theory, vol. 56, no. 1, pp. 103-122, Jan. 2010, first proved
the nonexistence of $[\binom{2m}{m-1}, 2m, \binom{2m-1}{m-1}]$, $m$ odd, COD.
Combining with the previous result that decoding delay should be an integer
multiple of $\binom{2m}{m-1}$, they solved the final case $n \equiv 2 \pmod 4$
of the decoding delay problem for maximum rate complex orthogonal designs.
  In this paper, we give another proof of the nonexistence of COD with
parameter $[\binom{2m}{m-1}, 2m, \binom{2m-1}{m-1}]$, $m$ odd. Our new proof is
based on the uniqueness of $[\binom{2m}{m-1}, 2m-1, \binom{2m-1}{m-1}]$ under
equivalence operation, where an explicit-form representation is proposed to
help the proof. Then, by proving it's impossible to add an extra orthogonal
column on COD $[\binom{2m}{m-1}, 2m-1, \binom{2m-1}{m-1}]$ when $m$ is odd, we
complete the proof of the nonexistence of COD $[\binom{2m}{m-1}, 2m,
\binom{2m-1}{m-1}]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2930</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2930</id><created>2011-09-13</created><updated>2012-10-31</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Hoobin</keyname><forenames>Christopher</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Faster Approximate Pattern Matching in Compressed Repetitive Texts</title><categories>cs.DS</categories><comments>Journal version of ISAAC '11 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the imminent growth of massive, highly redundant genomic
databases, we study the problem of compressing a string database while
simultaneously supporting fast random access, substring extraction and pattern
matching to the underlying string(s). Bille et al. (2011) recently showed how,
given a straight-line program with $r$ rules for a string $s$ of length $n$, we
can build an $\Oh{r}$-word data structure that allows us to extract any
substring of length $m$ in $\Oh{\log n + m}$ time. They also showed how, given
a pattern $p$ of length $m$ and an edit distance (k \leq m), their data
structure supports finding all \occ approximate matches to $p$ in $s$ in $\Oh{r
(\min (m k, k^4 + m) + \log n) + \occ}$ time. Rytter (2003) and Charikar et al.
(2005) showed that $r$ is always at least the number $z$ of phrases in the LZ77
parse of $s$, and gave algorithms for building straight-line programs with
$\Oh{z \log n}$ rules. In this paper we give a simple $\Oh{z \log n}$-word data
structure that takes the same time for substring extraction but only $\Oh{z
\min (m k, k^4 + m) + \occ}$ time for approximate pattern matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2944</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2944</id><created>2011-09-13</created><authors><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Real Interference Alignment and Degrees of Freedom Region of Wireless X
  Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single hop wireless X network with $K$ transmitters and $J$
receivers, all with single antenna. Each transmitter conveys for each receiver
an independent message. The channel is assumed to have constant coefficients.
We develop interference alignment scheme for this setup and derived several
achievable degrees of freedom regions. We show that in some cases, the derived
region meets a previous outer bound and are hence the DoF region. For our
achievability schemes, we divide each message into streams and use real
interference alignment on the streams. Several previous results on the DoF
region and total DoF for various special cases can be recovered from our
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2950</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2950</id><created>2011-09-13</created><authors><author><keyname>Estrada</keyname><forenames>Ernesto</forenames><affiliation>U. Strathclyde</affiliation></author><author><keyname>Hatano</keyname><forenames>Naomichi</forenames><affiliation>U. Tokyo</affiliation></author><author><keyname>Benzi</keyname><forenames>Michele</forenames><affiliation>Emory U.</affiliation></author></authors><title>The Physics of Communicability in Complex Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI math-ph math.MP</categories><comments>Review Article. 90 pages, 14 figures. Contents: Introduction;
  Communicability in Networks; Physical Analogies; Comparing Communicability
  Functions; Communicability and the Analysis of Networks; Communicability and
  Localization in Complex Networks; Computability of Communicability Functions;
  Conclusions and Prespectives</comments><journal-ref>Phys. Rep. 514 (2012) 89-119</journal-ref><doi>10.1016/j.physrep.2012.01.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in the study of complex networks is to provide
quantitative measures of correlation and information flow between different
parts of a system. To this end, several notions of communicability have been
introduced and applied to a wide variety of real-world networks in recent
years. Several such communicability functions are reviewed in this paper. It is
emphasized that communication and correlation in networks can take place
through many more routes than the shortest paths, a fact that may not have been
sufficiently appreciated in previously proposed correlation measures. In
contrast to these, the communicability measures reviewed in this paper are
defined by taking into account all possible routes between two nodes, assigning
smaller weights to longer ones. This point of view naturally leads to the
definition of communicability in terms of matrix functions, such as the
exponential, resolvent, and hyperbolic functions, in which the matrix argument
is either the adjacency matrix or the graph Laplacian associated with the
network. Considerable insight on communicability can be gained by modeling a
network as a system of oscillators and deriving physical interpretations, both
classical and quantum-mechanical, of various communicability functions.
Applications of communicability measures to the analysis of complex systems are
illustrated on a variety of biological, physical and social networks. The last
part of the paper is devoted to a review of the notion of locality in complex
networks and to computational aspects that by exploiting sparsity can greatly
reduce the computational efforts for the calculation of communicability
functions for large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2954</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2954</id><created>2011-09-13</created><authors><author><keyname>Martonosi</keyname><forenames>Susan E.</forenames></author><author><keyname>Altner</keyname><forenames>Doug</forenames></author><author><keyname>Ernst</keyname><forenames>Michael</forenames></author><author><keyname>Ferme</keyname><forenames>Elizabeth</forenames></author><author><keyname>Langsjoen</keyname><forenames>Kira</forenames></author><author><keyname>Lindsay</keyname><forenames>Danika</forenames></author><author><keyname>Plott</keyname><forenames>Sean</forenames></author><author><keyname>Ronan</keyname><forenames>Andrew S.</forenames></author></authors><title>A New Framework for Network Disruption</title><categories>cs.SI math.CO math.OC physics.soc-ph</categories><comments>Submitted for peer review on September 13, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional network disruption approaches focus on disconnecting or
lengthening paths in the network. We present a new framework for network
disruption that attempts to reroute flow through critical vertices via vertex
deletion, under the assumption that this will render those vertices vulnerable
to future attacks. We define the load on a critical vertex to be the number of
paths in the network that must flow through the vertex. We present
graph-theoretic and computational techniques to maximize this load, firstly by
removing either a single vertex from the network, secondly by removing a subset
of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2957</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2957</id><created>2011-09-13</created><authors><author><keyname>Firouzabadi</keyname><forenames>Sina</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Downlink Performance and Capacity of Distributed Antenna Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the performance of the downlink channel in
distributed antenna systems. We first establish the ergodic capacity of
distributed antennas, under different channel side information (CSI)
assumptions. We consider a generalized distributed antenna system with $N$
distributed ports, each of which is equipped with an array of $L$ transmit
antennas and constrained by a fixed transmit power. For this system we
calculate the downlink capacity to a single antenna receiver, under different
assumptions about the availability of the channel states at the transmitter.
Having established this information theoretic analysis of the ergodic capacity
of distributed antenna systems, this paper also investigates the effect of
antenna placement on the performance of such systems. In particular, we
investigate the optimal placement of the transmit antennas in distributed
antenna systems. We present a fairly general framework for this optimization
with no constraint on the location of the antennas. Based on stochastic
approximation theory, we adopt a formulation that is suitable for node
placement optimization in various wireless network scenarios. We show that
optimal placement of antennas inside the coverage region can significantly
improve the power efficiency of wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2963</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2963</id><created>2011-09-13</created><authors><author><keyname>Comin</keyname><forenames>Cesar H.</forenames></author><author><keyname>Bunoro</keyname><forenames>Jo&#xe3;o B.</forenames></author><author><keyname>Viana</keyname><forenames>Matheus P.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Unveiling the Relationship Between Structure and Dynamics in Complex
  Networks</title><categories>physics.data-an cs.SI nlin.CD physics.soc-ph stat.ME</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last years, a great deal of attention has been focused on complex
networked systems, characterized by intricate structure and dynamics. The
latter has been often represented in terms of overall statistics (e.g. average
and standard deviations) of the time signals. While such approaches have led to
many insights, they have failed to take into account that signals at different
parts of the system can undergo distinct evolutions, which cannot be properly
represented in terms of average values. A novel framework for identifying the
principal aspects of the dynamics and how it is influenced by the network
structure is proposed in this work. The potential of this approach is
illustrated with respect to three important models (Integrate-and-Fire, SIS and
Kuramoto), allowing the identification of highly structured dynamics, in the
sense that different groups of nodes not only presented specific dynamics but
also felt the structure of the network in different ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2964</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2964</id><created>2011-09-13</created><authors><author><keyname>Zhu</keyname><forenames>Junjie</forenames></author><author><keyname>Govindasamy</keyname><forenames>Siddhartan</forenames></author></authors><title>Performance of Multi-Antenna MMSE Receivers in Non-homogeneous Poisson
  Networks</title><categories>cs.IT math.IT</categories><doi>10.1109/ICC.2012.6364332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique to compute the Cumulative Distribution Function (CDF) of the
Signal-to-Interference-plus-Noise-Ratio (SINR) for a wireless link with a
multi-antenna, Linear, Minimum-Mean-Square-Error (MMSE) receiver in the
presence of interferers distributed according to a non-homogenous Poisson point
process on the plane, and independent Rayleigh fading between antennas is
presented. This technique is used to compute the CDF of the SINR for several
different models of intensity functions, in particular, power-law intensity
functions, circular-symmetric Gaussian intensity functions and intensity
functions described by a polynomial in a bounded domain. Additionally it is
shown that if the number of receiver antennas is scaled linearly with the
intensity function, the SINR converges in probability to a limit determined by
the &quot;shape&quot; of the underlying intensity function. This work generalizes known
results for homogenous Poisson networks to non-homogenous Poisson networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2984</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2984</id><created>2011-09-13</created><authors><author><keyname>Huang</keyname><forenames>Baoqi</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author><author><keyname>Yu</keyname><forenames>Changbin</forenames></author></authors><title>A Statistically Modelling Method for Performance Limits in Sensor
  Localization</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study performance limits of sensor localization from a
novel perspective. Specifically, we consider the Cramer-Rao Lower Bound (CRLB)
in single-hop sensor localization using measurements from received signal
strength (RSS), time of arrival (TOA) and bearing, respectively, but
differently from the existing work, we statistically analyze the trace of the
associated CRLB matrix (i.e. as a scalar metric for performance limits of
sensor localization) by assuming anchor locations are random. By the Central
Limit Theorems for $U$-statistics, we show that as the number of the anchors
increases, this scalar metric is asymptotically normal in the RSS/bearing case,
and converges to a random variable which is an affine transformation of a
chi-square random variable of degree 2 in the TOA case. Moreover, we provide
formulas quantitatively describing the relationship among the mean and standard
deviation of the scalar metric, the number of the anchors, the parameters of
communication channels, the noise statistics in measurements and the spatial
distribution of the anchors. These formulas, though asymptotic in the number of
the anchors, in many cases turn out to be remarkably accurate in predicting
performance limits, even if the number is small. Simulations are carried out to
confirm our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2992</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2992</id><created>2011-09-14</created><updated>2013-04-14</updated><authors><author><keyname>Yu</keyname><forenames>Seung Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Downlink Capacity and Base Station Density in Cellular Networks</title><categories>cs.NI</categories><comments>to appear in Proc. IEEE WiOpt Workshop on Spatial Stochastic Models
  for Wireless Networks (SpaSWiN 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been a bulk of analytic results about the performance of cellular
networks where base stations are regularly located on a hexagonal or square
lattice. This regular model cannot reflect the reality, and tends to
overestimate the network performance. Moreover, tractable analysis can be
performed only for a fixed location user (e.g., cell center or edge user). In
this paper, we use the stochastic geometry approach, where base stations can be
modeled as a homogeneous Poisson point process. We also consider the user
density, and derive the user outage probability that an arbitrary user is under
outage owing to low signal-to-interference-plus-noise ratio or high congestion
by multiple users. Using the result, we calculate the density of success
transmissions in the downlink cellular network. An interesting observation is
that the success transmission density increases with the base station density,
but the increasing rate diminishes. This means that the number of base stations
installed should be more than $n$-times to increase the network capacity by a
factor of $n$. Our results will provide a framework for performance analysis of
the wireless infrastructure with a high density of access points, which will
significantly reduce the burden of network-level simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2993</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2993</id><created>2011-09-14</created><authors><author><keyname>Yazd</keyname><forenames>Maryam Faramarzi</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>A Delay-Constrained General Achievable Rate and Certain Capacity Results
  for UWB Relay Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, accepted for publication and presentation at
  ISWCS 2011, Aachen, Germany, 6th-9th November</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive UWB version of (i) general best achievable rate for
the relay channel with decode-andforward strategy and (ii) max-flow min-cut
upper bound, such that the UWB relay channel can be studied considering the
obtained lower and upper bounds. Then, we show that by appropriately choosing
the noise correlation coefficients, our new upper bound coincides with the
lower bound in special cases of degraded and reversely degraded UWB relay
channels. Finally, some numerical results are illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2997</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2997</id><created>2011-09-14</created><authors><author><keyname>Andreyev</keyname><forenames>Sergey</forenames></author></authors><title>Rejecting Adaptive Interface</title><categories>cs.HC</categories><comments>14 pages, 3 tables, 5 figures</comments><acm-class>H.5.2; D.2.2; H.1.2; I.3.6; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs have to be designed in such a way as to make them looking good and
being handy for all users. Adaptive interface, with all the numerous
achievements throughout 30 years of its history, contains and in reality is
based on one fundamental flaw - on the assumption that designer knows better
than anyone else what is good for users in each and all cases. Programs of the
new type - user-driven applications - still deliver to users the best results
of developers' work but at the same time give users the full control over
applications and in this way really allow each user to change an application in
such a way as he wants it to look at each particular moment. Users can move and
resize each and all of the screen objects while an application is running, and
this changes the whole programming philosophy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.2998</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.2998</id><created>2011-09-14</created><authors><author><keyname>Chang</keyname><forenames>Weng-Long</forenames></author><author><keyname>Feng</keyname><forenames>Mang</forenames></author><author><keyname>Lin</keyname><forenames>Kawuu Weicheng</forenames></author><author><keyname>Wang</keyname><forenames>Chih-Chiang</forenames></author><author><keyname>Chen</keyname><forenames>Ju-Chin</forenames></author></authors><title>Quantum Algorithms of Solving the Backtracking of One-dimensional
  Cellular Automata</title><categories>cs.DS nlin.CG</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [Wolfram 1982; Wolfram 1983; Wolfram 2002], the backtracking of
one-dimensional cellular automata is to find out which of the 2n possible
initial configurations of width n evolve to a specific configuration. In this
paper, in one-dimensional cellular automata for a specific configuration of
width n, its unique initial configuration can be found by mean of the proposed
quantum algorithm with polynomial quantum gates, polynomial quantum bits and
the successful probability that is the same as that of Shor's quantum
order-finding algorithm in [Shor 1994].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3031</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3031</id><created>2011-09-14</created><updated>2011-09-27</updated><authors><author><keyname>Schwinghammer</keyname><forenames>Jan</forenames><affiliation>Saarland University</affiliation></author><author><keyname>Birkedal</keyname><forenames>Lars</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Reus</keyname><forenames>Bernhard</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Yang</keyname><forenames>Hongseok</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Nested Hoare Triples and Frame Rules for Higher-order Store</title><categories>cs.LO</categories><comments>42 pages</comments><proxy>LMCS</proxy><acm-class>F.3.1, F.3.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  28, 2011) lmcs:996</journal-ref><doi>10.2168/LMCS-7(3:21)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation logic is a Hoare-style logic for reasoning about programs with
heap-allocated mutable data structures. As a step toward extending separation
logic to high-level languages with ML-style general (higher-order) storage, we
investigate the compatibility of nested Hoare triples with several variations
of higher-order frame rules. The interaction of nested triples and frame rules
can be subtle, and the inclusion of certain frame rules is in fact unsound. A
particular combination of rules can be shown consistent by means of a Kripke
model where worlds live in a recursively defined ultrametric space. The
resulting logic allows us to elegantly prove programs involving stored code. In
particular, using recursively defined assertions, it leads to natural
specifications and proofs of invariants required for dealing with recursion
through the store.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3041</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3041</id><created>2011-09-14</created><updated>2013-05-08</updated><authors><author><keyname>Decelle</keyname><forenames>Aurelien</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Asymptotic analysis of the stochastic block model for modular networks
  and its algorithmic applications</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.SI physics.soc-ph</categories><comments>Typos in eq. (40) on p. 13 fixed</comments><journal-ref>Phys. Rev. E 84, 066106 (2011)</journal-ref><doi>10.1103/PhysRevE.84.066106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend our previous work on the stochastic block model, a
commonly used generative model for social and biological networks, and the
problem of inferring functional groups or communities from the topology of the
network. We use the cavity method of statistical physics to obtain an
asymptotically exact analysis of the phase diagram. We describe in detail
properties of the detectability/undetectability phase transition and the
easy/hard phase transition for the community detection problem. Our analysis
translates naturally into a belief propagation algorithm for inferring the
group memberships of the nodes in an optimal way, i.e., that maximizes the
overlap with the underlying group memberships, and learning the underlying
parameters of the block model. Finally, we apply the algorithm to two examples
of real-world networks and discuss its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3056</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3056</id><created>2011-09-14</created><updated>2012-05-13</updated><authors><author><keyname>Delporte-Gallet</keyname><forenames>Carole</forenames></author><author><keyname>Fauconnier</keyname><forenames>Hugues</forenames></author><author><keyname>Gafni</keyname><forenames>Eli</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author></authors><title>Wait-Freedom with Advice</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We motivate and propose a new way of thinking about failure detectors which
allows us to define, quite surprisingly, what it means to solve a distributed
task \emph{wait-free} \emph{using a failure detector}. In our model, the system
is composed of \emph{computation} processes that obtain inputs and are supposed
to output in a finite number of steps and \emph{synchronization} processes that
are subject to failures and can query a failure detector. We assume that, under
the condition that \emph{correct} synchronization processes take sufficiently
many steps, they provide the computation processes with enough \emph{advice} to
solve the given task wait-free: every computation process outputs in a finite
number of its own steps, regardless of the behavior of other computation
processes. Every task can thus be characterized by the \emph{weakest} failure
detector that allows for solving it, and we show that every such failure
detector captures a form of set agreement. We then obtain a complete
classification of tasks, including ones that evaded comprehensible
characterization so far, such as renaming or weak symmetry breaking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3069</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3069</id><created>2011-09-14</created><updated>2012-03-08</updated><authors><author><keyname>Bartz</keyname><forenames>Daniel</forenames></author><author><keyname>Hatrick</keyname><forenames>Kerr</forenames></author><author><keyname>Hesse</keyname><forenames>Christian W.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Lemm</keyname><forenames>Steven</forenames></author></authors><title>Directional Variance Adjustment: improving covariance estimates for
  high-dimensional portfolio optimization</title><categories>q-fin.PM cs.CE q-fin.ST</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust and reliable covariance estimates play a decisive role in financial
and many other applications. An important class of estimators is based on
Factor models. Here, we show by extensive Monte Carlo simulations that
covariance matrices derived from the statistical Factor Analysis model exhibit
a systematic error, which is similar to the well-known systematic error of the
spectrum of the sample covariance matrix. Moreover, we introduce the
Directional Variance Adjustment (DVA) algorithm, which diminishes the
systematic error. In a thorough empirical study for the US, European, and Hong
Kong market we show that our proposed method leads to improved portfolio
allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3070</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3070</id><created>2011-09-14</created><authors><author><keyname>Haimovich</keyname><forenames>Hernan</forenames></author><author><keyname>Braslavsky</keyname><forenames>Julio H.</forenames></author></authors><title>Sufficient conditions for the genericity of feedback stabilisability of
  switching systems via Lie-algebraic solvability</title><categories>cs.SY math.OC</categories><comments>Extended version of that accepted at the XIV Workshop on Information
  Processing and Control (RPIC), Oro Verde, Entre Rios, Argentina, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the stabilisation of discrete-time switching linear
systems (DTSSs) with control inputs under arbitrary switching, based on the
existence of a common quadratic Lyapunov function (CQLF). The authors have
begun a line of work dealing with control design based on the Lie-algebraic
solvability property. The present paper expands on earlier work by deriving
sufficient conditions under which the closed-loop system can be caused to
satisfy the Lie-algebraic solvability property generically, i.e. for almost
every set of system parameters, furthermore admitting straightforward and
efficient numerical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3071</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3071</id><created>2011-09-14</created><updated>2012-10-24</updated><authors><author><keyname>Caputo</keyname><forenames>Jean-Guy</forenames></author><author><keyname>Knippel</keyname><forenames>Arnaud</forenames></author><author><keyname>Simo</keyname><forenames>Elie</forenames></author></authors><title>Oscillations of simple networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI nlin.CD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To describe the flow of a miscible quantity on a network, we introduce the
graph wave equation where the standard continuous Laplacian is replaced by the
graph Laplacian. This is a natural description of an array of inductances and
capacities, of fluid flow in a network of ducts and of a system of masses and
springs. The structure of the graph influences strongly the dynamics which is
naturally described using the basis of the eigenvectors. In particular, we show
that if two outer nodes are connected to a common third node with the same
coupling, then this coupling is an eigenvalue of the Laplacian. Assuming the
graph is forced and damped at specific nodes, we derive the amplitude
equations. These are analyzed for two simple non trivial networks: a tree and a
graph with a cycle. Forcing the network at a resonant frequency reveals that
damping can be ineffective if applied to the wrong node, leading to a
disastrous resonance and destruction of the network. These results could be
useful for complex physical networks and engineering networks like power grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3074</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3074</id><created>2011-09-14</created><authors><author><keyname>Lastovetsky</keyname><forenames>Alexey</forenames></author><author><keyname>Reddy</keyname><forenames>Ravi</forenames></author><author><keyname>Rychkov</keyname><forenames>Vladimir</forenames></author><author><keyname>Clarke</keyname><forenames>David</forenames></author></authors><title>Design and implementation of self-adaptable parallel algorithms for
  scientific computing on highly heterogeneous HPC platforms</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional heterogeneous parallel algorithms, designed for heterogeneous
clusters of workstations, are based on the assumption that the absolute speed
of the processors does not depend on the size of the computational task. This
assumption proved inaccurate for modern and perspective highly heterogeneous
HPC platforms. New class of algorithms based on the functional performance
model (FPM), representing the speed of the processor by a function of problem
size, has been recently proposed. These algorithms cannot be however employed
in self-adaptable applications because of very high cost of construction of the
functional performance model. The paper presents a new class of parallel
algorithms for highly heterogeneous HPC platforms. Like traditional FPM-based
algorithms, these algorithms assume that the speed of the processors is
characterized by speed functions rather than speed constants. Unlike the
traditional algorithms, they do not assume the speed functions to be given.
Instead, they estimate the speed functions of the processors for different
problem sizes during their execution. These algorithms do not construct the
full speed function for each processor but rather build and use their partial
estimates sufficient for optimal distribution of computations with a given
accuracy. The low execution cost of distribution of computations between
heterogeneous processors in these algorithms make them suitable for employment
in self-adaptable applications. Experiments with parallel matrix multiplication
applications based on this approach are performed on local and global
heterogeneous computational clusters. The results show that the execution time
of optimal matrix distribution between processors is significantly less, by
orders of magnitude, than the total execution time of the optimized
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3075</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3075</id><created>2011-09-14</created><authors><author><keyname>Mohanty</keyname><forenames>Rakesh</forenames></author><author><keyname>Das</keyname><forenames>Manas</forenames></author><author><keyname>Prasanna</keyname><forenames>M. Lakshmi</forenames></author><author><keyname>Sudhashree</keyname></author></authors><title>Design and Performance Evaluation of A New Proposed Fittest Job First
  Dynamic Round Robin(FJFDRR) Scheduling Algorithm</title><categories>cs.OS</categories><comments>05 Pages, 12 Figures, International Journal of Computer Information
  Systems Vol. 2, No. 2, February 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have proposed a new variant of Round Robin scheduling
algorithm by executing the processes according to the new calculated Fit Factor
f and using the concept of dynamic time quantum. We have compared the
performance of our proposed Fittest Job First Dynamic Round Robin(FJFDRR)
algorithm with the Priority Based Static Round Robin(PBSRR) algorithm.
Experimental results show that our proposed algorithm performs better than
PBSRR in terms of reducing the number of context switches, average waiting time
and average turnaround time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3076</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3076</id><created>2011-09-14</created><authors><author><keyname>Behera</keyname><forenames>H. S.</forenames></author><author><keyname>Mohanty</keyname><forenames>Rakesh</forenames></author><author><keyname>Sahu</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Bhoi</keyname><forenames>Sourav Kumar</forenames></author></authors><title>Comparative performance analysis of multi dynamic time quantum Round
  Robin(MDTQRR) algorithm with arrival time</title><categories>cs.OS</categories><comments>10 pages, 18 Figures, Indian Journal of Computer Science and
  Engineering vol. 2 no. 2 April-May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CPU being considered a primary computer resource, its scheduling is central
to operating-system design. A thorough performance evaluation of various
scheduling algorithms manifests that Round Robin Algorithm is considered as
optimal in time shared environment because the static time is equally shared
among the processes. We have proposed an efficient technique in the process
scheduling algorithm by using dynamic time quantum in Round Robin. Our approach
is based on the calculation of time quantum twice in single round robin cycle.
Taking into consideration the arrival time, we implement the algorithm.
Experimental analysis shows better performance of this improved algorithm over
the Round Robin algorithm and the Shortest Remaining Burst Round Robin
algorithm. It minimizes the overall number of context switches, average waiting
time and average turn-around time. Consequently the throughput and CPU
utilization is better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3092</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3092</id><created>2011-09-14</created><updated>2012-05-28</updated><authors><author><keyname>Christofides</keyname><forenames>Demetres</forenames></author><author><keyname>Edwards</keyname><forenames>Katherine</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author></authors><title>A note on hitting maximum and maximal cliques with a stable set</title><categories>cs.DM math.CO</categories><comments>7 pages, two figures, accepted to J. Graph Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently proved that any graph satisfying $\omega &gt; \frac
23(\Delta+1)$ contains a stable set hitting every maximum clique. In this note
we prove that the same is true for graphs satisfying $\omega \geq \frac
23(\Delta+1)$ unless the graph is the strong product of $K_{\omega/2}$ and an
odd hole. We also provide a counterexample to a recent conjecture on the
existence of a stable set hitting every sufficiently large maximal clique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3094</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3094</id><created>2011-09-14</created><authors><author><keyname>Geiger</keyname><forenames>Martin Josef</forenames></author><author><keyname>Sevaux</keyname><forenames>Marc</forenames></author></authors><title>On the use of reference points for the biobjective Inventory Routing
  Problem</title><categories>cs.AI</categories><journal-ref>Proceedings of the 9th Metaheuristics International Conference MIC
  2011, July 25-28, 2011, Udine, Italy, Pages 141-149</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents a study on the biobjective inventory routing problem.
Contrary to most previous research, the problem is treated as a true
multi-objective optimization problem, with the goal of identifying
Pareto-optimal solutions. Due to the hardness of the problem at hand, a
reference point based optimization approach is presented and implemented into
an optimization and decision support system, which allows for the computation
of a true subset of the optimal outcomes. Experimental investigation involving
local search metaheuristics are conducted on benchmark data, and numerical
results are reported and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3095</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3095</id><created>2011-09-14</created><authors><author><keyname>Guo</keyname><forenames>Wangmei</forenames></author><author><keyname>Cai</keyname><forenames>Ning</forenames></author><author><keyname>Sun</keyname><forenames>Qifu Tyler</forenames></author></authors><title>Convolutional Network Coding Based on Matrix Power Series Representation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, convolutional network coding is formulated by means of matrix
power series representation of the local encoding kernel (LEK) matrices and
global encoding kernel (GEK) matrices to establish its theoretical fundamentals
for practical implementations. From the encoding perspective, the GEKs of a
convolutional network code (CNC) are shown to be uniquely determined by its LEK
matrix $K(z)$ if $K_0$, the constant coefficient matrix of $K(z)$, is
nilpotent. This will simplify the CNC design because a nilpotent $K_0$ suffices
to guarantee a unique set of GEKs. Besides, the relation between coding
topology and $K(z)$ is also discussed. From the decoding perspective, the main
theme is to justify that the first $L+1$ terms of the GEK matrix $F(z)$ at a
sink $r$ suffice to check whether the code is decodable at $r$ with delay $L$
and to start decoding if so. The concomitant decoding scheme avoids dealing
with $F(z)$, which may contain infinite terms, as a whole and hence reduces the
complexity of decodability check. It potentially makes CNCs applicable to
wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3102</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3102</id><created>2011-09-14</created><authors><author><keyname>Walk</keyname><forenames>Philipp</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>Approximation of L\&quot;owdin Orthogonalization to a Spectrally Efficient
  Orthogonal Overlapping PPM Design for UWB Impulse Radio</title><categories>cs.IT math.IT</categories><comments>33 pages, 11 figures. Accepted for publication 9 Sep 2011</comments><doi>10.1016/j.sigpro.2011.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the design of spectrally efficient time-limited
pulses for ultrawideband (UWB) systems using an overlapping pulse position
modulation scheme. For this we investigate an orthogonalization method, which
was developed in 1950 by Per-Olov L\&quot;owdin. Our objective is to obtain a set of
N orthogonal (L\&quot;owdin) pulses, which remain time-limited and spectrally
efficient for UWB systems, from a set of N equidistant translates of a
time-limited optimal spectral designed UWB pulse. We derive an approximate
L\&quot;owdin orthogonalization (ALO) by using circulant approximations for the Gram
matrix to obtain a practical filter implementation. We show that the centered
ALO and L\&quot;owdin pulses converge pointwise to the same Nyquist pulse as N tends
to infinity. The set of translates of the Nyquist pulse forms an orthonormal
basis or the shift-invariant space generated by the initial spectral optimal
pulse. The ALO transform provides a closed-form approximation of the L\&quot;owdin
transform, which can be implemented in an analog fashion without the need of
analog to digital conversions. Furthermore, we investigate the interplay
between the optimization and the orthogonalization procedure by using methods
from the theory of shift-invariant spaces. Finally we develop a connection
between our results and wavelet and frame theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3114</identifier>
 <datestamp>2012-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3114</id><created>2011-09-14</created><updated>2012-02-09</updated><authors><author><keyname>Chechik</keyname><forenames>Shiri</forenames></author></authors><title>Improved Distance Oracles and Spanners for Vertex-Labeled Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an undirected weighted graph G=(V,E) with |V|=n and |E|=m, where
each vertex v is assigned a label from a set L of \ell labels. We show how to
construct a compact distance oracle that can answer queries of the form: &quot;what
is the distance from v to the closest lambda-labeled node&quot; for a given node v
in V and label lambda in L.
  This problem was introduced by Hermelin, Levy, Weimann and Yuster [ICALP
2011] where they present several results for this problem. In the first result,
they show how to construct a vertex-label distance oracle of expected size
O(kn^{1+1/k}) with stretch (4k - 5) and query time O(k). In a second result,
they show how to reduce the size of the data structure to O(kn \ell^{1/k}) at
the expense of a huge stretch, the stretch of this construction grows
exponentially in k, (2^k-1). In the third result they present a dynamic
vertex-label distance oracle that is capable of handling label changes in a
sub-linear time. The stretch of this construction is also exponential in k, (2
3^{k-1}+1).
  We manage to significantly improve the stretch of their constructions,
reducing the dependence on k from exponential to polynomial (4k-5), without
requiring any tradeoff regarding any of the other variables.
  In addition, we introduce the notion of vertex-label spanners: subgraphs that
preserve distances between every node v and label lambda. We present an
efficient construction for vertex-label spanners with stretch-size tradeoff
close to optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3119</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3119</id><created>2011-09-14</created><authors><author><keyname>van Gemmeren</keyname><forenames>Peter</forenames></author><author><keyname>Malon</keyname><forenames>David</forenames></author></authors><title>Persistent Data Layout and Infrastructure for Efficient Selective
  Retrieval of Event Data in ATLAS</title><categories>physics.data-an cs.CE cs.DB hep-ex</categories><comments>Proceedings of the DPF-2011 Conference, Providence, RI, August 8-13,
  2011 8 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The ATLAS detector at CERN has completed its first full year of recording
collisions at 7 TeV, resulting in billions of events and petabytes of data. At
these scales, physicists must have the capability to read only the data of
interest to their analyses, with the importance of efficient selective access
increasing as data taking continues. ATLAS has developed a sophisticated
event-level metadata infrastructure and supporting I/O framework allowing event
selections by explicit specification, by back navigation, and by selection
queries to a TAG database via an integrated web interface. These systems and
their performance have been reported on elsewhere. The ultimate success of such
a system, however, depends significantly upon the efficiency of selective event
retrieval. Supporting such retrieval can be challenging, as ATLAS stores its
event data in column-wise orientation using ROOT trees for a number of reasons,
including compression considerations, histogramming use cases, and more. For
2011 data, ATLAS will utilize new capabilities in ROOT to tune the persistent
storage layout of event data, and to significantly speed up selective event
reading. The new persistent layout strategy and its implications for I/O
performance are described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3125</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3125</id><created>2011-09-14</created><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>The mathematical law of evolutionary information dynamics and an
  observer's evolution regularities</title><categories>cs.IT math.IT math.OC nlin.AO</categories><msc-class>92D15, 93E03, 93E35</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interactive stochastics, evaluated by an entropy functional (EF) of a
random field and informational process' path functional (IPF), allows us
modeling the evolutionary information processes and revealing regularities of
evolution dynamics. Conventional Shannon's information measure evaluates a
sequence of the process' static events for each information state and do not
reveal hidden dynamic connections between these events. The paper formulates
the mathematical forms of the information regularities, based on a minimax
variation principle (VP) for IPF, applied to the evolution's both random
microprocesses and dynamic macroprocesses. The paper shows that the VP single
form of the mathematical law leads to the following evolutionary regularities:
-creation of the order from stochastics through the evolutionary macrodynamics,
described by a gradient of dynamic potential, evolutionary speed and the
evolutionary conditions of a fitness and diversity; -the evolutionary hierarchy
with growing information values and potential adaptation; -the adaptive
self-controls and a self-organization with a mechanism of copying to a genetic
code. This law and the regularities determine unified functional informational
mechanisms of evolution dynamics. By introducing both objective and subjective
information observers, we consider the observers' information acquisition,
interactive cognitive evolution dynamics, and neurodynamics, based on the
EF-IPF approach. An evolution improvement consists of the subjective observer s
ability to attract and encode information whose value progressively increases.
The specific properties of a common information structure of evolution
processes are identifiable for each particular object-organism by collecting a
behavioral data from these organisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3126</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3126</id><created>2011-09-14</created><authors><author><keyname>Martyushev</keyname><forenames>Evgeniy</forenames></author></authors><title>A Non-Iterative Solution to the Four-Point Three-Views Pose Problem in
  Case of Collinear Cameras</title><categories>cs.CV</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a non-iterative solution to a particular case of the four-point
three-views pose problem when three camera centers are collinear. Using the
well-known Cayley representation of orthogonal matrices, we derive from the
epipolar constraints a system of three polynomial equations in three variables.
The eliminant of that system is a multiple of a 36th degree univariate
polynomial. The true (unique) solution to the problem can be expressed in terms
of one of real roots of that polynomial. Experiments on synthetic data confirm
that our method is robust enough even in case of planar configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3138</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3138</id><created>2011-09-14</created><authors><author><keyname>Mas</keyname><forenames>Massimiliano Dal</forenames></author></authors><title>Folksodriven Structure Network</title><categories>cs.IR</categories><comments>4 pages, 2 figures; for details see: http://www.maxdalmas.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays folksonomy is used as a system derived from user-generated
electronic tags or keywords that annotate and describe online content. But it
is not a classification system as an ontology. To consider it as a
classification system it would be necessary to share a representation of
contexts by all the users. This paper is proposing the use of folksonomies and
network theory to devise a new concept: a &quot;Folksodriven Structure Network&quot; to
represent folksonomies. This paper proposed and analyzed the network structure
of Folksodriven tags thought as folsksonomy tags suggestions for the user on a
dataset built on chosen websites. It is observed that the Folksodriven Network
has relative low path lengths checking it with classic networking measures
(clustering coefficient). Experiment result shows it can facilitate
serendipitous discovery of content among users. Neat examples and clear
formulas can show how a &quot;Folksodriven Structure Network&quot; can be used to tackle
ontology mapping challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3145</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3145</id><created>2011-09-14</created><authors><author><keyname>Shkolnik</keyname><forenames>Alexander</forenames></author><author><keyname>Tedrake</keyname><forenames>Russ</forenames></author></authors><title>Sample-Based Planning with Volumes in Configuration Space</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple sample-based planning method is presented which approximates
connected regions of free space with volumes in Configuration space instead of
points. The algorithm produces very sparse trees compared to point-based
planning approaches, yet it maintains probabilistic completeness guarantees.
The planner is shown to improve performance on a variety of planning problems,
by focusing sampling on more challenging regions of a planning problem,
including collision boundary areas such as narrow passages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3150</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3150</id><created>2011-09-14</created><authors><author><keyname>Sinha</keyname><forenames>Abhishek</forenames></author><author><keyname>Das</keyname><forenames>Swagatam</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>On Periodic Node Deployment in Wireless Sensor Networks: A Statistical
  Analysis</title><categories>cs.NI</categories><comments>Submitted for review in Journal of Network and Systems Management</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid progress made in the field of sensor technology, wireless
communication, and computer networks in recent past, led to the development of
wireless Ad-hoc sensor networks, consisting of small, low-cost sensors, which
can monitor wide and remote areas with precision and liveliness unseen to the
date without the intervention of a human operator. This work comes up with a
stochastic model for periodic sensor-deployment (in face of their limited
amount of battery-life) to maintain a minimal node-connectivity in wireless
sensor networks. The node deployment cannot be modeled by using results from
conventional continuous birth-death process, since new nodes are added to the
network in bursts, i.e. the birth process is not continuous in practical
situations. We analyze the periodic node deployment process using discrete
birth-continuous death process and obtain two important statistical measures of
the existing number of nodes in the network, namely the mean and variance. We
show that the above mentioned sequences of mean and variances always converge
to finite steady state values, thus ensuring the stability of the system. We
also develop a cost function for the process of periodic deployment of sensor
nodes and minimize it to find the optimal time ({\tau}) and optimum number of
re-deployment (q) for maintaining minimum connectivity in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3151</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3151</id><created>2011-09-14</created><authors><author><keyname>Kizilkale</keyname><forenames>Arman C.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Regulation, Volatility and Efficiency in Continuous-Time Markets</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the efficiency of markets with friction, particularly power
markets. We model the market as a dynamic system with $(d_t;\,t\geq 0)$ the
demand process and $(s_t;\,t\geq 0)$ the supply process. Using stochastic
differential equations to model the dynamics with friction, we investigate the
efficiency of the market under an integrated expected undiscounted cost
function solving the optimal control problem. Then, we extend the setup to a
game theoretic model where multiple suppliers and consumers interact
continuously by setting prices in a dynamic market with friction. We
investigate the equilibrium, and analyze the efficiency of the market under an
integrated expected social cost function. We provide an intriguing
efficiency-volatility no-free-lunch trade-off theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3160</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3160</id><created>2011-09-14</created><updated>2012-04-27</updated><authors><author><keyname>Katenka</keyname><forenames>Natallia</forenames></author><author><keyname>Kolaczyk</keyname><forenames>Eric D.</forenames></author></authors><title>Inference and Characterization of Multi-Attribute Networks with
  Application to Computational Biology</title><categories>stat.AP cs.SI physics.soc-ph q-bio.MN</categories><comments>Updated bibliography references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our work is motivated by and illustrated with application of association
networks in computational biology, specifically in the context of gene/protein
regulatory networks. Association networks represent systems of interacting
elements, where a link between two different elements indicates a sufficient
level of similarity between element attributes. While in reality relational
ties between elements can be expected to be based on similarity across multiple
attributes, the vast majority of work to date on association networks involves
ties defined with respect to only a single attribute. We propose an approach
for the inference of multi-attribute association networks from measurements on
continuous attribute variables, using canonical correlation and a
hypothesis-testing strategy. Within this context, we then study the impact of
partial information on multi-attribute network inference and characterization,
when only a subset of attributes is available. We consider in detail the case
of two attributes, wherein we examine through a combination of analytical and
numerical techniques the implications of the choice and number of node
attributes on the ability to detect network links and, more generally, to
estimate higher-level network summary statistics, such as node degree,
clustering coefficients, and measures of centrality. Illustration and
applications throughout the paper are developed using gene and protein
expression measurements on human cancer cell lines from the NCI-60 database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3180</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3180</id><created>2011-09-14</created><updated>2013-05-27</updated><authors><author><keyname>Lee</keyname><forenames>Choongbum</forenames></author><author><keyname>Loh</keyname><forenames>Po-Shen</forenames></author><author><keyname>Sudakov</keyname><forenames>Benny</forenames></author></authors><title>Bisections of graphs</title><categories>math.CO cs.DM</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bisection of a graph is a bipartition of its vertex set in which the number
of vertices in the two parts differ by at most 1, and its size is the number of
edges which go across the two parts. In this paper, motivated by several
questions and conjectures of Bollob\'as and Scott, we study maximum bisections
of graphs. First, we extend the classical Edwards bound on maximum cuts to
bisections. A simple corollary of our result implies that every graph on $n$
vertices and $m$ edges with no isolated vertices, and maximum degree at most
$n/3 + 1$, admits a bisection of size at least $m/2 + n/6$. Then using the
tools that we developed to extend Edwards's bound, we prove a judicious
bisection result which states that graphs with large minimum degree have a
bisection in which both parts span relatively few edges. A special case of this
general theorem answers a conjecture of Bollob\'as and Scott, and shows that
every graph on $n$ vertices and $m$ edges of minimum degree at least 2 admits a
bisection in which the number of edges in each part is at most $(1/3+o(1))m$.
We also present several other results on bisections of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3195</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3195</id><created>2011-09-14</created><updated>2012-03-24</updated><authors><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Dupuis</keyname><forenames>Frederic</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Efficient Quantum Polar Coding</title><categories>quant-ph cs.IT math.IT</categories><comments>v1: 15 pages, 4 figures. v2: 5+3 pages, 3 figures; argumentation
  simplified and improved</comments><journal-ref>Phys. Rev. Lett. 109, 050504 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.050504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar coding, introduced 2008 by Arikan, is the first (very) efficiently
encodable and decodable coding scheme whose information transmission rate
provably achieves the Shannon bound for classical discrete memoryless channels
in the asymptotic limit of large block sizes. Here we study the use of polar
codes for the transmission of quantum information. Focusing on the case of
qubit Pauli channels and qubit erasure channels, we use classical polar codes
to construct a coding scheme which, using some pre-shared entanglement,
asymptotically achieves a net transmission rate equal to the coherent
information using efficient encoding and decoding operations and code
construction. Furthermore, for channels with sufficiently low noise level, we
demonstrate that the rate of preshared entanglement required is zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3218</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3218</id><created>2011-09-14</created><updated>2011-09-18</updated><authors><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Colouring the Triangles Determined by a Point Set</title><categories>math.CO cs.CG</categories><journal-ref>J. Computational Geometry 3:86-101, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P be a set of n points in general position in the plane. We study the
chromatic number of the intersection graph of the open triangles determined by
P. It is known that this chromatic number is at least n^3/27+O(n^2), and if P
is in convex position, the answer is n^3/24+O(n^2). We prove that for arbitrary
P, the chromatic number is at most n^3/19.259+O(n^2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3227</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3227</id><created>2011-09-14</created><updated>2012-01-30</updated><authors><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Multiple Beamforming with Perfect Coding</title><categories>cs.IT math.IT</categories><comments>accepted to journal</comments><journal-ref>IEEE TCOM, Vol. 60, No. 6, Pages 1575-1586, Jun. 2012</journal-ref><doi>10.1109/TCOMM.2012.042712.110322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perfect Space-Time Block Codes (PSTBCs) achieve full diversity, full rate,
nonvanishing constant minimum determinant, uniform average transmitted energy
per antenna, and good shaping. However, the high decoding complexity is a
critical issue for practice. When the Channel State Information (CSI) is
available at both the transmitter and the receiver, Singular Value
Decomposition (SVD) is commonly applied for a Multiple-Input Multiple-Output
(MIMO) system to enhance the throughput or the performance. In this paper, two
novel techniques, Perfect Coded Multiple Beamforming (PCMB) and Bit-Interleaved
Coded Multiple Beamforming with Perfect Coding (BICMB-PC), are proposed,
employing both PSTBCs and SVD with and without channel coding, respectively.
With CSI at the transmitter (CSIT), the decoding complexity of PCMB is
substantially reduced compared to a MIMO system employing PSTBC, providing a
new prospect of CSIT. Especially, because of the special property of the
generation matrices, PCMB provides much lower decoding complexity than the
state-of-the-art SVD-based uncoded technique in dimensions 2 and 4. Similarly,
the decoding complexity of BICMB-PC is much lower than the state-of-the-art
SVD-based coded technique in these two dimensions, and the complexity gain is
greater than the uncoded case. Moreover, these aforementioned complexity
reductions are achieved with only negligible or modest loss in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3240</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3240</id><created>2011-09-14</created><authors><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Zhu</keyname><forenames>Yaojia</forenames></author><author><keyname>Rouquier</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Lane</keyname><forenames>Terran</forenames></author></authors><title>Active Learning for Node Classification in Assortative and
  Disassortative Networks</title><categories>cs.IT cs.LG cs.SI math.IT physics.soc-ph stat.ML</categories><comments>9 pages, 7 figures, KDD 2011: The 17th ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world networks, nodes have class labels, attributes, or
variables that affect the network's topology. If the topology of the network is
known but the labels of the nodes are hidden, we would like to select a small
subset of nodes such that, if we knew their labels, we could accurately predict
the labels of all the other nodes. We develop an active learning algorithm for
this problem which uses information-theoretic techniques to choose which nodes
to explore. We test our algorithm on networks from three different domains: a
social network, a network of English words that appear adjacently in a novel,
and a marine food web. Our algorithm makes no initial assumptions about how the
groups connect, and performs well even when faced with quite general types of
network structure. In particular, we do not assume that nodes of the same class
are more likely to be connected to each other---only that they connect to the
rest of the network in similar ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3248</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3248</id><created>2011-09-14</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author></authors><title>Reconstruction of sequential data with density models</title><categories>cs.LG stat.ML</categories><comments>30 pages, 9 figures. Original manuscript dated January 27, 2004 and
  not updated since. Current author's email address:
  mcarreira-perpinan@ucmerced.edu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the problem of reconstructing a sequence of multidimensional
real vectors where some of the data are missing. This problem contains
regression and mapping inversion as particular cases where the pattern of
missing data is independent of the sequence index. The problem is hard because
it involves possibly multivalued mappings at each vector in the sequence, where
the missing variables can take more than one value given the present variables;
and the set of missing variables can vary from one vector to the next. To solve
this problem, we propose an algorithm based on two redundancy assumptions:
vector redundancy (the data live in a low-dimensional manifold), so that the
present variables constrain the missing ones; and sequence redundancy (e.g.
continuity), so that consecutive vectors constrain each other. We capture the
low-dimensional nature of the data in a probabilistic way with a joint density
model, here the generative topographic mapping, which results in a Gaussian
mixture. Candidate reconstructions at each vector are obtained as all the modes
of the conditional distribution of missing variables given present variables.
The reconstructed sequence is obtained by minimising a global constraint, here
the sequence length, by dynamic programming. We present experimental results
for a toy problem and for inverse kinematics of a robot arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3256</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3256</id><created>2011-09-14</created><authors><author><keyname>Voets</keyname><forenames>Dean</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author></authors><title>Non-termination Analysis of Logic Programs with Integer arithmetics</title><categories>cs.PL cs.LO</categories><comments>15 pages, 2 figures, journal TPLP (special issue on the international
  conference of logic programming)</comments><msc-class>68Q25</msc-class><journal-ref>TPLP, 2011, volume 11, number 4-5, pages 521 --536</journal-ref><doi>10.1017/S1471068411000159</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past years, analyzers have been introduced to detect classes of
non-terminating queries for definite logic programs. Although these
non-termination analyzers have shown to be rather precise, their applicability
on real-life Prolog programs is limited because most Prolog programs use
non-logical features. As a first step towards the analysis of Prolog programs,
this paper presents a non-termination condition for Logic Programs containing
integer arithmetics. The analyzer is based on our non-termination analyzer
presented at ICLP 2009. The analysis starts from a class of queries and infers
a subclass of non-terminating ones. In a first phase, we ignore the outcome
(success or failure) of the arithmetic operations, assuming success of all
arithmetic calls. In a second phase, we characterize successful arithmetic
calls as a constraint problem, the solution of which determines the
non-terminating queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3272</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3272</id><created>2011-09-15</created><updated>2013-03-14</updated><authors><author><keyname>Han</keyname><forenames>Weijia</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author><author><keyname>Li</keyname><forenames>Zan</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>Qin</forenames></author></authors><title>On the Performance of Cooperative Spectrum Sensing under Quantization</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author since partial information
  is confidential</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio, the cooperative spectrum sensing (CSS) plays a key role
in determining the performance of secondary networks. However, there have not
been feasible approaches that can analytically calculate the performance of CSS
with regard to the multi-level quantization. In this paper, we not only show
the cooperative false alarm probability and cooperative detection probability
impacted by quantization, but also formulate them by two closed form
expressions. These two expressions enable the calculation of cooperative false
alarm probability and cooperative detection probability tractable efficiently,
and provide a feasible approach for optimization of sensing performance.
Additionally, to facilitate this calculation, we derive Normal approximation
for evaluating the sensing performance conveniently. Furthermore, two
optimization methods are proposed to achieve the high sensing performance under
quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3277</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3277</id><created>2011-09-15</created><updated>2012-03-27</updated><authors><author><keyname>G&#xfc;nther</keyname><forenames>Florian</forenames></author><author><keyname>Janoschek</keyname><forenames>Florian</forenames></author><author><keyname>Frijters</keyname><forenames>Stefan</forenames></author><author><keyname>Harting</keyname><forenames>Jens</forenames></author></authors><title>Lattice Boltzmann simulations of anisotropic particles at liquid
  interfaces</title><categories>cond-mat.soft cs.DC physics.flu-dyn</categories><comments>10 pages, 5 figures; ParCFD 2011 proceedings contribution</comments><doi>10.1016/j.compfluid.2012.03.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex colloidal fluids, such as emulsions stabilized by complex shaped
particles, play an important role in many industrial applications. However,
understanding their physics requires a study at sufficiently large length
scales while still resolving the microscopic structure of a large number of
particles and of the local hydrodynamics. Due to its high degree of locality,
the lattice Boltzmann method, when combined with a molecular dynamics solver
and parallelized on modern supercomputers, provides a tool that allows such
studies. Still, running simulations on hundreds of thousands of cores is not
trivial. We report on our practical experiences when employing large fractions
of an IBM Blue Gene/P system for our simulations. Then, we extend our model for
spherical particles in multicomponent flows to anisotropic ellipsoidal objects
rendering the shape of e.g. clay particles. The model is applied to a number of
test cases including the adsorption of single particles at fluid interfaces and
the formation and stabilization of Pickering emulsions or bijels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3311</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3311</id><created>2011-09-15</created><authors><author><keyname>Bercher</keyname><forenames>J. -F.</forenames></author></authors><title>Escort entropies and divergences and related canonical distribution</title><categories>math-ph cond-mat.stat-mech cs.IT math.IT math.MP</categories><journal-ref>Physics Letters A, vol. 375, no. 33, p. 2969-2973, 2011</journal-ref><doi>10.1016/j.physleta.2011.06.057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss two families of two-parameter entropies and divergences, derived
from the standard R\'enyi and Tsallis entropies and divergences. These
divergences and entropies are found as divergences or entropies of escort
distributions. Exploiting the nonnegativity of the divergences, we derive the
expression of the canonical distribution associated to the new entropies and a
observable given as an escort-mean value. We show that this canonical
distribution extends, and smoothly connects, the results obtained in
nonextensive thermodynamics for the standard and generalized mean value
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3313</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3313</id><created>2011-09-15</created><authors><author><keyname>Geiger</keyname><forenames>Martin Josef</forenames></author><author><keyname>Sevaux</keyname><forenames>Marc</forenames></author><author><keyname>Voss</keyname><forenames>Stefan</forenames></author></authors><title>Neigborhood Selection in Variable Neighborhood Search</title><categories>cs.AI</categories><comments>ISBN 978-88-900984-3-7</comments><journal-ref>Proceedings of the 9th Metaheuristics International Conference MIC
  2011, July 25-28, 2011, Udine, Italy, Pages 571-573</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable neighborhood search (VNS) is a metaheuristic for solving
optimization problems based on a simple principle: systematic changes of
neighborhoods within the search, both in the descent to local minima and in the
escape from the valleys which contain them. Designing these neighborhoods and
applying them in a meaningful fashion is not an easy task. Moreover, an
appropriate order in which they are applied must be determined. In this paper
we attempt to investigate this issue. Assume that we are given an optimization
problem that is intended to be solved by applying the VNS scheme, how many and
which types of neighborhoods should be investigated and what could be
appropriate selection criteria to apply these neighborhoods. More specifically,
does it pay to &quot;look ahead&quot; (see, e.g., in the context of VNS and GRASP) when
attempting to switch from one neighborhood to another?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3316</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3316</id><created>2011-09-15</created><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author><author><keyname>Verbeek</keyname><forenames>Kevin</forenames></author></authors><title>Angle-Restricted Steiner Arborescences for Flow Map Layout</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new variant of the geometric Steiner arborescence problem,
motivated by the layout of flow maps. Flow maps show the movement of objects
between places. They reduce visual clutter by bundling lines smoothly and
avoiding self-intersections. To capture these properties, our angle-restricted
Steiner arborescences, or flux trees, connect several targets to a source with
a tree of minimal length whose arcs obey a certain restriction on the angle
they form with the source.
  We study the properties of optimal flux trees and show that they are planar
and consist of logarithmic spirals and straight lines. Flux trees have the
shallow-light property. We show that computing optimal flux trees is NP-hard.
Hence we consider a variant of flux trees which uses only logarithmic spirals.
Spiral trees approximate flux trees within a factor depending on the angle
restriction. Computing optimal spiral trees remains NP-hard, but we present an
efficient 2-approximation, which can be extended to avoid &quot;positive monotone&quot;
obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3317</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3317</id><created>2011-09-15</created><authors><author><keyname>Mollah</keyname><forenames>Ayatullah Faruk</forenames></author><author><keyname>Majumder</keyname><forenames>Nabamita</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author></authors><title>Design of an Optical Character Recognition System for Camera-based
  Handheld Devices</title><categories>cs.CV</categories><journal-ref>Int'l J. of Computer Science Issues, Vol. 8, Issue 4, pp. 283-289,
  July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a complete Optical Character Recognition (OCR) system for
camera captured image/graphics embedded textual documents for handheld devices.
At first, text regions are extracted and skew corrected. Then, these regions
are binarized and segmented into lines and characters. Characters are passed
into the recognition module. Experimenting with a set of 100 business card
images, captured by cell phone camera, we have achieved a maximum recognition
accuracy of 92.74%. Compared to Tesseract, an open source desktop-based
powerful OCR engine, present recognition accuracy is worth contributing.
Moreover, the developed technique is computationally efficient and consumes low
memory so as to be applicable on handheld devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3318</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3318</id><created>2011-09-15</created><updated>2013-04-22</updated><authors><author><keyname>Tomozei</keyname><forenames>Dan-Cristian</forenames></author><author><keyname>Massouli&#xe9;</keyname><forenames>Laurent</forenames></author></authors><title>Distributed User Profiling via Spectral Methods</title><categories>cs.LG</categories><comments>31 pages</comments><msc-class>60B20</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User profiling is a useful primitive for constructing personalised services,
such as content recommendation. In the present paper we investigate the
feasibility of user profiling in a distributed setting, with no central
authority and only local information exchanges between users. We compute a
profile vector for each user (i.e., a low-dimensional vector that characterises
her taste) via spectral transformation of observed user-produced ratings for
items. Our two main contributions follow: i) We consider a low-rank
probabilistic model of user taste. More specifically, we consider that users
and items are partitioned in a constant number of classes, such that users and
items within the same class are statistically identical. We prove that without
prior knowledge of the compositions of the classes, based solely on few random
observed ratings (namely $O(N\log N)$ such ratings for $N$ users), we can
predict user preference with high probability for unrated items by running a
local vote among users with similar profile vectors. In addition, we provide
empirical evaluations characterising the way in which spectral profiling
performance depends on the dimension of the profile space. Such evaluations are
performed on a data set of real user ratings provided by Netflix. ii) We
develop distributed algorithms which provably achieve an embedding of users
into a low-dimensional space, based on spectral transformation. These involve
simple message passing among users, and provably converge to the desired
embedding. Our method essentially relies on a novel combination of gossiping
and the algorithm proposed by Oja and Karhunen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3320</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3320</id><created>2011-09-15</created><authors><author><keyname>Dinh</keyname><forenames>Quoc Tran</forenames></author><author><keyname>Gumussoy</keyname><forenames>Suat</forenames></author><author><keyname>Michiels</keyname><forenames>Wim</forenames></author><author><keyname>Diehl</keyname><forenames>Moritz</forenames></author></authors><title>Combining Convex-Concave Decompositions and Linearization Approaches for
  solving BMIs, with application to Static Output Feedback</title><categories>math.OC cs.SY</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel optimization method is proposed to minimize a convex function subject
to bilinear matrix inequality (BMI) constraints. The key idea is to decompose
the bilinear mapping as a difference between two positive semidefinite convex
mappings. At each iteration of the algorithm the concave part is linearized,
leading to a convex subproblem.Applications to various output feedback
controller synthesis problems are presented. In these applications the
subproblem in each iteration step can be turned into a convex optimization
problem with linear matrix inequality (LMI) constraints. The performance of the
algorithm has been benchmarked on the data from COMPleib library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3322</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3322</id><created>2011-09-15</created><updated>2012-11-26</updated><authors><author><keyname>Sietsma</keyname><forenames>Floor</forenames></author><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Common Knowledge in Email Exchanges</title><categories>cs.LO</categories><comments>34 pages. To appear in ACM Transactions on Computational Logic</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a framework in which a group of agents communicates by means of
emails, with the possibility of replies, forwards and blind carbon copies
(BCC). We study the epistemic consequences of such email exchanges by
introducing an appropriate epistemic language and semantics. This allows us to
find out what agents learn from the emails they receive and to determine when a
group of agents acquires common knowledge of the fact that an email was sent.
We also show that in our framework from the epistemic point of view the BCC
feature of emails cannot be simulated using messages without BCC recipients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3367</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3367</id><created>2011-09-15</created><updated>2012-04-20</updated><authors><author><keyname>Nicolas</keyname><forenames>Francois</forenames></author><author><keyname>B&#xf6;cker</keyname><forenames>Sebastian</forenames></author></authors><title>Various complexity results for computational mass spectrometry problems</title><categories>cs.CC</categories><comments>10 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Define Minimum Soapy Union (MinSU) as the following optimization problem:
given a $k$-tuple $(X_1, X_2,..., X_k)$ of finite integer sets, find a
$k$-tuple $(t_1, t_2,..., t_k)$ of integers that minimizes the cardinality of
$(X_1 + t_1) \cup (X_2 + t_2) \cup ... \cup (X_n + t_k)$. We show that MinSU is
NP-complete, APX-hard, and polynomial for fixed $k$. MinSU appears naturally in
the context of protein shotgun sequencing: Here, the protein is cleaved into
short and overlapping peptides, which are then analyzed by tandem mass
spectrometry. To improve the quality of such spectra, one then asks for the
mass of the unknown prefix (the shift) of the spectrum, such that the resulting
shifted spectra show a maximum agreement. For real-world data the problem is
even more complicated than our definition of MinSU; but our intractability
results clearly indicate that it is unlikely to find a polynomial time
algorithm for shotgun protein sequencing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3370</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3370</id><created>2011-09-15</created><authors><author><keyname>Constable</keyname><forenames>Robert</forenames></author></authors><title>Effectively Nonblocking Consensus Procedures Can Execute Forever - a
  Constructive Version of FLP</title><categories>cs.LO</categories><comments>17 pages, 6 figures, uses pstricks; http://hdl.handle.net/1813/11512</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fischer-Lynch-Paterson theorem (FLP) says that it is impossible for
processes in an asynchronous distributed system to achieve consensus on a
binary value when a single process can fail; it is a widely cited theoretical
result about network computing. All proofs that I know depend essentially on
classical (nonconstructive) logic, although they use the hypothetical
construction of a nonterminating execution as a main lemma.
  FLP is also a guide for protocol designers, and in that role there is a
connection to an important property of consensus procedures, namely that they
should not block, i.e. reach a global state in which no process can decide.
  A deterministic fault-tolerant consensus protocol is effectively nonblocking
if from any reachable global state we can find an execution path that decides.
In this article we effectively construct a nonterminating execution of any such
protocol. That is, given any effectively nonblocking protocol P and a natural
number n, we show how to compute the n-th step of an infinitely indecisive
computation of P. From this fully constructive result, the classical FLP
follows as a corollary as well as a stronger classical result, called here
Strong FLP. Moreover, the construction focuses attention on the important role
of nonblocking in protocol design.
  An interesting consequence of the constructive proof is that we can, in
principle, build an undefeatable attacker for a consensus protocol that is
provably correct, indeed because it is provably correct. We can do this in
practice on certain kinds of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3381</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3381</id><created>2011-09-15</created><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Li</keyname><forenames>Baiyu</forenames></author></authors><title>Syntactic Complexity of Star-Free Languages</title><categories>cs.FL</categories><comments>17 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic complexity of a regular language is the cardinality of its
syntactic semigroup. The syntactic complexity of a subclass of regular
languages is the maximal syntactic complexity of languages in that subclass,
taken as a function of the state complexity of these languages. We study the
syntactic complexity of star-free regular languages, that is, languages that
can be constructed from finite languages using union, complement and
concatenation. We find tight upper bounds on the syntactic complexity of
languages accepted by monotonic and partially monotonic automata. We introduce
&quot;nearly monotonic&quot; automata, which accept star-free languages, and find a tight
upper bound on the syntactic complexity of languages accepted by such automata.
We conjecture that this bound is also an upper bound on the syntactic
complexity of star-free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3385</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3385</id><created>2011-09-15</created><authors><author><keyname>Bercher</keyname><forenames>J. -F.</forenames></author></authors><title>Source coding with escort distributions and Renyi entropy bounds</title><categories>math-ph cond-mat.stat-mech cs.IT math.IT math.MP</categories><journal-ref>Physics Letters A, vol. 373, no. 36, p. 3235-3238, 2009</journal-ref><doi>10.1016/j.physleta.2009.07.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the interest of escort distributions and R\'enyi entropy in the
context of source coding. We first recall a source coding theorem by Campbell
relating a generalized measure of length to the R\'enyi-Tsallis entropy. We
show that the associated optimal codes can be obtained using considerations on
escort-distributions. We propose a new family of measure of length involving
escort-distributions and we show that these generalized lengths are also
bounded below by the R\'enyi entropy. Furthermore, we obtain that the standard
Shannon codes lengths are optimum for the new generalized lengths measures,
whatever the entropic index. Finally, we show that there exists in this setting
an interplay between standard and escort distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3390</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3390</id><created>2011-09-15</created><authors><author><keyname>Gebauer</keyname><forenames>Heidi</forenames></author><author><keyname>Gundert</keyname><forenames>Anna</forenames></author><author><keyname>Moser</keyname><forenames>Robin A.</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author></authors><title>Not All Saturated 3-Forests Are Tight</title><categories>math.CO cs.DM</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic statement in graph theory is that every inclusion-maximal forest is
connected, i.e. a tree. Using a definiton for higher dimensional forests by
Graham and Lovasz and the connectivity-related notion of tightness for
hypergraphs introduced by Arocha, Bracho and Neumann-Lara in, we provide an
example of a saturated, i.e. inclusion-maximal 3-forest that is not tight. This
resolves an open problem posed by Strausz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3401</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3401</id><created>2011-09-15</created><updated>2012-10-29</updated><authors><author><keyname>Hellerstein</keyname><forenames>Lisa</forenames></author><author><keyname>&#xd6;zkan</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author><author><keyname>Sellie</keyname><forenames>Linda</forenames></author></authors><title>Max-Throughput for (Conservative) k-of-n Testing</title><categories>cs.DS cs.DC</categories><comments>17 pages. An extended abstract of this paper appeared in the
  Proceedings of the 22nd International Symposium on Algorithms and Computation
  (ISAAC 2011)</comments><acm-class>F.2.2; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a variant of k-of-n testing that we call conservative k-of-n
testing. We present a polynomial-time, combinatorial algorithm for the problem
of maximizing throughput of conservative k-of-n testing, in a parallel setting.
This extends previous work of Kodialam and Condon et al., who presented
combinatorial algorithms for parallel pipelined filter ordering, which is the
special case where k=1 (or k = n). We also consider the problem of maximizing
throughput for standard k-of-n testing, and show how to obtain a
polynomial-time algorithm based on the ellipsoid method using previous
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3411</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3411</id><created>2011-09-15</created><authors><author><keyname>Hartikainen</keyname><forenames>Markus</forenames></author><author><keyname>Ojalehto</keyname><forenames>Vesa</forenames></author></authors><title>Demonstrating the Applicability of PAINT to Computationally Expensive
  Real-life Multiobjective Optimization</title><categories>cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the applicability of a new PAINT method to speed up iterations
of interactive methods in multiobjective optimization. As our test case, we
solve a computationally expensive non-linear, five-objective problem of
designing and operating a wastewater treatment plant. The PAINT method
interpolates between a given set of Pareto optimal outcomes and constructs a
computationally inexpensive mixed integer linear surrogate problem for the
original problem. We develop an IND-NIMBUS(R) PAINT module to combine the
interactive NIMBUS method and the PAINT method and to find a preferred solution
to the original problem. With the PAINT method, the solution process with the
NIMBUS method take a comparatively short time even though the original problem
is computationally expensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3418</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3418</id><created>2011-09-15</created><authors><author><keyname>Furer</keyname><forenames>Martin</forenames></author><author><keyname>Yu</keyname><forenames>Huiwen</forenames></author></authors><title>Packing-Based Approximation Algorithm for the k-Set Cover Problem</title><categories>cs.DS</categories><comments>26 pages, 5 figures</comments><journal-ref>Proceedings 22nd International Symposium on Algorithms and
  Computation (ISAAC 2011). Springer-Verlag LNCS 7074:484-493</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a packing-based approximation algorithm for the $k$-Set Cover
problem. We introduce a new local search-based $k$-set packing heuristic, and
call it Restricted $k$-Set Packing. We analyze its tight approximation ratio
via a complicated combinatorial argument. Equipped with the Restricted $k$-Set
Packing algorithm, our $k$-Set Cover algorithm is composed of the $k$-Set
Packing heuristic \cite{schrijver} for $k\geq 7$, Restricted $k$-Set Packing
for $k=6,5,4$ and the semi-local $(2,1)$-improvement \cite{furer} for 3-Set
Cover. We show that our algorithm obtains a tight approximation ratio of
$H_k-0.6402+\Theta(\frac{1}{k})$, where $H_k$ is the $k$-th harmonic number.
For small $k$, our results are 1.8667 for $k=6$, 1.7333 for $k=5$ and 1.5208
for $k=4$. Our algorithm improves the currently best approximation ratio for
the $k$-Set Cover problem of any $k\geq 4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3428</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3428</id><created>2011-09-15</created><authors><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Wolff</keyname><forenames>Spencer</forenames></author><author><keyname>Van Godtsenhoven</keyname><forenames>Karen</forenames></author></authors><title>One, None and One Hundred Thousand Profiles: Re-imagining the
  Pirandellian Identity Dilemma in the Era of Online Social Networks</title><categories>cs.SI cs.CY</categories><comments>An abridged version of this paper will be presented/performed under
  the title &quot;Identity dilemmas on Facebook&quot; at the Symposium on the Dynamics of
  the Internet and Society &quot;A Decade in Internet Time&quot; to be held Wednesday 21
  - Saturday 24 September 2011 at the Oxford Internet Institute, University of
  Oxford, UK</comments><journal-ref>One, None, One Hundred Thousand Profiles. Alberto Pepe, Spencer
  Wolff, Karen Van Godtsenhoven. First Monday. Volume 17, Number 4 - 2 April
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uno, Nessuno, Centomila (&quot;One, No One and One Hundred Thousand&quot;) is a classic
novel by Italian playwright Luigi Pirandello. Published in 1925, it recounts
the tragedy of Vitangelo Moscarda, a man who struggles to reclaim a coherent
and unitary identity for himself in the face of an inherently social and
multi-faceted world. What would Moscarda identity tragedy look like today? In
this article we transplant Moscarda's identity play from its offline setting to
the contemporary arena of social media and online social networks. With
reference to established theories on identity construction, performance, and
self-presentation, we re-imagine how Moscarda would go about defending the
integrity of his selfhood in the face of the discountenancing influences of the
online world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3437</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3437</id><created>2011-09-15</created><updated>2012-03-24</updated><authors><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Cheung</keyname><forenames>William K.</forenames></author><author><keyname>Liu</keyname><forenames>Jiming</forenames></author></authors><title>Learning Topic Models by Belief Propagation</title><categories>cs.LG</categories><comments>14 pages, 17 figures</comments><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Volume 33, Number 5, Pages 1121-1134, 2013</journal-ref><doi>10.1109/TPAMI.2012.185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model
for probabilistic topic modeling, which attracts worldwide interests and
touches on many important applications in text mining, computer vision and
computational biology. This paper represents LDA as a factor graph within the
Markov random field (MRF) framework, which enables the classic loopy belief
propagation (BP) algorithm for approximate inference and parameter estimation.
Although two commonly-used approximate inference methods, such as variational
Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in
learning LDA, the proposed BP is competitive in both speed and accuracy as
validated by encouraging experimental results on four large-scale document data
sets. Furthermore, the BP algorithm has the potential to become a generic
learning scheme for variants of LDA-based topic models. To this end, we show
how to learn two typical variants of LDA-based topic models, such as
author-topic models (ATM) and relational topic models (RTM), using BP based on
the factor graph representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3444</identifier>
 <datestamp>2011-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3444</id><created>2011-09-15</created><authors><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author><author><keyname>Cliff</keyname><forenames>Dave</forenames></author><author><keyname>Calinescu</keyname><forenames>Radu</forenames></author><author><keyname>Keen</keyname><forenames>Justin</forenames></author><author><keyname>Kelly</keyname><forenames>Tim</forenames></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames></author><author><keyname>McDermid</keyname><forenames>John</forenames></author><author><keyname>Paige</keyname><forenames>Richard</forenames></author></authors><title>Large-scale Complex IT Systems</title><categories>cs.SE cs.CY</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the issues around the construction of large-scale complex
systems which are built as 'systems of systems' and suggests that there are
fundamental reasons, derived from the inherent complexity in these systems, why
our current software engineering methods and techniques cannot be scaled up to
cope with the engineering challenges of constructing such systems. It then goes
on to propose a research and education agenda for software engineering that
identifies the major challenges and issues in the development of large-scale
complex, software-intensive systems. Central to this is the notion that we
cannot separate software from the socio-technical environment in which it is
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3475</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3475</id><created>2011-09-15</created><updated>2012-04-13</updated><authors><author><keyname>Horak</keyname><forenames>Peter</forenames></author><author><keyname>AlBdaiwi</keyname><forenames>Bader F.</forenames></author></authors><title>Diameter Perfect Lee Codes</title><categories>cs.IT math.CO math.IT</categories><journal-ref>IEEE Transactions on Information Theory, Vol 58, No 8, pp.
  5490--5499, August 2012</journal-ref><doi>10.1109/TIT.2012.2196257</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lee codes have been intensively studied for more than 40 years. Interest in
these codes has been triggered by the Golomb-Welch conjecture on the existence
of the perfect error-correcting Lee codes. In this paper we deal with the
existence and enumeration of diameter perfect Lee codes. As main results we
determine all $q$ for which there exists a linear diameter-4 perfect Lee code
of word length $n$ over $Z_{q},$ and prove that for each $n\geq 3$ there are
uncountable many diameter-4 perfect Lee codes of word length $n$ over $Z.$ This
is in a strict contrast with perfect error-correcting Lee codes of word length
$n$ over $Z\,$\ as there is a unique such code for $n=3,$ and its is
conjectured that this is always the case when $2n+1$ is a prime. We produce
diameter perfect Lee codes by an algebraic construction that is based on a
group homomorphism. This will allow us to design an efficient algorithm for
their decoding. We hope that this construction will turn out to be useful far
beyond the scope of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3488</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3488</id><created>2011-09-15</created><updated>2012-01-02</updated><authors><author><keyname>Clark</keyname><forenames>Andrew</forenames></author><author><keyname>Kenyon</keyname><forenames>Jeff</forenames></author></authors><title>Using MOEAs To Outperform Stock Benchmarks In The Presence of Typical
  Investment Constraints</title><categories>q-fin.PM cs.CE cs.NE stat.AP stat.CO</categories><comments>21 pages, Index Terms - multi-objective evolutionary algorithms
  (MOEA), mean-variance optimization, financial constraints, multi-period MOEAs
  Updated version of paper. Will appear in Journal of Investing in 2012</comments><msc-class>68Q05</msc-class><acm-class>F.1.1; G.1.6; J.1; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Portfolio managers are typically constrained by turnover limits, minimum and
maximum stock positions, cardinality, a target market capitalization and
sometimes the need to hew to a style (such as growth or value). In addition,
portfolio managers often use multifactor stock models to choose stocks based
upon their respective fundamental data.
  We use multiobjective evolutionary algorithms (MOEAs) to satisfy the above
real-world constraints. The portfolios generated consistently outperform
typical performance benchmarks and have statistically significant asset
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3510</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3510</id><created>2011-09-15</created><updated>2015-03-17</updated><authors><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Diversity Analysis of Bit-Interleaved Coded Multiple Beamforming with
  Orthogonal Frequency Division Multiplexing</title><categories>cs.IT math.IT</categories><comments>accepted to journal</comments><journal-ref>IEEE TCOM, Vol. 61, No. 9, Pages 3794-3805, Sep. 2013</journal-ref><doi>10.1109/TCOMM.2013.080113.130247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For broadband wireless communication systems, Orthogonal Frequency Division
Multiplexing (OFDM) has been combined with Multi-Input Multi-Output (MIMO)
techniques. Bit-Interleaved Coded Multiple Beamforming (BICMB) can achieve both
spatial diversity and spatial multiplexing for flat fading MIMO channels. For
frequency selective fading MIMO channels, BICMB with OFDM (BICMB-OFDM) can be
applied to achieve both spatial diversity and multipath diversity, making it an
important technique. However, analyzing the diversity of BICMB-OFDM is a
challenging problem. In this paper, the diversity analysis of BICMB-OFDM is
carried out. First, the maximum achievable diversity is derived and a full
diversity condition RcSL &lt;= 1 is proved, where Rc, S, and L are the code rate,
the number of parallel steams transmitted at each subcarrier, and the number of
channel taps, respectively. Then, the performance degradation due to the
correlation among subcarriers is investigated. Finally, the subcarrier grouping
technique is employed to combat the performance degradation and provide
multi-user compatibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3524</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3524</id><created>2011-09-16</created><authors><author><keyname>Layton</keyname><forenames>Simon K</forenames></author><author><keyname>Krishnan</keyname><forenames>Anush</forenames></author><author><keyname>Barba</keyname><forenames>Lorena A.</forenames></author></authors><title>cuIBM -- A GPU-accelerated Immersed Boundary Method</title><categories>cs.CE</categories><comments>ParCFD 2011, C&amp;F special issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A projection-based immersed boundary method is dominated by sparse linear
algebra routines. Using the open-source Cusp library, we observe a speedup
(with respect to a single CPU core) which reflects the constraints of a
bandwidth-dominated problem on the GPU. Nevertheless, GPUs offer the capacity
to solve large problems on commodity hardware. This work includes validation
and a convergence study of the GPU-accelerated IBM, and various optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3532</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3532</id><created>2011-09-16</created><authors><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Trappenberg</keyname><forenames>Thomas</forenames></author></authors><title>A Characterization of the Combined Effects of Overlap and Imbalance on
  the SVM Classifier</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate that two common problems in Machine
Learning---imbalanced and overlapping data distributions---do not have
independent effects on the performance of SVM classifiers. This result is
notable since it shows that a model of either of these factors must account for
the presence of the other. Our study of the relationship between these problems
has lead to the discovery of a previously unreported form of &quot;covert&quot;
overfitting which is resilient to commonly used empirical regularization
techniques. We demonstrate the existance of this covert phenomenon through
several methods based around the parametric regularization of trained SVMs. Our
findings in this area suggest a possible approach to quantifying overlap in
real world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3544</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3544</id><created>2011-09-16</created><updated>2012-02-28</updated><authors><author><keyname>Hellwig</keyname><forenames>Matthias</forenames></author><author><keyname>Souza</keyname><forenames>Alexander</forenames></author></authors><title>Approximation Algorithms for Variable-Sized and Generalized Bin Covering</title><categories>cs.DS</categories><comments>Improved Approximation Guarantee for Generalized Bin Covering and
  added AFPTAS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Generalized Bin Covering (GBC) problem: We are given $m$ bin
types, where each bin of type $i$ has profit $p_i$ and demand $d_i$.
Furthermore, there are $n$ items, where item $j$ has size $s_j$. A bin of type
$i$ is covered if the set of items assigned to it has total size at least the
demand $d_i$. In that case, the profit of $p_i$ is earned and the objective is
to maximize the total profit. To the best of our knowledge, only the cases $p_i
= d_i = 1$ (Bin Covering) and $p_i = d_i$ (Variable-Sized Bin Covering (VSBC))
have been treated before. We study two models of bin supply: In the unit supply
model, we have exactly one bin of each type, i.\,e., we have individual bins.
By contrast, in the infinite supply model, we have arbitrarily many bins of
each type. Clearly, the unit supply model is a generalization of the infinite
supply model. To the best of our knowledge the unit supply model has not been
studied yet.
  Our results for the unit supply model hold not only asymptotically, but for
all instances. This contrasts most of the previous work on \prob{Bin Covering}.
We prove that there is a combinatorial 5-approximation algorithm for GBC with
unit supply, which has running time $\bigO{nm\sqrt{m+n}}$. Furthermore, for
VSBC we show that the natural and fast Next Fit Decreasing ($\NFD$) algorithm
is a 9/4-approximation in the unit supply model. The bound is tight for the
algorithm and close to being best-possible. We show that there is an AFPTAS for
VSBC in the \emph{infinite} supply model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3547</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3547</id><created>2011-09-16</created><authors><author><keyname>Els&#xe4;sser</keyname><forenames>Robert</forenames></author><author><keyname>Ogierman</keyname><forenames>Adrian</forenames></author></authors><title>Awareness and Movement vs. the Spread of Epidemics - Analyzing a Dynamic
  Model for Urban Social/Technological Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the spread of epidemics in technological and social networks. How
do people react? Does awareness and cautious behavior help? We analyze these
questions and present a dynamic model to describe the movement of individuals
and/or their mobile devices in a certain (idealistic) urban environment.
Furthermore, our model incorporates the fact that different locations can
accommodate a different number of people (possibly with their mobile devices),
who may pass the infection to each other. We obtain two main results. First, we
prove that w.r.t. our model at least a small part of the system will remain
uninfected even if no countermeasures are taken. The second result shows that
with certain counteractions in use, which only influence the individuals'
behavior, a prevalent epidemic can be avoided. The results explain possible
courses of a disease, and point out why cost-efficient countermeasures may
reduce the number of total infections from a high percentage of the population
to a negligible fraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3555</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3555</id><created>2011-09-16</created><authors><author><keyname>Pagano</keyname><forenames>Francesco</forenames></author><author><keyname>Pagano</keyname><forenames>Davide</forenames></author></authors><title>Using In-Memory Encrypted Databases on the Cloud</title><categories>cs.CR cs.DB cs.DC</categories><comments>8 pages, 8 figures</comments><acm-class>D.4.6; C.2.4; E.3</acm-class><journal-ref>2011 1st International Workshop on Securing Ser vices on the Cloud
  IWSSC 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Storing data in the cloud poses a number of privacy issues. A way to handle
them is supporting data replication and distribution on the cloud via a local,
centrally synchronized storage. In this paper we propose to use an in-memory
RDBMS with row-level data encryption for granting and revoking access rights to
distributed data. This type of solution is rarely adopted in conventional
RDBMSs because it requires several complex steps. In this paper we focus on
implementation and benchmarking of a test system, which shows that our simple
yet effective solution overcomes most of the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3556</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3556</id><created>2011-09-16</created><authors><author><keyname>Parlangeli</keyname><forenames>Gianfranco</forenames></author><author><keyname>Notarstefano</keyname><forenames>Giuseppe</forenames></author></authors><title>On the reachability and observability of path and cycle graphs</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the reachability and observability properties of
a network system, running a Laplacian based average consensus algorithm, when
the communication graph is a path or a cycle. More in detail, we provide
necessary and sufficient conditions, based on simple algebraic rules from
number theory, to characterize all and only the nodes from which the network
system is reachable (respectively observable). Interesting immediate
corollaries of our results are: (i) a path graph is reachable (observable) from
any single node if and only if the number of nodes of the graph is a power of
two, $n=2^i, i\in \natural$, and (ii) a cycle is reachable (observable) from
any pair of nodes if and only if $n$ is a prime number. For any set of control
(observation) nodes, we provide a closed form expression for the (unreachable)
unobservable eigenvalues and for the eigenvectors of the (unreachable)
unobservable subsystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3561</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3561</id><created>2011-09-16</created><authors><author><keyname>Bernard</keyname><forenames>Thibault</forenames></author><author><keyname>Bui</keyname><forenames>Alain</forenames></author><author><keyname>Sohier</keyname><forenames>Devan</forenames></author></authors><title>Universal adaptive self-stabilizing traversal scheme: random walk and
  reloading wave</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate random walk based token circulation in dynamic
environments subject to failures. We describe hypotheses on the dynamic
environment that allow random walks to meet the important property that the
token visits any node infinitely often. The randomness of this scheme allows it
to work on any topology, and require no adaptation after a topological change,
which is a desirable property for applications to dynamic systems. For random
walks to be a traversal scheme and to answer the concurrence problem, one needs
to guarantee that exactly one token circulates in the system. In the presence
of transient failures, configurations with multiple tokens or with no token can
occur. The meeting property of random walks solves the cases with multiple
tokens. The reloading wave mechanism we propose, together with timeouts, allows
to detect and solve cases with no token. This traversal scheme is
self-stabilizing, and universal, meaning that it needs no assumption on the
system topology. We describe conditions on the dynamicity (with a local
detection criterion) under which the algorithm is tolerant to dynamic
reconfigurations. We conclude by a study on the time between two visits of the
token to a node, which we use to tune the parameters of the reloading wave
mechanism according to some system characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3563</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3563</id><created>2011-09-16</created><updated>2013-08-01</updated><authors><author><keyname>Zhukov</keyname><forenames>Victor P.</forenames></author></authors><title>Verification, Validation and Testing of Kinetic Mechanisms of Hydrogen
  Combustion in Fluid Dynamic Computations</title><categories>cs.CE physics.flu-dyn</categories><comments>The alternate reference of this paper: V.P. Zhukov, &quot;Verification,
  Validation and Testing of Kinetic Models of Hydrogen Combustion in Fluid
  Dynamic Computations&quot;, Paper ID35 at 4th European Conference for Aerospace
  Sciences, Saint Petersburg, Russia, 4--8 July, 2011</comments><journal-ref>ISRN Mechanical Engineering, vol. 2012, Article ID 475607, 11
  pages, 2012</journal-ref><doi>10.5402/2012/475607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A one-step, a two-step, an abridged, a skeletal and four detailed kinetic
schemes of hydrogen oxidation have been tested. A new skeletal kinetic scheme
of hydrogen oxidation has been developed. The CFD calculations were carried out
using ANSYS CFX software. Ignition delay times and speeds of flames were
derived from the computational results. The computational data obtained using
ANSYS CFX and CHEMKIN, and experimental data were compared. The precision,
reliability, and range of validity of the kinetic schemes in CFD simulations
were estimated. The impact of kinetic scheme on the results of computations was
discussed. The relationship between grid spacing, timestep, accuracy, and
computational cost were analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3569</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3569</id><created>2011-09-16</created><authors><author><keyname>Cacace</keyname><forenames>Simone</forenames></author><author><keyname>Cristiani</keyname><forenames>Emiliano</forenames></author><author><keyname>Falcone</keyname><forenames>Maurizio</forenames></author></authors><title>Numerical approximation of Nash equilibria for a class of
  non-cooperative differential games</title><categories>math.NA cs.SY math.AP math.OC</categories><msc-class>49N90, 35F21, 65N20, 65N12</msc-class><journal-ref>L. Petrosjan, V. Mazalov (eds.), Game Theory and Applications,
  Vol. 16, Chap. 4, 45-58, Nova Publishers, New York, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a numerical method to obtain an approximation of
Nash equilibria for multi-player non-cooperative games with a special
structure. We consider the infinite horizon problem in a case which leads to a
system of Hamilton-Jacobi equations. The numerical method is based on the
Dynamic Programming Principle for every equation and on a global fixed point
iteration. We present the numerical solutions of some two-player games in one
and two dimensions. The paper has an experimental nature, but some features and
properties of the approximation scheme are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3577</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3577</id><created>2011-09-16</created><updated>2012-11-06</updated><authors><author><keyname>Cacace</keyname><forenames>Simone</forenames></author><author><keyname>Cristiani</keyname><forenames>Emiliano</forenames></author><author><keyname>Falcone</keyname><forenames>Maurizio</forenames></author><author><keyname>Picarelli</keyname><forenames>Athena</forenames></author></authors><title>A patchy Dynamic Programming scheme for a class of
  Hamilton-Jacobi-Bellman equations</title><categories>math.NA cs.SY math.OC</categories><msc-class>65N55, 49L20</msc-class><journal-ref>SIAM J. Sci. Comput., Vol. 34 (2012), No. 5, pp. A2625-A2649</journal-ref><doi>10.1137/110841576</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new algorithm for the solution of
Hamilton-Jacobi-Bellman equations related to optimal control problems. The key
idea is to divide the domain of computation into subdomains which are shaped by
the optimal dynamics of the underlying control problem. This can result in a
rather complex geometrical subdivision, but it has the advantage that every
subdomain is invariant with respect to the optimal dynamics, and then the
solution can be computed independently in each subdomain. The features of this
dynamics-dependent domain decomposition can be exploited to speed up the
computation and for an efficient parallelization, since the classical
transmission conditions at the boundaries of the subdomains can be avoided. For
their properties, the subdomains are patches in the sense introduced by Ancona
and Bressan [ESAIM Control Optim. Calc. Var., 4 (1999), pp. 445-471]. Several
examples in two and three dimensions illustrate the properties of the new
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3606</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3606</id><created>2011-09-16</created><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Krehbiel</keyname><forenames>Sara</forenames></author><author><keyname>Piliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author></authors><title>Near Optimality in Covering and Packing Games by Exposing Global
  Information</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering and packing problems can be modeled as games to encapsulate
interesting social and engineering settings. These games have a high Price of
Anarchy in their natural formulation. However, existing research applicable to
specific instances of these games has only been able to prove fast convergence
to arbitrary equilibria. This paper studies general classes of covering and
packing games with learning dynamics models that incorporate a central
authority who broadcasts weak, socially beneficial signals to agents that
otherwise only use local information in their decision-making. Rather than
illustrating convergence to an arbitrary equilibrium that may have very high
social cost, we show that these systems quickly achieve near-optimal
performance.
  In particular, we show that in the public service advertising model, reaching
a small constant fraction of the agents is enough to bring the system to a
state within a log n factor of optimal in a broad class of set cover and set
packing games or a constant factor of optimal in the special cases of vertex
cover and maximum independent set, circumventing social inefficiency of bad
local equilibria that could arise without a central authority. We extend these
results to the learn-then-decide model, in which agents use any of a broad
class of learning algorithms to decide in a given round whether to behave
according to locally optimal behavior or the behavior prescribed by the
broadcast signal. The new techniques we use for analyzing these games could be
of broader interest for analyzing more general classic optimization problems in
a distributed fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3617</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3617</id><created>2011-09-16</created><authors><author><keyname>Kornienko</keyname><forenames>S.</forenames></author><author><keyname>Kornienko</keyname><forenames>S.</forenames></author></authors><title>IR-based Communication and Perception in Microrobotic Swarms</title><categories>cs.RO</categories><comments>IROS 2005, WS on Task-oriented Mobile Actuator and Sensor Networks,
  Edmonton, Canada. Extended version appeared on the 7th Workshop on Collective
  \&amp; Swarm Robotics, 18 November, University of Stuttgart, Germany, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider development of IR-based communication and perception
mechanisms for real microrobotic systems. It is demonstrated that a specific
combination of hardware and software elements provides capabilities for
navigation, objects recognition, directional and unidirectional communication.
We discuss open issues and their resolution based on the experiments in the
swarm of microrobots &quot;Jasmine&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3627</identifier>
 <datestamp>2012-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3627</id><created>2011-09-16</created><updated>2011-12-06</updated><authors><author><keyname>Lipowski</keyname><forenames>Adam</forenames></author><author><keyname>Lipowska</keyname><forenames>Dorota</forenames></author></authors><title>Roulette-wheel selection via stochastic acceptance</title><categories>cs.NE cond-mat.stat-mech cs.CC physics.comp-ph</categories><comments>4 pages, Physica A, accepted</comments><journal-ref>Physica A 391 (2012) pp. 2193-2196</journal-ref><doi>10.1016/j.physa.2011.12.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roulette-wheel selection is a frequently used method in genetic and
evolutionary algorithms or in modeling of complex networks. Existing routines
select one of N individuals using search algorithms of O(N) or O(log(N))
complexity. We present a simple roulette-wheel selection algorithm, which
typically has O(1) complexity and is based on stochastic acceptance instead of
searching. We also discuss a hybrid version, which might be suitable for highly
heterogeneous weight distributions, found, for example, in some models of
complex networks. With minor modifications, the algorithm might also be used
for sampling with fitness cut-off at a certain value or for sampling without
replacement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3637</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3637</id><created>2011-09-16</created><authors><author><keyname>Guerreiro</keyname><forenames>Rui F. C.</forenames></author><author><keyname>Aguiar</keyname><forenames>Pedro M. Q.</forenames></author></authors><title>Connectivity-Enforcing Hough Transform for the Robust Extraction of Line
  Segments</title><categories>cs.CV</categories><comments>Submitted for publication</comments><doi>10.1109/TIP.2012.2202673</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global voting schemes based on the Hough transform (HT) have been widely used
to robustly detect lines in images. However, since the votes do not take line
connectivity into account, these methods do not deal well with cluttered
images. In opposition, the so-called local methods enforce connectivity but
lack robustness to deal with challenging situations that occur in many
realistic scenarios, e.g., when line segments cross or when long segments are
corrupted. In this paper, we address the critical limitations of the HT as a
line segment extractor by incorporating connectivity in the voting process.
This is done by only accounting for the contributions of edge points lying in
increasingly larger neighborhoods and whose position and directional content
agree with potential line segments. As a result, our method, which we call
STRAIGHT (Segment exTRAction by connectivity-enforcInG HT), extracts the
longest connected segments in each location of the image, thus also integrating
into the HT voting process the usually separate step of individual segment
extraction. The usage of the Hough space mapping and a corresponding
hierarchical implementation make our approach computationally feasible. We
present experiments that illustrate, with synthetic and real images, how
STRAIGHT succeeds in extracting complete segments in several situations where
current methods fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3639</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3639</id><created>2011-09-16</created><updated>2011-12-24</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Weinstein</keyname><forenames>Amit</forenames></author></authors><title>Local Correction of Juntas</title><categories>cs.CC cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Boolean function f over n variables is said to be q-locally correctable if,
given a black-box access to a function g which is &quot;close&quot; to an isomorphism
f_sigma of f, we can compute f_sigma(x) for any x in Z_2^n with good
probability using q queries to g.
  We observe that any k-junta, that is, any function which depends only on k of
its input variables, is O(2^k)-locally correctable. Moreover, we show that
there are examples where this is essentially best possible, and locally
correcting some k-juntas requires a number of queries which is exponential in
k. These examples, however, are far from being typical, and indeed we prove
that for almost every k-junta, O(k log k) queries suffice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3649</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3649</id><created>2011-09-16</created><updated>2012-03-21</updated><authors><author><keyname>Davenport</keyname><forenames>Mark A.</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Compressive Sensing of Analog Signals Using Discrete Prolate Spheroidal
  Sequences</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) has recently emerged as a framework for efficiently
capturing signals that are sparse or compressible in an appropriate basis.
While often motivated as an alternative to Nyquist-rate sampling, there remains
a gap between the discrete, finite-dimensional CS framework and the problem of
acquiring a continuous-time signal. In this paper, we attempt to bridge this
gap by exploiting the Discrete Prolate Spheroidal Sequences (DPSS's), a
collection of functions that trace back to the seminal work by Slepian, Landau,
and Pollack on the effects of time-limiting and bandlimiting operations. DPSS's
form a highly efficient basis for sampled bandlimited functions; by modulating
and merging DPSS bases, we obtain a dictionary that offers high-quality sparse
approximations for most sampled multiband signals. This multiband modulated
DPSS dictionary can be readily incorporated into the CS framework. We provide
theoretical guarantees and practical insight into the use of this dictionary
for recovery of sampled multiband signals from compressive measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3650</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3650</id><created>2011-09-16</created><authors><author><keyname>Agrawal</keyname><forenames>Rohan</forenames></author></authors><title>Bi-Objective Community Detection (BOCD) in Networks using Genetic
  Algorithm</title><categories>cs.SI cs.AI cs.NE physics.soc-ph</categories><comments>11 pages, 3 Figures, 3 Tables. arXiv admin note: substantial text
  overlap with arXiv:0906.0612</comments><doi>10.1007/978-3-642-22606-9_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of research effort has been put into community detection from all
corners of academic interest such as physics, mathematics and computer science.
In this paper I have proposed a Bi-Objective Genetic Algorithm for community
detection which maximizes modularity and community score. Then the results
obtained for both benchmark and real life data sets are compared with other
algorithms using the modularity and MNI performance metrics. The results show
that the BOCD algorithm is capable of successfully detecting community
structure in both real life and synthetic datasets, as well as improving upon
the performance of previous techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3651</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3651</id><created>2011-09-16</created><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Optimization, Randomized Approximability, and Boolean Constraint
  Satisfaction Problems</title><categories>cs.CC</categories><comments>A4, 11pt, 9 pages. This is an extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a unified treatment to optimization problems that can be expressed in
the form of nonnegative-real-weighted Boolean constraint satisfaction problems.
Creignou, Khanna, Sudan, Trevisan, and Williamson studied the complexity of
approximating their optimal solutions whose optimality is measured by the sums
of outcomes of constraints. To explore a wider range of optimization constraint
satisfaction problems, following an early work of Marchetti-Spaccamela and
Romano, we study the case where the optimality is measured by products of
constraints' outcomes. We completely classify those problems into three
categories: PO problems, NPO-hard problems, and intermediate problems that lie
between the former two categories. To prove this trichotomy theorem, we analyze
characteristics of nonnegative-real-weighted constraints using a variant of the
notion of T-constructibility developed earlier for complex-weighted counting
constraint satisfaction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3656</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3656</id><created>2011-09-16</created><updated>2012-10-30</updated><authors><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Kim</keyname><forenames>Myung Sub</forenames></author></authors><title>Computing the Hermite Form of a Matrix of Ore Polynomials</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let R=F[D;sigma,delta] be the ring of Ore polynomials over a field (or skew
field) F, where sigma is a automorphism of F and delta is a sigma-derivation.
Given a an m by n matrix A over R, we show how to compute the Hermite form H of
A and a unimodular matrix U such that UA=H. The algorithm requires a polynomial
number of operations in F in terms of both the dimensions m and n, and the
degree of the entries in A. When F=k(z) for some field k, it also requires time
polynomial in the degree in z, and if k is the rational numbers Q, it requires
time polynomial in the bit length of the coefficients as well. Explicit
analyses are provided for the complexity, in particular for the important cases
of differential and shift polynomials over Q(z). To accomplish our algorithm,
we apply the Dieudonne determinant and quasideterminant theory for Ore
polynomial rings to get explicit bounds on the degrees and sizes of entries in
H and U.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3685</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3685</id><created>2011-09-16</created><authors><author><keyname>Doberkat</keyname><forenames>Ernst-Erich</forenames></author></authors><title>Towards a Coalgebraic Interpretation of Propositional Dynamic Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interpretation of propositional dynamic logic (PDL) through Kripke models
requires the relations constituting the interpreting Kripke model to closely
observe the syntax of the modal operators. This poses a significant challenge
for an interpretation of PDL through stochastic Kripke models, because the
programs' operations do not always have a natural counterpart in the set of
stochastic relations. We use rewrite rules for building up an interpretation of
PDL. It is shown that each program corresponds to an essentially unique
irreducible tree, which in turn is assigned a predicate lifting, serving as the
program's interpretation. The paper establishes and studies this
interpretation. It discusses the expressivity of probabilistic models for PDL
and relates properties like logical and behavioral equivalence or bisimilarity
to the corresponding properties of a Kripke model for a closely related
non-dynamic logic of the Hennessy-Milner type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3687</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3687</id><created>2011-09-16</created><updated>2012-03-16</updated><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author><author><keyname>Mamane</keyname><forenames>Lionel</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Dependencies in Formal Mathematics: Applications and Extraction for Coq
  and Mizar</title><categories>cs.DL cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two methods for extracting detailed formal dependencies from the Coq and
Mizar system are presented and compared. The methods are used for dependency
extraction from two large mathematical repositories: the Coq Repository at
Nijmegen and the Mizar Mathematical Library. Several applications of the
detailed dependency analysis are described and proposed. Motivated by the
different applications, we discuss the various kinds of dependencies that we
are interested in,and the suitability of various dependency extraction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3688</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3688</id><created>2011-09-16</created><authors><author><keyname>Salehi</keyname><forenames>Saba</forenames></author><author><keyname>Cabibihan</keyname><forenames>John-John</forenames></author><author><keyname>Ge</keyname><forenames>Shuzhi Sam</forenames></author></authors><title>Artificial Skin Ridges Enhance Local Tactile Shape Discrimination</title><categories>physics.med-ph cs.RO physics.ins-det</categories><comments>10 figures</comments><journal-ref>Salehi, S.; Cabibihan, J.-J.; Ge, S.S. Artificial Skin Ridges
  Enhance Local Tactile Shape Discrimination. Sensors 2011, 11, 8626-8642</journal-ref><doi>10.3390/s110908626</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  One of the fundamental requirements for an artificial hand to successfully
grasp and manipulate an object is to be able to distinguish different objects'
shapes and, more specifically, the objects' surface curvatures. In this study,
we investigate the possibility of enhancing the curvature detection of embedded
tactile sensors by proposing a ridged fingertip structure, simulating human
fingerprints. In addition, a curvature detection approach based on machine
learning methods is proposed to provide the embedded sensors with the ability
to discriminate the surface curvature of different objects. For this purpose, a
set of experiments were carried out to collect tactile signals from a 2 \times
2 tactile sensor array, then the signals were processed and used for learning
algorithms. To achieve the best possible performance for our machine learning
approach, three different learning algorithms of Na\&quot;ive Bayes (NB), Artificial
Neural Networks (ANN), and Support Vector Machines (SVM) were implemented and
compared for various parameters. Finally, the most accurate method was selected
to evaluate the proposed skin structure in recognition of three different
curvatures. The results showed an accuracy rate of 97.5% in surface curvature
discrimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3700</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3700</id><created>2011-09-16</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames><affiliation>UNM</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Osswald</keyname><forenames>Christophe</forenames><affiliation>E3I2</affiliation></author></authors><title>Contradiction measures and specificity degrees of basic belief
  assignments</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Information Fusion, Chicago : United
  States (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the theory of belief functions, many measures of uncertainty have been
introduced. However, it is not always easy to understand what these measures
really try to represent. In this paper, we re-interpret some measures of
uncertainty in the theory of belief functions. We present some interests and
drawbacks of the existing measures. On these observations, we introduce a
measure of contradiction. Therefore, we present some degrees of non-specificity
and Bayesianity of a mass. We propose a degree of specificity based on the
distance between a mass and its most specific associated mass. We also show how
to use the degree of specificity to measure the specificity of a fusion rule.
Illustrations on simple examples are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3701</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3701</id><created>2011-09-16</created><updated>2011-12-09</updated><authors><author><keyname>Jamieson</keyname><forenames>Kevin G.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author></authors><title>Active Ranking using Pairwise Comparisons</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>17 pages, an extended version of our NIPS 2011 paper. The new version
  revises the argument of the robust section and slightly modifies the result
  there to give it more impact</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the problem of ranking a collection of objects using
pairwise comparisons (rankings of two objects). In general, the ranking of $n$
objects can be identified by standard sorting methods using $n log_2 n$
pairwise comparisons. We are interested in natural situations in which
relationships among the objects may allow for ranking using far fewer pairwise
comparisons. Specifically, we assume that the objects can be embedded into a
$d$-dimensional Euclidean space and that the rankings reflect their relative
distances from a common reference point in $R^d$. We show that under this
assumption the number of possible rankings grows like $n^{2d}$ and demonstrate
an algorithm that can identify a randomly selected ranking using just slightly
more than $d log n$ adaptively selected pairwise comparisons, on average. If
instead the comparisons are chosen at random, then almost all pairwise
comparisons must be made in order to identify any ranking. In addition, we
propose a robust, error-tolerant algorithm that only requires that the pairwise
comparisons are probably correct. Experimental studies with synthetic and real
datasets support the conclusions of our theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3702</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3702</id><created>2011-09-16</created><authors><author><keyname>Birbaumer</keyname><forenames>Mirko</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Agent-Based Modeling of Intracellular Transport</title><categories>q-bio.SC cs.MA nlin.PS</categories><journal-ref>European Physical Journal B vol. 82, no. 3-4 (2011) pp. 245-255</journal-ref><doi>10.1140/epjb/e2011-20283-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an agent-based model of the motion and pattern formation of
vesicles. These intracellular particles can be found in four different modes of
(undirected and directed) motion and can fuse with other vesicles. While the
size of vesicles follows a log-normal distribution that changes over time due
to fusion processes, their spatial distribution gives rise to distinct
patterns. Their occurrence depends on the concentration of proteins which are
synthesized based on the transcriptional activities of some genes. Hence,
differences in these spatio-temporal vesicle patterns allow indirect
conclusions about the (unknown) impact of these genes.
  By means of agent-based computer simulations we are able to reproduce such
patterns on real temporal and spatial scales. Our modeling approach is based on
Brownian agents with an internal degree of freedom, $\theta$, that represents
the different modes of motion. Conditions inside the cell are modeled by an
effective potential that differs for agents dependent on their value $\theta$.
Agent's motion in this effective potential is modeled by an overdampted
Langevin equation, changes of $\theta$ are modeled as stochastic transitions
with values obtained from experiments, and fusion events are modeled as
space-dependent stochastic transitions. Our results for the spatio-temporal
vesicle patterns can be used for a statistical comparison with experiments. We
also derive hypotheses of how the silencing of some genes may affect the
intracellular transport, and point to generalizations of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3714</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3714</id><created>2011-09-16</created><updated>2012-09-25</updated><authors><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>High-dimensional regression with noisy and missing data: Provable
  guarantees with nonconvexity</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1018 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1018</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 3, 1637-1664</journal-ref><doi>10.1214/12-AOS1018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the standard formulations of prediction problems involve
fully-observed and noiseless data drawn in an i.i.d. manner, many applications
involve noisy and/or missing data, possibly involving dependence, as well. We
study these issues in the context of high-dimensional sparse linear regression,
and propose novel estimators for the cases of noisy, missing and/or dependent
data. Many standard approaches to noisy or missing data, such as those using
the EM algorithm, lead to optimization problems that are inherently nonconvex,
and it is difficult to establish theoretical guarantees on practical
algorithms. While our approach also involves optimizing nonconvex programs, we
are able to both analyze the statistical error associated with any global
optimum, and more surprisingly, to prove that a simple algorithm based on
projected gradient descent will converge in polynomial time to a small
neighborhood of the set of all global minimizers. On the statistical side, we
provide nonasymptotic bounds that hold with high probability for the cases of
noisy, missing and/or dependent data. On the computational side, we prove that
under the same types of conditions required for statistical consistency, the
projected gradient descent algorithm is guaranteed to converge at a geometric
rate to a near-global minimizer. We illustrate these theoretical predictions
with simulations, showing close agreement with the predicted scalings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3737</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3737</id><created>2011-09-16</created><authors><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Bazzani</keyname><forenames>Loris</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Learning where to Attend with Deep Architectures for Image Tracking</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss an attentional model for simultaneous object tracking and
recognition that is driven by gaze data. Motivated by theories of perception,
the model consists of two interacting pathways: identity and control, intended
to mirror the what and where pathways in neuroscience models. The identity
pathway models object appearance and performs classification using deep
(factored)-Restricted Boltzmann Machines. At each point in time the
observations consist of foveated images, with decaying resolution toward the
periphery of the gaze. The control pathway models the location, orientation,
scale and speed of the attended object. The posterior distribution of these
states is estimated with particle filtering. Deeper in the control pathway, we
encounter an attentional mechanism that learns to select gazes so as to
minimize tracking uncertainty. Unlike in our previous work, we introduce gaze
selection strategies which operate in the presence of partial information and
on a continuous action space. We show that a straightforward extension of the
existing approach to the partial information setting results in poor
performance, and we propose an alternative method based on modeling the reward
surface as a Gaussian Process. This approach gives good performance in the
presence of partial information and allows us to expand the action space from a
small, discrete set of fixation points to a continuous domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3739</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3739</id><created>2011-09-16</created><updated>2012-04-26</updated><authors><author><keyname>Buluc</keyname><forenames>Aydin</forenames></author><author><keyname>Gilbert</keyname><forenames>John</forenames></author></authors><title>Parallel Sparse Matrix-Matrix Multiplication and Indexing:
  Implementation and Experiments</title><categories>cs.DC cs.MS cs.NA cs.PF</categories><msc-class>05C50, 05C85, 65F50, 68W10</msc-class><journal-ref>SIAM J. Sci. Comput., 34(4), 170 - 191, 2012</journal-ref><doi>10.1137/110848244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key
primitive for many high performance graph algorithms as well as for some linear
solvers, such as algebraic multigrid. Here we show that SpGEMM also yields
efficient algorithms for general sparse-matrix indexing in distributed memory,
provided that the underlying SpGEMM implementation is sufficiently flexible and
scalable. We demonstrate that our parallel SpGEMM methods, which use
two-dimensional block data distributions with serial hypersparse kernels, are
indeed highly flexible, scalable, and memory-efficient in the general case.
This algorithm is the first to yield increasing speedup on an unbounded number
of processors; our experiments show scaling up to thousands of processors in a
variety of test scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3745</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3745</id><created>2011-09-16</created><updated>2014-09-21</updated><authors><author><keyname>Sen</keyname><forenames>Abhijit</forenames></author><author><keyname>Ahalpara</keyname><forenames>Dilip P.</forenames></author><author><keyname>Thyagaraja</keyname><forenames>Anantanarayanan</forenames></author><author><keyname>Krishnaswami</keyname><forenames>Govind S.</forenames></author></authors><title>A KdV-like advection-dispersion equation with some remarkable properties</title><categories>nlin.PS cs.NE math.AP physics.flu-dyn</categories><comments>15 pages, 4 figures, corrected sign typo in KdV Lagrangian above
  equation 31</comments><journal-ref>Communications in Nonlinear Science and Numerical Simulation 17
  (2012), pp. 4115-4124</journal-ref><doi>10.1016/j.cnsns.2012.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a new non-linear PDE, u_t + (2 u_xx/u) u_x = epsilon u_xxx,
invariant under scaling of dependent variable and referred to here as SIdV. It
is one of the simplest such translation and space-time reflection-symmetric
first order advection-dispersion equations. This PDE (with dispersion
coefficient unity) was discovered in a genetic programming search for equations
sharing the KdV solitary wave solution. It provides a bridge between non-linear
advection, diffusion and dispersion. Special cases include the mKdV and linear
dispersive equations. We identify two conservation laws, though initial
investigations indicate that SIdV does not follow from a polynomial Lagrangian
of the KdV sort. Nevertheless, it possesses solitary and periodic travelling
waves. Moreover, numerical simulations reveal recurrence properties usually
associated with integrable systems. KdV and SIdV are the simplest in an
infinite dimensional family of equations sharing the KdV solitary wave. SIdV
and its generalizations may serve as a testing ground for numerical and
analytical techniques and be a rich source for further explorations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3753</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3753</id><created>2011-09-17</created><authors><author><keyname>Frackiewicz</keyname><forenames>Piotr</forenames></author></authors><title>Quantum repeated games revisited</title><categories>quant-ph cs.GT</categories><doi>10.1088/1751-8113/45/8/085307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scheme for playing quantum repeated 2x2 games based on the
Marinatto and Weber's approach to quantum games. As a potential application, we
study twice repeated Prisoner's Dilemma game. We show that results not
available in classical game can be obtained when the game is played in the
quantum way. Before we present our idea, we comment on the previous scheme of
playing quantum repeated games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3765</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3765</id><created>2011-09-17</created><authors><author><keyname>Kornienko</keyname><forenames>S.</forenames></author><author><keyname>Kornienko</keyname><forenames>O.</forenames></author></authors><title>New Principles of Coordination in Large-scale Micro- and
  Molecular-Robotic Groups</title><categories>cs.RO</categories><journal-ref>IARP - IEEE/RAS - EURON Joint Workshop on MICRO &amp; NANO ROBOTICS,
  Paris, 23 - 24 October, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro- and molecular-robotic systems act as large-scale swarms. Capabilities
of sensing, communication and information processing are very limited on these
scales. This short position paper describes a swarm-based minimalistic
approach, which can be applied for coordinating collective behavior in such
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3767</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3767</id><created>2011-09-17</created><authors><author><keyname>Ahmad</keyname><forenames>Othman</forenames></author></authors><title>Generalised Object Detection and Semantic Analysis: Casino Example using
  Matlab</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matlab version 7.1 had been used to detect playing cards on a Casino table
and the suits and ranks of these cards had been identified. The process gives
an example of an application of computer vision to a problem where rectangular
objects are to be detected and the information content of the objects are
extracted out. In the case of playing cards, it is the suit and rank of each
card. The image processing system is done in two passes. Pass 1 detects
rectangular shapes and template matched with a template of the left and right
edges of the cards. Pass 2 extracts the suit and rank of the cards by matching
the top left portion of the card that contains both rank and suit information,
with stored templates of ranks and suits of the playing cards using a series of
if-then statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3772</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3772</id><created>2011-09-17</created><updated>2011-10-11</updated><authors><author><keyname>Bako</keyname><forenames>Laurent</forenames></author><author><keyname>Chen</keyname><forenames>Dulin</forenames></author><author><keyname>Lecoeuche</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>A numerical solution to the minimum-time control problem for linear
  discrete-time systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum-time control problem consists in finding a control policy that
will drive a given dynamic system from a given initial state to a given target
state (or a set of states) as quickly as possible. This is a well-known
challenging problem in optimal control theory for which closed-form solutions
exist only for a few systems of small dimensions. This paper presents a very
generic solution to the minimum-time problem for arbitrary discrete-time linear
systems. It is a numerical solution based on sparse optimization, that is the
minimization of the number of nonzero elements in the state sequence over a
fixed control horizon. We consider both single input and multiple inputs
systems. An important observation is that, contrary to the continuous-time
case, the minimum-time control for discrete-time systems is not necessarily
entirely bang-bang.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3781</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3781</id><created>2011-09-17</created><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Liu</keyname><forenames>Xiangdong</forenames></author></authors><title>Distributed Robust Control of Linear Multi-Agent Systems with Parameter
  Uncertainties</title><categories>cs.SY math.OC</categories><comments>17 pages, 3 figures. Submitted to International Journal of Robust and
  Nonlinear Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the distributed robust control problems of uncertain
linear multi-agent systems with undirected communication topologies. It is
assumed that the agents have identical nominal dynamics while subject to
different norm-bounded parameter uncertainties, leading to weakly heterogeneous
multi-agent systems. Distributed controllers are designed for both continuous-
and discrete-time multi-agent systems, based on the relative states of
neighboring agents and a subset of absolute states of the agents. It is shown
for both the continuous- and discrete-time cases that the distributed robust
control problems under such controllers in the sense of quadratic stability are
equivalent to the $H_\infty$ control problems of a set of decoupled linear
systems having the same dimensions as a single agent. A two-step algorithm is
presented to construct the distributed controller for the continuous-time case,
which does not involve any conservatism and meanwhile decouples the feedback
gain design from the communication topology. Furthermore, a sufficient
existence condition in terms of linear matrix inequalities is derived for the
distributed discrete-time controller. Finally, the distributed robust
$H_\infty$ control problems of uncertain linear multi-agent systems subject to
external disturbances are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3782</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3782</id><created>2011-09-17</created><updated>2012-04-30</updated><authors><author><keyname>Mohr</keyname><forenames>Daniel P.</forenames><affiliation>Institute of Mathematics and Computer Applications, Department of Aerospace Engineering, Universit&#xe4;t der Bundeswehr M&#xfc;nchen, Neubiberg, Germany</affiliation></author><author><keyname>Stein</keyname><forenames>Ina</forenames><affiliation>Institute of Mathematics and Computer Applications, Department of Aerospace Engineering, Universit&#xe4;t der Bundeswehr M&#xfc;nchen, Neubiberg, Germany</affiliation></author><author><keyname>Matzies</keyname><forenames>Thomas</forenames><affiliation>Institute of Lightweight Structures, Department of Aerospace Engineering, Universit&#xe4;t der Bundeswehr M&#xfc;nchen, Neubiberg, Germany</affiliation></author><author><keyname>Knapek</keyname><forenames>Christina A.</forenames><affiliation>Max-Planck-Institut f&#xfc;r extraterrestrische Physik, Garching, Germany</affiliation></author></authors><title>Robust Topology Optimization of Truss with regard to Volume</title><categories>math.OC cs.SY</categories><msc-class>74P05, 74S99, 74B05, 74K10, 90C05, 46N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common problem in the optimization of structures is the handling of
uncertainties in the parameters. If the parameters appear in the constraints,
the uncertainties can lead to an infinite number of constraints. Usually the
constraints have to be approximated by finite expressions to generate a
computable problem. Here, using the example of the topology optimization of a
truss, a method is proposed to deal with such uncertainties by using robust
optimization techniques, leading to an approach without the necessity of any
approximation. With adequately chosen load cases, the final expression is
equivalent to the multiple load case. Simple numerical examples of typical
problems illustrate the application of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3791</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3791</id><created>2011-09-17</created><updated>2011-09-21</updated><authors><author><keyname>Zhou</keyname><forenames>Fangfei</forenames></author><author><keyname>Zhang</keyname><forenames>Liang</forenames></author><author><keyname>Franco</keyname><forenames>Eric</forenames></author><author><keyname>Revis</keyname><forenames>Richard</forenames></author><author><keyname>Mislove</keyname><forenames>Alan</forenames></author><author><keyname>Sundaram</keyname><forenames>Ravi</forenames></author></authors><title>WebCloud: Recruiting web browsers for content distribution</title><categories>cs.SI</categories><comments>This paper is withdraw by the author because we don't want to make it
  publicly available for now</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are at the beginning of a shift in how content is created and exchanged
over the web. While content was previously created primarily by a small set of
entities, today, individual users -- empowered by devices like digital cameras
and services like online social networks -- are creating content that
represents a significant fraction of Internet traffic. As a result, content
today is increasingly generated and exchanged at the edge of the network.
Unfortunately, the existing techniques and infrastructure that are still used
to serve this content, such as centralized content distribution networks, are
ill-suited for these new patterns of content exchange. In this paper, we take a
first step towards addressing this situation by introducing WebCloud, a content
distribution system for online social networking sites that works by re-
purposing web browsers to help serve content. In other words, when a user
browses content, WebCloud tries to fetch it from one of that user's friend's
browsers, instead of from the social networking site. The result is a more
direct exchange of content ; essentially, WebCloud leverages the spatial and
temporal locality of interest between social network users. Because WebCloud is
built using techniques already present in many web browsers, it can be applied
today to many social networking sites. We demonstrate the practicality of
WebCloud with microbenchmarks, simulations, and a prototype deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3798</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3798</id><created>2011-09-17</created><authors><author><keyname>Dasanayake</keyname><forenames>Isuru</forenames></author><author><keyname>Li</keyname><forenames>Jr-Shin</forenames></author></authors><title>Charge-Balanced Minimum-Power Controls for Spiking Neuron Oscillators</title><categories>math.OC cs.SY math.DS q-bio.NC</categories><comments>24 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the optimal control of phase models for spiking
neuron oscillators. We focus on the design of minimum-power current stimuli
that elicit spikes in neurons at desired times. We furthermore take the
charge-balanced constraint into account because in practice undesirable side
effects may occur due to the accumulation of electric charge resulting from
external stimuli. Charge-balanced minimum-power controls are derived for a
general phase model using the maximum principle, where the cases with unbounded
and bounded control amplitude are examined. The latter is of practical
importance since phase models are more accurate for weak forcing. The developed
optimal control strategies are then applied to both mathematically ideal and
experimentally observed phase models to demonstrate their applicability,
including the phase model for the widely studied Hodgkin-Huxley equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3799</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3799</id><created>2011-09-17</created><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Xiangdong</forenames></author><author><keyname>Fu</keyname><forenames>Mengyin</forenames></author></authors><title>Consensus of Multi-Agent Systems with General Linear and Lipschitz
  Nonlinear Dynamics Using Distributed Adaptive Protocols</title><categories>cs.SY math.OC</categories><comments>15 pages, 6 figures, submitted to IEEE TAC</comments><journal-ref>IEEE Transactions on Automatic Control, Vol. 58, No. 7, July 2013</journal-ref><doi>10.1109/TAC.2012.2235715</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the distributed consensus problems for multi-agent
systems with general linear and Lipschitz nonlinear dynamics. Distributed
relative-state consensus protocols with an adaptive law for adjusting the
coupling weights between neighboring agents are designed for both the linear
and nonlinear cases, under which consensus is reached for all undirected
connected communication graphs. Extensions to the case with a leader-follower
communication graph are further studied. In contrast to the existing results in
the literature, the adaptive consensus protocols here can be implemented by
each agent in a fully distributed fashion without using any global information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3800</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3800</id><created>2011-09-17</created><updated>2011-11-11</updated><authors><author><keyname>Sharma</keyname><forenames>Amit K.</forenames></author><author><keyname>Sharma</keyname><forenames>Anuradha</forenames></author></authors><title>MacWilliams type identities for some new $m$-spotty weight enumerators</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn</comments><msc-class>94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Past few years have seen an extensive use of high-density RAM chips with wide
I/O data (e.g., 16, 32, 64 bits) in computer memory systems. These chips are
highly vulnerable to a special type of byte error, called an $m$-spotty byte
error, which can be effectively detected or corrected using byte error-control
codes. In this paper, we present joint $m$-spotty weight enumerator and split
$m$-spotty weight enumerator for byte error-control codes over the ring of
integers modulo $\ell$ ($\ell \geq 2$ is an integer) and over arbitrary finite
fields. We also derive MacWilliams type identities for each of the
aforementioned enumerators and discuss some of their applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3804</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3804</id><created>2011-09-17</created><updated>2012-07-16</updated><authors><author><keyname>Jaksic</keyname><forenames>V.</forenames></author><author><keyname>Ogata</keyname><forenames>Y.</forenames></author><author><keyname>Pillet</keyname><forenames>C. -A.</forenames></author><author><keyname>Seiringer</keyname><forenames>R.</forenames></author></authors><title>Quantum Hypothesis Testing and Non-Equilibrium Statistical Mechanics</title><categories>math-ph cs.IT math.IT math.MP quant-ph</categories><comments>60 pages</comments><journal-ref>Rev. Math. Phys. 24, 1230002 (2012)</journal-ref><doi>10.1142/S0129055X12300026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the mathematical theory of quantum hypothesis testing to the
general $W^*$-algebraic setting and explore its relation with recent
developments in non-equilibrium quantum statistical mechanics. In particular,
we relate the large deviation principle for the full counting statistics of
entropy flow to quantum hypothesis testing of the arrow of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3827</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3827</id><created>2011-09-17</created><updated>2011-09-20</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Balzano</keyname><forenames>Laura</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Online Robust Subspace Tracking from Partial Information</title><categories>cs.IT cs.CV cs.SY math.IT math.OC stat.ML</categories><comments>28 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents GRASTA (Grassmannian Robust Adaptive Subspace Tracking
Algorithm), an efficient and robust online algorithm for tracking subspaces
from highly incomplete information. The algorithm uses a robust $l^1$-norm cost
function in order to estimate and track non-stationary subspaces when the
streaming data vectors are corrupted with outliers. We apply GRASTA to the
problems of robust matrix completion and real-time separation of background
from foreground in video. In this second application, we show that GRASTA
performs high-quality separation of moving objects from background at
exceptional speeds: In one popular benchmark video example, GRASTA achieves a
rate of 57 frames per second, even when run in MATLAB on a personal laptop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3838</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3838</id><created>2011-09-17</created><updated>2011-09-22</updated><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Liu</keyname><forenames>Xiangdong</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Distributed Consensus of Linear Multi-Agent Systems with Adaptive
  Dynamic Protocols</title><categories>cs.SY math.OC</categories><comments>17 pages, 5 figues</comments><journal-ref>Automatica, 49: 1986-1995, 2013</journal-ref><doi>10.1016/j.automatica.2013.03.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the distributed consensus problem of multi-agent systems
with general continuous-time linear dynamics. Two distributed adaptive dynamic
consensus protocols are proposed, based on the relative output information of
neighboring agents. One protocol assigns an adaptive coupling weight to each
edge in the communication graph while the other uses an adaptive coupling
weight for each node. These two adaptive protocols are designed to ensure that
consensus is reached in a fully distributed fashion for any undirected
connected communication graphs without using any global information. A
sufficient condition for the existence of these adaptive protocols is that each
agent is stabilizable and detectable. The cases with leader-follower and
switching communication graphs are also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3839</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3839</id><created>2011-09-17</created><updated>2012-11-13</updated><authors><author><keyname>Adnan</keyname><forenames>Muhammad Abdullah</forenames></author><author><keyname>Sugihara</keyname><forenames>Ryo</forenames></author><author><keyname>Ma</keyname><forenames>Yan</forenames></author><author><keyname>Gupta</keyname><forenames>Rajesh</forenames></author></authors><title>Dynamic Deferral of Workload for Capacity Provisioning in Data Centers</title><categories>cs.NI</categories><comments>12 pages, 13 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent increase in energy prices has led researchers to find better ways for
capacity provisioning in data centers to reduce energy wastage due to the
variation in workload. This paper explores the opportunity for cost saving
utilizing the flexibility from the Service Level Agreements (SLAs) and proposes
a novel approach for capacity provisioning under bounded latency requirements
of the workload. We investigate how many servers to be kept active and how much
workload to be delayed for energy saving while meeting every deadline. We
present an offline LP formulation for capacity provisioning by dynamic deferral
and give two online algorithms to determine the capacity of the data center and
the assignment of workload to servers dynamically. We prove the feasibility of
the online algorithms and show that their worst case performance are bounded by
a constant factor with respect to the offline formulation. We validate our
algorithms on a MapReduce workload by provisioning capacity on a Hadoop cluster
and show that the algorithms actually perform much better in practice compared
to the naive `follow the workload' provisioning, resulting in 20-40%
cost-savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3841</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3841</id><created>2011-09-18</created><updated>2012-04-12</updated><authors><author><keyname>Su</keyname><forenames>Han-I</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Limits on the Benefits of Energy Storage for Renewable Integration</title><categories>math.OC cs.SY</categories><comments>45 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high variability of renewable energy resources presents significant
challenges to the operation of the electric power grid. Conventional generators
can be used to mitigate this variability but are costly to operate and produce
carbon emissions. Energy storage provides a more environmentally friendly
alternative, but is costly to deploy in large amounts. This paper studies the
limits on the benefits of energy storage to renewable energy: How effective is
storage at mitigating the adverse effects of renewable energy variability? How
much storage is needed? What are the optimal control policies for operating
storage? To provide answers to these questions, we first formulate the power
flow in a single-bus power system with storage as an infinite horizon
stochastic program. We find the optimal policies for arbitrary net renewable
generation process when the cost function is the average conventional
generation (environmental cost) and when it is the average loss of load
probability (reliability cost). We obtain more refined results by considering
the multi-timescale operation of the power system. We view the power flow in
each timescale as the superposition of a predicted (deterministic) component
and an prediction error (residual) component and formulate the residual power
flow problem as an infinite horizon dynamic program. Assuming that the net
generation prediction error is an IID process, we quantify the asymptotic
benefits of storage. With the additional assumption of Laplace distributed
prediction error, we obtain closed form expressions for the stationary
distribution of storage and conventional generation. Finally, we propose a
two-threshold policy that trades off conventional generation saving with loss
of load probability. We illustrate our results and corroborate the IID and
Laplace assumptions numerically using datasets from CAISO and NREL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3843</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3843</id><created>2011-09-18</created><updated>2012-12-04</updated><authors><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Fast approximation of matrix coherence and statistical leverage</title><categories>cs.DS cs.DM cs.LG</categories><comments>29 pages; conference version is in ICML; journal version is in JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical leverage scores of a matrix $A$ are the squared row-norms of
the matrix containing its (top) left singular vectors and the coherence is the
largest leverage score. These quantities are of interest in recently-popular
problems such as matrix completion and Nystr\&quot;{o}m-based low-rank matrix
approximation as well as in large-scale statistical data analysis applications
more generally; moreover, they are of interest since they define the key
structural nonuniformity that must be dealt with in developing fast randomized
matrix algorithms. Our main result is a randomized algorithm that takes as
input an arbitrary $n \times d$ matrix $A$, with $n \gg d$, and that returns as
output relative-error approximations to all $n$ of the statistical leverage
scores. The proposed algorithm runs (under assumptions on the precise values of
$n$ and $d$) in $O(n d \log n)$ time, as opposed to the $O(nd^2)$ time required
by the na\&quot;{i}ve algorithm that involves computing an orthogonal basis for the
range of $A$. Our analysis may be viewed in terms of computing a relative-error
approximation to an underconstrained least-squares approximation problem, or,
relatedly, it may be viewed as an application of Johnson-Lindenstrauss type
ideas. Several practically-important extensions of our basic result are also
described, including the approximation of so-called cross-leverage scores, the
extension of these ideas to matrices with $n \approx d$, and the extension to
streaming environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3850</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3850</id><created>2011-09-18</created><authors><author><keyname>Lee</keyname><forenames>Dae-Woong</forenames></author></authors><title>On the digital homology groups of digital images</title><categories>cs.CV</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study the digital homology groups of digital images which
are based on the singular homology groups of topological spaces in algebraic
topology. Specifically, we define a digitally standard $n$-simplex, a digitally
singular $n$-simplex, and the digital homology groups of digital images with
$k$-adjacency relations. We then construct a covariant functor from a category
of digital images and digitally continuous functions to the one of abelian
groups and group homomorphisms, and investigate some fundamental and
interesting properties of digital homology groups of digital images, such as
the digital version of the dimension axiom which is one of the
Eilenberg-Steenrod axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3863</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3863</id><created>2011-09-18</created><authors><author><keyname>Phung</keyname><forenames>Kim Dang</forenames></author><author><keyname>Wang</keyname><forenames>Gengsheng</forenames></author></authors><title>An observability for parabolic equations from a measurable set in time</title><categories>math.AP cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new observability estimate for parabolic equations in
$\Omega\times(0,T)$, where $\Omega$ is a convex domain. The observation region
is restricted over a product set of an open nonempty subset of $\Omega$ and a
subset of positive measure in $(0,T)$. This estimate is derived with the aid of
a quantitative unique continuation at one point in time. Applications to the
bang-bang property for norm and time optimal control problems are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3876</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3876</id><created>2011-09-18</created><authors><author><keyname>Alfandary</keyname><forenames>Liam</forenames></author><author><keyname>Raphaeli</keyname><forenames>Dan</forenames></author></authors><title>Two-Dimensional Tail-Biting Convolutional Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multidimensional convolutional codes are an extension of the notion of
convolutional codes (CCs) to several dimensions of time. This paper explores
the class of two-dimensional convolutional codes (2D CCs) and 2D tail-biting
convolutional codes (2D TBCCs), in particular, from several aspects. First, we
derive several basic algebraic properties of these codes, applying algebraic
methods in order to find bijective encoders, create parity check matrices and
to inverse encoders. Next, we discuss the minimum distance and weight
distribution properties of these codes. Extending an existing tree-search
algorithm to two dimensions, we apply it to find codes with high minimum
distance. Word-error probability asymptotes for sample codes are given and
compared with other codes. The results of this approach suggest that 2D TBCCs
can perform better than comparable 1D TBCCs or other codes. We then present
several novel iterative suboptimal algorithms for soft decoding 2D CCs, which
are based on belief propagation. Two main approaches to decoding are
considered. We first focus on a decoder which extends the concept of trellis
decoding to two dimensions. Second, we investigate algorithms which use the
code's parity check matrices. We apply conventional BP in the parity domain,
but improve it with a novel modification. Next, we test the generalized belief
propagation (GBP) algorithm. Performance results are presented and compared
with optimum decoding techniques and bounds. The results show that our
suboptimal algorithms achieve respectable results, in some cases coming as
close as 0.2dB from optimal (maximum-likelihood) decoding. However for some of
the codes there is still a large gap from the optimal decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3887</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3887</id><created>2011-09-18</created><updated>2011-09-30</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>An Algorithmic Approach to Information and Meaning</title><categories>cs.IT math.IT</categories><comments>preprint reviewed version closer to the version accepted by the
  journal</comments><journal-ref>APA Newsletter on Philosophy and Computers, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I will survey some matters of relevance to a philosophical discussion of
information, taking into account developments in algorithmic information theory
(AIT). I will propose that meaning is deep in the sense of Bennett's logical
depth, and that algorithmic probability may provide the stability needed for a
robust algorithmic definition of meaning, one that takes into consideration the
interpretation and the recipient's own knowledge encoded in the story attached
to a message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3890</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3890</id><created>2011-09-18</created><authors><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author></authors><title>A Dynamic Stabbing-Max Data Structure with Sub-Logarithmic Query Time</title><categories>cs.DS cs.CG</categories><comments>Extended version of a paper accepted to ISAAC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a dynamic data structure that answers
one-dimensional stabbing-max queries in optimal $O(\log n/\log\log n)$ time.
Our data structure uses linear space and supports insertions and deletions in
$O(\log n)$ and $O(\log n/\log \log n)$ amortized time respectively.
  We also describe a $O(n(\log n/\log\log n)^{d-1})$ space data structure that
answers $d$-dimensional stabbing-max queries in $O((\log n/\log\log n)^{d})$
time. Insertions and deletions are supported in $O((\log n/\log\log
n)^d\log\log n)$ and $O((\log n/\log\log n)^d)$ amortized time respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3893</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3893</id><created>2011-09-18</created><updated>2012-04-04</updated><authors><author><keyname>Vegh</keyname><forenames>Laszlo A.</forenames></author></authors><title>Concave Generalized Flows with Applications to Market Equilibria</title><categories>cs.DS cs.DM cs.GT q-fin.CP</categories><comments>Major revision. Instead of highest gain augmenting paths, we employ
  the Fat-Path framework. Many parts simplified, running time for the linear
  case improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a nonlinear extension of the generalized network flow model, with
the flow leaving an arc being an increasing concave function of the flow
entering it, as proposed by Truemper and Shigeno. We give a polynomial time
combinatorial algorithm for solving corresponding flow maximization problems,
finding an epsilon-approximate solution in O(m(m+log n)log(MUm/epsilon))
arithmetic operations and value oracle queries, where M and U are upper bounds
on simple parameters. This also gives a new algorithm for linear generalized
flows, an efficient, purely scaling variant of the Fat-Path algorithm by
Goldberg, Plotkin and Tardos, not using any cycle cancellations.
  We show that this general convex programming model serves as a common
framework for several market equilibrium problems, including the linear Fisher
market model and its various extensions. Our result immediately extends these
market models to more general settings. We also obtain a combinatorial
algorithm for nonsymmetric Arrow-Debreu Nash bargaining, settling an open
question by Vazirani.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3898</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3898</id><created>2011-09-18</created><authors><author><keyname>Patwari</keyname><forenames>Neal</forenames></author><author><keyname>Wilson</keyname><forenames>Joey</forenames></author><author><keyname>R.</keyname><forenames>Sai Ananthanarayanan P.</forenames></author><author><keyname>Kasera</keyname><forenames>Sneha K.</forenames></author><author><keyname>Westenskow</keyname><forenames>Dwayne</forenames></author></authors><title>Monitoring Breathing via Signal Strength in Wireless Networks</title><categories>cs.NI cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows experimentally that standard wireless networks which measure
received signal strength (RSS) can be used to reliably detect human breathing
and estimate the breathing rate, an application we call &quot;BreathTaking&quot;. We show
that although an individual link cannot reliably detect breathing, the
collective spectral content of a network of devices reliably indicates the
presence and rate of breathing. We present a maximum likelihood estimator (MLE)
of breathing rate, amplitude, and phase, which uses the RSS data from many
links simultaneously. We show experimental results which demonstrate that
reliable detection and frequency estimation is possible with 30 seconds of
data, within 0.3 breaths per minute (bpm) RMS error. Use of directional
antennas is shown to improve robustness to motion near the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3911</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3911</id><created>2011-09-18</created><authors><author><keyname>Maiya</keyname><forenames>Arun S.</forenames></author><author><keyname>Berger-Wolf</keyname><forenames>Tanya Y.</forenames></author></authors><title>Benefits of Bias: Towards Better Characterization of Network Sampling</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages; KDD 2011: 17th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From social networks to P2P systems, network sampling arises in many
settings. We present a detailed study on the nature of biases in network
sampling strategies to shed light on how best to sample from networks. We
investigate connections between specific biases and various measures of
structural representativeness. We show that certain biases are, in fact,
beneficial for many applications, as they &quot;push&quot; the sampling process towards
inclusion of desired properties. Finally, we describe how these sampling biases
can be exploited in several, real-world applications including disease outbreak
detection and market research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3934</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3934</id><created>2011-09-18</created><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>A linear-time algorithm for the strong chromatic index of Halin graphs</title><categories>cs.DS</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exists a linear-time algorithm that computes the strong
chromatic index of Halin graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3940</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3940</id><created>2011-09-19</created><authors><author><keyname>Shi</keyname><forenames>Yuan</forenames></author><author><keyname>Noh</keyname><forenames>Yung-Kyun</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author><author><keyname>Lee</keyname><forenames>Daniel D.</forenames></author></authors><title>Learning Discriminative Metrics via Generative Models and Kernel
  Learning</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metrics specifying distances between data points can be learned in a
discriminative manner or from generative models. In this paper, we show how to
unify generative and discriminative learning of metrics via a kernel learning
framework. Specifically, we learn local metrics optimized from parametric
generative models. These are then used as base kernels to construct a global
kernel that minimizes a discriminative training criterion. We consider both
linear and nonlinear combinations of local metric kernels. Our empirical
results show that these combinations significantly improve performance on
classification tasks. The proposed learning algorithm is also very efficient,
achieving order of magnitude speedup in training time compared to previous
discriminative baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3948</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3948</id><created>2011-09-19</created><updated>2012-02-06</updated><authors><author><keyname>Agaev</keyname><forenames>R. P.</forenames></author><author><keyname>Chebotarev</keyname><forenames>P. Yu.</forenames></author></authors><title>The Projection Method for Reaching Consensus and the Regularized Power
  Limit of a Stochastic Matrix</title><categories>cs.MA cs.NI cs.SY math.OC math.PR</categories><comments>19 pages, 2 figures</comments><msc-class>93A14, 15B51, 05C50, 05C05</msc-class><journal-ref>Automation and Remote Control, 2011, vol.72, No.12, P.2458-2476</journal-ref><doi>10.1134/S0005117911120034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the coordination/consensus problem for multi-agent systems, a well-known
condition of achieving consensus is the presence of a spanning arborescence in
the communication digraph. The paper deals with the discrete consensus problem
in the case where this condition is not satisfied. A characterization of the
subspace $T_P$ of initial opinions (where $P$ is the influence matrix) that
\emph{ensure} consensus in the DeGroot model is given. We propose a method of
coordination that consists of: (1) the transformation of the vector of initial
opinions into a vector belonging to $T_P$ by orthogonal projection and (2)
subsequent iterations of the transformation $P.$ The properties of this method
are studied. It is shown that for any non-periodic stochastic matrix $P,$ the
resulting matrix of the orthogonal projection method can be treated as a
regularized power limit of $P.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3952</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3952</id><created>2011-09-19</created><authors><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Gowda</keyname><forenames>Kiran T.</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Gaussian Two-way Relay Channel with Private Information for the Relay</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted for publication in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a generalized two-way relay channel where two sources exchange
information (not necessarily of the same rate) with help from a relay, and each
source additionally sends private information to the relay. We consider the
Gaussian setting where all point-to-point links are Gaussian channels. For this
channel, we consider a two-phase protocol consisting of a multiple access
channel (MAC) phase and a broadcast channel (BC) phase. We propose a general
decode-and-forward (DF) scheme where the MAC phase is related to computation
over MAC, while the BC phase is related to BC with receiver side information.
In the MAC phase, we time share a capacity-achieving code for the MAC and a
superposition code with a lattice code as its component code. We show that the
proposed DF scheme is near optimal for any channel conditions, in that it
achieves rates within half bit of the capacity region of the two-phase
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3954</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3954</id><created>2011-09-19</created><updated>2012-09-26</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>A Faster Grammar-Based Self-Index</title><categories>cs.DS</categories><comments>journal version of LATA '12 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To store and search genomic databases efficiently, researchers have recently
started building compressed self-indexes based on grammars. In this paper we
show how, given a straight-line program with $r$ rules for a string (S [1..n])
whose LZ77 parse consists of $z$ phrases, we can store a self-index for $S$ in
$\Oh{r + z \log \log n}$ space such that, given a pattern (P [1..m]), we can
list the $\occ$ occurrences of $P$ in $S$ in $\Oh{m^2 + \occ \log \log n}$
time. If the straight-line program is balanced and we accept a small
probability of building a faulty index, then we can reduce the $\Oh{m^2}$ term
to $\Oh{m \log m}$. All previous self-indexes are larger or slower in the worst
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3957</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3957</id><created>2011-09-19</created><authors><author><keyname>Rajendiran</keyname><forenames>M.</forenames></author><author><keyname>Srivatsa</keyname><forenames>S. K.</forenames></author></authors><title>On-Demand Multicasting in Ad-hoc Networks: Performance Evaluation of
  AODV, ODMRP and FSR</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Adhoc networks are characterized by connectivity through a collection of
wireless nodes and fast changing network topology. Wireless nodes are free to
move independent of each other which makes routing much difficult. This calls
for the need of an efficient dynamic routing protocol. Mesh-based multicast
routing technique establishes communications between mobile nodes of wireless
adhoc networks in a faster and efficient way. In this article the performance
of prominent on-demand routing protocols for mobile adhoc networks such as
ODMRP (On Demand Multicast Routing Protocol), AODV (Adhoc on Demand Distance
Vector) and FSR (Fisheye State Routing protocol) was studied. The parameters
viz., average throughput, packet delivery ration and end-to-end delay were
evaluated. From the simulation results and analysis, a suitable routing
protocol can be chosen for a specified network. The results show that the ODMRP
protocol performance is remarkably superior as compared with AODV and FSR
routing protocols. Keywords: MANET, Multicast Routing, ODMRP, AODV, FSR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3987</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3987</id><created>2011-09-19</created><authors><author><keyname>Gavalas</keyname><forenames>Damianos</forenames></author><author><keyname>Pantziou</keyname><forenames>Grammati</forenames></author><author><keyname>Konstantopoulos</keyname><forenames>Charalampos</forenames></author><author><keyname>Mamalis</keyname><forenames>Basilis</forenames></author></authors><title>Clustering of Mobile Ad Hoc Networks: An Adaptive Broadcast Period
  Approach</title><categories>cs.DC cs.DS</categories><comments>7 pages, 9 figures; IEEE International Conference on Communications,
  2006. ICC '06</comments><doi>10.1109/ICC.2006.255712</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organization, scalability and routing have been identified as key problems
hindering viability and commercial success of mobile ad hoc networks.
Clustering of mobile nodes among separate domains has been proposed as an
efficient approach to address those issues. In this work, we introduce an
efficient distributed clustering algorithm that uses both location and energy
metrics for cluster formation. Our proposed solution mainly addresses cluster
stability, manageability and energy efficiency issues. Also, unlike existing
active clustering methods, our algorithm relieves the network from the
unnecessary burden of control messages broadcasting, especially for relatively
static network topologies. This is achieved through adapting broadcast period
according to mobile nodes mobility pattern. The efficiency, scalability and
competence of our algorithm against alternative approaches have been
demonstrated through simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3989</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3989</id><created>2011-09-19</created><updated>2011-10-11</updated><authors><author><keyname>Oetsch</keyname><forenames>Johannes</forenames></author><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author></authors><title>The SeaLion has Landed: An IDE for Answer-Set Programming---Preliminary
  Report</title><categories>cs.PL cs.AI</categories><comments>Proceedings of the 19th International Conference on Applications of
  Declarative Programming and Knowledge Management (INAP 2011) and 25th
  Workshop on Logic Programming (WLP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report about the current state and designated features of the tool
SeaLion, aimed to serve as an integrated development environment (IDE) for
answer-set programming (ASP). A main goal of SeaLion is to provide a
user-friendly environment for supporting a developer to write, evaluate, debug,
and test answer-set programs. To this end, new support techniques have to be
developed that suit the requirements of the answer-set semantics and meet the
constraints of practical applicability. In this respect, SeaLion benefits from
the research results of a project on methods and methodologies for answer-set
program development in whose context SeaLion is realised. Currently, the tool
provides source-code editors for the languages of Gringo and DLV that offer
syntax highlighting, syntax checking, and a visual program outline. Further
implemented features are support for external solvers and visualisation as well
as visual editing of answer sets. SeaLion comes as a plugin of the popular
Eclipse platform and provides itself interfaces for future extensions of the
IDE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3994</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3994</id><created>2011-09-19</created><authors><author><keyname>Misztal</keyname><forenames>Krzysztof</forenames></author><author><keyname>Spurek</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>k-means Approach to the Karhunen-Loeve Transform</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>6 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simultaneous generalization of the well-known Karhunen-Loeve
(PCA) and k-means algorithms. The basic idea lies in approximating the data
with k affine subspaces of a given dimension n. In the case n=0 we obtain the
classical k-means, while for k=1 we obtain PCA algorithm. We show that for some
data exploration problems this method gives better result then either of the
classical approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3997</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3997</id><created>2011-09-19</created><authors><author><keyname>Gavalas</keyname><forenames>Damianos</forenames></author><author><keyname>Pantziou</keyname><forenames>Grammati</forenames></author><author><keyname>Konstantopoulos</keyname><forenames>Charalampos</forenames></author><author><keyname>Mamalis</keyname><forenames>Basilis</forenames></author></authors><title>Lowest-ID with Adaptive ID Reassignment: A Novel Mobile Ad-Hoc Networks
  Clustering Algorithm</title><categories>cs.DC</categories><comments>5 pages, 4 figures; Proceedings of the 1st IEEE International
  Symposium on Wireless Pervasive Computing (ISWPC'2006)</comments><doi>10.1109/ISWPC.2006.1613559</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is a promising approach for building hierarchies and simplifying
the routing process in mobile ad-hoc network environments. The main objective
of clustering is to identify suitable node representatives, i.e. cluster heads
(CHs), to store routing and topology information and maximize clusters
stability. Traditional clustering algorithms suggest CH election exclusively
based on node IDs or location information and involve frequent broadcasting of
control packets, even when network topology remains unchanged. More recent
works take into account additional metrics (such as energy and mobility) and
optimize initial clustering. However, in many situations (e.g. in relatively
static topologies) re-clustering procedure is hardly ever invoked; hence
initially elected CHs soon reach battery exhaustion. Herein, we introduce an
efficient distributed clustering algorithm that uses both mobility and energy
metrics to provide stable cluster formations. CHs are initially elected based
on the time and cost-efficient lowest-ID method. During clustering maintenance
phase though, node IDs are re-assigned according to nodes mobility and energy
status, ensuring that nodes with low-mobility and sufficient energy supply are
assigned low IDs and, hence, are elected as CHs. Our algorithm also reduces
control traffic volume since broadcast period is adjusted according to nodes
mobility pattern: we employ infrequent broadcasting for relative static network
topologies, and increase broadcast frequency for highly mobile network
configurations. Simulation results verify that energy consumption is uniformly
distributed among network nodes and that signaling overhead is significantly
decreased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.3999</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.3999</id><created>2011-09-19</created><authors><author><keyname>Gavalas</keyname><forenames>Damianos</forenames></author></authors><title>A Lightweight and Flexible Mobile Agent Platform Tailored to Management
  Applications</title><categories>cs.NI</categories><comments>7 pages, 5 figures; Proceedings of the 2006 Conference on Mobile
  Computing and Wireless Communications (MCWC'2006)</comments><doi>10.1109/MCWC.2006.4375222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Agents (MAs) represent a distributed computing technology that
promises to address the scalability problems of centralized network management.
A critical issue that will affect the wider adoption of MA paradigm in
management applications is the development of MA Platforms (MAPs) expressly
oriented to distributed management. However, most of available platforms impose
considerable burden on network and system resources and also lack of essential
functionality. In this paper, we discuss the design considerations and
implementation details of a complete MAP research prototype that sufficiently
addresses all the aforementioned issues. Our MAP has been implemented in Java
and tailored for network and systems management applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4017</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4017</id><created>2011-09-19</created><updated>2011-09-27</updated><authors><author><keyname>Ummels</keyname><forenames>Michael</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Wojtczak</keyname><forenames>Dominik</forenames><affiliation>CWI Amsterdam</affiliation></author></authors><title>The Complexity of Nash Equilibria in Stochastic Multiplayer Games</title><categories>cs.GT cs.CC</categories><proxy>LMCS</proxy><acm-class>F.1.2, G.1.6, G.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  28, 2011) lmcs:1209</journal-ref><doi>10.2168/LMCS-7(3:20)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the computational complexity of finding Nash equilibria in
turn-based stochastic multiplayer games with omega-regular objectives. We show
that restricting the search space to equilibria whose payoffs fall into a
certain interval may lead to undecidability. In particular, we prove that the
following problem is undecidable: Given a game G, does there exist a Nash
equilibrium of G where Player 0 wins with probability 1? Moreover, this problem
remains undecidable when restricted to pure strategies or (pure) strategies
with finite memory. One way to obtain a decidable variant of the problem is to
restrict the strategies to be positional or stationary. For the complexity of
these two problems, we obtain a common lower bound of NP and upper bounds of NP
and PSPACE respectively. Finally, we single out a special case of the general
problem that, in many cases, admits an efficient solution. In particular, we
prove that deciding the existence of an equilibrium in which each player either
wins or loses with probability 1 can be done in polynomial time for games where
the objective of each player is given by a parity condition with a bounded
number of priorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4032</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4032</id><created>2011-09-19</created><updated>2011-09-30</updated><authors><author><keyname>&#x160;i&#x161;ka</keyname><forenames>David</forenames></author></authors><title>Error estimates for finite difference approximations of American put
  option price</title><categories>q-fin.CP cs.SY math.NA math.OC math.PR q-fin.PR</categories><msc-class>65M06, 65M12, 60G40, 35R35, 91G80, 91G60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite difference approximations to multi-asset American put option price are
considered. The assets are modelled as a multi-dimensional diffusion process
with variable drift and volatility. Approximation error of order one quarter
with respect to the time discretisation parameter and one half with respect to
the space discretisation parameter is proved by reformulating the corresponding
optimal stopping problem as a solution of a degenerate Hamilton-Jacobi-Bellman
equation. Furthermore, the error arising from restricting the discrete problem
to a finite grid by reducing the original problem to a bounded domain is
estimated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4034</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4034</id><created>2011-09-19</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author></authors><title>Tying up the loose ends in fully LZW-compressed pattern matching</title><categories>cs.DS</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a natural generalization of the classical pattern matching
problem: given compressed representations of a pattern p[1..M] and a text
t[1..N] of sizes m and n, respectively, does p occur in t? We develop an
optimal linear time solution for the case when both p and t are compressed
using the LZW method. This improves the previously known O((n+m)log(n+m)) time
solution of Gasieniec and Rytter, and essentially closes the line of research
devoted to studying LZW-compressed exact pattern matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4039</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4039</id><created>2011-09-19</created><updated>2011-10-16</updated><authors><author><keyname>Blond</keyname><forenames>Stevens Le</forenames><affiliation>MPI for Software Systems</affiliation></author><author><keyname>Chao</keyname><forenames>Zhang</forenames><affiliation>NYU-Poly</affiliation></author><author><keyname>Legout</keyname><forenames>Arnaud</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Ross</keyname><forenames>Keith W.</forenames><affiliation>NYU-Poly</affiliation></author><author><keyname>Dabbous</keyname><forenames>Walid</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>I Know Where You are and What You are Sharing: Exploiting P2P
  Communications to Invade Users' Privacy</title><categories>cs.NI</categories><comments>This is the authors' version of the ACM/USENIX Internet Measurement
  Conference (IMC) 2011 paper</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show how to exploit real-time communication applications to
determine the IP address of a targeted user. We focus our study on Skype,
although other real-time communication applications may have similar privacy
issues. We first design a scheme that calls an identified targeted user
inconspicuously to find his IP address, which can be done even if he is behind
a NAT. By calling the user periodically, we can then observe the mobility of
the user. We show how to scale the scheme to observe the mobility patterns of
tens of thousands of users. We also consider the linkability threat, in which
the identified user is linked to his Internet usage. We illustrate this threat
by combining Skype and BitTorrent to show that it is possible to determine the
file-sharing usage of identified users. We devise a scheme based on the
identification field of the IP datagrams to verify with high accuracy whether
the identified user is participating in specific torrents. We conclude that any
Internet user can leverage Skype, and potentially other real-time communication
systems, to observe the mobility and file-sharing usage of tens of millions of
identified users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4048</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4048</id><created>2011-09-19</created><authors><author><keyname>Kono</keyname><forenames>Shinji</forenames></author><author><keyname>Yogi</keyname><forenames>Kento</forenames></author></authors><title>Implementing Continuation based language in GCC</title><categories>cs.PL</categories><comments>Continuation Festa 2008 Tokyo</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We have implemented C like Continuation based programming language.
Continuation based C, CbC was implemented using micro-C on various
architecture, and we have tried several CbC programming experiments. Here we
report new implementation of CbC compiler based on GCC 4.2.3. Since it contains
full C capability, we can use CbC and C in a mixture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4074</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4074</id><created>2011-09-19</created><updated>2011-09-19</updated><authors><author><keyname>Li</keyname><forenames>Xiaolin</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author></authors><title>Secure Multiplex Coding Over Interference Channel with Confidential
  Messages</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, inner and outer bounds on the capacity region of two-user
interference channels with two confidential messages have been proposed. By
adding secure multiplex coding to the error correction method in [15] which
achieves the best achievable capacity region for interference channel up to
now, we have shown that the improved secure capacity region compared with [2]
now is the whole Han-Kobayashi region. In addition, this construction not only
removes the rate loss incurred by adding dummy messages to achieve security,
but also change the original weak security condition in [2] to strong security.
Then the equivocation rate for a collection of secret messages has also been
evaluated, when the length of the message is finite or the information rate is
high, our result provides a good approximation for bounding the worst case
equivocation rate. Our results can be readily extended to the Gaussian
interference channel with little efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4095</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4095</id><created>2011-09-19</created><updated>2011-10-11</updated><authors><author><keyname>Kloim&#xfc;llner</keyname><forenames>Christian</forenames></author><author><keyname>Oetsch</keyname><forenames>Johannes</forenames></author><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author></authors><title>Kara: A System for Visualising and Visual Editing of Interpretations for
  Answer-Set Programs</title><categories>cs.LO cs.AI cs.GR cs.PL</categories><comments>Proceedings of the 19th International Conference on Applications of
  Declarative Programming and Knowledge Management (INAP 2011) and 25th
  Workshop on Logic Programming (WLP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In answer-set programming (ASP), the solutions of a problem are encoded in
dedicated models, called answer sets, of a logical theory. These answer sets
are computed from the program that represents the theory by means of an ASP
solver and returned to the user as sets of ground first-order literals. As this
type of representation is often cumbersome for the user to interpret, tools
like ASPVIZ and IDPDraw were developed that allow for visualising answer sets.
The tool Kara, introduced in this paper, follows these approaches, using ASP
itself as a language for defining visualisations of interpretations. Unlike
existing tools that position graphic primitives according to static coordinates
only, Kara allows for more high-level specifications, supporting graph
structures, grids, and relative positioning of graphical elements. Moreover,
generalising the functionality of previous tools, Kara provides modifiable
visualisations such that interpretations can be manipulated by graphically
editing their visualisations. This is realised by resorting to abductive
reasoning techniques. Kara is part of SeaLion, a forthcoming integrated
development environment (IDE) for ASP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4102</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4102</id><created>2011-09-19</created><updated>2012-01-10</updated><authors><author><keyname>Ru</keyname><forenames>Yu</forenames></author><author><keyname>Kleissl</keyname><forenames>Jan</forenames></author><author><keyname>Martinez</keyname><forenames>Sonia</forenames></author></authors><title>Storage Size Determination for Grid-Connected Photovoltaic Systems</title><categories>math.OC cs.SY</categories><comments>Submitted to IEEE Transactions on Sustainable Energy, June 2011; Jan
  2012 (revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of determining the size of battery
storage used in grid-connected photovoltaic (PV) systems. In our setting,
electricity is generated from PV and is used to supply the demand from loads.
Excess electricity generated from the PV can be stored in a battery to be used
later on, and electricity must be purchased from the electric grid if the PV
generation and battery discharging cannot meet the demand. Due to the
time-of-use electricity pricing, electricity can also be purchased from the
grid when the price is low, and be sold back to the grid when the price is
high. The objective is to minimize the cost associated with purchasing from (or
selling back to) the electric grid and the battery capacity loss while at the
same time satisfying the load and reducing the peak electricity purchase from
the grid. Essentially, the objective function depends on the chosen battery
size. We want to find a unique critical value (denoted as $C_{ref}^c$) of the
battery size such that the total cost remains the same if the battery size is
larger than or equal to $C_{ref}^c$, and the cost is strictly larger if the
battery size is smaller than $C_{ref}^c$. We obtain a criterion for evaluating
the economic value of batteries compared to purchasing electricity from the
grid, propose lower and upper bounds on $C_{ref}^c$, and introduce an efficient
algorithm for calculating its value; these results are validated via
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4104</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4104</id><created>2011-09-19</created><updated>2011-09-22</updated><authors><author><keyname>Castellani</keyname><forenames>Marco</forenames></author><author><keyname>Brescia</keyname><forenames>Massimo</forenames></author><author><keyname>Mancini</keyname><forenames>Ettore</forenames></author><author><keyname>Pellecchia</keyname><forenames>Luca</forenames></author><author><keyname>Longo</keyname><forenames>Giuseppe</forenames></author></authors><title>VOGCLUSTERS: an example of DAME web application</title><categories>astro-ph.IM cs.DB</categories><comments>4 pages, 1 figure. Proceedings of &quot;Advances in Computational
  Astrophysics: methods, tools and outcomes&quot; (Cefal\`u, Sicily, June 2011). To
  be published on ASP Conference Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the alpha release of the VOGCLUSTERS web application, specialized
for data and text mining on globular clusters. It is one of the web2.0
technology based services of Data Mining &amp; Exploration (DAME) Program, devoted
to mine and explore heterogeneous information related to globular clusters
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4114</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4114</id><created>2011-09-19</created><authors><author><keyname>Andreev</keyname><forenames>Konstantin</forenames></author><author><keyname>Maggs</keyname><forenames>Bruce M.</forenames></author><author><keyname>Meyerson</keyname><forenames>Adam</forenames></author><author><keyname>Saks</keyname><forenames>Jevan</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh K.</forenames></author></authors><title>Algorithms for Constructing Overlay Networks For Live Streaming</title><categories>cs.NI cs.DC cs.DS</categories><acm-class>F.2.2; C.2.1; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial time approximation algorithm for constructing an
overlay multicast network for streaming live media events over the Internet.
The class of overlay networks constructed by our algorithm include networks
used by Akamai Technologies to deliver live media events to a global audience
with high fidelity. We construct networks consisting of three stages of nodes.
The nodes in the first stage are the entry points that act as sources for the
live streams. Each source forwards each of its streams to one or more nodes in
the second stage that are called reflectors. A reflector can split an incoming
stream into multiple identical outgoing streams, which are then sent on to
nodes in the third and final stage that act as sinks and are located in edge
networks near end-users. As the packets in a stream travel from one stage to
the next, some of them may be lost. A sink combines the packets from multiple
instances of the same stream (by reordering packets and discarding duplicates)
to form a single instance of the stream with minimal loss. Our primary
contribution is an algorithm that constructs an overlay network that provably
satisfies capacity and reliability constraints to within a constant factor of
optimal, and minimizes cost to within a logarithmic factor of optimal. Further
in the common case where only the transmission costs are minimized, we show
that our algorithm produces a solution that has cost within a factor of 2 of
optimal. We also implement our algorithm and evaluate it on realistic traces
derived from Akamai's live streaming network. Our empirical results show that
our algorithm can be used to efficiently construct large-scale overlay networks
in practice with near-optimal cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4156</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4156</id><created>2011-09-19</created><authors><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Approximate Distance Oracles with Improved Preprocessing Time</title><categories>cs.DM</categories><comments>To appear at the Twenty-Third Annual ACM-SIAM Symposium on Discrete
  Algorithms (SODA), Kyoto, 2012</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G$ with $m$ edges, $n$ vertices, and non-negative
edge weights, and given an integer $k\geq 1$, we show that for some universal
constant $c$, a $(2k-1)$-approximate distance oracle for $G$ of size $O(kn^{1 +
1/k})$ can be constructed in $O(\sqrt km + kn^{1 + c/\sqrt k})$ time and can
answer queries in $O(k)$ time. We also give an oracle which is faster for
smaller $k$. Our results break the quadratic preprocessing time bound of
Baswana and Kavitha for all $k\geq 6$ and improve the $O(kmn^{1/k})$ time bound
of Thorup and Zwick except for very sparse graphs and small $k$. When $m =
\Omega(n^{1 + c/\sqrt k})$ and $k = O(1)$, our oracle is optimal w.r.t.\ both
stretch, size, preprocessing time, and query time, assuming a widely believed
girth conjecture by Erd\H{o}s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4173</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4173</id><created>2011-09-19</created><authors><author><keyname>Xia</keyname><forenames>Dong</forenames></author><author><keyname>Zhang</keyname><forenames>Jian-Kang</forenames></author><author><keyname>Dumitrescu</keyname><forenames>Sorina</forenames></author></authors><title>Energy-Efficient Full Diversity Collaborative Unitary Space-Time Block
  Code Design via Unique Factorization of Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel concept called a \textit{uniquely factorable
constellation pair} (UFCP) is proposed for the systematic design of a
noncoherent full diversity collaborative unitary space-time block code by
normalizing two Alamouti codes for a wireless communication system having two
transmitter antennas and a single receiver antenna. It is proved that such a
unitary UFCP code assures the unique identification of both channel
coefficients and transmitted signals in a noise-free case as well as full
diversity for the noncoherent maximum likelihood (ML) receiver in a noise case.
To further improve error performance, an optimal unitary UFCP code is designed
by appropriately and uniquely factorizing a pair of energy-efficient cross
quadrature amplitude modulation (QAM) constellations to maximize the coding
gain subject to a transmission bit rate constraint. After a deep investigation
of the fractional coding gain function, a technical approach developed in this
paper to maximizing the coding gain is to carefully design an energy scale to
compress the first three largest energy points in the corner of the QAM
constellations in the denominator of the objective as well as carefully design
a constellation triple forming two UFCPs, with one collaborating with the other
two so as to make the accumulated minimum Euclidean distance along the two
transmitter antennas in the numerator of the objective as large as possible and
at the same time, to avoid as many corner points of the QAM constellations with
the largest energy as possible to achieve the minimum of the numerator. In
other words, the optimal coding gain is attained by intelligent constellations
collaboration and efficient energy compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4179</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4179</id><created>2011-09-19</created><updated>2013-09-16</updated><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Golrezaei</keyname><forenames>Negin</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>FemtoCaching: Wireless Video Content Delivery through Distributed
  Caching Helpers</title><categories>cs.NI cs.IT math.IT</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video on-demand streaming from Internet-based servers is becoming one of the
most important services offered by wireless networks today. In order to improve
the area spectral efficiency of video transmission in cellular systems, small
cells heterogeneous architectures (e.g., femtocells, WiFi off-loading) are
being proposed, such that video traffic to nomadic users can be handled by
short-range links to the nearest small cell access points (referred to as
&quot;helpers&quot;). As the helper deployment density increases, the backhaul capacity
becomes the system bottleneck. In order to alleviate such bottleneck we propose
a system where helpers with low-rate backhaul but high storage capacity cache
popular video files. Files not available from helpers are transmitted by the
cellular base station. We analyze the optimum way of assigning files to the
helpers, in order to minimize the expected downloading time for files. We
distinguish between the uncoded case (where only complete files are stored) and
the coded case, where segments of Fountain-encoded versions of the video files
are stored at helpers. We show that the uncoded optimum file assignment is
NP-hard, and develop a greedy strategy that is provably within a factor 2 of
the optimum. Further, for a special case we provide an efficient algorithm
achieving a provably better approximation ratio of $1-(1-1/d)^d$, where $d$ is
the maximum number of helpers a user can be connected to. We also show that the
coded optimum cache assignment problem is convex that can be further reduced to
a linear program. We present numerical results comparing the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4201</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4201</id><created>2011-09-19</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Park</keyname><forenames>Jaeok</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Production and Network Formation Games with Content Heterogeneity</title><categories>cs.SI cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks (e.g. Facebook, Twitter, Youtube) provide a popular,
cost-effective and scalable framework for sharing user-generated contents. This
paper addresses the intrinsic incentive problems residing in social networks
using a game-theoretic model where individual users selfishly trade off the
costs of forming links (i.e. whom they interact with) and producing contents
personally against the potential rewards from doing so. Departing from the
assumption that contents produced by difference users is perfectly
substitutable, we explicitly consider heterogeneity in user-generated contents
and study how it influences users' behavior and the structure of social
networks. Given content heterogeneity, we rigorously prove that when the
population of a social network is sufficiently large, every (strict)
non-cooperative equilibrium should consist of either a symmetric network
topology where each user produces the same amount of content and has the same
degree, or a two-level hierarchical topology with all users belonging to either
of the two types: influencers who produce large amounts of contents and
subscribers who produce small amounts of contents and get most of their
contents from influencers. Meanwhile, the law of the few disappears in such
networks. Moreover, we prove that the social optimum is always achieved by
networks with symmetric topologies, where the sum of users' utilities is
maximized. To provide users with incentives for producing and mutually sharing
the socially optimal amount of contents, a pricing scheme is proposed, with
which we show that the social optimum can be achieved as a non-cooperative
equilibrium with the pricing of content acquisition and link formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4221</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4221</id><created>2011-09-20</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Three Cases of Connectivity and Global Information Transfer in Robot
  Swarms</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider three different cases of robot-robot interactions
and resulting global information transfer in robot swarms. These mechanisms
define cooperative properties of the system and can be used for designing
collective behavior. These three cases are demonstrated and discussed based on
experiments in a swarm of microrobots &quot;Jasmine&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4240</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4240</id><created>2011-09-20</created><authors><author><keyname>Plantikow</keyname><forenames>Stefan</forenames></author></authors><title>Actor Continuation Passing: Efficient and Extensible Request Routing for
  Event-Driven Architectures</title><categories>cs.PL cs.DC cs.SE</categories><acm-class>H.2.4; D.1.3; D.3.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The logic for handling of application requests to a staged, event-driven
architecture is often distributed over different portions of the source code.
This complicates changing and understanding the flow of events in the system.
The article presents an approach that extracts request handling logic from
regular stage functionality into a set of request scripts. These scripts are
executed step-wise by sending continuations that encapsulate their request's
current execution state to stages for local processing and optional forwarding
of follow-up continuations. A new internal domain specific language (DSL) that
aims to simplify writing of request scripts is described along with its
implementation for the scala actors library. Evaluation results indicate that
request handling with actor continuations performs about equally or better
compared to using separate stages for request handling logic for scripts of at
least 3 sequential steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4244</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4244</id><created>2011-09-20</created><authors><author><keyname>Pucha&#x142;a</keyname><forenames>Zbigniew</forenames></author><author><keyname>Miszczak</keyname><forenames>Jaros&#x142;aw Adam</forenames></author></authors><title>Symbolic integration with respect to the Haar measure on the unitary
  group in Mathematica</title><categories>physics.comp-ph cs.SC quant-ph</categories><comments>17 pages, no figures, software available at:
  http://zksi.iitis.gliwice.pl/wiki/projects:intu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present IntU package for Mathematica computer algebra system. The
presented package performs a symbolic integration of polynomial functions over
the unitary group with respect to unique normalized Haar measure. We describe a
number of special cases which can be used to optimize the calculation speed for
some classes of integrals. We also provide some examples of usage of the
presented package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4250</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4250</id><created>2011-09-20</created><authors><author><keyname>Galla</keyname><forenames>Tobias</forenames></author><author><keyname>Farmer</keyname><forenames>J. Doyne</forenames></author></authors><title>Complex dynamics in learning complicated games</title><categories>nlin.CD cs.GT physics.soc-ph</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game theory is the standard tool used to model strategic interactions in
evolutionary biology and social science. Traditional game theory studies the
equilibria of simple games. But is traditional game theory applicable if the
game is complicated, and if not, what is? We investigate this question here,
defining a complicated game as one with many possible moves, and therefore many
possible payoffs conditional on those moves. We investigate two-person games in
which the players learn based on experience. By generating games at random we
show that under some circumstances the strategies of the two players converge
to fixed points, but under others they follow limit cycles or chaotic
attractors. The dimension of the chaotic attractors can be very high, implying
that the dynamics of the strategies are effectively random. In the chaotic
regime the payoffs fluctuate intermittently, showing bursts of rapid change
punctuated by periods of quiescence, similar to what is observed in fluid
turbulence and financial markets. Our results suggest that such intermittency
is a highly generic phenomenon, and that there is a large parameter regime for
which complicated strategic interactions generate inherently unpredictable
behavior that is best described in the language of dynamical systems theory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4257</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4257</id><created>2011-09-20</created><authors><author><keyname>Dutta</keyname><forenames>Ruma</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Offering A Product Recommendation System in E-commerce</title><categories>cs.IR</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a number of explicit and implicit ratings in product
recommendation system for Business-to-customer e-commerce purposes. The system
recommends the products to a new user. It depends on the purchase pattern of
previous users whose purchase pattern is close to that of a user who asks for a
recommendation. The system is based on weighted cosine similarity measure to
find out the closest user profile among the profiles of all users in database.
It also implements Association rule mining rule in recommending the products.
Also, this product recommendation system takes into consideration the time of
transaction of purchasing the items, thus eliminating sequence recognition
problem. Experimental result shows for implicit rating, the proposed method
gives acceptable performance in recommending the products. It also shows
introduction of association rule improves the performance measure of
recommendation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4288</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4288</id><created>2011-09-20</created><updated>2012-04-16</updated><authors><author><keyname>Zeng</keyname><forenames>Qiang</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaorui</forenames></author><author><keyname>Zhuge</keyname><forenames>Hai</forenames></author></authors><title>Adding Logical Operators to Tree Pattern Queries on Graph-Structured
  Data</title><categories>cs.DB</categories><comments>16 pages</comments><journal-ref>PVLDB 5(8):728-739, 2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As data are increasingly modeled as graphs for expressing complex
relationships, the tree pattern query on graph-structured data becomes an
important type of queries in real-world applications. Most practical query
languages, such as XQuery and SPARQL, support logical expressions using
logical-AND/OR/NOT operators to define structural constraints of tree patterns.
In this paper, (1) we propose generalized tree pattern queries (GTPQs) over
graph-structured data, which fully support propositional logic of structural
constraints. (2) We make a thorough study of fundamental problems including
satisfiability, containment and minimization, and analyze the computational
complexity and the decision procedures of these problems. (3) We propose a
compact graph representation of intermediate results and a pruning approach to
reduce the size of intermediate results and the number of join operations --
two factors that often impair the efficiency of traditional algorithms for
evaluating tree pattern queries. (4) We present an efficient algorithm for
evaluating GTPQs using 3-hop as the underlying reachability index. (5)
Experiments on both real-life and synthetic data sets demonstrate the
effectiveness and efficiency of our algorithm, from several times to orders of
magnitude faster than state-of-the-art algorithms in terms of evaluation time,
even for traditional tree pattern queries with only conjunctive operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4299</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4299</id><created>2011-09-20</created><updated>2012-03-16</updated><authors><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>One-bit compressed sensing by linear programming</title><categories>cs.IT math.IT math.PR</categories><comments>15 pages, 1 figure, to appear in CPAM. Small changes based on referee
  comments</comments><msc-class>94A12 (Primary), 60D05, 90C25 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first computationally tractable and almost optimal solution to
the problem of one-bit compressed sensing, showing how to accurately recover an
s-sparse vector x in R^n from the signs of O(s log^2(n/s)) random linear
measurements of x. The recovery is achieved by a simple linear program. This
result extends to approximately sparse vectors x. Our result is universal in
the sense that with high probability, one measurement scheme will successfully
recover all sparse vectors simultaneously. The argument is based on solving an
equivalent geometric problem on random hyperplane tessellations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4305</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4305</id><created>2011-09-20</created><authors><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>Braunstein</keyname><forenames>Lidia A.</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>Strategy of Competition between Two Groups based on a Contrarian Opinion
  Model</title><categories>physics.data-an cs.SI physics.soc-ph</categories><doi>10.1103/PhysRevE.84.066101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a contrarian opinion (CO) model in which a fraction p of
contrarians within a group holds a strong opinion opposite to the opinion held
by the rest of the group. At the initial stage, stable clusters of two
opinions, A and B exist. Then we introduce contrarians which hold a strong B
opinion into the opinion A group. Through their interactions, the contrarians
are able to decrease the size of the largest A opinion cluster, and even
destroy it. We see this kind of method in operation, e.g when companies send
free new products to potential customers in order to convince them to adopt the
product and influence others. We study the CO model, using two different
strategies, on both ER and scale-free networks. In strategy I, the contrarians
are positioned at random. In strategy II, the contrarians are chosen to be the
highest degrees nodes. We find that for both strategies the size of the largest
A cluster decreases to zero as p increases as in a phase transition. At a
critical threshold value p_c the system undergoes a second-order phase
transition that belongs to the same universality class of mean field
percolation. We find that even for an ER type model, where the degrees of the
nodes are not so distinct, strategy II is significantly more effctive in
reducing the size of the largest A opinion cluster and, at very small values of
p, the largest A opinion cluster is destroyed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4306</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4306</id><created>2011-09-20</created><updated>2012-04-26</updated><authors><author><keyname>Mehta</keyname><forenames>Neil</forenames></author><author><keyname>Duel-Hallen</keyname><forenames>Alexandra</forenames></author><author><keyname>Wang</keyname><forenames>Wenye</forenames></author></authors><title>Enabling Adaptive Rate and Relay Selection for 802.11 Mobile Ad Hoc
  Networks</title><categories>cs.NI</categories><comments>Accepted for publication to the Wireless Communications Symposium at
  ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad hoc networks (MANETs) are self-configuring wireless networks that
lack permanent infrastructure and are formed among mobile nodes on demand.
Rapid node mobility results in dramatic channel variation, or fading, that
degrades MANET performance. Employing channel state information (CSI) at the
transmitter can improve the throughput of routing and medium access control
(MAC) protocols for mobile ad hoc networks. Several routing algorithms in the
literature explicitly incorporate the fading signal strength into the routing
metric, thus selecting the routes with strong channel conditions. While these
studies show that adaptation to the time-variant channel gain is beneficial in
MANETs, they do not address the effect of the outdated fading CSI at the
transmitter. For realistic mobile node speeds, the channel gain is rapidly
varying, and becomes quickly outdated due the feedback delay. We analyze the
link throughput of joint rate adaptation and adaptive relay selection in the
presence of imperfect CSI. Moreover, for an 802.11 network that employs
geographic opportunistic routing with adaptive rate and relay selection, we
propose a novel method to reduce the effect of the feedback delay at the MAC
layer in the presence of Rayleigh fading. This method exploits channel
reciprocity and fading prediction and does not require significant modification
to the existing 802.11 frame structure. Extensive network simulations
demonstrate that the proposed approach significantly improves the throughput,
delay, and packet delivery ratio for high mobile velocities relative to
previously proposed approaches that employ outdated CSI at the transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4311</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4311</id><created>2011-09-20</created><authors><author><keyname>Beveridge</keyname><forenames>Andrew</forenames></author><author><keyname>Bradonji&#x107;</keyname><forenames>Milan</forenames></author></authors><title>On the Mixing Time of Geographical Threshold Graphs</title><categories>math.PR cs.DM math.CO</categories><doi>10.1016/j.disc.2011.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the mixing time of random graphs in the $d$-dimensional toric unit
cube $[0,1]^d$ generated by the geographical threshold graph (GTG) model, a
generalization of random geometric graphs (RGG). In a GTG, nodes are
distributed in a Euclidean space, and edges are assigned according to a
threshold function involving the distance between nodes as well as randomly
chosen node weights, drawn from some distribution. The connectivity threshold
for GTGs is comparable to that of RGGs, essentially corresponding to a
connectivity radius of $r=(\log n/n)^{1/d}$. However, the degree distributions
at this threshold are quite different: in an RGG the degrees are essentially
uniform, while RGGs have heterogeneous degrees that depend upon the weight
distribution. Herein, we study the mixing times of random walks on
$d$-dimensional GTGs near the connectivity threshold for $d \geq 2$. If the
weight distribution function decays with $\mathbb{P}[W \geq x] =
O(1/x^{d+\nu})$ for an arbitrarily small constant $\nu&gt;0$ then the mixing time
of GTG is $\mixbound$. This matches the known mixing bounds for the
$d$-dimensional RGG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4314</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4314</id><created>2011-09-20</created><updated>2013-11-07</updated><authors><author><keyname>Abdoli</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Ghasemi</keyname><forenames>Akbar</forenames></author><author><keyname>Khandani</keyname><forenames>Amir Keyvan</forenames></author></authors><title>On the Degrees of Freedom of $K$-User SISO Interference and X Channels
  with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, 2 tables; Published in IEEE Transactions on
  Information Theory, Oct. 2013</comments><journal-ref>IEEE Trans. Inf. Theory, vol. 59, no. 10, pp. 6542-6561, 2013</journal-ref><doi>10.1109/TIT.2013.2268154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $K$-user single-input single-output (SISO) AWGN interference channel and
$2\times K$ SISO AWGN X channel are considered where the transmitters have the
delayed channel state information (CSI) through noiseless feedback links.
Multi-phase transmission schemes are proposed for both channels which possess
novel ingredients, namely, multi-phase partial interference nulling,
distributed interference management via user scheduling, and distributed
higher-order symbol generation. The achieved degrees of freedom (DoF) values
are greater than the best previously known DoFs for both channels with delayed
CSI at transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4323</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4323</id><created>2011-09-20</created><authors><author><keyname>Poteaux</keyname><forenames>Adrien</forenames></author><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author></authors><title>On the complexity of computing with zero-dimensional triangular sets</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of some fundamental operations for triangular sets in
dimension zero. Using Las-Vegas algorithms, we prove that one can perform such
operations as change of order, equiprojectable decomposition, or quasi-inverse
computation with a cost that is essentially that of modular composition. Over
an abstract field, this leads to a subquadratic cost (with respect to the
degree of the underlying algebraic set). Over a finite field, in a boolean RAM
model, we obtain a quasi-linear running time using Kedlaya and Umans' algorithm
for modular composition. Conversely, we also show how to reduce the problem of
modular composition to change of order for triangular sets, so that all these
problems are essentially equivalent. Our algorithms are implemented in Maple;
we present some experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4335</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4335</id><created>2011-07-28</created><updated>2015-05-05</updated><authors><author><keyname>Camps</keyname><forenames>Rosa</forenames></author><author><keyname>Mora</keyname><forenames>Xavier</forenames></author><author><keyname>Saumell</keyname><forenames>Laia</forenames></author></authors><title>Social choice rules driven by propositional logic</title><categories>cs.AI</categories><comments>The title has been changed</comments><msc-class>91B14, 91B06, 03B42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several rules for social choice are examined from a unifying point of view
that looks at them as procedures for revising a system of degrees of belief in
accordance with certain specified logical constraints. Belief is here a social
attribute, its degrees being measured by the fraction of people who share a
given opinion. Different known rules and some new ones are obtained depending
on which particular constraints are assumed. These constraints allow to model
different notions of choiceness. In particular, we give a new method to deal
with approval-disapproval-preferential voting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4347</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4347</id><created>2011-09-20</created><authors><author><keyname>Akama</keyname><forenames>Yohji</forenames></author><author><keyname>Irie</keyname><forenames>Kei</forenames></author></authors><title>VC dimension of ellipsoids</title><categories>math.CO cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will establish that the VC dimension of the class of d-dimensional
ellipsoids is (d^2+3d)/2, and that maximum likelihood estimate with N-component
d-dimensional Gaussian mixture models induces a geometric class having VC
dimension at least N(d^2+3d)/2.
  Keywords: VC dimension; finite dimensional ellipsoid; Gaussian mixture model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4350</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4350</id><created>2011-09-20</created><authors><author><keyname>Wang</keyname><forenames>Chenwei</forenames></author><author><keyname>Gou</keyname><forenames>Tiangao</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Subspace Alignment Chains and the Degrees of Freedom of the Three-User
  MIMO Interference Channel</title><categories>cs.IT math.IT</categories><comments>84 pages, 19 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the 3 user M_T x M_R MIMO interference channel has
d(M,N)=min(M/(2-1/k),N/(2+1/k)) degrees of freedom (DoF) normalized by time,
frequency, and space dimensions, where M=min(M_T,M_R), N=max(M_T,M_R),
k=ceil{M/(N-M)}. While the DoF outer bound is established for every M_T, M_R
value, the achievability is established in general subject to normalization
with respect to spatial-extensions. Given spatial-extensions, the achievability
relies only on linear beamforming based interference alignment schemes with no
need for time/frequency extensions. In the absence of spatial extensions, we
show through examples how essentially the same scheme may be applied over
time/frequency extensions. The central new insight to emerge from this work is
the notion of subspace alignment chains as DoF bottlenecks.
  The DoF value d(M,N) is a piecewise linear function of M,N, with either M or
N being the bottleneck within each linear segment. The corner points of these
piecewise linear segments correspond to A={1/2,2/3,3/4,...} and
B={1/3,3/5,5/7,...}. The set A contains all values of M/N and only those for
which there is redundancy in both M and N. The set B contains all values of M/N
and only those for which there is no redundancy in either M or N.
  Our results settle the feasibility of linear interference alignment,
introduced by Cenk et al., for the 3 user M_T x M_R MIMO interference channel,
completely for all values of M_T, M_R. Specifically, the linear interference
alignment problem (M_T x M_R, d)^3 (as defined in previous work by Cenk et al.)
is feasible if and only if d&lt;=floor{d(M,N)}. With and only with the exception
of the values M/N\in B, we show that for every M/N value there are proper
systems that are not feasible.
  Our results show that M/N\in A are the only values for which there is no DoF
benefit of joint processing among co-located antennas at the transmitters or
receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4351</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4351</id><created>2011-09-20</created><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Helmstetter</keyname><forenames>Claude</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Joloboff</keyname><forenames>Vania</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Monin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>LIAMA, UJF</affiliation></author><author><keyname>Shi</keyname><forenames>Xiaomu</forenames><affiliation>LIAMA</affiliation></author></authors><title>Designing a CPU model: from a pseudo-formal document to fast code</title><categories>cs.SE cs.AR</categories><comments>3rd Workshop on: Rapid Simulation and Performance Evaluation: Methods
  and Tools (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For validating low level embedded software, engineers use simulators that
take the real binary as input. Like the real hardware, these full-system
simulators are organized as a set of components. The main component is the CPU
simulator (ISS), because it is the usual bottleneck for the simulation speed,
and its development is a long and repetitive task. Previous work showed that an
ISS can be generated from an Architecture Description Language (ADL). In the
work reported in this paper, we generate a CPU simulator directly from the
pseudo-formal descriptions of the reference manual. For each instruction, we
extract the information describing its behavior, its binary encoding, and its
assembly syntax. Next, after automatically applying many optimizations on the
extracted information, we generate a SystemC/TLM ISS. We also generate tests
for the decoder and a formal specification in Coq. Experiments show that the
generated ISS is as fast and stable as our previous hand-written ISS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4353</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4353</id><created>2011-09-20</created><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Kirchner</keyname><forenames>Claude</forenames><affiliation>INRIA Bordeaux Sud-Ouest</affiliation></author><author><keyname>Riba</keyname><forenames>Colin</forenames><affiliation>LIP</affiliation></author></authors><title>On the confluence of lambda-calculus with conditional rewriting</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>Theoretical Computer Science 411, 37 (2010) 3301-3327</journal-ref><doi>10.1016/j.tcs.2009.07.058</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The confluence of untyped \lambda-calculus with unconditional rewriting is
now well un- derstood. In this paper, we investigate the confluence of
\lambda-calculus with conditional rewriting and provide general results in two
directions. First, when conditional rules are algebraic. This extends results
of M\&quot;uller and Dougherty for unconditional rewriting. Two cases are
considered, whether \beta-reduction is allowed or not in the evaluation of
conditions. Moreover, Dougherty's result is improved from the assumption of
strongly normalizing \beta-reduction to weakly normalizing \beta-reduction. We
also provide examples showing that outside these conditions, modularity of
confluence is difficult to achieve. Second, we go beyond the algebraic
framework and get new confluence results using a restricted notion of
orthogonality that takes advantage of the conditional part of rewrite rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4356</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4356</id><created>2011-09-20</created><updated>2012-12-12</updated><authors><author><keyname>Hirschowitz</keyname><forenames>Tom</forenames><affiliation>LAMA</affiliation></author><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>LIG</affiliation></author></authors><title>Innocent strategies as presheaves and interactive equivalences for CCS
  (expanded version)</title><categories>cs.LO</categories><comments>53 pages. Expanded version of ICE '11 paper DOI 10.4204/EPTCS.59.2</comments><proxy>ccsd</proxy><journal-ref>Scientific Annals of Computer Science 22, 1 (2012) 147-199</journal-ref><doi>10.7561/SACS.2012.1.147</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seeking a general framework for reasoning about and comparing programming
languages, we derive a new view of Milner's CCS. We construct a category E of
'plays', and a subcategory V of 'views'. We argue that presheaves on V
adequately represent 'innocent' strategies, in the sense of game semantics. We
equip innocent strategies with a simple notion of interaction. We then prove
decomposition results for innocent strategies, and, restricting to presheaves
of finite ordinals, prove that innocent strategies are a final coalgebra for a
polynomial functor derived from the game. This leads to a translation of CCS
with recursive equations. Finally, we propose a notion of 'interactive
equivalence' for innocent strategies, which is close in spirit to Beffara's
interpretation of testing equivalences in concurrency theory. In this
framework, we consider analogues of fair testing and must testing. We show that
must testing is strictly finer in our model than in CCS, since it avoids what
we call 'spatial unfairness'. Still, it differs from fair testing, and we show
that it coincides with a relaxed form of fair testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4357</identifier>
 <datestamp>2011-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4357</id><created>2011-09-20</created><authors><author><keyname>Suzuki</keyname><forenames>Sho</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Kusakari</keyname><forenames>Keiichirou</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA</affiliation></author></authors><title>Argument filterings and usable rules in higher-order rewrite systems</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>IPSJ Transactions on Programming 4, 2 (2011) 1-12</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The static dependency pair method is a method for proving the termination of
higher-order rewrite systems a la Nipkow. It combines the dependency pair
method introduced for first-order rewrite systems with the notion of strong
computability introduced for typed lambda-calculi. Argument filterings and
usable rules are two important methods of the dependency pair framework used by
current state-of-the-art first-order automated termination provers. In this
paper, we extend the class of higher-order systems on which the static
dependency pair method can be applied. Then, we extend argument filterings and
usable rules to higher-order rewriting, hence providing the basis for a
powerful automated termination prover for higher-order rewrite systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4373</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4373</id><created>2011-09-20</created><authors><author><keyname>Almeida</keyname><forenames>Paulo S.</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author><author><keyname>Jesus</keyname><forenames>Paulo</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author></authors><title>Fault-Tolerant Aggregation: Flow-Updating Meets Mass-Distribution</title><categories>cs.DC cs.DS</categories><comments>18 pages, 5 figures, To appear in OPODIS 2011</comments><doi>10.1007/978-3-642-25873-2_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow-Updating (FU) is a fault-tolerant technique that has proved to be
efficient in practice for the distributed computation of aggregate functions in
communication networks where individual processors do not have access to global
information. Previous distributed aggregation protocols, based on repeated
sharing of input values (or mass) among processors, sometimes called
Mass-Distribution (MD) protocols, are not resilient to communication failures
(or message loss) because such failures yield a loss of mass. In this paper, we
present a protocol which we call Mass-Distribution with Flow-Updating (MDFU).
We obtain MDFU by applying FU techniques to classic MD. We analyze the
convergence time of MDFU showing that stochastic message loss produces low
overhead. This is the first convergence proof of an FU-based algorithm. We
evaluate MDFU experimentally, comparing it with previous MD and FU protocols,
and verifying the behavior predicted by the analysis. Finally, given that MDFU
incurs a fixed deviation proportional to the message-loss rate, we adjust the
accuracy of MDFU heuristically in a new protocol called MDFU with Linear
Prediction (MDFU-LP). The evaluation shows that both MDFU and MDFU-LP behave
very well in practice, even under high rates of message loss and even changing
the input values dynamically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4404</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4404</id><created>2011-09-06</created><updated>2012-02-23</updated><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>Ghini</keyname><forenames>Vittorio</forenames></author></authors><title>Mitigation of Random Query String DoS via Gossip</title><categories>cs.CR</categories><comments>a revised version will appear in Proc. of the 6th International
  Conference on Information Systems, Technology and Management (ICISTM-2012),
  Grenoble, France, Springer Series in Communications in Computer and
  Information Science (CCIS), March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a mitigation scheme to cope with the random query string
Denial of Service (DoS) attack, which is based on a vulnerability of current
Content Delivery Networks (CDNs). The attack exploits the fact that edge
servers composing a CDN, receiving an HTTP request for a resource with an
appended random query string never saw before, ask the origin server for a
(novel) copy of the resource. Such characteristics can be employed to take an
attack against the origin server by exploiting edge servers. Our strategy
adopts a simple gossip protocol executed by edge servers to detect the attack.
Based on such a detection, countermeasures can be taken to protect the origin
server and the CDN against the attack. We provide simulation results that show
the viability of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4424</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4424</id><created>2011-09-20</created><updated>2012-06-06</updated><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Sausset</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Sun</keyname><forenames>Yifan</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Statistical physics-based reconstruction in compressed sensing</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>20 pages, 8 figures, 3 tables. Related codes and data are available
  at http://aspics.krzakala.org</comments><journal-ref>Phys. Rev. X 2, 021005 (2012)</journal-ref><doi>10.1103/PhysRevX.2.021005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is triggering a major evolution in signal acquisition. It
consists in sampling a sparse signal at low rate and later using computational
power for its exact reconstruction, so that only the necessary information is
measured. Currently used reconstruction techniques are, however, limited to
acquisition rates larger than the true density of the signal. We design a new
procedure which is able to reconstruct exactly the signal with a number of
measurements that approaches the theoretical limit in the limit of large
systems. It is based on the joint use of three essential ingredients: a
probabilistic approach to signal reconstruction, a message-passing algorithm
adapted from belief propagation, and a careful design of the measurement matrix
inspired from the theory of crystal nucleation. The performance of this new
algorithm is analyzed by statistical physics methods. The obtained improvement
is confirmed by numerical studies of several cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4433</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4433</id><created>2011-09-20</created><authors><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Cohen</keyname><forenames>Johanne</forenames></author><author><keyname>Koegler</keyname><forenames>Xavier</forenames></author><author><keyname>Rabie</keyname><forenames>Mikael</forenames></author></authors><title>Asymetric Pavlovian Populations</title><categories>cs.DC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population protocols have been introduced by Angluin et al. as a model of
networks consisting of very limited mobile agents that interact in pairs but
with no control over their own movement. A collection of anonymous agents,
modeled by finite automata, interact pairwise according to some rules that
update their states. Predicates on the initial configurations that can be
computed by such protocols have been characterized as semi-linear predicates.
In an orthogonal way, several distributed systems have been termed in
literature as being realizations of games in the sense of game theory. We
investigate under which conditions population protocols, or more generally
pairwise interaction rules, correspond to games. We show that restricting to
asymetric games is not really a restric- tion: all predicates computable by
protocols can actually be computed by protocols corresponding to games, i.e.
any semi-linear predicate can be computed by a Pavlovian population
multi-protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4455</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4455</id><created>2011-09-20</created><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author></authors><title>Periodic sequences with stable $k$-error linear complexity</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The linear complexity of a sequence has been used as an important measure of
keystream strength, hence designing a sequence which possesses high linear
complexity and $k$-error linear complexity is a hot topic in cryptography and
communication. Niederreiter first noticed many periodic sequences with high
$k$-error linear complexity over GF(q). In this paper, the concept of stable
$k$-error linear complexity is presented to study sequences with high $k$-error
linear complexity. By studying linear complexity of binary sequences with
period $2^n$, the method using cube theory to construct sequences with maximum
stable $k$-error linear complexity is presented. It is proved that a binary
sequence with period $2^n$ can be decomposed into some disjoint cubes. The cube
theory is a new tool to study $k$-error linear complexity. Finally, it is
proved that the maximum $k$-error linear complexity is $2^n-(2^l-1)$ over all
$2^n$-periodic binary sequences, where $2^{l-1}\le k&lt;2^{l}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4457</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4457</id><created>2011-09-20</created><authors><author><keyname>Lee</keyname><forenames>Taeyoung</forenames></author><author><keyname>Leok</keyname><forenames>Melvin</forenames></author><author><keyname>McClamroch</keyname><forenames>N. Harris</forenames></author></authors><title>Nonlinear Robust Tracking Control of a Quadrotor UAV on SE(3)</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides nonlinear tracking control systems for a quadrotor
unmanned aerial vehicle (UAV) that are robust to bounded uncertainties. A
mathematical model of a quadrotor UAV is defined on the special Euclidean
group, and nonlinear output-tracking controllers are developed to follow (1) an
attitude command, and (2) a position command for the vehicle center of mass.
The controlled system has the desirable properties that the tracking errors are
uniformly ultimately bounded, and the size of the ultimate bound can be
arbitrarily reduced by control system parameters. Numerical examples
illustrating complex maneuvers are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4459</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4459</id><created>2011-09-20</created><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author><author><keyname>Xiong</keyname><forenames>Wei</forenames></author></authors><title>An Algorithm for Computing $m$-Tight Error Linear Complexity of
  Sequences over $GF(p^{m})$ with Period $p^{m}$</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The linear complexity (LC) of a sequence has been used as a convenient
measure of the randomness of a sequence. Based on the theories of linear
complexity, $k$-error linear complexity, the minimum error and the $k$-error
linear complexity profile, the notion of $m$-tight error linear complexity is
presented. An efficient algorithm for computing $m$-tight error linear
complexity is derived from the algorithm for computing $k$-error linear
complexity of sequences over GF($p^{m}$) with period $p^n$, where $p$ is a
prime. The validity of the algorithm is shown. The algorithm is also realized
with C language, and an example is presented to illustrate the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4460</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4460</id><created>2011-09-20</created><authors><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author></authors><title>A Simple Linear-Space Data Structure for Constant-Time Range Minimum
  Query</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the range minimum query problem and present a new O(n)-space data
structure that supports queries in O(1) time. Although previous data structures
exist whose asymptotic bounds match ours, our goal is to introduce a new
solution that is simple, intuitive, and practical without increasing costs for
query time or space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4467</identifier>
 <datestamp>2011-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4467</id><created>2011-09-20</created><updated>2011-12-20</updated><authors><author><keyname>Van Horn</keyname><forenames>David</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author></authors><title>Pushdown Abstractions of JavaScript</title><categories>cs.PL</categories><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a family of program analyses for JavaScript that make no
approximation in matching calls with returns, exceptions with handlers, and
breaks with labels. We do so by starting from an established reduction
semantics for JavaScript and systematically deriving its intensional abstract
interpretation. Our first step is to transform the semantics into an equivalent
low-level abstract machine: the JavaScript Abstract Machine (JAM). We then give
an infinite-state yet decidable pushdown machine whose stack precisely models
the structure of the concrete program stack. The precise model of stack
structure in turn confers precise control-flow analysis even in the presence of
control effects, such as exceptions and finally blocks. We give pushdown
generalizations of traditional forms of analysis such as k-CFA, and prove the
pushdown framework for abstract interpretation is sound and computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4474</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4474</id><created>2011-09-21</created><authors><author><keyname>Ling</keyname><forenames>Amy Poh Ai</forenames></author><author><keyname>Masao</keyname><forenames>Mukaidono</forenames></author></authors><title>Smart Grid Information Security (IS) Functional Requirement</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1108.0267</comments><journal-ref>Int. J. Emerg. Sci., 1(3), 371-386, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is important to implement safe smart grid environment to enhance people's
lives and livelihoods. This paper provides information on smart grid IS
functional requirement by illustrating some discussion points to the sixteen
identified requirements. This paper introduces the smart grid potential hazards
that can be referred as a triggering factor to improve the system and security
of the entire grid. The background of smart information infrastructure and the
needs for smart grid IS is described with the adoption of hermeneutic circle as
methodology. Grid information technology and security-s session discusses that
grid provides the chance of a simple and transparent access to different
information sources. In addition, the transformation between traditional versus
smart grid networking trend and the IS importance on the communication field
reflects the criticality of grid IS functional requirement identification is
introduces. The smart grid IS functional requirements described in this paper
are general and can be adopted or modified to suit any smart grid system. This
paper has tutorial contents where some related backgrounds were provided,
especially for networking community, covering the cyber security requirement of
smart grid information infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4487</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4487</id><created>2011-09-21</created><authors><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Vilone</keyname><forenames>Daniele</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Angel</forenames></author></authors><title>Disentangling Social and Group heterogeneities: Public Goods games on
  Complex Networks</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 5 figures, 6 formulae</comments><journal-ref>Europhys. Lett., 95, 68003 (2011)</journal-ref><doi>10.1209/0295-5075/95/68003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Letter we present a new perspective for the study of the Public Goods
games on complex networks. The idea of our approach is to consider a realistic
structure for the groups in which Public goods games are played. Instead of
assuming that the social network of contacts self-defines a group structure
with identical topological properties, we disentangle these two interaction
patterns so to deal with systems having groups of definite sizes embedded in
social networks with a tunable degree of heterogeneity. Surpisingly, this
realistic framework, reveals that social heterogeneity may not foster
cooperation depending on the game setting and the updating rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4490</identifier>
 <datestamp>2012-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4490</id><created>2011-09-21</created><authors><author><keyname>Savitha</keyname><forenames>K.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>C.</forenames></author></authors><title>Vertical Handover decision schemes using SAW and WPM for Network
  selection in Heterogeneous Wireless Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1108.0141</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seamless continuity is the main goal and challenge in fourth generation
Wireless networks (FGWNs), to achieve seamless connectivity &quot;HANDOVER&quot;
technique is used,Handover mechanism are mainly used when a mobile terminal(MT)
is in overlapping area for service continuity. In Heterogeneous wireless
networks main challenge is continual connection among the different networks
like WiFi, WiMax, WLAN, WPAN etc. In this paper, Vertical handover decision
schemes are compared, Simple Additive Weighting method (SAW) and Weighted
product model (WPM) are used to choose the best network from the available
Visitor networks(VTs) for the continuous connection by the mobile terminal. In
our work we mainly concentrated to the handover decision phase and to reduce
the processing delay in the period of handover. In this paper both SAW and WPM
methods are compared with the Qos parameters of the mobile terminal (MT) to
connect with the best network. Keywords: Handover, Vertical handover decision
schemes, Simple additive weighting, Weight product method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4499</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4499</id><created>2011-09-21</created><authors><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author><author><keyname>Voroninski</keyname><forenames>Vladislav</forenames></author></authors><title>PhaseLift: Exact and Stable Signal Recovery from Magnitude Measurements
  via Convex Programming</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we wish to recover a signal x in C^n from m intensity measurements of
the form |&lt;x,z_i&gt;|^2, i = 1, 2,..., m; that is, from data in which phase
information is missing. We prove that if the vectors z_i are sampled
independently and uniformly at random on the unit sphere, then the signal x can
be recovered exactly (up to a global phase factor) by solving a convenient
semidefinite program---a trace-norm minimization problem; this holds with large
probability provided that m is on the order of n log n, and without any
assumption about the signal whatsoever. This novel result demonstrates that in
some instances, the combinatorial phase retrieval problem can be solved by
convex programming techniques. Finally, we also prove that our methodology is
robust vis a vis additive noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4503</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4503</id><created>2011-09-21</created><authors><author><keyname>Tamayo</keyname><forenames>Alain</forenames></author><author><keyname>Viciano</keyname><forenames>Pablo</forenames></author><author><keyname>Granell</keyname><forenames>Carlos</forenames></author><author><keyname>Huerta</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Empirical study of sensor observation services server instances</title><categories>cs.NI cs.ET cs.SE</categories><comments>25 pages, 11 tables, 6 figures</comments><journal-ref>Advancing Geoinformation Science for a Changing World, Lecture
  Notes in Geoinformation and Cartography, pp. 185-210, 2011</journal-ref><doi>10.1007/978-3-642-19789-5_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of Sensor Observation Service (SOS) instances available online has
been increasing in the last few years. The SOS specification standardises
interfaces and data formats for exchanging sensor-related in-formation between
information providers and consumers. SOS in conjunction with other
specifications in the Sensor Web Enablement initiative, at-tempts to realise
the Sensor Web vision, a worldwide system where sensor networks of any kind are
interconnected. In this paper we present an empirical study of actual instances
of servers implementing SOS. The study focuses mostly in which parts of the
specification are more frequently included in real implementations, and how
exchanged messages follows the structure defined by XML Schema files. Our
findings can be of practical use when implementing servers and clients based on
the SOS specification, as they can be optimized for common scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4521</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4521</id><created>2011-09-21</created><authors><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Criado</keyname><forenames>Regino</forenames></author><author><keyname>Romance</keyname><forenames>Miguel</forenames></author><author><keyname>Russo</keyname><forenames>Giovanni</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Controlling centrality in complex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>7 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral centrality measures allow to identify influential individuals in
social groups, to rank Web pages by their popularity, and even to determine the
impact of scientific researches. The centrality score of a node within a
network crucially depends on the entire pattern of connections, so that the
usual approach is to compute the node centralities once the network structure
is assigned. We face here with the inverse problem, that is, we study how to
modify the centrality scores of the nodes by acting on the structure of a given
network. We prove that there exist particular subsets of nodes, called
controlling sets, which can assign any prescribed set of centrality values to
all the nodes of a graph, by cooperatively tuning the weights of their
out-going links. We show that many large networks from the real world have
surprisingly small controlling sets, containing even less than 5-10% of the
nodes. These results suggest that rankings obtained from spectral centrality
measures have to be considered with extreme care, since they can be easily
controlled and even manipulated by a small group of nodes acting in a
coordinated way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4530</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4530</id><created>2011-09-21</created><authors><author><keyname>Dudziuk</keyname><forenames>Grzegorz</forenames></author><author><keyname>Niezg&#xf3;dka</keyname><forenames>Marek</forenames></author></authors><title>Closed-loop control of a reaction-diffusion system</title><categories>math.OC cs.SY</categories><msc-class>35Q79, 35Q93</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system of a parabolic partial differential equation coupled with ordinary
differential inclusions that arises from a closed-loop control problem for a
thermodynamic process governed by the Allen-Cahn diffusion reaction model is
studied. A feedback law for the closed-loop control is proposed and implemented
in the case of a finite number of control devices located inside the process
domain basing on the process dynamics observed at a finite number of
measurement points. The existence of solutions to the discussed system of
differential equations is proved with the use of a generalization of the
Kakutani fixed point theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4531</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4531</id><created>2011-09-21</created><authors><author><keyname>Kujala</keyname><forenames>Janne V.</forenames></author><author><keyname>Keurulainen</keyname><forenames>Aleksi</forenames></author></authors><title>A Probabilistic Approach to Pronunciation by Analogy</title><categories>cs.CL</categories><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between written and spoken words is convoluted in languages
with a deep orthography such as English and therefore it is difficult to devise
explicit rules for generating the pronunciations for unseen words.
Pronunciation by analogy (PbA) is a data-driven method of constructing
pronunciations for novel words from concatenated segments of known words and
their pronunciations. PbA performs relatively well with English and outperforms
several other proposed methods. However, the best published word accuracy of
65.5% (for the 20,000 word NETtalk corpus) suggests there is much room for
improvement in it.
  Previous PbA algorithms have used several different scoring strategies such
as the product of the frequencies of the component pronunciations of the
segments, or the number of different segmentations that yield the same
pronunciation, and different combinations of these methods, to evaluate the
candidate pronunciations. In this article, we instead propose to use a
probabilistically justified scoring rule. We show that this principled approach
alone yields better accuracy (66.21% for the NETtalk corpus) than any
previously published PbA algorithm. Furthermore, combined with certain ad hoc
modifications motivated by earlier algorithms, the performance climbs up to
66.6%, and further improvements are possible by combining this method with
other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4540</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4540</id><created>2011-09-21</created><updated>2012-06-05</updated><authors><author><keyname>Genovese</keyname><forenames>Christopher R.</forenames></author><author><keyname>Perone-Pacifico</keyname><forenames>Marco</forenames></author><author><keyname>Verdinelli</keyname><forenames>Isabella</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Manifold estimation and singular deconvolution under Hausdorff loss</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS994 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS994</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 2, 941-963</journal-ref><doi>10.1214/12-AOS994</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find lower and upper bounds for the risk of estimating a manifold in
Hausdorff distance under several models. We also show that there are close
connections between manifold estimation and the problem of deconvolving a
singular measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4544</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4544</id><created>2011-09-21</created><authors><author><keyname>Barbero-Li&#xf1;&#xe1;n</keyname><forenames>Mar&#xed;a</forenames></author></authors><title>Characterization of accessibility for affine connection control systems
  at some points with nonzero velocity</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affine connection control systems are mechanical control systems that model a
wide range of real systems such as robotic legs, hovercrafts, planar rigid
bodies, rolling pennies, snakeboards and so on. In 1997 the accessibility and a
particular notion of controllability was intrinsically described by A. D. Lewis
and R. Murray at points of zero velocity. Here, we present a novel
generalization of the description of accessibility algebra for those systems at
some points with nonzero velocity as long as the affine connection restricts to
the distribution given by the symmetric closure. The results are used to
describe the accessibility algebra of different mechanical control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4552</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4552</id><created>2011-08-26</created><authors><author><keyname>Kornyushkin</keyname><forenames>A.</forenames></author></authors><title>About a Discrete Cellular Soliton (computer simulation)</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the first time a mathematical object is presented - a reversible cellular
Automaton - with many paradoxical qualities, the main ones among them are: a
frequent quickly return to its original state, the presence of a large number
of conservation laws and paradoxical &quot;fuzzy&quot; symmetries, which connects the
current position of the automaton with its signature Main Integral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4554</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4554</id><created>2011-09-21</created><authors><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author></authors><title>Surface Split Decompositions and Subgraph Isomorphism in Graphs on
  Surfaces</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Subgraph Isomorphism problem asks, given a host graph G on n vertices and
a pattern graph P on k vertices, whether G contains a subgraph isomorphic to P.
The restriction of this problem to planar graphs has often been considered.
After a sequence of improvements, the current best algorithm for planar graphs
is a linear time algorithm by Dorn (STACS '10), with complexity $2^{O(k)}
O(n)$.
  We generalize this result, by giving an algorithm of the same complexity for
graphs that can be embedded in surfaces of bounded genus. At the same time, we
simplify the algorithm and analysis. The key to these improvements is the
introduction of surface split decompositions for bounded genus graphs, which
generalize sphere cut decompositions for planar graphs. We extend the algorithm
for the problem of counting and generating all subgraphs isomorphic to P, even
for the case where P is disconnected. This answers an open question by Eppstein
(SODA '95 / JGAA '99).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4564</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4564</id><created>2011-09-21</created><updated>2011-10-05</updated><authors><author><keyname>Ohannessian</keyname><forenames>Mesrob I.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A.</forenames></author></authors><title>Canonical Estimation in a Rare-Events Regime</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>To be presented at Allerton conference 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general methodology for performing statistical inference within
a `rare-events regime' that was recently suggested by Wagner, Viswanath and
Kulkarni. Our approach allows one to easily establish consistent estimators for
a very large class of canonical estimation problems, in a large alphabet
setting. These include the problems studied in the original paper, such as
entropy and probability estimation, in addition to many other interesting ones.
We particularly illustrate this approach by consistently estimating the size of
the alphabet and the range of the probabilities. We start by proposing an
abstract methodology based on constructing a probability measure with the
desired asymptotic properties. We then demonstrate two concrete constructions
by casting the Good-Turing estimator as a pseudo-empirical measure, and by
using the theory of mixture model estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4565</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4565</id><created>2011-08-26</created><authors><author><keyname>Moharir</keyname><forenames>Minal</forenames></author><author><keyname>Suresh</keyname><forenames>A. V.</forenames></author></authors><title>An Adaptive Technique using Advanced Encryption Standard to Implement
  Hard Disk Security</title><categories>cs.CR</categories><comments>Information Security, Integrity, confidentiality, Authentication,
  Encryption</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of the paper is to study and develop an efficient method
for Hard Disk Drive(HDD) Security using Full Disk Encryption (FDE) with
Advanced Encryption Standards(AES) for data security specifically for Personal
Computers(PCS) and Laptops . The focus of this work is to authenticate and
protect the content of HDD from illegal use. The paper proposes an adaptive
methods for protecting a HDD based on FDE. The proposed method is labeled as
DiskTrust. FDE encrypts entire content or a single volume on your disk.
DiskTrust implements Symmetric key cryptography with, Advanced Encryption
Standards. Finally, the applicability of these methodologies for HDD security
will be evaluated on a set of data files with different key sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4570</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4570</id><created>2011-09-21</created><authors><author><keyname>van Bakel</keyname><forenames>Steffen</forenames></author></authors><title>Reduction in X does not agree with Intersection and Union Types
  (Extended abstract)</title><categories>cs.LO</categories><comments>4th International Workshop on Intersection Types and Related Systems
  (ITRS'08), Turin, Italy, March 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines intersection and union type assignment for the calculus X,
a substitution free language that enjoys the Curry-Howard correspondence with
respect to Gentzen's sequent calculus for classical logic. We show that this
notion is closed for subject-expansion, and show that it needs to be restricted
to satisfy subject-reduction as well, making it unsuitable to define a
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4587</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4587</id><created>2011-09-21</created><updated>2012-03-01</updated><authors><author><keyname>Tavan</keyname><forenames>Mehrnaz</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Karout</keyname><forenames>Johnny</forenames></author></authors><title>Bandlimited Intensity Modulation</title><categories>cs.IT math.IT</categories><comments>28 pages 10 Figures</comments><journal-ref>IEEE Transactions on Communications, vol. 60, no. 11, pp.
  3429-3439, Nov. 2012</journal-ref><doi>10.1109/TCOMM.2012.091712.110496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the design and analysis of a new bandwidth-efficient signaling
method over the bandlimited intensity-modulated direct-detection (IM/DD)
channel is presented. The channel can be modeled as a bandlimited channel with
nonnegative input and additive white Gaussian noise (AWGN). Due to the
nonnegativity constraint, standard methods for coherent bandlimited channels
cannot be applied here. Previously established techniques for the IM/DD channel
require bandwidth twice the required bandwidth over the conventional coherent
channel. We propose a method to transmit without intersymbol interference in a
bandwidth no larger than the bit rate. This is done by combining Nyquist or
root-Nyquist pulses with a constant bias and using higher-order modulation
formats. In fact, we can transmit with a bandwidth equal to that of coherent
transmission. A trade-off between the required average optical power and the
bandwidth is investigated. Depending on the bandwidth required, the most
power-efficient transmission is obtained by the parametric linear pulse, the
so-called &quot;better than Nyquist&quot; pulse, or the root-raised cosine pulse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4590</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4590</id><created>2011-09-21</created><authors><author><keyname>Kim</keyname><forenames>H.</forenames></author><author><keyname>Del Genio</keyname><forenames>C. I.</forenames></author><author><keyname>Bassler</keyname><forenames>K. E.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Z.</forenames></author></authors><title>Constructing and sampling directed graphs with given degree sequences</title><categories>physics.soc-ph cond-mat.stat-mech cs.DS cs.SI</categories><comments>20 pages, 5 figures</comments><doi>10.1088/1367-2630/14/2/023012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interactions between the components of complex networks are often
directed. Proper modeling of such systems frequently requires the construction
of ensembles of digraphs with a given sequence of in- and out-degrees. As the
number of simple labeled graphs with a given degree sequence is typically very
large even for short sequences, sampling methods are needed for statistical
studies. Currently, there are two main classes of methods that generate
samples. One of the existing methods first generates a restricted class of
graphs, then uses a Markov Chain Monte-Carlo algorithm based on edge swaps to
generate other realizations. As the mixing time of this process is still
unknown, the independence of the samples is not well controlled. The other
class of methods is based on the Configuration Model that may lead to
unacceptably many sample rejections due to self-loops and multiple edges. Here
we present an algorithm that can directly construct all possible realizations
of a given bi-degree sequence by simple digraphs. Our method is rejection free,
guarantees the independence of the constructed samples, and provides their
weight. The weights can then be used to compute statistical averages of network
observables as if they were obtained from uniformly distributed sampling, or
from any other chosen distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4599</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4599</id><created>2011-09-21</created><updated>2012-04-23</updated><authors><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Iezzi</keyname><forenames>Michela</forenames></author><author><keyname>Graziosi</keyname><forenames>Fabio</forenames></author></authors><title>On the Diversity Order and Coding Gain of Multi-Source Multi-Relay
  Cooperative Wireless Networks with Binary Network Coding</title><categories>cs.IT math.IT</categories><comments>35 pages, submitted as a Journal Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multi-source multi-relay cooperative wireless network with
binary modulation and binary network coding is studied. The system model
encompasses: i) a demodulate-and-forward protocol at the relays, where the
received packets are forwarded regardless of their reliability; and ii) a
maximum-likelihood optimum demodulator at the destination, which accounts for
possible demodulations errors at the relays. An asymptotically-tight and
closed-form expression of the end-to-end error probability is derived, which
clearly showcases diversity order and coding gain of each source. Unlike other
papers available in the literature, the proposed framework has three main
distinguishable features: i) it is useful for general network topologies and
arbitrary binary encoding vectors; ii) it shows how network code and two-hop
forwarding protocol affect diversity order and coding gain; and ii) it accounts
for realistic fading channels and demodulation errors at the relays. The
framework provides three main conclusions: i) each source achieves a diversity
order equal to the separation vector of the network code; ii) the coding gain
of each source decreases with the number of mixed packets at the relays; and
iii) if the destination cannot take into account demodulation errors at the
relays, it loses approximately half of the diversity order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4603</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4603</id><created>2011-09-21</created><authors><author><keyname>Cotter</keyname><forenames>Andrew</forenames></author><author><keyname>Keshet</keyname><forenames>Joseph</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Explicit Approximations of the Gaussian Kernel</title><categories>cs.AI</categories><comments>11 pages, 2 tables, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate training and using Gaussian kernel SVMs by approximating the
kernel with an explicit finite- dimensional polynomial feature representation
based on the Taylor expansion of the exponential. Although not as efficient as
the recently-proposed random Fourier features [Rahimi and Recht, 2007] in terms
of the number of features, we show how this polynomial representation can
provide a better approximation in terms of the computational cost involved.
This makes our &quot;Taylor features&quot; especially attractive for use on very large
data sets, in conjunction with online or stochastic training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4609</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4609</id><created>2011-09-21</created><authors><author><keyname>Merrikh-Bayat</keyname><forenames>Farnood</forenames></author><author><keyname>Shouraki</keyname><forenames>Saeed Bagheri</forenames></author></authors><title>Memristive fuzzy edge detector</title><categories>cs.NE cs.AI cs.AR cs.LG</categories><comments>21 pages, 6 figures, submitted to IET Image Processing Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy inference systems always suffer from the lack of efficient structures
or platforms for their hardware implementation. In this paper, we tried to
overcome this problem by proposing new method for the implementation of those
fuzzy inference systems which use fuzzy rule base to make inference. To achieve
this goal, we have designed a multi-layer neuro-fuzzy computing system based on
the memristor crossbar structure by introducing some new concepts like fuzzy
minterms. Although many applications can be realized through the use of our
proposed system, in this study we show how the fuzzy XOR function can be
constructed and how it can be used to extract edges from grayscale images. Our
memristive fuzzy edge detector (implemented in analog form) compared with other
common edge detectors has this advantage that it can extract edges of any given
image all at once in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4618</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4618</id><created>2011-09-21</created><authors><author><keyname>van Bakel</keyname><forenames>Steffen</forenames></author><author><keyname>Rowe</keyname><forenames>Reuben N. S.</forenames></author></authors><title>Semantic Predicate Types and Approximation for Class-based Object
  Oriented Programming</title><categories>cs.LO</categories><comments>Proceedings of 11th Workshop on Formal Techniques for Java-like
  Programs (FTfJP'09), Genova, Italy, July 6 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the principles of the intersection type discipline to the study of
class-based object oriented programs and; our work follows from a similar
approach (in the context of Abadi and Cardelli's Varsigma-object calculus)
taken by van Bakel and de'Liguoro. We define an extension of Featherweight
Java, FJc and present a predicate system which we show to be sound and
expressive. We also show that our system provides a semantic underpinning for
the object oriented paradigm by generalising the concept of approximant from
the Lambda Calculus and demonstrating an approximation result: all expressions
to which we can assign a predicate have an approximant that satisfies the same
predicate. Crucial to this result is the notion of predicate language, which
associates a family of predicates with a class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4622</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4622</id><created>2011-09-21</created><updated>2013-12-20</updated><authors><author><keyname>Kelmans</keyname><forenames>Alexander</forenames></author></authors><title>Operations on Graphs Increasing Some Graph Parameters</title><categories>math.CO cs.DM</categories><comments>58 pages, added 11 figures and some references, some typos corrected</comments><msc-class>05C22, 05C31, 05C50, 05C76</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this partly expository paper we discuss and describe some of our old and
recent results on partial orders on the set (m,n)-graphs (i.e. graphs with n
vertices and m edges) and some operations on graphs that are monotone with
respect to these partial orders. The partial orders under consideration include
those related with some Laplacian characteristics of graphs as well as with
some probabilistic characteristics of graphs with randomly deleted edges.
Section 2 provides some notions, notation, and simple observations. Section 3
contains some basic facts on the Laplacian polynomial of a graph. Section 4
describes various graph operation and their properties. In Section 5 we
introduce some partial orders on the set of (m,n)-graphs related, in
particular, with the graph Laplacian and the graph reliability (Laplacian
posets and reliability posets}). Section 6 contains some old and recent results
on the monotonicity of some graph operations with respect to Laplacian posets.
Section 7 and 8 include some old and recent results on the monotonicity of some
graph operations with respect to reliability posets and to some other
parameters of graphs as well as some open problems. Section 9 contains some
generalizations of the described results on weighted graphs.
  Keywords: graph, graph operations, graph posets, random graphs, decomposable
graphs, threshold graphs, weighted graphs, spanning subgraphs, Laplacian
polynomial and spectrum, adjacency polynomial and spectrum, graph reliability,
trees, forests, Hamiltonian cycle and path, symmetric polynomials
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4623</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4623</id><created>2011-09-21</created><updated>2013-10-30</updated><authors><author><keyname>Angiulli</keyname><forenames>F.</forenames></author><author><keyname>Ben-Eliyahu-Zohary</keyname><forenames>R.</forenames></author><author><keyname>Palopoli</keyname><forenames>L.</forenames></author></authors><title>Outlier detection in default logics: the tractability/intractability
  frontier</title><categories>cs.AI cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In default theories, outliers denote sets of literals featuring unexpected
properties. In previous papers, we have defined outliers in default logics and
investigated their formal properties. Specifically, we have looked into the
computational complexity of outlier detection problems and proved that while
they are generally intractable, interesting tractable cases can be singled out.
Following those results, we study here the tractability frontier in outlier
detection problems, by analyzing it with respect to (i) the considered outlier
detection problem, (ii) the reference default logic fragment, and (iii) the
adopted notion of outlier. As for point (i), we shall consider three problems
of increasing complexity, called Outlier-Witness Recognition, Outlier
Recognition and Outlier Existence, respectively. As for point (ii), as we look
for conditions under which outlier detection can be done efficiently, attention
will be limited to subsets of Disjunction-free propositional default theories.
As for point (iii), we shall refer to both the notion of outlier of [ABP08] and
a new and more restrictive one, called strong outlier. After complexity
results, we present a polynomial time algorithm for enumerating all strong
outliers of bounded size in an quasi-acyclic normal unary default theory. Some
of our tractability results rely on the Incremental Lemma that provides
conditions for a deafult logic fragment to have a monotonic behavior. Finally,
in order to show that the simple fragments of DL we deal with are still rich
enough to solve interesting problems and, therefore, the tractability results
that we prove are interesting not only on the mere theoretical side, insights
into the expressive capabilities of these fragments are provided, by showing
that normal unary theories express all NL queries, hereby indirectly answering
a question raised by Kautz and Selman.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4627</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4627</id><created>2011-09-20</created><authors><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Distributed Recursive Least-Squares: Stability and Performance Analysis</title><categories>cs.NI cs.SY math.OC</categories><comments>30 pages, 4 figures, submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2012.2194290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recursive least-squares (RLS) algorithm has well-documented merits for
reducing complexity and storage requirements, when it comes to online
estimation of stationary signals as well as for tracking slowly-varying
nonstationary processes. In this paper, a distributed recursive least-squares
(D-RLS) algorithm is developed for cooperative estimation using ad hoc wireless
sensor networks. Distributed iterations are obtained by minimizing a separable
reformulation of the exponentially-weighted least-squares cost, using the
alternating-minimization algorithm. Sensors carry out reduced-complexity tasks
locally, and exchange messages with one-hop neighbors to consent on the
network-wide estimates adaptively. A steady-state mean-square error (MSE)
performance analysis of D-RLS is conducted, by studying a stochastically-driven
`averaged' system that approximates the D-RLS dynamics asymptotically in time.
For sensor observations that are linearly related to the time-invariant
parameter vector sought, the simplifying independence setting assumptions
facilitate deriving accurate closed-form expressions for the MSE steady-state
values. The problems of mean- and MSE-sense stability of D-RLS are also
investigated, and easily-checkable sufficient conditions are derived under
which a steady-state is attained. Without resorting to diminishing step-sizes
which compromise the tracking ability of D-RLS, stability ensures that per
sensor estimates hover inside a ball of finite radius centered at the true
parameter vector, with high-probability, even when inter-sensor communication
links are noisy. Interestingly, computer simulations demonstrate that the
theoretical findings are accurate also in the pragmatic settings whereby
sensors acquire temporally-correlated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4631</identifier>
 <datestamp>2011-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4631</id><created>2011-09-21</created><updated>2011-12-12</updated><authors><author><keyname>Bizhani</keyname><forenames>Golnoosh</forenames></author><author><keyname>Grassberger</keyname><forenames>Peter</forenames></author><author><keyname>Paczuski</keyname><forenames>Maya</forenames></author></authors><title>Random Sequential Renormalization and Agglomerative Percolation in
  Networks: Application to Erd&quot;os-R'enyi and Scale-free Graphs</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><journal-ref>Phys. Rev. E 84, 066111 (2011)</journal-ref><doi>10.1103/PhysRevE.84.066111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the statistical behavior under random sequential
renormalization(RSR) of several network models including Erd&quot;os R'enyi (ER)
graphs, scale-free networks and an annealed model (AM) related to ER graphs. In
RSR the network is locally coarse grained by choosing at each renormalization
step a node at random and joining it to all its neighbors. Compared to previous
(quasi-)parallel renormalization methods [C.Song et.al], RSR allows a more
fine-grained analysis of the renormalization group (RG) flow, and unravels new
features, that were not discussed in the previous analyses. In particular we
find that all networks exhibit a second order transition in their RG flow. This
phase transition is associated with the emergence of a giant hub and can be
viewed as a new variant of percolation, called agglomerative percolation. We
claim that this transition exists also in previous graph renormalization
schemes and explains some of the scaling laws seen there. For critical trees it
happens as N/N0 -&gt; 0 in the limit of large systems (where N0 is the initial
size of the graph and N its size at a given RSR step). In contrast, it happens
at finite N/N0 in sparse ER graphs and in the annealed model, while it happens
for N/N0 -&gt; 1 on scale-free networks. Critical exponents seem to depend on the
type of the graph but not on the average degree and obey usual scaling
relations for percolation phenomena. For the annealed model they agree with the
exponents obtained from a mean-field theory. At late times, the networks
exhibit a star-like structure in agreement with the results of Radicchi et. al.
While degree distributions are of main interest when regarding the scheme as
network renormalization, mass distributions (which are more relevant when
considering 'supernodes' as clusters) are much easier to study using the fast
Newman-Ziff algorithm for percolation, allowing us to obtain very high
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4653</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4653</id><created>2011-09-21</created><updated>2012-03-04</updated><authors><author><keyname>Vieira</keyname><forenames>Vilson</forenames></author><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Can the evolution of music be analyzed in a quantitative manner?</title><categories>physics.data-an cs.SD stat.ME</categories><comments>8 pages, 6 figures, added references for sections 1 and 4.C, better
  mathematical description on section 2. New values and interpretation, now
  considering a bootstrap method</comments><doi>10.1088/1742-5468/2012/08/P08010</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We propose a methodology to study music development by applying multivariate
statistics on composers characteristics. Seven representative composers were
considered in terms of eight main musical features. Grades were assigned to
each characteristic and their correlations were analyzed. A bootstrap method
was applied to simulate hundreds of artificial composers influenced by the
seven representatives chosen. Afterwards we quantify non-numeric relations like
dialectics, opposition and innovation. Composers differences on style and
technique were represented as geometrical distances in the feature space,
making it possible to quantify, for example, how much Bach and Stockhausen
differ from other composers or how much Beethoven influenced Brahms. In
addition, we compared the results with a prior investigation on philosophy.
Opposition, strong on philosophy, was not remarkable on music. Supporting an
observation already considered by music theorists, strong influences were
identified between composers by the quantification of dialectics, implying
inheritance and suggesting a stronger master-disciple evolution when compared
to the philosophy analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4654</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4654</id><created>2011-09-21</created><authors><author><keyname>Hunter</keyname><forenames>Christopher</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Distributed Protocols for Interference Management in Cooperative
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In scenarios where devices are too small to support MIMO antenna arrays,
symbol-level cooperation may be used to pool the resources of distributed
single-antenna devices to create a virtual MIMO antenna array. We address
design fundamentals for distributed cooperative protocols where relays have an
incomplete view of network information. A key issue in distributed networks is
potential loss in spatial reuse due to the increased radio footprint of flows
with cooperative relays. Hence, local gains from cooperation have to balance
against network level losses. By using a novel binary network model that
simplifies the space over which cooperative protocols must be designed, we
develop a mechanism for the systematic and computational development of
cooperative protocols as functions of the amount of network state information
available at relay nodes. Through extensive network analysis and simulations,
we demonstrate the successful application of this method to a series of
protocols that span a range of network information availability at cooperative
relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4668</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4668</id><created>2011-09-21</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author></authors><title>Robust estimation of latent tree graphical models: Inferring hidden
  states with inexact parameters</title><categories>math.PR cs.LG math.ST q-bio.PE stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent tree graphical models are widely used in computational biology, signal
and image processing, and network tomography. Here we design a new efficient,
estimation procedure for latent tree models, including Gaussian and discrete,
reversible models, that significantly improves on previous sample requirement
bounds. Our techniques are based on a new hidden state estimator which is
robust to inaccuracies in estimated parameters. More precisely, we prove that
latent tree models can be estimated with high probability in the so-called
Kesten-Stigum regime with $O(log^2 n)$ samples where $n$ is the number of
nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4677</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4677</id><created>2011-09-21</created><authors><author><keyname>Toubiana</keyname><forenames>Vincent</forenames></author><author><keyname>Subramanian</keyname><forenames>Lakshminarayanan</forenames></author><author><keyname>Nissenbaum</keyname><forenames>Helen</forenames></author></authors><title>TrackMeNot: Enhancing the privacy of Web Search</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most search engines can potentially infer the preferences and interests of a
user based on her history of search queries. While search engines can use these
inferences for a variety of tasks, including targeted advertisements, such
tasks do impose an serious threat to user privacy. In 2006, after AOL disclosed
the search queries of 650,000 users, TrackMeNot was released as a simple
browser extension that sought to hide user search preferences in a cloud of
queries. The first versions of TrackMeNot, though used extensively in the past
three years, was fairly simplistic in design and did not provide any strong
privacy guarantees. In this paper, we present the new design and implementation
of TrackMeNot, which address many of the limitations of the first release.
TrackMeNot addresses two basic problems. First, using a model for
characterizing search queries, TrackMeNot provides a mechanism for obfuscating
the search preferences of a user from a search engine. Second, TrackMeNot
prevents the leakage of information revealing the use of obfuscation to a
search engine via several potential side channels in existing browsers such as
clicks, cookies etc. Finally, we show that TrackMeNot cannot be detected by
current search bot detection mechanisms and demonstrate the effectiveness of
TrackMeNot in obfuscating user interests by testing its efficiency on a major
search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4680</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4680</id><created>2011-09-21</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>The Push Algorithm for Spectral Ranking</title><categories>cs.SI cs.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The push algorithm was proposed first by Jeh and Widom in the context of
personalized PageRank computations (albeit the name &quot;push algorithm&quot; was
actually used by Andersen, Chung and Lang in a subsequent paper). In this note
we describe the algorithm at a level of generality that make the computation of
the spectral ranking of any nonnegative matrix possible. Actually, the main
contribution of this note is that the description is very simple (almost
trivial), and it requires only a few elementary linear-algebra computations.
Along the way, we give new precise ways of estimating the convergence of the
algorithm, and describe some of the contribution of the existing literature,
which again turn out to be immediate when recast in our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4683</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4683</id><created>2011-09-21</created><authors><author><keyname>Ayvaci</keyname><forenames>Alper</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>Detachable Object Detection: Segmentation and Depth Ordering From
  Short-Baseline Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an approach for segmenting an image into regions that correspond
to surfaces in the scene that are partially surrounded by the medium. It
integrates both appearance and motion statistics into a cost functional, that
is seeded with occluded regions and minimized efficiently by solving a linear
programming problem. Where a short observation time is insufficient to
determine whether the object is detachable, the results of the minimization can
be used to seed a more costly optimization based on a longer sequence of video
data. The result is an entirely unsupervised scheme to detect and segment an
arbitrary and unknown number of objects. We test our scheme to highlight the
potential, as well as limitations, of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4684</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4684</id><created>2011-09-21</created><authors><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author><author><keyname>Ip</keyname><forenames>Horace H. S.</forenames></author><author><keyname>Peng</keyname><forenames>Yuxin</forenames></author></authors><title>Exhaustive and Efficient Constraint Propagation: A Semi-Supervised
  Learning Perspective and Its Applications</title><categories>cs.AI cs.LG</categories><comments>The short version of this paper appears as oral paper in ECCV 2010</comments><journal-ref>International Journal of Computer Vision (IJCV), 2012</journal-ref><doi>10.1007/s11263-012-0602-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel pairwise constraint propagation approach by
decomposing the challenging constraint propagation problem into a set of
independent semi-supervised learning subproblems which can be solved in
quadratic time using label propagation based on k-nearest neighbor graphs.
Considering that this time cost is proportional to the number of all possible
pairwise constraints, our approach actually provides an efficient solution for
exhaustively propagating pairwise constraints throughout the entire dataset.
The resulting exhaustive set of propagated pairwise constraints are further
used to adjust the similarity matrix for constrained spectral clustering. Other
than the traditional constraint propagation on single-source data, our approach
is also extended to more challenging constraint propagation on multi-source
data where each pairwise constraint is defined over a pair of data points from
different sources. This multi-source constraint propagation has an important
application to cross-modal multimedia retrieval. Extensive results have shown
the superior performance of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4709</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4709</id><created>2011-09-22</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author><author><keyname>Ghosh</keyname><forenames>Surjya</forenames></author><author><keyname>Biswas</keyname><forenames>Sudipto</forenames></author><author><keyname>Chakraborty</keyname><forenames>Poulami</forenames></author></authors><title>An Approach for Message Hiding using Substitution Techniques and Audio
  Hiding in Steganography</title><categories>cs.CR</categories><comments>4 pages, 4 figures; The Institution of Engineers (India) Journal, Vol
  86, November 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crypto system can be used to encrypt messages sent between two
communicating parties so that an eavesdropper who overhears the encrypted
messages will not be able to decode them. The paper mainly concentrates on the
method in which the substitution technique of steganography can been used to
hide data in a 24-bit bitmap file. Popular audio hiding techniques based on
methods of steganography is also discussed here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4729</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4729</id><created>2011-09-22</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author></authors><title>Parameterized Complexity of Firefighting Revisited</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Firefighter problem is to place firefighters on the vertices of a graph
to prevent a fire with known starting point from lighting up the entire graph.
In each time step, a firefighter may be permanently placed on an unburned
vertex and the fire spreads to its neighborhood in the graph in so far no
firefighters are protecting those vertices. The goal is to let as few vertices
burn as possible. This problem is known to be NP-complete, even when restricted
to bipartite graphs or to trees of maximum degree three. Initial study showed
the Firefighter problem to be fixed-parameter tractable on trees in various
parameterizations. We complete these results by showing that the problem is in
FPT on general graphs when parameterized by the number of burned vertices, but
has no polynomial kernel on trees, resolving an open problem. Conversely, we
show that the problem is W[1]-hard when parameterized by the number of unburned
vertices, even on bipartite graphs. For both parameterizations, we additionally
give refined algorithms on trees, improving on the running times of the known
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4744</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4744</id><created>2011-09-22</created><authors><author><keyname>Srinivasan</keyname><forenames>S. Deepak</forenames></author><author><keyname>Obermayer</keyname><forenames>Klaus</forenames></author></authors><title>Probabilistic prototype models for attributed graphs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution proposes a new approach towards developing a class of
probabilistic methods for classifying attributed graphs. The key concept is
random attributed graph, which is defined as an attributed graph whose nodes
and edges are annotated by random variables. Every node/edge has two random
processes associated with it- occurence probability and the probability
distribution over the attribute values. These are estimated within the maximum
likelihood framework. The likelihood of a random attributed graph to generate
an outcome graph is used as a feature for classification. The proposed approach
is fast and robust to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4750</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4750</id><created>2011-09-22</created><updated>2013-03-08</updated><authors><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Ebbing</keyname><forenames>Johannes</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>Dependence logic with a majority quantifier</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the extension of dependence logic D by a majority quantifier M over
finite structures. We show that the resulting logic is equi-expressive with the
extension of second-order logic by second-order majority quantifiers of all
arities. Our results imply that, from the point of view of descriptive
complexity theory, D(M) captures the complexity class counting hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4770</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4770</id><created>2011-09-22</created><updated>2012-02-06</updated><authors><author><keyname>Greferath</keyname><forenames>Marcus</forenames></author><author><keyname>Zumbr&#xe4;gel</keyname><forenames>Jens</forenames></author></authors><title>On the algebraic representation of selected optimal non-linear binary
  codes</title><categories>cs.IT math.CO math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Revisiting an approach by Conway and Sloane we investigate a collection of
optimal non-linear binary codes and represent them as (non-linear) codes over
Z4. The Fourier transform will be used in order to analyze these codes, which
leads to a new algebraic representation involving subgroups of the group of
units in a certain ring.
  One of our results is a new representation of Best's (10, 40, 4) code as a
coset of a subgroup in the group of invertible elements of the group ring
Z4[Z5]. This yields a particularly simple algebraic decoding algorithm for this
code.
  The technique at hand is further applied to analyze Julin's (12, 144, 4) code
and the (12, 24, 12) Hadamard code. It can also be used in order to construct a
(non-optimal) binary (14, 56, 6) code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4803</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4803</id><created>2011-09-22</created><updated>2011-12-29</updated><authors><author><keyname>Cho</keyname><forenames>Y. S.</forenames></author><author><keyname>Kahng</keyname><forenames>B.</forenames></author></authors><title>Suppression effect on explosive percolations</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Contacting mail address : bkahng@snu.ac.kr, 4figures</comments><journal-ref>Phys. Rev. Lett 107, 275703 (2011)</journal-ref><doi>10.1103/PhysRevLett.107.275703</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a group of people unknown to each other meet and familiarize among
themselves, over time they form a community on a macroscopic scale. This
phenomenon can be understood in the context of percolation transition (PT) of
networks, which takes place continuously in the classical random graph model.
Recently, a modified model was introduced in which the formation of the
community was suppressed. Then the PT occurs explosively at a delayed
transition time. Whether the explosive PT is indeed discontinuous or continuous
becomes controversial. Here we show that type of PT depends on a detailed
dynamic rule. Thus, when the dynamic rule is designed to suppress the growth of
overall clusters, then the explosive PT could be discontinuous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4817</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4817</id><created>2011-09-22</created><authors><author><keyname>van Bakel</keyname><forenames>Steffen</forenames></author><author><keyname>Cardelli</keyname><forenames>Luca</forenames></author><author><keyname>Vigliotti</keyname><forenames>Maria Grazia</forenames></author></authors><title>From X to Pi; Representing the Classical Sequent Calculus in the
  Pi-calculus</title><categories>cs.LO</categories><comments>International Workshop on Classical Logic and Computation (CL&amp;C'08),
  Reykjavik, Iceland, July 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Pi-calculus, enriched with pairing and non-blocking input, and
define a notion of type assignment that uses the type constructor &quot;arrow&quot;. We
encode the circuits of the calculus X into this variant of Pi, and show that
all reduction (cut-elimination) and assignable types are preserved. Since X
enjoys the Curry-Howard isomorphism for Gentzen's calculus LK, this implies
that all proofs in LK have a representation in Pi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4843</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4843</id><created>2011-09-22</created><authors><author><keyname>van Bakel</keyname><forenames>Steffen</forenames></author><author><keyname>Vigliotti</keyname><forenames>Maria Grazia</forenames></author></authors><title>Note on a simple type system for non-interference</title><categories>cs.LO</categories><comments>Nordic Workshop on Programming Theory (NWPT'07), Oslo, October 10-12,
  2007 Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider CCS with value passing and elaborate a notion of noninterference
for the process calculi, which matches closely that of the programming
language. The idea is to view channels as information carriers rather than as
&quot;events&quot;, so that emitting a secret on output channel can be considered safe,
while inputting a secret may lead to some kind of leakage. This is in contrast
with the standard notion of noninterference for the process calculi where any
causal dependency of low-level action from any high-level action is forbidden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4856</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4856</id><created>2011-09-22</created><updated>2011-10-03</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>On the Information Loss in Memoryless Systems: The Multivariate Case</title><categories>cs.IT math.IT nlin.SI</categories><comments>7 pages, 6 figures; submitted to a conference</comments><journal-ref>Proc. Int. Zurich Seminar on Communications, 2012, pp. 32 - 35</journal-ref><doi>10.3929/ethz-a-007023900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we give a concise definition of information loss from a
system-theoretic point of view. Based on this definition, we analyze the
information loss in static input-output systems subject to a continuous-valued
input. For a certain class of multiple-input, multiple-output systems the
information loss is quantified. An interpretation of this loss is accompanied
by upper bounds which are simple to evaluate.
  Finally, a class of systems is identified for which the information loss is
necessarily infinite. Quantizers and limiters are shown to belong to this
class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4875</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4875</id><created>2011-09-22</created><authors><author><keyname>Chicano</keyname><forenames>Francisco</forenames></author><author><keyname>Luque</keyname><forenames>Gabriel</forenames></author><author><keyname>Alba</keyname><forenames>Enrique</forenames></author></authors><title>Elementary Components of the Quadratic Assignment Problem</title><categories>cs.DM</categories><comments>10 pages, 1 figure. An extended version of this paper was published
  in GECCO 2010</comments><acm-class>I.2.8</acm-class><doi>10.1145/1830483.1830745</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quadratic Assignment Problem (QAP) is a well-known NP-hard combinatorial
optimization problem that is at the core of many real-world optimization
problems. We prove that QAP can be written as the sum of three elementary
landscapes when the swap neighborhood is used. We present a closed formula for
each of the three elementary components and we compute bounds for the
autocorrelation coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4900</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4900</id><created>2011-09-22</created><authors><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Evaluating links through spectral decomposition</title><categories>physics.soc-ph cs.SI</categories><doi>10.1088/1742-5468/2012/01/P01015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral decomposition has been rarely used to investigate complex networks.
In this work we apply this concept in order to define two types of
link-directed attacks while quantifying their respective effects on the
topology. Several other types of more traditional attacks are also adopted and
compared. These attacks had substantially diverse effects, depending on each
specific network (models and real-world structures). It is also showed that the
spectral-based attacks have special effect in affecting the transitivity of the
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4906</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4906</id><created>2011-09-22</created><authors><author><keyname>Piton</keyname><forenames>Odile</forenames><affiliation>SAMM</affiliation></author><author><keyname>Mesfar</keyname><forenames>Slim</forenames><affiliation>RIADI</affiliation></author><author><keyname>Pignot</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>SAMM</affiliation></author></authors><title>Automatic transcription of 17th century English text in Contemporary
  English with NooJ: Method and Evaluation</title><categories>cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since 2006 we have undertaken to describe the differences between 17th
century English and contemporary English thanks to NLP software. Studying a
corpus spanning the whole century (tales of English travellers in the Ottoman
Empire in the 17th century, Mary Astell's essay A Serious Proposal to the
Ladies and other literary texts) has enabled us to highlight various lexical,
morphological or grammatical singularities. Thanks to the NooJ linguistic
platform, we created dictionaries indexing the lexical variants and their
transcription in CE. The latter is often the result of the validation of forms
recognized dynamically by morphological graphs. We also built syntactical
graphs aimed at transcribing certain archaic forms in contemporary English. Our
previous research implied a succession of elementary steps alternating textual
analysis and result validation. We managed to provide examples of
transcriptions, but we have not created a global tool for automatic
transcription. Therefore we need to focus on the results we have obtained so
far, study the conditions for creating such a tool, and analyze possible
difficulties. In this paper, we will be discussing the technical and linguistic
aspects we have not yet covered in our previous work. We are using the results
of previous research and proposing a transcription method for words or
sequences identified as archaic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4909</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4909</id><created>2011-09-22</created><authors><author><keyname>Slaughter</keyname><forenames>Chris</forenames></author><author><keyname>Yang</keyname><forenames>Allen Y.</forenames></author><author><keyname>Bagwell</keyname><forenames>Justin</forenames></author><author><keyname>Checkles</keyname><forenames>Costa</forenames></author><author><keyname>Sentis</keyname><forenames>Luis</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D
  Rigid-Body Motion Registration</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by an emerging theory of robust low-rank matrix representation, in
this paper, we introduce a novel solution for online rigid-body motion
registration. The goal is to develop algorithmic techniques that enable a
robust, real-time motion registration solution suitable for low-cost, portable
3-D camera devices. Assuming 3-D image features are tracked via a standard
tracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape
representation of the rigid body. Robust PCA finds the global optimal solution
of the initialization, while its complexity is comparable to singular value
decomposition. In the online update stage, we propose a more efficient
algorithm for sparse subspace projection to sequentially project new feature
observations onto the shape subspace. The lightweight update stage guarantees
the real-time performance of the solution while maintaining good registration
even when the image sequence is contaminated by noise, gross data corruption,
outlying features, and missing data. The state-of-the-art accuracy of the
solution is validated through extensive simulation and a real-world experiment,
while the system enjoys one to two orders of magnitude speed-up compared to
well-established RANSAC solutions. The new algorithm will be released online to
aid peer evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4910</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4910</id><created>2011-09-22</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author><author><keyname>Wu</keyname><forenames>Yu</forenames></author></authors><title>Inapproximability of Treewidth, One-Shot Pebbling, and Related Layout
  Problems</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximability of a number of graph problems: treewidth and
pathwidth of graphs, one-shot black (and black-white) pebbling costs of
directed acyclic graphs, and a variety of different graph layout problems such
as minimum cut linear arrangement and interval graph completion. We show that,
assuming the recently introduced Small Set Expansion Conjecture, all of these
problems are hard to approximate within any constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4919</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4919</id><created>2011-09-22</created><authors><author><keyname>Lahti</keyname><forenames>Leo</forenames></author></authors><title>A brief overview on the BioPAX and SBML standards for formal
  presentation of complex biological knowledge</title><categories>cs.DS q-bio.MN</categories><comments>14 pages, 2 figures, 1 appendix</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A brief informal overview on the BioPAX and SBML standards for formal
presentation of complex biological knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4920</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4920</id><created>2011-09-22</created><updated>2011-11-08</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Beyond pixels and regions: A non local patch means (NLPM) method for
  content-level restoration, enhancement, and reconstruction of degraded
  document images</title><categories>cs.CV cs.IR</categories><comments>This paper has been withdrawn by the author to avoid duplication on
  the DBLP bibliography</comments><journal-ref>Pattern Recognition 44 (2011) 363-374</journal-ref><doi>10.1016/j.patcog.2010.07.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A patch-based non-local restoration and reconstruction method for
preprocessing degraded document images is introduced. The method collects
relative data from the whole input image, while the image data are first
represented by a content-level descriptor based on patches. This
patch-equivalent representation of the input image is then corrected based on
similar patches identified using a modified genetic algorithm (GA) resulting in
a low computational load. The corrected patch-equivalent is then converted to
the output restored image. The fact that the method uses the patches at the
content level allows it to incorporate high-level restoration in an objective
and self-sufficient way. The method has been applied to several degraded
document images, including the DIBCO'09 contest dataset with promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4925</identifier>
 <datestamp>2011-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4925</id><created>2011-09-22</created><authors><author><keyname>Marzulo</keyname><forenames>Leandro A. J.</forenames></author><author><keyname>Alves</keyname><forenames>Tiago A. O.</forenames></author><author><keyname>Fran&#xe7;a</keyname><forenames>Felipe M. G.</forenames></author><author><keyname>Costa</keyname><forenames>V&#xed;tor Santos</forenames></author></authors><title>Couillard: Parallel Programming via Coarse-Grained Data-Flow Compilation</title><categories>cs.DC</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-flow is a natural approach to parallelism. However, describing
dependencies and control between fine-grained data-flow tasks can be complex
and present unwanted overheads. TALM (TALM is an Architecture and Language for
Multi-threading) introduces a user-defined coarse-grained parallel data-flow
model, where programmers identify code blocks, called super-instructions, to be
run in parallel and connect them in a data-flow graph. TALM has been
implemented as a hybrid Von Neumann/data-flow execution system: the
\emph{Trebuchet}. We have observed that TALM's usefulness largely depends on
how programmers specify and connect super-instructions. Thus, we present
\emph{Couillard}, a full compiler that creates, based on an annotated
C-program, a data-flow graph and C-code corresponding to each
super-instruction. We show that our toolchain allows one to benefit from
data-flow execution and explore sophisticated parallel programming techniques,
with small effort. To evaluate our system we have executed a set of real
applications on a large multi-core machine. Comparison with popular parallel
programming methods shows competitive speedups, while providing an easier
parallel programing approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4928</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4928</id><created>2011-09-22</created><updated>2013-04-06</updated><authors><author><keyname>Lahti</keyname><forenames>Leo</forenames></author><author><keyname>Elo</keyname><forenames>Laura L.</forenames></author><author><keyname>Aittokallio</keyname><forenames>Tero</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>RPA: Probabilistic analysis of probe performance and robust
  summarization</title><categories>cs.CE stat.AP stat.ML</categories><comments>Replaced by extended work which forms an independent publication</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Probe-level models have led to improved performance in microarray studies but
the various sources of probe-level contamination are still poorly understood.
Data-driven analysis of probe performance can be used to quantify the
uncertainty in individual probes and to highlight the relative contribution of
different noise sources. Improved understanding of the probe-level effects can
lead to improved preprocessing techniques and microarray design.
  We have implemented probabilistic tools for probe performance analysis and
summarization on short oligonucleotide arrays. In contrast to standard
preprocessing approaches, the methods provide quantitative estimates of
probe-specific noise and affinity terms and tools to investigate these
parameters. Tools to incorporate prior information of the probes in the
analysis are provided as well. Comparisons to known probe-level error sources
and spike-in data sets validate the approach.
  Implementation is freely available in R/BioConductor:
http://www.bioconductor.org/packages/release/bioc/html/RPA.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4960</identifier>
 <datestamp>2012-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4960</id><created>2011-09-22</created><updated>2012-08-06</updated><authors><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Moura</keyname><forenames>Jose' M. F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Distributed Linear Parameter Estimation: Asymptotically Efficient
  Adaptive Strategies</title><categories>math.OC cs.SY math.PR math.ST stat.TH</categories><comments>Submitted to SIAM Journal on Control and Optimization journal.
  Initial Submission: Sept. 2011. Revised: Aug. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the problem of distributed adaptive linear parameter
estimation in multi-agent inference networks. Local sensing model information
is only partially available at the agents and inter-agent communication is
assumed to be unpredictable. The paper develops a generic mixed time-scale
stochastic procedure consisting of simultaneous distributed learning and
estimation, in which the agents adaptively assess their relative observation
quality over time and fuse the innovations accordingly. Under rather weak
assumptions on the statistical model and the inter-agent communication, it is
shown that, by properly tuning the consensus potential with respect to the
innovation potential, the asymptotic information rate loss incurred in the
learning process may be made negligible. As such, it is shown that the agent
estimates are asymptotically efficient, in that their asymptotic covariance
coincides with that of a centralized estimator (the inverse of the centralized
Fisher information rate for Gaussian systems) with perfect global model
information and having access to all observations at all times. The proof
techniques are mainly based on convergence arguments for non-Markovian mixed
time scale stochastic approximation procedures. Several approximation results
developed in the process are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4974</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4974</id><created>2011-09-22</created><updated>2011-12-09</updated><authors><author><keyname>Voorsluys</keyname><forenames>William</forenames></author><author><keyname>Broberg</keyname><forenames>James</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Cost of Virtual Machine Live Migration in Clouds: A Performance
  Evaluation</title><categories>cs.DC cs.PF</categories><comments>CloudCom 2009</comments><doi>10.1007/978-3-642-10665-1_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization has become commonplace in modern data centers, often referred
as &quot;computing clouds&quot;. The capability of virtual machine live migration brings
benefits such as improved performance, manageability and fault tolerance, while
allowing workload movement with a short service downtime. However, service
levels of applications are likely to be negatively affected during a live
migration. For this reason, a better understanding of its effects on system
performance is desirable. In this paper, we evaluate the effects of live
migration of virtual machines on the performance of applications running inside
Xen VMs. Results show that, in most cases, migration overhead is acceptable but
cannot be disregarded, especially in systems where availability and
responsiveness are governed by strict Service Level Agreements. Despite that,
there is a high potential for live migration applicability in data centers
serving modernInternet applications. Our results are based on a workload
covering the domain of multi-tier Web 2.0 applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4979</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4979</id><created>2011-09-22</created><authors><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author><author><keyname>Peng</keyname><forenames>Yuxin</forenames></author></authors><title>Latent Semantic Learning with Structured Sparse Representation for Human
  Action Recognition</title><categories>cs.MM cs.AI cs.LG</categories><comments>The short version of this paper appears in ICCV 2011</comments><doi>10.1016/j.patcog.2012.09.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel latent semantic learning method for extracting
high-level features (i.e. latent semantics) from a large vocabulary of abundant
mid-level features (i.e. visual keywords) with structured sparse
representation, which can help to bridge the semantic gap in the challenging
task of human action recognition. To discover the manifold structure of
midlevel features, we develop a spectral embedding approach to latent semantic
learning based on L1-graph, without the need to tune any parameter for graph
construction as a key step of manifold learning. More importantly, we construct
the L1-graph with structured sparse representation, which can be obtained by
structured sparse coding with its structured sparsity ensured by novel L1-norm
hypergraph regularization over mid-level features. In the new embedding space,
we learn latent semantics automatically from abundant mid-level features
through spectral clustering. The learnt latent semantics can be readily used
for human action recognition with SVM by defining a histogram intersection
kernel. Different from the traditional latent semantic analysis based on topic
models, our latent semantic learning method can explore the manifold structure
of mid-level features in both L1-graph construction and spectral embedding,
which results in compact but discriminative high-level features. The
experimental results on the commonly used KTH action dataset and unconstrained
YouTube action dataset show the superior performance of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4994</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4994</id><created>2011-09-22</created><updated>2015-02-04</updated><authors><author><keyname>Margolus</keyname><forenames>Norman</forenames></author></authors><title>The maximum average rate of state change</title><categories>quant-ph cs.IT gr-qc hep-th math-ph math.IT math.MP</categories><comments>4 pages, 2 figures, ancillary file with 12 supplemental figures,
  ancillary file with mathematica notebook, ancillary file with pdf printout of
  mathematica notebook</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an isolated quantum system, average energy above the ground state and
average momentum determine the minimum time and distance needed to transition
through $N$ distinct states. Other moments of energy and momentum provide
similar bounds. For $N\gg1$, we can equate average energy and momentum with the
maximum average rates of distinct state change in time and space. Thus at least
vestiges of finite-state dynamics persist in the classical world, and in its
interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.4995</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.4995</id><created>2011-09-22</created><updated>2011-09-27</updated><authors><author><keyname>Margolus</keyname><forenames>Norman</forenames></author></authors><title>Quantum emulation of classical dynamics</title><categories>quant-ph cs.IT math.IT nlin.CG</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In statistical mechanics, it is well known that finite-state classical
lattice models can be recast as quantum models, with distinct classical
configurations identified with orthogonal basis states. This mapping makes
classical statistical mechanics on a lattice a special case of quantum
statistical mechanics, and classical combinatorial entropy a special case of
quantum entropy.
  In a similar manner, finite-state classical dynamics can be recast as
finite-energy quantum dynamics. This mapping translates continuous quantities,
concepts and machinery of quantum mechanics into a simplified finite-state
context in which they have a purely classical and combinatorial interpretation.
For example, in this mapping quantum average energy becomes the classical
update rate.
  Interpolation theory and communication theory help explain the truce achieved
here between perfect classical determinism and quantum uncertainty, and between
discrete and continuous dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5002</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5002</id><created>2011-09-23</created><updated>2013-02-22</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Alignment-free phylogenetic reconstruction: Sample complexity via a
  branching process analysis</title><categories>math.PR cs.CE cs.DS math.ST q-bio.PE stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AAP852 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP852</report-no><journal-ref>Annals of Applied Probability 2013, Vol. 23, No. 2, 693-721</journal-ref><doi>10.1214/12-AAP852</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient phylogenetic reconstruction algorithm allowing
insertions and deletions which provably achieves a sequence-length requirement
(or sample complexity) growing polynomially in the number of taxa. Our
algorithm is distance-based, that is, it relies on pairwise sequence
comparisons. More importantly, our approach largely bypasses the difficult
problem of multiple sequence alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5005</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5005</id><created>2011-09-23</created><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author></authors><title>Robust Linear Transceiver Design for Multi-Hop Non-Regenerative MIMO
  Relaying Systems</title><categories>cs.IT math.IT</categories><comments>Accepted by WCSP2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, optimal linear transceiver designs for multi-hop
amplify-and-forward (AF) Multiple-input Multiple-out (MIMO) relaying systems
with Gaussian distributed channel estimation errors are investigated. Some
commonly used transceiver design criteria are unified into a single
matrix-variate optimization problem. With novel applications of majorization
theory and properties of matrix-variate function, the optimal structure of
robust transceiver is first derived. Based on the optimal structure, the
original transceiver design problems are reduced to much simpler problems with
only scalar variables whose solutions are readily obtained by iterative
water-filling algorithms. The performance advantages of the proposed robust
designs are demonstrated by the simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5018</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5018</id><created>2011-09-23</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author></authors><title>An O(n^2) Time Algorithm for Alternating B\&quot;uchi Games</title><categories>cs.GT cs.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the winning set for B{\&quot;u}chi objectives in alternating games on
graphs is a central problem in computer aided verification with a large number
of applications. The long standing best known upper bound for solving the
problem is $\tilde{O}(n \cdot m)$, where $n$ is the number of vertices and $m$
is the number of edges in the graph. We are the first to break the
$\tilde{O}(n\cdot m)$ bound by presenting a new technique that reduces the
running time to $O(n^2)$. This bound also leads to an $O(n^2)$ algorithm time
for computing the set of almost-sure winning vertices in alternating games with
probabilistic transitions (improving an earlier bound of $\tilde{O}(n\cdot m)$)
and in concurrent graph games with constant actions (improving an earlier bound
of $O(n^3)$). We also show that the same technique can be used to compute the
maximal end-component decomposition of a graph in time $O(n^2)$. Finally, we
show how to maintain the winning set for B{\&quot;u}chi objectives in alternating
games under a sequence of edge insertions or a sequence of edge deletions in
O(n) amortized time per operation. This is the first dynamic algorithm for this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5034</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5034</id><created>2011-09-23</created><updated>2013-03-19</updated><authors><author><keyname>Romaszewski</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>G&#x142;omb</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Gawron</keyname><forenames>Piotr</forenames></author></authors><title>Natural hand gestures for human identification in a Human-Computer
  Interface</title><categories>cs.HC</categories><comments>13 pages, 3 figures, This is a major rewrite of previous version of
  the paper. The same dataset as in previous version was used. The analysis is
  now focused on application of the gestures classification methods to human
  identification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is the identification of humans based on motion data in
the form of natural hand gestures. In this paper, the identification problem is
formulated as classification with classes corresponding to persons' identities,
based on recorded signals of performed gestures. The identification performance
is examined with a database of twenty-two natural hand gestures recorded with
two types of hardware and three state-of-art classifiers: Linear Discrimination
Analysis (LDA), Support Vector machines (SVM) and k-Nearest Neighbour (k-NN).
Results show that natural hand gestures allow for an effective human
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5036</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5036</id><created>2011-09-23</created><updated>2013-01-03</updated><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Testing first-order properties for subclasses of sparse graphs</title><categories>cs.DM cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a linear-time algorithm for deciding first-order (FO) properties
in classes of graphs with bounded expansion, a notion recently introduced by
Nesetril and Ossona de Mendez. This generalizes several results from the
literature, because many natural classes of graphs have bounded expansion:
graphs of bounded tree-width, all proper minor-closed classes of graphs, graphs
of bounded degree, graphs with no subgraph isomorphic to a subdivision of a
fixed graph, and graphs that can be drawn in a fixed surface in such a way that
each edge crosses at most a constant number of other edges. We deduce that
there is an almost linear-time algorithm for deciding FO properties in classes
of graphs with locally bounded expansion.
  More generally, we design a dynamic data structure for graphs belonging to a
fixed class of graphs of bounded expansion. After a linear-time initialization
the data structure allows us to test an FO property in constant time, and the
data structure can be updated in constant time after addition/deletion of an
edge, provided the list of possible edges to be added is known in advance and
their simultaneous addition results in a graph in the class. All our results
also hold for relational structures and are based on the seminal result of
Nesetril and Ossona de Mendez on the existence of low tree-depth colorings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5052</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5052</id><created>2011-09-23</created><authors><author><keyname>Edelsbrunner</keyname><forenames>Herbert</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author></authors><title>Alexander Duality for Functions: the Persistent Behavior of Land and
  Water and Shore</title><categories>math.AT cs.CG math.GT</categories><comments>Keywords: Algebraic topology, homology, Alexander duality,
  Mayer-Vietoris sequences, persistent homology, point calculus</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note contributes to the point calculus of persistent homology by
extending Alexander duality to real-valued functions. Given a perfect Morse
function $f: S^{n+1} \to [0,1]$ and a decomposition $S^{n+1} = U \cup V$ such
that $M = \U \cap V$ is an $n$-manifold, we prove elementary relationships
between the persistence diagrams of $f$ restricted to $U$, to $V$, and to $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5053</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5053</id><created>2011-09-23</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Sinha</keyname><forenames>Sukanta</forenames></author></authors><title>A New Approach to Design Graph Based Search Engine for Multiple Domains
  Using Different Ontologies</title><categories>cs.IR</categories><comments>8 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search Engine has become a major tool for searching any information from the
World Wide Web (WWW). While searching the huge digital library available in the
WWW, every effort is made to retrieve the most relevant results. But in WWW
majority of the Web pages are in HTML format and there are no such tags which
tells the crawler to find any specific domain. To find more relevant result we
use Ontology for that particular domain. If we are working with multiple
domains then we use multiple ontologies. Now in order to design a domain
specific search engine for multiple domains, crawler must crawl through the
domain specific Web pages in the WWW according to the predefined ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5072</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5072</id><created>2011-09-23</created><authors><author><keyname>Insa-Cabrera</keyname><forenames>Javier</forenames></author><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>Analysis of first prototype universal intelligence tests: evaluating and
  comparing AI algorithms and humans</title><categories>cs.AI</categories><comments>114 pages, master thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g., playing chess, solving mazes or land a helicopter). However, these
methods are not appropriate if we want to evaluate the general intelligence of
AI and, even less, if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms, but also to assess any intelligent
system (human beings, animals, AI, aliens?,...), and letting us to place their
results on the same scale and, therefore, to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find, be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms, this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to, in the future, being
able to achieve a truly universal intelligence test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5078</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5078</id><created>2011-09-23</created><authors><author><keyname>Bedoya-Puerta</keyname><forenames>Jorge-Alonso</forenames></author><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>Application of distances between terms for flat and hierarchical data</title><categories>cs.LG</categories><comments>in Spanish, Master Thesis, 101 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning, distance-based algorithms, and other approaches, use
information that is represented by propositional data. However, this kind of
representation can be quite restrictive and, in many cases, it requires more
complex structures in order to represent data in a more natural way. Terms are
the basis for functional and logic programming representation. Distances
between terms are a useful tool not only to compare terms, but also to
determine the search space in many of these applications. This dissertation
applies distances between terms, exploiting the features of each distance and
the possibility to compare from propositional data types to hierarchical
representations. The distances between terms are applied through the k-NN
(k-nearest neighbor) classification algorithm using XML as a common language
representation. To be able to represent these data in an XML structure and to
take advantage of the benefits of distance between terms, it is necessary to
apply some transformations. These transformations allow the conversion of flat
data into hierarchical data represented in XML, using some techniques based on
intuitive associations between the names and values of variables and
associations based on attribute similarity.
  Several experiments with the distances between terms of Nienhuys-Cheng and
Estruch et al. were performed. In the case of originally propositional data,
these distances are compared to the Euclidean distance. In all cases, the
experiments were performed with the distance-weighted k-nearest neighbor
algorithm, using several exponents for the attraction function (weighted
distance). It can be seen that in some cases, the term distances can
significantly improve the results on approaches applied to flat
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5083</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5083</id><created>2011-09-23</created><updated>2012-12-31</updated><authors><author><keyname>Nittoor</keyname><forenames>Vivek S</forenames></author><author><keyname>Suda</keyname><forenames>Reiji</forenames></author></authors><title>A Mathematical Approach to Balanced Tanner Graph Enumeration</title><categories>cs.IT math.CO math.IT</categories><comments>8 pages - results in this paper have been superceded by new results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes our latest understanding and results about the
application of the Mathematics Of Enumeration to Tanner Graphs that have a
regular structure called Balanced Tanner Graphs. Some preliminaries of
permutation groups have been presented followed by various enumeration
theorems, and finally our approach for enumeration of Balanced Tanner Graphs
has been explained, and several open questions have been raised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5088</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5088</id><created>2011-09-23</created><authors><author><keyname>Ballardin</keyname><forenames>Francesco</forenames></author><author><keyname>Macedonio</keyname><forenames>Damiano</forenames></author><author><keyname>Merro</keyname><forenames>Massimo</forenames></author><author><keyname>Tirapelle</keyname><forenames>Mattia</forenames></author></authors><title>A Semantic Analysis of Key Management Protocols for Wireless Sensor
  Networks</title><categories>cs.CR cs.NI</categories><comments>51 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple timed broadcasting process calculus for modelling
wireless network protocols. The operational semantics of our calculus is given
in terms of a labelled transition semantics which is used to derive a standard
(weak) bi-simulation theory. Based on our simulation theory, we reformulate
Gorrieri and Martinelli's timed Generalized Non-Deducibility on Compositions
(tGNDC) scheme, a well-known general framework for the definition of timed
properties of security protocols. We use tGNDC to perform a semantic analysis
of three well-known key management protocols for wireless sensor networks: \mu
TESLA, LEAP+ and LiSP. As a main result, we provide a number of attacks to
these protocols which, to our knowledge, have not yet appeared in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5100</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5100</id><created>2011-09-23</created><authors><author><keyname>Botchev</keyname><forenames>Mikhail A.</forenames></author></authors><title>Block Krylov subspace exact time integration of linear ODE systems. Part
  1: algorithm description</title><categories>math.NA cs.NA physics.comp-ph</categories><comments>4 pages</comments><msc-class>65F60, 65F10, 65F30, 65N22, 65L05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a time-exact Krylov-subspace-based method for solving linear ODE
(ordinary differential equation) systems of the form $y'=-Ay + g(t)$, where
$y(t)$ is the unknown function. The method consists of two stages. The first
stage is an accurate polynomial approximation of the source term $g(t)$,
constructed with the help of the truncated SVD (singular value decomposition).
The second stage is a special residual-based block Krylov subspace method.
  The accuracy of the method is only restricted by the accuracy of the
polynomial approximation and by the error of the block Krylov process. Since
both errors can, in principle, be made arbitrarily small, this yields, at some
costs, a time-exact method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5111</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5111</id><created>2011-09-23</created><updated>2011-09-26</updated><authors><author><keyname>van Renesse</keyname><forenames>Robbert</forenames></author><author><keyname>Schneider</keyname><forenames>Fred B.</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author></authors><title>Nerio: Leader Election and Edict Ordering</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordination in a distributed system is facilitated if there is a unique
process, the leader, to manage the other processes. The leader creates edicts
and sends them to other processes for execution or forwarding to other
processes. The leader may fail, and when this occurs a leader election protocol
selects a replacement. This paper describes Nerio, a class of such leader
election protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5114</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5114</id><created>2011-09-23</created><updated>2012-02-22</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author><author><keyname>Sanyal</keyname><forenames>Sebanti</forenames></author></authors><title>Improvements on &quot;Fast space-variant elliptical filtering using box
  splines&quot;</title><categories>cs.CV</categories><comments>7 figures</comments><journal-ref>IEEE Transactions on Image Processing, vol. 21(9), pp. 3915 -
  3923, 2012</journal-ref><doi>10.1109/TIP.2012.2198222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that box filters can be efficiently computed using
pre-integrations and local finite-differences
[Crow1984,Heckbert1986,Viola2001]. By generalizing this idea and by combining
it with a non-standard variant of the Central Limit Theorem, a constant-time or
O(1) algorithm was proposed in [Chaudhury2010] that allowed one to perform
space-variant filtering using Gaussian-like kernels. The algorithm was based on
the observation that both isotropic and anisotropic Gaussians could be
approximated using certain bivariate splines called box splines. The attractive
feature of the algorithm was that it allowed one to continuously control the
shape and size (covariance) of the filter, and that it had a fixed
computational cost per pixel, irrespective of the size of the filter. The
algorithm, however, offered a limited control on the covariance and accuracy of
the Gaussian approximation. In this work, we propose some improvements by
appropriately modifying the algorithm in [Chaudhury2010].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5120</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5120</id><created>2011-09-23</created><updated>2012-12-31</updated><authors><author><keyname>Nittoor</keyname><forenames>Vivek S</forenames></author><author><keyname>Suda</keyname><forenames>Reiji</forenames></author></authors><title>Algorithms for Enumerating Balanced Tanner Graphs</title><categories>cs.IT math.IT</categories><comments>6 pages - results in this paper have been superceded by new results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This summarizes our latest understanding and results about the algorithms for
enumerating Tanner Graphs that have a regular structure called Balanced Tanner
Graphs. Enumeration algorithms for Balanced Tanner Graphs based upon Cyclic
Permutation Groups have been developed in this paper. A constrained enumeration
algorithm that enumerates Balanced Tanner Graphs that have a relatively larger
length of minimum cycle has been described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5135</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5135</id><created>2011-09-23</created><updated>2012-09-03</updated><authors><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Magniez</keyname><forenames>Frederic</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author></authors><title>A learning graph based quantum query algorithm for finding constant-size
  subgraphs</title><categories>quant-ph cs.DS</categories><comments>The analysis has been refined and a second algorithm included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $H$ be a fixed $k$-vertex graph with $m$ edges and minimum degree $d &gt;0$.
We use the learning graph framework of Belovs to show that the bounded-error
quantum query complexity of determining if an $n$-vertex graph contains $H$ as
a subgraph is $O(n^{2-2/k-t})$, where $ t = \max{\frac{k^2-
2(m+1)}{k(k+1)(m+1)}, \frac{2k - d - 3}{k(d+1)(m-d+2)}}$. The previous best
algorithm of Magniez et al. had complexity $\widetilde O(n^{2-2/k})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5153</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5153</id><created>2011-09-23</created><authors><author><keyname>Golab</keyname><forenames>Wojciech</forenames></author></authors><title>A Complexity Separation Between the Cache-Coherent and Distributed
  Shared Memory Models</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider asynchronous multiprocessor systems where processes communicate
by accessing shared memory. Exchange of information among processes in such a
multiprocessor necessitates costly memory accesses called \emph{remote memory
references} (RMRs), which generate communication on the interconnect joining
processors and main memory. In this paper we compare two popular shared memory
architecture models, namely the \emph{cache-coherent} (CC) and
\emph{distributed shared memory} (DSM) models, in terms of their power for
solving synchronization problems efficiently with respect to RMRs. The
particular problem we consider entails one process sending a &quot;signal&quot; to a
subset of other processes. We show that a variant of this problem can be solved
very efficiently with respect to RMRs in the CC model, but not so in the DSM
model, even when we consider amortized RMR complexity.
  To our knowledge, this is the first separation in terms of amortized RMR
complexity between the CC and DSM models. It is also the first separation in
terms of RMR complexity (for asynchronous systems) that does not rely in any
way on wait-freedom---the requirement that a process makes progress in a
bounded number of its own steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5190</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5190</id><created>2011-09-23</created><authors><author><keyname>Dekate</keyname><forenames>Chirag</forenames></author><author><keyname>Anderson</keyname><forenames>Matthew</forenames></author><author><keyname>Brodowicz</keyname><forenames>Maciej</forenames></author><author><keyname>Kaiser</keyname><forenames>Hartmut</forenames></author><author><keyname>Adelstein-Lelbach</keyname><forenames>Bryce</forenames></author><author><keyname>Sterling</keyname><forenames>Thomas</forenames></author></authors><title>Improving the scalability of parallel N-body applications with an event
  driven constraint based execution model</title><categories>cs.DC</categories><comments>11 figures</comments><journal-ref>International Journal of High Performance Computing Applications,
  April 11, 2012</journal-ref><doi>10.1177/1094342012440585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scalability and efficiency of graph applications are significantly
constrained by conventional systems and their supporting programming models.
Technology trends like multicore, manycore, and heterogeneous system
architectures are introducing further challenges and possibilities for emerging
application domains such as graph applications. This paper explores the space
of effective parallel execution of ephemeral graphs that are dynamically
generated using the Barnes-Hut algorithm to exemplify dynamic workloads. The
workloads are expressed using the semantics of an Exascale computing execution
model called ParalleX. For comparison, results using conventional execution
model semantics are also presented. We find improved load balancing during
runtime and automatic parallelism discovery improving efficiency using the
advanced semantics for Exascale computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5197</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5197</id><created>2011-09-23</created><authors><author><keyname>Veliz-Cuba</keyname><forenames>Alan</forenames></author><author><keyname>Arthur</keyname><forenames>Joseph</forenames></author><author><keyname>Hochstetler</keyname><forenames>Laura</forenames></author><author><keyname>Klomps</keyname><forenames>Victoria</forenames></author><author><keyname>Korpi</keyname><forenames>Erikka</forenames></author></authors><title>On the relationship of steady states of continuous and discrete models</title><categories>math.CA cs.DM math.CO math.DS q-bio.MN</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide theoretical results that relate steady states of
continuous and discrete models arising from biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5201</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5201</id><created>2011-09-23</created><authors><author><keyname>Anderson</keyname><forenames>Matthew</forenames></author><author><keyname>Brodowicz</keyname><forenames>Maciej</forenames></author><author><keyname>Kaiser</keyname><forenames>Hartmut</forenames></author><author><keyname>Sterling</keyname><forenames>Thomas</forenames></author></authors><title>An Application Driven Analysis of the ParalleX Execution Model</title><categories>cs.DC</categories><comments>9 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exascale systems, expected to emerge by the end of the next decade, will
require the exploitation of billion-way parallelism at multiple hierarchical
levels in order to achieve the desired sustained performance. The task of
assessing future machine performance is approached by identifying the factors
which currently challenge the scalability of parallel applications. It is
suggested that the root cause of these challenges is the incoherent coupling
between the current enabling technologies, such as Non-Uniform Memory Access of
present multicore nodes equipped with optional hardware accelerators and the
decades older execution model, i.e., the Communicating Sequential Processes
(CSP) model best exemplified by the message passing interface (MPI) application
programming interface. A new execution model, ParalleX, is introduced as an
alternative to the CSP model. In this paper, an overview of the ParalleX
execution model is presented along with details about a ParalleX-compliant
runtime system implementation called High Performance ParalleX (HPX). Scaling
and performance results for an adaptive mesh refinement numerical relativity
application developed using HPX are discussed. The performance results of this
HPX-based application are compared with a counterpart MPI-based mesh refinement
code. The overheads associated with HPX are explored and hardware solutions are
introduced for accelerating the runtime system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5214</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5214</id><created>2011-09-23</created><authors><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author></authors><title>A Market for Air Traffic Flow Management</title><categories>cs.CC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two somewhat conflicting requirements of efficiency and fairness make
ATFM an unsatisfactorily solved problem, despite its overwhelming importance.
In this paper, we present an economics motivated solution that is based on the
notion of a free market. Our contention is that in fact the airlines themselves
are the best judge of how to achieve efficiency and our market-based solution
gives them the ability to pay, at the going rate, to buy away the desired
amount of delay on a per flight basis.
  The issue of fairness is simply finessed away by our solution -- whoever pays
gets smaller delays. We show how our solution has the potential of enabling
travelers from a large spectrum of affordability and punctuality requirements
to achieve an end that is most desirable to them.
  Our market model is particularly simple, requiring only one parameter per
flight from the airline company. Furthermore, we show that it admits a
combinatorial, strongly polynomial algorithm for computing an equilibrium
landing schedule and prices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5222</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5222</id><created>2011-09-23</created><authors><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Completion Time in Multi-Access Channel: An Information Theoretic
  Perspective</title><categories>cs.IT math.IT</categories><comments>short version to appear in Proc. 2011 Information Theory Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-access channel, completion time refers to the number of channel
uses required for users, each with some given fixed bit pool, to complete the
transmission of all their data bits. In this paper, the characterization of the
completion time region is based on the concept of constrained rates, where
users' rates are defined over possibly different number of channel uses. An
information theoretic formulation of completion time is given and the
completion time region is then established for two-user Gaussian multi-access
channel, which, analogous to capacity region, characterizes all possible
trade-offs between users' completion times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5229</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5229</id><created>2011-09-24</created><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Distributed Algorithms for Optimal Power Flow Problem</title><categories>math.OC cs.SY</categories><comments>12 pages, short (8-page) version submitted to American Control
  Conference 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal power flow (OPF) is an important problem for power generation and it
is in general non-convex. With the employment of renewable energy, it will be
desirable if OPF can be solved very efficiently so its solution can be used in
real time. With some special network structure, e.g. trees, the problem has
been shown to have a zero duality gap and the convex dual problem yields the
optimal solution. In this paper, we propose a primal and a dual algorithm to
coordinate the smaller subproblems decomposed from the convexified OPF. We can
arrange the subproblems to be solved sequentially and cumulatively in a central
node or solved in parallel in distributed nodes. We test the algorithms on IEEE
radial distribution test feeders, some random tree-structured networks, and the
IEEE transmission system benchmarks. Simulation results show that the
computation time can be improved dramatically with our algorithms over the
centralized approach of solving the problem without decomposition, especially
in tree-structured problems. The computation time grows linearly with the
problem size with the cumulative approach while the distributed one can have
size-independent computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5231</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5231</id><created>2011-09-24</created><updated>2012-10-13</updated><authors><author><keyname>Manwani</keyname><forenames>Naresh</forenames></author><author><keyname>Sastry</keyname><forenames>P. S.</forenames></author></authors><title>Noise Tolerance under Risk Minimization</title><categories>cs.LG</categories><doi>10.1109/TSMCB.2012.2223460</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore noise tolerant learning of classifiers. We formulate
the problem as follows. We assume that there is an ${\bf unobservable}$
training set which is noise-free. The actual training set given to the learning
algorithm is obtained from this ideal data set by corrupting the class label of
each example. The probability that the class label of an example is corrupted
is a function of the feature vector of the example. This would account for most
kinds of noisy data one encounters in practice. We say that a learning method
is noise tolerant if the classifiers learnt with the ideal noise-free data and
with noisy data, both have the same classification accuracy on the noise-free
data. In this paper we analyze the noise tolerance properties of risk
minimization (under different loss functions), which is a generic method for
learning classifiers. We show that risk minimization under 0-1 loss function
has impressive noise tolerance properties and that under squared error loss is
tolerant only to uniform noise; risk minimization under other loss functions is
not noise tolerant. We conclude the paper with some discussion on implications
of these theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5235</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5235</id><created>2011-09-24</created><updated>2012-03-13</updated><authors><author><keyname>Christakis</keyname><forenames>Nicholas A.</forenames></author><author><keyname>Fowler</keyname><forenames>James H.</forenames></author></authors><title>Social Contagion Theory: Examining Dynamic Social Networks and Human
  Behavior</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we review the research we have done on social contagion. We describe
the methods we have employed (and the assumptions they have entailed) in order
to examine several datasets with complementary strengths and weaknesses,
including the Framingham Heart Study, the National Longitudinal Study of
Adolescent Health, and other observational and experimental datasets that we
and others have collected. We describe the regularities that led us to propose
that human social networks may exhibit a &quot;three degrees of influence&quot; property,
and we review statistical approaches we have used to characterize
inter-personal influence with respect to phenomena as diverse as obesity,
smoking, cooperation, and happiness. We do not claim that this work is the
final word, but we do believe that it provides some novel, informative, and
stimulating evidence regarding social contagion in longitudinally followed
networks. Along with other scholars, we are working to develop new methods for
identifying causal effects using social network data, and we believe that this
area is ripe for statistical development as current methods have known and
often unavoidable limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5240</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5240</id><created>2011-09-24</created><authors><author><keyname>Olympio</keyname><forenames>Joris T.</forenames></author></authors><title>A Continuous Feedback Optimal Control based on Second-Variations for
  Problems with Control Constraints</title><categories>math.OC cs.SY</categories><comments>17 pages, 3 figures</comments><msc-class>49M05, 49M37, 34H05, 34K35, 65K10, 90C52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes a continuous second-variation algorithm to solve optimal
control problems where the control is defined on a closed set. A second order
expansion of a Lagrangian provides linear updates of the control to construct a
locally feedback optimal control of the problem. Since the process involves a
backward and a forward stage, which require storing trajectories, a method has
been devised to accurately store continuous solutions of ordinary differential
equations. Thanks to the continuous approach, the method adapts implicitly the
numerical time mesh. The novel method is demonstrated on bang-bang optimal
control problems, showing the suitability of the method to identify
automatically optimal switching points in the control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5241</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5241</id><created>2011-09-24</created><authors><author><keyname>Gaubert</keyname><forenames>Stephane</forenames></author><author><keyname>McEneaney</keyname><forenames>William</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author></authors><title>Curse of dimensionality reduction in max-plus based approximation
  methods: theoretical estimates and improved pruning algorithms</title><categories>math.OC cs.SY</categories><comments>8pages 5 figures</comments><journal-ref>Proceedings of the 50th IEEE Conference on Decision and Control
  and European Control Conference (CDC-ECC 11),Orlando, FL, USA, pp. 1054-1061,
  2011</journal-ref><doi>10.1109/CDC.2011.6161386</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-plus based methods have been recently developed to approximate the value
function of possibly high dimensional optimal control problems. A critical step
of these methods consists in approximating a function by a supremum of a small
number of functions (max-plus &quot;basis functions&quot;) taken from a prescribed
dictionary. We study several variants of this approximation problem, which we
show to be continuous versions of the facility location and $k$-center
combinatorial optimization problems, in which the connection costs arise from a
Bregman distance. We give theoretical error estimates, quantifying the number
of basis functions needed to reach a prescribed accuracy. We derive from our
approach a refinement of the curse of dimensionality free method introduced
previously by McEneaney, with a higher accuracy for a comparable computational
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5242</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5242</id><created>2011-09-24</created><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author></authors><title>A Counterexample to rapid mixing of the Ge-Stefankovic Process</title><categories>math.PR cs.DS math.CO</categories><msc-class>60J10 60C05 68W20 05C69 05C31</msc-class><journal-ref>Electronic Communications in Probability, 17 (2012) no. 5, 1-6</journal-ref><doi>10.1214/ECP.v17-1712</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ge and Stefankovic have recently introduced a novel two-variable graph
polynomial. When specialised to a bipartite graphs G and evaluated at the point
(1/2,1) this polynomial gives the number of independent sets in the graph.
Inspired by this polynomial, they also introduced a Markov chain which, if
rapidly mixing, would provide an efficient sampling procedure for independent
sets in G. This sampling procedure in turn would imply the existence of
efficient approximation algorithms for a number of significant counting
problems whose complexity is so far unresolved. The proposed Markov chain is
promising, in the sense that it overcomes the most obvious barrier to mixing.
However, we show here, by exhibiting a sequence of counterexamples, that the
mixing time of their Markov chain is exponential in the size of the input when
the input is chosen from a particular infinite family of bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5244</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5244</id><created>2011-09-24</created><updated>2012-02-09</updated><authors><author><keyname>Han</keyname><forenames>Kai</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Luo</keyname><forenames>Jun</forenames></author></authors><title>Minimum-Energy All-to-All Multicasting in Multi-hop Wireless Networks</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to new graphs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing energy-efficient all-to-all multicasting protocols is of of great
importance for multi-hop wireless networks such as wireless sensor networks and
wireless ad hoc networks. In an all-to-all multicast session, there exists a
set of wireless destination nodes, and each destination node needs to send some
data packets to all other destination nodes. We consider the problem of
building a shared multicast tree spanning the destination nodes such that the
total energy consumption of realizing an all-to-all multicast session using the
shared multicast tree is minimized. Since building such a multicast tree has
been proved to be NP-complete, we provide both centralized and distributed
approximation algorithms with provable approximation ratios for it. When the
transmission power of each wireless node is fixed, our centralized and
distributed algorithms have the approximation ratios of $4ln(\Delta+1)+7$ and
13, respectively, where $\Delta$ is the maximum node degree in the network.
When the transmission power of each wireless node is adjustable, both of our
centralized and distributed algorithms have the constant approximation ratio of
145.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5267</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5267</id><created>2011-09-24</created><updated>2011-12-20</updated><authors><author><keyname>Kobayashi</keyname><forenames>Naoki</forenames><affiliation>Graduate School of Information Sciences, Tohoku University</affiliation></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames><affiliation>Oxford University Computing Laboratory</affiliation></author></authors><title>Complexity of Model Checking Recursion Schemes for Fragments of the
  Modal Mu-Calculus</title><categories>cs.LO cs.PL</categories><proxy>LMCS</proxy><acm-class>F.3.1, D.2.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 4 (January
  18, 2012) lmcs:1211</journal-ref><doi>10.2168/LMCS-7(4:9)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ong has shown that the modal mu-calculus model checking problem
(equivalently, the alternating parity tree automaton (APT) acceptance problem)
of possibly-infinite ranked trees generated by order-n recursion schemes is
n-EXPTIME complete. We consider two subclasses of APT and investigate the
complexity of the respective acceptance problems. The main results are that,
for APT with a single priority, the problem is still n-EXPTIME complete;
whereas, for APT with a disjunctive transition function, the problem is
(n-1)-EXPTIME complete. This study was motivated by Kobayashi's recent work
showing that the resource usage verification of functional programs can be
reduced to the model checking of recursion schemes. As an application, we show
that the resource usage verification problem is (n-1)-EXPTIME complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5269</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5269</id><created>2011-09-24</created><updated>2012-09-24</updated><authors><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Porat</keyname><forenames>Benny</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author></authors><title>Parameterized Matching in the Streaming Model</title><categories>cs.DS</categories><comments>19 pages, 3 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of parameterized matching in a stream where we want to
output matches between a pattern of length m and the last m symbols of the
stream before the next symbol arrives. Parameterized matching is a natural
generalisation of exact matching where an arbitrary one-to-one relabelling of
pattern symbols is allowed. We show how this problem can be solved in constant
time per arriving stream symbol and sublinear, near optimal space with high
probability. Our results are surprising and important: it has been shown that
almost no streaming pattern matching problems can be solved (not even
randomised) in less than Theta(m) space, with exact matching as the only known
problem to have a sublinear, near optimal space solution. Here we demonstrate
that a similar sublinear, near optimal space solution is achievable for an even
more challenging problem. The proof is considerably more complex than that for
exact matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5278</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5278</id><created>2011-09-24</created><authors><author><keyname>Bickel</keyname><forenames>David R.</forenames></author></authors><title>Controlling the degree of caution in statistical inference with the
  Bayesian and frequentist approaches as opposite extremes</title><categories>math.ST cs.IT math.IT stat.ME stat.TH</categories><msc-class>62A99</msc-class><journal-ref>Bickel, D. R. (2012). Controlling the degree of caution in
  statistical inference with the Bayesian and frequentist approaches as
  opposite extremes. Electronic Journal of Statistics, 6, 686-709</journal-ref><doi>10.1214/12-EJS689</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In statistical practice, whether a Bayesian or frequentist approach is used
in inference depends not only on the availability of prior information but also
on the attitude taken toward partial prior information, with frequentists
tending to be more cautious than Bayesians. The proposed framework defines that
attitude in terms of a specified amount of caution, thereby enabling data
analysis at the level of caution desired and on the basis of any prior
information. The caution parameter represents the attitude toward partial prior
information in much the same way as a loss function represents the attitude
toward risk. When there is very little prior information and nonzero caution,
the resulting inferences correspond to those of the candidate confidence
intervals and p-values that are most similar to the credible intervals and
hypothesis probabilities of the specified Bayesian posterior. On the other
hand, in the presence of a known physical distribution of the parameter,
inferences are based only on the corresponding physical posterior. In those
extremes of either negligible prior information or complete prior information,
inferences do not depend on the degree of caution. Partial prior information
between those two extremes leads to intermediate inferences that are more
frequentistic to the extent that the caution is high and more Bayesian to the
extent that the caution is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5282</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5282</id><created>2011-09-24</created><authors><author><keyname>Zapata</keyname><forenames>Elena Villarreal</forenames></author><author><keyname>Salazar</keyname><forenames>Francisco Cruz Ordaz</forenames></author></authors><title>Aut\'omatas celulares elementales aplicados a la encriptaci\'on de datos</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For data ciphering a key is usually needed as a base, so it is indispensable
to have one that is strong and trustworthy, so as to keep others from accessing
the ciphered data. This requires a pseudo-random number generator that would
provide such a key, so it is proposed to work with cellular automata helped
along with \emph{Mathematica} to check that the rules and to what level are
actually pseudo-random. This project centers on the examination of possible
mathematical rules, analyzing their characteristics in a detailed manner, and
submitting them to a set of randomness tests with the end of knowing which of
them will enable us to obtain those pseudo-random numbers that will conform the
key for data ciphering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5302</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5302</id><created>2011-09-24</created><updated>2012-04-18</updated><authors><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Wenwu</forenames></author></authors><title>Simultaneous Codeword Optimization (SimCO) for Dictionary Update and
  Learning</title><categories>cs.LG cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the data-driven dictionary learning problem. The goal is to seek
an over-complete dictionary from which every training signal can be best
approximated by a linear combination of only a few codewords. This task is
often achieved by iteratively executing two operations: sparse coding and
dictionary update. In the literature, there are two benchmark mechanisms to
update a dictionary. The first approach, such as the MOD algorithm, is
characterized by searching for the optimal codewords while fixing the sparse
coefficients. In the second approach, represented by the K-SVD method, one
codeword and the related sparse coefficients are simultaneously updated while
all other codewords and coefficients remain unchanged. We propose a novel
framework that generalizes the aforementioned two methods. The unique feature
of our approach is that one can update an arbitrary set of codewords and the
corresponding sparse coefficients simultaneously: when sparse coefficients are
fixed, the underlying optimization problem is similar to that in the MOD
algorithm; when only one codeword is selected for update, it can be proved that
the proposed algorithm is equivalent to the K-SVD method; and more importantly,
our method allows us to update all codewords and all sparse coefficients
simultaneously, hence the term simultaneous codeword optimization (SimCO).
Under the proposed framework, we design two algorithms, namely, primitive and
regularized SimCO. We implement these two algorithms based on a simple gradient
descent mechanism. Simulations are provided to demonstrate the performance of
the proposed algorithms, as compared with two baseline algorithms MOD and
K-SVD. Results show that regularized SimCO is particularly appealing in terms
of both learning performance and running speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5311</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5311</id><created>2011-09-24</created><authors><author><keyname>Sapir</keyname><forenames>Marina</forenames></author></authors><title>Bias Plus Variance Decomposition for Survival Analysis Problems</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bias - variance decomposition of the expected error defined for regression
and classification problems is an important tool to study and compare different
algorithms, to find the best areas for their application. Here the
decomposition is introduced for the survival analysis problem. In our
experiments, we study bias -variance parts of the expected error for two
algorithms: original Cox proportional hazard regression and CoxPath, path
algorithm for L1-regularized Cox regression, on the series of increased
training sets. The experiments demonstrate that, contrary expectations, CoxPath
does not necessarily have an advantage over Cox regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5319</identifier>
 <datestamp>2012-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5319</id><created>2011-09-24</created><updated>2012-01-31</updated><authors><author><keyname>Enright</keyname><forenames>John J.</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Optimal Foraging of Renewable Resources</title><categories>cs.RO</categories><comments>14 pages, 4 figures, submitted to IEEE Transactions on Robotics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a team of agents in the plane searching for and visiting target
points that appear in a bounded environment according to a stochastic renewal
process with a known absolutely continuous spatial distribution. Agents must
detect targets with limited-range onboard sensors. It is desired to minimize
the expected waiting time between the appearance of a target point, and the
instant it is visited. When the sensing radius is small, the system time is
dominated by time spent searching, and it is shown that the optimal policy
requires the agents to search a region at a relative frequency proportional to
the square root of its renewal rate. On the other hand, when targets appear
frequently, the system time is dominated by time spent servicing known targets,
and it is shown that the optimal policy requires the agents to service a region
at a relative frequency proportional to the cube root of its renewal rate.
Furthermore, the presented algorithms in this case recover the optimal
performance achieved by agents with full information of the environment.
Simulation results verify the theoretical performance of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5322</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5322</id><created>2011-09-25</created><authors><author><keyname>Zlotnik</keyname><forenames>Anatoly</forenames></author><author><keyname>Li</keyname><forenames>Jr-Shin</forenames></author></authors><title>Synthesis of Optimal Ensemble Controls for Linear Systems using the
  Singular Value Decomposition</title><categories>math.OC cs.SY</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging and challenging area in mathematical control theory called
Ensemble Control encompasses a class of problems that involves the guidance of
an uncountably infinite collection of structurally identical dynamical systems,
which are indexed by a parameter set, by applying the same open-loop control.
The subject originates from the study of complex spin dynamics in Nuclear
Magnetic Resonance (NMR) spectroscopy and imaging (MRI). A fundamental question
concerns ensemble controllability, which determines the existence of controls
that transfer the system between desired initial and target states. For
ensembles of finite-dimensional time-varying linear systems, the necessary and
sufficient controllability conditions and analytical optimal control laws have
been shown to depend on the singular system of the operator characterizing the
system dynamics. Because analytical solutions are available only in the
simplest cases, there is a need to develop numerical methods for synthesizing
these controls. We introduce a direct, accurate, and computationally efficient
algorithm based on the singular value decomposition (SVD) that approximates
ensemble controls of minimum norm for such systems. This method enables the
application of ensemble control to engineering problems involving complex,
time-varying, and high-dimensional linear dynamic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5323</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5323</id><created>2011-09-25</created><authors><author><keyname>Lee</keyname><forenames>Jeremy</forenames></author></authors><title>Squiggle - A Glyph Recognizer for Gesture Input</title><categories>cs.HC cs.CV</categories><comments>10 pages</comments><acm-class>H.5.2; I.4.7; I.5.5; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Squiggle is a template-based glyph recognizer in the lineage of `$1
Recognizer' and `Protractor'. It seeks a good fit linear affine mapping between
the input and template glyphs which are represented as a list of milestone
points along the glyph path. The algorithm can recognize input glyphs invariant
of rotation, scaling, skew, and reflection symmetries. In practice the
algorithm is fast and robust enough to recognize user-generated glyphs as they
are being drawn in real time, and to project `shadows' of the matching
templates as feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5325</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5325</id><created>2011-09-25</created><updated>2013-02-18</updated><authors><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author></authors><title>Online Sum-Radii Clustering</title><categories>cs.DS</categories><comments>Supported by the project AlgoNow, co-financed by the European Union
  (European Social Fund - ESF) and Greek national funds, through the
  Operational Program &quot;Education and Lifelong Learning&quot;, under the research
  funding program THALES. An extended abstract of this work appeared in the
  Proc. of MFCS 2012, B. Rovan, V. Sassone, and P. Widmayer (Editors), LNCS
  7464, pp. 395-406, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Online Sum-Radii Clustering, n demand points arrive online and must be
irrevocably assigned to a cluster upon arrival. The cost of each cluster is the
sum of a fixed opening cost and its radius, and the objective is to minimize
the total cost of the clusters opened by the algorithm. We show that the
deterministic competitive ratio of Online Sum-Radii Clustering for general
metric spaces is \Theta(\log n), where the upper bound follows from a
primal-dual algorithm and holds for general metric spaces, and the lower bound
is valid for ternary Hierarchically Well-Separated Trees (HSTs) and for the
Euclidean plane. Combined with the results of (Csirik et al., MFCS 2010), this
result demonstrates that the deterministic competitive ratio of Online
Sum-Radii Clustering changes abruptly, from constant to logarithmic, when we
move from the line to the plane. We also show that Online Sum-Radii Clustering
in metric spaces induced by HSTs is closely related to the Parking Permit
problem introduced by (Meyerson, FOCS 2005). Exploiting the relation to Parking
Permit, we obtain a lower bound of \Omega(\log\log n) on the randomized
competitive ratio of Online Sum-Radii Clustering in tree metrics. Moreover, we
present a simple randomized O(\log n)-competitive algorithm, and a
deterministic O(\log\log n)-competitive algorithm for the fractional version of
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5329</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5329</id><created>2011-09-25</created><updated>2011-10-01</updated><authors><author><keyname>Lapenta</keyname><forenames>Giovanni</forenames></author></authors><title>Space Weather Prediction with Exascale Computing</title><categories>astro-ph.SR cs.CE physics.plasm-ph physics.space-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space weather refers to conditions on the Sun, in the interplanetary space
and in the Earth space environment that can influence the performance and
reliability of space-borne and ground-based technological systems and can
endanger human life or health. Adverse conditions in the space environment can
cause disruption of satellite operations, communications, navigation, and
electric power distribution grids, leading to a variety of socioeconomic
losses. The conditions in space are also linked to the Earth climate. The
activity of the Sun affects the total amount of heat and light reaching the
Earth and the amount of cosmic rays arriving in the atmosphere, a phenomenon
linked with the amount of cloud cover and precipitation. Given these great
impacts on society, space weather is attracting a growing attention and is the
subject of international efforts worldwide. We focus here on the steps
necessary for achieving a true physics-based ability to predict the arrival and
consequences of major space weather storms. Great disturbances in the space
environment are common but their precise arrival and impact on human activities
varies greatly. Simulating such a system is a grand- challenge, requiring
computing resources at the limit of what is possible not only with current
technology but also with the foreseeable future generations of super computers
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5336</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5336</id><created>2011-09-25</created><authors><author><keyname>Jafarian</keyname><forenames>Amin</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Achievable Rates for K-user Gaussian Interference Channels</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to study the achievable rates for a $K$ user
Gaussian interference channels for any SNR using a combination of lattice and
algebraic codes. Lattice codes are first used to transform the Gaussian
interference channel (G-IFC) into a discrete input-output noiseless channel,
and subsequently algebraic codes are developed to achieve good rates over this
new alphabet. In this context, a quantity called efficiency is introduced which
reflects the effectiveness of the algebraic coding strategy. The paper first
addresses the problem of finding high efficiency algebraic codes. A combination
of these codes with Construction-A lattices is then used to achieve non trivial
rates for the original Gaussian interference channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5338</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5338</id><created>2011-09-25</created><authors><author><keyname>Banerjee</keyname><forenames>Abhik</forenames></author><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Yeo</keyname><forenames>Chai Kiat</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Lee</keyname><forenames>Bu Sung</forenames></author></authors><title>Self-Organization of Wireless Ad Hoc Networks as Small Worlds Using Long
  Range Directional Beams</title><categories>cs.NI</categories><comments>Accepted to Joint workshop on complex networks and pervasive group
  communication (CCNet/PerGroup), in conjunction with IEEE Globecom 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how long range directional beams can be used for self-organization
of a wireless network to exhibit small world properties. Using simulation
results for randomized beamforming as a guideline, we identify crucial design
issues for algorithm design. Subsequently, we propose an algorithm for
deterministic creation of small worlds. We define a new centrality measure that
estimates the structural importance of nodes based on traffic flow in the
network, which is used to identify the optimum nodes for beamforming. This
results in significant reduction in path length while maintaining connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5346</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5346</id><created>2011-09-25</created><updated>2013-02-17</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author></authors><title>Polar codes for degradable quantum channels</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, 1 figure; v2: IEEE format, minor changes including new
  figure; v3: minor changes, accepted for publication in IEEE Transactions on
  Information Theory</comments><journal-ref>IEEE Transactions on Information Theory vol. 59, no. 7, pages
  4718-4729 (July 2013)</journal-ref><doi>10.1109/TIT.2013.2250575</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel polarization is a phenomenon in which a particular recursive encoding
induces a set of synthesized channels from many instances of a memoryless
channel, such that a fraction of the synthesized channels becomes near perfect
for data transmission and the other fraction becomes near useless for this
task. Mahdavifar and Vardy have recently exploited this phenomenon to construct
codes that achieve the symmetric private capacity for private data transmission
over a degraded wiretap channel. In the current paper, we build on their work
and demonstrate how to construct quantum wiretap polar codes that achieve the
symmetric private capacity of a degraded quantum wiretap channel with a
classical eavesdropper. Due to the Schumacher-Westmoreland correspondence
between quantum privacy and quantum coherence, we can construct quantum polar
codes by operating these quantum wiretap polar codes in superposition, much
like Devetak's technique for demonstrating the achievability of the coherent
information rate for quantum data transmission. Our scheme achieves the
symmetric coherent information rate for quantum channels that are degradable
with a classical environment. This condition on the environment may seem
restrictive, but we show that many quantum channels satisfy this criterion,
including amplitude damping channels, photon-detected jump channels, dephasing
channels, erasure channels, and cloning channels. Our quantum polar coding
scheme has the desirable properties of being channel-adapted and symmetric
capacity-achieving along with having an efficient encoder, but we have not
demonstrated that the decoding is efficient. Also, the scheme may require
entanglement assistance, but we show that the rate of entanglement consumption
vanishes in the limit of large blocklength if the channel is degradable with
classical environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5348</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5348</id><created>2011-09-25</created><authors><author><keyname>Tang</keyname><forenames>Shanjian</forenames></author><author><keyname>Yang</keyname><forenames>Zhou</forenames></author></authors><title>Dynkin Game of Stochastic Differential Equations with Random
  Coefficients, and Associated Backward Stochastic Partial Differential
  Variational Inequality</title><categories>math.OC cs.SY math.AP</categories><comments>40 pages</comments><msc-class>35R60, 47J20, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Dynkin game is considered for stochastic differential equations with random
coefficients. We first apply Qiu and Tang's maximum principle for backward
stochastic partial differential equations to generalize Krylov estimate for the
distribution of a Markov process to that of a non-Markov process, and establish
a generalized It\^o-Kunita-Wentzell's formula allowing the test function to be
a random field of It\^o's type which takes values in a suitable Sobolev space.
We then prove the verification theorem that the Nash equilibrium point and the
value of the Dynkin game are characterized by the strong solution of the
associated Hamilton-Jacobi-Bellman-Isaacs equation, which is currently a
backward stochastic partial differential variational inequality (BSPDVI, for
short) with two obstacles. We obtain the existence and uniqueness result and a
comparison theorem for strong solution of the BSPDVI. Moreover, we study the
monotonicity on the strong solution of the BSPDVI by the comparison theorem for
BSPDVI and define the free boundaries. Finally, we identify the counterparts
for an optimal stopping time problem as a special Dynkin game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5351</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5351</id><created>2011-09-25</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Data processing inequalities based on a certain structured class of
  information measures with application to estimation theory</title><categories>cs.IT math.IT</categories><comments>34 pages, one figure; submitted to IEEE Trans. on Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study data processing inequalities that are derived from a certain class
of generalized information measures, where a series of convex functions and
multiplicative likelihood ratios are nested alternately. While these
information measures can be viewed as a special case of the most general
Zakai-Ziv generalized information measure, this special nested structure calls
for attention and motivates our study. Specifically, a certain choice of the
convex functions leads to an information measure that extends the notion of the
Bhattacharyya distance (or the Chernoff divergence): While the ordinary
Bhattacharyya distance is based on the (weighted) geometric mean of two
replicas of the channel's conditional distribution, the more general
information measure allows an arbitrary number of such replicas. We apply the
data processing inequality induced by this information measure to a detailed
study of lower bounds of parameter estimation under additive white Gaussian
noise (AWGN) and show that in certain cases, tighter bounds can be obtained by
using more than two replicas. While the resulting lower bound may not compete
favorably with the best bounds available for the ordinary AWGN channel, the
advantage of the new lower bound, relative to the other bounds, becomes
significant in the presence of channel uncertainty, like unknown fading. This
different behavior in the presence of channel uncertainty is explained by the
convexity property of the information measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5370</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5370</id><created>2011-09-25</created><authors><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Feng</keyname><forenames>Wei</forenames></author><author><keyname>Cheung</keyname><forenames>William K.</forenames></author><author><keyname>Li</keyname><forenames>Chun-Hung</forenames></author></authors><title>Higher-Order Markov Tag-Topic Models for Tagged Documents and Images</title><categories>cs.CV cs.AI cs.IR cs.LG</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the topic modeling problem of tagged documents and images.
Higher-order relations among tagged documents and images are major and
ubiquitous characteristics, and play positive roles in extracting reliable and
interpretable topics. In this paper, we propose the tag-topic models (TTM) to
depict such higher-order topic structural dependencies within the Markov random
field (MRF) framework. First, we use the novel factor graph representation of
latent Dirichlet allocation (LDA)-based topic models from the MRF perspective,
and present an efficient loopy belief propagation (BP) algorithm for
approximate inference and parameter estimation. Second, we propose the factor
hypergraph representation of TTM, and focus on both pairwise and higher-order
relation modeling among tagged documents and images. Efficient loopy BP
algorithm is developed to learn TTM, which encourages the topic labeling
smoothness among tagged documents and images. Extensive experimental results
confirm the incorporation of higher-order relations to be effective in
enhancing the overall topic modeling performance, when compared with current
state-of-the-art topic models, in many text and image mining tasks of broad
interests such as word and link prediction, document classification, and tag
recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5373</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5373</id><created>2011-09-25</created><updated>2012-10-25</updated><authors><author><keyname>Tandon</keyname><forenames>Ravi</forenames><affiliation>Shitz</affiliation></author><author><keyname>Mohajer</keyname><forenames>Soheil</forenames><affiliation>Shitz</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Degrees of Freedom Region of the MIMO Interference Channel with Output
  Feedback and Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-user multiple-input multiple-output (MIMO) interference channel (IC)
with arbitrary number of antennas at each terminal is considered and the
degrees of freedom (DoF) region is characterized in the presence of noiseless
channel output feedback from each receiver to its respective transmitter and
availability of delayed channel state information at the transmitters (CSIT).
It is shown that having output feedback and delayed CSIT can strictly enlarge
the DoF region of the MIMO IC when compared to the case in which only delayed
CSIT is present. The proposed coding schemes that achieve the corresponding DoF
region with feedback and delayed CSIT utilize both resources, i.e., feedback
and delayed CSIT in a non-trivial manner. It is also shown that the DoF region
with local feedback and delayed CSIT is equal to the DoF region with global
feedback and delayed CSIT, i.e., local feedback and delayed CSIT is equivalent
to global feedback and delayed CSIT from the perspective of the degrees of
freedom region. The converse is proved for a stronger setting in which the
channels to the two receivers need not be statistically equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5375</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5375</id><created>2011-09-25</created><authors><author><keyname>Albano</keyname><forenames>Paolo</forenames></author><author><keyname>Cannarsa</keyname><forenames>Piermarco</forenames></author><author><keyname>Nguyen</keyname><forenames>Khai Tien</forenames></author><author><keyname>Sinestrari</keyname><forenames>Carlo</forenames></author></authors><title>Singular gradient flow of the distance function and homotopy equivalence</title><categories>math.AP cs.SY math.DG math.OC</categories><msc-class>35A21, 26B25, 49J52, 55P10</msc-class><journal-ref>Math. Ann. 356, 23-43 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a generally shared opinion that significant information about the
topology of a bounded domain $\Omega $ of a riemannian manifold $M$ is encoded
into the properties of the distance, $d_{\partial\Omega}$, %,
$d:\Omega\rightarrow [0,\infty [$, from the boundary of $\Omega$. To confirm
such an idea we propose an approach based on the invariance of the singular set
of the distance function with respect to the generalized gradient flow of of
$d_{\partial\Omega}$. As an application, we deduce that such a singular set has
the same homotopy type as $\Omega$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5382</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5382</id><created>2011-09-25</created><authors><author><keyname>Galli</keyname><forenames>Stefano</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Discrete-Time Block Models for Transmission Line Channels: Static and
  Doubly Selective Cases</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most methodologies for modeling Transmission Line (TL) based channels define
the input-output relationship in the frequency domain (FD) and handle the TL
resorting to a two-port network (2PN) formalism. These techniques have not yet
been formally mapped into a discrete-time (DT) block model, which is useful to
simulate and estimate the channel response as well as to design optimal
precoding strategies. TL methods also fall short when they are applied to Time
Varying (TV) systems, such as the power line channel. The objective of this
paper is to establish if and how one can introduce a DT block model for the
Power Line Channel. We prove that it is possible to use Lifting and Trailing
Zeros (L&amp;TZ) techniques to derive a DT block model that maps the TL-based
input-output description directly in the time domain (TD) block channel model.
More specifically, we find an interesting relationship between the elements of
an ABCD matrix, defined in the FD, and filtering kernels that allow an elegant
representation of the channel in the TD. The same formalism is valid for both
the Linear Time Invariant (LTI) and the Linear TV (LTV) cases, and bridges
communications and signal processing methodologies with circuits and systems
analysis tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5388</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5388</id><created>2011-09-25</created><updated>2013-05-21</updated><authors><author><keyname>Bhadauria</keyname><forenames>Rohit</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Survey on Security Issues in Cloud Computing</title><categories>cs.CR</categories><comments>This review is incomplete and contains some erroneous interpretation
  of existing works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing holds the potential to eliminate the requirements for setting
up of high-cost computing infrastructure for the IT-based solutions and
services that the industry uses. It promises to provide a flexible IT
architecture, accessible through internet for lightweight portable devices.
This would allow many-fold increase in the capacity or capabilities of the
existing and new software. In a cloud computing environment, the entire data
reside over a set of networked resources, enabling the data to be accessed
through virtual machines. Since these data centers may lie in any corner of the
world beyond the reach and control of users, there are multifarious security
and privacy challenges that need to be understood and taken care of. Also, one
can never deny the possibility of a server breakdown that has been witnessed,
rather quite often in the recent times. There are various issues that need to
be dealt with respect to security and privacy in a cloud computing scenario.
This extensive survey paper aims to elaborate and analyze the numerous
unresolved issues threatening the Cloud computing adoption and diffusion
affecting the various stake-holders linked to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5396</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5396</id><created>2011-09-25</created><authors><author><keyname>Annapureddy</keyname><forenames>V. Sreekanth</forenames></author><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Degrees of Freedom of Interference Channels with CoMP Transmission and
  Reception</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 58, no. 9, pp.
  5740-5760, Sep. 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Degrees of Freedom (DoF) of the K-user interference channel with
coordinated multi-point (CoMP) transmission and reception. Each message is
jointly transmitted by M_t successive transmitters, and is jointly received by
M_r successive receivers. We refer to this channel as the CoMP channel with a
transmit cooperation order of M_t and receive cooperation order of M_r. Since
the channel has a total of K transmit antennas and K receive antennas, the
maximum possible DoF is equal to K. We show that the CoMP channel has K DoF if
and only if M_t + M_r is greater than or equal to K+1. For the general case, we
derive an outer bound that states that the DoF is bounded above by the ceiling
of (K+M_t+M_r-2)/2. For the special case with only CoMP transmission, i.e, M_r
= 1, we propose a scheme that can achieve (K+M_t-1)/2 DoF for all K &lt; 10, and
conjecture that the result holds true for all K . The achievability proofs are
based on the notion of algebraic independence from algebraic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5404</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5404</id><created>2011-09-25</created><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Jose M.</forenames></author></authors><title>Towards Optimal Learning of Chain Graphs</title><categories>stat.ML cs.AI math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend Meek's conjecture (Meek 1997) from directed and
acyclic graphs to chain graphs, and prove that the extended conjecture is true.
Specifically, we prove that if a chain graph H is an independence map of the
independence model induced by another chain graph G, then (i) G can be
transformed into H by a sequence of directed and undirected edge additions and
feasible splits and mergings, and (ii) after each operation in the sequence H
remains an independence map of the independence model induced by G. Our result
has the same important consequence for learning chain graphs from data as the
proof of Meek's conjecture in (Chickering 2002) had for learning Bayesian
networks from data: It makes it possible to develop efficient and
asymptotically correct learning algorithms under mild assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5415</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5415</id><created>2011-09-25</created><updated>2013-03-17</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels</title><categories>cs.IT math.IT</categories><comments>accepted to IEEE Transactions on Information Theory, 2013</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 59, No. 8, pp.
  4889-4914, August 2013</journal-ref><doi>10.1109/TIT.2013.2254171</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore two fundamental questions at the intersection of sampling theory
and information theory: how channel capacity is affected by sampling below the
channel's Nyquist rate, and what sub-Nyquist sampling strategy should be
employed to maximize capacity. In particular, we derive the capacity of sampled
analog channels for three prevalent sampling strategies: sampling with
filtering, sampling with filter banks, and sampling with modulation and filter
banks. These sampling mechanisms subsume most nonuniform sampling techniques
applied in practice. Our analyses illuminate interesting connections between
under-sampled channels and multiple-input multiple-output channels. The optimal
sampling structures are shown to extract out the frequencies with the highest
SNR from each aliased frequency set, while suppressing aliasing and out-of-band
noise. We also highlight connections between undersampled channel capacity and
minimum mean-squared error (MSE) estimation from sampled data. In particular,
we show that the filters maximizing capacity and the ones minimizing MSE are
equivalent under both filtering and filter-bank sampling strategies. These
results demonstrate the effect upon channel capacity of sub-Nyquist sampling
techniques, and characterize the tradeoff between information rate and sampling
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5416</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5416</id><created>2011-09-25</created><updated>2013-02-25</updated><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Matrix Code</title><categories>cs.PL</categories><comments>39 pages, 19 figures; extensions and minor corrections</comments><report-no>DCS-341-IR</report-no><acm-class>D.1.4; D.2.4; D.3.3; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix Code gives imperative programming a mathematical semantics and
heuristic power comparable in quality to functional and logic programming. A
program in Matrix Code is developed incrementally from a specification in
pre/post-condition form. The computations of a code matrix are characterized by
powers of the matrix when it is interpreted as a transformation in a space of
vectors of logical conditions. Correctness of a code matrix is expressed in
terms of a fixpoint of the transformation. The abstract machine for Matrix Code
is the dual-state machine, which we present as a variant of the classical
finite-state machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5420</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5420</id><created>2011-09-25</created><updated>2012-12-12</updated><authors><author><keyname>Zhou</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Incremental Relaying for the Gaussian Interference Channel with a
  Degraded Broadcasting Relay</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. on Inf. Theory</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper studies incremental relay strategies for a two-user Gaussian
relay-interference channel with an in-band-reception and
out-of-band-transmission relay, where the link between the relay and the two
receivers is modelled as a degraded broadcast channel. It is shown that
generalized hash-and-forward (GHF) can achieve the capacity region of this
channel to within a constant number of bits in a certain weak relay regime,
where the transmitter-to-relay link gains are not unboundedly stronger than the
interference links between the transmitters and the receivers. The GHF relaying
strategy is ideally suited for the broadcasting relay because it can be
implemented in an incremental fashion, i.e., the relay message to one receiver
is a degraded version of the message to the other receiver. A
generalized-degree-of-freedom (GDoF) analysis in the high signal-to-noise ratio
(SNR) regime reveals that in the symmetric channel setting, each common relay
bit can improve the sum rate roughly by either one bit or two bits
asymptotically depending on the operating regime, and the rate gain can be
interpreted as coming solely from the improvement of the common message rates,
or alternatively in the very weak interference regime as solely coming from the
rate improvement of the private messages. Further, this paper studies an
asymmetric case in which the relay has only a single single link to one of the
destinations. It is shown that with only one relay-destination link, the
approximate capacity region can be established for a larger regime of channel
parameters. Further, from a GDoF point of view, the sum-capacity gain due to
the relay can now be thought as coming from either signal relaying only, or
interference forwarding only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5423</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5423</id><created>2011-09-25</created><updated>2011-10-01</updated><authors><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames></author><author><keyname>Gallagher</keyname><forenames>Aaron</forenames></author></authors><title>Reconciling taxonomy and phylogenetic inference: formalism and
  algorithms for describing discord and inferring taxonomic roots</title><categories>q-bio.PE cs.DS</categories><comments>Version submitted to Algorithms for Molecular Biology. A number of
  fixes from previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although taxonomy is often used informally to evaluate the results of
phylogenetic inference and find the root of phylogenetic trees, algorithmic
methods to do so are lacking. In this paper we formalize these procedures and
develop algorithms to solve the relevant problems. In particular, we introduce
a new algorithm that solves a &quot;subcoloring&quot; problem for expressing the
difference between the taxonomy and phylogeny at a given rank. This algorithm
improves upon the current best algorithm in terms of asymptotic complexity for
the parameter regime of interest; we also describe a branch-and-bound algorithm
that saves orders of magnitude in computation on real data sets. We also
develop a formalism and an algorithm for rooting phylogenetic trees according
to a taxonomy. All of these algorithms are implemented in freely-available
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5426</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5426</id><created>2011-09-25</created><authors><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Kim</keyname><forenames>Cheulsoon</forenames></author></authors><title>The capacity for the linear time-invariant Gaussian relay channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures. Submitted to ICASSP 2012</comments><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Gaussian relay channel with linear time-invariant relay
filtering is considered. Based on spectral theory for stationary processes, the
maximum achievable rate for this subclass of linear Gaussian relay operation is
obtained in finite-letter characterization. The maximum rate can be achieved by
dividing the overall frequency band into at most eight subbands and by making
the relay behave as an instantaneous amplify-and-forward relay at each subband.
Numerical results are provided to evaluate the performance of LTI relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5430</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5430</id><created>2011-09-25</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>Recovery of Block-Sparse Representations from Noisy Observations via
  Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>7 pages in two-column format; 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recovering the sparsity pattern of block-sparse
signals from noise-corrupted measurements. A simple, efficient recovery method,
namely, a block-version of the orthogonal matching pursuit (OMP) method, is
considered in this paper and its behavior for recovering the block-sparsity
pattern is analyzed. We provide sufficient conditions under which the
block-version of the OMP can successfully recover the block-sparse
representations in the presence of noise. Our analysis reveals that exploiting
block-sparsity can improve the recovery ability and lead to a guaranteed
recovery for a higher sparsity level. Numerical results are presented to
corroborate our theoretical claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5433</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5433</id><created>2011-09-25</created><updated>2012-10-16</updated><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Optimal Precoding Design and Power Allocation for Decentralized
  Detection of Deterministic Signals</title><categories>cs.IR</categories><comments>13 pages in two-column format; 6 figures</comments><doi>10.1109/TSP.2012.2190598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a decentralized detection problem in a power-constrained wireless
sensor networks (WSNs), in which a number of sensor nodes collaborate to detect
the presence of a deterministic vector signal. The signal to be detected is
assumed known \emph{a priori}. Given a constraint on the total amount of
transmit power, we investigate the optimal linear precoding design for each
sensor node. More specifically, in order to achieve the best detection
performance, shall sensor nodes transmit their raw data to the fusion center
(FC), or transmit compressed versions of their original data? The optimal power
allocation among sensors is studied as well. Also, assuming a fixed total
transmit power, we examine how the detection performance behaves with the
number of sensors in the network. A new concept &quot;detection outage&quot; is proposed
to quantify the reliability of the overall detection system. Finally,
decentralized detection with unknown signals is studied. Numerical results are
conducted to corroborate our theoretical analysis and to illustrate the
performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5453</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5453</id><created>2011-09-26</created><updated>2012-04-03</updated><authors><author><keyname>Katsuki</keyname><forenames>Takayuki</forenames></author><author><keyname>Torii</keyname><forenames>Akira</forenames></author><author><keyname>Inoue</keyname><forenames>Masato</forenames></author></authors><title>Posterior Mean Super-resolution with a Causal Gaussian Markov Random
  Field Prior</title><categories>cs.CV</categories><comments>11 pages, 20 figures, submitted to IEEE Transactions on Image
  Processing</comments><msc-class>68U10, 62F15</msc-class><acm-class>I.4.5; I.4.4; G.3</acm-class><doi>10.1109/TIP.2012.2189578</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian image super-resolution (SR) method with a causal
Gaussian Markov random field (MRF) prior. SR is a technique to estimate a
spatially high-resolution image from given multiple low-resolution images. An
MRF model with the line process supplies a preferable prior for natural images
with edges. We improve the existing image transformation model, the compound
MRF model, and its hyperparameter prior model. We also derive the optimal
estimator -- not the joint maximum a posteriori (MAP) or marginalized maximum
likelihood (ML), but the posterior mean (PM) -- from the objective function of
the L2-norm (mean square error) -based peak signal-to-noise ratio (PSNR). Point
estimates such as MAP and ML are generally not stable in ill-posed
high-dimensional problems because of overfitting, while PM is a stable
estimator because all the parameters in the model are evaluated as
distributions. The estimator is numerically determined by using variational
Bayes. Variational Bayes is a widely used method that approximately determines
a complicated posterior distribution, but it is generally hard to use because
it needs the conjugate prior. We solve this problem with simple Taylor
approximations. Experimental results have shown that the proposed method is
more accurate or comparable to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5454</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5454</id><created>2011-09-26</created><authors><author><keyname>Telesford</keyname><forenames>Qawi K.</forenames></author><author><keyname>Joyce</keyname><forenames>Karen E.</forenames></author><author><keyname>Hayasaka</keyname><forenames>Satoru</forenames></author><author><keyname>Burdette</keyname><forenames>Jonathan H.</forenames></author><author><keyname>Laurienti</keyname><forenames>Paul J.</forenames></author></authors><title>The ubiquity of small-world networks</title><categories>nlin.AO cs.SI physics.soc-ph</categories><comments>29 pages, 8 figures, 2 tables</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Small-world networks by Watts and Strogatz are a class of networks that are
highly clustered, like regular lattices, yet have small characteristic path
lengths, like random graphs. These characteristics result in networks with
unique properties of regional specialization with efficient information
transfer. Social networks are intuitive examples of this organization with
cliques or clusters of friends being interconnected, but each person is really
only 5-6 people away from anyone else. While this qualitative definition has
prevailed in network science theory, in application, the standard quantitative
application is to compare path length (a surrogate measure of distributed
processing) and clustering (a surrogate measure of regional specialization) to
an equivalent random network. It is demonstrated here that comparing network
clustering to that of a random network can result in aberrant findings and
networks once thought to exhibit small-world properties may not. We propose a
new small-world metric, {\omega} (omega), which compares network clustering to
an equivalent lattice network and path length to a random network, as Watts and
Strogatz originally described. Example networks are presented that would be
interpreted as small-world when clustering is compared to a random network but
are not small-world according to {\omega}. These findings have significant
implications in network science as small-world networks have unique topological
properties, and it is critical to accurately distinguish them from networks
without simultaneous high clustering and low path length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5460</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5460</id><created>2011-09-26</created><authors><author><keyname>Liang</keyname><forenames>Xiao</forenames></author><author><keyname>Zheng</keyname><forenames>Xudong</forenames></author><author><keyname>Lv</keyname><forenames>Weifeng</forenames></author><author><keyname>Zhu</keyname><forenames>Tongyu</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>The scaling of human mobility by taxis is exponential</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages, 7 figures</comments><journal-ref>Physica A 391 (2012) 2135-2144</journal-ref><doi>10.1016/j.physa.2011.11.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a significant factor in urban planning, traffic forecasting and prediction
of epidemics, modeling patterns of human mobility draws intensive attention
from researchers for decades. Power-law distribution and its variations are
observed from quite a few real-world human mobility datasets such as the
movements of banking notes, trackings of cell phone users' locations and
trajectories of vehicles. In this paper, we build models for 20 million
trajectories with fine granularity collected from more than 10 thousand taxis
in Beijing. In contrast to most models observed in human mobility data, the
taxis' traveling displacements in urban areas tend to follow an exponential
distribution instead of a power-law. Similarly, the elapsed time can also be
well approximated by an exponential distribution. Worth mentioning, analysis of
the interevent time indicates the bursty nature of human mobility, similar to
many other human activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5466</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5466</id><created>2011-09-26</created><authors><author><keyname>Malik</keyname><forenames>Waseem A.</forenames></author><author><keyname>Martins</keyname><forenames>Nuno C.</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author></authors><title>Optimal Sensor Placement for Intruder Detection</title><categories>cs.SY math.OC</categories><comments>63 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the centralized detection of an intruder, whose location is
modeled as uniform across a specified set of points, using an optimally placed
team of sensors. These sensors make conditionally independent observations. The
local detectors at the sensors are also assumed to be identical, with detection
probability $(P_{_{D}})$ and false alarm probability $(P_{_{F}})$. We formulate
the problem as an N-ary hypothesis testing problem, jointly optimizing the
sensor placement and detection policies at the fusion center. We prove that
uniform sensor placement is never strictly optimal when the number of sensors
$(M)$ equals the number of placement points $(N)$. We prove that for $N_{2} &gt;
N_{1} &gt; M$, where $N_{1},N_{2}$ are number of placement points, the framework
utilizing $M$ sensors and $N_{1}$ placement points has the same optimal
placement structure as the one utilizing $M$ sensors and $N_{2}$ placement
points. For $M\leq 5$ and for fixed $P_{_{D}}$, increasing $P_{_{F}}$ leads to
optimal placements that are higher in the majorization-based placement scale.
Similarly for $M\leq 5$ and for fixed $P_{_{F}}$, increasing $P_{_{D}}$ leads
to optimal placements that are higher in the majorization-based placement
scale. For $M&gt;5$, this result does not necessarily hold and we provide a simple
counterexample. It is conjectured that the set of optimal placements for a
given $(M,N)$ can always be placed on a majorization-based placement scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5468</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5468</id><created>2011-09-26</created><authors><author><keyname>Kusakari</keyname><forenames>Keiichirou</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Isogai</keyname><forenames>Yasuo</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Sakai</keyname><forenames>Masahiko</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA</affiliation></author></authors><title>Static Dependency Pair Method based on Strong Computability for
  Higher-Order Rewrite Systems</title><categories>cs.LO</categories><comments>IEICE Transactions on Information and Systems (2009)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order rewrite systems (HRSs) and simply-typed term rewriting systems
(STRSs) are computational models of functional programs. We recently proposed
an extremely powerful method, the static dependency pair method, which is based
on the notion of strong computability, in order to prove termination in STRSs.
In this paper, we extend the method to HRSs. Since HRSs include
\lambda-abstraction but STRSs do not, we restructure the static dependency pair
method to allow \lambda-abstraction, and show that the static dependency pair
method also works well on HRSs without new restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5482</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5482</id><created>2011-09-26</created><authors><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Social Learning in a Changing World</title><categories>cs.SI physics.soc-ph</categories><comments>18 Pages. The 7th Workshop on Internet &amp; Network Economics (WINE
  2011)</comments><journal-ref>The 7th Workshop on Internet and Network Economics, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model of learning on social networks in dynamic environments,
describing a group of agents who are each trying to estimate an underlying
state that varies over time, given access to weak signals and the estimates of
their social network neighbors.
  We study three models of agent behavior. In the &quot;fixed response&quot; model,
agents use a fixed linear combination to incorporate information from their
peers into their own estimate. This can be thought of as an extension of the
DeGroot model to a dynamic setting. In the &quot;best response&quot; model, players
calculate minimum variance linear estimators of the underlying state.
  We show that regardless of the initial configuration, fixed response dynamics
converge to a steady state, and that the same holds for best response on the
complete graph. We show that best response dynamics can, in the long term, lead
to estimators with higher variance than is achievable using well chosen fixed
responses.
  The &quot;penultimate prediction&quot; model is an elaboration of the best response
model. While this model only slightly complicates the computations required of
the agents, we show that in some cases it greatly increases the efficiency of
learning, and on complete graphs is in fact optimal, in a strong sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5484</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5484</id><created>2011-09-26</created><authors><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author><author><keyname>Devillers</keyname><forenames>Bertrand</forenames></author></authors><title>Two-hop Communication with Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures. To be presented at the 4th International Workshop
  on Computational Advances in Multi-Sensor Adaptive Processing, Dec. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication nodes with the ability to harvest energy from the environment
have the potential to operate beyond the timeframe limited by the finite
capacity of their batteries; and accordingly, to extend the overall network
lifetime. However, the optimization of the communication system in the presence
of energy harvesting devices requires a new paradigm in terms of power
allocation since the energy becomes available over time. In this paper, we
consider the problem of two-hop relaying in the presence of energy harvesting
nodes. We identify the optimal offline transmission scheme for energy
harvesting source and relay when the relay operates in the full-duplex mode. In
the case of a half-duplex relay, we provide the optimal transmission scheme
when the source has a single energy packet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5488</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5488</id><created>2011-09-26</created><authors><author><keyname>Valot</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Langella</keyname><forenames>Olivier</forenames></author><author><keyname>Nano</keyname><forenames>Edlira</forenames></author><author><keyname>Zivy</keyname><forenames>Michel</forenames></author></authors><title>MassChroQ: A versatile tool for mass spectrometry quantification</title><categories>q-bio.QM cs.CE</categories><comments>23 pages, 8 figures</comments><journal-ref>Proteomics, volume 11, issue 17, pages 3572-3577, September 2011</journal-ref><doi>10.1002/pmic.201100120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, many software tools have been developed to perform quantification
in LC-MS analyses. However, most of them are specific to either a
quantification strategy (e.g. label-free or isotopic labelling) or a
mass-spectrometry system (e.g. high or low resolution).
  In this context, we have developed MassChroQ, a versatile software that
performs LC-MS data alignment and peptide quantification by peak area
integration on extracted ion chromatograms. MassChroQ is suitable for
quantification with or without labelling and is not limited to high resolution
systems. Peptides of interest (for example all the identified peptides) can be
determined automatically or manually by providing targeted m/z and retention
time values. It can handle large experiments that include protein or peptide
fractionation (as SDS-PAGE, 2D-LC). It is fully configurable. Every processing
step is traceable, the produced data are in open standard format and its
modularity allows easy integration into proteomic pipelines. The output results
are ready for use in statistical analyses.
  Evaluation of MassChroQ on complex label-free data obtained from low and high
resolution mass spectrometers showed low CVs for technical reproducibility
(1.4%) and high coefficients of correlation to protein quantity (0.98).
  MassChroQ is freely available under the GNU General Public Licence v3.0 at
http://pappso.inra.fr/bioinfo/masschroq/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5490</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5490</id><created>2011-09-26</created><updated>2012-03-13</updated><authors><author><keyname>Devillers</keyname><forenames>Bertrand</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author></authors><title>A General Framework for the Optimization of Energy Harvesting
  Communication Systems with Battery Imperfections</title><categories>cs.IT math.IT</categories><comments>To appear in Journal of Communications and Networks, Special Issue on
  Energy Harvesting in Wireless Networks, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting has emerged as a powerful technology for complementing
current battery-powered communication systems in order to extend their
lifetime. In this paper a general framework is introduced for the optimization
of communication systems in which the transmitter is able to harvest energy
from its environment. Assuming that the energy arrival process is known
non-causally at the transmitter, the structure of the optimal transmission
scheme, which maximizes the amount of transmitted data by a given deadline, is
identified. Our framework includes models with continuous energy arrival as
well as battery constraints. A battery that suffers from energy leakage is
studied further, and the optimal transmission scheme is characterized for a
constant leakage rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5505</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5505</id><created>2011-09-26</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Boyer</keyname><forenames>Benoit</forenames></author><author><keyname>Nguyen</keyname><forenames>Thanh Hung</forenames></author></authors><title>On the Simulation of Time-Triggered Systems on a Chip with BIP</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we present functional models for software and hardware
components of Time-Triggered Systems on a Chip (TTSoC). These are modeled in
the asynchronous component based language BIP. We demonstrate the usability of
our components for simulation of software which is developed for the TTSoC. Our
software comprises services and an application part. Our approach allows us to
simulate and validate aspects of the software system at an early stage in the
development process and without the need to have the TTSoC hardware at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5506</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5506</id><created>2011-09-26</created><authors><author><keyname>Tian</keyname><forenames>Cong</forenames></author><author><keyname>Duan</keyname><forenames>Zhenhua</forenames></author></authors><title>Detecting Spurious Counterexamples Efficiently in Abstract Model
  Checking</title><categories>cs.LO</categories><comments>13 pages,6 figures. arXiv admin note: substantial text overlap with
  arXiv:1007.3569</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstraction is one of the most important strategies for dealing with the
state space explosion problem in model checking. In the abstract model, the
state space is largely reduced, however, a counterexample found in such a model
may not be a real counterexample in the concrete model. Accordingly, the
abstract model needs to be further refined. How to check whether or not a
reported counterexample is spurious is a key problem in the
abstraction-refinement loop. In this paper, a formal definition for spurious
path is given. Based on it, efficient algorithms for detecting spurious
counterexamples are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5520</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5520</id><created>2011-09-26</created><authors><author><keyname>Waaijers</keyname><forenames>Leo</forenames></author></authors><title>Viva the h-index</title><categories>physics.soc-ph cs.DL</categories><comments>Commenting article http://arxiv.org/abs/1108.3901 by Ludo Waltman and
  Nees Jan van Neck on &quot;The inconsistency of the h-index&quot;</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In their article 'The inconsistency of the h-index' Ludo Waltman and Nees Jan
van Neck give three examples to demonstrate the inconsistency of the h-index.
As will be explained, a little extension of their examples just illustrate the
opposite, a stable feature of the h-index. For starting authors it, the h-index
that is, focusses on the number of articles; for experienced authors its focus
shifts towards the citation scores. This feature may be liked or not but does
not make the h-index an inconsistent and inappropriate indicator, as the
authors claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5522</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5522</id><created>2011-09-26</created><authors><author><keyname>Mittermayr</keyname><forenames>Robert</forenames></author><author><keyname>Blieberger</keyname><forenames>Johann</forenames></author></authors><title>Shared Memory Concurrent System Verification using Kronecker Algebra</title><categories>cs.LO</categories><comments>31 pages</comments><report-no>183/1-155</report-no><acm-class>D.2.4; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The verification of multithreaded software is still a challenge. This comes
mainly from the fact that the number of thread interleavings grows
exponentially in the number of threads. The idea that thread interleavings can
be studied with a matrix calculus is a novel approach in this research area.
Our sparse matrix representations of the program are manipulated using a lazy
implementation of Kronecker algebra. One goal is the generation of a data
structure called Concurrent Program Graph (CPG) which describes all possible
interleavings and incorporates synchronization while preserving completeness.
We prove that CPGs in general can be represented by sparse adjacency matrices.
Thus the number of entries in the matrices is linear in their number of lines.
Hence efficient algorithms can be applied to CPGs. In addition, due to
synchronization only very small parts of the resulting matrix are actually
needed, whereas the rest is unreachable in terms of automata. Thanks to the
lazy implementation of the matrix operations the unreachable parts are never
calculated. This speeds up processing significantly and shows that this
approach is very promising. Various applications including data flow analysis
can be performed on CPGs. Furthermore, the structure of the matrices can be
used to prove properties of the underlying program for an arbitrary number of
threads. For example, deadlock freedom is proved for a large class of programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5526</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5526</id><created>2011-09-26</created><updated>2011-10-16</updated><authors><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>Are random axioms useful?</title><categories>math.LO cs.IT cs.LO math.IT</categories><msc-class>03F99</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous G\&quot;odel incompleteness theorem says that for every sufficiently
rich formal theory (containing formal arithmetic in some natural sense) there
exist true unprovable statements. Such statements would be natural candidates
for being added as axioms, but where can we obtain them? One classical (and
well studied) approach is to add (to some theory T) an axiom that claims the
consistency of T. In this note we discuss the other one (motivated by Chaitin's
version of the G\&quot;odel theorem) and show that it is not really useful (in the
sense that it does not help us to prove new interesting theorems), at least if
we are not limiting the proof complexity. We discuss also some related
questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5531</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5531</id><created>2011-09-26</created><authors><author><keyname>Plotnikov</keyname><forenames>Anatoly D.</forenames></author></authors><title>On the relationship between classes P and NP</title><categories>cs.DM cs.CC</categories><comments>9 pages</comments><msc-class>68Q15</msc-class><acm-class>F.2.2; G.2.1; G.2.2</acm-class><journal-ref>On the relationship between classes P and NP. J. Comput. Sci., 8,
  2012: p.1036-1040</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discusses the relationship between the known classes P and
NP. We show that the difficulties in solving problem &quot;P versus NP&quot; have
methodological in nature. An algorithm for solving any problem is sensitive to
even small changes in its formulation. As we will shown in the paper, these
difficulties are exactly in the formulation of some problems of the class NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5534</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5534</id><created>2011-09-26</created><updated>2011-09-26</updated><authors><author><keyname>Li</keyname><forenames>Shasha</forenames></author><author><keyname>Li</keyname><forenames>Xueliang</forenames></author></authors><title>Note on the complexity of deciding the rainbow connectedness for
  bipartite graphs</title><categories>cs.CC math.CO</categories><comments>6 pages</comments><msc-class>05C15, 05C40, 68Q25, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A path in an edge-colored graph is said to be a rainbow path if no two edges
on the path have the same color. An edge-colored graph is (strongly) rainbow
connected if there exists a rainbow (geodesic) path between every pair of
vertices. The (strong) rainbow connection number of $G$, denoted by ($scr(G)$,
respectively) $rc(G)$, is the smallest number of colors that are needed in
order to make $G$ (strongly) rainbow connected. Though for a general graph $G$
it is NP-Complete to decide whether $rc(G)=2$, in this paper, we show that the
problem becomes easy when $G$ is a bipartite graph. Moreover, it is known that
deciding whether a given edge-colored (with an unbound number of colors) graph
is rainbow connected is NP-Complete. We will prove that it is still NP-Complete
even when the edge-colored graph is bipartite. We also show that a few NP-hard
problems on rainbow connection are indeed NP-Complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5542</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5542</id><created>2011-09-26</created><updated>2012-09-16</updated><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Gaming security by obscurity</title><categories>cs.CR cs.GT cs.LO</categories><comments>15 pages, 9 figures, 2 tables; final version appeared in the
  Proceedings of New Security Paradigms Workshop 2011 (ACM 2011); typos
  corrected</comments><msc-class>91A80</msc-class><acm-class>K.6.5; F.2; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon sought security against the attacker with unlimited computational
powers: *if an information source conveys some information, then Shannon's
attacker will surely extract that information*. Diffie and Hellman refined
Shannon's attacker model by taking into account the fact that the real
attackers are computationally limited. This idea became one of the greatest new
paradigms in computer science, and led to modern cryptography.
  Shannon also sought security against the attacker with unlimited logical and
observational powers, expressed through the maxim that &quot;the enemy knows the
system&quot;. This view is still endorsed in cryptography. The popular formulation,
going back to Kerckhoffs, is that &quot;there is no security by obscurity&quot;, meaning
that the algorithms cannot be kept obscured from the attacker, and that
security should only rely upon the secret keys. In fact, modern cryptography
goes even further than Shannon or Kerckhoffs in tacitly assuming that *if there
is an algorithm that can break the system, then the attacker will surely find
that algorithm*. The attacker is not viewed as an omnipotent computer any more,
but he is still construed as an omnipotent programmer.
  So the Diffie-Hellman step from unlimited to limited computational powers has
not been extended into a step from unlimited to limited logical or programming
powers. Is the assumption that all feasible algorithms will eventually be
discovered and implemented really different from the assumption that everything
that is computable will eventually be computed? The present paper explores some
ways to refine the current models of the attacker, and of the defender, by
taking into account their limited logical and programming powers. If the
adaptive attacker actively queries the system to seek out its vulnerabilities,
can the system gain some security by actively learning attacker's methods, and
adapting to them?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5559</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5559</id><created>2011-09-26</created><authors><author><keyname>Groen</keyname><forenames>Derek</forenames><affiliation>UCL</affiliation></author><author><keyname>Rieder</keyname><forenames>Steven</forenames><affiliation>Leiden Observatory</affiliation></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames><affiliation>Leiden Observatory</affiliation></author></authors><title>High performance cosmological simulations on a grid of supercomputers</title><categories>cs.DC astro-ph.CO</categories><comments>Accepted for the INFOCOMP 2011 conference, 6 pages, 2 figures, 4
  tables</comments><msc-class>68M14 (primary), 68M20, 85A40 (secondary)</msc-class><acm-class>C.2.4; C.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present results from our cosmological N-body simulation which consisted of
2048x2048x2048 particles and ran distributed across three supercomputers
throughout Europe. The run, which was performed as the concluding phase of the
Gravitational Billion Body Problem DEISA project, integrated a 30 Mpc box of
dark matter using an optimized Tree/Particle Mesh N-body integrator. We ran the
simulation up to the present day (z=0), and obtained an efficiency of about
0.93 over 2048 cores compared to a single supercomputer run. In addition, we
share our experiences on using multiple supercomputers for high performance
computing and provide several recommendations for future projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5560</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5560</id><created>2011-09-26</created><authors><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Gualdi</keyname><forenames>Stanislao</forenames></author></authors><title>Temporal effects in the growth of networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.DL cs.SI</categories><comments>4 pages, 3 figures</comments><journal-ref>Physical Review Letters 107, 238701, 2011</journal-ref><doi>10.1103/PhysRevLett.107.238701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that to explain the growth of the citation network by preferential
attachment (PA), one has to accept that individual nodes exhibit heterogeneous
fitness values that decay with time. While previous PA-based models assumed
either heterogeneity or decay in isolation, we propose a simple analytically
treatable model that combines these two factors. Depending on the input
assumptions, the resulting degree distribution shows an exponential, log-normal
or power-law decay, which makes the model an apt candidate for modeling a wide
range of real systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5579</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5579</id><created>2011-09-26</created><authors><author><keyname>Curien</keyname><forenames>Nicolas</forenames></author></authors><title>Strong convergence of partial match queries in random quadtrees</title><categories>math.PR cs.DS</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the rescaled costs of partial match queries in a random
two-dimensional quadtree converge almost surely towards a random limit which is
identified as the terminal value of a martingale. Our approach shares many
similarities with the theory of self-similar fragmentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5589</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5589</id><created>2011-09-26</created><authors><author><keyname>Koca</keyname><forenames>Mutlu</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author></authors><title>A General Framework for Performance Analysis of Spatial Modulation over
  Correlated Fading Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures. Submitted to IEEE International Conference on
  Communications (ICC) 2012, Ottawa, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general method for the error analysis of spatial modulation (SM)
systems over correlated and uncorrelated Rayleigh and Rician fading channels.
The proposed method, making use of the properties of proper complex random
variables and vectors, provides an exact upper bound for the class of fading
channels considered for any number of transmit and receive antennas and for a
wide family of linear modulation alphabets. Theoretical derivations are
validated via simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5593</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5593</id><created>2011-09-26</created><updated>2012-01-17</updated><authors><author><keyname>Schaub</keyname><forenames>Michael T.</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Yaliraki</keyname><forenames>Sophia N.</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>Markov dynamics as a zooming lens for multiscale community detection:
  non clique-like communities and the field-of-view limit</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages, 6 figures</comments><journal-ref>PLoS ONE, 2012, 7(2), e32210</journal-ref><doi>10.1371/journal.pone.0032210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been a surge of interest in community detection
algorithms for complex networks. A variety of computational heuristics, some
with a long history, have been proposed for the identification of communities
or, alternatively, of good graph partitions. In most cases, the algorithms
maximize a particular objective function, thereby finding the `right' split
into communities. Although a thorough comparison of algorithms is still
lacking, there has been an effort to design benchmarks, i.e., random graph
models with known community structure against which algorithms can be
evaluated. However, popular community detection methods and benchmarks normally
assume an implicit notion of community based on clique-like subgraphs, a form
of community structure that is not always characteristic of real networks.
Specifically, networks that emerge from geometric constraints can have natural
non clique-like substructures with large effective diameters, which can be
interpreted as long-range communities. In this work, we show that long-range
communities escape detection by popular methods, which are blinded by a
restricted `field-of-view' limit, an intrinsic upper scale on the communities
they can detect. The field-of-view limit means that long-range communities tend
to be overpartitioned. We show how by adopting a dynamical perspective towards
community detection (Delvenne et al. (2010) PNAS:107: 12755-12760; Lambiotte et
al. (2008) arXiv:0812.1770), in which the evolution of a Markov process on the
graph is used as a zooming lens over the structure of the network at all
scales, one can detect both clique- or non clique-like communities without
imposing an upper scale to the detection. Consequently, the performance of
algorithms on inherently low-diameter, clique-like benchmarks may not always be
indicative of equally good results in real networks with local, sparser
connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5596</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5596</id><created>2011-09-26</created><authors><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Sheffer</keyname><forenames>Adam</forenames></author><author><keyname>Welzl</keyname><forenames>Emo</forenames></author></authors><title>Counting Plane Graphs: Perfect Matchings, Spanning Cycles, and
  Kasteleyn's Technique</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive improved upper bounds on the number of crossing-free straight-edge
spanning cycles (also known as Hamiltonian tours and simple polygonizations)
that can be embedded over any specific set of $N$ points in the plane. More
specifically, we bound the ratio between the number of spanning cycles (or
perfect matchings) that can be embedded over a point set and the number of
triangulations that can be embedded over it. The respective bounds are
$O(1.8181^N)$ for cycles and $O(1.1067^N)$ for matchings. These imply a new
upper bound of $O(54.543^N)$ on the number of crossing-free straight-edge
spanning cycles that can be embedded over any specific set of $N$ points in the
plane (improving upon the previous best upper bound $O(68.664^N)$). Our
analysis is based on Kasteleyn's linear algebra technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5615</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5615</id><created>2011-09-26</created><authors><author><keyname>Praveen</keyname><forenames>M.</forenames></author></authors><title>A Regularity Measure for Context Free Grammars</title><categories>cs.FL cs.CC cs.DS</categories><msc-class>68W01</msc-class><acm-class>F.2.2; F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parikh's theorem states that every Context Free Language (CFL) has the same
Parikh image as that of a regular language. A finite state automaton accepting
such a regular language is called a Parikh-equivalent automaton. In the worst
case, the number of states in any non-deterministic Parikh-equivalent automaton
is exponentially large in the size of the Context Free Grammar (CFG). We
associate a regularity width d with a CFG that measures the closeness of the
CFL with regular languages. The degree m of a CFG is one less than the maximum
number of variable occurrences in the right hand side of any production. Given
a CFG with n variables, we construct a Parikh-equivalent non-deterministic
automaton whose number of states is upper bounded by a polynomial in $n
(d^{2d(m+1)}), the degree of the polynomial being a small fixed constant. Our
procedure is constructive and runs in time polynomial in the size of the
automaton. In the terminology of parameterized complexity, we prove that
constructing a Parikh-equivalent automaton for a given CFG is Fixed Parameter
Tractable (FPT) when the degree m and regularity width d are parameters. We
also give an example from program verification domain where the degree and
regularity are small compared to the size of the grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5635</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5635</id><created>2011-09-26</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author></authors><title>Approximating Edit Distance in Near-Linear Time</title><categories>cs.DS</categories><comments>Preliminary version appeared in STOC 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to compute the edit distance between two strings of length n up
to a factor of 2^{\~O(sqrt(log n))} in n^(1+o(1)) time. This is the first
sub-polynomial approximation algorithm for this problem that runs in
near-linear time, improving on the state-of-the-art n^(1/3+o(1)) approximation.
Previously, approximation of 2^{\~O(sqrt(log n))} was known only for embedding
edit distance into l_1, and it is not known if that embedding can be computed
in less than quadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5636</identifier>
 <datestamp>2011-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5636</id><created>2011-09-26</created><updated>2011-12-19</updated><authors><author><keyname>Tosic</keyname><forenames>Tamara</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Distributed sensor failure detection in sensor networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of distributed sensors' failure detection in
networks with a small number of defective sensors, whose measurements differ
significantly from neighboring sensor measurements. Defective sensors are
represented by non-zero values in binary sparse signals. We build on the sparse
nature of the binary sensor failure signals and propose a new distributed
detection algorithm based on Group Testing (GT). The distributed GT algorithm
estimates the set of defective sensors from a small number of linearly
independent binary messages exchanged by the sensors. The distributed GT
algorithm uses a low complexity distance decoder that is robust to noisy
messages. We first consider networks with only one defective sensor and
determine the minimal number of linearly independent messages needed for
detection of the defective sensor with high probability. We then extend our
study to the detection of multiple defective sensors by modifying appropriately
the message exchange protocol and the decoding procedure. We show through
experimentation that, for small and medium sized networks, the number of
messages required for successful detection is actually smaller than the minimal
number computed in the analysis. Simulations demonstrate that the proposed
method outperforms methods based on random walk measurements collection in
terms of detection performance and convergence rate. Finally, the proposed
method is resilient to network dynamics due to the effective gossip-based
message dissemination protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5641</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5641</id><created>2011-09-26</created><authors><author><keyname>Mathew</keyname><forenames>Vimal</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh K.</forenames></author><author><keyname>Shenoy</keyname><forenames>Prashant</forenames></author></authors><title>Energy-Aware Load Balancing in Content Delivery Networks</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet-scale distributed systems such as content delivery networks (CDNs)
operate hundreds of thousands of servers deployed in thousands of data center
locations around the globe. Since the energy costs of operating such a large IT
infrastructure are a significant fraction of the total operating costs, we
argue for redesigning CDNs to incorporate energy optimizations as a first-order
principle. We propose techniques to turn off CDN servers during periods of low
load while seeking to balance three key design goals: maximize energy
reduction, minimize the impact on client-perceived service availability (SLAs),
and limit the frequency of on-off server transitions to reduce wear-and-tear
and its impact on hardware reliability. We propose an optimal offline algorithm
and an online algorithm to extract energy savings both at the level of local
load balancing within a data center and global load balancing across data
centers. We evaluate our algorithms using real production workload traces from
a large commercial CDN. Our results show that it is possible to reduce the
energy consumption of a CDN by more than 55% while ensuring a high level of
availability that meets customer SLA requirements and incurring an average of
one on-off transition per server per day. Further, we show that keeping even
10% of the servers as hot spares helps absorb load spikes due to global flash
crowds with little impact on availability SLAs. Finally, we show that
redistributing load across proximal data centers can enhance service
availability significantly, but has only a modest impact on energy savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5647</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5647</id><created>2011-09-26</created><updated>2012-12-09</updated><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Making Gradient Descent Optimal for Strongly Convex Stochastic
  Optimization</title><categories>cs.LG math.OC</categories><comments>Updated version which fixes a bug in the proof of lemma 1 and
  modifies the step size choice. As a result, constants are changed throughout
  the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent (SGD) is a simple and popular method to solve
stochastic optimization problems which arise in machine learning. For strongly
convex problems, its convergence rate was known to be O(\log(T)/T), by running
SGD for T iterations and returning the average point. However, recent results
showed that using a different algorithm, one can get an optimal O(1/T) rate.
This might lead one to believe that standard SGD is suboptimal, and maybe
should even be replaced as a method of choice. In this paper, we investigate
the optimality of SGD in a stochastic setting. We show that for smooth
problems, the algorithm attains the optimal O(1/T) rate. However, for
non-smooth problems, the convergence rate with averaging might really be
\Omega(\log(T)/T), and this is not just an artifact of the analysis. On the
flip side, we show that a simple modification of the averaging step suffices to
recover the O(1/T) rate, and no other change of the algorithm is necessary. We
also present experimental results which support our findings, and point out
open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5663</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5663</id><created>2011-09-26</created><authors><author><keyname>Edelkamp</keyname><forenames>S.</forenames></author><author><keyname>Hoffmann</keyname><forenames>J.</forenames></author></authors><title>The Deterministic Part of IPC-4: An Overview</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  519-579, 2005</journal-ref><doi>10.1613/jair.1677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an overview of the organization and results of the deterministic
part of the 4th International Planning Competition, i.e., of the part concerned
with evaluating systems doing deterministic planning. IPC-4 attracted even more
competing systems than its already large predecessors, and the competition
event was revised in several important respects. After giving an introduction
to the IPC, we briefly explain the main differences between the deterministic
part of IPC-4 and its predecessors. We then introduce formally the language
used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed
initial literals. We list the competing systems and overview the results of the
competition. The entire set of data is far too large to be presented in full.
We provide a detailed summary; the complete data is available in an online
appendix. We explain how we awarded the competition prizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5664</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5664</id><created>2011-09-26</created><updated>2013-06-21</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Deterministic Feature Selection for $k$-means Clustering</title><categories>cs.LG cs.DS</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study feature selection for $k$-means clustering. Although the literature
contains many methods with good empirical performance, algorithms with provable
theoretical behavior have only recently been developed. Unfortunately, these
algorithms are randomized and fail with, say, a constant probability. We
address this issue by presenting a deterministic feature selection algorithm
for k-means with theoretical guarantees. At the heart of our algorithm lies a
deterministic method for decompositions of the identity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5665</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5665</id><created>2011-09-26</created><authors><author><keyname>McDermott</keyname><forenames>D.</forenames></author></authors><title>PDDL2.1 - The Art of the Possible? Commentary on Fox and Long</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  145-148, 2003</journal-ref><doi>10.1613/jair.1996</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PDDL2.1 was designed to push the envelope of what planning algorithms can do,
and it has succeeded. It adds two important features: durative actions,which
take time (and may have continuous effects); and objective functions for
measuring the quality of plans. The concept of durative actions is flawed; and
the treatment of their semantics reveals too strong an attachment to the way
many contemporary planners work. Future PDDL innovators should focus on
producing a clean semantics for additions to the language, and let planner
implementers worry about coupling their algorithms to problems expressed in the
latest version of the language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5666</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5666</id><created>2011-09-26</created><authors><author><keyname>Smith</keyname><forenames>D. E.</forenames></author></authors><title>The Case for Durative Actions: A Commentary on PDDL2.1</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  149-154, 2003</journal-ref><doi>10.1613/jair.1997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The addition of durative actions to PDDL2.1 sparked some controversy. Fox and
Long argued that actions should be considered as instantaneous, but can start
and stop processes. Ultimately, a limited notion of durative actions was
incorporated into the language. I argue that this notion is still impoverished,
and that the underlying philosophical position of regarding durative actions as
being a shorthand for a start action, process, and stop action ignores the
realities of modelling and execution for complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5679</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5679</id><created>2011-09-26</created><authors><author><keyname>Ismail</keyname><forenames>Anis</forenames></author><author><keyname>Quafafou</keyname><forenames>Mohamed</forenames></author><author><keyname>Durand</keyname><forenames>Nicolas</forenames></author><author><keyname>Nachouki</keyname><forenames>Gilles</forenames></author><author><keyname>Hajjar</keyname><forenames>Mohammad</forenames></author></authors><title>Queries mining for efficient routing in P2P communities</title><categories>cs.PF cs.NI</categories><comments>20 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1108.1378</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer (P2P) computing is currently attracting enormous attention. In
P2P systems a very large number of autonomous computing nodes (the peers) pool
together their resources and rely on each other for data and services.
Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of
Internet traffic. Examples include P2P systems for network storage, web
caching, searching and indexing of relevant documents and distributed
network-threat analysis. Requirements for widely distributed information
systems supporting virtual organizations have given rise to a new category of
P2P systems called schema-based. In such systems each peer exposes its own
schema and the main objective is the efficient search across the P2P network by
processing each incoming query without overly consuming bandwidth. The
usability of these systems depends on effective techniques to find and retrieve
data; however, efficient and effective routing of content-based queries is a
challenging problem in P2P networks. This work was attended as an attempt to
motivate the use of mining algorithms and hypergraphs context to develop two
different methods that improve significantly the efficiency of P2P
communications. The proposed query routing methods direct the query to a set of
relevant peers in such way as to avoid network traffic and bandwidth
consumption. We compare the performance of the two proposed methods with the
baseline one and our experimental results prove that our proposed methods
generate impressive levels of performance and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5683</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5683</id><created>2011-09-26</created><authors><author><keyname>Bassino</keyname><forenames>Frederique</forenames></author><author><keyname>David</keyname><forenames>Julien</forenames></author><author><keyname>Sportiello</keyname><forenames>Andrea</forenames></author></authors><title>Asymptotic enumeration of Minimal Automata</title><categories>cs.FL math.CO</categories><comments>12+5 pages, 2 figures, submitted to STACS 2012</comments><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the asymptotic proportion of minimal automata, within n-state
accessible deterministic complete automata over a k-letter alphabet, with the
uniform distribution over the possible transition structures, and a binomial
distribution over terminal states, with arbitrary parameter b. It turns out
that a fraction ~ 1-C(k,b) n^{-k+2} of automata is minimal, with C(k,b) a
function, explicitly determined, involving the solution of a transcendental
equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5711</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5711</id><created>2011-09-26</created><authors><author><keyname>Li</keyname><forenames>L.</forenames></author><author><keyname>Onder</keyname><forenames>N.</forenames></author><author><keyname>Whelan</keyname><forenames>G. C.</forenames></author></authors><title>Engineering a Conformant Probabilistic Planner</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  1-15, 2006</journal-ref><doi>10.1613/jair.1701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a partial-order, conformant, probabilistic planner, Probapop which
competed in the blind track of the Probabilistic Planning Competition in IPC-4.
We explain how we adapt distance based heuristics for use with probabilistic
domains. Probapop also incorporates heuristics based on probability of success.
We explain the successes and difficulties encountered during the design and
implementation of Probapop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5712</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5712</id><created>2011-09-26</created><authors><author><keyname>Dutta</keyname><forenames>P. S.</forenames></author><author><keyname>Jennings</keyname><forenames>N. R.</forenames></author><author><keyname>Moreau</keyname><forenames>L.</forenames></author></authors><title>Cooperative Information Sharing to Improve Distributed Learning in
  Multi-Agent Systems</title><categories>cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  407-463, 2005</journal-ref><doi>10.1613/jair.1735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective coordination of agents actions in partially-observable domains is a
major challenge of multi-agent systems research. To address this, many
researchers have developed techniques that allow the agents to make decisions
based on estimates of the states and actions of other agents that are typically
learnt using some form of machine learning algorithm. Nevertheless, many of
these approaches fail to provide an actual means by which the necessary
information is made available so that the estimates can be learnt. To this end,
we argue that cooperative communication of state information between agents is
one such mechanism. However, in a dynamically changing environment, the
accuracy and timeliness of this communicated information determine the fidelity
of the learned estimates and the usefulness of the actions taken based on
these. Given this, we propose a novel information-sharing protocol,
post-task-completion sharing, for the distribution of state information. We
then show, through a formal analysis, the improvement in the quality of
estimates produced using our strategy over the widely used protocol of sharing
information between nearest neighbours. Moreover, communication heuristics
designed around our information-sharing principle are subjected to empirical
evaluation along with other benchmark strategies (including Littmans Q-routing
and Stones TPOT-RL) in a simulated call-routing application. These studies,
conducted across a range of environmental settings, show that, compared to the
different benchmarks used, our strategy generates an improvement of up to 60%
in the call connection rate; of more than 1000% in the ability to connect
long-distance calls; and incurs as low as 0.25 of the message overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5713</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5713</id><created>2011-09-26</created><authors><author><keyname>Hoffmann</keyname><forenames>J.</forenames></author></authors><title>Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning
  Benchmarks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  685-758, 2005</journal-ref><doi>10.1613/jair.1747</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Between 1998 and 2004, the planning community has seen vast progress in terms
of the sizes of benchmark examples that domain-independent planners can tackle
successfully. The key technique behind this progress is the use of heuristic
functions based on relaxing the planning task at hand, where the relaxation is
to assume that all delete lists are empty. The unprecedented success of such
methods, in many commonly used benchmark examples, calls for an understanding
of what classes of domains these methods are well suited for. In the
investigation at hand, we derive a formal background to such an understanding.
We perform a case study covering a range of 30 commonly used STRIPS and ADL
benchmark domains, including all examples used in the first four international
planning competitions. We *prove* connections between domain structure and
local search topology -- heuristic cost surface properties -- under an
idealized version of the heuristic functions used in modern planners. The
idealized heuristic function is called h^+, and differs from the practically
used functions in that it returns the length of an *optimal* relaxed plan,
which is NP-hard to compute. We identify several key characteristics of the
topology under h^+, concerning the existence/non-existence of unrecognized dead
ends, as well as the existence/non-existence of constant upper bounds on the
difficulty of escaping local minima and benches. These distinctions divide the
(set of all) planning domains into a taxonomy of classes of varying h^+
topology. As it turns out, many of the 30 investigated domains lie in classes
with a relatively easy topology. Most particularly, 12 of the domains lie in
classes where FFs search algorithm, provided with h^+, is a polynomial solving
mechanism. We also present results relating h^+ to its approximation as
implemented in FF. The behavior regarding dead ends is provably the same. We
summarize the results of an empirical investigation showing that, in many
domains, the topological qualities of h^+ are largely inherited by the
approximation. The overall investigation gives a rare example of a successful
analysis of the connections between typical-case problem structure, and search
performance. The theoretical investigation also gives hints on how the
topological phenomena might be automatically recognizable by domain analysis
techniques. We outline some preliminary steps we made into that direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5714</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5714</id><created>2011-09-26</created><authors><author><keyname>Samaras</keyname><forenames>N.</forenames></author><author><keyname>Stergiou</keyname><forenames>K.</forenames></author></authors><title>Binary Encodings of Non-binary Constraint Satisfaction Problems:
  Algorithms and Experimental Results</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  641-684, 2005</journal-ref><doi>10.1613/jair.1776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-binary Constraint Satisfaction Problem (CSP) can be solved directly
using extended versions of binary techniques. Alternatively, the non-binary
problem can be translated into an equivalent binary one. In this case, it is
generally accepted that the translated problem can be solved by applying
well-established techniques for binary CSPs. In this paper we evaluate the
applicability of the latter approach. We demonstrate that the use of standard
techniques for binary CSPs in the encodings of non-binary problems is
problematic and results in models that are very rarely competitive with the
non-binary representation. To overcome this, we propose specialized arc
consistency and search algorithms for binary encodings, and we evaluate them
theoretically and empirically. We consider three binary representations; the
hidden variable encoding, the dual encoding, and the double encoding.
Theoretical and empirical results show that, for certain classes of non-binary
constraints, binary encodings are a competitive option, and in many cases, a
better one than the non-binary representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5716</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5716</id><created>2011-09-26</created><authors><author><keyname>Adjiman</keyname><forenames>P.</forenames></author><author><keyname>Chatalic</keyname><forenames>P.</forenames></author><author><keyname>Goasdoue</keyname><forenames>F.</forenames></author><author><keyname>Rousset</keyname><forenames>M. C.</forenames></author><author><keyname>Simon</keyname><forenames>L.</forenames></author></authors><title>Distributed Reasoning in a Peer-to-Peer Setting: Application to the
  Semantic Web</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  269-314, 2006</journal-ref><doi>10.1613/jair.1785</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a peer-to-peer inference system, each peer can reason locally but can also
solicit some of its acquaintances, which are peers sharing part of its
vocabulary. In this paper, we consider peer-to-peer inference systems in which
the local theory of each peer is a set of propositional clauses defined upon a
local vocabulary. An important characteristic of peer-to-peer inference systems
is that the global theory (the union of all peer theories) is not known (as
opposed to partition-based reasoning systems). The main contribution of this
paper is to provide the first consequence finding algorithm in a peer-to-peer
setting: DeCA. It is anytime and computes consequences gradually from the
solicited peer to peers that are more and more distant. We exhibit a sufficient
condition on the acquaintance graph of the peer-to-peer inference system for
guaranteeing the completeness of this algorithm. Another important contribution
is to apply this general distributed reasoning setting to the setting of the
Semantic Web through the Somewhere semantic peer-to-peer data management
system. The last contribution of this paper is to provide an experimental
analysis of the scalability of the peer-to-peer infrastructure that we propose,
on large networks of 1000 peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5717</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5717</id><created>2011-09-26</created><authors><author><keyname>Hoos</keyname><forenames>H. H.</forenames></author><author><keyname>Pullan</keyname><forenames>W.</forenames></author></authors><title>Dynamic Local Search for the Maximum Clique Problem</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  159-185, 2006</journal-ref><doi>10.1613/jair.1815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce DLS-MC, a new stochastic local search algorithm
for the maximum clique problem. DLS-MC alternates between phases of iterative
improvement, during which suitable vertices are added to the current clique,
and plateau search, during which vertices of the current clique are swapped
with vertices not contained in the current clique. The selection of vertices is
solely based on vertex penalties that are dynamically adjusted during the
search, and a perturbation mechanism is used to overcome search stagnation. The
behaviour of DLS-MC is controlled by a single parameter, penalty delay, which
controls the frequency at which vertex penalties are reduced. We show
empirically that DLS-MC achieves substantial performance improvements over
state-of-the-art algorithms for the maximum clique problem over a large range
of the commonly used DIMACS benchmark instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5720</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5720</id><created>2011-09-26</created><updated>2011-11-10</updated><authors><author><keyname>Xie</keyname><forenames>Jierui</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author></authors><title>SLPA: Uncovering Overlapping Communities in Social Networks via A
  Speaker-listener Interaction Dynamic Process</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>IEEE ICDM 2011 Workshop on DMCCI xiej2@rpi.edu</comments><journal-ref>Proc. Data Mining Technologies for Computational Collective
  Intelligence Workshop at ICDM, Vancouver, CA, 2011, pp. 344-349</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overlap is one of the characteristics of social networks, in which a person
may belong to more than one social group. For this reason, discovering
overlapping structures is necessary for realistic social analysis. In this
paper, we present a novel, general framework to detect and analyze both
individual overlapping nodes and entire communities. In this framework, nodes
exchange labels according to dynamic interaction rules. A specific
implementation called Speaker-listener Label Propagation Algorithm (SLPA1)
demonstrates an excellent performance in identifying both overlapping nodes and
overlapping communities with different degrees of diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5730</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5730</id><created>2011-09-26</created><authors><author><keyname>Rother</keyname><forenames>Diego</forenames></author><author><keyname>Mahendran</keyname><forenames>Siddharth</forenames></author><author><keyname>Vidal</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Hypothesize and Bound: A Computational Focus of Attention Mechanism for
  Simultaneous 3D Shape Reconstruction, Pose Estimation and Classification from
  a Single 2D Image</title><categories>cs.CV cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a mathematical framework to simultaneously tackle the
problems of 3D reconstruction, pose estimation and object classification, from
a single 2D image. In sharp contrast with state of the art methods that rely
primarily on 2D information and solve each of these three problems separately
or iteratively, we propose a mathematical framework that incorporates prior
&quot;knowledge&quot; about the 3D shapes of different object classes and solves these
problems jointly and simultaneously, using a hypothesize-and-bound (H&amp;B)
algorithm. In the proposed H&amp;B algorithm one hypothesis is defined for each
possible pair [object class, object pose], and the algorithm selects the
hypothesis H that maximizes a function L(H) encoding how well each hypothesis
&quot;explains&quot; the input image. To find this maximum efficiently, the function L(H)
is not evaluated exactly for each hypothesis H, but rather upper and lower
bounds for it are computed at a much lower cost. In order to obtain bounds for
L(H) that are tight yet inexpensive to compute, we extend the theory of shapes
described in [14] to handle projections of shapes. This extension allows us to
define a probabilistic relationship between the prior knowledge given in 3D and
the 2D input image. This relationship is derived from first principles and is
proven to be the only relationship having the properties that we intuitively
expect from a &quot;projection.&quot; In addition to the efficiency and optimality
characteristics of H&amp;B algorithms, the proposed framework has the desirable
property of integrating information in the 2D image with information in the 3D
prior to estimate the optimal reconstruction. While this article focuses
primarily on the problem mentioned above, we believe that the theory presented
herein has multiple other potential applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5732</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5732</id><created>2011-09-26</created><authors><author><keyname>Gutnik</keyname><forenames>G.</forenames></author><author><keyname>Kaminka</keyname><forenames>G. A.</forenames></author></authors><title>Representing Conversations for Scalable Overhearing</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  349-387, 2006</journal-ref><doi>10.1613/jair.1829</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open distributed multi-agent systems are gaining interest in the academic
community and in industry. In such open settings, agents are often coordinated
using standardized agent conversation protocols. The representation of such
protocols (for analysis, validation, monitoring, etc) is an important aspect of
multi-agent applications. Recently, Petri nets have been shown to be an
interesting approach to such representation, and radically different approaches
using Petri nets have been proposed. However, their relative strengths and
weaknesses have not been examined. Moreover, their scalability and suitability
for different tasks have not been addressed. This paper addresses both these
challenges. First, we analyze existing Petri net representations in terms of
their scalability and appropriateness for overhearing, an important task in
monitoring open multi-agent systems. Then, building on the insights gained, we
introduce a novel representation using Colored Petri nets that explicitly
represent legal joint conversation states and messages. This representation
approach offers significant improvements in scalability and is particularly
suitable for overhearing. Furthermore, we show that this new representation
offers a comprehensive coverage of all conversation features of FIPA
conversation standards. We also present a procedure for transforming AUML
conversation protocol diagrams (a standard human-readable representation), to
our Colored Petri net representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5750</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5750</id><created>2011-09-26</created><authors><author><keyname>Haslum</keyname><forenames>P.</forenames></author></authors><title>Improving Heuristics Through Relaxed Search - An Analysis of TP4 and
  HSP*a in the 2004 Planning Competition</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  233-267, 2006</journal-ref><doi>10.1613/jair.1885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hm admissible heuristics for (sequential and temporal) regression
planning are defined by a parameterized relaxation of the optimal cost function
in the regression search space, where the parameter m offers a trade-off
between the accuracy and computational cost of theheuristic. Existing methods
for computing the hm heuristic require time exponential in m, limiting them to
small values (m andlt= 2). The hm heuristic can also be viewed as the optimal
cost function in a relaxation of the search space: this paper presents relaxed
search, a method for computing this function partially by searching in the
relaxed space. The relaxed search method, because it computes hm only
partially, is computationally cheaper and therefore usable for higher values of
m. The (complete) hm heuristic is combined with partial hm heuristics, for m =
3,..., computed by relaxed search, resulting in a more accurate heuristic.
  This use of the relaxed search method to improve on the hm heuristic is
evaluated by comparing two optimal temporal planners: TP4, which does not use
it, and HSP*a, which uses it but is otherwise identical to TP4. The comparison
is made on the domains used in the 2004 International Planning Competition, in
which both planners participated. Relaxed search is found to be cost effective
in some of these domains, but not all. Analysis reveals a characterization of
the domains in which relaxed search can be expected to be cost effective, in
terms of two measures on the original and relaxed search spaces. In the domains
where relaxed search is cost effective, expanding small states is
computationally cheaper than expanding large states and small states tend to
have small successor states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5752</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5752</id><created>2011-09-26</created><updated>2013-11-07</updated><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Fahim</keyname><forenames>Arash</forenames></author></authors><title>A Stochastic Approximation for Fully Nonlinear Free Boundary Parabolic
  Problems</title><categories>math.NA cs.NA math.PR q-fin.CP</categories><comments>To appear in the journal &quot;Numerical Methods for Partial Differential
  Equations&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a stochastic numerical method for solving fully non-linear free
boundary problems of parabolic type and provide a rate of convergence under
reasonable conditions on the non-linearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5770</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5770</id><created>2011-09-27</created><authors><author><keyname>Leng</keyname><forenames>Mei</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Cooperative and Distributed Localization for Wireless Sensor Networks in
  Multipath Environments</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sensor localization in a wireless network in a
multipath environment, where time and angle of arrival information are
available at each sensor. We propose a distributed algorithm based on belief
propagation, which allows sensors to cooperatively self-localize with respect
to one single anchor in a multihop network. The algorithm has low overhead and
is scalable. Simulations show that although the network is loopy, the proposed
algorithm converges, and achieves good localization accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5779</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5779</id><created>2011-09-27</created><updated>2011-10-31</updated><authors><author><keyname>Vaze</keyname><forenames>Chinmay S.</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The Degrees of Freedom Region of the MIMO Interference Channel with
  Shannon Feedback</title><categories>cs.IT math.IT</categories><comments>30 pages, 3 tables, 9 figures. This paper was submitted to the IEEE
  Trans. Inform. Th. Oct. 2011. It was presented in part at the 49th Annual
  Allerton Conference on Communications, Control and Computing in Sept. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-user multiple-input multiple-output (MIMO) fast-fading interference
channel (IC) with an arbitrary number of antennas at each of the four terminals
is studied under the settings of Shannon feedback, limited Shannon feedback,
and output feedback, wherein all or certain channel matrices and outputs, or
just the channel outputs, respectively, are available to the transmitters with
a finite delay. While for most numbers of antennas at the four terminals, it is
shown that the DoF regions with Shannon feedback and for the limited Shannon
feedback settings considered here are identical, and equal to the DoF region
with just delayed channel state information (CSIT), it is shown that this is
not always the case. For a specific class of MIMO ICs characterized by a
certain relationship between the numbers of antennas at the four nodes, the DoF
regions with Shannon and the limited Shannon feedback settings, while again
being identical, are strictly bigger than the DoF region with just delayed
CSIT. To realize these DoF gains with Shannon or limited Shannon feedback, a
new retrospective interference alignment scheme is developed wherein
transmitter cooperation made possible by output feedback in addition to delayed
CSIT is employed to effect a more efficient form of interference alignment than
is feasible with previously known schemes that use just delayed CSIT. The DoF
region for just output feedback, in which each transmitter has delayed
knowledge of only the receivers' outputs, is also obtained for all but a class
of MIMO ICs that satisfy one of two inequalities involving the numbers of
antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5789</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5789</id><created>2011-09-27</created><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Approximation Complexity of Complex-Weighted Degree-Two Counting
  Constraint Satisfaction Problems</title><categories>cs.CC</categories><comments>A4, 10pt, 23 pages. This is a complete version of the paper that
  appeared in the Proceedings of the 17th Annual International Computing and
  Combinatorics Conference (COCOON 2011), Lecture Notes in Computer Science,
  vol.6842, pp.122-133, Dallas, Texas, USA, August 14-16, 2011</comments><journal-ref>(journal version) Theoretical Computer Science, Vol.461,
  pp.86-105, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint satisfaction problems have been studied in numerous fields with
practical and theoretical interests. In recent years, major breakthroughs have
been made in a study of counting constraint satisfaction problems (or #CSPs).
In particular, a computational complexity classification of bounded-degree
#CSPs has been discovered for all degrees except for two, where the &quot;degree&quot; of
an input instance is the maximal number of times that each input variable
appears in a given set of constraints. Despite the efforts of recent studies,
however, a complexity classification of degree-2 #CSPs has eluded from our
understandings. This paper challenges this open problem and gives its partial
solution by applying two novel proof techniques--T_{2}-constructibility and
parametrized symmetrization--which are specifically designed to handle
&quot;arbitrary&quot; constraints under randomized approximation-preserving reductions.
We partition entire constraints into four sets and we classify the
approximation complexity of all degree-2 #CSPs whose constraints are drawn from
two of the four sets into two categories: problems computable in
polynomial-time or problems that are at least as hard as #SAT. Our proof
exploits a close relationship between complex-weighted degree-2 #CSPs and
Holant problems, which are a natural generalization of complex-weighted #CSPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5790</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5790</id><created>2011-09-27</created><authors><author><keyname>Vaze</keyname><forenames>Chinmay S.</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The Degrees of Freedom of the 2-Hop, 2-User Interference Channel with
  Feedback</title><categories>cs.IT math.IT</categories><comments>Submitted, July 10, 2011 to the 2011 Allerton Conf. Commun., Control,
  Comput., Monticello, IL; accepted August 04, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The layered two-hop, two-flow interference network is considered that
consists of two sources, two relays and two destinations with the first hop
network between he sources and the relays and the second hop network between
relays and destinations both being i.i.d. Rayleigh fading Gaussian interference
channels. Two feedback models are studied. In the first one, called the delayed
channel state information at the sources (delayed CSI-S) model, the sources
know all channel coefficients with a finite delay but the relays have no side
information whatsoever. In the second feedback model, referred to as the
limited Shannon feedback model, the relays know first hop channel coefficients
instantaneously and the second hop channel with a finite delay and one relay
knows the received signal of one of the destinations with a finite delay and
the other relay knows the received signal of the other destination with a
finite delay but there is no side information at the sources whatsoever. It is
shown in this paper that under both these settings, the layered two-hop,
two-flow interference channel has 4/3 degrees of freedom. The result is
obtained by developing a broadcast-channel-type upper-bound and new
achievability schemes based on the ideas of retrospective interference
alignment and retro-cooperative interference alignment, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5796</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5796</id><created>2011-09-27</created><updated>2011-09-29</updated><authors><author><keyname>Vinh</keyname><forenames>Nguyen Xuan</forenames></author></authors><title>Genetic Testing for Complex Diseases: a Simulation Study Perspective</title><categories>stat.AP cs.CE q-bio.GN q-bio.QM</categories><comments>5 pages technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely recognized nowadays that complex diseases are caused by, amongst
the others, multiple genetic factors. The recent advent of genome-wide
association study (GWA) has triggered a wave of research aimed at discovering
genetic factors underlying common complex diseases. While the number of
reported susceptible genetic variants is increasing steadily, the application
of such findings into diseases prognosis for the general population is still
unclear, and there are doubts about whether the size of the contribution by
such factors is significant. In this respect, some recent simulation-based
studies have shed more light to the prospect of genetic tests. In this report,
we discuss several aspects of simulation-based studies: their parameters, their
assumptions, and the information they provide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5798</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5798</id><created>2011-09-27</created><authors><author><keyname>Ostapov</keyname><forenames>Yuriy</forenames></author></authors><title>Object-oriented semantics of English in natural language understanding
  system</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach to the problem of natural language understanding is proposed.
The knowledge domain under consideration is the social behavior of people.
English sentences are translated into set of predicates of a semantic database,
which describe persons, occupations, organizations, projects, actions, events,
messages, machines, things, animals, location and time of actions, relations
between objects, thoughts, cause-and-effect relations, abstract objects. There
is a knowledge base containing the description of semantics of objects
(functions and structure), actions (motives and causes), and operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5801</identifier>
 <datestamp>2012-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5801</id><created>2011-09-27</created><updated>2012-08-03</updated><authors><author><keyname>Durand</keyname><forenames>Fabien</forenames><affiliation>LAMFA</affiliation></author><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Multidimensional extension of the Morse--Hedlund theorem</title><categories>math.CO cs.DM math.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A celebrated result of Morse and Hedlund, stated in 1938, asserts that a
sequence $x$ over a finite alphabet is ultimately periodic if and only if, for
some $n$, the number of different factors of length $n$ appearing in $x$ is
less than $n+1$. Attempts to extend this fundamental result, for example, to
higher dimensions, have been considered during the last fifteen years. Let
$d\ge 2$. A legitimate extension to a multidimensional setting of the notion of
periodicity is to consider sets of $\ZZ^d$ definable by a first order formula
in the Presburger arithmetic $&lt;\ZZ;&lt;,+&gt;$. With this latter notion and using a
powerful criterion due to Muchnik, we exhibit a complete extension of the
Morse--Hedlund theorem to an arbitrary dimension $d$ and characterize sets of
$\ZZ^d$ definable in $&lt;\ZZ;&lt;,+&gt;$ in terms of some functions counting recurrent
blocks, that is, blocks occurring infinitely often.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5804</identifier>
 <datestamp>2012-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5804</id><created>2011-09-27</created><updated>2012-06-21</updated><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Langer</keyname><forenames>Alexander</forenames></author><author><keyname>Obdr&#x17e;&#xe1;lek</keyname><forenames>Jan</forenames></author><author><keyname>Rossmanith</keyname><forenames>Peter</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author></authors><title>Lower Bounds on the Complexity of MSO1 Model-Checking</title><categories>cs.LO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important algorithmic meta-theorems is a famous result by
Courcelle, which states that any graph problem definable in monadic
second-order logic with edge-set quantifications (i.e., MSO2 model-checking) is
decidable in linear time on any class of graphs of bounded tree-width.
Recently, Kreutzer and Tazari proved a corresponding complexity lower-bound -
that MSO2 model-checking is not even in XP wrt. the formula size as parameter
for graph classes that are subgraph-closed and whose tree-width is
poly-logarithmically unbounded. Of course, this is not an unconditional result
but holds modulo a certain complexity-theoretic assumption, namely, the
Exponential Time Hypothesis (ETH).
  In this paper we present a closely related result. We show that even MSO1
model-checking with a fixed set of vertex labels, but without edge-set
quantifications, is not in XP wrt. the formula size as parameter for graph
classes which are subgraph-closed and whose tree-width is poly-logarithmically
unbounded unless the non-uniform ETH fails. In comparison to Kreutzer and
Tazari; $(1)$ we use a stronger prerequisite, namely non-uniform instead of
uniform ETH, to avoid the effectiveness assumption and the construction of
certain obstructions used in their proofs; and $(2)$ we assume a different set
of problems to be efficiently decidable, namely MSO1-definable properties on
vertex labeled graphs instead of MSO2-definable properties on unlabeled graphs.
  Our result has an interesting consequence in the realm of digraph width
measures: Strengthening the recent result, we show that no subdigraph-monotone
measure can be &quot;algorithmically useful&quot;, unless it is within a poly-logarithmic
factor of undirected tree-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5827</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5827</id><created>2011-09-27</created><updated>2013-01-28</updated><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Security and complexity of the McEliece cryptosystem based on QC-LDPC
  codes</title><categories>cs.CR cs.IT math.IT</categories><comments>22 pages, 1 figure. This paper is a preprint of a paper accepted by
  IET Information Security and is subject to Institution of Engineering and
  Technology Copyright. When the final version is published, the copy of record
  will be available at IET Digital Library</comments><journal-ref>IET Information Security, ISSN 1751-8709, Vol. 7, No. 3, p.
  212-220, Sep. 2013</journal-ref><doi>10.1049/iet-ifs.2012.0127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of public key cryptography, the McEliece cryptosystem
represents a very smart solution based on the hardness of the decoding problem,
which is believed to be able to resist the advent of quantum computers. Despite
this, the original McEliece cryptosystem, based on Goppa codes, has encountered
limited interest in practical applications, partly because of some constraints
imposed by this very special class of codes. We have recently introduced a
variant of the McEliece cryptosystem including low-density parity-check codes,
that are state-of-the-art codes, now used in many telecommunication standards
and applications. In this paper, we discuss the possible use of a bit-flipping
decoder in this context, which gives a significant advantage in terms of
complexity. We also provide theoretical arguments and practical tools for
estimating the trade-off between security and complexity, in such a way to give
a simple procedure for the system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5851</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5851</id><created>2011-09-27</created><updated>2011-11-19</updated><authors><author><keyname>Place</keyname><forenames>Thomas</forenames><affiliation>ENS Cachan &amp; INRIA</affiliation></author><author><keyname>Segoufin</keyname><forenames>Luc</forenames><affiliation>INRIA &amp; ENS Cachan</affiliation></author></authors><title>A decidable characterization of locally testable tree languages</title><categories>cs.FL</categories><proxy>LMCS</proxy><acm-class>F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 4 (November
  22, 2011) lmcs:1210</journal-ref><doi>10.2168/LMCS-7(4:3)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A regular tree language L is locally testable if membership of a tree in L
depends only on the presence or absence of some fix set of neighborhoods in the
tree. In this paper we show that it is decidable whether a regular tree
language is locally testable. The decidability is shown for ranked trees and
for unranked unordered trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5876</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5876</id><created>2011-09-27</created><authors><author><keyname>Liuni</keyname><forenames>Marco</forenames></author><author><keyname>R&#xf6;bel</keyname><forenames>Axel</forenames></author><author><keyname>Romito</keyname><forenames>Marco</forenames></author><author><keyname>Rodet</keyname><forenames>Xavier</forenames></author></authors><title>R\'enyi Information Measures for Spectral Change Detection</title><categories>cs.SD</categories><comments>2011 IEEE Conference on Acoustics, Speech and Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change detection within an audio stream is an important task in several
domains, such as classification and segmentation of a sound or of a music
piece, as well as indexing of broadcast news or surveillance applications. In
this paper we propose two novel methods for spectral change detection without
any assumption about the input sound: they are both based on the evaluation of
information measures applied to a time- frequency representation of the signal,
and in particular to the spectrogram. The class of measures we consider, the
R\'enyi entropies, are obtained by extending the Shannon entropy definition: a
biasing of the spectrogram coefficients is realized through the dependence of
such measures on a parameter, which allows refined results compared to those
obtained with standard divergences. These methods provide a low computational
cost and are well-suited as a support for higher level analysis, segmentation
and classification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5893</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5893</id><created>2011-09-27</created><authors><author><keyname>Blasiak</keyname><forenames>Janusz</forenames></author><author><keyname>Krasinski</keyname><forenames>Tadeusz</forenames></author><author><keyname>Poplawski</keyname><forenames>Tomasz</forenames></author><author><keyname>Sakowski</keyname><forenames>Sebastian</forenames></author></authors><title>More powerful biomolecular computers</title><categories>cs.ET cs.FL physics.bio-ph</categories><comments>11.pages, submitted to Nature Nanotechnology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biomolecular computers, along with quantum computers, may be a future
alternative for traditional, silicon-based computers. Main advantages of
biomolecular computers are massive parallel processing of data, expanded
capacity of storing information and compatibility with living organisms (first
attempts of using biomolecular computers in cancer therapy through blocking of
improper genetic information are described in Benenson et al.(2004). However,
biomolecular computers have several drawbacks including time-consuming
procedures of preparing of input, problems in detecting output signals and
interference with by-products. Due to these obstacles, there are few laboratory
implementations of theoretically designed DNA computers (like the Turing
machine and pushdown automaton), but there are many implementations of DNA
computers for particular problems. The first practical laboratory
implementation of the general theoretical model of a machine performing
DNA-based calculations was a simple two-symbol two-state finite automaton
established by Benenson et al.(2001). In the present work, we propose a new
attitude, extending the capability of DNA-based finite automaton, by employing
two or potentially more restriction enzymes instead of one used in other works.
This creates an opportunity to implement in laboratories of more complex finite
automata and other theoretical models of computers: pushdown automata, Turing
machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5894</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5894</id><created>2011-09-27</created><authors><author><keyname>Mnih</keyname><forenames>Andriy</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Learning Item Trees for Probabilistic Modelling of Implicit Feedback</title><categories>cs.LG stat.ML</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User preferences for items can be inferred from either explicit feedback,
such as item ratings, or implicit feedback, such as rental histories. Research
in collaborative filtering has concentrated on explicit feedback, resulting in
the development of accurate and scalable models. However, since explicit
feedback is often difficult to collect it is important to develop effective
models that take advantage of the more widely available implicit feedback. We
introduce a probabilistic approach to collaborative filtering with implicit
feedback based on modelling the user's item selection process. In the interests
of scalability, we restrict our attention to tree-structured distributions over
items and develop a principled and efficient algorithm for learning item trees
from data. We also identify a problem with a widely used protocol for
evaluating implicit feedback models and propose a way of addressing it using a
small quantity of explicit feedback data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5913</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5913</id><created>2011-09-27</created><authors><author><keyname>Allix</keyname><forenames>Olivier</forenames><affiliation>LMT</affiliation></author><author><keyname>Kerfriden</keyname><forenames>Pierre</forenames><affiliation>LMT</affiliation></author><author><keyname>Gosselet</keyname><forenames>Pierre</forenames><affiliation>LMT</affiliation></author></authors><title>On the control of the load increments for a proper description of
  multiple delamination in a domain decomposition framework</title><categories>cs.NA</categories><proxy>ccsd</proxy><journal-ref>International Journal for Numerical Methods in Engineering 83, 11
  (2010) 1518-1540</journal-ref><doi>10.1002/nme.2884</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In quasi-static nonlinear time-dependent analysis, the choice of the time
discretization is a complex issue. The most basic strategy consists in
determining a value of the load increment that ensures the convergence of the
solution with respect to time on the base of preliminary simulations. In more
advanced applications, the load increments can be controlled for instance by
prescribing the number of iterations of the nonlinear resolution procedure, or
by using an arc-length algorithm. These techniques usually introduce a
parameter whose correct value is not easy to obtain. In this paper, an
alternative procedure is proposed. It is based on the continuous control of the
residual of the reference problem over time, whose measure is easy to
interpret. This idea is applied in the framework of a multiscale domain
decomposition strategy in order to perform 3D delamination analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5920</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5920</id><created>2011-09-27</created><authors><author><keyname>Grimes</keyname><forenames>Diarmuid</forenames><affiliation>4C UCC</affiliation></author><author><keyname>Hebrard</keyname><forenames>Emmanuel</forenames><affiliation>LAAS</affiliation></author></authors><title>Models and Strategies for Variants of the Job Shop Scheduling Problem</title><categories>cs.AI</categories><comments>Principles and Practice of Constraint Programming - CP 2011, Perugia
  : Italy (2011)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-23786-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a variety of constraint programming and Boolean satisfiability
approaches to scheduling problems have been introduced. They have in common the
use of relatively simple propagation mechanisms and an adaptive way to focus on
the most constrained part of the problem. In some cases, these methods compare
favorably to more classical constraint programming methods relying on
propagation algorithms for global unary or cumulative resource constraints and
dedicated search heuristics. In particular, we described an approach that
combines restarting, with a generic adaptive heuristic and solution guided
branching on a simple model based on a decomposition of disjunctive
constraints. In this paper, we introduce an adaptation of this technique for an
important subclass of job shop scheduling problems (JSPs), where the objective
function involves minimization of earliness/tardiness costs. We further show
that our technique can be improved by adding domain specific information for
one variant of the JSP (involving time lag constraints). In particular we
introduce a dedicated greedy heuristic, and an improved model for the case
where the maximal time lag is 0 (also referred to as no-wait JSPs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5931</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5931</id><created>2011-09-27</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Krishnaswamy</keyname><forenames>Ravishankar</forenames></author><author><keyname>Pruhs</keyname><forenames>Kirk</forenames></author></authors><title>Online Primal-Dual For Non-linear Optimization with Applications to
  Speed Scaling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reinterpret some online greedy algorithms for a class of nonlinear
&quot;load-balancing&quot; problems as solving a mathematical program online. For
example, we consider the problem of assigning jobs to (unrelated) machines to
minimize the sum of the alpha^{th}-powers of the loads plus assignment costs
(the online Generalized Assignment Problem); or choosing paths to connect
terminal pairs to minimize the alpha^{th}-powers of the edge loads (online
routing with speed-scalable routers). We give analyses of these online
algorithms using the dual of the primal program as a lower bound for the
optimal algorithm, much in the spirit of online primal-dual results for linear
problems.
  We then observe that a wide class of uni-processor speed scaling problems
(with essentially arbitrary scheduling objectives) can be viewed as such load
balancing problems with linear assignment costs. This connection gives new
algorithms for problems that had resisted solutions using the dominant
potential function approaches used in the speed scaling literature, as well as
alternate, cleaner proofs for other known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5938</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5938</id><created>2011-09-27</created><updated>2012-04-02</updated><authors><author><keyname>Fawzi</keyname><forenames>Alhussein</forenames></author><author><keyname>Tosic</keyname><forenames>Tamara</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Thresholding-based reconstruction of compressed correlated signals</title><categories>cs.NI cs.IT math.IT</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a set of correlated signals (e.g.,
images from different viewpoints) from a few linear measurements per signal. We
assume that each sensor in a network acquires a compressed signal in the form
of linear measurements and sends it to a joint decoder for reconstruction. We
propose a novel joint reconstruction algorithm that exploits correlation among
underlying signals. Our correlation model considers geometrical transformations
between the supports of the different signals. The proposed joint decoder
estimates the correlation and reconstructs the signals using a simple
thresholding algorithm. We give both theoretical and experimental evidence to
show that our method largely outperforms independent decoding in terms of
support recovery and reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5951</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5951</id><created>2011-09-27</created><updated>2011-09-29</updated><authors><author><keyname>Legg</keyname><forenames>Shane</forenames></author><author><keyname>Veness</keyname><forenames>Joel</forenames></author></authors><title>An Approximation of the Universal Intelligence Measure</title><categories>cs.AI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Universal Intelligence Measure is a recently proposed formal definition
of intelligence. It is mathematically specified, extremely general, and
captures the essence of many informal definitions of intelligence. It is based
on Hutter's Universal Artificial Intelligence theory, an extension of Ray
Solomonoff's pioneering work on universal induction. Since the Universal
Intelligence Measure is only asymptotically computable, building a practical
intelligence test from it is not straightforward. This paper studies the
practical issues involved in developing a real-world UIM-based performance
metric. Based on our investigation, we develop a prototype implementation which
we use to evaluate a number of different artificial agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5959</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5959</id><created>2011-09-27</created><authors><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Banerjee</keyname><forenames>Abhik</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Becker</keyname><forenames>Monique</forenames></author><author><keyname>Yeo</keyname><forenames>Chai Kiat</forenames></author><author><keyname>Lee</keyname><forenames>Bu Sung</forenames></author></authors><title>Self-organization of Nodes using Bio-Inspired Techniques for Achieving
  Small World Properties</title><categories>cs.NI</categories><comments>Accepted to Joint workshop on complex networks and pervasive group
  communication (CCNet/PerGroup), in conjunction with IEEE Globecom 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an autonomous wireless sensor network, self-organization of the nodes is
essential to achieve network wide characteristics. We believe that connectivity
in wireless autonomous networks can be increased and overall average path
length can be reduced by using beamforming and bio-inspired algorithms. Recent
works on the use of beamforming in wireless networks mostly assume the
knowledge of the network in aggregation to either heterogeneous or hybrid
deployment. We propose that without the global knowledge or the introduction of
any special feature, the average path length can be reduced with the help of
inspirations from the nature and simple interactions between neighboring nodes.
Our algorithm also reduces the number of disconnected components within the
network. Our results show that reduction in the average path length and the
number of disconnected components can be achieved using very simple local rules
and without the full network knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5966</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5966</id><created>2011-09-27</created><updated>2011-12-06</updated><authors><author><keyname>Simon</keyname><forenames>Emile</forenames></author></authors><title>Minimum settling time control design through direct search optimization</title><categories>math.OC cs.SY</categories><comments>This is a temporary third version of the paper, which present very
  succinctly two optimization results with two animations. The interested
  reader can still consult the second version for more details. A future fourth
  version developing clearly the interests and advantages of this contribution
  (including important convergence properties) will be deposited here in early
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to design controllers through explicit minimization
of the settling time of a closed-loop response, by using a class of methods
adequate for this objective. To the best of our knowledge, all the methods
available in the literature do not minimize directly the settling time but only
related objective functions. Indeed, the settling time objective function is
not only non-smooth but also discontinuous. Therefore we propose to use direct
search methods, which do not use any gradient information. An important reason
is a recent result that some direct search methods are guaranteed to
convergence on such discontinuous objective functions. The proposed approach is
self-standing but can also improve the solutions obtained with the alternatives
of the literature, which lead to good solutions but suboptimal in terms of the
settling time. Note also that this approach is very flexible and can be adapted
to a broad range of objectives as well as nonlinear systems or controllers, as
long as the time response can be simulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5981</identifier>
 <datestamp>2012-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5981</id><created>2011-09-27</created><updated>2012-02-18</updated><authors><author><keyname>Meng</keyname><forenames>Xiangrui</forenames></author><author><keyname>Saunders</keyname><forenames>Michael A.</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>LSRN: A Parallel Iterative Solver for Strongly Over- or Under-Determined
  Systems</title><categories>cs.DS cs.MS cs.NA</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a parallel iterative least squares solver named \texttt{LSRN}
that is based on random normal projection. \texttt{LSRN} computes the
min-length solution to $\min_{x \in \mathbb{R}^n} \|A x - b\|_2$, where $A \in
\mathbb{R}^{m \times n}$ with $m \gg n$ or $m \ll n$, and where $A$ may be
rank-deficient. Tikhonov regularization may also be included. Since $A$ is only
involved in matrix-matrix and matrix-vector multiplications, it can be a dense
or sparse matrix or a linear operator, and \texttt{LSRN} automatically speeds
up when $A$ is sparse or a fast linear operator. The preconditioning phase
consists of a random normal projection, which is embarrassingly parallel, and a
singular value decomposition of size $\lceil \gamma \min(m,n) \rceil \times
\min(m,n)$, where $\gamma$ is moderately larger than 1, e.g., $\gamma = 2$. We
prove that the preconditioned system is well-conditioned, with a strong
concentration result on the extreme singular values, and hence that the number
of iterations is fully predictable when we apply LSQR or the Chebyshev
semi-iterative method. As we demonstrate, the Chebyshev method is particularly
efficient for solving large problems on clusters with high communication cost.
Numerical results demonstrate that on a shared-memory machine, \texttt{LSRN}
outperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash
(SuiteSparseQR) on sparse problems. Further experiments demonstrate that
\texttt{LSRN} scales well on an Amazon Elastic Compute Cloud cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.5993</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.5993</id><created>2011-09-27</created><updated>2012-06-04</updated><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author><author><keyname>Lemvig</keyname><forenames>Jakob</forenames></author><author><keyname>Lim</keyname><forenames>Wang-Q</forenames></author></authors><title>Optimally sparse approximations of 3D functions by compactly supported
  shearlet frames</title><categories>math.FA cs.IT cs.NA math.IT</categories><comments>56 pages, 6 figures</comments><msc-class>42C40 (Primary) 42C15, 41A30, 94A08 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study efficient and reliable methods of capturing and sparsely
representing anisotropic structures in 3D data. As a model class for
multidimensional data with anisotropic features, we introduce generalized
three-dimensional cartoon-like images. This function class will have two
smoothness parameters: one parameter \beta controlling classical smoothness and
one parameter \alpha controlling anisotropic smoothness. The class then
consists of piecewise C^\beta-smooth functions with discontinuities on a
piecewise C^\alpha-smooth surface. We introduce a pyramid-adapted, hybrid
shearlet system for the three-dimensional setting and construct frames for
L^2(R^3) with this particular shearlet structure. For the smoothness range
1&lt;\alpha =&lt; \beta =&lt; 2 we show that pyramid-adapted shearlet systems provide a
nearly optimally sparse approximation rate within the generalized cartoon-like
image model class measured by means of non-linear N-term approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6018</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6018</id><created>2011-09-27</created><authors><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Jiang</keyname><forenames>Long</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>User-level sentiment analysis incorporating social networks</title><categories>cs.CL cs.IR physics.data-an physics.soc-ph</categories><comments>Proceedings of KDD 2011. Poster</comments><acm-class>I.2.7; H.3.m; H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that information about social relationships can be used to improve
user-level sentiment analysis. The main motivation behind our approach is that
users that are somehow &quot;connected&quot; may be more likely to hold similar opinions;
therefore, relationship information can complement what we can extract about a
user's viewpoints from their utterances. Employing Twitter as a source for our
experimental data, and working within a semi-supervised framework, we propose
models that are induced either from the Twitter follower/followee network or
from the network in Twitter formed by users referring to each other using &quot;@&quot;
mentions. Our transductive learning results reveal that incorporating
social-network information can indeed lead to statistically significant
sentiment-classification improvements over the performance of an approach based
on Support Vector Machines having access only to textual features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6029</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6029</id><created>2011-09-27</created><authors><author><keyname>Schroedl</keyname><forenames>S.</forenames></author></authors><title>An Improved Search Algorithm for Optimal Multiple-Sequence Alignment</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  587-623, 2005</journal-ref><doi>10.1613/jair.1534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple sequence alignment (MSA) is a ubiquitous problem in computational
biology. Although it is NP-hard to find an optimal solution for an arbitrary
number of sequences, due to the importance of this problem researchers are
trying to push the limits of exact algorithms further. Since MSA can be cast as
a classical path finding problem, it is attracting a growing number of AI
researchers interested in heuristic search algorithms as a challenge with
actual practical relevance. In this paper, we first review two previous,
complementary lines of research. Based on Hirschbergs algorithm, Dynamic
Programming needs O(kN^(k-1)) space to store both the search frontier and the
nodes needed to reconstruct the solution path, for k sequences of length N.
Best first search, on the other hand, has the advantage of bounding the search
space that has to be explored using a heuristic. However, it is necessary to
maintain all explored nodes up to the final solution in order to prevent the
search from re-expanding them at higher cost. Earlier approaches to reduce the
Closed list are either incompatible with pruning methods for the Open list, or
must retain at least the boundary of the Closed list. In this article, we
present an algorithm that attempts at combining the respective advantages; like
A* it uses a heuristic for pruning the search space, but reduces both the
maximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming. The
underlying idea is to conduct a series of searches with successively increasing
upper bounds, but using the DP ordering as the key for the Open priority queue.
With a suitable choice of thresholds, in practice, a running time below four
times that of A* can be expected. In our experiments we show that our algorithm
outperforms one of the currently most successful algorithms for optimal
multiple sequence alignments, Partial Expansion A*, both in time and memory.
Moreover, we apply a refined heuristic based on optimal alignments not only of
pairs of sequences, but of larger subsets. This idea is not new; however, to
make it practically relevant we show that it is equally important to bound the
heuristic computation appropriately, or the overhead can obliterate any
possible gain. Furthermore, we discuss a number of improvements in time and
space efficiency with regard to practical implementations. Our algorithm, used
in conjunction with higher-dimensional heuristics, is able to calculate for the
first time the optimal alignment for almost all of the problems in Reference 1
of the benchmark database BAliBASE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6030</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6030</id><created>2011-09-27</created><authors><author><keyname>Beetz</keyname><forenames>M.</forenames></author><author><keyname>Grosskreutz</keyname><forenames>H.</forenames></author></authors><title>Probabilistic Hybrid Action Models for Predicting Concurrent
  Percept-driven Robot Behavior</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 24, pages
  799-849, 2005</journal-ref><doi>10.1613/jair.1565</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic
causal model for predicting the behavior generated by modern percept-driven
robot plans. PHAMs represent aspects of robot behavior that cannot be
represented by most action models used in AI planning: the temporal structure
of continuous control processes, their non-deterministic effects, several modes
of their interferences, and the achievement of triggering conditions in
closed-loop robot plans.
  The main contributions of this article are: (1) PHAMs, a model of concurrent
percept-driven behavior, its formalization, and proofs that the model generates
probably, qualitatively accurate predictions; and (2) a resource-efficient
inference method for PHAMs based on sampling projections from probabilistic
action models and state descriptions. We show how PHAMs can be applied to
planning the course of action of an autonomous robot office courier based on
analytical and experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6033</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6033</id><created>2011-09-27</created><authors><author><keyname>DeJong</keyname><forenames>G.</forenames></author><author><keyname>Epshteyn</keyname><forenames>A.</forenames></author></authors><title>Generative Prior Knowledge for Discriminative Classification</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  25-53, 2006</journal-ref><doi>10.1613/jair.1934</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework for integrating prior knowledge into
discriminative classifiers. Our framework allows discriminative classifiers
such as Support Vector Machines (SVMs) to utilize prior knowledge specified in
the generative setting. The dual objective of fitting the data and respecting
prior knowledge is formulated as a bilevel program, which is solved
(approximately) via iterative application of second-order cone programming. To
test our approach, we consider the problem of using WordNet (a semantic
database of English language) to improve low-sample classification accuracy of
newsgroup categorization. WordNet is viewed as an approximate, but readily
available source of background knowledge, and our framework is capable of
utilizing it in a flexible way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6037</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6037</id><created>2011-09-27</created><authors><author><keyname>Baillieul</keyname><forenames>J.</forenames></author><author><keyname>&#xd6;zcimder</keyname><forenames>K.</forenames></author></authors><title>The Control Theory of Motion-Based Communication: Problems in Teaching
  Robots to Dance</title><categories>cs.SY</categories><comments>20 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes results on two components of a research program focused
on motion-based communication mediated by the dynamics of a control system.
Specifically we are interested in how mobile agents engaged in a shared
activity such as dance can use motion as a medium for transmitting certain
types of messages. The first part of the paper adopts the terminology of motion
description languages and deconstructs an elementary form of the well-known
popular dance, Salsa, in terms of four motion primitives (dance steps). Several
notions of dance complexity are introduced. We describe an experiment in which
ten performances by an actual pair of dancers are evaluated by judges and then
compared in terms of proposed complexity metrics. An energy metric is also
defined. Values of this metric are obtained by summing the lengths of motion
segments executed by wheeled robots replicating the movements of the human
dancers in each of the ten dance performances. Of all the metrics that are
considered in this experiment, energy is the most closely correlated with the
human judges' assessments of performance quality.
  The second part of the paper poses a general class of dual objective motion
control problems in which a primary objective (artistic execution of a dance
step or efficient movement toward a specified terminal state) is combined with
a communication objective. Solutions of varying degrees of explicitness can be
given in several classes of problems of communicating through the dynamics of
finite dimensional linear control systems. In this setting it is shown that the
cost of adding a communication component to motions that steer a system between
prescribed pairs of states is independent of those states. At the same time,
the optimal encoding problem itself is shown to be a problem of packing
geometric objects, and it remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6046</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6046</id><created>2011-09-27</created><authors><author><keyname>Paul</keyname><forenames>Thomas</forenames></author><author><keyname>Puscher</keyname><forenames>Daniel</forenames></author><author><keyname>Strufe</keyname><forenames>Thorsten</forenames></author></authors><title>Improving the Usability of Privacy Settings in Facebook</title><categories>cs.CR cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever increasing popularity of Facebook and other Online Social Networks
has left a wealth of personal and private data on the web, aggregated and
readily accessible for broad and automatic retrieval. Protection from both
undesired recipients as well as harvesting through crawlers is implemented by
simple access control at the provider, configured by manual authorization
through the publishing user. Several studies demonstrate that standard settings
directly cause an unnoticed over-sharing and that the users have trouble
understanding and configuring adequate settings. Using the three simple
principles of color coding, ease of access, and application of common
practices, we developed a new privacy interface that increases the usability
significantly. The results of our user study underlines the extent of the
initial problem and documents that our interface enables faster, more precise
authorisation and leads to increased intelligibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6051</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6051</id><created>2011-09-27</created><authors><author><keyname>Helmert</keyname><forenames>M.</forenames></author></authors><title>The Fast Downward Planning System</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  191-246, 2006</journal-ref><doi>10.1613/jair.1705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Downward is a classical planning system based on heuristic search. It
can deal with general deterministic planning problems encoded in the
propositional fragment of PDDL2.2, including advanced features like ADL
conditions and effects and derived predicates (axioms). Like other well-known
planners such as HSP and FF, Fast Downward is a progression planner, searching
the space of world states of a planning task in the forward direction. However,
unlike other PDDL planning systems, Fast Downward does not use the
propositional PDDL representation of a planning task directly. Instead, the
input is first translated into an alternative representation called
multi-valued planning tasks, which makes many of the implicit constraints of a
propositional planning task explicit. Exploiting this alternative
representation, Fast Downward uses hierarchical decompositions of planning
tasks for computing its heuristic function, called the causal graph heuristic,
which is very different from traditional HSP-like heuristics based on ignoring
negative interactions of operators.
  In this article, we give a full account of Fast Downwards approach to solving
multi-valued planning tasks. We extend our earlier discussion of the causal
graph heuristic to tasks involving axioms and conditional effects and present
some novel techniques for search control that are used within Fast Downwards
best-first search algorithm: preferred operators transfer the idea of helpful
actions from local search to global best-first search, deferred evaluation of
heuristic functions mitigates the negative effect of large branching factors on
search performance, and multi-heuristic best-first search combines several
heuristic evaluation functions within a single search algorithm in an
orthogonal way. We also describe efficient data structures for fast state
expansion (successor generators and axiom evaluators) and present a new
non-heuristic search algorithm called focused iterative-broadening search,
which utilizes the information encoded in causal graphs in a novel way.
  Fast Downward has proven remarkably successful: It won the &quot;classical (i.e.,
propositional, non-optimising) track of the 4th International Planning
Competition at ICAPS 2004, following in the footsteps of planners such as FF
and LPG. Our experiments show that it also performs very well on the benchmarks
of the earlier planning competitions and provide some insights about the
usefulness of the new search enhancements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6052</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6052</id><created>2011-09-27</created><authors><author><keyname>Lesser</keyname><forenames>V. R.</forenames></author><author><keyname>Mailler</keyname><forenames>R.</forenames></author></authors><title>Asynchronous Partial Overlay: A New Algorithm for Solving Distributed
  Constraint Satisfaction Problems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  529-576, 2006</journal-ref><doi>10.1613/jair.1786</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Constraint Satisfaction (DCSP) has long been considered an
important problem in multi-agent systems research. This is because many
real-world problems can be represented as constraint satisfaction and these
problems often present themselves in a distributed form. In this article, we
present a new complete, distributed algorithm called Asynchronous Partial
Overlay (APO) for solving DCSPs that is based on a cooperative mediation
process. The primary ideas behind this algorithm are that agents, when acting
as a mediator, centralize small, relevant portions of the DCSP, that these
centralized subproblems overlap, and that agents increase the size of their
subproblems along critical paths within the DCSP as the problem solving
unfolds. We present empirical evidence that shows that APO outperforms other
known, complete DCSP techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6060</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6060</id><created>2011-09-27</created><updated>2011-12-26</updated><authors><author><keyname>Itoh</keyname><forenames>Toshiya</forenames></author><author><keyname>Yoshimoto</keyname><forenames>Seiji</forenames></author></authors><title>Greedy Algorithms for Multi-Queue Buffer Management with Class
  Segregation</title><categories>cs.DM</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on a multi-queue buffer management in which packets
of different values are segregated in different queues. Our model consists of m
packets values and m queues. Recently, Al-Bawani and Souza (arXiv:1103.6049v2
[cs.DS] 30 Mar 2011) presented an online multi-queue buffer management
algorithm Greedy and showed that it is 2-competitive for the general m-valued
case, i.e., m packet values are 0 &lt; v_{1} &lt; v_{2} &lt; ... &lt; v_{m}, and
(1+v_{1}/v_{2})-competitive for the two-valued case, i.e., two packet values
are 0 &lt; v_{1} &lt; v_{2}. For the general m-valued case, let c_i = (v_{i} +
\sum_{j=1}^{i-1} 2^{j-1} v_{i-j})/(v_{i+1} + \sum_{j=1}^{i-1}2^{j-1}v_{i-j})
for 1 \leq i \leq m-1, and let c_{m}^{*} = \max_{i} c_{i}. In this paper, we
precisely analyze the competitive ratio of Greedy for the general m-valued
case, and show that the algorithm Greedy is (1+c_{m}^{*})-competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6064</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6064</id><created>2011-09-27</created><authors><author><keyname>Jiang</keyname><forenames>Albert Xin</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>A General Framework for Computing Optimal Correlated Equilibria in
  Compact Games</title><categories>cs.GT</categories><comments>14 pages. Short version to appear in WINE 2011</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of computing a correlated equilibrium that optimizes
some objective (e.g., social welfare). Papadimitriou and Roughgarden [2008]
gave a sufficient condition for the tractability of this problem; however, this
condition only applies to a subset of existing representations. We propose a
different algorithmic approach for the optimal CE problem that applies to all
compact representations, and give a sufficient condition that generalizes that
of Papadimitriou and Roughgarden. In particular, we reduce the optimal CE
problem to the deviation-adjusted social welfare problem, a combinatorial
optimization problem closely related to the optimal social welfare problem.
This framework allows us to identify new classes of games for which the optimal
CE problem is tractable; we show that graphical polymatrix games on tree graphs
are one example. We also study the problem of computing the optimal coarse
correlated equilibrium, a solution concept closely related to CE. Using a
similar approach we derive a sufficient condition for this problem, and use it
to prove that the problem is tractable for singleton congestion games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6073</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6073</id><created>2011-09-27</created><authors><author><keyname>Heinrich</keyname><forenames>Julian</forenames></author><author><keyname>Luo</keyname><forenames>Yuan</forenames></author><author><keyname>Kirkpatrick</keyname><forenames>Arthur E.</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Weiskopf</keyname><forenames>Daniel</forenames></author></authors><title>Evaluation of a Bundling Technique for Parallel Coordinates</title><categories>cs.GR</categories><report-no>TR-2011-08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique for bundled curve representations in
parallel-coordinates plots and present a controlled user study evaluating their
effectiveness. Replacing the traditional C^0 polygonal lines by C^1 continuous
piecewise Bezier curves makes it easier to visually trace data points through
each coordinate axis. The resulting Bezier curves can then be bundled to
visualize data with given cluster structures. Curve bundles are efficient to
compute, provide visual separation between data clusters, reduce visual
clutter, and present a clearer overview of the dataset. A controlled user study
with 14 participants confirmed the effectiveness of curve bundling for
parallel-coordinates visualization: 1) compared to polygonal lines, it is
equally capable of revealing correlations between neighboring data attributes;
2) its geometric cues can be effective in displaying cluster information. For
some datasets curve bundling allows the color perceptual channel to be applied
to other data attributes, while for complex cluster patterns, bundling and
color can represent clustering far more clearly than either alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6101</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6101</id><created>2011-09-28</created><updated>2011-11-16</updated><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T</forenames></author><author><keyname>Namboodiri</keyname><forenames>Vishnu</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Channel Quantization for Physical Layer Network-Coded Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>17 pages and 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of modulation schemes for the physical layer network-coded two way
relaying scenario is considered with the protocol which employs two phases:
Multiple access (MA) Phase and Broadcast (BC) phase. It was observed by
Koike-Akino et al. that adaptively changing the network coding map used at the
relay according to the channel conditions greatly reduces the impact of
multiple access interference which occurs at the relay during the MA phase. In
other words, the set of all possible channel realizations (the complex plane)
is quantized into a finite number of regions, with a specific network coding
map giving the best performance in a particular region. We highlight the issues
associated with the scheme proposed by Koike-Akino et al. and propose a scheme
which solves these issues. We obtain a quantization of the set of all possible
channel realizations analytically for the case when $M$-PSK (for $M$ any power
of 2) is the signal set used during the MA phase. It is shown that the complex
plane can be classified into two regions: a region in which any network coding
map which satisfies the so called exclusive law gives the same best performance
and a region in which the choice of the network coding map affects the
performance, which is further quantized based on the choice of the network
coding map which optimizes the performance. The quantization thus obtained
analytically, leads to the same as the one obtained using computer search for
4-PSK signal set by Koike-Akino et al., when specialized for $M=4.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6112</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6112</id><created>2011-09-28</created><authors><author><keyname>Abdelraouf</keyname><forenames>Islam</forenames></author><author><keyname>Abdennadher</keyname><forenames>Slim</forenames></author><author><keyname>Gervet</keyname><forenames>Carmen</forenames></author></authors><title>A Visual Entity-Relationship Model for Constraint-Based University
  Timetabling</title><categories>cs.AI cs.PL</categories><comments>12 pages, 7 figures, INAP 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  University timetabling (UTT) is a complex problem due to its combinatorial
nature but also the type of constraints involved. The holy grail of
(constraint) programming: &quot;the user states the problem the program solves it&quot;
remains a challenge since solution quality is tightly coupled with deriving
&quot;effective models&quot;, best handled by technology experts. In this paper, focusing
on the field of university timetabling, we introduce a visual graphic
communication tool that lets the user specify her problem in an abstract
manner, using a visual entity-relationship model. The entities are nodes of
mainly two types: resource nodes (lecturers, assistants, student groups) and
events nodes (lectures, lab sessions, tutorials). The links between the nodes
signify a desired relationship between them. The visual modeling abstraction
focuses on the nature of the entities and their relationships and abstracts
from an actual constraint model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6126</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6126</id><created>2011-09-28</created><authors><author><keyname>Li</keyname><forenames>Lianlin</forenames></author></authors><title>The Statistical Coherence-based Theory of Robust Recovery of Sparsest
  Overcomplete Representation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recovery of sparsest overcomplete representation has recently attracted
intensive research activities owe to its important potential in the many
applied fields such as signal processing, medical imaging, communication, and
so on. This problem can be stated in the following, i.e., to seek for the
sparse coefficient vector of the given noisy observation over a redundant
dictionary such that, where is the corrupted error. Elad et al. made the
worst-case result, which shows the condition of stable recovery of sparest
overcomplete representation over is where . Although it's of easy operation for
any given matrix, this result can't provide us realistic guide in many cases.
On the other hand, most of popular analysis on the sparse reconstruction relies
heavily on the so-called RIP (Restricted Isometric Property) for matrices
developed by Candes et al., which is usually very difficult or impossible to be
justified for a given measurement matrix. In this article, we introduced a
simple and efficient way of determining the ability of given D used to recover
the sparse signal based on the statistical analysis of coherence coefficients,
where is the coherence coefficients between any two different columns of given
measurement matrix . The key mechanism behind proposed paradigm is the analysis
of statistical distribution (the mean and covariance) of . We proved that if
the resulting mean of are zero, and their covariance are as small as possible,
one can faithfully recover approximately sparse signals from a minimal number
of noisy measurements with overwhelming probability. The resulting theory is
not only suitable for almost all models - e.g. Gaussian, frequency
measurements-discussed in the literature of compressed sampling, but also
provides a framework for new measurement strategies as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6131</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6131</id><created>2011-09-28</created><authors><author><keyname>Frid</keyname><forenames>Anna</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca</forenames></author></authors><title>On automatic infinite permutations</title><categories>cs.DM math.CO</categories><comments>Reported at Journ\'ees Montoises 2010; accepted to RAIRO ITA</comments><msc-class>05A05, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An infinite permutation $\alpha$ is a linear ordering of $\mathbb N$. We
study properties of infinite permutations analogous to those of infinite words,
and show some resemblances and some differences between permutations and words.
In this paper, we try to extend to permutations the notion of automaticity. As
we shall show, the standard definitions which are equivalent in the case of
words are not equivalent in the context of permutations. We investigate the
relationships between these definitions and prove that they constitute a chain
of inclusions. We also construct and study an automaton generating the
Thue-Morse permutation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6147</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6147</id><created>2011-09-28</created><updated>2012-12-27</updated><authors><author><keyname>Ma</keyname><forenames>Xiongfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Zhen</forenames></author><author><keyname>Tan</keyname><forenames>Xiaoqing</forenames></author></authors><title>Explicit combinatorial design</title><categories>math.CO cs.CC cs.CR cs.DM quant-ph</categories><comments>13 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A combinatorial design is a family of sets that are almost disjoint, which is
applied in pseudo random number generations and randomness extractions. The
parameter, $\rho$, quantifying the overlap between the sets within the family,
is directly related to the length of a random seed needed and the efficiency of
an extractor. Nisan and Wigderson proposed an explicit construction of designs
in 1994. Later in 2003, Hartman and Raz proved a bound of $\rho\le e^2$ for the
Nisan-Wigderson construction in a limited parameter regime. In this work, we
prove a tighter bound of $\rho&lt;e$ with the entire parameter range by slightly
refining the Nisan-Wigderson construction. Following the block idea used by
Raz, Reingold, and Vadhan, we present an explicit weak design with $\rho=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6166</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6166</id><created>2011-09-28</created><updated>2012-06-02</updated><authors><author><keyname>Wu</keyname><forenames>Yu</forenames></author><author><keyname>Bui</keyname><forenames>Loc</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author></authors><title>Heavy Traffic Approximation of Equilibria in Resource Sharing Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a model of priced resource sharing that combines both queueing
behavior and strategic behavior. We study a priority service model where a
single server allocates its capacity to agents in proportion to their payment
to the system, and users from different classes act to minimize the sum of
their cost for processing delay and payment. As the exact processing time of
this system is hard to compute, we introduce the notion of heavy traffic
equilibrium as an approximation of the Nash equilibrium, derived by considering
the asymptotic regime where the system load approaches capacity. We discuss
efficiency and revenue, and in particular provide a bound for the price of
anarchy of the heavy traffic equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6178</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6178</id><created>2011-09-28</created><updated>2011-11-29</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author><author><keyname>Vardi</keyname><forenames>Shai</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author></authors><title>Space-efficient Local Computation Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently Rubinfeld et al. (ICS 2011, pp. 223--238) proposed a new model of
sublinear algorithms called \emph{local computation algorithms}. In this model,
a computation problem $F$ may have more than one legal solution and each of
them consists of many bits. The local computation algorithm for $F$ should
answer in an online fashion, for any index $i$, the $i^{\mathrm{th}}$ bit of
some legal solution of $F$. Further, all the answers given by the algorithm
should be consistent with at least one solution of $F$.
  In this work, we continue the study of local computation algorithms. In
particular, we develop a technique which under certain conditions can be
applied to construct local computation algorithms that run not only in
polylogarithmic time but also in polylogarithmic \emph{space}. Moreover, these
local computation algorithms are easily parallelizable and can answer all
parallel queries consistently. Our main technical tools are pseudorandom
numbers with bounded independence and the theory of branching processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6181</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6181</id><created>2011-09-28</created><updated>2011-10-04</updated><authors><author><keyname>Gravier</keyname><forenames>Sylvain</forenames></author><author><keyname>Javelle</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Mhalla</keyname><forenames>Mehdi</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>Optimal accessing and non-accessing structures for graph protocols</title><categories>cs.CC math.CO quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accessing set in a graph is a subset B of vertices such that there exists
D subset of B, such that each vertex of V\B has an even number of neighbors in
D. In this paper, we introduce new bounds on the minimal size kappa'(G) of an
accessing set, and on the maximal size kappa(G) of a non-accessing set of a
graph G. We show strong connections with perfect codes and give explicitly
kappa(G) and kappa'(G) for several families of graphs. Finally, we show that
the corresponding decision problems are NP-Complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6182</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6182</id><created>2011-09-28</created><authors><author><keyname>Garg</keyname><forenames>Jugal</forenames></author><author><keyname>Jiang</keyname><forenames>Albert Xin</forenames></author><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author></authors><title>Bilinear Games: Polynomial Time Algorithms for Rank Based Subclasses</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the sequence form formulation of Koller et al. (GEB'96), this
paper defines {\em bilinear games}, and proposes efficient algorithms for its
rank based subclasses. Bilinear games are two-player non-cooperative
single-shot games with compact polytopal strategy sets and two payoff matrices
(A,B) such that when (x,y) is the played strategy profile, the payoffs of the
players are xAy and xBy respectively. We show that bilinear games are very
general and capture many interesting classes of games like bimatrix games, two
player Bayesian games, polymatrix games, two-player extensive form games with
perfect recall etc. as special cases, and hence are hard to solve in general.
  Existence of a (symmetric) Nash equilibrium for (symmetric) bilinear games
follow directly from the known results. For a given bilinear game, we define
its {\em Best Response Polytopes} (BRPs) and characterize the set of Nash
equilibria as {\em fully-labeled} pairs in the BRPs. We consider a rank based
hierarchy of bilinear games, where rank of a game (A,B) is defined as
rank(A+B). In this paper, we give polynomial time algorithms to compute Nash
equilibrium for special classes of bilinear games:
  (i) Rank-1 games (i.e., rank(A+B)=1). (ii) FPTAS for constant rank games
(i.e., rank(A+B) is constant). (iii) When rank(A) or rank(B) is constant. This
improves the results by Lipton et al. (EC'03) and Kannan et al. (ET'09), for
bimatrix games with low rank matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6191</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6191</id><created>2011-09-28</created><authors><author><keyname>Hlinka</keyname><forenames>Ondrej</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author><author><keyname>Djuric</keyname><forenames>Petar M.</forenames></author></authors><title>Likelihood Consensus-Based Distributed Particle Filtering with
  Distributed Proposal Density Adaptation</title><categories>stat.AP cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a consensus-based distributed particle filter (PF) for wireless
sensor networks. Each sensor runs a local PF to compute a global state estimate
that takes into account the measurements of all sensors. The local PFs use the
joint (all-sensors) likelihood function, which is calculated in a distributed
way by a novel generalization of the likelihood consensus scheme. A performance
improvement (or a reduction of the required number of particles) is achieved by
a novel distributed, consensus-based method for adapting the proposal densities
of the local PFs. The performance of the proposed distributed PF is
demonstrated for a target tracking problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6199</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6199</id><created>2011-09-28</created><authors><author><keyname>Khan</keyname><forenames>Wazir Zada</forenames></author><author><keyname>Aalsalem</keyname><forenames>Mohammed Y.</forenames></author><author><keyname>Arshad</keyname><forenames>Quratul Ain</forenames></author></authors><title>The Aware Cricket Ground</title><categories>cs.OH</categories><comments>5 pages, 5 figure</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 2, July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most profound technologies are those that disappear. They weave
themselves into fabrics of everyday life until they are indistinguishable from
it [1]. This research work is a mere effort for automated decision making
during sports of most common interest leveraging ubiquitous computing.
Primarily cricket has been selected for the first implementation of the idea. A
positioning system is used for locating the objects moving in the field. Main
objectives of the research are to help achieve the following goals. 1) Make
Decisions where human eye can make error due to human limitations. 2) Simulate
the Match activity during and after the game in a 3D computerized Graphics
system. 3) Make various types of game and performance analysis of a certain
team or a player.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6202</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6202</id><created>2011-09-28</created><authors><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>On Variable Density Compressive Sampling</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Signal Processing Letters, vol. 18(10), pp. 595-598, 2011</journal-ref><doi>10.1109/LSP.2011.2163712</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We advocate an optimization procedure for variable density sampling in the
context of compressed sensing. In this perspective, we introduce a minimization
problem for the coherence between the sparsity and sensing bases, whose
solution provides an optimized sampling profile. This minimization problem is
solved with the use of convex optimization algorithms. We also propose a
refinement of our technique when prior information is available on the signal
support in the sparsity basis. The effectiveness of the method is confirmed by
numerical experiments. Our results also provide a theoretical underpinning to
state-of-the-art variable density Fourier sampling procedures used in magnetic
resonance imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6206</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6206</id><created>2011-09-28</created><authors><author><keyname>Jyoti</keyname></author><author><keyname>Sharma</keyname><forenames>A. K.</forenames></author><author><keyname>Goel</keyname><forenames>Amit</forenames></author></authors><title>A Framework for Prefetching Relevant Web Pages using Predictive
  Prefetching Engine (PPE)</title><categories>cs.IR</categories><comments>9 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 7,
  Issue 6, November 2010 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework for increasing the relevancy of the web pages
retrieved by the search engine. The approach introduces a Predictive
Prefetching Engine (PPE) which makes use of various data mining algorithms on
the log maintained by the search engine. The underlying premise of the approach
is that in the case of cluster accesses, the next pages requested by users of
the Web server are typically based on the current and previous pages requested.
Based on same, rules are drawn which then lead the path for prefetching the
desired pages. To carry out the desired task of prefetching the more relevant
pages, agents have been introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6211</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6211</id><created>2011-09-28</created><authors><author><keyname>Mryglod</keyname><forenames>O.</forenames></author><author><keyname>Holovatch</keyname><forenames>Yu.</forenames></author><author><keyname>Mryglod</keyname><forenames>I.</forenames></author></authors><title>Editorial process in scientific journals: analysis and modeling</title><categories>physics.data-an cs.DL</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The editorial handling of papers in scientific journals as a human activity
process is considered. Using recently proposed approaches of human dynamics
theory we examine the probability distributions of random variables reflecting
the temporal characteristics of studied processes. The first part of this paper
contains our results of analysis of the real data about papers published in
scientific journals. The second part is devoted to modeling of time-series
connected with editorial work. The purpose of our work is to present new object
that can be studied in terms of human dynamics theory and to corroborate the
scientometrical application of the results obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6220</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6220</id><created>2011-09-28</created><authors><author><keyname>Ummels</keyname><forenames>Michael</forenames></author><author><keyname>Wojtczak</keyname><forenames>Dominik</forenames></author></authors><title>The Complexity of Nash Equilibria in Limit-Average Games</title><categories>cs.GT cs.CC</categories><comments>34 pages</comments><doi>10.1007/978-3-642-23217-6_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of Nash equilibria in concurrent games
with limit-average objectives. In particular, we prove that the existence of a
Nash equilibrium in randomised strategies is undecidable, while the existence
of a Nash equilibrium in pure strategies is decidable, even if we put a
constraint on the payoff of the equilibrium. Our undecidability result holds
even for a restricted class of concurrent games, where nonzero rewards occur
only on terminal states. Moreover, we show that the constrained existence
problem is undecidable not only for concurrent games but for turn-based games
with the same restriction on rewards. Finally, we prove that the constrained
existence problem for Nash equilibria in (pure or randomised) stationary
strategies is decidable and analyse its complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6221</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6221</id><created>2011-09-28</created><authors><author><keyname>Baek</keyname><forenames>Seung Ki</forenames></author><author><keyname>Minnhagen</keyname><forenames>Petter</forenames></author><author><keyname>Kim</keyname><forenames>Beom Jun</forenames></author></authors><title>The Ten Thousand Kims</title><categories>physics.soc-ph cs.DL</categories><comments>13 pages, 8 figures</comments><journal-ref>New J. Phys. 13, 073036 (2011)</journal-ref><doi>10.1088/1367-2630/13/7/073036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Korean culture the family members are recorded in special family
books. This makes it possible to follow the distribution of Korean family names
far back in history. It is here shown that these name distributions are well
described by a simple null model, the random group formation (RGF) model. This
model makes it possible to predict how the name distributions change and these
predictions are shown to be borne out. In particular, the RGF model predicts
that, for married women entering a collection of family books in a certain
year, the occurrence of the most common family name &quot;Kim&quot; should be directly
proportional the total number of married women with the same proportionality
constant for all the years. This prediction is also borne out to high degree.
We speculate that it reflects some inherent social stability in the Korean
culture. In addition, we obtain an estimate of the total population of the
Korean culture down to year 500 AD, based on the RGF model and find about ten
thousand Kims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6222</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6222</id><created>2011-09-28</created><updated>2012-10-02</updated><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Dossal</keyname><forenames>Charles</forenames><affiliation>IMB</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal</forenames><affiliation>GREYC</affiliation></author></authors><title>Robust Sparse Analysis Regularization</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the theoretical guarantees of L1-analysis
regularization when solving linear inverse problems. Most of previous works in
the literature have mainly focused on the sparse synthesis prior where the
sparsity is measured as the L1 norm of the coefficients that synthesize the
signal from a given dictionary. In contrast, the more general analysis
regularization minimizes the L1 norm of the correlations between the signal and
the atoms in the dictionary, where these correlations define the analysis
support. The corresponding variational problem encompasses several well-known
regularizations such as the discrete total variation and the Fused Lasso. Our
main contributions consist in deriving sufficient conditions that guarantee
exact or partial analysis support recovery of the true signal in presence of
noise. More precisely, we give a sufficient condition to ensure that a signal
is the unique solution of the L1-analysis regularization in the noiseless case.
The same condition also guarantees exact analysis support recovery and
L2-robustness of the L1-analysis minimizer vis-a-vis an enough small noise in
the measurements. This condition turns to be sharp for the robustness of the
analysis support. To show partial support recovery and L2-robustness to an
arbitrary bounded noise, we introduce a stronger sufficient condition. When
specialized to the L1-synthesis regularization, our results recover some
corresponding recovery and robustness guarantees previously known in the
literature. From this perspective, our work is a generalization of these
results. We finally illustrate these theoretical findings on several examples
to study the robustness of the 1-D total variation and Fused Lasso
regularizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6263</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6263</id><created>2011-09-28</created><authors><author><keyname>Linden</keyname><forenames>Greg</forenames><affiliation>Microsoft</affiliation></author><author><keyname>Meek</keyname><forenames>Christopher</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Chickering</keyname><forenames>Max</forenames><affiliation>Microsoft</affiliation></author></authors><title>The Pollution Effect: Optimizing Keyword Auctions by Favoring Relevant
  Advertising</title><categories>cs.GT cs.CY cs.IR</categories><comments>Presented at the Fifth Workshop on Ad Auctions, July 6, 2009,
  Stanford, CA, USA</comments><acm-class>K.4.4; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6264</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6264</id><created>2011-09-28</created><authors><author><keyname>Hague</keyname><forenames>Matthew</forenames></author></authors><title>Parameterised Pushdown Systems with Non-Atomic Writes</title><categories>cs.FL</categories><comments>This is the long version of a paper appearing in FSTTCS 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider the master/slave parameterised reachability problem for networks
of pushdown systems, where communication is via a global store using only
non-atomic reads and writes. We show that the control-state reachability
problem is decidable. As part of the result, we provide a constructive
extension of a theorem by Ehrenfeucht and Rozenberg to produce an NFA
equivalent to certain kinds of CFG. Finally, we show that the non-parameterised
version is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6269</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6269</id><created>2011-09-28</created><updated>2012-04-05</updated><authors><author><keyname>Zhu</keyname><forenames>Hao</forenames></author><author><keyname>Prasad</keyname><forenames>Narayan</forenames></author><author><keyname>Rangarajan</keyname><forenames>Sampath</forenames></author></authors><title>Precoder Design for Physical Layer Multicasting</title><categories>cs.IT math.IT</categories><comments>37 pages, 8 figures, submitted to IEEE Trans. Signal Proc</comments><doi>10.1109/TSP.2012.2210710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the instantaneous rate maximization and the weighted sum
delay minimization problems over a K-user multicast channel, where multiple
antennas are available at the transmitter as well as at all the receivers.
Motivated by the degree of freedom optimality and the simplicity offered by
linear precoding schemes, we consider the design of linear precoders using the
aforementioned two criteria. We first consider the scenario wherein the linear
precoder can be any complex-valued matrix subject to rank and power
constraints. We propose cyclic alternating ascent based precoder design
algorithms and establish their convergence to respective stationary points.
Simulation results reveal that our proposed algorithms considerably outperform
known competing solutions. We then consider a scenario in which the linear
precoder can be formed by selecting and concatenating precoders from a given
finite codebook of precoding matrices, subject to rank and power constraints.
We show that under this scenario, the instantaneous rate maximization problem
is equivalent to a robust submodular maximization problem which is strongly NP
hard. We propose a deterministic approximation algorithm and show that it
yields a bicriteria approximation. For the weighted sum delay minimization
problem we propose a simple deterministic greedy algorithm, which at each step
entails approximately maximizing a submodular set function subject to multiple
knapsack constraints, and establish its performance guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6270</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6270</id><created>2011-09-27</created><authors><author><keyname>Ghosh</keyname><forenames>Avishek</forenames></author><author><keyname>Banerjee</keyname><forenames>Joydeep</forenames></author><author><keyname>Hassan</keyname><forenames>Sk. S.</forenames></author><author><keyname>Choudhury</keyname><forenames>P. Pal</forenames></author></authors><title>Fractal String Generation and Its Application in Music Composition</title><categories>cs.SD cs.DM</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Music is a string of some of the notes out of 12 notes (Sa, Komal_re, Re,
Komal_ga, Ga, Ma, Kari_ma, Pa, Komal_dha, Dha, Komal_ni, Ni) and their
harmonics. Each note corresponds to a particular frequency. When such strings
are encoded to form discrete sequences, different frequencies present in the
music corresponds to different amplitude levels (value) of the discrete
sequence. Initially, a class of discrete sequences has been generated using
logistic map. All these discrete sequences have at most n-different amplitude
levels (value) (depending on the particular raga). Without loss of generality,
we have chosen two discrete sequences of two types of Indian raga viz. Bhairabi
and Bhupali having same number of amplitude levels to obtain/search close
relatives from the class. The relative / closeness can be assured through
correlation coefficient.The search is unbiased, random and non-adaptive. The
obtained string is that which maximally resembles the given two sequences. The
same can be thought of as a music composition of the given two strings. It is
to be noted that all these string are fractal string which can be persuaded by
fractal dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6273</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6273</id><created>2011-09-28</created><updated>2014-03-16</updated><authors><author><keyname>Simmons</keyname><forenames>Robert J.</forenames></author></authors><title>Structural focalization</title><categories>cs.LO</categories><comments>A Twelf formalization is included and an Agda formalization is
  available at https://github.com/robsimmons/agda-lib/tree/focalization</comments><msc-class>03F05, 03F07, 03F65</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing, introduced by Jean-Marc Andreoli in the context of classical linear
logic, defines a normal form for sequent calculus derivations that cuts down on
the number of possible derivations by eagerly applying invertible rules and
grouping sequences of non-invertible rules. A focused sequent calculus is
defined relative to some non-focused sequent calculus; focalization is the
property that every non-focused derivation can be transformed into a focused
derivation.
  In this paper, we present a focused sequent calculus for propositional
intuitionistic logic and prove the focalization property relative to a standard
presentation of propositional intuitionistic logic. Compared to existing
approaches, the proof is quite concise, depending only on the internal
soundness and completeness of the focused logic. In turn, both of these
properties can be established (and mechanically verified) by structural
induction in the style of Pfenning's structural cut elimination without the
need for any tedious and repetitious invertibility lemmas. The proof of cut
admissibility for the focused system, which establishes internal soundness, is
not particularly novel. The proof of identity expansion, which establishes
internal completeness, is a major contribution of this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6276</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6276</id><created>2011-09-28</created><authors><author><keyname>Fernandes</keyname><forenames>Fabio</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Lattices for Physical-layer Secrecy: A Computational Perspective</title><categories>cs.IT cs.CR math.IT</categories><comments>accepted to the Physical Layer Secrecy Workshop at ICC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use the hardness of quantization over general lattices as
the basis of developing a physical layer secrecy system. Assuming that the
channel state observed by the legitimate receiver and the eavesdropper are
distinct, this asymmetry is used to develop a cryptosystem that resembles the
McEliece cryptosystem, designed to be implemented at the physical layer. We
ensure that the legitimate receiver observes a specific lattice over which
decoding is known to be possible in polynomial-time. while the eavesdropper
observes a lattice over which decoding will prove to have the complexity of
lattice quantization over a general lattice
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6279</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6279</id><created>2011-09-28</created><authors><author><keyname>Sagraloff</keyname><forenames>Michael</forenames></author></authors><title>When Newton meets Descartes: A Simple and Fast Algorithm to Isolate the
  Real Roots of a Polynomial</title><categories>cs.SC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new algorithm denoted DSC2 to isolate the real roots of a
univariate square-free polynomial f with integer coefficients. The algorithm
iteratively subdivides an initial interval which is known to contain all real
roots of f. The main novelty of our approach is that we combine Descartes' Rule
of Signs and Newton iteration. More precisely, instead of using a fixed
subdivision strategy such as bisection in each iteration, a Newton step based
on the number of sign variations for an actual interval is considered, and,
only if the Newton step fails, we fall back to bisection. Following this
approach, our analysis shows that, for most iterations, we can achieve
quadratic convergence towards the real roots. In terms of complexity, our
method induces a recursion tree of almost optimal size O(nlog(n tau)), where n
denotes the degree of the polynomial and tau the bitsize of its coefficients.
The latter bound constitutes an improvement by a factor of tau upon all
existing subdivision methods for the task of isolating the real roots. In
addition, we provide a bit complexity analysis showing that DSC2 needs only
\tilde{O}(n^3tau) bit operations to isolate all real roots of f. This matches
the best bound known for this fundamental problem. However, in comparison to
the much more involved algorithms by Pan and Sch\&quot;onhage (for the task of
isolating all complex roots) which achieve the same bit complexity, DSC2
focuses on real root isolation, is very easy to access and easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6288</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6288</id><created>2011-09-28</created><authors><author><keyname>Gargantini</keyname><forenames>Angelo</forenames></author></authors><title>Using Stereoscopic 3D Technologies for the Diagnosis and Treatment of
  Amblyopia in Children</title><categories>cs.HC cs.GR</categories><comments>Extended version of the HEALTHINF 2011 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D4Amb project aims at developing a system based on the stereoscopic 3D
techonlogy, like the NVIDIA 3D Vision, for the diagnosis and treatment of
amblyopia in young children. It exploits the active shutter technology to
provide binocular vision, i.e. to show different images to the amblyotic (or
lazy) and the normal eye. It would allow easy diagnosis of amblyopia and its
treatment by means of interactive games or other entertainment activities. It
should not suffer from the compliance problems of the classical treatment, it
is suitable to domestic use, and it could at least partially substitute
occlusion or patching of the normal eye.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6297</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6297</id><created>2011-09-28</created><authors><author><keyname>Ram&#xed;rez</keyname><forenames>Ignacio</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Low-rank data modeling via the Minimum Description Length principle</title><categories>cs.IT cs.MM math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust low-rank matrix estimation is a topic of increasing interest, with
promising applications in a variety of fields, from computer vision to data
mining and recommender systems. Recent theoretical results establish the
ability of such data models to recover the true underlying low-rank matrix when
a large portion of the measured matrix is either missing or arbitrarily
corrupted. However, if low rank is not a hypothesis about the true nature of
the data, but a device for extracting regularity from it, no current guidelines
exist for choosing the rank of the estimated matrix. In this work we address
this problem by means of the Minimum Description Length (MDL) principle -- a
well established information-theoretic approach to statistical inference -- as
a guideline for selecting a model for the data at hand. We demonstrate the
practical usefulness of our formal approach with results for complex background
extraction in video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6299</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6299</id><created>2011-09-28</created><authors><author><keyname>Belohlavek</keyname><forenames>Radim</forenames></author><author><keyname>Urbanova</keyname><forenames>Lucie</forenames></author><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Sensitivity Analysis for Declarative Relational Query Languages with
  Ordinal Ranks</title><categories>cs.DB</categories><comments>The paper will appear in Proceedings of the 19th International
  Conference on Applications of Declarative Programming and Knowledge
  Management (INAP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present sensitivity analysis for results of query executions in a
relational model of data extended by ordinal ranks. The underlying model of
data results from the ordinary Codd's model of data in which we consider
ordinal ranks of tuples in data tables expressing degrees to which tuples match
queries. In this setting, we show that ranks assigned to tuples are insensitive
to small changes, i.e., small changes in the input data do not yield large
changes in the results of queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6303</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6303</id><created>2011-09-28</created><updated>2013-01-02</updated><authors><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Reduced-Dimension Multiuser Detection</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a reduced-dimension multiuser detector (RD-MUD) structure for
synchronous systems that significantly decreases the number of required
correlation branches at the receiver front-end, while still achieving
performance similar to that of the conventional matched-filter (MF) bank.
RD-MUD exploits the fact that, in some wireless systems, the number of active
users may be small relative to the total number of users in the system. Hence,
the ideas of analog compressed sensing may be used to reduce the number of
correlators. The correlating signals used by each correlator are chosen as an
appropriate linear combination of the users' spreading waveforms. We derive the
probability-of-symbol-error when using two methods for recovery of active users
and their transmitted symbols: the reduced-dimension decorrelating (RDD)
detector, which combines subspace projection and thresholding to determine
active users and sign detection for data recovery, and the reduced-dimension
decision-feedback (RDDF) detector, which combines decision-feedback matching
pursuit for active user detection and sign detection for data recovery. We
derive probability of error bounds for both detectors, and show that the number
of correlators needed to achieve a small probability-of-symbol-error is on the
order of the logarithm of the number of users in the system. The theoretical
performance results are validated via numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6310</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6310</id><created>2011-09-28</created><updated>2011-12-07</updated><authors><author><keyname>Wang</keyname><forenames>Da</forenames></author><author><keyname>Ingber</keyname><forenames>Amir</forenames></author><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author></authors><title>The Dispersion of Joint Source-Channel Coding</title><categories>cs.IT math.IT</categories><comments>Extended version of work presented in the 2011 Allerton conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the behavior of the distortion threshold that can
be guaranteed in joint source-channel coding, to within a prescribed
excess-distortion probability. We show that the gap between this threshold and
the optimal average distortion is governed by a constant that we call the joint
source-channel dispersion. This constant can be easily computed, since it is
the sum of the source and channel dispersions, previously derived. The
resulting performance is shown to be better than that of any separation-based
scheme. For the proof, we use unequal error protection channel coding, thus we
also evaluate the dispersion of that setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6313</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6313</id><created>2011-09-27</created><authors><author><keyname>Liuni</keyname><forenames>M.</forenames></author><author><keyname>R&#xf6;bel</keyname><forenames>A.</forenames></author><author><keyname>Romito</keyname><forenames>M.</forenames></author><author><keyname>Rodet</keyname><forenames>X.</forenames></author></authors><title>A Reduced Multiple Gabor Frame for Local Time Adaptation of the
  Spectrogram</title><categories>cs.SD</categories><journal-ref>Proc. of the 13th Int. Conference on Digital Audio Effects
  (DAFx-10), Graz, Austria , September 6-10, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method for automatic local time adap- tation of
the spectrogram of an audio signal, based on its decomposition within a Gabor
multi-frame. The sparsity of the analyses within each individual frame is
evaluated through the R\'enyi entropies measures. According to the sparsity of
the decompositions, an optimal resolution and a reduced multi-frame are
determined, defining an adapted spectrogram with variable resolution and hop
size. The composition of such a reduced multi-frame allows an immediate
definition of a dual frame: re-synthesis techniques for this adapted analysis
are easily derived by the traditional phase vocoder scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6314</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6314</id><created>2011-09-27</created><authors><author><keyname>Liuni</keyname><forenames>M.</forenames></author><author><keyname>R&#xf6;bel</keyname><forenames>A.</forenames></author><author><keyname>Romito</keyname><forenames>M.</forenames></author><author><keyname>Rodet</keyname><forenames>X.</forenames></author></authors><title>An Entropy Based Method for Local Time-Adaptation of the Spectrogram</title><categories>cs.SD</categories><journal-ref>CMMR 2010, LNCS 6684, pp. 60-75, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for automatic local time-adaptation of the spectrogram of
audio signals: it is based on the decomposition of a signal within a Gabor
multi-frame through the STFT operator. The sparsity of the analysis in every
individual frame of the multi-frame is evaluated through the R\'enyi entropy
measures: the best local resolution is determined minimizing the entropy
values. The overall spectrogram of the signal we obtain thus provides local
optimal resolution adaptively evolving over time. We give examples of the
performance of our algorithm with an instrumental sound and a synthetic one,
showing the improvement in spectrogram displaying obtained with an automatic
adaptation of the resolution. The analysis operator is invertible, thus leading
to a perfect reconstruction of the original signal through the analysis
coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6340</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6340</id><created>2011-09-28</created><authors><author><keyname>Endriss</keyname><forenames>U.</forenames></author><author><keyname>Maudet</keyname><forenames>N.</forenames></author><author><keyname>Sadri</keyname><forenames>F.</forenames></author><author><keyname>Toni</keyname><forenames>F.</forenames></author></authors><title>Negotiating Socially Optimal Allocations of Resources</title><categories>cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  315-348, 2006</journal-ref><doi>10.1613/jair.1870</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiagent system may be thought of as an artificial society of autonomous
software agents and we can apply concepts borrowed from welfare economics and
social choice theory to assess the social welfare of such an agent society. In
this paper, we study an abstract negotiation framework where agents can agree
on multilateral deals to exchange bundles of indivisible resources. We then
analyse how these deals affect social welfare for different instances of the
basic framework and different interpretations of the concept of social welfare
itself. In particular, we show how certain classes of deals are both sufficient
and necessary to guarantee that a socially optimal allocation of resources will
be reached eventually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6341</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6341</id><created>2011-09-28</created><authors><author><keyname>Daume</keyname><forenames>H.</forenames><suffix>III</suffix></author><author><keyname>Marcu</keyname><forenames>D.</forenames></author></authors><title>Domain Adaptation for Statistical Classifiers</title><categories>cs.LG cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  101-126, 2006</journal-ref><doi>10.1613/jair.1872</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most basic assumption used in statistical learning theory is that
training data and test data are drawn from the same underlying distribution.
Unfortunately, in many applications, the &quot;in-domain&quot; test data is drawn from a
distribution that is related, but not identical, to the &quot;out-of-domain&quot;
distribution of the training data. We consider the common case in which labeled
out-of-domain data is plentiful, but labeled in-domain data is scarce. We
introduce a statistical formulation of this problem in terms of a simple
mixture model and present an instantiation of this framework to maximum entropy
classifiers and their linear chain counterparts. We present efficient inference
algorithms for this special case based on the technique of conditional
expectation maximization. Our experimental results show that our approach leads
to improved performance on three real world tasks on four different data sets
from the natural language processing domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6344</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6344</id><created>2011-09-28</created><authors><author><keyname>Booth</keyname><forenames>R.</forenames></author><author><keyname>Meyer</keyname><forenames>T.</forenames></author></authors><title>Admissible and Restrained Revision</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  127-151, 2006</journal-ref><doi>10.1613/jair.1874</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As partial justification of their framework for iterated belief revision
Darwiche and Pearl convincingly argued against Boutiliers natural revision and
provided a prototypical revision operator that fits into their scheme. We show
that the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller
class of operators which we refer to as admissible. Admissible revision ensures
that the penultimate input is not ignored completely, thereby eliminating
natural revision, but includes the Darwiche-Pearl operator, Nayaks
lexicographic revision operator, and a newly introduced operator called
restrained revision. We demonstrate that restrained revision is the most
conservative of admissible revision operators, effecting as few changes as
possible, while lexicographic revision is the least conservative, and point out
that restrained revision can also be viewed as a composite operator, consisting
of natural revision preceded by an application of a &quot;backwards revision&quot;
operator previously studied by Papini. Finally, we propose the establishment of
a principled approach for choosing an appropriate revision operator in
different contexts and discuss future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6345</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6345</id><created>2011-09-28</created><authors><author><keyname>Brafman</keyname><forenames>R. I.</forenames></author><author><keyname>Domshlak</keyname><forenames>C.</forenames></author><author><keyname>Shimony</keyname><forenames>S. E.</forenames></author></authors><title>On Graphical Modeling of Preference and Importance</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  389-424, 2006</journal-ref><doi>10.1613/jair.1895</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, CP-nets have emerged as a useful tool for supporting
preference elicitation, reasoning, and representation. CP-nets capture and
support reasoning with qualitative conditional preference statements,
statements that are relatively natural for users to express. In this paper, we
extend the CP-nets formalism to handle another class of very natural
qualitative statements one often uses in expressing preferences in daily life -
statements of relative importance of attributes. The resulting formalism,
TCP-nets, maintains the spirit of CP-nets, in that it remains focused on using
only simple and natural preference statements, uses the ceteris paribus
semantics, and utilizes a graphical representation of this information to
reason about its consistency and to perform, possibly constrained, optimization
using it. The extra expressiveness it provides allows us to better model
tradeoffs users would like to make, more faithfully representing their
preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6346</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6346</id><created>2011-09-28</created><authors><author><keyname>Pistore</keyname><forenames>M.</forenames></author><author><keyname>Vardi</keyname><forenames>M. Y.</forenames></author></authors><title>The Planning Spectrum - One, Two, Three, Infinity</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  101-132, 2007</journal-ref><doi>10.1613/jair.1909</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Temporal Logic (LTL) is widely used for defining conditions on the
execution paths of dynamic systems. In the case of dynamic systems that allow
for nondeterministic evolutions, one has to specify, along with an LTL formula
f, which are the paths that are required to satisfy the formula. Two extreme
cases are the universal interpretation A.f, which requires that the formula be
satisfied for all execution paths, and the existential interpretation E.f,
which requires that the formula be satisfied for some execution path.
  When LTL is applied to the definition of goals in planning problems on
nondeterministic domains, these two extreme cases are too restrictive. It is
often impossible to develop plans that achieve the goal in all the
nondeterministic evolutions of a system, and it is too weak to require that the
goal is satisfied by some execution.
  In this paper we explore alternative interpretations of an LTL formula that
are between these extreme cases. We define a new language that permits an
arbitrary combination of the A and E quantifiers, thus allowing, for instance,
to require that each finite execution can be extended to an execution
satisfying an LTL formula (AE.f), or that there is some finite execution whose
extensions all satisfy an LTL formula (EA.f). We show that only eight of these
combinations of path quantifiers are relevant, corresponding to an alternation
of the quantifiers of length one (A and E), two (AE and EA), three (AEA and
EAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for
the new language that is based on an automata-theoretic approach, and study its
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6347</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6347</id><created>2011-09-28</created><authors><author><keyname>Bakhshi</keyname><forenames>Saeideh</forenames></author><author><keyname>Dovrolis</keyname><forenames>Constantine</forenames></author></authors><title>Incremental Versus Optimized Network Design</title><categories>cs.NI physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though the problem of network topology design is often studied as a
&quot;clean-slate&quot; optimization, in practice most service-provider and enterprise
networks are designed incrementally over time. This evolutionary process is
driven by changes in the underlying parameters and constraints (the
&quot;environment&quot;) and it aims to minimize the modification cost after each change
in the environment. In this paper, we first formulate the incremental design
approach (in three variations), and compare that with the more traditional
optimized design approach in which the objective is to minimize the total
network cost. We evaluate the cost overhead and evolvability of incremental
design under two network expansion models (random and gradual), comparing
incremental and optimized networks in terms of cost, topological similarity,
delay and robustness. We find that even though incremental design has some cost
overhead, that overhead does not increase as the network grows. Also, it is
less costly to evolve an existing network than to design it &quot;from scratch&quot; as
long as the network expansion factor is less than a critical value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6348</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6348</id><created>2011-09-28</created><authors><author><keyname>Roy</keyname><forenames>A.</forenames></author></authors><title>Fault Tolerant Boolean Satisfiability</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  503-527, 2006</journal-ref><doi>10.1613/jair.1914</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A delta-model is a satisfying assignment of a Boolean formula for which any
small alteration, such as a single bit flip, can be repaired by flips to some
small number of other bits, yielding a new satisfying assignment. These
satisfying assignments represent robust solutions to optimization problems
(e.g., scheduling) where it is possible to recover from unforeseen events
(e.g., a resource becoming unavailable). The concept of delta-models was
introduced by Ginsberg, Parkes and Roy (AAAI 1998), where it was proved that
finding delta-models for general Boolean formulas is NP-complete. In this
paper, we extend that result by studying the complexity of finding delta-models
for classes of Boolean formulas which are known to have polynomial time
satisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, Affine-SAT,
dual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the
complexity of finding delta-models, e.g., while 2-SAT and Affine-SAT have
polynomial time tests for delta-models, testing whether a Horn-SAT formula has
one is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6361</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6361</id><created>2011-09-28</created><authors><author><keyname>Chai</keyname><forenames>J. Y.</forenames></author><author><keyname>Prasov</keyname><forenames>Z.</forenames></author><author><keyname>Qu</keyname><forenames>S.</forenames></author></authors><title>Cognitive Principles in Robust Multimodal Interpretation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  55-83, 2006</journal-ref><doi>10.1613/jair.1936</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodal conversational interfaces provide a natural means for users to
communicate with computer systems through multiple modalities such as speech
and gesture. To build effective multimodal interfaces, automated interpretation
of user multimodal inputs is important. Inspired by the previous investigation
on cognitive status in multimodal human machine interaction, we have developed
a greedy algorithm for interpreting user referring expressions (i.e.,
multimodal reference resolution). This algorithm incorporates the cognitive
principles of Conversational Implicature and Givenness Hierarchy and applies
constraints from various sources (e.g., temporal, semantic, and contextual) to
resolve references. Our empirical results have shown the advantage of this
algorithm in efficiently resolving a variety of user references. Because of its
simplicity and generality, this approach has the potential to improve the
robustness of multimodal input interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6369</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6369</id><created>2011-09-27</created><authors><author><keyname>Anderson</keyname><forenames>Dana</forenames></author></authors><title>How to prolong network life-span in wireless networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important problems in wireless sensor network is to develop a
routing protocol that has energy efficiency. Since the power of the sensor
Nodes are limited, conserving energy and network life is a critical issue in
wireless sensor network. Clustering is one of the known methods widely used to
face these challenges. In this paper, a cluster based communication protocol
with considering the low energy consumption in wireless sensor networks, is
introduced which balances the energy load among sensor nodes. The nodes close
to each other have more overlap; they sense the same data from environment and
cause a waste of energy by generating repetitive data. In this paper, a cluster
based routing protocol is introduced, in the proposed protocol, in each round a
certain number of nodes are specified; the nodes which have at least one
neighboring node at a distance less than the threshold. Then, among them the
nodes with less energy and greater overlap with their neighbors have been
chosen to go to sleep mode, Also, the energy imbalance among sensor nodes is
reduced by integrating the distance of the nodes from the base station into
clustering policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6371</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6371</id><created>2011-09-28</created><authors><author><keyname>Adhikary</keyname><forenames>Ansuman</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos C.</forenames></author><author><keyname>Ramprashad</keyname><forenames>Sean A.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Multi-User MIMO with outdated CSI: Training, Feedback and Scheduling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional MU-MIMO techniques, e.g. Linear Zero-Forced Beamforming (LZFB),
require sufficiently accurate channel state information at the transmitter
(CSIT) in order to realize spectral efficient transmission (degree of freedom
gains). In practical settings, however, CSIT accuracy can be limited by a
number of issues including CSI estimation, CSI feedback delay between user
terminals to base stations, and the time/frequency coherence of the channel.
The latter aspects of CSIT-feedback delay and channel-dynamics can lead to
significant challenges in the deployment of efficient MU-MIMO systems. Recently
it has been shown by Maddah-Ali and Tse (MAT) that degree of freedom gains can
be realized by MU-MIMO even when the knowledge of CSIT is completely outdated.
Specifically, outdated CSIT, albeit perfect CSIT, is known for transmissions
only after they have taken place. This aspect of insensitivity to CSIT-feedback
delay is of particular interest since it allows one to reconsider MU-MIMO
design in dynamic channel conditions. Indeed, as we show, with appropriate
scheduling, and even in the context of CSI estimation and feedback errors, the
proposed MAT scheme can have performance advantages over conventional MU-MIMO
in such scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6390</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6390</id><created>2011-09-28</created><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Performance of Orthogonal Matching Pursuit for Multiple Measurement
  Vectors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider orthogonal matching pursuit (OMP) algorithm for
multiple measurement vectors (MMV) problem. The robustness of OMPMMV is studied
under general perturbations---when the measurement vectors as well as the
sensing matrix are incorporated with additive noise. The main result shows that
although exact recovery of the sparse solutions is unrealistic in noisy
scenario, recovery of the support set of the solutions is guaranteed under
suitable conditions. Specifically, a sufficient condition is derived that
guarantees exact recovery of the sparse solutions in noiseless scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6391</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6391</id><created>2011-09-28</created><authors><author><keyname>Dominguez-Garcia</keyname><forenames>Alejandro D.</forenames></author><author><keyname>Hadjicostis</keyname><forenames>Christoforos N.</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author></authors><title>Distributed Algorithms for Consensus and Coordination in the Presence of
  Packet-Dropping Communication Links - Part I: Statistical Moments Analysis
  Approach</title><categories>cs.SY math.OC</categories><comments>University of Illinois at Urbana-Champaign. Coordinated Sciences
  Laboratory technical report</comments><report-no>UILU-ENG-11-2207 (CRHC-11-05)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This two-part paper discusses robustification methodologies for
linear-iterative distributed algorithms for consensus and coordination problems
in multicomponent systems, in which unreliable communication links may drop
packets. We consider a setup where communication links between components can
be asymmetric (i.e., component j might be able to send information to component
i, but not necessarily vice-versa), so that the information exchange between
components in the system is in general described by a directed graph that is
assumed to be strongly connected. In the absence of communication link
failures, each component i maintains two auxiliary variables and updates each
of their values to be a linear combination of their corresponding previous
values and the corresponding previous values of neighboring components (i.e.,
components that send information to node i). By appropriately initializing
these two (decoupled) iterations, the system components can asymptotically
calculate variables of interest in a distributed fashion; in particular, the
average of the initial conditions can be calculated as a function that involves
the ratio of these two auxiliary variables. The focus of this paper to
robustify this double-iteration algorithm against communication link failures.
We achieve this by modifying the double-iteration algorithm (by introducing
some additional auxiliary variables) and prove that the modified
double-iteration converges almost surely to average consensus. In the first
part of the paper, we study the first and second moments of the two iterations,
and use them to establish convergence, and illustrate the performance of the
algorithm with several numerical examples. In the second part, in order to
establish the convergence of the algorithm, we use coefficients of ergodicity
commonly used in analyzing inhomogeneous Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6392</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6392</id><created>2011-09-28</created><authors><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author><author><keyname>Hadjicostis</keyname><forenames>Christoforos N.</forenames></author><author><keyname>Dominguez-Garcia</keyname><forenames>Alejandro D.</forenames></author></authors><title>Distributed Algorithms for Consensus and Coordination in the Presence of
  Packet-Dropping Communication Links - Part II: Coefficients of Ergodicity
  Analysis Approach</title><categories>cs.SY math.OC</categories><comments>University of Illinois at Urbana-Champaign. Coordinated Sciences
  Laboratory technical report</comments><report-no>UILU-ENG-11-2208 (CRHC-11-06)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this two-part paper, we consider multicomponent systems in which each
component can iteratively exchange information with other components in its
neighborhood in order to compute, in a distributed fashion, the average of the
components' initial values or some other quantity of interest (i.e., some
function of these initial values). In particular, we study an iterative
algorithm for computing the average of the initial values of the nodes. In this
algorithm, each component maintains two sets of variables that are updated via
two identical linear iterations. The average of the initial values of the nodes
can be asymptotically computed by each node as the ratio of two of the
variables it maintains. In the first part of this paper, we show how the update
rules for the two sets of variables can be enhanced so that the algorithm
becomes tolerant to communication links that may drop packets, independently
among them and independently between different transmission times. In this
second part, by rewriting the collective dynamics of both iterations, we show
that the resulting system is mathematically equivalent to a finite inhomogenous
Markov chain whose transition matrix takes one of finitely many values at each
step. Then, by using e a coefficients of ergodicity approach, a method commonly
used for convergence analysis of Markov chains, we prove convergence of the
robustified consensus scheme. The analysis suggests that similar convergence
should hold under more general conditions as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6401</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6401</id><created>2011-09-29</created><authors><author><keyname>Dambreville</keyname><forenames>Frederic</forenames><affiliation>ENSIETA</affiliation></author></authors><title>An Interpretation of Belief Functions by means of a Probabilistic
  Multi-modal Logic</title><categories>cs.LO cs.AI math.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While belief functions may be seen formally as a generalization of
probabilistic distributions, the question of the interactions between belief
functions and probability is still an issue in practice. This question is
difficult, since the contexts of use of these theory are notably different and
the semantics behind these theories are not exactly the same. A prominent issue
is increasingly regarded by the community, that is the management of the
conflicting information. Recent works have introduced new rules for handling
the conflict redistribution while combining belief functions. The notion of
conflict, or its cancellation by an hypothesis of open world, seems by itself
to prevent a direct interpretation of belief function in a probabilistic
framework. This paper addresses the question of a probabilistic interpretation
of belief functions. It first introduces and implements a theoretically
grounded rule, which is in essence an adaptive conjunctive rule. It is shown,
how this rule is derived from a logical interpretation of the belief functions
by means of a probabilistic multimodal logic; in addition, a concept of source
independence is introduced, based on a principle of entropy maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6402</identifier>
 <datestamp>2011-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6402</id><created>2011-09-29</created><updated>2011-12-15</updated><authors><author><keyname>Dambreville</keyname><forenames>Frederic</forenames><affiliation>ENSIETA</affiliation></author></authors><title>Extension of Boolean algebra by a Bayesian operator; application to the
  definition of a Deterministic Bayesian Logic</title><categories>math.LO cs.AI cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work contributes to the domains of Boolean algebra and of Bayesian
probability, by proposing an algebraic extension of Boolean algebras, which
implements an operator for the Bayesian conditional inference and is closed
under this operator. It is known since the work of Lewis (Lewis' triviality)
that it is not possible to construct such conditional operator within the space
of events. Nevertheless, this work proposes an answer which complements Lewis'
triviality, by the construction of a conditional operator outside the space of
events, thus resulting in an algebraic extension. In particular, it is proved
that any probability defined on a Boolean algebra may be extended to its
algebraic extension in compliance with the multiplicative definition of the
conditional probability. In the last part of this paper, a new bivalent logic
is introduced on the basis of this algebraic extension, and basic properties
are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6416</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6416</id><created>2011-09-29</created><updated>2012-07-05</updated><authors><author><keyname>Mahalanobis</keyname><forenames>Ayan</forenames></author></authors><title>The ElGamal cryptosystem over circulant matrices</title><categories>cs.CR math.GR math.RA</categories><msc-class>94A60, 20G40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study extensively the discrete logarithm problem in the
group of non-singular circulant matrices. The emphasis of this study was to
find the exact parameters for the group of circulant matrices for a secure
implementation. We tabulate these parameters. We also compare the discrete
logarithm problem in the group of circulant matrices with the discrete
logarithm problem in finite fields and with the discrete logarithm problem in
the group of rational points of an elliptic curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6437</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6437</id><created>2011-09-29</created><updated>2013-01-09</updated><authors><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>An Error Probability Approach to MIMO Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>27 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider MIMO (Multiple Input Multiple Output) wiretap channels, where a
legitimate transmitter Alice is communicating with a legitimate receiver Bob in
the presence of an eavesdropper Eve, and communication is done via MIMO
channels. We suppose that Alice's strategy is to use a codebook which has a
lattice structure, which then allows her to perform coset encoding. We analyze
Eve's probability of correctly decoding the message Alice meant to Bob, and
from minimizing this probability, we derive a code design criterion for MIMO
lattice wiretap codes. The case of block fading channels is treated similarly,
and fast fading channels are derived as a particular case. The Alamouti code is
carefully studied as an illustration of the analysis provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6440</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6440</id><created>2011-09-29</created><updated>2015-04-13</updated><authors><author><keyname>Lad</keyname><forenames>Frank</forenames></author><author><keyname>Sanfilippo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Agr&#xf2;</keyname><forenames>Gianna</forenames></author></authors><title>Extropy: Complementary Dual of Entropy</title><categories>cs.IT math.IT math.PR math.ST physics.data-an stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/14-STS430 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS430</report-no><journal-ref>Statistical Science 2015, Vol. 30, No. 1, 40-58</journal-ref><doi>10.1214/14-STS430</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a completion to theories of information based on
entropy, resolving a longstanding question in its axiomatization as proposed by
Shannon and pursued by Jaynes. We show that Shannon's entropy function has a
complementary dual function which we call &quot;extropy.&quot; The entropy and the
extropy of a binary distribution are identical. However, the measure bifurcates
into a pair of distinct measures for any quantity that is not merely an event
indicator. As with entropy, the maximum extropy distribution is also the
uniform distribution, and both measures are invariant with respect to
permutations of their mass functions. However, they behave quite differently in
their assessments of the refinement of a distribution, the axiom which
concerned Shannon and Jaynes. Their duality is specified via the relationship
among the entropies and extropies of course and fine partitions. We also
analyze the extropy function for densities, showing that relative extropy
constitutes a dual to the Kullback-Leibler divergence, widely recognized as the
continuous entropy measure. These results are unified within the general
structure of Bregman divergences. In this context they identify half the $L_2$
metric as the extropic dual to the entropic directed distance. We describe a
statistical application to the scoring of sequential forecast distributions
which provoked the discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6441</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6441</id><created>2011-09-29</created><authors><author><keyname>Sudholt</keyname><forenames>Dirk</forenames></author></authors><title>Memetic Algorithms: Parametrization and Balancing Local and Global
  Search</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a preprint of a book chapter from the Handbook of Memetic Algorithms,
Studies in Computational Intelligence, Vol. 379, ISBN 978-3-642-23246-6,
Springer, edited by F. Neri, C. Cotta, and P. Moscato. It is devoted to the
parametrization of memetic algorithms and how to find a good balance between
global and local search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6442</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6442</id><created>2011-09-29</created><updated>2011-10-21</updated><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Raheja</keyname><forenames>Jagdish L.</forenames></author></authors><title>ABHIVYAKTI: A Vision Based Intelligent System for Elder and Sick Persons</title><categories>cs.CV</categories><comments>Proceedings of 3rd IEEE International Conference on Machine Vision,
  Hong Kong, ICMV 2010
  (http://www.ieee.org/conferences_events/conferences/conferencedetails/index.html?Conf_ID=18047),
  28-30 Dec, 2010, pp. 361-364</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an intelligent system ABHIVYAKTI, which would be
pervasive in nature and based on the Computer Vision. It would be very easy in
use and deployment. Elder and sick people who are not able to talk or walk,
they are dependent on other human beings and need continuous monitoring, while
our system provides flexibility to the sick or elder person to announce his or
her need to their caretaker by just showing a particular gesture with the
developed system, if the caretaker is not nearby. This system will use
fingertip detection techniques for acquiring gesture and Artificial Neural
Networks (ANNs) will be used for gesture recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6460</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6460</id><created>2011-09-29</created><authors><author><keyname>Neyezhmakov</keyname><forenames>P. I.</forenames></author><author><keyname>Zub</keyname><forenames>S. I.</forenames></author><author><keyname>Zub</keyname><forenames>S. S.</forenames></author></authors><title>The Grid: Prospects for Application in Metrology</title><categories>physics.ins-det cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global system of distributing computing - Grid - created as reply for
challenges, connected with the qualitative progress of complexity of
experimental physical assemblies and information systems, is presented as
optimal IT platform for assurance of measurement traceability in geographically
remote regions and measurement data protection in global networks. The new
component grid - Instrument Element (IE) - is intended for secure, remote,
joint team work on monitoring and managing instruments generated and stored on
distributed scientific equipment using conventional grid resources. The article
describes the variety of all possible IE applications within grid technology
for the tasks of metrology demanding IT support. Expanded by the new component
IE grid becomes an optimal environment for effective monitoring, management and
servicing of measuring resources which has the highest level of measurement
data transfer, storage and processing safety and reveals new opportunities to
track measurement procedures and assure a high level of confidence to these
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6494</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6494</id><created>2011-09-29</created><authors><author><keyname>Darles</keyname><forenames>Emmanuelle</forenames><affiliation>XLIM</affiliation></author><author><keyname>Crespin</keyname><forenames>Beno&#xee;t</forenames><affiliation>XLIM</affiliation></author><author><keyname>Ghazanfarpour</keyname><forenames>Djamchid</forenames><affiliation>XLIM</affiliation></author><author><keyname>Gonzato</keyname><forenames>Jean-Christophe</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>A Survey of Ocean Simulation and Rendering Techniques in Computer
  Graphics</title><categories>cs.GR</categories><proxy>ccsd</proxy><journal-ref>Computer Graphics Forum 30, 1 (2011) 43-60</journal-ref><doi>10.1111/j.1467-8659.2010.01828.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a survey of ocean simulation and rendering methods in
computer graphics. To model and animate the ocean's surface, these methods
mainly rely on two main approaches: on the one hand, those which approximate
ocean dynamics with parametric, spectral or hybrid models and use empirical
laws from oceanographic research. We will see that this type of methods
essentially allows the simulation of ocean scenes in the deep water domain,
without breaking waves. On the other hand, physically-based methods use
Navier-Stokes Equations (NSE) to represent breaking waves and more generally
ocean surface near the shore. We also describe ocean rendering methods in
computer graphics, with a special interest in the simulation of phenomena such
as foam and spray, and light's interaction with the ocean surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6502</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6502</id><created>2011-09-29</created><authors><author><keyname>Morshed</keyname><forenames>Md. Monzur</forenames></author><author><keyname>Rahman</keyname><forenames>Meftah Ur</forenames></author><author><keyname>Islam</keyname><forenames>Md. Rafiqul</forenames></author></authors><title>An Empirical Study of UDP (CBR) Packet Performance over AODV Single &amp;
  Multi-Channel Parallel Transmission in MANET</title><categories>cs.NI cs.PF</categories><comments>(5 pages, 11 figures, 5 tables) This is a joint research
  collaboration of AIUB &amp; TigerHATS Research Team. http://www.aiub.edu,
  http://www.tigerhats.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad-hoc Network is a temporary network which is the cooperative
engagement of a collection of standalone mobile nodes that are not connected to
any external network. It is a decentralized network where mobile nodes can be
easily deployed in almost any environment without sophisticated infrastructure
support. An empirical study has been done for AODV routing protocol under
single channel and multi channel environment using the tool NS2. To compare the
performance of AODV in the two environments, the simulation results have been
analyzed by graphical manner and trace file based on QoS metrics such as
throughput, packet drop, delay and jitter. The simulation result analysis
verifies the AODV routing protocol performances for single channel and multi
channel. After the analysis of the simulation scenario we suggest that use of
Parallel MAC (P-MAC) may enhance the performance for multi channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6505</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6505</id><created>2011-09-29</created><authors><author><keyname>ParandehGheibi</keyname><forenames>Ali</forenames></author><author><keyname>Roozbehani</keyname><forenames>Mardavij</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A</forenames></author></authors><title>The Reliability Value of Storage in a Volatile Environment</title><categories>math.OC cs.SY</categories><comments>submitted to ACC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the value of storage in securing reliability of a system
with uncertain supply and demand, and supply friction. The storage is
frictionless as a supply source, but once used, it cannot be filled up
instantaneously. The focus application is a power supply network in which the
base supply and demand are assumed to match perfectly, while deviations from
the base are modeled as random shocks with stochastic arrivals. Due to
friction, the random surge shocks cannot be tracked by the main supply sources.
Storage, when available, can be used to compensate, fully or partially, for the
surge in demand or loss of supply. The problem of optimal utilization of
storage with the objective of maximizing system reliability is formulated as
minimization of the expected discounted cost of blackouts over an infinite
horizon. It is shown that when the stage cost is linear in the size of the
blackout, the optimal policy is myopic in the sense that all shocks are
compensated by storage up to the available level of storage. However, when the
stage cost is strictly convex, it may be optimal to curtail some of the demand
and allow a small current blackout in the interest of maintaining a higher
level of reserve to avoid a large blackout in the future. The value of storage
capacity in improving system's reliability, as well as the effects of the
associated optimal policies under different stage costs on the probability
distribution of blackouts are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6510</identifier>
 <datestamp>2012-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6510</id><created>2011-09-29</created><updated>2012-07-09</updated><authors><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Exact Performance Analysis of Partial Relay Selection Based on Shadowing
  Side Information over Generalized Composite Fading Channels</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>Number of Figures: 5, Number of Tables: 1, Keywords: Partial relay
  selection, unified performance expression, average bit error probability,
  ergodic capacity, moments-generating function, shadowing side information,
  and extended generalized-K fading</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relay technology has recently gained great interest in millimeter wave (60
GHz or above) radio frequencies as a promising transmission technique improving
the quality of service, providing high data rate, and extending the coverage
area without additional transmit power in deeply shadowed fading environments.
The performance of relay-based systems considerably depends on which relay
selection protocols (RSPs) are used. These RSPs are typically using the channel
side information (CSI). Specifically, the relay terminal (RT) is chosen among
all available RTs by a central entity (CE) which receives all RTs' CSI via
feedback channels. However, in the millimeter wave radio frequencies, the rate
of the CSI variation is much higher than that of the CSI variation in 6 GHz
frequencies under the same mobility conditions, which evidently results in a
serious problem causing that the CSI at the CE is inaccurate for the RSP since
the feedback channels have a backhaul / transmission delay. However and
fortunately, the shadowing side information (SSI) varies very slowly in
comparison to the rate of the CSI variation. In this context, we propose in
this paper a partial-RSP in dual-hop amplify-and-forward relaying system, which
utilize only the SSI of the RTs instead of their CSI. Then for the performance
analysis, we obtain an exact average unified performance (AUP) of the proposed
SSI-based partial-RSP for a variety shadowed fading environments. In
particular, we offer a generic AUP expression whose special cases include the
average bit error probability (ABEP) analysis for binary modulation schemes,
the ergodic capacity analysis and the moments-generating function (MGF)-based
characterization. The correctness of our newly theoretical results is validated
with some selected numerical examples in an extended generalized-K fading
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6531</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6531</id><created>2011-09-29</created><authors><author><keyname>Corbellini</keyname><forenames>Giorgio</forenames></author><author><keyname>Abgrall</keyname><forenames>Cedric</forenames></author><author><keyname>Strinati</keyname><forenames>Emilio Calvanese</forenames></author><author><keyname>Duda</keyname><forenames>Andrzej</forenames></author></authors><title>Technical Report: Energy Evaluation of preamble Sampling MAC Protocols
  for Wireless Sensor Networks</title><categories>stat.OT cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The paper presents a simple probabilistic analysis of the energy consumption
in preamble sampling MAC protocols. We validate the analytical results with
simulations. We compare the classical MAC protocols (B-MAC and X-MAC) with
LAMAC, a method proposed in a companion paper. Our analysis highlights the
energy savings achievable with LA-MAC with respect to B-MAC and X-MAC. It also
shows that LA-MAC provides the best performance in the considered case of high
density networks under traffic congestion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6534</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6534</id><created>2011-09-29</created><authors><author><keyname>Becker</keyname><forenames>Florent</forenames></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames></author><author><keyname>Nisse</keyname><forenames>Nicolas</forenames></author><author><keyname>Rapaport</keyname><forenames>Ivan</forenames></author><author><keyname>Suchan</keyname><forenames>Karol</forenames></author></authors><title>Interconnection network with a shared whiteboard: Impact of
  (a)synchronicity on computing power</title><categories>cs.DC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the computational power of graph-based models of
distributed computing in which each node additionally has access to a global
whiteboard. A node can read the contents of the whiteboard and, when activated,
can write one message of O(log n) bits on it. When the protocol terminates,
each node computes the output based on the final contents of the whiteboard. We
consider several scheduling schemes for nodes, providing a strict ordering of
their power in terms of the problems which can be solved with exactly one
activation per node. The problems used to separate the models are related to
Maximal Independent Set, detection of cycles of length 4, and BFS spanning tree
constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6535</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6535</id><created>2011-09-29</created><authors><author><keyname>Munch</keyname><forenames>Elizabeth</forenames></author><author><keyname>Shapiro</keyname><forenames>Michael</forenames></author><author><keyname>Harer</keyname><forenames>John</forenames></author></authors><title>Failure Filtrations for Fenced Sensor Networks</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the question of sensor network coverage for a
2-dimensional domain. We seek to compute the probability that a set of sensors
fails to cover given only non-metric, local (who is talking to whom)
information and a probability distribution of failure of each node. This builds
on the work of de Silva and Ghrist who analyzed this problem in the
deterministic situation. We first show that a it is part of a slightly larger
class of problems which is #P-complete, and thus fast algorithms likely do not
exist unless P$=$NP. We then give a deterministic algorithm which is feasible
in the case of a small set of sensors, and give a dynamic algorithm for an
arbitrary set of sensors failing over time which utilizes a new criterion for
coverage based on the one proposed by de Silva and Ghrist. These algorithms
build on the theory of topological persistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6536</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6536</id><created>2011-09-29</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Delen</keyname><forenames>G. P. A. J.</forenames></author><author><keyname>van Vlijmen</keyname><forenames>S. F. M.</forenames></author></authors><title>Outsourcing Competence</title><categories>cs.OH</categories><acm-class>K.6.0; J.4; H.4.0; D.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topic of this paper, competences needed for outsourcing, is organized by
first providing a generic competence scheme, which is subsequently instantiated
to the area of sourcing and outsourcing. Sourcing and outsourcing are
positioned as different areas of activity, neither one of which is subsumed
under the other one. It is argued that competences relevant for outsourcing are
mainly community based rather than evidence based. Subjective ability and
objective ability are distinguished as categories, together making up ability,
which are distinct but not necessarily disjoint from competence. Conjectural
ability is introduced as a form of subjective ability. A person's competence
profile includes competences as well as abilities, including subjective ones.
Competence assessment and acquisition as well as the impact of assessed
competence on practical work is described. The analysis of competence and
ability thus developed is used as standpoint from which to extract a
specification of an audience for a theory of outsourcing, yet to be written.
Moreover, it allows to formulate requirements for and in preparation of the
development of an outsourcing theory. Formulating these requirements is done
under the assumption that a person's awareness of a theory of outsourcing is
expected to strengthen that person's outsourcing competence profile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6541</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6541</id><created>2011-09-29</created><updated>2013-03-08</updated><authors><author><keyname>Lee</keyname><forenames>Jung Hoon</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author></authors><title>On the Achievable DoF and User Scaling Law of Opportunistic Interference
  Alignment in 3-Transmitter MIMO Interference Channels</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2013.041713.120773</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose opportunistic interference alignment (OIA) schemes
for three-transmitter multiple-input multiple-output (MIMO) interference
channels (ICs). In the proposed OIA, each transmitter has its own user group
and selects a single user who has the most aligned interference signals. The
user dimensions provided by multiple users are exploited to align interfering
signals. Contrary to conventional IA, perfect channel state information of all
channel links is not required at the transmitter, and each user just feeds back
one scalar value to indicate how well the interfering channels are aligned. We
prove that each transmitter can achieve the same degrees of freedom (DoF) as
the interference free case via user selection in our system model that the
number of receive antennas is twice of the number of transmit antennas. Using
the geometric interpretation, we find the required user scaling to obtain an
arbitrary non-zero DoF. Two OIA schemes are proposed and compared with various
user selection schemes in terms of achievable rate/DoF and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6550</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6550</id><created>2011-09-29</created><authors><author><keyname>Mishra</keyname><forenames>Sumita</forenames></author><author><keyname>Chaudhary</keyname><forenames>Naresh K.</forenames></author><author><keyname>Singh</keyname><forenames>Kalyan</forenames></author></authors><title>Simulation and Optimization of MQW based optical modulator for on chip
  optical interconnect</title><categories>cs.ET physics.ins-det</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 8, Issue
  3, May 2011 ISSN (Online): 1694-0814 http://www.IJCSI.org</comments><journal-ref>International Journal of Computer Science Issues, Vol. 8, Issue 3,
  May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical interconnects are foreseen as a potential solution to improve the
performance of data transmission in high speed integrated circuits since
electrical interconnects operating at high bit rates have several limitations
which creates a bottleneck at the interconnect level. The objective of the work
is to model and then simulate the MQWM based optical interconnect transmitter.
The power output of the simulated modulator is then optimized with respect to
various parameters namely contrast ratio, insertion loss and bias current. The
methodology presented here is suitable for investigation of both analog and
digital modulation performance but it primarily deals with digital modulation.
We have not included the effect of carrier charge density in multiple quantum
well simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6618</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6618</id><created>2011-09-29</created><authors><author><keyname>Davidov</keyname><forenames>D.</forenames></author><author><keyname>Markovitch</keyname><forenames>S.</forenames></author></authors><title>Multiple-Goal Heuristic Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  417-451, 2006</journal-ref><doi>10.1613/jair.1940</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new framework for anytime heuristic search where the
task is to achieve as many goals as possible within the allocated resources. We
show the inadequacy of traditional distance-estimation heuristics for tasks of
this type and present alternative heuristics that are more appropriate for
multiple-goal search. In particular, we introduce the marginal-utility
heuristic, which estimates the cost and the benefit of exploring a subtree
below a search node. We developed two methods for online learning of the
marginal-utility heuristic. One is based on local similarity of the partial
marginal utility of sibling nodes, and the other generalizes marginal-utility
over the state feature space. We apply our adaptive and non-adaptive
multiple-goal search algorithms to several problems, including focused
crawling, and show their superiority over existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6619</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6619</id><created>2011-09-29</created><authors><author><keyname>Georgakopoulos</keyname><forenames>Agelos</forenames></author><author><keyname>Winkler</keyname><forenames>Peter</forenames></author></authors><title>New Bounds for Edge-Cover by Random Walk</title><categories>math.CO cs.DM</categories><msc-class>05C81</msc-class><doi>10.1017/S096354831400008X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the expected time for a random walk on a (multi-)graph $G$ to
traverse all $m$ edges of $G$, and return to its starting point, is at most
$2m^2$; if each edge must be traversed in both directions, the bound is $3m^2$.
Both bounds are tight and may be applied to graphs with arbitrary edge lengths,
with implications for Brownian motion on a finite or infinite network of total
edge-length $m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6621</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6621</id><created>2011-09-29</created><authors><author><keyname>Hoelldobler</keyname><forenames>S.</forenames></author><author><keyname>Karabaev</keyname><forenames>E.</forenames></author><author><keyname>Skvortsova</keyname><forenames>O.</forenames></author></authors><title>FluCaP: A Heuristic Search Planner for First-Order MDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  419-439, 2006</journal-ref><doi>10.1613/jair.1965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a heuristic search algorithm for solving first-order Markov
Decision Processes (FOMDPs). Our approach combines first-order state
abstraction that avoids evaluating states individually, and heuristic search
that avoids evaluating all states. Firstly, in contrast to existing systems,
which start with propositionalizing the FOMDP and then perform state
abstraction on its propositionalized version we apply state abstraction
directly on the FOMDP avoiding propositionalization. This kind of abstraction
is referred to as first-order state abstraction. Secondly, guided by an
admissible heuristic, the search is restricted to those states that are
reachable from the initial state. We demonstrate the usefulness of the above
techniques for solving FOMDPs with a system, referred to as FluCaP (formerly,
FCPlanner), that entered the probabilistic track of the 2004 International
Planning Competition (IPC2004) and demonstrated an advantage over other
planners on the problems represented in first-order terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6626</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6626</id><created>2011-09-28</created><authors><author><keyname>Samwald</keyname><forenames>Matthias</forenames></author><author><keyname>Stenzhorn</keyname><forenames>Holger</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author><author><keyname>Marshall</keyname><forenames>M. Scott</forenames></author><author><keyname>Luciano</keyname><forenames>Joanne</forenames></author><author><keyname>Adlassnig</keyname><forenames>Klaus-Peter</forenames></author></authors><title>Towards an interoperable information infrastructure providing decision
  support for genomic medicine</title><categories>cs.OH</categories><journal-ref>User Centred Networked Health Care - Proceedings of MIE 2011</journal-ref><doi>10.3233/978-1-60750-806-9-165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic dispositions play a major role in individual disease risk and
treatment response. Genomic medicine, in which medical decisions are refined by
genetic information of particular patients, is becoming increasingly important.
Here we describe our work and future visions around the creation of a
distributed infrastructure for pharmacogenetic data and medical decision
support, based on industry standards such as the Web Ontology Language (OWL)
and the Arden Syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6638</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6638</id><created>2011-09-29</created><updated>2011-09-30</updated><authors><author><keyname>Bergstra</keyname><forenames>James</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>The Statistical Inefficiency of Sparse Coding for Images (or, One Gabor
  to Rule them All)</title><categories>cs.CV cs.AI</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding is a proven principle for learning compact representations of
images. However, sparse coding by itself often leads to very redundant
dictionaries. With images, this often takes the form of similar edge detectors
which are replicated many times at various positions, scales and orientations.
An immediate consequence of this observation is that the estimation of the
dictionary components is not statistically efficient. We propose a factored
model in which factors of variation (e.g. position, scale and orientation) are
untangled from the underlying Gabor-like filters. There is so much redundancy
in sparse codes for natural images that our model requires only a single
dictionary element (a Gabor-like edge detector) to outperform standard sparse
coding. Our model scales naturally to arbitrary-sized images while achieving
much greater statistical efficiency during learning. We validate this claim
with a number of experiments showing, in part, superior compression of
out-of-sample data using a sparse coding dictionary learned with only a single
image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6642</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6642</id><created>2011-09-29</created><updated>2012-08-22</updated><authors><author><keyname>Schaub</keyname><forenames>Michael T.</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>Encoding dynamics for multiscale community detection: Markov time
  sweeping for the Map equation</title><categories>physics.soc-ph cs.IT cs.SI math.IT</categories><comments>10 pages, 6 figures</comments><journal-ref>Phys. Rev. E, 2012, 86(2), p 026112</journal-ref><doi>10.1103/PhysRevE.86.026112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection of community structure in networks is intimately related to
finding a concise description of the network in terms of its modules. This
notion has been recently exploited by the Map equation formalism (M. Rosvall
and C.T. Bergstrom, PNAS, 105(4), pp.1118--1123, 2008) through an
information-theoretic description of the process of coding inter- and
intra-community transitions of a random walker in the network at stationarity.
However, a thorough study of the relationship between the full Markov dynamics
and the coding mechanism is still lacking. We show here that the original Map
coding scheme, which is both block-averaged and one-step, neglects the internal
structure of the communities and introduces an upper scale, the `field-of-view'
limit, in the communities it can detect. As a consequence, Map is well tuned to
detect clique-like communities but can lead to undesirable overpartitioning
when communities are far from clique-like. We show that a signature of this
behavior is a large compression gap: the Map description length is far from its
ideal limit. To address this issue, we propose a simple dynamic approach that
introduces time explicitly into the Map coding through the analysis of the
weighted adjacency matrix of the time-dependent multistep transition matrix of
the Markov process. The resulting Markov time sweeping induces a dynamical
zooming across scales that can reveal (potentially multiscale) community
structure above the field-of-view limit, with the relevant partitions indicated
by a small compression gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6643</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6643</id><created>2011-09-29</created><updated>2013-10-25</updated><authors><author><keyname>Bilardi</keyname><forenames>Gianfranco</forenames></author><author><keyname>Versaci</keyname><forenames>Francesco</forenames></author></authors><title>Optimal Eviction Policies for Stochastic Address Traces</title><categories>cs.DS</categories><comments>37 pages, 3 figures</comments><journal-ref>Theoretical Computer Science, Volume 514, 25 November 2013, Pages
  36-60</journal-ref><doi>10.1016/j.tcs.2013.01.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The eviction problem for memory hierarchies is studied for the Hidden Markov
Reference Model (HMRM) of the memory trace, showing how miss minimization can
be naturally formulated in the optimal control setting. In addition to the
traditional version assuming a buffer of fixed capacity, a relaxed version is
also considered, in which buffer occupancy can vary and its average is
constrained. Resorting to multiobjective optimization, viewing occupancy as a
cost rather than as a constraint, the optimal eviction policy is obtained by
composing solutions for the individual addressable items.
  This approach is then specialized to the Least Recently Used Stack Model
(LRUSM), a type of HMRM often considered for traces, which includes V-1
parameters, where V is the size of the virtual space. A gain optimal policy for
any target average occupancy is obtained which (i) is computable in time O(V)
from the model parameters, (ii) is optimal also for the fixed capacity case,
and (iii) is characterized in terms of priorities, with the name of Least
Profit Rate (LPR) policy. An O(log C) upper bound (being C the buffer capacity)
is derived for the ratio between the expected miss rate of LPR and that of OPT,
the optimal off-line policy; the upper bound is tightened to O(1), under
reasonable constraints on the LRUSM parameters. Using the stack-distance
framework, an algorithm is developed to compute the number of misses incurred
by LPR on a given input trace, simultaneously for all buffer capacities, in
time O(log V) per access.
  Finally, some results are provided for miss minimization over a finite
horizon and over an infinite horizon under bias optimality, a criterion more
stringent than gain optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6646</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6646</id><created>2011-09-21</created><authors><author><keyname>Kiani</keyname><forenames>Abbas</forenames></author><author><keyname>Akhlaghi</keyname><forenames>Soroush</forenames></author></authors><title>A Non-MDS Erasure Code Scheme For Storage Applications</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the use of redundancy and self repairing against node
failures in distributed storage systems, using various strategies. In
replication method, access to one replication node is sufficient to reconstruct
a lost node, while in MDS erasure coded systems which are optimal in terms of
redundancy-reliability tradeoff, a single node failure is repaired after
recovering the entire stored data. Moreover, regenerating codes yield a
tradeoff curve between storage capacity and repair bandwidth. The current paper
aims at investigating a new storage code. Specifically, we propose a non-MDS
(2k, k) code that tolerates any three node failures and more importantly, it is
shown using our code a single node failure can be repaired through access to
only three nodes.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="24000" completeListSize="102538">1122234|25001</resumptionToken>
</ListRecords>
</OAI-PMH>
