<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:02:59Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|59001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1647</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1647</id><created>2014-04-06</created><authors><author><keyname>Li</keyname><forenames>Hu</forenames></author><author><keyname>Liu</keyname><forenames>Yuan`an</forenames></author><author><keyname>Hu</keyname><forenames>Hefei</forenames></author><author><keyname>Yuan</keyname><forenames>Dongming</forenames></author><author><keyname>Duan</keyname><forenames>Sirui</forenames></author></authors><title>Analysis of Capacity Region of Delay-Tolerant Hybrid Mobile Ad Hoc
  Networks</title><categories>cs.NI</categories><comments>6 pages, in Chinese, 5 figures</comments><journal-ref>Beijing Youdian Daxue Xuebao/Journal of Beijing University of
  Posts and Telecommunications, v 36, n 4, p 1-6, August 2013</journal-ref><doi>10.13190/jbupt.201304.1.lih</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network capacity region is an important character of mobile ad hoc networks.
Using cell-partitioned model, an expression of upper bound of delay-tolerant
hybrid mobile ad hoc network is deduced regardless of coverage of base
stations, types of mobile process, scheduling and routing algorithms. The
limitation of the upper bound is derived, Analysis of the limitation of upper
bound is carried out when the steady-state follows even-distribution law. The
relationship among limitation of capacity, the node density and coverage of
base station is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1653</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1653</id><created>2014-04-07</created><updated>2015-05-18</updated><authors><author><keyname>Yu</keyname><forenames>Lu</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author></authors><title>Multi-Linear Interactive Matrix Factorization</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems, which can significantly help users find their interested
items from the information era, has attracted an increasing attention from both
the scientific and application society. One of the widest applied
recommendation methods is the Matrix Factorization (MF). However, most of MF
based approaches focus on the user-item rating matrix, but ignoring the
ingredients which may have significant influence on users' preferences on
items. In this paper, we propose a multi-linear interactive MF algorithm
(MLIMF) to model the interactions between the users and each event associated
with their final decisions. Our model considers not only the user-item rating
information but also the pairwise interactions based on some empirically
supported factors. In addition, we compared the proposed model with three
typical other methods: user-based collaborative filtering (UCF), item-based
collaborative filtering (ICF) and regularized MF (RMF). Experimental results on
two real-world datasets, \emph{MovieLens} 1M and \emph{MovieLens} 100k, show
that our method performs much better than other three methods in the accuracy
of recommendation. This work may shed some light on the in-depth understanding
of modeling user online behaviors and the consequent decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1654</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1654</id><created>2014-04-07</created><updated>2014-12-08</updated><authors><author><keyname>Yue</keyname><forenames>Dian-Wu</forenames></author><author><keyname>Li</keyname><forenames>Geoffrey Ye</forenames></author></authors><title>LOS-based Conjugate Beamforming and Power-Scaling Law in Massive-MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>32 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with massive-MIMO systems over Rician flat fading
channels. In order to reduce the overhead to obtain full channel state
information and to avoid the pilot contamination problem, by treating the
scattered component as interference, we investigate a transmit and receive
conjugate beamforming (BF) transmission scheme only based on the line-of-sight
(LOS) component. Under Rank-1 model, we first consider a single-user system
with N transmit and M receive antennas, and focus on the problem of
power-scaling law when the transmit power is scaled down proportionally to
1/MN. It can be shown that as MN grows large, the scattered interference
vanishes, and the ergodic achievable rate is higher than that of the
corresponding BF scheme based fast fading and minimum mean-square error (MMSE)
channel estimation. Then we further consider uplink and downlink single-cell
scenarios where the base station (BS) has M antennas and each of K users has N
antennas. When the transmit power for each user is scaled down proportionally
to 1/MN, it can be shown for finite users that as M grows without bound, each
user obtains finally the same rate performance as in the single-user case. Even
when N grows without bound, however, there still remains inter-user LOS
interference that can not be cancelled. Regarding infinite users, there exists
such a power scaling law that when K and the b-th power of M go to infinity
with a fixed and finite ratio for a given b in (0, 1), not only inter-user LOS
interference but also fast fading effect can be cancelled, while fast fading
effect can not be cancelled if b=1. Extension to multi-cells and
frequency-selective channels are also discussed shortly. Moreover, numerical
results indicate that spacial antenna correlation does not have serious
influence on the rate performance, and the BS antennas may be allowed to be
placed compactly when M is very large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1664</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1664</id><created>2014-04-07</created><authors><author><keyname>Mittal</keyname><forenames>Namita</forenames></author><author><keyname>Agarwal</keyname><forenames>Basant</forenames></author><author><keyname>Gupta</keyname><forenames>Ajay</forenames></author><author><keyname>Madhur</keyname><forenames>Hemant</forenames></author></authors><title>Icon Based Information Retrieval and Disease Identification in
  Agriculture</title><categories>cs.HC cs.CV cs.CY cs.IR</categories><comments>Iconic Interface, Image Processing, Pattern Recognition, Data Mining,
  Information Retrieval</comments><journal-ref>International Journal of Advanced Studies in Computer Science &amp;
  Engineering IJASCSE, Volume 3, Issue 3, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in the ICT industry in past few decades has enabled the
quick and easy access to the information available on the internet. But,
digital literacy is the pre-requisite for its use. The main purpose of this
paper is to provide an interface for digitally illiterate users, especially
farmers to efficiently and effectively retrieve information through Internet.
In addition, to enable the farmers to identify the disease in their crop, its
cause and symptoms using digital image processing and pattern recognition
instantly without waiting for an expert to visit the farms and identify the
disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1668</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1668</id><created>2014-04-07</created><authors><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Tesi</keyname><forenames>Pietro</forenames></author></authors><title>On Resilient Control of Nonlinear Systems under Denial-of-Service</title><categories>cs.SY math.OC</categories><comments>7 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze and design a control strategy for nonlinear systems under
Denial-of-Service attacks. Based on an ISS-Lyapunov function analysis, we
provide a characterization of the maximal percentage of time during which
feedback information can be lost without resulting in the instability of the
system. Motivated by the presence of a digital channel we consider event-based
controllers for which a minimal inter-sampling time is explicitly
characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1669</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1669</id><created>2014-04-07</created><authors><author><keyname>Adebayo</keyname><forenames>Olawale S.</forenames></author><author><keyname>Abdulhamid</keyname><forenames>Shafii M.</forenames></author><author><keyname>Fluck</keyname><forenames>Andrew</forenames></author></authors><title>The Prospects for e-Examinations in Nigeria and Australia</title><categories>cs.CY cs.HC</categories><comments>13 pages, 3 figures, 1 table</comments><journal-ref>International Journal of Advances in Management, Technology and
  Engineering Sciences, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares the e-Examination system in Nigeria with that of
Australia. We consider the experiences of working with commercial firms such as
Electronic Testing Company (eTC) and using open-source software. It is
important to foster good relationships with accreditation authorities (such as
University Authorities, West African Examination Council (WAEC), Joint
Admissions and Matriculation Board (JAMB) etc. and the Tasmanian Qualifications
Authority) to assist in the transition from paperbased assessment to post-paper
assessment. The paper also considers the relative convenience for students,
administrators and lecturer/assessors; and to gauges the reliability and
security of the two systems in use. It examines the challenges in conducting
e-Examinations in both countries by juxtaposing the systems in the two
countries and suggests ways of developing more acceptable e-Examination
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1674</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1674</id><created>2014-04-07</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Channel Assignment With Access Contention Resolution for Cognitive Radio
  Networks</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>Le Thanh Tan; Long Bao Le, &quot;Channel Assignment With Access
  Contention Resolution for Cognitive Radio Networks,&quot; Vehicular Technology,
  IEEE Transactions on , vol.61, no.6, pp.2808,2823, July 2012 doi:
  10.1109/TVT.2012.2196532</journal-ref><doi>10.1109/TVT.2012.2196532</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the channel assignment problem for cognitive radio
networks with hardware-constrained secondary users (SUs). In particular, we
assume that SUs exploit spectrum holes on a set of channels where each SU can
use at most one available channel for communication. We present the optimal
brute-force search algorithm to solve the corresponding nonlinear integer
optimization problem and analyze its complexity. Because the optimal solution
has exponential complexity with the numbers of channels and SUs, we develop two
low-complexity channel assignment algorithms that can efficiently utilize the
spectrum holes. In the first algorithm, SUs are assigned distinct sets of
channels. We show that this algorithm achieves the maximum throughput limit if
the number of channels is sufficiently large. In addition, we propose an
overlapping channel assignment algorithm that can improve the throughput
performance compared with its nonoverlapping channel assignment counterpart.
Moreover, we design a distributed medium access control (MAC) protocol for
access contention resolution and integrate it into the overlapping channel
assignment algorithm. We then analyze the saturation throughput and the
complexity of the proposed channel assignment algorithms. We also present
several potential extensions, including the development of greedy channel
assignment algorithms under the max-min fairness criterion and throughput
analysis, considering sensing errors. Finally, numerical results are presented
to validate the developed theoretical results and illustrate the performance
gains due to the proposed channel assignment algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1675</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1675</id><created>2014-04-07</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Distributed MAC Protocol for Cognitive Radio Networks: Design,
  Analysis,and Optimization</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>Le Thanh Tan; Long Bao Le, &quot;Distributed MAC Protocol for Cognitive
  Radio Networks: Design, Analysis, and Optimization,&quot; Vehicular Technology,
  IEEE Transactions on , vol.60, no.8, pp.3990,4003, Oct. 2011</journal-ref><doi>10.1109/TVT.2011.2165325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the joint optimal sensing and distributed
Medium Access Control (MAC) protocol design problem for cognitive radio (CR)
networks. We consider both scenarios with single and multiple channels. For
each scenario, we design a synchronized MAC protocol for dynamic spectrum
sharing among multiple secondary users (SUs), which incorporates spectrum
sensing for protecting active primary users (PUs). We perform saturation
throughput analysis for the corresponding proposed MAC protocols that
explicitly capture the spectrum-sensing performance. Then, we find their
optimal configuration by formulating throughput maximization problems subject
to detection probability constraints for PUs. In particular, the optimal
solution of the optimization problem returns the required sensing time for PUs'
protection and optimal contention window to maximize the total throughput of
the secondary network. Finally, numerical results are presented to illustrate
developed theoretical findings in this paper and significant performance gains
of the optimal sensing and protocol configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1682</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1682</id><created>2014-04-07</created><updated>2014-08-06</updated><authors><author><keyname>Clemente</keyname><forenames>Carmine</forenames></author><author><keyname>Pallotta</keyname><forenames>Luca</forenames></author><author><keyname>Proudler</keyname><forenames>Ian</forenames></author><author><keyname>De Maio</keyname><forenames>Antonio</forenames></author><author><keyname>Soraghan</keyname><forenames>John J.</forenames></author><author><keyname>Farina</keyname><forenames>Alfonso</forenames></author></authors><title>Pseudo-Zernike Based Multi-Pass Automatic Target Recognition From
  Multi-Channel SAR</title><categories>cs.CV</categories><comments>The paper has been withdrawn due to conceptual errors in the
  performance analysis and to the fact that a substantial restructuring of the
  paper is required</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capability to exploit multiple sources of information is of fundamental
importance in a battlefield scenario. Information obtained from different
sources, and separated in space and time, provide the opportunity to exploit
diversities in order to mitigate uncertainty. For the specific challenge of
Automatic Target Recognition (ATR) from radar platforms, both channel (e.g.
polarization) and spatial diversity can provide useful information for such a
specific and critical task. In this paper the use of pseudo-Zernike moments
applied to multi-channel multi-pass data is presented exploiting diversities
and invariant properties leading to high confidence ATR, small computational
complexity and data transfer requirements. The effectiveness of the proposed
approach, in different configurations and data source availability is
demonstrated using real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1684</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1684</id><created>2014-04-07</created><updated>2014-09-29</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Gruska</keyname><forenames>Jozef</forenames></author><author><keyname>Zheng</keyname><forenames>Shenggen</forenames></author></authors><title>Exact quantum algorithms have advantage for almost all Boolean functions</title><categories>cs.CC quant-ph</categories><comments>17 pages. Accepted to Quantum information &amp; Computation</comments><msc-class>81P68, 03D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been proved that almost all $n$-bit Boolean functions have exact
classical query complexity $n$. However, the situation seemed to be very
different when we deal with exact quantum query complexity. In this paper, we
prove that almost all $n$-bit Boolean functions can be computed by an exact
quantum algorithm with less than $n$ queries. More exactly, we prove that
${AND}_n$ is the only $n$-bit Boolean function, up to isomorphism, that
requires $n$ queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1685</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1685</id><created>2014-04-07</created><updated>2015-01-25</updated><authors><author><keyname>Governatori</keyname><forenames>Guido</forenames></author></authors><title>Thou Shalt is not You Will</title><categories>cs.AI cs.LO</categories><report-no>NICTA Technical Report 8026</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss some reasons why temporal logic might not be
suitable to model real life norms. To show this, we present a novel deontic
logic contrary-to-duty/derived permission paradox based on the interaction of
obligations, permissions and contrary-to-duty obligations. The paradox is
inspired by real life norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1689</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1689</id><created>2014-04-07</created><updated>2014-11-25</updated><authors><author><keyname>Gruska</keyname><forenames>Jozef</forenames></author><author><keyname>Qiu</keyname><forenames>Daowen</forenames></author><author><keyname>Zheng</keyname><forenames>Shenggen</forenames></author></authors><title>Potential of quantum finite automata with exact acceptance</title><categories>cs.FL cs.CC quant-ph</categories><comments>We have improved the presentation of the paper. Accepted to
  International Journal of Foundation of Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential of the exact quantum information processing is an interesting,
important and intriguing issue. For examples, it has been believed that quantum
tools can provide significant, that is larger than polynomial, advantages in
the case of exact quantum computation only, or mainly, for problems with very
special structures. We will show that this is not the case.
  In this paper the potential of quantum finite automata producing outcomes not
only with a (high) probability, but with certainty (so called exactly) is
explored in the context of their uses for solving promise problems and with
respect to the size of automata. It is shown that for solving particular
classes $\{A^n\}_{n=1}^{\infty}$ of promise problems, even those without some
very special structure, that succinctness of the exact quantum finite automata
under consideration, with respect to the number of (basis) states, can be very
small (and constant) though it grows proportional to $n$ in the case
deterministic finite automata (DFAs) of the same power are used. This is here
demonstrated also for the case that the component languages of the promise
problems solvable by DFAs are non-regular. The method used can be applied in
finding more exact quantum finite automata or quantum algorithms for other
promise problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1695</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1695</id><created>2014-04-07</created><authors><author><keyname>Pham</keyname><forenames>Congduc</forenames></author><author><keyname>Matsuno</keyname><forenames>Fumitoshi</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author></authors><title>Proceedings of Third Workshop on Robots and Sensors integration in
  future rescue INformation system (ROSIN 2013)</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the proceedings of the third workshop on Robots and Sensors
integration in future rescue INformation system (ROSIN 2013)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1716</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1716</id><created>2014-04-07</created><authors><author><keyname>Stanek</keyname><forenames>Martin</forenames></author></authors><title>Memory-only selection of dictionary PINs</title><categories>cs.CR</categories><comments>8 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We estimate the security of dictionary-based PINs (Personal Identification
Numbers) that a user selects from his/her memory without any additional aids.
The estimates take into account the distribution of words in source language.
We use established security metrics, such as entropy, guesswork, marginal
guesswork and marginal success rate. The metrics are evaluated for various
scenarios -- aimed at improving the security of the produced PINs. In general,
plain and straightforward construction of memory-only dictionary PINs yields
unsatisfactory results and more involved methods must be used to produce secure
PINs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1718</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1718</id><created>2014-04-07</created><updated>2014-10-31</updated><authors><author><keyname>Leuenberger</keyname><forenames>Gabriel</forenames></author></authors><title>Universal Algorithmic Ethics</title><categories>cs.AI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper unifies the following existing formal theories: Universal
Intelligence Measure, Space-time-embedded Intelligence, Observer Localisation.
The unified theory provides a new simple agent framework. This new framework is
then used to address philosophical questions about the simulation argument, the
doomsday argument, personal identity, and ethics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1732</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1732</id><created>2014-04-07</created><updated>2014-07-07</updated><authors><author><keyname>Konrad</keyname><forenames>Christian</forenames></author><author><keyname>Kozma</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Streaming Algorithms for Partitioning Integer Sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of partitioning integer sequences in the one-pass data
streaming model. Given is an input stream of integers $X \in \{0, 1, \dots, m
\}^n$ of length $n$ with maximum element $m$, and a parameter $p$. The goal is
to output the positions of separators splitting the input stream into $p$
contiguous blocks such that the maximal weight of a block is minimized. We show
that computing an optimal solution requires linear space, and we design space
efficient $(1+\epsilon)$-approximation algorithms for this problem following
the parametric search framework. We demonstrate that parametric search can be
successfully applied in the streaming model, and we present more space
efficient refinements of the basic method. All discussed algorithms require
space $O( \frac{1}{\epsilon} \mathrm{polylog} (m,n,\frac{1}{\epsilon}))$, and
we prove that the linear dependency on $\frac{1}{\epsilon}$ is necessary for
any possibly randomized one-pass streaming algorithm that computes a
$(1+\epsilon)$-approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1736</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1736</id><created>2014-04-07</created><updated>2015-02-09</updated><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Faulty Successive Cancellation Decoding of Polar Codes for the Binary
  Erasure Channel</title><categories>cs.IT math.IT</categories><comments>As presented at ISITA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study faulty successive cancellation decoding of polar codes for the
binary erasure channel. To this end, we introduce a simple erasure-based fault
model and we show that, under this model, polarization does not happen, meaning
that fully reliable communication is not possible at any rate. Moreover, we
provide numerical results for the frame erasure rate and bit erasure rate and
we study an unequal error protection scheme that can significantly improve the
performance of the faulty successive cancellation decoder with negligible
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1741</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1741</id><created>2014-04-07</created><updated>2015-04-15</updated><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author></authors><title>Tighter Fourier Transform Complexity Tradeoffs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fourier Transform is one of the most important linear transformations
used in science and engineering. Cooley and Tukey's Fast Fourier Transform
(FFT) from 1964 is a method for computing this transformation in time $O(n\log
n)$. Achieving a matching lower bound in a reasonable computational model is
one of the most important open problems in theoretical computer science.
  In 2014, improving on his previous work, Ailon showed that if an algorithm
speeds up the FFT by a factor of $b=b(n)\geq 1$, then it must rely on
computing, as an intermediate &quot;bottleneck&quot; step, a linear mapping of the input
with condition number $\Omega(b(n))$. Our main result shows that a factor $b$
speedup implies existence of not just one but $\Omega(n)$ $b$-ill conditioned
bottlenecks occurring at $\Omega(n)$ different steps, each causing information
from independent (orthogonal) components of the input to either overflow or
underflow. This provides further evidence that beating FFT is hard. Our result
also gives the first quantitative tradeoff between computation speed and
information loss in Fourier computation on fixed word size architectures. The
main technical result is an entropy analysis of the Fourier transform under
transformations of low trace, which is interesting in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1743</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1743</id><created>2014-04-07</created><authors><author><keyname>Verma</keyname><forenames>Gourav</forenames></author><author><keyname>Ramaiya</keyname><forenames>Deepika</forenames></author></authors><title>Analysis, Review and Optimization of SONET/SDH Technology for today and
  future aspects</title><categories>cs.NI</categories><journal-ref>International Journal of Advanced Studies in Computer Science &amp;
  Engineering IJASCSE, Volume 3, Issue 3, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network layers are analyzed for their design and issues of researches, while
dense wavelength division multiplexing equipment has been deployed in networks
of major telecommunications carriers for a long time, the efficiency of
networking and relation with network control and management have not caught up
to those of digital cross-connect systems and packet-switched counterparts in
higher layer networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1774</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1774</id><created>2014-04-07</created><authors><author><keyname>Eder</keyname><forenames>Christian</forenames></author><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames></author></authors><title>A survey on signature-based Gr\&quot;obner basis computations</title><categories>math.AC cs.SC</categories><comments>53 pages, 8 figures, 11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey on the area of signature-based Gr\&quot;obner basis
algorithms that was initiated by Faug\`ere's F5 algorithm in 2002. We explain
the general ideas behind the usage of signatures. We show how to classify the
various known variants by 3 different orderings. For this we give translations
between different notations and show that besides notations many approaches are
just the same. Moreover, we give a general description of how the idea of
signatures is quite natural when performing the reduction process using linear
algebra. This survey shall help to outline this field of active research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1775</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1775</id><created>2014-04-07</created><updated>2014-09-30</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author></authors><title>Fun with Fonts: Algorithmic Typography</title><categories>cs.CG</categories><comments>14 pages, 12 figures. Revised paper with new glass cane font.
  Original version in Proceedings of the 7th International Conference on Fun
  with Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, we have designed six typefaces based on mathematical
theorems and open problems, specifically computational geometry. These
typefaces expose the general public in a unique way to intriguing results and
hard problems in hinged dissections, geometric tours, origami design,
computer-aided glass design, physical simulation, and protein folding. In
particular, most of these typefaces include puzzle fonts, where reading the
intended message requires solving a series of puzzles which illustrate the
challenge of the underlying algorithmic problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1777</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1777</id><created>2014-04-07</created><updated>2014-07-07</updated><authors><author><keyname>Babenko</keyname><forenames>Artem</forenames></author><author><keyname>Slesarev</keyname><forenames>Anton</forenames></author><author><keyname>Chigorin</keyname><forenames>Alexandr</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Neural Codes for Image Retrieval</title><categories>cs.CV</categories><comments>to appear at ECCV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that the activations invoked by an image within the top
layers of a large convolutional neural network provide a high-level descriptor
of the visual content of the image. In this paper, we investigate the use of
such descriptors (neural codes) within the image retrieval application. In the
experiments with several standard retrieval benchmarks, we establish that
neural codes perform competitively even when the convolutional neural network
has been trained for an unrelated classification task (e.g.\ Image-Net). We
also evaluate the improvement in the retrieval performance of neural codes,
when the network is retrained on a dataset of images that are similar to images
encountered at test time.
  We further evaluate the performance of the compressed neural codes and show
that a simple PCA compression provides very good short codes that give
state-of-the-art accuracy on a number of datasets. In general, neural codes
turn out to be much more resilient to such compression in comparison other
state-of-the-art descriptors. Finally, we show that discriminative
dimensionality reduction trained on a dataset of pairs of matched photographs
improves the performance of PCA-compressed neural codes even further. Overall,
our quantitative experiments demonstrate the promise of neural codes as visual
descriptors for image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1782</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1782</id><created>2014-04-07</created><updated>2014-04-08</updated><authors><author><keyname>Lotfi</keyname><forenames>Mohammad Hassan</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author></authors><title>Network Non-Neutrality on the Internet: Content Provision Under a
  Subscription Revenue Model</title><categories>cs.NI cs.GT</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to provide an insight into the equilibrium of the
Internet market, when the current balance of the market is disrupted, and one
of the ISPs switches to a non-neutral regime. We consider a content provider
with a subscription revenue model and a continuum of end-users. The CP is also
non-neutral, in the sense that she can charge users of different ISPs different
subscription fees, and use this &quot;leverage&quot; to control the equilibrium outcome.
Results reveal that the CP is able to control the non-neutral ISP to some
extend. However, switching to a non-neutral regime by an ISP tips the balance
of the market in favor of this ISP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1799</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1799</id><created>2014-04-07</created><authors><author><keyname>Kyriakou</keyname><forenames>Harris</forenames></author><author><keyname>Nickerson</keyname><forenames>Jeffrey V.</forenames></author></authors><title>Collective Innovation in Open Source Hardware</title><categories>cs.CY cs.HC</categories><comments>To appear in the proceedings of Collective Intelligence 2014 (Boston,
  June 10-12 2014)</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing community that shares digital 3D designs has created an opportunity
to study, encourage and stimulate innovation. This remix community allows
people not only to prototype at a minimal cost but also to work on projects
they are genuinely interested in. Participants free of the limitations
typically imposed by formal organizations develop products driven by their own
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1810</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1810</id><created>2014-04-07</created><authors><author><keyname>Pasquini</keyname><forenames>Lorenzo</forenames></author></authors><title>A class of AM-QFT algorithms for power-of-two FFT</title><categories>cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a class of power-of-two FFT (Fast Fourier Transform)
algorithms, called AM-QFT algorithms, that contains the improved QFT (Quick
Fourier Transform), an algorithm recently published, as a special case. The
main idea is to apply the Amplitude Modulation Double Sideband - Suppressed
Carrier (AM DSB-SC) to convert odd-indices signals into even-indices signals,
and to insert this elaboration into the improved QFT algorithm, substituting
the multiplication by secant function. The 8 variants of this class are
obtained by re-elaboration of the AM DSB-SC idea, and by means of duality. As a
result the 8 variants have both the same computational cost and the same memory
requirements than improved QFT. Differently, comparing this class of 8 variants
of AM-QFT algorithm with the split-radix 3add/3mul (one of the most performing
FFT approach appeared in the literature), we obtain the same number of
additions and multiplications, but employing half of the trigonometric
constants. This makes the proposed FFT algorithms interesting and useful for
fixed-point implementations. Some of these variants show advantages versus the
improved QFT. In fact one of this variant slightly enhances the numerical
accuracy of improved QFT, while other four variants use trigonometric constants
that are faster to compute in `on the fly' implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1812</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1812</id><created>2014-04-07</created><authors><author><keyname>Kumar</keyname><forenames>Anugrah</forenames></author></authors><title>Determining the Consistency factor of Autopilot using Rough Set Theory</title><categories>cs.AI</categories><comments>IEEE International Conference on Networking, Sensing and Control 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autopilot is a system designed to guide a vehicle without aid. Due to
increase in flight hours and complexity of modern day flight it has become
imperative to equip the aircrafts with autopilot. Thus reliability and
consistency of an Autopilot system becomes a crucial role in a flight. But the
increased complexity and demand for better accuracy has made the process of
evaluating the autopilot for consistency a difficult process .A vast amount of
imprecise data has been involved. Rough sets can be a potent tool for such kind
of Applications containing vague data. This paper proposes an approach towards
Consistency factor determination using Rough Set Theory. The seventeen basic
factors, that are crucial in determining the consistency of an Autopilot
system, are grouped into five Payloads based on their functionality.
Consistency Factor is evaluated through these payloads, using Rough Set Theory.
Consistency Factor determines the consistency and reliability of an autopilot
system and the conditions under which manual override becomes imperative. Using
Rough set Theory the most and the least influential factors towards Autopilot
system are also determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1814</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1814</id><created>2014-04-07</created><authors><author><keyname>Lestaris</keyname><forenames>G.</forenames></author><author><keyname>Charalampidis</keyname><forenames>I.</forenames></author><author><keyname>Berzano</keyname><forenames>D.</forenames></author><author><keyname>Blomer</keyname><forenames>J.</forenames></author><author><keyname>Buncic</keyname><forenames>P.</forenames></author><author><keyname>Ganis</keyname><forenames>G.</forenames></author><author><keyname>Meusel</keyname><forenames>R.</forenames></author></authors><title>CernVM Online and Cloud Gateway: a uniform interface for CernVM
  contextualization and deployment</title><categories>cs.DC</categories><comments>Conference paper at the 2013 Computing in High Energy Physics (CHEP)
  Conference, Amsterdam</comments><doi>10.1088/1742-6596/513/3/032055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a virtualized environment, contextualization is the process of configuring
a VM instance for the needs of various deployment use cases. Contextualization
in CernVM can be done by passing a handwritten context to the user data field
of cloud APIs, when running CernVM on the cloud, or by using CernVM web
interface when running the VM locally. CernVM Online is a publicly accessible
web interface that unifies these two procedures. A user is able to define,
store and share CernVM contexts using CernVM Online and then apply them either
in a cloud by using CernVM Cloud Gateway or on a local VM with the single-step
pairing mechanism. CernVM Cloud Gateway is a distributed system that provides a
single interface to use multiple and different clouds (by location or type,
private or public). Cloud gateway has been so far integrated with OpenNebula,
CloudStack and EC2 tools interfaces. A user, with access to a number of clouds,
can run CernVM cloud agents that will communicate with these clouds using their
interfaces, and then use one single interface to deploy and scale CernVM
clusters. CernVM clusters are defined in CernVM Online and consist of a set of
CernVM instances that are contextualized and can communicate with each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1820</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1820</id><created>2014-04-07</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Max-min Fair Wireless Energy Transfer for Secure Multiuser Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, invited paper, IEEE Information Theory Workshop 2014,
  Hobart, Tasmania, Australia, Nov. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers max-min fairness for wireless energy transfer in a
downlink multiuser communication system. Our resource allocation design
maximizes the minimum harvested energy among multiple multiple-antenna energy
harvesting receivers (potential eavesdroppers) while providing quality of
service (QoS) for secure communication to multiple single-antenna information
receivers. In particular, the algorithm design is formulated as a non-convex
optimization problem which takes into account a minimum required
signal-to-interference-plus-noise ratio (SINR) constraint at the information
receivers and a constraint on the maximum tolerable channel capacity achieved
by the energy harvesting receivers for a given transmit power budget. The
proposed problem formulation exploits the dual use of artificial noise
generation for facilitating efficient wireless energy transfer and secure
communication. A semidefinite programming (SDP) relaxation approach is
exploited to obtain a global optimal solution of the considered problem.
Simulation results demonstrate the significant performance gain in harvested
energy that is achieved by the proposed optimal scheme compared to two simple
baseline schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1830</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1830</id><created>2014-04-07</created><authors><author><keyname>Craciunas</keyname><forenames>Silviu S.</forenames></author><author><keyname>Kirsch</keyname><forenames>Christoph M.</forenames></author><author><keyname>Payer</keyname><forenames>Hannes</forenames></author><author><keyname>R&#xf6;ck</keyname><forenames>Harald</forenames></author><author><keyname>Sokolova</keyname><forenames>Ana</forenames></author></authors><title>Concurrency and Scalability versus Fragmentation and Compaction with
  Compact-fit</title><categories>cs.PL</categories><report-no>University of Salzburg, Department of Computer Sciences, Technical
  Report 2009-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study, formally and experimentally, the trade-off in temporal and spatial
overhead when managing contiguous blocks of memory using the explicit, dynamic
and real-time heap management system Compact-fit (CF). The key property of CF
is that temporal and spatial overhead can be bounded, related, and predicted in
constant time through the notion of partial and incremental compaction. Partial
compaction determines the maximally tolerated degree of memory fragmentation.
Incremental compaction of objects, introduced here, determines the maximal
amount of memory involved in any, logically atomic, portion of a compaction
operation. We explore CF's potential application space on (1) multiprocessor
and multicore systems as well as on (2) memory-constrained uniprocessor
systems. For (1), we argue that little or no compaction is likely to avoid the
worst case in temporal as well as spatial overhead but also observe that
scalability only improves by a constant factor. Scalability can be further
improved significantly by reducing overall data sharing through separate
instances of Compact-fit. For (2), we observe that incremental compaction can
effectively trade-off throughput and memory fragmentation for lower latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1831</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1831</id><created>2014-04-07</created><authors><author><keyname>Babenko</keyname><forenames>Artem</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Improving Bilayer Product Quantization for Billion-Scale Approximate
  Nearest Neighbors in High Dimensions</title><categories>cs.CV</categories><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The top-performing systems for billion-scale high-dimensional approximate
nearest neighbor (ANN) search are all based on two-layer architectures that
include an indexing structure and a compressed datapoints layer. An indexing
structure is crucial as it allows to avoid exhaustive search, while the lossy
data compression is needed to fit the dataset into RAM. Several of the most
successful systems use product quantization (PQ) for both the indexing and the
dataset compression layers. These systems are however limited in the way they
exploit the interaction of product quantization processes that happen at
different stages of these systems.
  Here we introduce and evaluate two approximate nearest neighbor search
systems that both exploit the synergy of product quantization processes in a
more efficient way. The first system, called Fast Bilayer Product Quantization
(FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by several
times, while achieving the same accuracy. The second system, Hierarchical
Bilayer Product Quantization (HBPQ) provides a significantly better recall for
the same runtime at a cost of small memory footprint increase. For the BIGANN
dataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17%
increase in Recall@10 is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1836</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1836</id><created>2014-04-07</created><authors><author><keyname>Tirodkar</keyname><forenames>Sagar</forenames></author><author><keyname>Baldawala</keyname><forenames>Yazad</forenames></author><author><keyname>Ulane</keyname><forenames>Sagar</forenames></author><author><keyname>Jori</keyname><forenames>Ashok</forenames></author></authors><title>Improved 3-Dimensional Security in Cloud Computing</title><categories>cs.CR cs.DC cs.NI</categories><comments>6 Pages, 10 Figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V9(5):242-247, March 2014. Published by Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V9P145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a trending technology in the field of Information
Technology as it allows sharing of resources over a network. The reason Cloud
computing gained traction so rapidly was because of its performance,
availability and low cost among other features. Besides these features,
companies are still refraining from binding their business with cloud computing
due to the fear of data leakage. The focus of this paper is on the problem of
data leakage. It proposes a framework which works in two phases. The first
phase consists of data encryption and classification which is performed before
storing the data. In this phase, the client may want to encrypt his data prior
to uploading. After encryption, data is classified using three parameters
namely Confidentiality [C], Integrity [I] and Availability [A]. With the help
of proposed algorithm, criticality rating (Cr) of the data is calculated.
According to the Cr, security will be provided on the basis of the 3 Dimensions
proposed in this paper. The second phase consists of data retrieval by the
client. As per the concept of 3D, users who want to access their data need to
be authenticated, to avoid data from being compromised. Before every access to
data, the users identity is verified for authorization. After the user is
authorized for data access, if the data is encrypted, the user can decrypt the
same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1847</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1847</id><created>2014-04-07</created><authors><author><keyname>Kalyani</keyname><forenames>Aditi</forenames></author><author><keyname>Kumud</keyname><forenames>Hemant</forenames></author><author><keyname>Singh</keyname><forenames>Shashi Pal</forenames></author><author><keyname>Kumar</keyname><forenames>Ajai</forenames></author><author><keyname>Darbari</keyname><forenames>Hemant</forenames></author></authors><title>Evaluation and Ranking of Machine Translated Output in Hindi Language
  using Precision and Recall Oriented Metrics</title><categories>cs.CL</categories><journal-ref>International Journal of Advanced Computer Research, Volume-4
  Number-1 Issue-14 March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation plays a crucial role in development of Machine translation
systems. In order to judge the quality of an existing MT system i.e. if the
translated output is of human translation quality or not, various automatic
metrics exist. We here present the implementation results of different metrics
when used on Hindi language along with their comparisons, illustrating how
effective are these metrics on languages like Hindi (free word order language).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1848</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1848</id><created>2014-04-07</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Minsky</keyname><forenames>Naftaly H.</forenames></author></authors><title>Establishing Global Policies over Decentralized Online Social Networks</title><categories>cs.SI</categories><comments>technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional online social networks (OSNs) are implemented in a centralized
manner. Although centralization is a convenient way for implementing OSNs, it
has several well known drawbacks. Chief among them are the risks they pose to
the security and privacy of the information maintained by the OSN; and the loss
of control over the information contributed by individual members.
  These concerns prompted several attempts to create decentralized OSNs, or
DOSNs. The basic idea underlying these attempts, is that each member of a
social network keeps its data under its own control, instead of surrendering it
to a central host; providing access to it to other members of the OSN according
to its own access-control policy. Unfortunately all existing DOSN projects have
a very serious limitation. Namely, they are unable to subject the membership of
a DOSN, and the interaction between its members, to any global policy.
  We adopt the decentralization idea underlying DOSNs, complementing it with a
means for specifying and enforcing a wide range of policies over the membership
of a social community, and over the interaction between its disparate
distributed members. And we do so in a scalable fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1849</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1849</id><created>2014-04-07</created><authors><author><keyname>Gemsa</keyname><forenames>Andreas</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Evaluation of Labeling Strategies for Rotating Maps</title><categories>cs.CG cs.DS</categories><comments>16 pages, extended version of a SEA 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem of labeling points in a dynamic map that
allows rotation. We are given a set of points in the plane labeled by a set of
mutually disjoint labels, where each label is an axis-aligned rectangle
attached with one corner to its respective point. We require that each label
remains horizontally aligned during the map rotation and our goal is to find a
set of mutually non-overlapping active labels for every rotation angle $\alpha
\in [0, 2\pi)$ so that the number of active labels over a full map rotation of
2$\pi$ is maximized. We discuss and experimentally evaluate several labeling
models that define additional consistency constraints on label activities in
order to reduce flickering effects during monotone map rotation. We introduce
three heuristic algorithms and compare them experimentally to an existing
approximation algorithm and exact solutions obtained from an integer linear
program. Our results show that on the one hand low flickering can be achieved
at the expense of only a small reduction in the objective value, and that on
the other hand the proposed heuristics achieve a high labeling quality
significantly faster than the other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1864</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1864</id><created>2014-04-07</created><authors><author><keyname>Bressan</keyname><forenames>Marco</forenames></author><author><keyname>Peserico</keyname><forenames>Enoch</forenames></author><author><keyname>Pretto</keyname><forenames>Luca</forenames></author></authors><title>Approximating PageRank locally with sublinear query complexity</title><categories>cs.DS cs.IR cs.SI</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of approximating the PageRank score of a node with minimal
information about the rest of the graph has attracted considerable attention in
the last decade; but its central question, whether it is in general necessary
to explore a non-vanishing fraction of the graph, remained open until now (only
for specific graphs and/or nodes was a solution known). We present the first
algorithm that produces a $(1\pm\epsilon)$-approximation of the score of any
one node in any $n$-node graph with probability $(1-\epsilon)$ visiting at most
$O(n^\frac{2}{3}\sqrt[3]{\log(n)})=o(n)$ nodes. Our result is essentially tight
(we prove that visiting $\Omega(n^\frac{2}{3})$ nodes is in general necessary
to solve even an easier &quot;ranking&quot; version of the problem under any &quot;natural&quot;
graph exploration model, including all those in the literature) but it can be
further improved for some classes of graphs and/or nodes of practical interest
- e.g. to $O(n^\frac{1}{2} \gamma^\frac{1}{2})$ nodes in graphs with maximum
outdegree $\gamma$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1869</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1869</id><created>2014-04-07</created><authors><author><keyname>Iandola</keyname><forenames>Forrest</forenames></author><author><keyname>Moskewicz</keyname><forenames>Matt</forenames></author><author><keyname>Karayev</keyname><forenames>Sergey</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Keutzer</keyname><forenames>Kurt</forenames></author></authors><title>DenseNet: Implementing Efficient ConvNet Descriptor Pyramids</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) can provide accurate object
classification. They can be extended to perform object detection by iterating
over dense or selected proposed object regions. However, the runtime of such
detectors scales as the total number and/or area of regions to examine per
image, and training such detectors may be prohibitively slow. However, for some
CNN classifier topologies, it is possible to share significant work among
overlapping regions to be classified. This paper presents DenseNet, an open
source system that computes dense, multiscale features from the convolutional
layers of a CNN based object classifier. Future work will involve training
efficient object detectors with DenseNet feature descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1872</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1872</id><created>2014-04-07</created><authors><author><keyname>Sigogne</keyname><forenames>Anthony</forenames><affiliation>LIGM</affiliation></author><author><keyname>Constant</keyname><forenames>Matthieu</forenames><affiliation>LIGM</affiliation></author><author><keyname>Laporte</keyname><forenames>Eric</forenames><affiliation>LIGM</affiliation></author></authors><title>Int\'egration des donn\'ees d'un lexique syntaxique dans un analyseur
  syntaxique probabiliste</title><categories>cs.CL</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Penser le Lexique-Grammaire. Perspectives actuelles, Fryni
  Kakoyianni-Doa (Ed.) (2014) 505-516</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reports the evaluation of the integration of data from a
syntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic
parser. We show that by changing the set of labels for verbs and predicational
nouns, we can improve the performance on French of a non-lexicalized
probabilistic parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1884</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1884</id><created>2014-04-07</created><authors><author><keyname>Tang</keyname><forenames>Guoming</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Lei</keyname><forenames>Jingsheng</forenames></author><author><keyname>Tang</keyname><forenames>Jiuyang</forenames></author></authors><title>Plug and Play! A Simple, Universal Model for Energy Disaggregation</title><categories>cs.AI</categories><comments>12 pages, 5 figures, and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy disaggregation is to discover the energy consumption of individual
appliances from their aggregated energy values. To solve the problem, most
existing approaches rely on either appliances' signatures or their state
transition patterns, both hard to obtain in practice. Aiming at developing a
simple, universal model that works without depending on sophisticated machine
learning techniques or auxiliary equipments, we make use of easily accessible
knowledge of appliances and the sparsity of the switching events to design a
Sparse Switching Event Recovering (SSER) method. By minimizing the total
variation (TV) of the (sparse) event matrix, SSER can effectively recover the
individual energy consumption values from the aggregated ones. To speed up the
process, a Parallel Local Optimization Algorithm (PLOA) is proposed to solve
the problem in active epochs of appliance activities in parallel. Using
real-world trace data, we compare the performance of our method with that of
the state-of-the-art solutions, including Least Square Estimation (LSE) and
iterative Hidden Markov Model (HMM). The results show that our approach has an
overall higher detection accuracy and a smaller overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1890</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1890</id><created>2014-04-07</created><authors><author><keyname>Bujok</keyname><forenames>Maksymilian</forenames></author><author><keyname>Fronczak</keyname><forenames>Piotr</forenames></author><author><keyname>Fronczak</keyname><forenames>Agata</forenames></author></authors><title>Polish and English wordnets -- statistical analysis of interconnected
  networks</title><categories>cs.CL physics.soc-ph</categories><comments>12 pages, 10 figures, Presented at Summer Solstice 2013 Conference on
  Discrete Models of Complex Systems, Warsaw, Poland</comments><journal-ref>Acta Phys. Pol. B Proc. Suppl. Vol. 7 (2014) 245-256</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wordnets are semantic networks containing nouns, verbs, adjectives, and
adverbs organized according to linguistic principles, by means of semantic
relations. In this work, we adopt a complex network perspective to perform a
comparative analysis of the English and Polish wordnets. We determine their
similarities and show that the networks exhibit some of the typical
characteristics observed in other real-world networks. We analyse interlingual
relations between both wordnets and deliberate over the problem of mapping the
Polish lexicon onto the English one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1891</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1891</id><created>2014-04-07</created><updated>2014-04-08</updated><authors><author><keyname>Herrmann</keyname><forenames>Richard</forenames></author></authors><title>A fractal approach to the dark silicon problem: a comparison of 3D
  computer architectures -- standard slices versus fractal Menger sponge
  geometry</title><categories>cs.ET</categories><comments>4 pages 2 figures</comments><journal-ref>Chaos Solitons Fractals 70, (2015) pp. 38-41</journal-ref><doi>10.1016/j.chaos.2014.11.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dark silicon problem, which limits the power-growth of future computer
generations, is interpreted as a heat energy transport problem when increasing
the energy emitting surface area within a given volume. A comparison of two
3D-configuration models, namely a standard slicing and a fractal surface
generation within the Menger sponge geometry is presented. It is shown, that
for iteration orders $n&gt;3$ the fractal model shows increasingly better thermal
behavior. As a consequence cooling problems may be minimized by using a fractal
architecture. Therefore the Menger sponge geometry is a good example for
fractal architectures applicable not only in computer science, but also e.g. in
chemistry when building chemical reactors, optimizing catalytic processes or in
sensor construction technology building highly effective sensors for toxic
gases or water analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1894</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1894</id><created>2014-04-07</created><updated>2016-01-21</updated><authors><author><keyname>Goodenough</keyname><forenames>Silvia</forenames><affiliation>LIPN</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>Overview of the Heisenberg--Weyl Algebra and Subsets of Riordan
  Subgroups</title><categories>cs.DM math.CO math.OA math.RA quant-ph</categories><comments>Version 3 of the paper entitled `On subsets of Riordan subgroups and
  Heisenberg--Weyl algebra' in [hal-00974929v2]The present article is published
  in The Electronic Journal of Combinatorics, Volume 22, Issue 4, 40 pages
  (Oct. 2015), pp.Id: 16</comments><proxy>ccsd</proxy><report-no>Institut Galil{\'e}e Universit{\'e} Paris 13, Sorbonne Paris
  Cit{\'e} LIPN CNRS UMR 7030</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a first part, we are concerned with the relationships between polynomials
in the two generators of the algebra of Heisenberg--Weyl, its Bargmann--Fock
representation with differential operators and the associated one-parameter
group.Upon this basis, the paper is then devoted to the groups of Riordan
matrices associated to the related transformations of matrices (i.e.
substitutions with prefunctions). Thereby, various properties are studied
arising in Riordan arrays, in the Riordan group and, more specifically, in the
`striped' Riordan subgroups; further, a striped quasigroup and a semigroup are
also examined. A few applications to combinatorial structures are also briefly
addressed in the Appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1900</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1900</id><created>2014-04-07</created><updated>2014-07-09</updated><authors><author><keyname>Saez</keyname><forenames>Y.</forenames></author><author><keyname>Cao</keyname><forenames>X.</forenames></author><author><keyname>Kish</keyname><forenames>L. B.</forenames></author><author><keyname>Pesti</keyname><forenames>G.</forenames></author></authors><title>Securing vehicle communication systems by the KLJN key exchange protocol</title><categories>cs.CR</categories><comments>12 pages, 5 figures, 1 table. Accepted for publication at FNL on May
  19, 2014</comments><journal-ref>Fluctuation and Noise Letters, Vol. 13, No. 3 (2014) 1450020 (14
  pages)</journal-ref><doi>10.1142/S0219477514500205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the security requirements for vehicular communication networks and
provide a critical assessment of some typical communication security solutions.
We also propose a novel unconditionally secure vehicular communication
architecture that utilizes the Kirchhoff-law-Johnson-noise (KLJN) key
distribution scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1908</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1908</id><created>2014-04-07</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Fair channel allocation and access design for cognitive ad hoc networks</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1404.1674</comments><journal-ref>Tan, L.T.; Long Bao Le, &quot;Fair channel allocation and access design
  for cognitive ad hoc networks,&quot; Global Communications Conference (GLOBECOM),
  2012 IEEE , vol., no., pp.1162,1167, 3-7 Dec. 2012 doi:
  10.1109/GLOCOM.2012.6503270</journal-ref><doi>10.1109/GLOCOM.2012.6503270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the fair channel assignment and access design problem for
cognitive radio ad hoc network in this paper. In particular, we consider a
scenario where ad hoc network nodes have hardware constraints which allow them
to access at most one channel at any time. We investigate a fair channel
allocation problem where each node is allocated a subset of channels which are
sensed and accessed periodically by their owners by using a MAC protocol.
Toward this end, we analyze the complexity of the optimal brute-force search
algorithm which finds the optimal solution for this NP-hard problem. We then
develop low-complexity algorithms that can work efficiently with a MAC protocol
algorithm, which resolves the access contention from neighboring secondary
nodes. Also, we develop a throughput analytical model, which is used in the
proposed channel allocation algorithm and for performance evaluation of its
performance. Finally, we present extensive numerical results to demonstrate the
efficacy of the proposed algorithms in achieving fair spectrum sharing among
traffic flows in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1911</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1911</id><created>2014-04-07</created><authors><author><keyname>Saket</keyname><forenames>Bahador</forenames></author><author><keyname>Simonetto</keyname><forenames>Paolo</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Borner</keyname><forenames>Katy</forenames></author></authors><title>Node, Node-Link, and Node-Link-Group Diagrams: An Evaluation</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effectively showing the relationships between objects in a dataset is one of
the main tasks in information visualization. Typically there is a well-defined
notion of distance between pairs of objects, and traditional approaches such as
principal component analysis or multi-dimensional scaling are used to place the
objects as points in 2D space, so that similar objects are close to each other.
In another typical setting, the dataset is visualized as a network graph, where
related nodes are connected by links. More recently, datasets are also
visualized as maps, where in addition to nodes and links, there is an explicit
representation of groups and clusters. We consider these three Techniques,
characterized by a progressive increase of the amount of encoded information:
node diagrams, node-link diagrams and node-link-group diagrams. We assess these
three types of diagrams with a controlled experiment that covers nine different
tasks falling broadly in three categories: node-based tasks, network-based
tasks and group-based tasks. Our findings indicate that adding links, or links
and group representations, does not negatively impact performance (time and
accuracy) of node-based tasks. Similarly, adding group representations does not
negatively impact the performance of network-based tasks. Node-link-group
diagrams outperform the others on group-based tasks. These conclusions
contradict results in other studies, in similar but subtly different settings.
Taken together, however, such results can have significant implications for the
design of standard and domain specific visualizations tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1943</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1943</id><created>2014-04-07</created><authors><author><keyname>Im</keyname><forenames>Sungjin</forenames></author><author><keyname>Kulkarni</keyname><forenames>Janardhan</forenames></author><author><keyname>Munagala</keyname><forenames>Kamesh</forenames></author><author><keyname>Pruhs</keyname><forenames>Kirk</forenames></author></authors><title>SELFISHMIGRATE: A Scalable Algorithm for Non-clairvoyantly Scheduling
  Heterogeneous Processors</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical problem of minimizing the total weighted flow-time
for unrelated machines in the online \emph{non-clairvoyant} setting. In this
problem, a set of jobs $J$ arrive over time to be scheduled on a set of $M$
machines. Each job $j$ has processing length $p_j$, weight $w_j$, and is
processed at a rate of $\ell_{ij}$ when scheduled on machine $i$. The online
scheduler knows the values of $w_j$ and $\ell_{ij}$ upon arrival of the job,
but is not aware of the quantity $p_j$. We present the {\em first} online
algorithm that is {\em scalable} ($(1+\eps)$-speed
$O(\frac{1}{\epsilon^2})$-competitive for any constant $\eps &gt; 0$) for the
total weighted flow-time objective. No non-trivial results were known for this
setting, except for the most basic case of identical machines. Our result
resolves a major open problem in online scheduling theory. Moreover, we also
show that no job needs more than a logarithmic number of migrations. We further
extend our result and give a scalable algorithm for the objective of minimizing
total weighted flow-time plus energy cost for the case of unrelated machines
and obtain a scalable algorithm. The key algorithmic idea is to let jobs
migrate selfishly until they converge to an equilibrium. Towards this end, we
define a game where each job's utility which is closely tied to the
instantaneous increase in the objective the job is responsible for, and each
machine declares a policy that assigns priorities to jobs based on when they
migrate to it, and the execution speeds. This has a spirit similar to
coordination mechanisms that attempt to achieve near optimum welfare in the
presence of selfish agents (jobs). To the best our knowledge, this is the first
work that demonstrates the usefulness of ideas from coordination mechanisms and
Nash equilibria for designing and analyzing online algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1947</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1947</id><created>2014-04-07</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Eine entscheidbare Klasse n-stelliger Horn-Pr\&quot;adikate</title><categories>cs.LO</categories><comments>in german; 11 pages</comments><msc-class>68Q42</msc-class><acm-class>F.4.2; I.2.3</acm-class><journal-ref>Peter H. Schmitt (ed.), Proc. 3rd Ann. Meeting of the German
  Computer Science Society (GI) Special Interest Group on Logic in Computer
  Science (FG 0.1.6), Karlsruhe University Internal Report 23/95, p.38-47, Jun
  1995</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similar to a tree grammar, a Horn theory can be used to describe an infinite
set of terms. In this paper, we present a class of Horn theories such that the
set of definable predicates is closed wrt. conjunction and such that the
satisfiability of a predicate is decidable. This extends previous results on
Horn clauses with unary predicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1950</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1950</id><created>2014-04-07</created><authors><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>On the power of homogeneous depth 4 arithmetic circuits</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove exponential lower bounds on the size of homogeneous depth 4
arithmetic circuits computing an explicit polynomial in $VP$. Our results hold
for the {\it Iterated Matrix Multiplication} polynomial - in particular we show
that any homogeneous depth 4 circuit computing the $(1,1)$ entry in the product
of $n$ generic matrices of dimension $n^{O(1)}$ must have size
$n^{\Omega(\sqrt{n})}$.
  Our results strengthen previous works in two significant ways.
  Our lower bounds hold for a polynomial in $VP$. Prior to our work, Kayal et
al [KLSS14] proved an exponential lower bound for homogeneous depth 4 circuits
(over fields of characteristic zero) computing a poly in $VNP$. The best known
lower bounds for a depth 4 homogeneous circuit computing a poly in $VP$ was the
bound of $n^{\Omega(\log n)}$ by [LSS, KLSS14].Our exponential lower bounds
also give the first exponential separation between general arithmetic circuits
and homogeneous depth 4 arithmetic circuits. In particular they imply that the
depth reduction results of Koiran [Koi12] and Tavenas [Tav13] are tight even
for reductions to general homogeneous depth 4 circuits (without the restriction
of bounded bottom fanin).
  Our lower bound holds over all fields. The lower bound of [KLSS14] worked
only over fields of characteristic zero. Prior to our work, the best lower
bound for homogeneous depth 4 circuits over fields of positive characteristic
was $n^{\Omega(\log n)}$ [LSS, KLSS14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1951</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1951</id><created>2014-04-07</created><updated>2015-03-06</updated><authors><author><keyname>Libert</keyname><forenames>Tim</forenames></author></authors><title>Privacy Implications of Health Information Seeking on the Web</title><categories>cs.CY cs.CR</categories><journal-ref>Communications of the ACM, 2015, Vol. 58 No. 3, Pages 68-77</journal-ref><doi>10.1145/2658983</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates privacy risks to those visiting health- related web
pages. The population of pages analyzed is derived from the 50 top search
results for 1,986 common diseases. This yielded a total population of 80,124
unique pages which were analyzed for the presence of third-party HTTP requests.
91% of pages were found to make requests to third parties. Investigation of
URIs revealed that 70% of HTTP Referer strings contained information exposing
specific conditions, treatments, and diseases. This presents a risk to users in
the form of personal identification and blind discrimination. An examination of
extant government and corporate policies reveals that users are insufficiently
protected from such risks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1955</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1955</id><created>2014-04-07</created><authors><author><keyname>Alizadeh</keyname><forenames>Mahnoosh</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>Capturing Aggregate Flexibility in Demand Response</title><categories>cs.SY</categories><comments>Submitted to IEEE CDC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexibility in electric power consumption can be leveraged by Demand Response
(DR) programs. The goal of this paper is to systematically capture the inherent
aggregate flexibility of a population of appliances. We do so by clustering
individual loads based on their characteristics and service constraints. We
highlight the challenges associated with learning the customer response to
economic incentives while applying demand side management to heterogeneous
appliances. We also develop a framework to quantify customer privacy in direct
load scheduling programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1957</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1957</id><created>2014-04-07</created><updated>2015-10-29</updated><authors><author><keyname>Arapostathis</keyname><forenames>Ari</forenames></author><author><keyname>Biswas</keyname><forenames>Anup</forenames></author><author><keyname>Pang</keyname><forenames>Guodong</forenames></author></authors><title>Ergodic control of multi-class $M/M/N+M$ queues in the Halfin-Whitt
  regime</title><categories>math.PR cs.SY math.OC</categories><comments>Published at http://dx.doi.org/10.1214/14-AAP1081 in the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP1081</report-no><journal-ref>Annals of Applied Probability 2015, Vol. 25, No. 6, 3511-3570</journal-ref><doi>10.1214/14-AAP1081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a dynamic scheduling problem for a multi-class queueing network with
a large pool of statistically identical servers. The arrival processes are
Poisson, and service times and patience times are assumed to be exponentially
distributed and class dependent. The optimization criterion is the expected
long time average (ergodic) of a general (nonlinear) running cost function of
the queue lengths. We consider this control problem in the Halfin-Whitt (QED)
regime, that is, the number of servers $n$ and the total offered load
$\mathbf{r}$ scale like $n\approx\mathbf{r}+\hat{\rho}\sqrt{\mathbf{r}}$ for
some constant $\hat{\rho}$. This problem was proposed in [Ann. Appl. Probab. 14
(2004) 1084-1134, Section 5.2]. The optimal solution of this control problem
can be approximated by that of the corresponding ergodic diffusion control
problem in the limit. We introduce a broad class of ergodic control problems
for controlled diffusions, which includes a large class of queueing models in
the diffusion approximation, and establish a complete characterization of
optimality via the study of the associated HJB equation. We also prove the
asymptotic convergence of the values for the multi-class queueing control
problem to the value of the associated ergodic diffusion control problem. The
proof relies on an approximation method by spatial truncation for the ergodic
control of diffusion processes, where the Markov policies follow a fixed
priority policy outside a fixed compact set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1958</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1958</id><created>2014-04-07</created><authors><author><keyname>Alizadeh</keyname><forenames>Mahnoosh</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Applebaum</keyname><forenames>Andy</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Levitt</keyname><forenames>Karl</forenames></author></authors><title>Scalable and Anonymous Modeling of Large Populations of Flexible
  Appliances</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To respond to volatility and congestion in the power grid, demand response
(DR) mechanisms allow for shaping the load compared to a base load profile.
When tapping on a large population of heterogeneous appliances as a DR
resource, the challenge is in modeling the dimensions available for control.
Such models need to strike the right balance between accuracy of the model and
tractability. The goal of this paper is to provide a medium-grained stochastic
hybrid model to represent a population of appliances that belong to two
classes: deferrable or thermostatically controlled loads. We preserve quantized
information regarding individual load constraints, while discarding information
about the identity of appliance owners. The advantages of our proposed
population model are 1) it allows us to model and control load in a scalable
fashion, useful for ex-ante planning by an aggregator or for real-time load
control; 2) it allows for the preservation of the privacy of end-use customers
that own submetered or directly controlled appliances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1972</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1972</id><created>2014-04-07</created><updated>2015-08-02</updated><authors><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author></authors><title>Regularization for Design</title><categories>math.OC cs.SY</categories><comments>Submitted to the IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When designing controllers for large-scale systems, the architectural aspects
of the controller such as the placement of actuators, sensors, and the
communication links between them can no longer be taken as given. The task of
designing this architecture is now as important as the design of the control
laws themselves. By interpreting controller synthesis (in a model matching
setup) as the solution of a particular linear inverse problem, we view the
challenge of obtaining a controller with a desired architecture as one of
finding a structured solution to an inverse problem. Building on this
conceptual connection, we formulate and analyze a framework called
\textit{Regularization for Design (RFD)}, in which we augment the variational
formulations of controller synthesis problems with convex penalty functions
that induce a desired controller architecture. The resulting regularized
formulations are convex optimization problems that can be solved efficiently,
these convex programs provide a unified computationally tractable approach for
the simultaneous co-design of a structured optimal controller and the
actuation, sensing and communication architecture required to implement it.
Further, these problems are natural control-theoretic analogs of prominent
approaches such as the Lasso, the Group Lasso, the Elastic Net, and others that
are employed in statistical modeling. In analogy to that literature, we show
that our approach identifies optimally structured controllers under a suitable
condition on a &quot;signal-to-noise&quot; type ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1978</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1978</id><created>2014-04-07</created><authors><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Bitar</keyname><forenames>Eilyan</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Vincent</keyname><forenames>Tyrone L.</forenames></author></authors><title>An Abrupt Change Detection Heuristic with Applications to Cyber Data
  Attacks on Power Systems</title><categories>math.DS cs.SY</categories><comments>to appear in the 2014 American Control Conference - ACC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analysis of a heuristic for abrupt change detection of systems
with bounded state variations. The proposed analysis is based on the Singular
Value Decomposition (SVD) of a history matrix built from system observations.
We show that monitoring the largest singular value of the history matrix can be
used as a heuristic for detecting abrupt changes in the system outputs. We
provide sufficient detectability conditions for the proposed heuristic. As an
application, we consider detecting malicious cyber data attacks on power
systems and test our proposed heuristic on the IEEE 39-bus testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1981</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1981</id><created>2014-04-07</created><authors><author><keyname>Uchoa</keyname><forenames>A. G. D.</forenames></author><author><keyname>Healy</keyname><forenames>C. T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Li</keyname><forenames>P.</forenames></author></authors><title>Iterative Detection and LDPC Decoding Algorithms for MIMO Systems in
  Block-Fading Channels</title><categories>cs.IT math.IT</categories><comments>10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an Iterative Detection and Decoding (IDD) scheme with Low Density
Parity Check (LDPC) codes for Multiple Input Multiple Output (MIMO) systems for
block-fading $F = 2$ and fast fading Rayleigh channels. An IDD receiver with
soft information processing that exploits the code structure and the behaviour
of the log likelihood ratios (LLR)'s is developed. Minimum Mean Square Error
(MMSE) with Successive Interference Cancellation (SIC) and with Parallel
Interference Cancellation (PIC) schemes are considered. The soft \textit{a
posteriori} output of the decoder in a block-fading channel with Root-Check
LDPC codes has allowed us to create a new strategy to improve the Bit Error
Rate (BER) of a MIMO IDD scheme. Our proposed strategy in some scenarios has
resulted in up to 3dB of gain in terms of BER for block-fading channels and up
to 1dB in fast fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1982</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1982</id><created>2014-04-07</created><authors><author><keyname>Samha</keyname><forenames>Amani K</forenames></author><author><keyname>Li</keyname><forenames>Yuefeng</forenames></author><author><keyname>Zhang</keyname><forenames>Jinglan</forenames></author></authors><title>Aspect-Based Opinion Extraction from Customer reviews</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text is the main method of communicating information in the digital age.
Messages, blogs, news articles, reviews, and opinionated information abound on
the Internet. People commonly purchase products online and post their opinions
about purchased items. This feedback is displayed publicly to assist others
with their purchasing decisions, creating the need for a mechanism with which
to extract and summarize useful information for enhancing the decision-making
process. Our contribution is to improve the accuracy of extraction by combining
different techniques from three major areas, named Data Mining, Natural
Language Processing techniques and Ontologies. The proposed framework
sequentially mines products aspects and users opinions, groups representative
aspects by similarity, and generates an output summary. This paper focuses on
the task of extracting product aspects and users opinions by extracting all
possible aspects and opinions from reviews using natural language, ontology,
and frequent (tag) sets. The proposed framework, when compared with an existing
baseline model, yielded promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1984</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1984</id><created>2014-04-07</created><authors><author><keyname>Gj&#xe6;re</keyname><forenames>Erlend Andreas</forenames><affiliation>SINTEF ICT</affiliation></author><author><keyname>Meland</keyname><forenames>Per H&#xe5;kon</forenames><affiliation>SINTEF ICT</affiliation></author></authors><title>Threats Management Throughout the Software Service Life-Cycle</title><categories>cs.SE cs.CR</categories><comments>In Proceedings GraMSec 2014, arXiv:1404.1634</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 148, 2014, pp. 1-14</journal-ref><doi>10.4204/EPTCS.148.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software services are inevitably exposed to a fluctuating threat picture.
Unfortunately, not all threats can be handled only with preventive measures
during design and development, but also require adaptive mitigations at
runtime. In this paper we describe an approach where we model composite
services and threats together, which allows us to create preventive measures at
design-time. At runtime, our specification also allows the service runtime
environment (SRE) to receive alerts about active threats that we have not
handled, and react to these automatically through adaptation of the composite
service. A goal-oriented security requirements modelling tool is used to model
business-level threats and analyse how they may impact goals. A process flow
modelling tool, utilising Business Process Model and Notation (BPMN) and
standard error boundary events, allows us to define how threats should be
responded to during service execution on a technical level. Throughout the
software life-cycle, we maintain threats in a centralised threat repository.
Re-use of these threats extends further into monitoring alerts being
distributed through a cloud-based messaging service. To demonstrate our
approach in practice, we have developed a proof-of-concept service for the Air
Traffic Management (ATM) domain. In addition to the design-time activities, we
show how this composite service duly adapts itself when a service component is
exposed to a threat at runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1985</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1985</id><created>2014-04-07</created><authors><author><keyname>Apvrille</keyname><forenames>Ludovic</forenames><affiliation>Institut Mines-Telecom, Telecom ParisTech, CNRS LTCI</affiliation></author><author><keyname>Roudier</keyname><forenames>Yves</forenames><affiliation>EURECOM</affiliation></author></authors><title>Towards the Model-Driven Engineering of Secure yet Safe Embedded Systems</title><categories>cs.SE cs.CR</categories><comments>In Proceedings GraMSec 2014, arXiv:1404.1634</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 148, 2014, pp. 15-30</journal-ref><doi>10.4204/EPTCS.148.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce SysML-Sec, a SysML-based Model-Driven Engineering environment
aimed at fostering the collaboration between system designers and security
experts at all methodological stages of the development of an embedded system.
A central issue in the design of an embedded system is the definition of the
hardware/software partitioning of the architecture of the system, which should
take place as early as possible. SysML-Sec aims to extend the relevance of this
analysis through the integration of security requirements and threats. In
particular, we propose an agile methodology whose aim is to assess early on the
impact of the security requirements and of the security mechanisms designed to
satisfy them over the safety of the system. Security concerns are captured in a
component-centric manner through existing SysML diagrams with only minimal
extensions. After the requirements captured are derived into security and
cryptographic mechanisms, security properties can be formally verified over
this design. To perform the latter, model transformation techniques are
implemented in the SysML-Sec toolchain in order to derive a ProVerif
specification from the SysML models. An automotive firmware flashing procedure
serves as a guiding example throughout our presentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1986</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1986</id><created>2014-04-07</created><authors><author><keyname>Paul</keyname><forenames>St&#xe9;phane</forenames><affiliation>Thales Research &amp; Technology</affiliation></author></authors><title>Towards Automating the Construction &amp; Maintenance of Attack Trees: a
  Feasibility Study</title><categories>cs.CR</categories><comments>In Proceedings GraMSec 2014, arXiv:1404.1634</comments><proxy>EPTCS</proxy><acm-class>K6.5</acm-class><journal-ref>EPTCS 148, 2014, pp. 31-46</journal-ref><doi>10.4204/EPTCS.148.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security risk management can be applied on well-defined or existing systems;
in this case, the objective is to identify existing vulnerabilities, assess the
risks and provide for the adequate countermeasures. Security risk management
can also be applied very early in the system's development life-cycle, when its
architecture is still poorly defined; in this case, the objective is to
positively influence the design work so as to produce a secure architecture
from the start. The latter work is made difficult by the uncertainties on the
architecture and the multiple round-trips required to keep the risk assessment
study and the system architecture aligned. This is particularly true for very
large projects running over many years. This paper addresses the issues raised
by those risk assessment studies performed early in the system's development
life-cycle. Based on industrial experience, it asserts that attack trees can
help solve the human cognitive scalability issue related to securing those
large, continuously-changing system-designs. However, big attack trees are
difficult to build, and even more difficult to maintain. This paper therefore
proposes a systematic approach to automate the construction and maintenance of
such big attack trees, based on the system's operational and logical
architectures, the system's traditional risk assessment study and a security
knowledge database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1987</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1987</id><created>2014-04-07</created><authors><author><keyname>Bauereiss</keyname><forenames>Thomas</forenames></author><author><keyname>Hutter</keyname><forenames>Dieter</forenames></author></authors><title>Possibilistic Information Flow Control for Workflow Management Systems</title><categories>cs.CR</categories><comments>In Proceedings GraMSec 2014, arXiv:1404.1634</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 148, 2014, pp. 47-62</journal-ref><doi>10.4204/EPTCS.148.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In workflows and business processes, there are often security requirements on
both the data, i.e. confidentiality and integrity, and the process, e.g.
separation of duty. Graphical notations exist for specifying both workflows and
associated security requirements. We present an approach for formally verifying
that a workflow satisfies such security requirements. For this purpose, we
define the semantics of a workflow as a state-event system and formalise
security properties in a trace-based way, i.e. on an abstract level without
depending on details of enforcement mechanisms such as Role-Based Access
Control (RBAC). This formal model then allows us to build upon well-known
verification techniques for information flow control. We describe how a
compositional verification methodology for possibilistic information flow can
be adapted to verify that a specification of a distributed workflow management
system satisfies security requirements on both data and processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1988</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1988</id><created>2014-04-07</created><authors><author><keyname>Prisacariu</keyname><forenames>Cristian</forenames><affiliation>University of Oslo</affiliation></author></authors><title>Actor Network Procedures as Psi-calculi for Security Ceremonies</title><categories>cs.CR</categories><comments>In Proceedings GraMSec 2014, arXiv:1404.1634</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 148, 2014, pp. 63-77</journal-ref><doi>10.4204/EPTCS.148.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The actor network procedures of Pavlovic and Meadows are a recent graphical
formalism developed for describing security ceremonies and for reasoning about
their security properties. The present work studies the relations of the actor
network procedures (ANP) to the recent psi-calculi framework. Psi-calculi is a
parametric formalism where calculi like spi- or applied-pi are found as
instances. Psi-calculi are operational and largely non-graphical, but have
strong foundation based on the theory of nominal sets and process algebras. One
purpose of the present work is to give a semantics to ANP through psi-calculi.
Another aim was to give a graphical language for a psi-calculus instance for
security ceremonies. At the same time, this work provides more insight into the
details of the ANPs formalization and the graphical representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1989</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1989</id><created>2014-04-07</created><authors><author><keyname>Vieira</keyname><forenames>Aitor Couce</forenames><affiliation>Secure-NOK AS, Norway</affiliation></author><author><keyname>Houmb</keyname><forenames>Siv Hilde</forenames><affiliation>Secure-NOK AS, Norway</affiliation></author><author><keyname>Insua</keyname><forenames>David Rios</forenames><affiliation>Royal Academy of Sciences, Spain</affiliation></author></authors><title>A Graphical Adversarial Risk Analysis Model for Oil and Gas Drilling
  Cybersecurity</title><categories>cs.CR cs.GT</categories><comments>In Proceedings GraMSec 2014, arXiv:1404.1634</comments><proxy>EPTCS</proxy><acm-class>K.6.5</acm-class><journal-ref>EPTCS 148, 2014, pp. 78-93</journal-ref><doi>10.4204/EPTCS.148.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oil and gas drilling is based, increasingly, on operational technology, whose
cybersecurity is complicated by several challenges. We propose a graphical
model for cybersecurity risk assessment based on Adversarial Risk Analysis to
face those challenges. We also provide an example of the model in the context
of an offshore drilling rig. The proposed model provides a more formal and
comprehensive analysis of risks, still using the standard business language
based on decisions, risks, and value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1990</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1990</id><created>2014-04-07</created><updated>2015-12-23</updated><authors><author><keyname>Botchkarev</keyname><forenames>Alexei</forenames></author></authors><title>Estimating the Accuracy of the Return on Investment (ROI) Performance
  Evaluations</title><categories>cs.CE</categories><journal-ref>Interdisciplinary Journal of Information, Knowledge, and
  Management, 2015, 10, 217-233</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Return on Investment (ROI) is one of the most popular performance measurement
and evaluation metrics. ROI analysis (when applied correctly) is a powerful
tool in comparing solutions and making informed decisions on the acquisitions
of information systems. The ROI sensitivity to error is a natural thought, and
common sense suggests that ROI evaluations cannot be absolutely accurate.
However, literature review revealed that in most publications and analyst firms
reports, this issue is just overlooked. On the one hand, the results of the ROI
calculations are implied to be produced with a mathematical rigor, possibility
of errors is not mentioned and amount of errors is not estimated. On the
contrary, another approach claims ROI evaluations to be absolutely inaccurate
because, in view of their authors, future benefits (especially, intangible)
cannot be estimated within any reasonable boundaries. The purpose of this study
is to provide a systematic research of the accuracy of the ROI evaluations in
the context of the information systems implementations. The main contribution
of the study is that this is the first systematic effort to evaluate ROI
accuracy. Analytical expressions have been derived for estimating errors of the
ROI evaluations. Results of the Monte Carlo simulation will help practitioners
in making informed decisions based on explicitly stated factors influencing the
ROI uncertainties. The results of this research are intended for researchers in
information systems, technology solutions and business management, and also for
information specialists, project managers, program managers, technology
directors, and information systems evaluators. Most results are applicable to
ROI evaluations in a wider subject area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1991</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1991</id><created>2014-04-07</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author><author><keyname>Quoc-Tuan</keyname><forenames>Nguyen</forenames></author></authors><title>Multiple-Symbol Differential Detection for Distributed Space-Time Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential distributed space-time coding (D-DSTC) technique has been
considered for relay networks to provide both diversity gain and high
throughput in the absence of channel state information. Conventional
differential detection (CDD) or two-symbol non-coherent detection over slow
-fading channels has been examined and shown to suffer 3-4 dB loss when
compared to coherent detections. Moreover, it has also been shown that the
performance of CDD severely degrades in fast-fading channels and an irreducible
error floor exists at high signal-to-noise ratio region. To overcome the error
floor experienced with fast-fading, a nearly optimal &quot;multiple-symbol&quot;
differential detection (MSDD) is developed in this paper. The MSDD algorithm
jointly processes a larger window of received signals for detection and
significantly improves the performance of D-DSTC in fast-fading channels. The
error performance of the MSDD algorithm is illustrated with simulation results
under different fading scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1995</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1995</id><created>2014-04-07</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Channel assignment for throughput maximization in cognitive radio
  networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.1674</comments><journal-ref>Wireless Communications and Networking Conference (WCNC), 2012
  IEEE , vol., no., pp.1427,1431, 1-4 April 2012</journal-ref><doi>10.1109/WCNC.2012.6214004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the channel allocation problem for throughput
maximization in cognitive radio networks with hardware-constrained secondary
users. Specifically, we assume that secondary users exploit spectrum holes on a
set of channels where each secondary user can use at most one available channel
for communication. We develop two channel assignment algorithms that can
efficiently utilize spectrum opportunities on these channels. In the first
algorithm, secondary users are assigned distinct sets of channels. We show that
this algorithm achieves the maximum throughput limit if the number of channels
is sufficiently large. In addition, we propose an overlapping channel
assignment algorithm, that can improve the throughput performance compared to
the non-overlapping channel assignment algorithm. In addition, we design a
distributed MAC protocol for access contention resolution and integrate the
derived MAC protocol overhead into the second channel assignment algorithm.
Finally, numerical results are presented to validate the theoretical results
and illustrate the performance gain due to the overlapping channel assignment
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1996</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1996</id><created>2014-04-07</created><authors><author><keyname>Phua</keyname><forenames>Clifton</forenames></author><author><keyname>Feng</keyname><forenames>Yuzhang</forenames></author><author><keyname>Ji</keyname><forenames>Junyao</forenames></author><author><keyname>Soh</keyname><forenames>Timothy</forenames></author></authors><title>Visual and Predictive Analytics on Singapore News: Experiments on GDELT,
  Wikipedia, and ^STI</title><categories>cs.OH</categories><comments>19 pages, 16 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The open-source Global Database of Events, Language, and Tone (GDELT) is the
most comprehensive and updated Big Data source of important terms extracted
from international news articles . We focus only on GDELT's Singapore events to
better understand the data quality of its news articles, accuracy of its term
extraction, and potential for prediction. To test news completeness and
validity, we visually compared GDELT (Singapore news articles' terms from 1979
to 2013) to Wikipedia's timeline of Singaporean history. To test term
extraction accuracy, we visually compared GDELT (CAMEO codes and TABARI system
of extraction from Singapore news articles' text from April to December 2013)
to SAS Text Miner's term and topic extraction. To perform predictive analytics,
we propose a novel feature engineering method to transform row-level GDELT from
articles to a user-specified temporal resolution. For example, we apply a
decision tree using daily counts of feature values from GDELT to predict
Singapore stock market's Straits Times Index (^STI). Of practical interest from
the above results is SAS Visual Analytics' ability to highlight the various
impacts of June 2013 Southeast Asian haze and December 2013 Little India riot
on Singapore. Although Singapore is unique as a sovereign city-state, a leading
financial centre, has strong international influence, and consists of a highly
multi-cultural population, the visual and predictive analytics reported here
are highly applicable to another country's GDELT data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1997</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1997</id><created>2014-04-07</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>General analytical framework for cooperative sensing and access
  trade-off optimization</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1404.1675</comments><journal-ref>Wireless Communications and Networking Conference (WCNC), 2013
  IEEE , vol., no., pp.1697,1702, 7-10 April 2013</journal-ref><doi>10.1109/WCNC.2013.6554819</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the joint cooperative spectrum sensing and
access design problem for multi-channel cognitive radio networks. A general
heterogeneous setting is considered where the probabilities that different
channels are available, SNRs of the signals received at secondary users (SUs)
due to transmissions from primary users (PUs) for different users and channels
can be different. We assume a cooperative sensing strategy with a general
a-out-of-b aggregation rule and design a synchronized MAC protocol so that SUs
can exploit available channels. We analyze the sensing performance and the
throughput achieved by the joint sensing and access design. Based on this
analysis, we develop algorithms to find optimal parameters for the sensing and
access protocols and to determine channel assignment for SUs to maximize the
system throughput. Finally, numerical results are presented to verify the
effectiveness of our design and demonstrate the relative performance of our
proposed algorithms and the optimal ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1998</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1998</id><created>2014-04-07</created><authors><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author></authors><title>A Light Discussion and Derivation of Entropy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expression for entropy sometimes appears mysterious - as it often is
asserted without justification. This short manuscript contains a discussion of
the underlying assumptions behind entropy as well as simple derivation of this
ubiquitous quantity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1999</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1999</id><created>2014-04-07</created><authors><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author></authors><title>Notes on Generalized Linear Models of Neurons</title><categories>cs.NE cs.LG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experimental neuroscience increasingly requires tractable models for
analyzing and predicting the behavior of neurons and networks. The generalized
linear model (GLM) is an increasingly popular statistical framework for
analyzing neural data that is flexible, exhibits rich dynamic behavior and is
computationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo et
al., 2005). What follows is a brief summary of the primary equations governing
the application of GLM's to spike trains with a few sentences linking this work
to the larger statistical literature. Latter sections include extensions of a
basic GLM to model spatio-temporal receptive fields as well as network activity
in an arbitrary numbers of neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2000</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2000</id><created>2014-04-07</created><authors><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author></authors><title>Notes on Kullback-Leibler Divergence and Likelihood</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kullback-Leibler (KL) divergence is a fundamental equation of information
theory that quantifies the proximity of two probability distributions. Although
difficult to understand by examining the equation, an intuition and
understanding of the KL divergence arises from its intimate relationship with
likelihood theory. We discuss how KL divergence arises from likelihood theory
in an attempt to provide some intuition and reserve a rigorous (but rather
simple) derivation for the appendix. Finally, we comment on recent applications
of KL divergence in the neural coding literature and highlight its natural
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2005</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2005</id><created>2014-04-08</created><authors><author><keyname>Chau</keyname><forenames>Duc Phu</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Bremond</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Thonnat</keyname><forenames>Monique</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Bak</keyname><forenames>Slawomir</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Automatic Tracker Selection w.r.t Object Detection Performance</title><categories>cs.CV</categories><comments>IEEE Winter Conference on Applications of Computer Vision (WACV 2014)
  (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tracking algorithm performance depends on video content. This paper
presents a new multi-object tracking approach which is able to cope with video
content variations. First the object detection is improved using Kanade-
Lucas-Tomasi (KLT) feature tracking. Second, for each mobile object, an
appropriate tracker is selected among a KLT-based tracker and a discriminative
appearance-based tracker. This selection is supported by an online tracking
evaluation. The approach has been experimented on three public video datasets.
The experimental results show a better performance of the proposed approach
compared to recent state of the art trackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2006</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2006</id><created>2014-04-08</created><updated>2015-03-25</updated><authors><author><keyname>Choi</keyname><forenames>Jaehyung</forenames></author><author><keyname>Mullhaupt</keyname><forenames>Andrew P.</forenames></author></authors><title>K\&quot;ahlerian information geometry for signal processing</title><categories>math.DG cs.IT cs.SY math.IT math.ST stat.TH</categories><comments>24 pages, published version</comments><journal-ref>Entropy 17(4), 1581-1605 (2015)</journal-ref><doi>10.3390/e17041581</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the correspondence between the information geometry of a signal
filter and a K\&quot;ahler manifold. The information geometry of a minimum-phase
linear system with a finite complex cepstrum norm is a K\&quot;ahler manifold. The
square of the complex cepstrum norm of the signal filter corresponds to the
K\&quot;ahler potential. The Hermitian structure of the K\&quot;ahler manifold is
explicitly emergent if and only if the impulse response function of the highest
degree in $z$ is constant in model parameters. The K\&quot;ahlerian information
geometry takes advantage of more efficient calculation steps for the metric
tensor and the Ricci tensor. Moreover, $\alpha$-generalization on the geometric
tensors is linear in $\alpha$. It is also robust to find Bayesian predictive
priors, such as superharmonic priors, because Laplace-Beltrami operators on
K\&quot;ahler manifolds are in much simpler forms than those of the non-K\&quot;ahler
manifolds. Several time series models are studied in the K\&quot;ahlerian
information geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2013</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2013</id><created>2014-04-08</created><authors><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author><author><keyname>Zhou</keyname><forenames>Michelle</forenames></author><author><keyname>Megiddo</keyname><forenames>Nimrod</forenames></author><author><keyname>Nichols</keyname><forenames>Jeffrey</forenames></author><author><keyname>Drews</keyname><forenames>Clemens</forenames></author></authors><title>Optimizing The Selection of Strangers To Answer Questions in Social
  Media</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millions of people express themselves on public social media, such as
Twitter. Through their posts, these people may reveal themselves as potentially
valuable sources of information. For example, real-time information about an
event might be collected through asking questions of people who tweet about
being at the event location. In this paper, we explore how to model and select
users to target with questions so as to improve answering performance while
managing the load on people who must be asked. We first present a feature-based
model that leverages users exhibited social behavior, including the content of
their tweets and social interactions, to characterize their willingness and
readiness to respond to questions on Twitter. We then use the model to predict
the likelihood for people to answer questions. To support real-world
information collection applications, we present an optimization-based approach
that selects a proper set of strangers to answer questions while achieving a
set of application-dependent objectives, such as achieving a desired number of
answers and minimizing the number of questions to be sent. Our cross-validation
experiments using multiple real-world data sets demonstrate the effectiveness
of our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2014</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2014</id><created>2014-04-08</created><authors><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author><author><keyname>Javed</keyname><forenames>Mohammed</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Entropy Computation of Document Images in Run-Length Compressed Domain</title><categories>cs.CV</categories><comments>Published in IEEE Proceedings 2014 Fifth International Conference on
  Signals and Image Processing</comments><journal-ref>In IEEE Proceedings 2014 Fifth International Conference on Signals
  and Image Processing, Pages 287-291</journal-ref><doi>10.1109/ICSIP.2014.51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression of documents, images, audios and videos have been traditionally
practiced to increase the efficiency of data storage and transfer. However, in
order to process or carry out any analytical computations, decompression has
become an unavoidable pre-requisite. In this research work, we have attempted
to compute the entropy, which is an important document analytic directly from
the compressed documents. We use Conventional Entropy Quantifier (CEQ) and
Spatial Entropy Quantifiers (SEQ) for entropy computations [1]. The entropies
obtained are useful in applications like establishing equivalence, word
spotting and document retrieval. Experiments have been performed with all the
data sets of [1], at character, word and line levels taking compressed
documents in run-length compressed domain. The algorithms developed are
computational and space efficient, and results obtained match 100% with the
results reported in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2019</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2019</id><created>2014-04-08</created><updated>2014-04-27</updated><authors><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Fineman</keyname><forenames>Jeremy T.</forenames></author><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author></authors><title>Cost-oblivious storage reallocation</title><categories>cs.DS</categories><comments>11 pages, 2 figures, to appear in PODS 2014. Added and updated
  references</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Databases need to allocate and free blocks of storage on disk. Freed blocks
introduce holes where no data is stored. Allocation systems attempt to reuse
such deallocated regions in order to minimize the footprint on disk. If
previously allocated blocks cannot be moved, the problem is called the memory
allocation problem, which is known to have a logarithmic overhead in the
footprint.
  This paper defines the storage reallocation problem, where previously
allocated blocks can be moved, or reallocated, but at some cost. The algorithms
presented here are cost oblivious, in that they work for a broad and reasonable
class of cost functions, even when they do not know what the cost function is.
  The objective is to minimize the storage footprint, that is, the largest
memory address containing an allocated object, while simultaneously minimizing
the reallocation costs. This paper gives asymptotically optimal algorithms for
storage reallocation, in which the storage footprint is at most (1+epsilon)
times optimal, and the reallocation cost is at most (1/epsilon) times the
original allocation cost, which is also optimal. The algorithms are cost
oblivious as long as the allocation/reallocation cost function is subadditive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2034</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2034</id><created>2014-04-08</created><authors><author><keyname>Alvarez</keyname><forenames>Victor</forenames></author><author><keyname>Schuhknecht</keyname><forenames>Felix Martin</forenames></author><author><keyname>Dittrich</keyname><forenames>Jens</forenames></author><author><keyname>Richter</keyname><forenames>Stefan</forenames></author></authors><title>Main Memory Adaptive Indexing for Multi-core Systems</title><categories>cs.DB</categories><comments>26 pages, 7 figures</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive indexing is a concept that considers index creation in databases as
a by-product of query processing; as opposed to traditional full index creation
where the indexing effort is performed up front before answering any queries.
Adaptive indexing has received a considerable amount of attention, and several
algorithms have been proposed over the past few years; including a recent
experimental study comparing a large number of existing methods. Until now,
however, most adaptive indexing algorithms have been designed single-threaded,
yet with multi-core systems already well established, the idea of designing
parallel algorithms for adaptive indexing is very natural. In this regard only
one parallel algorithm for adaptive indexing has recently appeared in the
literature: The parallel version of standard cracking. In this paper we
describe three alternative parallel algorithms for adaptive indexing, including
a second variant of a parallel standard cracking algorithm. Additionally, we
describe a hybrid parallel sorting algorithm, and a NUMA-aware method based on
sorting. We then thoroughly compare all these algorithms experimentally; along
a variant of a recently published parallel version of radix sort. Parallel
sorting algorithms serve as a realistic baseline for multi-threaded adaptive
indexing techniques. In total we experimentally compare seven parallel
algorithms. Additionally, we extensively profile all considered algorithms. The
initial set of experiments considered in this paper indicates that our parallel
algorithms significantly improve over previously known ones. Our results
suggest that, although adaptive indexing algorithms are a good design choice in
single-threaded environments, the rules change considerably in the parallel
case. That is, in future highly-parallel environments, sorting algorithms could
be serious alternatives to adaptive indexing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2037</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2037</id><created>2014-04-08</created><authors><author><keyname>Lindeberg</keyname><forenames>Tony</forenames></author><author><keyname>Friberg</keyname><forenames>Anders</forenames></author></authors><title>Idealized computational models for auditory receptive fields</title><categories>cs.SD q-bio.NC</categories><comments>55 pages, 22 figures, 3 tables</comments><journal-ref>PLoS ONE 10(3):e0119032:1-58, 2015</journal-ref><doi>10.1371/journal.pone.0119032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a theory by which idealized models of auditory receptive
fields can be derived in a principled axiomatic manner, from a set of
structural properties to enable invariance of receptive field responses under
natural sound transformations and ensure internal consistency between
spectro-temporal receptive fields at different temporal and spectral scales.
  For defining a time-frequency transformation of a purely temporal sound
signal, it is shown that the framework allows for a new way of deriving the
Gabor and Gammatone filters as well as a novel family of generalized Gammatone
filters, with additional degrees of freedom to obtain different trade-offs
between the spectral selectivity and the temporal delay of time-causal temporal
window functions.
  When applied to the definition of a second-layer of receptive fields from a
spectrogram, it is shown that the framework leads to two canonical families of
spectro-temporal receptive fields, in terms of spectro-temporal derivatives of
either spectro-temporal Gaussian kernels for non-causal time or the combination
of a time-causal generalized Gammatone filter over the temporal domain and a
Gaussian filter over the logspectral domain. For each filter family, the
spectro-temporal receptive fields can be either separable over the
time-frequency domain or be adapted to local glissando transformations that
represent variations in logarithmic frequencies over time. Within each domain
of either non-causal or time-causal time, these receptive field families are
derived by uniqueness from the assumptions.
  It is demonstrated how the presented framework allows for computation of
basic auditory features for audio processing and that it leads to predictions
about auditory receptive fields with good qualitative similarity to biological
receptive fields measured in the inferior colliculus (ICC) and primary auditory
cortex (A1) of mammals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2041</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2041</id><created>2014-04-08</created><updated>2015-06-09</updated><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>On the Complexity of Computing an Equilibrium in Combinatorial Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study combinatorial auctions where each item is sold separately but
simultaneously via a second price auction. We ask whether it is possible to
efficiently compute in this game a pure Nash equilibrium with social welfare
close to the optimal one.
  We show that when the valuations of the bidders are submodular, in many
interesting settings (e.g., constant number of bidders, budget additive
bidders) computing an equilibrium with good welfare is essentially as easy as
computing, completely ignoring incentives issues, an allocation with good
welfare. On the other hand, for subadditive valuations, we show that computing
an equilibrium requires exponential communication. Finally, for XOS (a.k.a.
fractionally subadditive) valuations, we show that if there exists an efficient
algorithm that finds an equilibrium, it must use techniques that are very
different from our current ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2053</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2053</id><created>2014-04-08</created><authors><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Shah</keyname><forenames>Asadullah</forenames></author><author><keyname>Karabasi</keyname><forenames>Mustafa</forenames></author><author><keyname>Mahesar</keyname><forenames>Waheed</forenames></author></authors><title>Expression driven Trignometric based Procedural Animation of Quadrupeds</title><categories>cs.GR</categories><comments>6 pages, 8 figures, Conference paper</comments><journal-ref>In Proceedings of the International Conference on Informatics and
  Creative Multimedia 2103 (ICICM'13), pp.104,109, 4-6 Sept. 2013 IEEEXplore.
  UTM, KUALA LUMPUR (3-6 Sept)</journal-ref><doi>10.1109/ICICM.2013.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research paper addresses the problem of generating involuntary and
precise animation of quadrupeds with automatic rigging system of various
character types. The technique proposed through this research is based on a two
tier animation control curve with base simulation being driven through dynamic
mathematical model using procedural algorithm and the top layer with a custom
user controlled animation provided with intuitive Graphical User Interface
(GUI). The character rig is based on forward and inverse kinematics driven
through trigonometric based motion equations. The User is provided with various
manipulators and attributes to control and handle the locomotion gaits of the
characters and choose between various types of simulated motions from walking,
running, trotting, ambling and galloping with complete custom controls to
easily extend the base simulation as per requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2071</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2071</id><created>2014-04-08</created><authors><author><keyname>Dann&#xe9;lls</keyname><forenames>Dana</forenames></author><author><keyname>Gr&#x16b;z&#x12b;tis</keyname><forenames>Normunds</forenames></author></authors><title>Extracting a bilingual semantic grammar from FrameNet-annotated corpora</title><categories>cs.CL</categories><journal-ref>Proceedings of the 9th International Conference on Language
  Resources and Evaluation (LREC), 2014, pp. 2466-2473</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the creation of an English-Swedish FrameNet-based grammar in
Grammatical Framework. The aim of this research is to make existing framenets
computationally accessible for multilingual natural language applications via a
common semantic grammar API, and to facilitate the porting of such grammar to
other languages. In this paper, we describe the abstract syntax of the semantic
grammar while focusing on its automatic extraction possibilities. We have
extracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley
FrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The
abstract syntax defines 769 frame-specific valence patterns that cover 77.8%
examples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.
As a side result, we provide a unified method for comparing semantic and
syntactic valence patterns across framenets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2074</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2074</id><created>2014-04-08</created><updated>2015-03-28</updated><authors><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Renewable Powered Cellular Networks: Energy Field Modeling and Network
  Coverage</title><categories>cs.IT math.IT</categories><comments>double-column, 13 pages; to appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powering radio access networks using renewables, such as wind and solar
power, promises dramatic reduction in the network operation cost and the
network carbon footprints. However, the spatial variation of the energy field
can lead to fluctuations in power supplied to the network and thereby affects
its coverage. This warrants research on quantifying the aforementioned negative
effect and countermeasure techniques, motivating the current work. First, a
novel energy field model is presented, in which fixed maximum energy intensity
$\gamma$ occurs at Poisson distributed locations, called energy centers. The
intensities fall off from the centers following an exponential decay function
of squared distance and the energy intensity at an arbitrary location is given
by the decayed intensity from the nearest energy center. The product between
the energy center density and the exponential rate of the decay function,
denoted as $\psi$, is shown to determine the energy field distribution. Next,
the paper considers a cellular downlink network powered by harvesting energy
from the energy field and analyzes its network coverage. For the case of
harvesters deployed at the same sites as base stations (BSs), as $\gamma$
increases, the mobile outage probability is shown to scale as $(c
\gamma^{-\pi\psi}+p)$, where $p$ is the outage probability corresponding to a
flat energy field and $c$ a constant. Subsequently, a simple scheme is proposed
for counteracting the energy randomness by spatial averaging. Specifically,
distributed harvesters are deployed in clusters and the generated energy from
the same cluster is aggregated and then redistributed to BSs. As the cluster
size increases, the power supplied to each BS is shown to converge to a
constant proportional to the number of harvesters per BS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2076</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2076</id><created>2014-04-08</created><authors><author><keyname>Agarwal</keyname><forenames>Dr. Amit</forenames></author><author><keyname>Jain</keyname><forenames>Saloni</forenames></author></authors><title>Efficient Optimal Algorithm of Task Scheduling in Cloud Computing
  Environment</title><categories>cs.DC</categories><comments>6,1. published in IJCTT 2014 mARCH</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is an emerging technology in distributed computing which
facilitates pay per model as per user demand and requirement.Cloud consist of a
collection of virtual machine which includes both computational and storage
facility. The primary aim of cloud computing is to provide efficient access to
remote and geographically distributed resources. Cloud is developing day by day
and faces many challenges, one of them is scheduling. Scheduling refers to a
set of policies to control the order of work to be performed by a computer
system. A good scheduler adapts its scheduling strategy according to the
changing environment and the type of task. In this research paper we presented
a Generalized Priority algorithm for efficient execution of task and comparison
with FCFS and Round Robin Scheduling. Algorithm should be tested in cloud Sim
toolkit and result shows that it gives better performance compared to other
traditional scheduling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2078</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2078</id><created>2014-04-08</created><updated>2015-02-03</updated><authors><author><keyname>Broekens</keyname><forenames>Joost</forenames></author><author><keyname>Baarslag</keyname><forenames>Tim</forenames></author></authors><title>Optimistic Risk Perception in the Temporal Difference error Explains the
  Relation between Risk-taking, Gambling, Sensation-seeking and Low Fear</title><categories>cs.LG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the affective, cognitive and behavioural processes involved in
risk taking is essential for treatment and for setting environmental conditions
to limit damage. Using Temporal Difference Reinforcement Learning (TDRL) we
computationally investigated the effect of optimism in risk perception in a
variety of goal-oriented tasks. Optimism in risk perception was studied by
varying the calculation of the Temporal Difference error, i.e., delta, in three
ways: realistic (stochastically correct), optimistic (assuming action control),
and overly optimistic (assuming outcome control). We show that for the gambling
task individuals with 'healthy' perception of control, i.e., action optimism,
do not develop gambling behaviour while individuals with 'unhealthy' perception
of control, i.e., outcome optimism, do. We show that high intensity of
sensations and low levels of fear co-occur due to optimistic risk perception.
We found that overly optimistic risk perception (outcome optimism) results in
risk taking and in persistent gambling behaviour in addition to high intensity
of sensations. We discuss how our results replicate risk-taking related
phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2081</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2081</id><created>2014-04-08</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Ochs</keyname><forenames>Karlheinz</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Simultaneous Diagonalization: On the DoF Region of the K-user MIMO
  Multi-way Relay Channel</title><categories>cs.IT math.IT</categories><comments>6 pages, accepted for publication in EW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K-user MIMO Y-channel consisting of K users which want to exchange
messages among each other via a common relay node is studied in this paper. A
transmission strategy based on channel diagonalization using zero-forcing
beam-forming is proposed. This strategy is then combined with signal-space
alignment for network-coding, and the achievable degrees-of-freedom region is
derived. A new degrees-of-freedom outer bound is also derived and it is shown
that the proposed strategy achieves this outer bound if the users have more
antennas than the relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2083</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2083</id><created>2014-04-08</created><authors><author><keyname>Burnaev</keyname><forenames>Evgeny</forenames></author><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Efficiency of conformalized ridge regression</title><categories>cs.LG stat.ML</categories><comments>22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a method of producing prediction sets that can be
applied on top of a wide range of prediction algorithms. The method has a
guaranteed coverage probability under the standard IID assumption regardless of
whether the assumptions (often considerably more restrictive) of the underlying
algorithm are satisfied. However, for the method to be really useful it is
desirable that in the case where the assumptions of the underlying algorithm
are satisfied, the conformal predictor loses little in efficiency as compared
with the underlying algorithm (whereas being a conformal predictor, it has the
stronger guarantee of validity). In this paper we explore the degree to which
this additional requirement of efficiency is satisfied in the case of Bayesian
ridge regression; we find that asymptotically conformal prediction sets differ
little from ridge regression prediction intervals when the standard Bayesian
assumptions are satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2086</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2086</id><created>2014-04-08</created><updated>2014-11-20</updated><authors><author><keyname>Schmidt</keyname><forenames>Uwe</forenames></author><author><keyname>Jancsary</keyname><forenames>Jeremy</forenames></author><author><keyname>Nowozin</keyname><forenames>Sebastian</forenames></author><author><keyname>Roth</keyname><forenames>Stefan</forenames></author><author><keyname>Rother</keyname><forenames>Carsten</forenames></author></authors><title>Cascades of Regression Tree Fields for Image Restoration</title><categories>cs.CV</categories><comments>Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional random fields (CRFs) are popular discriminative models for
computer vision and have been successfully applied in the domain of image
restoration, especially to image denoising. For image deblurring, however,
discriminative approaches have been mostly lacking. We posit two reasons for
this: First, the blur kernel is often only known at test time, requiring any
discriminative approach to cope with considerable variability. Second, given
this variability it is quite difficult to construct suitable features for
discriminative prediction. To address these challenges we first show a
connection between common half-quadratic inference for generative image priors
and Gaussian CRFs. Based on this analysis, we then propose a cascade model for
image restoration that consists of a Gaussian CRF at each stage. Each stage of
our cascade is semi-parametric, i.e. it depends on the instance-specific
parameters of the restoration problem, such as the blur kernel. We train our
model by loss minimization with synthetically generated training data. Our
experiments show that when applied to non-blind image deblurring, the proposed
approach is efficient and yields state-of-the-art restoration quality on images
corrupted with synthetic and real blur. Moreover, we demonstrate its
suitability for image denoising, where we achieve competitive results for
grayscale and color images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2106</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2106</id><created>2014-04-08</created><updated>2015-05-04</updated><authors><author><keyname>Gundert</keyname><forenames>Anna</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>On Topological Minors in Random Simplicial Complexes</title><categories>math.CO cs.DM math.AT</categories><comments>15 pages</comments><msc-class>55U10, 05C80, 60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For random graphs, the containment problem considers the probability that a
binomial random graph $G(n,p)$ contains a given graph as a substructure. When
asking for the graph as a topological minor, i.e., for a copy of a subdivision
of the given graph, it is well-known that the (sharp) threshold is at $p=1/n$.
We consider a natural analogue of this question for higher-dimensional random
complexes $X^k(n,p)$, first studied by Cohen, Costa, Farber and Kappeler for
$k=2$.
  Improving previous results, we show that $p=\Theta(1/\sqrt{n})$ is the
(coarse) threshold for containing a subdivision of any fixed complete
$2$-complex. For higher dimensions $k&gt;2$, we get that $p=O(n^{-1/k})$ is an
upper bound for the threshold probability of containing a subdivision of a
fixed $k$-dimensional complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2115</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2115</id><created>2014-04-08</created><authors><author><keyname>Benammar</keyname><forenames>Bouchra</forenames></author><author><keyname>Thomas</keyname><forenames>Nathalie</forenames></author><author><keyname>Boucheret</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Poulliat</keyname><forenames>Charly</forenames></author><author><keyname>Dervin</keyname><forenames>Mathieu</forenames></author></authors><title>An efficient time domain representation for Single-Carrier Frequency
  Division Multiple Access</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a physical model for Single Carrier-Frequency Division
Mutliple Access (SC-FDMA). We specifically show that by using mutlirate signal
processing we derive a general time domain description of Localised SC-FDMA
systems relying on circular convolution. This general model has the advantage
of encompassing different implementations with flexible rates as well as
additional frequency precoding such as spectral shaping. Based on this
time-domain model, we study the Power Spectral Density (PSD) and the Signal to
Interference and Noise Ratio (SINR). Different implementations of SC-FDMA are
investigated and analytical expressions of both PSD and SINR compared to
simulations results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2116</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2116</id><created>2014-04-08</created><authors><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author></authors><title>Rational Counterfactuals</title><categories>cs.AI</categories><comments>To appear in Artificial Intelligence for Rational Decision Making
  (Springer-Verlag)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the concept of rational countefactuals which is an idea
of identifying a counterfactual from the factual (whether perceived or real)
that maximizes the attainment of the desired consequent. In counterfactual
thinking if we have a factual statement like: Saddam Hussein invaded Kuwait and
consequently George Bush declared war on Iraq then its counterfactuals is: If
Saddam Hussein did not invade Kuwait then George Bush would not have declared
war on Iraq. The theory of rational counterfactuals is applied to identify the
antecedent that gives the desired consequent necessary for rational decision
making. The rational countefactual theory is applied to identify the values of
variables Allies, Contingency, Distance, Major Power, Capability, Democracy, as
well as Economic Interdependency that gives the desired consequent Peace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2119</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2119</id><created>2014-04-08</created><authors><author><keyname>Ji</keyname><forenames>Yalei</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Bockelmann</keyname><forenames>Carsten</forenames></author><author><keyname>Dekorsy</keyname><forenames>Armin</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Characterization of Coded Random Access with Compressive Sensing based
  Multi-User Detection</title><categories>cs.IT math.IT</categories><comments>Submitted to Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of Machine-to-Machine (M2M) communication requires new Medium
Access Control (MAC) schemes and physical (PHY) layer concepts to support a
massive number of access requests. The concept of coded random access,
introduced recently, greatly outperforms other random access methods and is
inherently capable to take advantage of the capture effect from the PHY layer.
Furthermore, at the PHY layer, compressive sensing based multi-user detection
(CS-MUD) is a novel technique that exploits sparsity in multi-user detection to
achieve a joint activity and data detection. In this paper, we combine coded
random access with CS-MUD on the PHY layer and show very promising results for
the resulting protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2131</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2131</id><created>2014-04-08</created><authors><author><keyname>Chelli</keyname><forenames>Ali</forenames></author><author><keyname>Hadjtaieb</keyname><forenames>Amir</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Performance Analysis of Hybrid ARQ with Incremental Redundancy over
  Amplify-and-Forward Dual-Hop Relay Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a three node relay network comprising a source, a
relay, and a destination. The source transmits the message to the destination
using hybrid automatic repeat request (HARQ) with incremental redundancy (IR).
The relay overhears the transmitted message, amplifies it using a variable gain
amplifier, and then forwards the message to the destination. This latter
combines both the source and the relay message and tries to decode the
information. In case of decoding failure, the destination sends a negative
acknowledgement. A new replica of the message containing new parity bits is
then transmitted in the subsequent HARQ round. This process continues until
successful decoding occurs at the destination or a maximum number $M$ of rounds
is reached. We study the performance of HARQ-IR over the considered relay
channel from an information theoretic perspective. We derive exact expressions
and bounds for the information outage probability, the average number of
transmissions, and the average transmission rate. Moreover, we evaluate the
delay experienced by Poisson arriving packets over the considered relay
network. We also provide analytical expressions for the expected waiting time,
the sojourn time, and the energy efficiency. The derived exact expressions are
validated by Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2138</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2138</id><created>2014-04-08</created><authors><author><keyname>Lidanski</keyname><forenames>Eduardo</forenames></author><author><keyname>Nguyen</keyname><forenames>Supervisor- Giang</forenames></author></authors><title>Multiple-Tree Push-based Overlay Streaming</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-Tree Overlay Streaming has attracted a great amount of attention
from researchers in the past years. Multiple-tree streaming is a promising
alternative to single-tree streaming in terms of node dynamics and load
balancing, among others, which in turn addresses the perceived video quality by
the streaming user on node dynamics or when heterogeneous nodes join the
network. This article presents a comprehensive survey of the different
aproaches and techniques used in this research area. In this paper we identify
node-disjointness as the property most approaches aim to achieve. We also
present an alternative technique which does not try to achieve this but does
local optimizations aiming global optimizations. Thus, we identify this
property as not being absolute necessary for creating robust and heterogeneous
multi-tree overlays. We identify two main design goals: robustness and support
for heterogeneity, and classify existing approaches into these categories as
their main focus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2149</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2149</id><created>2014-04-08</created><updated>2014-08-28</updated><authors><author><keyname>Gallet</keyname><forenames>Matteo</forenames></author><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Bond theory for pentapods and hexapods</title><categories>cs.RO math.AG</categories><comments>17 pages, 1 figure</comments><msc-class>53A17, 14L35, 14P99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the old and classical problem of determining necessary
conditions for the overconstrained mobility of some mechanical device. In
particular, we show that the mobility of pentapods/hexapods implies either a
collinearity condition on the anchor points, or a geometric condition on the
normal projections of base and platform points. The method is based on a
specific compactification of the group of direct isometries of $\mathbb{R}^3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2151</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2151</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Timur</forenames></author><author><keyname>Benson</keyname><forenames>Bruce</forenames></author><author><keyname>Hillhouse</keyname><forenames>David</forenames></author><author><keyname>Lewis</keyname><forenames>Mickey</forenames></author></authors><title>Datacenter Changes vs. Employment Rates for Datacenter Managers In the
  Cloud Computing Era</title><categories>cs.CY</categories><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT) ISSN: 2221-0741 Vol. 3, No. 3, 65-69, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the evolving Cloud Computing paradigm, there is a prevailing concern
that in the near future data center managers may be in short supply. Cloud
computing, as a whole, is becoming more prevalent into today s computing world.
In fact, cloud computing has become so popular that some are now referring to
data centers as cloud centers. How does this interest in cloud computing
translate into employment rates for data center managers? The popularity of the
public and private cloud models are the prevailing force behind answering this
question. Therefore, the skill set of the datacenter manager has evolved to
harness the on demand self-services, broad network access, resource pooling,
rapid elasticity, measured service, and multi tenacity characteristics of cloud
computing. Using diverse sources ranging from the Bureau of Labor and
Statistics to trade articles, this manuscript takes an in-depth look at these
employment rates related to the cloud and the determining factors behind them.
Based on the information available, datacenter manager employment rates in the
cloud computing era will continue to increase well into 2016.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2153</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2153</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Timur</forenames></author><author><keyname>Gingo</keyname><forenames>Gerard</forenames></author><author><keyname>Stawchansky</keyname><forenames>Mike</forenames></author><author><keyname>White</keyname><forenames>Tracy</forenames></author></authors><title>Apple IOS Devices for Network Administrators</title><categories>cs.OH</categories><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT)ISSN: 2221-0741 Vol. 3, No. 6, 114-119, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As tablet devices continue to gain market share at the expense of the
traditional PC, they become a more integral part of the corporate landscape.
Tablets are no longer being utilized only by sales executives for presentation
purposes, or as addition to the traditional laptop. Users are attempting to
perform significant amounts of their daily work on tablet devices, some even
abandoning the ubiquitous laptop or desktop entirely. Operating exclusively
from a tablet device, specifically Apple IOS tablet devices creates unique
challenges in a corporate environment traditionally dominated by Microsoft
Windows operating systems. Interactions with file shares, presentation media,
VPN, and remote access present barriers that users and helpdesk support are
unfamiliar with in a relation to an iPad or iPhone. Many solutions are being
offered to these challenges some of which are analyzed by this manuscript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2155</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2155</id><created>2014-04-07</created><authors><author><keyname>Doostali</keyname><forenames>Saeed</forenames></author></authors><title>An Efficient Solution for Model Checking Abstract State Machine Using
  Bogor</title><categories>cs.SE</categories><comments>57 pages, 25 figures, 4 tables, Technical Report</comments><acm-class>F.4.3; D.2.4; D.3.1; F.1.1</acm-class><journal-ref>V. Rafe, S. Doostali, ASM2Bogor: An approach for verification of
  models specified through Asmeta language, Journal of Visual Languages and
  Computing 23 (5) (2012) 287-298</journal-ref><doi>10.1016/j.jvlc.2012.05.002</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Nowadays, publish subscribe and event based architecture are frequently used
for developing loosely coupled distributed systems. Hence, it is desirable to
find a proper solution to specify different systems through these
architectures. Abstract state machine (ASM) is a useful means to visually and
formally model publish subscribe and event based architectures. However,
modeling per se is not enough since the designers want to be able to verify the
designed models. As the model checking is a proper approach to verify software
and hardware systems. In this paper, we present an approach to verify ASM
models specified in terms of AsmetaL language using Bogor. In our approach, the
AsmetaL specification is automatically encoded to BIR, the input language of
the Bogor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2157</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2157</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author><author><keyname>Alvarez</keyname><forenames>Ramon</forenames></author></authors><title>Leveraging VMware vCloud Director Virtual Applications (vApps) for
  Operational Expense (OpEx) Efficiency</title><categories>cs.DC</categories><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT)ISSN: 2221-0741 Vol. 3, No. 9, 156-163, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization technology has provided many benefits to organizations, but it
cannot provide automation. This causes operational expenditure (OpEx)
inefficiencies, which are solved by cloud computing (vCloud Director vApps).
Organizations have adopted virtualization technology to reduce IT costs and
meet business needs. In addition to improved CapEx efficiency, virtualization
has enabled organizations to respond to business needs faster. While
virtualization has dramatically optimized core IT infrastructures,
organizations struggle to reduce OpEx costs. Because virtualization only
addresses server consolidation, administrators are faced with the manual and
resource-intensive day-to-day tasks of managing the rest of the data center:
networking, storage, user management. This manuscript presents details on how
leverage vApps based on a virtualized platform to improve CapEx efficiency in
today s data center. The combination of virtualization and cloud computing can
transform the data center into a dynamic, scalable, and agile resource capable
of achieving significant CapEx and OpEx cost savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2160</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2160</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Timur</forenames></author><author><keyname>Brockman</keyname><forenames>Craig</forenames></author></authors><title>SAP HANA and its performance benefits</title><categories>cs.DB</categories><comments>i-managers Journal on Information Technology, Vol. 2, No. 1 December
  2012 February 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-memory computing has changed the landscape of database technology. Within
the database and technology field, advancements occur over the course of time
that has had the capacity to transform some fundamental tenants of the
technology and how it is applied. The concept of Database Management Systems
(DBMS) was realized in industry during the 1960s, allowing users and developers
to use a navigational model to access the data stored by the computers of that
day as they grew in speed and capability. This manuscript is specifically
examines the SAPHigh Performance Analytics Appliance(HANA) approach, which is
one of the commonly used technologies today. Additionally, this manuscript
provides the analysis of the first two of the four common main usecases to
utilize SAP HANA's in-memory computing database technology. The performance
benefits are important factors for DB calculations.Some of the benefits are
quantified and the demonstrated by the defined sets of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2162</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2162</id><created>2014-04-08</created><authors><author><keyname>Kaes</keyname><forenames>Georg</forenames></author><author><keyname>Manger</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author><author><keyname>Vigne</keyname><forenames>Ralph</forenames></author></authors><title>The NNN Formalization: Review and Development of Guideline Specification
  in the Care Domain</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Due to an ageing society, it can be expected that less nursing personnel will
be responsible for an increasing number of patients in the future. One way to
address this challenge is to provide system-based support for nursing personnel
in creating, executing, and adapting patient care processes. In care practice,
these processes are following the general care process definition and
individually specified according to patient-specific data as well as diagnoses
and guidelines from the NANDA, NIC, and NOC (NNN) standards. In addition,
adaptations to running patient processes become necessary frequently and are to
be conducted by nursing personnel including NNN knowledge. In order to provide
semi-automatic support for design and adaption of care processes, a
formalization of NNN knowledge is indispensable. This technical report presents
the NNN formalization that is developed targeting at goals such as
completeness, flexibility, and later exploitation for creating and adapting
patient care processes. The formalization also takes into consideration an
extensive evaluation of existing formalization standards for clinical
guidelines. The NNN formalization as well as its usage are evaluated based on
case study FATIGUE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2163</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2163</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author><author><keyname>Sack</keyname><forenames>Lawton</forenames></author></authors><title>Webpage Load Speed: ASP.NET vs. PHP</title><categories>cs.PL cs.HC</categories><journal-ref>i-managers Journal on Information Technology, Vol. 2, No. 2, March
  May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As data transmission speeds over the Internet continue to increase, it is
necessary to research and identify technologies that can take advantage of the
increased speeds by enhancing the loading speed of webpages. One area of
consideration is found in the type of framework that is utilized for a website.
There are numerous frameworks that can be chosen from to be used to support a
website, each with their distinctive advantages. There are many different
opinions that have been tested on which framework should be used to fully
realize the optimization of page load speed. This manuscript examines and
implements testing methods for two popular frameworks, Active Server Pages .net
and PHP, to make a final determination of which framework is most beneficial
for webpage load speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2166</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2166</id><created>2014-04-08</created><authors><author><keyname>Dobson</keyname><forenames>Andrew</forenames></author><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Bekris</keyname><forenames>Kostas E.</forenames></author></authors><title>Sampling-based Roadmap Planners are Probably Near-Optimal after Finite
  Computation</title><categories>cs.RO</categories><comments>17 pages, 5 figures, 3 tables. Submitted to the Eleventh
  International Workshop on the Algorithmic Foundations of Robotics (WAFR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling-based motion planners have proven to be efficient solutions to a
variety of high-dimensional, geometrically complex motion planning problems
with applications in several domains. The traditional view of these approaches
is that they solve challenges efficiently by giving up formal guarantees and
instead attain asymptotic properties in terms of completeness and optimality.
Recent work has argued based on Monte Carlo experiments that these approaches
also exhibit desirable probabilistic properties in terms of completeness and
optimality after finite computation. The current paper formalizes these
guarantees. It proves a formal bound on the probability that solutions returned
by asymptotically optimal roadmap-based methods (e.g., PRM*) are within a bound
of the optimal path length I* with clearance {\epsilon} after a finite
iteration n. This bound has the form P(|In - I* | {\leq} {\delta}I*) {\leq}
Psuccess, where {\delta} is an error term for the length a path in the PRM*
graph, In. This bound is proven for general dimension Euclidean spaces and
evaluated in simulation. A discussion on how this bound can be used in
practice, as well as bounds for sparse roadmaps are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2167</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2167</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Timur</forenames></author></authors><title>Employing Virtualization for Information Technology Education</title><categories>cs.CY cs.DC</categories><comments>Technology Interface International Journal, Vol 12. No.1 Fall Winter
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript presents teaching and curriculum design for Information
Technology classes. Today, students demand hands-on activities for the newest
technologies. It is feasible to satisfy this appetite for exciting education by
employing server virtualization technologies to teach advanced concepts with
extensive hands-on assignments. Through utilization of virtualized servers,
students are able to deploy, secure and manage virtual machines and networks in
a contained environment. Various techniques, assessment tools and experiences
will be analyzed and presented by this manuscript. Previous teaching cases for
Information Systems or Information Technology classes are done using
non-commercial products, such as free VMware Server or VMware Player. Such
products have very limited functionality in terms of networking, storage and
resource management. Several advanced datacenter functions, such as Distributed
Power Management (DPM), vMotion and others, are not available in desktop
versions of that type of virtualization software. This manuscript introduces
the utilization of commercial software, such as vSphere 4.1, with full
datacenter functionality and operations for teaching Information Technology
classes of various levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2172</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2172</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author><author><keyname>White</keyname><forenames>Stacey</forenames></author></authors><title>The Role of Client Isolation in Protecting Wi-Fi Users from ARP Spoofing
  Attacks</title><categories>cs.NI cs.CR</categories><journal-ref>i-managers Journal on Information Technology, Vol. 1, No. 2, March
  May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates the role of the client isolation technology Public
Secure Packet Forwarding (PSPF) in defending 802.11 wireless (Wi-Fi) clients,
connected to a public wireless access point, from Address Resolution Protocol
(ARP)cache poisoning attacks, or ARP spoofing. Exploitation of wireless attack
vectors such as these have been on the rise and some have made national and
international news. Although client isolation technologies are common place in
most wireless access points, they are rarely enabled by default. Since an
average user generally has a limited understanding of IP networking concepts,
it is rarely enabled during access point configurations. Isolating wireless
clients from one another on unencrypted wireless networks is a simple and
potentially effective way of protection. The purpose of this research is to
determine if a commonly available and easily implementable wireless client
isolation security technology, such as PSPF, is an effective method for
defending wireless clients against attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2174</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2174</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author><author><keyname>Yang</keyname><forenames>Dr. Baijian</forenames></author></authors><title>Securing Virtualized Datacenters</title><categories>cs.CR cs.DC</categories><journal-ref>International Journal Of Engineering Research and Innovation |
  Vol. 2, No. 1, Spring 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization is a very popular solution to many problems in datacenter
management. It offers increased utilization of existing system resources
through effective consolidation, negating the need for more servers and
additional rack space. Furthermore, it offers essential capabilities in terms
of disaster recovery and potential savings on energy and maintenance costs.
However, these benefits may be tempered by the increased complexities of
securing virtual infrastructure. Do the benefits of virtualization outweigh the
risks? In this study, the authors evaluated the functionalities of the basic
components of virtual datacenters, identified the major risks to the data
infrastructure, and present here several solutions for overcoming potential
threats to virtual infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2176</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2176</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Synchronous replication of remote storage</title><categories>cs.NI cs.DC</categories><journal-ref>Journal of Communication and Computer, Mar. 2009, Volume 6, No.3
  (Serial No.52) ISSN 1548-7709</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storage replication is one of the essential requirements for network
environments. While many forms of Network Attached Storage (NAS), Storage Area
Networks (SAN) and other forms of network storage exist, there is a need for a
reliable synchronous storage replication technique between distant sites (less
than 1 mile). Such technology allows setting new standards for network failover
and failback systems for virtual servers; specifically, addressing the growing
need for effective disaster recovery (DR) planning. The purpose of this
manuscript is to identify newest technologies such as SAN/iQ and Storage
VMotion that allow for remote storage synchronous replication for virtual
servers. This study provides an analysis and a comparison of various SANs that
create solutions for enterprise needs. Additionally, the interoperability of
these technologies with the industry s leading product VMware ESX Server will
be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2187</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2187</id><created>2014-03-12</created><authors><author><keyname>Cohen</keyname><forenames>Ernie</forenames></author></authors><title>Coherent Causal Memory</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coherent causal memory (CCM) is causal memory in which prefixes of an
execution can be mapped to global memory states in a consistent way. While CCM
requires conflicting pairs of writes to be globally ordered, it allows writes
to remain unordered with respect to both reads and nonconflicting writes.
Nevertheless, it supports assertional, state-based program reasoning using
generalized Owicki-Gries proof outlines (where assertions can be attached to
any causal program edge). Indeed, we show that from a reasoning standpoint, CCM
differs from sequentially consistent (SC) memory only in that ghost code added
by the user is not allowed to introduce new write-write races.
  While CCM provides most of the formal reasoning leverage of SC memory, it is
much more efficiently implemented. As an illustration, we describe a simple
programming discipline that provides CCM on top of x86-TSO. The discipline is
considerably more relaxed than the one needed to ensure SC; for example, it
introduces no burden whatsoever for programs in which at most one thread writes
to any variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2188</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2188</id><created>2014-04-08</created><authors><author><keyname>Kalchbrenner</keyname><forenames>Nal</forenames></author><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>A Convolutional Neural Network for Modelling Sentences</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to accurately represent sentences is central to language
understanding. We describe a convolutional architecture dubbed the Dynamic
Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of
sentences. The network uses Dynamic k-Max Pooling, a global pooling operation
over linear sequences. The network handles input sentences of varying length
and induces a feature graph over the sentence that is capable of explicitly
capturing short and long-range relations. The network does not rely on a parse
tree and is easily applicable to any language. We test the DCNN in four
experiments: small scale binary and multi-class sentiment prediction, six-way
question classification and Twitter sentiment prediction by distant
supervision. The network achieves excellent performance in the first three
tasks and a greater than 25% error reduction in the last task with respect to
the strongest baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2195</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2195</id><created>2014-04-08</created><authors><author><keyname>Komenda</keyname><forenames>Jan</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>van Schuppen</keyname><forenames>Jan H.</forenames></author></authors><title>A Note on Relative Observability in Coordination Control</title><categories>math.OC cs.FL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1403.4762</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relative observability has been introduced and studied in the framework of
partially observed discrete-event systems as a condition stronger than
observability, but weaker than normality. However, unlike observability,
relative observability is closed under language unions, which makes it
interesting for practical applications. In this paper, we investigate this
notion in the framework of coordination control. We prove that conditional
normality is a stronger condition than conditional (strong) relative
observability, hence conditional strong relative observability can be used in
coordination control instead of conditional normality, and present a
distributive procedure for the computation of a conditionally controllable and
conditionally observable sublanguage of the specification that contains the
supremal conditionally strong relative observable sublanguage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2201</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2201</id><created>2014-04-08</created><authors><author><keyname>Newstadt</keyname><forenames>Gregory E.</forenames></author><author><keyname>Wei</keyname><forenames>Dennis L.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Resource-Constrained Adaptive Search and Tracking for Sparse Dynamic
  Targets</title><categories>cs.IT math.IT</categories><comments>49 pages, 1 table, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of resource-constrained and noise-limited
localization and estimation of dynamic targets that are sparsely distributed
over a large area. We generalize an existing framework [Bashan et al, 2008] for
adaptive allocation of sensing resources to the dynamic case, accounting for
time-varying target behavior such as transitions to neighboring cells and
varying amplitudes over a potentially long time horizon. The proposed adaptive
sensing policy is driven by minimization of a modified version of the
previously introduced ARAP objective function, which is a surrogate function
for mean squared error within locations containing targets. We provide
theoretical upper bounds on the performance of adaptive sensing policies by
analyzing solutions with oracle knowledge of target locations, gaining insight
into the effect of target motion and amplitude variation as well as sparsity.
Exact minimization of the multi-stage objective function is infeasible, but
myopic optimization yields a closed-form solution. We propose a simple
non-myopic extension, the Dynamic Adaptive Resource Allocation Policy (D-ARAP),
that allocates a fraction of resources for exploring all locations rather than
solely exploiting the current belief state. Our numerical studies indicate that
D-ARAP has the following advantages: (a) it is more robust than the myopic
policy to noise, missing data, and model mismatch; (b) it performs comparably
to well-known approximate dynamic programming solutions but at significantly
lower computational complexity; and (c) it improves greatly upon non-adaptive
uniform resource allocation in terms of estimation error and probability of
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2203</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2203</id><created>2014-03-12</created><authors><author><keyname>Li</keyname><forenames>Chunguo</forenames></author></authors><title>Sum-rate maximization of OFDMA femtocell networks that incorporates the
  QoS of macro mobile stations</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a power allocation scheme with co-channel allocation for
a femto base station (BS) that maximizes the sum-rate of its own femto mobile
stations (MSs) with a constraint that limits the degradation of quality of
service (QoS) of macro MSs. We have found a closed-form solution for the upper
limit on the transmission power of each sub-channel that satisfies the
constraint in a probabilistic sense. The proposed scheme is practical since it
uses only the information easily obtained by the femto BS. Moreover, our scheme
meets the constraint with minimal degradation compared to the optimal sum-rate
of the femto MSs achieved without the constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2226</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2226</id><created>2014-03-12</created><updated>2014-08-25</updated><authors><author><keyname>Ciss</keyname><forenames>Abdoul Aziz</forenames></author></authors><title>Two-sources Randomness Extractors for Elliptic Curves</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the task of two-sources randomness extractors for elliptic
curves defined over finite fields $K$, where $K$ can be a prime or a binary
field. In fact, we introduce new constructions of functions over elliptic
curves which take in input two random points from two differents subgroups. In
other words, for a ginven elliptic curve $E$ defined over a finite field
$\mathbb{F}_q$ and two random points $P \in \mathcal{P}$ and $Q\in
\mathcal{Q}$, where $\mathcal{P}$ and $\mathcal{Q}$ are two subgroups of
$E(\mathbb{F}_q)$, our function extracts the least significant bits of the
abscissa of the point $P\oplus Q$ when $q$ is a large prime, and the $k$-first
$\mathbb{F}_p$ coefficients of the asbcissa of the point $P\oplus Q$ when $q =
p^n$, where $p$ is a prime greater than $5$. We show that the extracted bits
are close to uniform.
  Our construction extends some interesting randomness extractors for elliptic
curves, namely those defined in \cite{op} and \cite{ciss1,ciss2}, when
$\mathcal{P} = \mathcal{Q}$. The proposed constructions can be used in any
cryptographic schemes which require extraction of random bits from two sources
over elliptic curves, namely in key exchange protole, design of strong
pseudo-random number generators, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2228</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2228</id><created>2014-04-06</created><updated>2015-12-12</updated><authors><author><keyname>Phung-Duc</keyname><forenames>Tuan</forenames></author></authors><title>Batch Arrival Multiserver Queue with Setup Time</title><categories>cs.PF math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Queues with setup time are extensively studied because they have application
in performance evaluation of power-saving data centers. In a data center, there
are a huge number of servers which consume a large amount of energy. In the
current technology, an idle server still consumes about 60\% of its peak
processing a job. Thus, the only way to save energy is to turn off servers
which are not processing a job. However, when there are some waiting jobs, we
have to turn on the OFF servers. A server needs some setup time to be active
during which it consumes energy but cannot process a job. Therefore, there
exists a trade-off between power consumption and delay performance. Gandhi et
al. \cite{Gandhi10a,Gandhi10} analyze this trade-off using an M/M/$c$ queue
with staggered setup (one server in setup at a time). In this paper, using an
alternative approach, we obtain generating functions for the joint stationary
distribution of the number of active servers and that of jobs in the system for
a more general model with batch arrivals and state-dependent setup time. We
further obtain moments for the queue size. Numerical results reveal that
keeping the same traffic intensity, the mean power consumption decreases with
the mean batch size for the case of fixed batch size. One of the main
theoretical contribution is a new conditional decomposition formula showing
that the number of waiting customers under the condition that all servers are
busy can be decomposed to the sum of two independent random variables where the
first is the same quantity in the corresponding model without setup time while
the second is the number of waiting customers before an arbitrary customer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2229</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2229</id><created>2014-04-08</created><updated>2014-06-04</updated><authors><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Harper</keyname><forenames>Chris</forenames></author><author><keyname>Leonards</keyname><forenames>Ute</forenames></author></authors><title>Towards the Safety of Human-in-the-Loop Robotics: Challenges and
  Opportunities for Safety Assurance of Robotic Co-Workers</title><categories>cs.RO cs.LG</categories><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of the human-robot co-worker team in a flexible manufacturing
environment where robots learn from demonstration heavily relies on the correct
and safe operation of the robot. How this can be achieved is a challenge that
requires addressing both technical as well as human-centric research questions.
In this paper we discuss the state of the art in safety assurance, existing as
well as emerging standards in this area, and the need for new approaches to
safety assurance in the context of learning machines. We then focus on robotic
learning from demonstration, the challenges these techniques pose to safety
assurance and indicate opportunities to integrate safety considerations into
algorithms &quot;by design&quot;. Finally, from a human-centric perspective, we stipulate
that, to achieve high levels of safety and ultimately trust, the robotic
co-worker must meet the innate expectations of the humans it works with. It is
our aim to stimulate a discussion focused on the safety aspects of
human-in-the-loop robotics, and to foster multidisciplinary collaboration to
address the research challenges identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2231</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2231</id><created>2014-04-08</created><authors><author><keyname>Cen</keyname><forenames>Feng</forenames></author></authors><title>Distributed Joint Source and Channel Coding with Low-Density
  Parity-Check Codes</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figure, extended version of the published journal paper</comments><journal-ref>Communications Letters, IEEE, vol.17, no.12, pp. 2336-2339, Dec.
  2013</journal-ref><doi>10.1109/LCOMM.2013.101613.131616</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LDPC) codes with the parity-based approach for
distributed joint source channel coding (DJSCC) with decoder side information
is described in this paper. The parity-based approach is theoretical limit
achievable. Different edge degree distributions are used for source variable
nodes and parity variable nodes. Particularly, the codeword-averaged density
evolution (CADE) is presented for asymmetrically correlated nonuniform sources
over the asymmetric memoryless transmission channel. Extensive simulations show
that the splitting of variable nodes can improve the coding efficiency of
suboptimal codes and lower the error floor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2233</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2233</id><created>2014-03-14</created><authors><author><keyname>Mowla</keyname><forenames>Md. Munjure</forenames></author><author><keyname>Hasan</keyname><forenames>S. M. Mahmud</forenames></author></authors><title>Performance Improvement of PAPR Reduction for OFDM Signal In LTE System</title><categories>cs.NI cs.IT math.IT</categories><comments>13 Pages, 8 Figures, 5 Tables, Published: International Journal of
  Wireless &amp; Mobile Networks (IJWMN), Vol. 5, No. 4, pp.35-47, August 2013
  (ISSN: 0975-3834). arXiv admin note: substantial text overlap with
  arXiv:1403.3349</comments><doi>10.5121/ijwmn.2013.5403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing (OFDM) is an emerging research
field of wireless communication. It is one of the most proficient multi-carrier
transmission techniques widely used today as broadband wired &amp; wireless
applications having several attributes such as provides greater immunity to
multipath fading &amp; impulse noise, eliminating inter symbol interference (ISI),
inter carrier interference (ICI) &amp; the need for equalizers. OFDM signals have a
general problem of high peak to average power ratio (PAPR) which is defined as
the ratio of the peak power to the average power of the OFDM signal. The
drawback of high PAPR is that the dynamic range of the power amplifier (PA) and
digital-to-analog converter (DAC). In this paper, an improved scheme of
amplitude clipping &amp; filtering method is proposed and implemented which shows
the significant improvement in case of PAPR reduction while increasing slight
BER compare to an existing method. Also, the comparative studies of different
parameters will be covered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2237</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2237</id><created>2014-02-02</created><authors><author><keyname>Kwiatkowska</keyname><forenames>Monika</forenames></author><author><keyname>Swierczewski</keyname><forenames>Lukasz</forenames></author></authors><title>Steganography - coding and intercepting the information from encoded
  pictures in the absence of any initial information</title><categories>cs.MM cs.CR cs.DC</categories><comments>10 pages, 5 figures, 5 tables, LVEE 2014 Conference Proceedings</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The work includes implementation and extraction algorithms capabilities test,
without any additional data (starting position, the number of bits used, gap
between the amount of data encoded) information from encoded files (mostly
images). The software is written using OpenMP standard [1], which allowed them
to run on parallel computers. Performance tests were carried out on computers,
Blue Gene/P [2], Blue Gene/Q [3] and the system consisting of four AMD Opteron
6272 [4]. Source code is available under GNU GPL v3 license and are available
in a repository OLib [5].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2258</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2258</id><created>2014-04-08</created><authors><author><keyname>Wang</keyname><forenames>Chenwei</forenames></author><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Genie Chains: Exploring Outer Bounds on the Degrees of Freedom of MIMO
  Interference Networks</title><categories>cs.IT math.IT</categories><comments>58 pages, 10 figures. This paper was presented in part at IEEE ISIT
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel genie chains approach to obtain information
theoretic degrees of freedom (DoF) outer bounds for MIMO wireless interference
networks. This new approach creates a chain of mappings from genie signals
provided to a receiver to the exposed signal spaces at that receiver, which
then serve as the genie signals for the next receiver in the chain subject to
certain linear independence requirements, essentially converting an information
theoretic DoF outer bound problem into a linear algebra problem. Several
applications of the genie chains approach are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2259</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2259</id><created>2014-04-08</created><authors><author><keyname>Nguyen</keyname><forenames>Luan Viet</forenames></author><author><keyname>Johnson</keyname><forenames>Taylor T.</forenames></author></authors><title>Virtual Prototyping and Distributed Control for Solar Array with
  Distributed Multilevel Inverter</title><categories>cs.DC cs.SY</categories><comments>Preprint draft under review, submitted on March 18, 2014 to IEEE
  Transactions on Energy Conversion, Special Issue: Advanced Distributed
  Control of Energy Conversion Devices and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the virtual prototyping of a solar array with a
grid-tie implemented as a distributed inverter and controlled using distributed
algorithms. Due to the distributed control and inherent redundancy in the array
composed of many panels and inverter modules, the virtual prototype exhibits
fault-tolerance capabilities. The distributed identifier algorithm allows the
system to keep track of the number of operating panels to appropriately
regulate the DC voltage output of the panels using buck-boost converters, and
determine appropriate switching times for H-bridges in the grid-tie. We
evaluate the distributed inverter, its control strategy, and fault-tolerance
through simulation in Simulink/Stateflow. Our virtual prototyping framework
allows for generating arrays and grid-ties consisting of many panels, and we
evaluate arrays of five to dozens of panels. Our analysis suggests the
achievable total harmonic distortion (THD) of the system may allow for
operating the array in spite of failures of the power electronics, control
software, and other subcomponents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2261</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2261</id><created>2014-04-08</created><authors><author><keyname>Nia</keyname><forenames>Mehran Alidoost</forenames></author><author><keyname>Ghorbani</keyname><forenames>Aida</forenames></author><author><keyname>Atani</keyname><forenames>Reza Ebrahimi</forenames></author></authors><title>A Novel Anonymous Cloud Architecture Design; Providing Secure Online
  Services and Electronic Payments</title><categories>cs.CR cs.DC cs.NI</categories><comments>in proceeding of the first international conference on Electronic
  Commerce and Economy, Tehran, April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymous cloud architecture provides secure environment for business and
also e-commerce approaches. By using this type of architecture, we can propose
anonymous online applications. Customers who need secure and reliable online
services should pay for provided services. A big problem is electronic payment
that is needed for billing customers. But customer identity should be remained
anonymous during and also after payment procedure. In this paper we propose a
novel and modified anonymous architecture that ensures customers that hide
their identity from others. This architecture is used from common network
protocols and we eliminate Tor anonymous service from architecture design space
because of independency. The here is introduced scalability parameter in
anonymous cloud architecture design space. After all we compare proposed
architecture with other popular cloud architectures in this range and we obtain
its advantages according to efficiency, security and anonymity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2266</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2266</id><created>2014-04-08</created><authors><author><keyname>Bonald</keyname><forenames>Thomas</forenames></author><author><keyname>Roberts</keyname><forenames>James</forenames></author></authors><title>Enhanced Cluster Computing Performance Through Proportional Fairness</title><categories>cs.PF cs.DC cs.NI</categories><comments>Submitted to Performance 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of cluster computing depends on how concurrent jobs share
multiple data center resource types like CPU, RAM and disk storage. Recent
research has discussed efficiency and fairness requirements and identified a
number of desirable scheduling objectives including so-called dominant resource
fairness (DRF). We argue here that proportional fairness (PF), long recognized
as a desirable objective in sharing network bandwidth between ongoing flows, is
preferable to DRF. The superiority of PF is manifest under the realistic
modelling assumption that the population of jobs in progress is a stochastic
process. In random traffic the strategy-proof property of DRF proves
unimportant while PF is shown by analysis and simulation to offer a
significantly better efficiency-fairness tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2267</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2267</id><created>2014-04-08</created><updated>2015-02-15</updated><authors><author><keyname>van der Helm</keyname><forenames>Peter A.</forenames></author></authors><title>Transparallel mind: Classical computing with quantum power</title><categories>cs.AI</categories><comments>38 pages (incl. Appendix with proofs), 10 figures, Supplementary
  Material (incl. algorithm) available at
  http://perswww.kuleuven.be/~u0084530/doc/pisa.html. Minor revision: added 2
  figures, 7 references, and a few clarifications</comments><acm-class>I.2.0; D.1.3; F.2.2</acm-class><doi>10.1007/s10462-015-9429-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the extraordinary computing power promised by quantum computers,
the quantum mind hypothesis postulated that quantum mechanical phenomena are
the source of neuronal synchronization, which, in turn, might underlie
consciousness. Here, I present an alternative inspired by a classical computing
method with quantum power. This method relies on special distributed
representations called hyperstrings. Hyperstrings are superpositions of up to
an exponential number of strings, which -- by a single-processor classical
computer -- can be evaluated in a transparallel fashion, that is,
simultaneously as if only one string were concerned. Building on a neurally
plausible model of human visual perceptual organization, in which hyperstrings
are formal counterparts of transient neural assemblies, I postulate that
synchronization in such assemblies is a manifestation of transparallel
information processing. This accounts for the high combinatorial capacity and
speed of human visual perceptual organization and strengthens ideas that
self-organizing cognitive architecture bridges the gap between neurons and
consciousness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2268</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2268</id><created>2014-04-09</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Yeung</keyname><forenames>Sai-Kit</forenames></author></authors><title>A Compact Linear Programming Relaxation for Binary Sub-modular MRF</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel compact linear programming (LP) relaxation for binary
sub-modular MRF in the context of object segmentation. Our model is obtained by
linearizing an $l_1^+$-norm derived from the quadratic programming (QP) form of
the MRF energy. The resultant LP model contains significantly fewer variables
and constraints compared to the conventional LP relaxation of the MRF energy.
In addition, unlike QP which can produce ambiguous labels, our model can be
viewed as a quasi-total-variation minimization problem, and it can therefore
preserve the discontinuities in the labels. We further establish a relaxation
bound between our LP model and the conventional LP model. In the experiments,
we demonstrate our method for the task of interactive object segmentation. Our
LP model outperforms QP when converting the continuous labels to binary labels
using different threshold values on the entire Oxford interactive segmentation
dataset. The computational complexity of our LP is of the same order as that of
the QP, and it is significantly lower than the conventional LP relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2269</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2269</id><created>2014-04-08</created><updated>2014-06-11</updated><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Improving soft FEC performance for higher-order modulations via
  optimized bit channel mappings</title><categories>cs.IT math.IT physics.optics</categories><comments>This paper was published in Optics Express and is made available as
  an electronic reprint with the permission of OSA. The paper can be found at
  the following URL on the OSA website:
  http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-22-12-14544</comments><journal-ref>Optics Express, Vol. 22, Issue 12, pp. 14544-14558 (2014)</journal-ref><doi>10.1364/OE.22.014544</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft forward error correction with higher-order modulations is often
implemented in practice via the pragmatic bit-interleaved coded modulation
paradigm, where a single binary code is mapped to a nonbinary modulation. In
this paper, we study the optimization of the mapping of the coded bits to the
modulation bits for a polarization-multiplexed fiber-optical system without
optical inline dispersion compensation. Our focus is on protograph-based
low-density parity-check (LDPC) codes which allow for an efficient hardware
implementation, suitable for high-speed optical communications. The
optimization is applied to the AR4JA protograph family, and further extended to
protograph-based spatially coupled LDPC codes assuming a windowed decoder. Full
field simulations via the split-step Fourier method are used to verify the
analysis. The results show performance gains of up to 0.25 dB, which translate
into a possible extension of the transmission reach by roughly up to 8%,
without significantly increasing the system complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2289</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2289</id><created>2014-04-08</created><updated>2014-11-26</updated><authors><author><keyname>Kim</keyname><forenames>Kangjin</forenames></author><author><keyname>Fainekos</keyname><forenames>Georgios E.</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Sriram</forenames></author></authors><title>On the Minimal Revision Problem of Specification Automata</title><categories>cs.SY cs.RO</categories><comments>23 pages, 16 figures, 2 tables, International Joural of Robotics
  Research 2014 Major Revision (submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As robots are being integrated into our daily lives, it becomes necessary to
provide guarantees on the safe and provably correct operation. Such guarantees
can be provided using automata theoretic task and mission planning where the
requirements are expressed as temporal logic specifications. However, in
real-life scenarios, it is to be expected that not all user task requirements
can be realized by the robot. In such cases, the robot must provide feedback to
the user on why it cannot accomplish a given task. Moreover, the robot should
indicate what tasks it can accomplish which are as &quot;close&quot; as possible to the
initial user intent. This paper establishes that the latter problem, which is
referred to as the minimal specification revision problem, is NP complete. A
heuristic algorithm is presented that can compute good approximations to the
Minimal Revision Problem (MRP) in polynomial time. The experimental study of
the algorithm demonstrates that in most problem instances the heuristic
algorithm actually returns the optimal solution. Finally, some cases where the
algorithm does not return the optimal solution are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2300</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2300</id><created>2014-03-14</created><authors><author><keyname>Mowla</keyname><forenames>Md. Munjure</forenames></author><author><keyname>Ali</keyname><forenames>Md. Yeakub</forenames></author><author><keyname>Suman</keyname><forenames>Abdulla Al</forenames></author></authors><title>Better Performance ACF Operation for PAPR Reduction of OFDM Signal</title><categories>cs.NI cs.IT math.IT</categories><comments>7 Pages, 6 Figures, 3 Tables; Published: American Academic &amp;
  Scholarly Research Journal, Vol. 6, No. 1, pp. 59-65, January 2014 (ISSN:
  2162-3228). arXiv admin note: substantial text overlap with arXiv:1403.3349,
  arXiv:1404.2233</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing (OFDM) is a promising modulation
radio access scheme for next generation wireless communication systems because
of its inherent immunity to multipath interference due to a low symbol rate,
the use of a cyclic prefix, and its affinity to different transmission
bandwidth arrangements. OFDM has already been adopted as a radio access scheme
for several of the latest cellular system specifications such as the long-term
evolution (LTE) system in the 3GPP (3rd Generation Partnership Project).
Nevertheless, peak-to-average power ratio (PAPR) of OFDM signal is a
significant drawback since it restricts the efficiency of the transmitter. A
number of promising approaches have been proposed &amp; implemented to reduce PAPR
with the expense of increase transmit signal power, bit error rate (BER) &amp;
computational complexity and data rate loss, etc. In this paper, a relatively
better scheme of amplitude clipping &amp; filtering operation (ACF) is proposed and
implemented which shows the significant improvement in case of PAPR reduction
while increasing slight BER compare to an present method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2302</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2302</id><created>2014-04-09</created><authors><author><keyname>Ghosh</keyname><forenames>Sutani</forenames></author></authors><title>Performance Analysis on The Basis of a Comparative Study Between
  Multipath Rayleigh Fading And AWGN Channel in The Presence of Various
  Interference</title><categories>cs.IT cs.NI math.IT</categories><comments>Journal Paper With 8 Pages (Number of Figures - 12)</comments><doi>10.5121/ijmnct.2014.4102</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Interference is the most important issue for present wireless communication.
There are various kinds of channel used in wireless communication. Here I want
to show a performance analysis on the basis of two different channels - AWGN
and Multipath Rayleigh fading channel. This is the comparative analysis with
different kinds of modulation techniques. Here I have also measured the Bit
Error Rate with respect to different modulation techniques and compare the rate
in different channels. My objective is to compare the different characteristics
of the transmitter and receiver for different types of channels and modulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2303</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2303</id><created>2014-04-08</created><authors><author><keyname>Gonnet</keyname><forenames>Pedro</forenames></author></authors><title>Efficient and Scalable Algorithms for Smoothed Particle Hydrodynamics on
  Hybrid Shared/Distributed-Memory Architectures</title><categories>cs.DC astro-ph.IM physics.comp-ph</categories><comments>Submitted to SIAM Journal on Scientific Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new fast and implicitly parallel approach to
neighbour-finding in multi-resolution Smoothed Particle Hydrodynamics (SPH)
simulations. This new approach is based on hierarchical cell decompositions and
sorted interactions, within a task-based formulation. It is shown to be faster
than traditional tree-based codes, and to scale better than domain
decomposition-based approaches on hybrid shared/distributed-memory parallel
architectures, e.g. clusters of multi-cores, achieving a $40\times$ speedup
over the Gadget-2 simulation code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2305</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2305</id><created>2014-02-20</created><authors><author><keyname>Pressacco</keyname><forenames>Flavio</forenames><affiliation>DIES</affiliation></author><author><keyname>Plazzotta</keyname><forenames>Giacomo</forenames><affiliation>DIES</affiliation></author><author><keyname>Ziani</keyname><forenames>Laura</forenames><affiliation>DIES</affiliation></author></authors><title>Twin relationships in Parsimonious Games: some results</title><categories>math.OC cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a vintage paper concerning Parsimonious games, a subset of constant sum
homogeneous weighted majority games, Isbell introduced a twin relationship
based on transposition properties of the incidence matrices upon minimal
winning coalitions of such games. A careful investigation of such properties
allowed the discovery of some results on twin games presented in this paper. In
detail we show that a) twin games have the same minimal winning quota and b)
each Parsimonious game admits a unique balanced lottery on minimal winning
coalitions, whose probabilities are given by the individual weights of its twin
game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2306</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2306</id><created>2014-02-17</created><authors><author><keyname>Reale</keyname><forenames>Cesco</forenames></author></authors><title>How much is convenient to defect? A method to estimate the cooperation
  probability in Prisoner's Dilemma and other games</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many cases the Nash equilibria are not predictive of the experimental
players' behaviour. For some games of Game Theory it is proposed here a method
to estimate the probabilities with which the different options will be actually
chosen by balanced players, i.e. players that are neither too competitive, nor
too cooperative. This will allow to measure the intrinsec cooperativeness
degree of a game, only in function of its payoffs. The method is shaped on the
Prisoner's Dilemma, then generalized for asymmetric tables, N players and N
options. It is adapted to other conditions like Chicken Game, Battle of the
Sexes, Stag Hunt and Translators (a new name proposed for a particular
condition). Then the method is applied to other games like Diner's Dilemma,
Public Goods Game, Traveler's Dilemma and War of Attrition. These games are so
analyzed in a probabilistic way that is consistent to what we could expect
intuitively, overcoming some known paradoxes of the Game Theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2313</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2313</id><created>2014-04-08</created><authors><author><keyname>Nakamura</keyname><forenames>Eita</forenames></author><author><keyname>Nakamura</keyname><forenames>Tomohiko</forenames></author><author><keyname>Saito</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Ono</keyname><forenames>Nobutaka</forenames></author><author><keyname>Sagayama</keyname><forenames>Shigeki</forenames></author></authors><title>Outer-Product Hidden Markov Model and Polyphonic MIDI Score Following</title><categories>cs.AI cs.SD</categories><comments>42 pages, 8 figures, version submitted to JNMR. To appear in Journal
  of New Music Research (2014)</comments><journal-ref>Journal of New Music Research, Vol. 43, No. 2 (2014) 183-201</journal-ref><doi>10.1080/09298215.2014.884145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polyphonic MIDI score-following algorithm capable of following
performances with arbitrary repeats and skips, based on a probabilistic model
of musical performances. It is attractive in practical applications of score
following to handle repeats and skips which may be made arbitrarily during
performances, but the algorithms previously described in the literature cannot
be applied to scores of practical length due to problems with large
computational complexity. We propose a new type of hidden Markov model (HMM) as
a performance model which can describe arbitrary repeats and skips including
performer tendencies on distributed score positions before and after them, and
derive an efficient score-following algorithm that reduces computational
complexity without pruning. A theoretical discussion on how much such
information on performer tendencies improves the score-following results is
given. The proposed score-following algorithm also admits performance mistakes
and is demonstrated to be effective in practical situations by carrying out
evaluations with human performances. The proposed HMM is potentially valuable
for other topics in information processing and we also provide a detailed
description of inference algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2314</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2314</id><created>2014-04-08</created><authors><author><keyname>Nakamura</keyname><forenames>Eita</forenames></author><author><keyname>Ono</keyname><forenames>Nobutaka</forenames></author><author><keyname>Sagayama</keyname><forenames>Shigeki</forenames></author><author><keyname>Watanabe</keyname><forenames>Kenji</forenames></author></authors><title>A Stochastic Temporal Model of Polyphonic MIDI Performance with
  Ornaments</title><categories>cs.AI cs.SD</categories><comments>32 pages, 5 figures, version submitted to JNMR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study indeterminacies in realization of ornaments and how they can be
incorporated in a stochastic performance model applicable for music information
processing such as score-performance matching. We point out the importance of
temporal information, and propose a hidden Markov model which describes it
explicitly and represents ornaments with several state types. Following a
review of the indeterminacies, they are carefully incorporated into the model
through its topology and parameters, and the state construction for quite
general polyphonic scores is explained in detail. By analyzing piano
performance data, we find significant overlaps in inter-onset-interval
distributions of chordal notes, ornaments, and inter-chord events, and the data
is used to determine details of the model. The model is applied for score
following and offline score-performance matching, yielding highly accurate
matching for performances with many ornaments and relatively frequent mistakes,
repeats, and skips.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2319</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2319</id><created>2014-04-08</created><updated>2015-03-06</updated><authors><author><keyname>Malestein</keyname><forenames>Justin</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author></authors><title>Ultrarigid periodic frameworks</title><categories>math.MG cs.CG math.CO</categories><comments>34 pages, 3 figures (v4, updated references and discussion, author
  contact data)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algebraic characterization of when a $d$-dimensional periodic
framework has no non-trivial, symmetry preserving, motion for any choice of
periodicity lattice. Our condition is decidable, and we provide a simple
algorithm that does not require complicated algebraic computations. In
dimension $d = 2$, we give a combinatorial characterization in the special case
when the the number of edge orbits is the minimum possible for ultrarigidity.
All our results apply to a fully flexible, fixed area, or fixed periodicity
lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2325</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2325</id><created>2014-04-08</created><authors><author><keyname>Clegg</keyname><forenames>Richard G.</forenames></author><author><keyname>Landa</keyname><forenames>Raul</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Jo&#xe3;o Taveira</forenames></author><author><keyname>Mykoniati</keyname><forenames>Eleni</forenames></author><author><keyname>Griffin</keyname><forenames>David</forenames></author><author><keyname>Rio</keyname><forenames>Miguel</forenames></author></authors><title>TARDIS: Stably shifting traffic in space and time (extended version)</title><categories>cs.NI</categories><comments>12 pages 9 figures. This is the full paper for the two page paper of
  the same name accepted as a short paper for SIGMETRICS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes TARDIS (Traffic Assignment and Retiming Dynamics with
Inherent Stability) which is an algorithmic procedure designed to reallocate
traffic within Internet Service Provider (ISP) networks. Recent work has
investigated the idea of shifting traffic in time (from peak to off-peak) or in
space (by using different links). This work gives a unified scheme for both
time and space shifting to reduce costs. Particular attention is given to the
commonly used 95th percentile pricing scheme.
  The work has three main innovations: firstly, introducing the Shapley
Gradient, a way of comparing traffic pricing between different links at
different times of day; secondly, a unified way of reallocating traffic in time
and/or in space; thirdly, a continuous approximation to this system is proved
to be stable. A trace-driven investigation using data from two service
providers shows that the algorithm can create large savings in transit costs
even when only small proportions of the traffic can be shifted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2329</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2329</id><created>2014-04-08</created><updated>2015-10-12</updated><authors><author><keyname>Giannakopoulos</keyname><forenames>Yiannis</forenames></author><author><keyname>Koutsoupias</keyname><forenames>Elias</forenames></author></authors><title>Duality Theory for Optimal Multidimensional Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general duality-theory framework for revenue maximization in
additive Bayesian auctions. The framework extends linear programming duality
and complementarity to constraints with partial derivatives. The dual system
reveals the geometric nature of the problem and highlights its connection with
the theory of bipartite graph matchings. We demonstrate the power of the
framework by applying it to a multiple-good monopoly setting where the buyer
has uniformly distributed valuations for the items, the canonical long-standing
open problem in the area. We propose a deterministic selling mechanism called
Straight-Jacket Auction (SJA) which we prove to be exactly optimal for up to 6
items, and conjecture its optimality for any number of goods. The duality
framework is used not only for proving optimality, but perhaps more
importantly, for deriving the optimal mechanism itself; as a result, SJA is
defined by natural geometric constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2334</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2334</id><created>2014-04-08</created><updated>2014-11-28</updated><authors><author><keyname>Gammell</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha S.</forenames></author><author><keyname>Barfoot</keyname><forenames>Timothy D.</forenames></author></authors><title>Informed RRT*: Optimal Sampling-based Path Planning Focused via Direct
  Sampling of an Admissible Ellipsoidal Heuristic</title><categories>cs.RO</categories><comments>8 pages, 11 figures. Videos available at
  https://www.youtube.com/watch?v=d7dX5MvDYTc and
  https://www.youtube.com/watch?v=nsl-5MZfwu4</comments><journal-ref>2014 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2014), pp. 2997-3004, 14-18 Sept. 2014</journal-ref><doi>10.1109/IROS.2014.6942976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapidly-exploring random trees (RRTs) are popular in motion planning because
they find solutions efficiently to single-query problems. Optimal RRTs (RRT*s)
extend RRTs to the problem of finding the optimal solution, but in doing so
asymptotically find the optimal path from the initial state to every state in
the planning domain. This behaviour is not only inefficient but also
inconsistent with their single-query nature.
  For problems seeking to minimize path length, the subset of states that can
improve a solution can be described by a prolate hyperspheroid. We show that
unless this subset is sampled directly, the probability of improving a solution
becomes arbitrarily small in large worlds or high state dimensions. In this
paper, we present an exact method to focus the search by directly sampling this
subset.
  The advantages of the presented sampling technique are demonstrated with a
new algorithm, Informed RRT*. This method retains the same probabilistic
guarantees on completeness and optimality as RRT* while improving the
convergence rate and final solution quality. We present the algorithm as a
simple modification to RRT* that could be further extended by more advanced
path-planning algorithms. We show experimentally that it outperforms RRT* in
rate of convergence, final solution cost, and ability to find difficult
passages while demonstrating less dependence on the state dimension and range
of the planning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2340</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2340</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Disaster Recovery Using Virtual Machines</title><categories>cs.DC</categories><journal-ref>Kabardino-Balkarian State University Journal, Technical Sciences
  Series, Edition 6. Publishing Department of KBSU. Nalchik 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, the importance of having 100% uptime for businesses and industries is
clear: financial reasons and often strict government regulations for certain
industries require 100% business continuity. The concept of business continuity
(BC), as Microsoft defines it: the ability of an organization to continue to
function even after a disastrous event, accomplished through the deployment of
redundant hardware and software, the use of fault tolerant systems, as well as
a solid backup and recovery strategy, directly relates to an organization s
ability to quickly restore and deploy IT backups and business operations in a
short period of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2342</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2342</id><created>2014-04-08</created><authors><author><keyname>Hsiao</keyname><forenames>Ko-Jen</forenames></author><author><keyname>Kulesza</keyname><forenames>Alex</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Social Collaborative Retrieval</title><categories>cs.IR</categories><comments>10 pages</comments><doi>10.1109/JSTSP.2014.2317286</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Socially-based recommendation systems have recently attracted significant
interest, and a number of studies have shown that social information can
dramatically improve a system's predictions of user interests. Meanwhile, there
are now many potential applications that involve aspects of both recommendation
and information retrieval, and the task of collaborative retrieval---a
combination of these two traditional problems---has recently been introduced.
Successful collaborative retrieval requires overcoming severe data sparsity,
making additional sources of information, such as social graphs, particularly
valuable. In this paper we propose a new model for collaborative retrieval, and
show that our algorithm outperforms current state-of-the-art approaches by
incorporating information from social networks. We also provide empirical
analyses of the ways in which cultural interests propagate along a social graph
using a real-world music dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2343</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2343</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Wireless Transmission of Video for Biomechanical Analysis</title><categories>cs.CE cs.MM cs.NI</categories><journal-ref>Information systems. Problems, perspectives, innovation approaches
  Volume 2. 2007. Saint Petersburg State University of Aerospace
  Instrumentation. ISBN 978-5-80888-0244-5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When there is a possibility to wirelessly stream video over a network, a
sophisticated computer analysis of the transmitted video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it using specific software such as Arial Performance Analysis Systems
or Dartfish. This manuscript presents some approaches and challenges in
streaming video as well as some applications of Information Technology in
biomechanics. An example of how scientists from Indiana State University
approached the wireless transmission of video is also introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2344</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2344</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Analysis of Computer Hardware Affecting Video Transmission via IEEE
  1394a connection</title><categories>cs.MM</categories><journal-ref>Information systems. Problems, perspectives, innovation
  approaches. Volume 2. 2007. Saint Petersburg State University of Aerospace
  Instrumentation. ISBN 978-5-80888-0244-5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When 60 de-interlaced fields per second digital uncompressed video is
streamed to a computer, some video fields are lost and not able to be stored on
a computer s hard drive successfully. Additionally, this problem amplifies once
multiple video sources are deployed. If it is possible to stream digital
uncompressed video without dropped video fields, then a sophisticated computer
analysis of the transmitted via IEEE 1394a connection video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it. If a loss of video fields occurs, then a quality analysis of
video is not possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2345</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2345</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Low Rate Wireless Personal Area Networks (LR-WPAN 802.15.4 standard)</title><categories>cs.NI</categories><journal-ref>Education for All June 25-29, 2005. pp.61-64. Saint Petersburg,
  Russia. ISBN 580880150-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript will provide a brief overview on Low Rate Wireless Personal
Area Networks (LR-WPAN) for 802.15.4 protocol standard which was approved by
IEEE Computer Society in May 2003. 802.15.4 standard presents some advantages
for structuring sensor networks and other types of applications that require
low rate communications. Some security considerations will also be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2346</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2346</id><created>2014-04-08</created><authors><author><keyname>Jovanovic</keyname><forenames>Dr. V.</forenames></author><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Teaching Network Storage Technology Assessment Outcomes and Directions</title><categories>cs.OH</categories><journal-ref>ACM 2008, New York, NY, USA ISBN: 978-1-60558-329-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents academic content, delivery and assessment mechanisms used,
available resources including initial lessons from teaching Networked Storage
Technology as a special topics course to students enrolled in two specific
programs - IT and CS. The course is based on the EMC s vendor-neutral Storage
Technology Fundamentals course. Furthermore, this manuscript provides a
detailed review of how the course fits into our curriculum, particularly, how
it helps achieving the 2008 ABET assessment requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2347</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2347</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Automation Security</title><categories>cs.OH</categories><journal-ref>Proceedings of the Second International Seminar, Saint Petersburg
  State University of Aerospace Instrumentation. January 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web-based Automated Process Control systems are a new type of applications
that use the Internet to control industrial processes with the access to the
real-time data. Supervisory control and data acquisition (SCADA) networks
contain computers and applications that perform key functions in providing
essential services and commodities (e.g., electricity, natural gas, gasoline,
water, waste treatment, transportation) to all Americans. As such, they are
part of the nation s critical infrastructure and require protection from a
variety of threats that exist in cyber space today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2348</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2348</id><created>2014-04-08</created><authors><author><keyname>Feng</keyname><forenames>Xiaojun</forenames></author><author><keyname>Lin</keyname><forenames>Peng</forenames></author><author><keyname>Zhang</keyname><forenames>Qian</forenames></author></authors><title>FlexAuc: Serving Dynamic Demands in a Spectrum Trading Market with
  Flexible Auction</title><categories>cs.NI cs.GT</categories><comments>11 pages, 7 figures, Preliminary version accepted in INFOCOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In secondary spectrum trading markets, auctions are widely used by spectrum
holders (SHs) to redistribute their unused channels to secondary wireless
service providers (WSPs). As sellers, the SHs design proper auction schemes to
stimulate more participants and maximize the revenue from the auction. As
buyers, the WSPs determine the bidding strategies in the auction to better
serve their end users.
  In this paper, we consider a three-layered spectrum trading market consisting
of the SH, the WSPs and the end users. We jointly study the strategies of the
three parties. The SH determines the auction scheme and spectrum supplies to
optimize its revenue. The WSPs have flexible bidding strategies in terms of
both demands and valuations considering the strategies of the end users. We
design FlexAuc, a novel auction mechanism for this market to enable dynamic
supplies and demands in the auction. We prove theoretically that FlexAuc not
only maximizes the social welfare but also preserves other nice properties such
as truthfulness and computational tractability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2352</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2352</id><created>2014-04-08</created><authors><author><keyname>Chowdhury</keyname><forenames>Mainak</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Low-complexity Decoding is Asymptotically Optimal in the SIMO MAC</title><categories>cs.IT math.IT</categories><comments>13 pages, 2 figures, submitted to IT Transactions</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A single input multiple output (SIMO) multiple access channel, with a large
number of transmitters sending symbols from a constellation to the receiver of
a multi-antenna base station, is considered. The fundamental limits of joint
decoding of the signals from all the users using a low complexity convex
relaxation of the maximum likelihood decoder (ML, constellation search) is
investigated. It has been shown that in a rich scattering environment, and in
the asymptotic limit of a large number of transmitters, reliable communication
is possible even without employing coding at the transmitters. This holds even
when the number of receiver antennas per transmitter is arbitrarily small, with
scaling behaviour arbitrarily close to what is achievable with coding. Thus,
the diversity of a large system not only makes the scaling law for coded
systems similar to that of uncoded systems, but, as we show, also allows
efficient decoders to realize close to the optimal performance of
maximum-likelihood decoding. However, while there is no performance loss
relative to the scaling laws of the optimal decoder, our proposed
low-complexity decoder exhibits a loss of the exponential or near-exponential
rates of decay of error probability relative to the optimal ML decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2353</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2353</id><created>2014-04-08</created><authors><author><keyname>Kurbatsky</keyname><forenames>Victor</forenames></author><author><keyname>Tomin</keyname><forenames>Nikita</forenames></author><author><keyname>Spiryaev</keyname><forenames>Vadim</forenames></author><author><keyname>Leahy</keyname><forenames>Paul</forenames></author><author><keyname>Sidorov</keyname><forenames>Denis</forenames></author><author><keyname>Zhukov</keyname><forenames>Alexei</forenames></author></authors><title>Power System Parameters Forecasting Using Hilbert-Huang Transform and
  Machine Learning</title><categories>cs.LG stat.ML</categories><msc-class>62M10, 91B84</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel hybrid data-driven approach is developed for forecasting power system
parameters with the goal of increasing the efficiency of short-term forecasting
studies for non-stationary time-series. The proposed approach is based on mode
decomposition and a feature analysis of initial retrospective data using the
Hilbert-Huang transform and machine learning algorithms. The random forests and
gradient boosting trees learning techniques were examined. The decision tree
techniques were used to rank the importance of variables employed in the
forecasting models. The Mean Decrease Gini index is employed as an impurity
function. The resulting hybrid forecasting models employ the radial basis
function neural network and support vector regression. Apart from introduction
and references the paper is organized as follows. The section 2 presents the
background and the review of several approaches for short-term forecasting of
power system parameters. In the third section a hybrid machine learning-based
algorithm using Hilbert-Huang transform is developed for short-term forecasting
of power system parameters. Fourth section describes the decision tree learning
algorithms used for the issue of variables importance. Finally in section six
the experimental results in the following electric power problems are
presented: active power flow forecasting, electricity price forecasting and for
the wind speed and direction forecasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2357</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2357</id><created>2014-04-08</created><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Multiple Access Analog Fountain Codes</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE International Symposium on Information Theory
  (ISIT), Honolulu, HI, USA, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel rateless multiple access scheme based on
the recently proposed capacity-approaching analog fountain code (AFC). We show
that the multiple access process will create an equivalent analog fountain
code, referred to as the multiple access analog fountain code (MA-AFC), at the
destination. Thus, the standard belief propagation (BP) decoder can be
effectively used to jointly decode all the users. We further analyse the
asymptotic performance of the BP decoder by using a density evolution approach
and show that the average log-likelihood ratio (LLR) of each user's information
symbol is proportional to its transmit signal to noise ratio (SNR), when all
the users utilize the same AFC code. Simulation results show that the proposed
scheme can approach the sum-rate capacity of the Gaussian multiple access
channel in a wide range of signal to noise ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2364</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2364</id><created>2014-04-09</created><authors><author><keyname>Kaushik</keyname><forenames>Dr Manju</forenames></author><author><keyname>Jain</keyname><forenames>Rashmi</forenames></author></authors><title>Gesture Based Interaction NUI: An Overview</title><categories>cs.HC</categories><comments>4 pages.&quot;Published with International Journal of Engineering Trends
  and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  9(12), March 2014. Published by Seventh Sense Research Group</journal-ref><doi>10.14445/22315381/IJETT-V9P319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Touch,face,voice recognition and movement sensors all are part of an emerging
field of computing often called natural user interface, or NUI. Interacting
with technology in these humanistic ways is no longer limited to high tech
secret agents. Gesture Touch, face, voice recognition and movement sensors all
are part of an emerging field of computing often called natural user interface,
or NUI. Interacting with technology in these humanistic ways is no longer
limited to high tech secret agents. Gesture recognition is the process by which
gestures formed by a user are made known to the system. In completely immersive
VR environments, the keyboard is generally not included, Technology
incorporates face, voice, gesture, and object recognition to give users a
variety of ways to interact with the console, all without needing a controller.
This paper focuses on the emerging way of human computer interaction, Gesture
recognition concept and gesture types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2366</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2366</id><created>2014-04-09</created><authors><author><keyname>Ni</keyname><forenames>Minming</forenames></author><author><keyname>Zheng</keyname><forenames>Lei</forenames></author><author><keyname>Tong</keyname><forenames>Fei</forenames></author><author><keyname>Pan</keyname><forenames>Jianping</forenames></author><author><keyname>Cai</keyname><forenames>Lin</forenames></author></authors><title>A Geometrical-Based Throughput Bound Analysis for Device-to-Device
  Communications in Cellular Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Journal on Selected Areas in Communications,
  Special Issue on D2D Communications in Cellular Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communications in cellular networks are promising
technologies for improving network throughput, spectrum efficiency, and
transmission delay. In this paper, we first introduce the concept of guard
distance to explore a proper system model for enabling multiple concurrent D2D
pairs in the same cell. Considering the Signal to Interference Ratio (SIR)
requirements for both macro-cell and D2D communications, a geometrical method
is proposed to obtain the guard distances from a D2D user equipment (DUE) to
the base station (BS), to the transmitting cellular user equipment (CUE), and
to other communicating D2D pairs, respectively, when the uplink resource is
reused. By utilizing the guard distances, we then derive the bounds of the
maximum throughput improvement provided by D2D communications in a cell.
Extensive simulations are conducted to demonstrate the impact of different
parameters on the optimal maximum throughput. We believe that the obtained
results can provide useful guidelines for the deployment of future cellular
networks with underlaying D2D communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2367</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2367</id><created>2014-04-09</created><updated>2015-02-15</updated><authors><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Detecting Possible Manipulators in Elections</title><categories>cs.MA cs.GT</categories><comments>Accepted in AAMAS 2015</comments><acm-class>F.2; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manipulation is a problem of fundamental importance in the context of voting
in which the voters exercise their votes strategically instead of voting
honestly to prevent selection of an alternative that is less preferred. The
Gibbard-Satterthwaite theorem shows that there is no strategy-proof voting rule
that simultaneously satisfies certain combinations of desirable properties.
Researchers have attempted to get around the impossibility results in several
ways such as domain restriction and computational hardness of manipulation.
However these approaches have been shown to have limitations. Since prevention
of manipulation seems to be elusive, an interesting research direction
therefore is detection of manipulation. Motivated by this, we initiate the
study of detection of possible manipulators in an election.
  We formulate two pertinent computational problems - Coalitional Possible
Manipulators (CPM) and Coalitional Possible Manipulators given Winner (CPMW),
where a suspect group of voters is provided as input to compute whether they
can be a potential coalition of possible manipulators. In the absence of any
suspect group, we formulate two more computational problems namely Coalitional
Possible Manipulators Search (CPMS), and Coalitional Possible Manipulators
Search given Winner (CPMSW). We provide polynomial time algorithms for these
problems, for several popular voting rules. For a few other voting rules, we
show that these problems are in NP-complete. We observe that detecting
manipulation maybe easy even when manipulation is hard, as seen for example, in
the case of the Borda voting rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2371</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2371</id><created>2014-04-09</created><authors><author><keyname>Bishop</keyname><forenames>Kayla</forenames></author><author><keyname>Hong</keyname><forenames>Hoon</forenames></author></authors><title>The Secant-Newton Map is Optimal Among Contracting $n^{th}$ Degree Maps
  for $n^{th}$ Root Computation</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem: given a real number $x$ and an error bound $\epsilon$,
find an interval such that it contains the $\sqrt[n]{x}$ and its width is less
than $\epsilon$. One way to solve the problem is to start with an initial
interval and to repeatedly update it by applying an interval refinement map on
it until it becomes narrow enough. In this paper, we prove that the well known
Secant-Newton map is optimal among a certain family of natural generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2374</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2374</id><created>2014-04-09</created><authors><author><keyname>Bhan</keyname><forenames>Ashish</forenames></author><author><keyname>Ray</keyname><forenames>Animesh</forenames></author></authors><title>A signature of power law network dynamics</title><categories>q-bio.QM cs.SI physics.soc-ph q-bio.MN</categories><comments>13 pages total, with 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can one hear the 'sound' of a growing network? We address the problem of
recognizing the topology of evolving biological or social networks. Starting
from percolation theory, we analytically prove a linear inverse relationship
between two simple graph parameters--the logarithm of the average cluster size
and logarithm of the ratio of the edges of the graph to the theoretically
maximum number of edges for that graph--that holds for all growing power law
graphs. The result establishes a novel property of evolving power-law networks
in the asymptotic limit of network size. Numerical simulations as well as
fitting to real-world citation co-authorship networks demonstrate that the
result holds for networks of finite sizes, and provides a convenient measure of
the extent to which an evolving family of networks belongs to the same
power-law class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2380</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2380</id><created>2014-04-09</created><updated>2014-04-23</updated><authors><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author></authors><title>A Direct Approach to Computing Spatially Averaged Outage Probability</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, accepted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter describes a direct method for computing the spatially averaged
outage probability of a network with interferers located according to a point
process and signals subject to fading. Unlike most common approaches, it does
not require transforms such as a Laplace transform. Examples show how to
directly obtain the outage probability in the presence of Rayleigh fading in
networks whose interferers are drawn from binomial and Poisson point processes
defined over arbitrary regions. We furthermore show that, by extending the
arbitrary region to the entire plane, the result for Poisson point processes
converges to the same expression found by Baccelli et al..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2387</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2387</id><created>2014-04-09</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author></authors><title>Fast Structuring of Radio Networks for Multi-Message Communications</title><categories>cs.NI cs.DC</categories><doi>10.1007/978-3-642-41527-2_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce collision free layerings as a powerful way to structure radio
networks. These layerings can replace hard-to-compute BFS-trees in many
contexts while having an efficient randomized distributed construction. We
demonstrate their versatility by using them to provide near optimal distributed
algorithms for several multi-message communication primitives.
  Designing efficient communication primitives for radio networks has a rich
history that began 25 years ago when Bar-Yehuda et al. introduced fast
randomized algorithms for broadcasting and for constructing BFS-trees. Their
BFS-tree construction time was $O(D \log^2 n)$ rounds, where $D$ is the network
diameter and $n$ is the number of nodes. Since then, the complexity of a
broadcast has been resolved to be $T_{BC} = \Theta(D \log \frac{n}{D} + \log^2
n)$ rounds. On the other hand, BFS-trees have been used as a crucial building
block for many communication primitives and their construction time remained a
bottleneck for these primitives.
  We introduce collision free layerings that can be used in place of BFS-trees
and we give a randomized construction of these layerings that runs in nearly
broadcast time, that is, w.h.p. in $T_{Lay} = O(D \log \frac{n}{D} +
\log^{2+\epsilon} n)$ rounds for any constant $\epsilon&gt;0$. We then use these
layerings to obtain: (1) A randomized algorithm for gathering $k$ messages
running w.h.p. in $O(T_{Lay} + k)$ rounds. (2) A randomized $k$-message
broadcast algorithm running w.h.p. in $O(T_{Lay} + k \log n)$ rounds. These
algorithms are optimal up to the small difference in the additive
poly-logarithmic term between $T_{BC}$ and $T_{Lay}$. Moreover, they imply the
first optimal $O(n \log n)$ round randomized gossip algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2393</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2393</id><created>2014-04-09</created><updated>2014-07-22</updated><authors><author><keyname>Moloudi</keyname><forenames>Saeedeh</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>Spatially Coupled Turbo Codes</title><categories>cs.IT math.IT</categories><comments>in Proc. 8th International Symposium on Turbo Codes &amp; Iterative
  Information Processing 2014, Bremen, Germany, August 2014. To appear. (The
  PCC ensemble is changed with respect to the one in the previous version of
  the paper. However, it gives identical thresholds)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the concept of spatially coupled turbo codes
(SC-TCs), as the turbo codes counterpart of spatially coupled low-density
parity-check codes. We describe spatial coupling for both Berrou et al. and
Benedetto et al. parallel and serially concatenated codes. For the binary
erasure channel, we derive the exact density evolution (DE) equations of SC-TCs
by using the method proposed by Kurkoski et al. to compute the decoding erasure
probability of convolutional encoders. Using DE, we then analyze the asymptotic
behavior of SC-TCs. We observe that the belief propagation (BP) threshold of
SC-TCs improves with respect to that of the uncoupled ensemble and approaches
its maximum a posteriori threshold. This phenomenon is especially significant
for serially concatenated codes, whose uncoupled ensemble suffers from a poor
BP threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2396</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2396</id><created>2014-04-09</created><updated>2014-06-13</updated><authors><author><keyname>Chiplunkar</keyname><forenames>Ashish</forenames></author><author><keyname>Vishwanathan</keyname><forenames>Sundar</forenames></author></authors><title>Approximating the Regular Graphic TSP in near linear time</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized approximation algorithm for computing traveling
salesperson tours in undirected regular graphs. Given an $n$-vertex,
$k$-regular graph, the algorithm computes a tour of length at most
$\left(1+\frac{7}{\ln k-O(1)}\right)n$, with high probability, in $O(nk \log
k)$ time. This improves upon a recent result by Vishnoi (\cite{Vishnoi12}, FOCS
2012) for the same problem, in terms of both approximation factor, and running
time. The key ingredient of our algorithm is a technique that uses
edge-coloring algorithms to sample a cycle cover with $O(n/\log k)$ cycles with
high probability, in near linear time.
  Additionally, we also give a deterministic
$\frac{3}{2}+O\left(\frac{1}{\sqrt{k}}\right)$ factor approximation algorithm
running in time $O(nk)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2399</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2399</id><created>2014-04-09</created><authors><author><keyname>Zhao</keyname><forenames>Dong</forenames></author><author><keyname>Ma</keyname><forenames>Huadong</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author></authors><title>Frugal Online Incentive Mechanisms for Crowdsourcing Tasks Truthfully</title><categories>cs.GT</categories><comments>14 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1306.5677</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Crowd Sensing (MCS) is a new paradigm which takes advantage of
pervasive smartphones to efficiently collect data, enabling numerous novel
applications. To achieve good service quality for a MCS application, incentive
mechanisms are necessary to attract more user participation. Most of existing
mechanisms apply only for the offline scenario where all users' information are
known a priori. On the contrary, we focus on a more realistic scenario where
users arrive one by one online in a random order. Based on the online auction
model, we investigate the problem that users submit their private profiles to
the crowdsourcer when they arrive, and the crowdsourcer aims at selecting a
subset of users before a specified deadline for minimizing the total payment
while a specific number of tasks can be completed.We design three online
mechanisms, Homo-OMZ, Hetero-OMZ and Hetero-OMG, all of which can satisfy the
computational efficiency, individual rationality, cost-truthfulness, and
consumer sovereignty. The Homo-OMZ mechanism is applicable to the homogeneous
user model and can satisfy the social efficiency but not constant frugality.
The Hetero-OMZ and Hetero-OMG mechanisms are applicable to both the homogeneous
and heterogeneous user models, and can satisfy the constant frugality. Besides,
the Hetero-OMG mechanism can also satisfy the time-truthfulness. Through
extensive simulations, we evaluate the performance and validate the theoretical
properties of our online mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2403</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2403</id><created>2014-04-09</created><updated>2014-06-28</updated><authors><author><keyname>Manzano</keyname><forenames>Marc</forenames></author><author><keyname>Sahneh</keyname><forenames>Faryad</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author><author><keyname>Calle</keyname><forenames>Eusebi</forenames></author><author><keyname>Marzo</keyname><forenames>Jose Luis</forenames></author></authors><title>Robustness surfaces of complex networks</title><categories>cs.SI</categories><comments>submitted to Scientific Reports</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the robustness of complex networks has been extensively studied in
the last decade, there still lacks a unifying framework able to embrace all the
proposed metrics. In the literature there are two open issues related to this
gap: (a) how to dimension several metrics to allow their summation and (b) how
to weight each of the metrics. In this work we propose a solution for the two
aforementioned problems by defining the $R^*$-value and introducing the concept
of \emph{robustness surface} ($\Omega$). The rationale of our proposal is to
make use of Principal Component Analysis (PCA). We firstly adjust to 1 the
initial robustness of a network. Secondly, we find the most informative
robustness metric under a specific failure scenario. Then, we repeat the
process for several percentage of failures and different realizations of the
failure process. Lastly, we join these values to form the robustness surface,
which allows the visual assessment of network robustness variability. Results
show that a network presents different robustness surfaces (i.e., dissimilar
shapes) depending on the failure scenario and the set of metrics. In addition,
the robustness surface allows the robustness of different networks to be
compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2409</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2409</id><created>2014-04-09</created><authors><author><keyname>Marin</keyname><forenames>Mircea</forenames></author><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Learning cover context-free grammars from structural data</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning an unknown context-free grammar when the
only knowledge available and of interest to the learner is about its structural
descriptions with depth at most $\ell.$ The goal is to learn a cover
context-free grammar (CCFG) with respect to $\ell$, that is, a CFG whose
structural descriptions with depth at most $\ell$ agree with those of the
unknown CFG. We propose an algorithm, called $LA^\ell$, that efficiently learns
a CCFG using two types of queries: structural equivalence and structural
membership. We show that $LA^\ell$ runs in time polynomial in the number of
states of a minimal deterministic finite cover tree automaton (DCTA) with
respect to $\ell$. This number is often much smaller than the number of states
of a minimum deterministic finite tree automaton for the structural
descriptions of the unknown grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2413</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2413</id><created>2014-04-09</created><authors><author><keyname>An</keyname><forenames>Fu-Tai</forenames></author><author><keyname>Hsueh</keyname><forenames>Yu-Li</forenames></author><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author><author><keyname>White</keyname><forenames>Ian M.</forenames></author><author><keyname>Kazovsky</keyname><forenames>Leonid G.</forenames></author></authors><title>A New Dynamic Bandwidth Allocation Protocol with Quality of Service in
  Ethernet-based Passive Optical Networks</title><categories>cs.NI</categories><comments>Proc. of IASTED International Conference on Wireless and Optical
  Communications (WOC 2003), Banff, Canada, Jul. 2003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ethernet-based Passive optical network (E-PON) is the key for next generation
access networks. It must have the property of high efficiency, low cost, and
support quality of service (QoS). We present a novel media access control (MAC)
protocol that maximizes network efficiency by using dynamic bandwidth
allocation (DBA) algorithm suitable for E-PON. This protocol minimizes packet
delay and delay variation for high priority traffic to ensure QoS. Simulation
results show excellent network throughput. Simulation results also show low
packet delay and packet delay variation for high priority traffic compare with
traditional MAC protocol of E-PON. When the network performs ranging, this
protocol ensures zero interruption of high priority traffic, such as audio or
video applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2415</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2415</id><created>2014-04-09</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>On The Evolution of PON-Based FTTH Solutions</title><categories>cs.NI</categories><comments>4 pages; 6 figures; (Invited paper) Proc. of JCIS 2002, Research
  Triangle Park, NC, USA, pp. 1402-1405, Mar. 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passive Optical Network (PON)-based Fiber-To-The-Home (FTTH) are promising
solutions that can break through the economic barrier of traditional
point-to-point solutions. Once fibers are deployed with PON-based FTTH
solutions, it becomes critical how to migrate to Wavelength Division
Multiplexing (WDM)-PON because Time Division Multiplexing (TDM) used in current
PON solutions cannot exploit the huge bandwidth of the optical fibers and
therefore will not be able to meet ever-increasing demands for higher bandwidth
by future network applications. In this paper we review and compare the current
PON-based FTTH solutions, ATM-PON (APON) and Ethernet PON (EPON), and provide a
possible evolution scenario to future WDM-PON.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2458</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2458</id><created>2014-04-09</created><updated>2016-01-22</updated><authors><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author><author><keyname>Yu</keyname><forenames>Jia Yuan</forenames></author></authors><title>r-Extreme Signalling for Congestion Control</title><categories>math.OC cs.AI cs.MA</categories><doi>10.1080/00207179.2016.1146968</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many &quot;smart city&quot; applications, congestion arises in part due to the
nature of signals received by individuals from a central authority. In the
model of Marecek et al. [arXiv:1406.7639, Int. J. Control 88(10), 2015], each
agent uses one out of multiple resources at each time instant. The per-use cost
of a resource depends on the number of concurrent users. A central authority
has up-to-date knowledge of the congestion across all resources and uses
randomisation to provide a scalar or an interval for each resource at each
time. In this paper, the interval to broadcast per resource is obtained by
taking the minima and maxima of costs observed within a time window of length
r, rather than by randomisation. We show that the resulting distribution of
agents across resources also converges in distribution, under plausible
assumptions about the evolution of the population over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2464</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2464</id><created>2014-04-09</created><authors><author><keyname>Guo</keyname><forenames>Jiong</forenames></author><author><keyname>Shrestha</keyname><forenames>Yash Raj</forenames></author><author><keyname>Yang</keyname><forenames>Yongjie</forenames></author></authors><title>How Credible is the Prediction of a Party-Based Election?</title><categories>cs.MA cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a party-based election system, the voters are grouped into parties and all
voters of a party are assumed to vote according to the party preferences over
the candidates. Hence, once the party preferences are declared the outcome of
the election can be determined. However, in the actual election, the members of
some &quot;instable&quot; parties often leave their own party to join other parties. We
introduce two parameters to measure the credibility of the prediction based on
party preferences: Min is the minimum number of voters leaving the instable
parties such that the prediction is no longer true, while Max is the maximum
number of voters leaving the instable parties such that the prediction remains
valid. Concerning the complexity of computing Min and Max, we consider both
positional scoring rules (Plurality, Veto, r-Approval and Borda) and
Condorcet-consistent rules (Copeland and Maximin). We show that for all
considered scoring rules, Min is polynomial-time computable, while it is
NP-hard to compute Min for Copeland and Maximin. With the only exception of
Borda, Max can be computed in polynomial time for other scoring rules. We have
NP-hardness results for the computation of Max under Borda, Maximin and
Copeland.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2465</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2465</id><created>2014-04-09</created><authors><author><keyname>Ruiz</keyname><forenames>Alfonso de la Fuente</forenames></author></authors><title>Quantum annealing</title><categories>cs.DS</categories><comments>21 pages, several figures from different authors. Seminar given in
  2013 at SBC (University of Shanghai for Science &amp; Technology)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Brief description on the state of the art of some local optimization methods:
Quantum annealing Quantum annealing (also known as alloy, crystallization or
tempering) is analogous to simulated annealing but in substitution of thermal
activation by quantum tunneling. The class of algorithmic methods for quantum
annealing (dubbed: 'QA'), sometimes referred by the italian school as Quantum
Stochastic Optimization ('QSO'), is a promising metaheuristic tool for solving
local search problems in multivariable optimization contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2471</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2471</id><created>2014-04-09</created><updated>2014-04-10</updated><authors><author><keyname>Bellini</keyname><forenames>Emanuele</forenames></author></authors><title>Yet another algorithm to compute the nonlinearity of a Boolean function</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We associate to each Boolean function a polynomial whose evaluations
represents the distances from all possible Boolean affine functions. Both
determining the coefficients of this polynomial from the truth table of the
Boolean function and computing its evaluation vector requires a worst-case
complexity of $O(n2^n)$ integer operations. This way, with a different
approach, we reach the same complexity of established algorithms, such as those
based on the fast Walsh transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2505</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2505</id><created>2014-04-09</created><updated>2014-06-14</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>de Moya-Aneg&#xf3;n</keyname><forenames>F&#xe9;lix</forenames></author><author><keyname>de Nooy</keyname><forenames>Wouter</forenames></author></authors><title>Aggregated journal-journal citation relations in Scopus and
  Web-of-Science matched and compared in terms of networks, maps, and
  interactive overlays</title><categories>cs.DL</categories><comments>the paper is accepted for publication in the Journal of the
  Association for Information Science and Technology (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the network of aggregated journal-journal citation relations
provided by the Journal Citation Reports (JCR) 2012 of the Science and Social
Science Citation Indexes (SCI and SSCI) with similar data based on Scopus 2012.
First, global maps were developed for the two sets separately; sets of
documents can then be compared using overlays to both maps. Using fuzzy-string
matching and ISSN numbers, we were able to match 10,524 journal names between
the two sets; that is, 96.4% of the 10,936 journals contained in JCR or 51.2%
of the 20,554 journals covered by Scopus. Network analysis was then pursued on
the set of journals shared between the two databases and the two sets of unique
journals. Citations among the shared journals are more comprehensively covered
in JCR than Scopus, so the network in JCR is denser and more connected than in
Scopus. The ranking of shared journals in terms of indegree (that is, numbers
of citing journals) or total citations is similar in both databases overall
(Spearman's \r{ho} &gt; 0.97), but some individual journals rank very differently.
Journals that are unique to Scopus seem to be less important--they are citing
shared journals rather than being cited by them--but the humanities are covered
better in Scopus than in JCR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2512</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2512</id><created>2014-04-07</created><authors><author><keyname>Jha</keyname><forenames>Vaibhav</forenames></author><author><keyname>Deol</keyname><forenames>Sunny</forenames></author><author><keyname>Jha</keyname><forenames>Mohit</forenames></author><author><keyname>Sharma</keyname><forenames>GK</forenames></author></authors><title>Energy and Latency Aware Application Mapping Algorithm &amp; Optimization
  for Homogeneous 3D Network on Chip</title><categories>cs.OH</categories><comments>15 pages, 11 figure, CCSEA 2014</comments><doi>10.5121/csit.2014.4302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency is one of the most critical issue in design of System on
Chip. In Network On Chip (NoC) based system, energy consumption is influenced
dramatically by mapping of Intellectual Property (IP) which affect the
performance of the system. In this paper we test the antecedently extant
proposed algorithms and introduced a new energy proficient algorithm stand for
3D NoC architecture. In addition a hybrid method has also been implemented
using bioinspired optimization (particle swarm optimization) technique. The
proposed algorithm has been implemented and evaluated on randomly generated
benchmark and real life application such as MMS, Telecom and VOPD. The
algorithm has also been tested with the E3S benchmark and has been compared
with the existing algorithm (spiral and crinkle) and has shown better reduction
in the communication energy consumption and shows improvement in the
performance of the system. Comparing our work with spiral and crinkle,
experimental result shows that the average reduction in communication energy
consumption is 19% with spiral and 17% with crinkle mapping algorithms, while
reduction in communication cost is 24% and 21% whereas reduction in latency is
of 24% and 22% with spiral and crinkle. Optimizing our work and the existing
methods using bio-inspired technique and having the comparison among them an
average energy reduction is found to be of 18% and 24%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2514</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2514</id><created>2014-04-06</created><updated>2015-05-21</updated><authors><author><keyname>Ghosh</keyname><forenames>Arnob</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author></authors><title>Quality Sensitive Price Competition in Spectrum Oligopoly:Part 1</title><categories>cs.GT</categories><comments>Accepted for publication in IEEE/ACM Transactions on Networking. 41
  pages single column format.Conference version is available at arXiv:1305.3351</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a spectrum oligopoly market where primaries lease their
channels to secondaries in lieu of financial remuneration. Transmission quality
of a channel evolves randomly. Each primary has to select the price it would
quote without knowing the transmission qualities of its competitors' channels.
Each secondary buys a channel depending on the price and the transmission
quality a channel offers. We formulate the price selection problem as a non
co-operative game with primaries as players. In the one-shot game, we show that
there exists a unique symmetric Nash Equilibrium(NE) strategy profile and
explicitly compute it. Our analysis reveals that under the NE strategy profile
a primary prices its channel to render high quality channel more preferable to
the secondary; this negates the popular belief that prices ought to be selected
to render channels equally preferable to the secondary regardless of their
qualities. We show the loss of revenue in the asymptotic limit due to the non
co-operation of primaries. In the repeated version of the game, we characterize
a subgame perfect NE where a primary can attain a payoff arbitrarily close to
the payoff it would obtain when primaries co-operate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2520</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2520</id><created>2014-04-05</created><updated>2014-09-08</updated><authors><author><keyname>Truong</keyname><forenames>Lan V.</forenames></author></authors><title>A Novel Time-Varying Coding Scheme for the Gaussian Broadcast Channel
  with Feedback</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we generalize a coding strategy for the Gaussian broadcast
channel with feedback, analyze error probabilities and achievable rate region
for this coding strategy by using the iterated random function theory [1], [2],
[6]. By changing some parameters in our scheme, we realize a variant of the
Ozarow-Leung's code for the general two-user broadcast channel with feedback. A
capacity-achieving coding scheme for the degraded broadcast channel with
feedback is drawn. More interestingly, we come to a coding scheme for the
symmetric Gaussian broadcast channel with feedback which achieves the same
sum-rate as the LQG code [18] and strictly outperforms the Kramer code [14].
The fact that our coding scheme is a variant of the Kramer's code or
time-varying posterior matching code [1] may give useful information to solve
some open problems proposed in [19], [20].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2537</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2537</id><created>2014-04-09</created><authors><author><keyname>Everett</keyname><forenames>Evan</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>A Signal-Space Analysis of Spatial Self-Interference Isolation for
  Full-Duplex Wireless</title><categories>cs.IT math.IT</categories><comments>To Appear at 2014 International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge to in-band full-duplex wireless communication is managing
self-interference. Many designs have employed spatial isolation mechanisms,
such as shielding or multi-antenna beamforming, to isolate the
self-interference wave from the receiver. Such spatial isolation methods are
effective, but by confining the transmit and receive signals to a subset of the
available space, the full spatial resources of the channel be under-utilized,
expending a cost that may nullify the net benefit of operating in full-duplex
mode. In this paper we leverage an antenna-theory-based channel model to
analyze the spatial degrees of freedom available to a full-duplex capable base
station, and observe that whether or not spatial isolation out-performs
time-division (i.e. half-duplex) depends heavily on the geometric distribution
of scatterers. Unless the angular spread of the objects that scatter to the
intended users is overlapped by the spread of objects that backscatter to the
base station, then spatial isolation outperforms time division, otherwise time
division may be optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2570</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2570</id><created>2014-04-09</created><updated>2014-05-28</updated><authors><author><keyname>Richier</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>Elazouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Altman</keyname><forenames>Tania</forenames></author><author><keyname>Linares</keyname><forenames>Georges</forenames></author><author><keyname>Portilla</keyname><forenames>Yonathan</forenames></author></authors><title>Modelling View-count Dynamics in YouTube</title><categories>cs.SI physics.soc-ph</categories><comments>Technical report, 10 pages. Added MER definition analysis. Added
  interval confidence intervals. Added prediction results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to study the behaviour of view-count in YouTube. We
first propose several bio-inspired models for the evolution of the view-count
of YouTube videos. We show, using a large set of empirical data, that the
view-count for 90% of videos in YouTube can indeed be associated to at least
one of these models, with a Mean Error which does not exceed 5%. We derive
automatic ways of classifying the view-count curve into one of these models and
of extracting the most suitable parameters of the model. We study empirically
the impact of videos' popularity and category on the evolution of its
view-count. We finally use the above classification along with the automatic
parameters extraction in order to predict the evolution of videos' view-count.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2571</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2571</id><created>2014-04-09</created><authors><author><keyname>Rajchl</keyname><forenames>Martin</forenames></author><author><keyname>Baxter</keyname><forenames>John S. H.</forenames></author><author><keyname>Qiu</keyname><forenames>Wu</forenames></author><author><keyname>Khan</keyname><forenames>Ali R.</forenames></author><author><keyname>Fenster</keyname><forenames>Aaron</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author></authors><title>RANCOR: Non-Linear Image Registration with Total Variation
  Regularization</title><categories>cs.CV</categories><comments>9 pages, 1 figure, technical note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization techniques have been widely used in deformable registration,
allowing for the incorporation of similarity metrics with regularization
mechanisms. These regularization mechanisms are designed to mitigate the
effects of trivial solutions to ill-posed registration problems and to
otherwise ensure the resulting deformation fields are well-behaved. This paper
introduces a novel deformable registration algorithm, RANCOR, which uses
iterative convexification to address deformable registration problems under
total-variation regularization. Initial comparative results against four
state-of-the-art registration algorithms are presented using the Internet Brain
Segmentation Repository (IBSR) database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2576</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2576</id><created>2014-04-09</created><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author></authors><title>Asymptotics of Fingerprinting and Group Testing: Tight Bounds from
  Channel Capacities</title><categories>cs.IT cs.CR math.IT</categories><comments>14 pages, 6 figures</comments><journal-ref>IEEE Transactions on Information Forensics and Security, vol. 10,
  no. 9, pp. 1967-1980, 2015</journal-ref><doi>10.1109/TIFS.2015.2440190</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the large-coalition asymptotics of various
fingerprinting and group testing games, and derive explicit expressions for the
capacities for each of these models. We do this both for simple decoders (fast
but suboptimal) and for joint decoders (slow but optimal).
  For fingerprinting, we show that if the pirate strategy is known, the
capacity often decreases linearly with the number of colluders, instead of
quadratically as in the uninformed fingerprinting game. For many attacks the
joint capacity is further shown to be strictly higher than the simple capacity.
  For group testing, we improve upon known results about the joint capacities,
and derive new explicit asymptotics for the simple capacities. These show that
existing simple group testing algorithms are suboptimal, and that simple
decoders cannot asymptotically be as efficient as joint decoders. For the
traditional group testing model, we show that the gap between the simple and
joint capacities is a factor 1.44 for large numbers of defectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2584</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2584</id><created>2014-04-09</created><authors><author><keyname>Amor</keyname><forenames>Selma Belhadj</forenames></author><author><keyname>Steinberg</keyname><forenames>Yossef</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>MIMO MAC-BC Duality with Linear-Feedback Coding Schemes</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for the multi-antenna Gaussian multi-access channel (MAC) and
broadcast channel (BC) with perfect feedback, the rate regions achieved by
linear-feedback coding schemes (called linear-feedback capacity regions)
coincide when the same total input-power constraint is imposed on both channels
and when the MAC channel matrices are the transposes of the BC channel
matrices. Such a pair of MAC and BC is called dual. We also identify
sub-classes of linear-feedback coding schemes that achieve the linear-feedback
capacity regions of these two channels and present multi-letter expressions for
the linear-feedback capacity regions. Moreover, within the two sub-classes of
coding schemes that achieve the linear-feedback capacity regions for a given
MAC and its dual BC, we identify for each MAC scheme a BC scheme and for each
BC scheme a MAC scheme so that the two schemes have same total input power and
achieve the same rate regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2590</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2590</id><created>2014-04-09</created><updated>2014-05-06</updated><authors><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author><author><keyname>Crimaldi</keyname><forenames>Irene</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author><author><keyname>Trapin</keyname><forenames>Luca</forenames></author></authors><title>Cluster analysis of weighted bipartite networks: a new copula-based
  approach</title><categories>physics.data-an cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0109507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we are interested in identifying clusters of &quot;positional
equivalent&quot; actors, i.e. actors who play a similar role in a system. In
particular, we analyze weighted bipartite networks that describes the
relationships between actors on one side and features or traits on the other,
together with the intensity level to which actors show their features. The main
contribution of our work is twofold. First, we develop a methodological
approach that takes into account the underlying multivariate dependence among
groups of actors. The idea is that positions in a network could be defined on
the basis of the similar intensity levels that the actors exhibit in expressing
some features, instead of just considering relationships that actors hold with
each others. Second, we propose a new clustering procedure that exploits the
potentiality of copula functions, a mathematical instrument for the
modelization of the stochastic dependence structure. Our clustering algorithm
can be applied both to binary and real-valued matrices. We validate it with
simulations and applications to real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2592</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2592</id><created>2014-04-08</created><authors><author><keyname>Mirzoev</keyname><forenames>Dr. Timur</forenames></author></authors><title>Reduction of Field Loss by a Video Processing System</title><categories>cs.MM cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.2344</comments><journal-ref>Southeast Decision Sciences Institute. Conference Proceedings.
  SEDSI 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming of 60 de-interlaced fields per second digital uncompressed video
with 720x480 resolution without a loss of video fields is one of the desired
technologies by scientists in biomechanics. If it is possible to stream digital
uncompressed video without dropped video fields, then a sophisticated computer
analysis of the transmitted via IEEE 1394a connection video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it using specific software such as Arial Performance Analysis
Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2603</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2603</id><created>2014-04-09</created><updated>2015-02-13</updated><authors><author><keyname>Dienes</keyname><forenames>Keith R.</forenames></author></authors><title>Completing $h$</title><categories>physics.soc-ph cs.DL</categories><comments>13 pages, LaTeX, 2 figures, 1 table</comments><journal-ref>Journal of Informetrics 9 (2015) 385-397</journal-ref><doi>10.1016/j.joi.2015.01.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nearly a decade ago, the science community was introduced to the $h$-index, a
proposed statistical measure of the collective impact of the publications of
any individual researcher. It is of course undeniable that any method of
reducing a complex data set to a single number will necessarily have certain
limitations and introduce certain biases. However, in this paper we point out
that the definition of the $h$-index actually suffers from something far
deeper: a hidden mathematical incompleteness intrinsic to its definition. In
particular, we point out that one critical step within the definition of $h$
has been missed until now, resulting in an index which only achieves its stated
objectives under certain rather limited circumstances. For example, this
incompleteness explains why the $h$-index ultimately has more utility in
certain scientific subfields than others. In this paper, we expose the origin
of this incompleteness and then also propose a method of completing the
definition of $h$ in a way which remains close to its original guiding
principle. As a result, this &quot;completed&quot; $h$ not only reduces to the usual $h$
in cases where the $h$-index already achieves its objectives, but also extends
the validity of the $h$-index into situations where it currently does not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2632</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2632</id><created>2014-04-09</created><updated>2014-04-29</updated><authors><author><keyname>Javanmardi</keyname><forenames>Saeed</forenames></author><author><keyname>Shojafar</keyname><forenames>Mohammad</forenames></author><author><keyname>Shariatmadari</keyname><forenames>Shahdad</forenames></author><author><keyname>Ahrabi</keyname><forenames>Sima S.</forenames></author></authors><title>FRTRUST: a fuzzy reputation based model for trust management in semantic
  P2P grids</title><categories>cs.DC cs.NI</categories><comments>12 Pages, 10 Figures, 3 Tables, InderScience, International Journal
  of Grid and Utility Computing</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Grid and peer-to-peer (P2P) networks are two ideal technologies for file
sharing. A P2P grid is a special case of grid networks in which P2P
communications are used for communication between nodes and trust management.
Use of this technology allows creation of a network with greater distribution
and scalability. Semantic grids have appeared as an expansion of grid networks
in which rich resource metadata are revealed and clearly handled. In a semantic
P2P grid, nodes are clustered into different groups based on the semantic
similarities between their services. This paper proposes a reputation model for
trust management in a semantic P2P Grid. We use fuzzy theory, in a trust
overlay network named FR TRUST that models the network structure and the
storage of reputation information. In fact we present a reputation collection
and computation system for semantic P2P Grids. The system uses fuzzy theory to
compute a peer trust level, which can be either: Low, Medium, or High. Our
experimental results demonstrate that FR TRUST combines low (and therefore
desirable) a good computational complexity with high ranking accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2637</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2637</id><created>2014-04-09</created><authors><author><keyname>Machado</keyname><forenames>Guilherme Sperb</forenames></author><author><keyname>Hecht</keyname><forenames>Fabio</forenames></author><author><keyname>Waldburger</keyname><forenames>Martin</forenames></author><author><keyname>Stiller</keyname><forenames>Burkhard</forenames></author></authors><title>Bypassing Cloud Providers' Data Validation to Store Arbitrary Data</title><categories>cs.NI cs.CR</categories><comments>8 pages, ISBN 978-1-4673-5229-1</comments><journal-ref>IFIP/IEEE Integrated Network Management Symposium, Ghent, Belgium,
  2013, pp. 1-8</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A fundamental Software-as-a-Service (SaaS) characteristic in Cloud Computing
is to be application-specific; depending on the application, Cloud Providers
(CPs) restrict data formats and attributes allowed into their servers via a
data validation process. An ill-defined data validation process may directly
impact both security (e.g. application failure, legal issues) and accounting
and charging (e.g. trusting metadata in file headers). Therefore, this paper
investigates, evaluates (by means of tests), and discusses data validation
processes of popular CPs. A proof of concept system was thus built,
implementing encoders carefully crafted to circumvent data validation
processes, ultimately demonstrating how large amounts of unaccounted, arbitrary
data can be stored into CPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2644</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2644</id><created>2014-04-09</created><updated>2015-01-12</updated><authors><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Garakani</keyname><forenames>Alireza Bagheri</forenames></author><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse
  Learning</title><categories>cs.DC cs.LG stat.ML</categories><comments>Extended version of the SIAM Data Mining 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning sparse combinations is a frequent theme in machine learning. In this
paper, we study its associated optimization problem in the distributed setting
where the elements to be combined are not centrally located but spread over a
network. We address the key challenges of balancing communication costs and
optimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)
algorithm. We obtain theoretical guarantees on the optimization error
$\epsilon$ and communication cost that do not depend on the total number of
combining elements. We further show that the communication cost of dFW is
optimal by deriving a lower-bound on the communication cost required to
construct an $\epsilon$-approximate solution. We validate our theoretical
analysis with empirical studies on synthetic and real-world data, which
demonstrate that dFW outperforms both baselines and competing methods. We also
study the performance of dFW when the conditions of our analysis are relaxed,
and show that dFW is fairly robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2653</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2653</id><created>2014-04-09</created><authors><author><keyname>Aldous</keyname><forenames>David</forenames></author><author><keyname>Lando</keyname><forenames>Tamar</forenames></author></authors><title>The Stretch - Length Tradeoff in Geometric Networks: Average Case and
  Worst Case Study</title><categories>math.PR cs.CG</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a network linking the points of a rate-$1$ Poisson point process on
the plane. Write $\Psi^{\mbox{ave}}(s)$ for the minimum possible mean length
per unit area of such a network, subject to the constraint that the
route-length between every pair of points is at most $s$ times the Euclidean
distance. We give upper and lower bounds on the function
$\Psi^{\mbox{ave}}(s)$, and on the analogous &quot;worst-case&quot; function
$\Psi^{\mbox{worst}}(s)$ where the point configuration is arbitrary subject to
average density one per unit area. Our bounds are numerically crude, but raise
the question of whether there is an exponent $\alpha$ such that each function
has $\Psi(s) \asymp (s-1)^{-\alpha}$ as $s \downarrow 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2655</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2655</id><created>2014-04-09</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Khoo</keyname><forenames>Yuehaw</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Open problem: Tightness of maximum likelihood semidefinite relaxations</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have observed an interesting, yet unexplained, phenomenon: Semidefinite
programming (SDP) based relaxations of maximum likelihood estimators (MLE) tend
to be tight in recovery problems with noisy data, even when MLE cannot exactly
recover the ground truth. Several results establish tightness of SDP based
relaxations in the regime where exact recovery from MLE is possible. However,
to the best of our knowledge, their tightness is not understood beyond this
regime. As an illustrative example, we focus on the generalized Procrustes
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2656</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2656</id><created>2014-04-09</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Sampath</keyname><forenames>Ashwin</forenames></author><author><keyname>Maharshi</keyname><forenames>Atul</forenames></author><author><keyname>Koymen</keyname><forenames>Ozge</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author></authors><title>Wireless Backhaul Node Placement for Small Cell Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Invited paper at Conference on Information Science &amp; Systems (CISS)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cells have been proposed as a vehicle for wireless networks to keep up
with surging demand. Small cells come with a significant challenge of providing
backhaul to transport data to(from) a gateway node in the core network. Fiber
based backhaul offers the high rates needed to meet this requirement, but is
costly and time-consuming to deploy, when not readily available. Wireless
backhaul is an attractive option for small cells as it provides a less
expensive and easy-to-deploy alternative to fiber. However, there are multitude
of bands and features (e.g. LOS/NLOS, spatial multiplexing etc.) associated
with wireless backhaul that need to be used intelligently for small cells.
Candidate bands include: sub-6 GHz band that is useful in non-line-of-sight
(NLOS) scenarios, microwave band (6-42 GHz) that is useful in point-to-point
line-of-sight (LOS) scenarios, and millimeter wave bands (e.g. 60, 70 and 80
GHz) that are recently being commercially used in LOS scenarios. In many
deployment topologies, it is advantageous to use aggregator nodes, located at
the roof tops of tall buildings near small cells. These nodes can provide high
data rate to multiple small cells in NLOS paths, sustain the same data rate to
gateway nodes using LOS paths and take advantage of all available bands. This
work performs the joint cost optimal aggregator node placement, power
allocation, channel scheduling and routing to optimize the wireless backhaul
network. We formulate mixed integer nonlinear programs (MINLP) to capture the
different interference and multiplexing patterns at sub-6 GHz and microwave
band. We solve the MINLP through linear relaxation and branch-and-bound
algorithm and apply our algorithm in an example wireless backhaul network of
downtown Manhattan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2668</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2668</id><created>2014-04-09</created><updated>2014-08-09</updated><authors><author><keyname>Ebrahimi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Ghasemiesfeh</keyname><forenames>Golnaz</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author></authors><title>How Complex Contagions Spread Quickly in the Preferential Attachment
  Model and Other Time-Evolving Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the spreading speed of complex contagions in a social
network. A $k$-complex contagion starts from a set of initially infected seeds
such that any node with at least $k$ infected neighbors gets infected. Simple
contagions, i.e., $k=1$, quickly spread to the entire network in small world
graphs. However, fast spreading of complex contagions appears to be less likely
and more delicate; the successful cases depend crucially on the network
structure~\cite{G08,Ghasemiesfeh:2013:CCW}.
  Our main result shows that complex contagions can spread fast in a general
family of time-evolving networks that includes the preferential attachment
model~\cite{barabasi99emergence}. We prove that if the initial seeds are chosen
as the oldest nodes in a network of this family, a $k$-complex contagion covers
the entire network of $n$ nodes in $O(\log n)$ steps. We show that the choice
of the initial seeds is crucial. If the initial seeds are uniformly randomly
chosen in the PA model, even with a polynomial number of them, a complex
contagion would stop prematurely. The oldest nodes in a preferential attachment
model are likely to have high degrees. However, we remark that it is actually
not the power law degree distribution per se that facilitates fast spreading of
complex contagions, but rather the evolutionary graph structure of such models.
Some members of the said family do not even have a power-law distribution.
  We also prove that complex contagions are fast in the copy
model~\cite{KumarRaRa00}, a variant of the preferential attachment family.
  Finally, we prove that when a complex contagion starts from an arbitrary set
of initial seeds on a general graph, determining if the number of infected
vertices is above a given threshold is $\mathbf{P}$-complete. Thus, one cannot
hope to categorize all the settings in which complex contagions percolate in a
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2670</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2670</id><created>2014-04-09</created><authors><author><keyname>Harding</keyname><forenames>Brendan</forenames></author><author><keyname>Hegland</keyname><forenames>Markus</forenames></author><author><keyname>Larson</keyname><forenames>Jay</forenames></author><author><keyname>Southern</keyname><forenames>James</forenames></author></authors><title>Scalable and Fault Tolerant Computation with the Sparse Grid Combination
  Technique</title><categories>math.NA cs.DC cs.NA</categories><comments>23 pages, 4 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues to develop a fault tolerant extension of the sparse grid
combination technique recently proposed in [B. Harding and M. Hegland, ANZIAM
J., 54 (CTAC2012), pp. C394-C411]. The approach is novel for two reasons, first
it provides several levels in which one can exploit parallelism leading towards
massively parallel implementations, and second, it provides algorithm-based
fault tolerance so that solutions can still be recovered if failures occur
during computation. We present a generalisation of the combination technique
from which the fault tolerant algorithm is a consequence. Using a model for the
time between faults on each node of a high performance computer we provide
bounds on the expected error for interpolation with this algorithm. Numerical
experiments on the scalar advection PDE demonstrate that the algorithm is
resilient to faults on a real application. It is observed that the trade-off of
recovery time to decreased accuracy of the solution is suitably small. A
comparison with traditional checkpoint-restart methods applied to the
combination technique show that our approach is highly scalable with respect to
the number of faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2671</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2671</id><created>2014-04-09</created><authors><author><keyname>Ai</keyname><forenames>Lingqing</forenames></author><author><keyname>Wu</keyname><forenames>Xian</forenames></author><author><keyname>Huang</keyname><forenames>Lingxiao</forenames></author><author><keyname>Huang</keyname><forenames>Longbo</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author></authors><title>The Multi-shop Ski Rental Problem</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2677</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2677</id><created>2014-04-09</created><updated>2014-10-03</updated><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Thankachan</keyname><forenames>Sharma V.</forenames></author></authors><title>Optimal Encodings for Range Majority Queries</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of designing a data structure that reports the positions
of the distinct $\tau$-majorities within any range of an array $A[1,n]$,
without storing $A$. A $\tau$-majority in a range $A[i,j]$, for $0&lt;\tau&lt; 1$, is
an element that occurs more than $\tau(j-i+1)$ times in $A[i,j]$. We show that
$\Omega(n\log(1/\tau))$ bits are necessary for any data structure able just to
count the number of distinct $\tau$-majorities in any range. Then, we design a
structure using $O(n\log(1/\tau))$ bits that returns one position of each
$\tau$-majority of $A[i,j]$ in $O((1/\tau)\log\log_w(1/\tau)\log n)$ time, on a
RAM machine with word size $w$ (it can output any further position where each
$\tau$-majority occurs in $O(1)$ additional time). Finally, we show how to
remove a $\log n$ factor from the time by adding $O(n\log\log n)$ bits of space
to the structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2697</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2697</id><created>2014-04-10</created><updated>2014-11-19</updated><authors><author><keyname>Wilson</keyname><forenames>Duane</forenames></author><author><keyname>Ateniese</keyname><forenames>Giuseppe</forenames></author></authors><title>To Share or Not to Share in Client-Side Encrypted Clouds</title><categories>cs.CR cs.DC</categories><journal-ref>Information Security, Lecture Notes in Computer Science Volume
  8783, 2014, pp 401-412</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of cloud computing, a number of cloud providers have arisen
to provide Storage-as-a-Service (SaaS) offerings to both regular consumers and
business organizations. SaaS (different than Software-as-a-Service in this
context) refers to an architectural model in which a cloud provider provides
digital storage on their own infrastructure. Three models exist amongst SaaS
providers for protecting the confidentiality data stored in the cloud: 1) no
encryption (data is stored in plain text), 2) server-side encryption (data is
encrypted once uploaded), and 3) client-side encryption (data is encrypted
prior to upload). This paper seeks to identify weaknesses in the third model,
as it claims to offer 100% user data confidentiality throughout all data
transactions (e.g., upload, download, sharing) through a combination of Network
Traffic Analysis, Source Code Decompilation, and Source Code Disassembly. The
weaknesses we uncovered primarily center around the fact that the cloud
providers we evaluated were each operating in a Certificate Authority capacity
to facilitate data sharing. In this capacity, they assume the role of both
certificate issuer and certificate authorizer as denoted in a Public-Key
Infrastructure (PKI) scheme - which gives them the ability to view user data
contradicting their claims of 100% data confidentiality. We have collated our
analysis and findings in this paper and explore some potential solutions to
address these weaknesses in these sharing methods. The solutions proposed are a
combination of best practices associated with the use of PKI and other
cryptographic primitives generally accepted for protecting the confidentiality
of shared information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2713</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2713</id><created>2014-04-10</created><authors><author><keyname>Parmeza</keyname><forenames>Ditmar</forenames></author><author><keyname>Fifo</keyname><forenames>Miraldi</forenames></author></authors><title>Transaction Handling in COM, EJB and .NET</title><categories>cs.SE</categories><comments>17 pages, 8 figures, published at IJSEA Journal Vol. 5, No. 2, March
  2014</comments><journal-ref>International Journal of Software Engineering and Applications
  (IJSEA), Vol. 5, No. 2, March 2014</journal-ref><doi>10.5121/ijsea.2014.5204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technology evolution has shown a very impressive performance in the last
years by introducing several technologies that are based on the concept of
component. As time passes, new versions of Component- Based technologies are
released in order to improve services provided by previous ones. One important
issue that regards these technologies is transactional activity. Transactions
are important because they consist in sending different small amounts of
information collected properly in a single combined unit which makes the
process simpler, less expensive and also improves the reliability of the whole
system, reducing its chances to go through possible failures. Different
Component-Based technologies offer different ways of handling transactions. In
this paper, we will review and discuss how transactions are handled in three of
them: COM, EJB and .NET. It can be expected that .NET offers more efficient
mechanisms due to the fact of being released later than the other two
technologies. Nevertheless, COM and EJB are still present in the market and
their services are still widely used. Comparing transaction handling in these
technologies will be helpful to analyze the advantages and disadvantages of
each of them. This comparison and evaluation will be seen in two main
perspectives: performance and security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2725</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2725</id><created>2014-04-10</created><authors><author><keyname>Walton</keyname><forenames>Neil</forenames></author></authors><title>Concave Switching in Single and Multihop Networks</title><categories>cs.SY cs.NI math.OC math.PR</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Switched queueing networks model wireless networks, input queued switches and
numerous other networked communications systems. For single-hop networks, we
consider a {($\alpha,g$)-switch policy} which combines the MaxWeight policies
with bandwidth sharing networks -- a further well studied model of Internet
congestion. We prove the maximum stability property for this class of
randomized policies. Thus these policies have the same first order behavior as
the MaxWeight policies. However, for multihop networks some of these
generalized polices address a number of critical weakness of the
MaxWeight/BackPressure policies.
  For multihop networks with fixed routing, we consider the Proportional
Scheduler (or (1,log)-policy). In this setting, the BackPressure policy is
maximum stable, but must maintain a queue for every route-destination, which
typically grows rapidly with a network's size. However, this proportionally
fair policy only needs to maintain a queue for each outgoing link, which is
typically bounded in number. As is common with Internet routing, by maintaining
per-link queueing each node only needs to know the next hop for each packet and
not its entire route. Further, in contrast to BackPressure, the Proportional
Scheduler does not compare downstream queue lengths to determine weights, only
local link information is required. This leads to greater potential for
decomposed implementations of the policy. Through a reduction argument and an
entropy argument, we demonstrate that, whilst maintaining substantially less
queueing overhead, the Proportional Scheduler achieves maximum throughput
stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2728</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2728</id><created>2014-04-10</created><updated>2014-04-21</updated><authors><author><keyname>Hu</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Fan</forenames></author><author><keyname>Du</keyname><forenames>Qian</forenames></author></authors><title>Real-time Decolorization using Dominant Colors</title><categories>cs.GR cs.CV</categories><comments>This paper has been withdrawn by the author due to some errors in
  equation 9 related descriptions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decolorization is the process to convert a color image or video to its
grayscale version, and it has received great attention in recent years. An
ideal decolorization algorithm should preserve the original color contrast as
much as possible. Meanwhile, it should provide the final decolorized result as
fast as possible. However, most of the current methods are suffering from
either unsatisfied color information preservation or high computational cost,
limiting their application value. In this paper, a simple but effective
technique is proposed for real-time decolorization. Based on the typical
rgb2gray() color conversion model, which produces a grayscale image by linearly
combining R, G, and B channels, we propose a dominant color hypothesis and a
corresponding distance measurement metric to evaluate the quality of grayscale
conversion. The local optimum scheme provides several &quot;good&quot; candidates in a
confidence interval, from which the &quot;best&quot; result can be extracted.
Experimental results demonstrate that remarkable simplicity of the proposed
method facilitates the process of high resolution images and videos in
real-time using a common CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2737</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2737</id><created>2014-04-10</created><updated>2014-05-13</updated><authors><author><keyname>Singer</keyname><forenames>Robert</forenames></author></authors><title>User Centered Development of Agent-based Business Process Models and
  Notations</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss questions about user centric development of business process
modeling notations. In the center of our research there is a fully featured
multi-enterprise business process platform (ME-BPP) based on the concepts of
agent-based business processes, which builds on the formal foundations of the
subject-oriented business process management methodology (S-BPM). The platform
is implemented based on cloud technology using commercial services.
Additionally we developed a &quot;block modeling&quot; technique to find a semantically
transparent modeling notation which can be used by novice users to model
subject-oriented business process (S-BPM) models. As this is ongoing research
there are still serious open questions. But, the presented approach breaks with
some of the rules of typical process modeling notations and hopefully
stimulates innovation. Additionally we want to continue our research towards
the enhancement of our modeling approach towards a user centric &quot;syntax and
semantic free&quot; modeling technique to develop user and domain specific modeling
notations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2739</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2739</id><created>2014-04-10</created><authors><author><keyname>Devi</keyname><forenames>M. Rathna</forenames></author><author><keyname>Anju</keyname><forenames>A.</forenames></author></authors><title>Multiprocessor Scheduling of Dependent Tasks to Minimize Makespan and
  Reliability Cost Using NSGA-II</title><categories>cs.DC</categories><comments>13 pages,7 figures</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.4, No.2, March 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Algorithms developed for scheduling applications on heterogeneous
multiprocessor system focus on asingle objective such as execution time, cost
or total data transmission time. However, if more than oneobjective (e.g.
execution cost and time, which may be in conflict) are considered, then the
problem becomes more challenging. This project is proposed to develop a
multiobjective scheduling algorithm using Evolutionary techniques for
scheduling a set of dependent tasks on available resources in a multiprocessor
environment which will minimize the makespan and reliability cost. A
Non-dominated sorting Genetic Algorithm-II procedure has been developed to get
the pareto- optimal solutions. NSGA-II is a Elitist Evolutionary algorithm, and
it takes the initial parental solution without any changes, in all iteration to
eliminate the problem of loss of some pareto-optimal solutions.NSGA-II uses
crowding distance concept to create a diversity of the solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2741</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2741</id><created>2014-04-10</created><authors><author><keyname>Bellini</keyname><forenames>E.</forenames></author><author><keyname>Simonetti</keyname><forenames>I.</forenames></author><author><keyname>Sala</keyname><forenames>M.</forenames></author></authors><title>Nonlinearity of Boolean functions: an algorithmic approach based on
  multivariate polynomials</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1404.2471</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute the nonlinearity of Boolean functions with Groebner basis
techniques, providing two algorithms: one over the binary field and the other
over the rationals. We also estimate their complexity. Then we show how to
improve our rational algorithm, arriving at a worst-case complexity of
$O(n2^n)$ operations over the integers, that is, sums and doublings. This way,
with a different approach, we reach the same complexity of established
algorithms, such as those based on the fast Walsh transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2742</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2742</id><created>2014-04-10</created><authors><author><keyname>Dey</keyname><forenames>Sumit Kumar</forenames></author><author><keyname>Anand</keyname><forenames>Shubham</forenames></author></authors><title>Algorithm For Multi-Hand Finger Counting: An Easy Approach</title><categories>cs.HC</categories><comments>8 pages, 4 sets of figures</comments><journal-ref>Advances in Vision Computing: An International Journal(AVC) Vol.1,
  No.1, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an easy algorithm for real time hand finger counting
involving one or more than one hand. Hand finger counting is a simple medium
for Human-Computer Interface which can prove to be a convenient input method
for driving interactive menus, small applications and interactive games. Here,
we also calculate the orientation of the hand which can be used to provide
inputs for the direction and/or speed control of a robot, controlling mouse
cursor or slide-show presentations. If being used by a single person, with his
or her two hands, the person can trigger ten different commands with fingers in
addition to the orientation of the hand. We tried to use very simple algorithm
using very basic mathematics of 2D coordinate geometry and avoided the
conventional use of calculus, contours and convex hull. Anyone seeking for an
easy to implement hand finger counting algorithm can refer to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2743</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2743</id><created>2014-04-10</created><authors><author><keyname>Glebov</keyname><forenames>Roman</forenames></author><author><keyname>Klimosova</keyname><forenames>Tereza</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author></authors><title>Infinite dimensional finitely forcible graphon</title><categories>math.CO cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:1309.6695</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphons are analytic objects associated with convergent sequences of dense
graphs. Problems from extremal combinatorics and theoretical computer science
led to a study of finitely forcible graphons, i.e., graphons determined by
finitely many subgraph densities. Lovasz and Szegedy conjectured that the
topological space of typical vertices of such a graphon always has finite
dimension, which would also have implications on the number of parts in its
weak regular partition. We disprove the conjecture by constructing a finitely
forcible graphon with the space of typical vertices with infinite dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2745</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2745</id><created>2014-04-10</created><authors><author><keyname>Pandolfi</keyname><forenames>L.</forenames></author><author><keyname>Halanay</keyname><forenames>A.</forenames></author></authors><title>Approximate controllability and lack of controllability to zero of the
  heat equation with memory</title><categories>cs.SY math.OC</categories><msc-class>45K05, 93B03, 93B05, 93C22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the heat equation with memory in a bounded region
$\Omega \subset\mathbb{R}^d$, $d\geq 1$, in the case that the propagation speed
of the signal is infinite (i.e. the Colemann-Gurtin model). The memory kernel
is of class $C^1$. We examine its controllability properties both under the
action of boundary controls or when the controls are distributed in a subregion
of $\Omega$. We prove approximate controllability of the system and, in
contrast with this, we prove the existence of initial conditions which cannot
be steered to hit the target $0$ in a certain time $T$, of course when the
memory kernel is not identically zero. In both the cases we derive our results
from well known properties of the heat equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2750</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2750</id><created>2014-04-10</created><updated>2015-09-18</updated><authors><author><keyname>Kelly</keyname><forenames>Frank</forenames></author><author><keyname>Key</keyname><forenames>Peter</forenames></author><author><keyname>Walton</keyname><forenames>Neil</forenames></author></authors><title>Efficient Advert Assignment</title><categories>cs.GT cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for the analysis of large-scale Ad-auctions where
adverts are assigned over a continuum of search types. For this pay-per-click
market, we provide an efficient and highly decomposed mechanism that maximizes
social welfare. In particular, we show that the social welfare optimization can
be solved in separate optimizations conducted on the time-scales relevant to
the advertisement platform and advertisers. Here, on each search occurrence,
the platform solves an assignment problem and, on a slower time scale, each
advertiser submits a bid which matches its demand for click-throughs with
supply. Importantly knowledge of global parameters, such as the distribution of
search terms, is not required when separating the problem in this way. This
decomposition is implemented in an adversarial setting. Exploiting the
information asymmetry between the platform and advertiser, we describe a simple
mechanism which incentivizes truthful bidding and has a unique Nash equilibrium
that is socially optimal, and thus implements our decomposition. Further, we
consider models where advertisers adapt their bids smoothly over time, and
prove convergence to the solution that maximizes aggregate utility. Finally, we
describe several extensions which illustrate the flexibility and tractability
of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2759</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2759</id><created>2014-04-10</created><authors><author><keyname>Bendaoud</keyname><forenames>Fayssal</forenames></author><author><keyname>Abdennebi</keyname><forenames>Marwen</forenames></author><author><keyname>Didi</keyname><forenames>Fedoua</forenames></author></authors><title>Survey On Scheduling And Radio Resources Allocation In Lte</title><categories>cs.NI</categories><journal-ref>International Journal of Next Generation Network ( IJNGN ), Volume
  6, March 2014</journal-ref><doi>10.5121/ijngn.2014.6102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on an essential task of the enhanced NodeB eNodeB element
in LTE architecture, the Radio Resource Manager RRM, which aims to accept or
reject requests for connection to the network based on some constraints and
ensuring optimal distribution of radio resources between Users Equipments UEs.
Its main functionalities include Admission Control AC and Packet Scheduling PS.
This paper will center mainly on the PS part of the RRM task, which performs
the radio resource allocation in both uplink and downlink directions. Several
approaches and algorithms have been proposed in the literature to address this
need ie : allocate resources efficiently, the diversity and multitude of
algorithms is related to the factors considered for the optimal management of
radio resource, specifically, the traffic type and the QoS Quality of Service
requested by the UE. In this article, an art's state of the radio resource
allocation strategies and a detailed study of several scheduling algorithms
proposed for LTE (uplink and downlink) are made. Therefore, we offer our
evaluation and criticism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2761</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2761</id><created>2014-04-10</created><authors><author><keyname>Rashid</keyname><forenames>Jibran</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Implications of quantum automata for contextuality</title><categories>quant-ph cs.CC cs.FL</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct zero-error quantum finite automata (QFAs) for promise problems
which cannot be solved by bounded-error probabilistic finite automata (PFAs).
Here is a summary of our results:
  - There is a promise problem solvable by an exact two-way QFA in exponential
expected time, but not by any bounded-error sublogarithmic space probabilistic
Turing machine (PTM).
  - There is a promise problem solvable by an exact two-way QFA in quadratic
expected time, but not by any bounded-error $ o(\log \log n) $-space PTMs in
polynomial expected time. The same problem can be solvable by a one-way Las
Vegas (or exact two-way) QFA with quantum head in linear (expected) time.
  - There is a promise problem solvable by a Las Vegas realtime QFA, but not by
any bounded-error realtime PFA. The same problem can be solvable by an exact
two-way QFA in linear expected time but not by any exact two-way PFA.
  - There is a family of promise problems such that each promise problem can be
solvable by a two-state exact realtime QFAs, but, there is no such bound on the
number of states of realtime bounded-error PFAs solving the members this
family.
  Our results imply that there exist zero-error quantum computational devices
with a \emph{single qubit} of memory that cannot be simulated by any finite
memory classical computational model. This provides a computational perspective
on results regarding ontological theories of quantum mechanics \cite{Hardy04},
\cite{Montina08}. As a consequence we find that classical automata based
simulation models \cite{Kleinmann11}, \cite{Blasiak13} are not sufficiently
powerful to simulate quantum contextuality. We conclude by highlighting the
interplay between results from automata models and their application to
developing a general framework for quantum contextuality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2768</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2768</id><created>2014-04-10</created><authors><author><keyname>pira</keyname><forenames>Einollah</forenames></author><author><keyname>Miralvand</keyname><forenames>Mohammad Reza Zand</forenames></author><author><keyname>Soltani</keyname><forenames>Fakhteh</forenames></author></authors><title>Verification of confliction and unreachability in rule-based expert
  systems with model checking</title><categories>cs.AI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is important to find optimal solutions for structural errors in rule-based
expert systems .Solutions to discovering such errors by using model checking
techniques have already been proposed, but these solutions have problems such
as state space explosion. In this paper, to overcome these problems, we model
the rule-based systems as finite state transition systems and express
confliction and unreachability as Computation Tree Logic (CTL) logic formula
and then use the technique of model checking to detect confliction and
unreachability in rule-based systems with the model checker UPPAAL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2772</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2772</id><created>2014-04-10</created><authors><author><keyname>Ranjan</keyname><forenames>Ravi</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>A New Clustering Approach for Anomaly Intrusion Detection</title><categories>cs.DC cs.CR cs.LG</categories><comments>10 pages with 3 figures,2 Tables This paper explains about clustering
  methodology used in Data Mining field for Intrusion Detection in the area of
  Network Security</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP),ISSN:2230-9608[Online],2231-007X[Print] Vol.4, No.2, March
  2014, page(s): 29-38</journal-ref><doi>10.5121/ijdkp.2014.4203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in technology have made our work easier compare to earlier
times. Computer network is growing day by day but while discussing about the
security of computers and networks it has always been a major concerns for
organizations varying from smaller to larger enterprises. It is true that
organizations are aware of the possible threats and attacks so they always
prepare for the safer side but due to some loopholes attackers are able to make
attacks. Intrusion detection is one of the major fields of research and
researchers are trying to find new algorithms for detecting intrusions.
Clustering techniques of data mining is an interested area of research for
detecting possible intrusions and attacks. This paper presents a new clustering
approach for anomaly intrusion detection by using the approach of K-medoids
method of clustering and its certain modifications. The proposed algorithm is
able to achieve high detection rate and overcomes the disadvantages of K-means
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2796</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2796</id><created>2014-04-10</created><authors><author><keyname>Lipmaa</keyname><forenames>Helger</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author></authors><title>Linear Batch Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an application, where a client wants to obtain many elements from a large
database, it is often desirable to have some load balancing. Batch codes
(introduced by Ishai et al. in STOC 2004) make it possible to do exactly that:
the large database is divided between many servers, so that the client has to
only make a small number of queries to every server to obtain sufficient
information to reconstruct all desired elements. Other important parameters of
the batch codes are total storage and the number of servers. Batch codes also
have applications in cryptography (namely, in the construction of multi-query
computationally-private information retrieval protocols).
  In this work, we initiate the study of linear batch codes. These codes, in
particular, are of potential use in distributed storage systems. We show that a
generator matrix of a binary linear batch code is also a generator matrix of
classical binary linear error-correcting code. This immediately yields that a
variety of upper bounds, which were developed for error-correcting codes, are
applicable also to binary linear batch codes. We also propose new methods to
construct large linear batch codes from the smaller ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2813</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2813</id><created>2014-04-10</created><authors><author><keyname>Banisch</keyname><forenames>Ralf</forenames></author><author><keyname>Sch&#xfc;tte</keyname><forenames>Christof</forenames></author><author><keyname>Conrad</keyname><forenames>Nata&#x161;a Djurdjevac</forenames></author></authors><title>Loop-based Module Detection in Directed Networks</title><categories>physics.data-an cs.SI physics.soc-ph</categories><msc-class>60J20, 94C15</msc-class><doi>10.1209/0295-5075/108/68008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of finding modules (or clusters, communities) in
directed networks. Until now, most articles on this topic have been oriented
towards finding full network partitions, where every node belongs to exactly
one module. In this paper, we will present a novel random walk based approach
for fuzzy partitions of the directed network into modules, such that some nodes
do not belong to only one of the modules but to several or to none at all.
Despite the fact that the new random walk process is reversible, we will show
that it inherits all necessary information about directions and structure of
the original network. We accompany our theoretical considerations with
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2816</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2816</id><created>2014-04-10</created><authors><author><keyname>Maslennikova</keyname><forenames>Marina</forenames></author></authors><title>Reset Complexity of Ideal Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new characteristic of a regular ideal language called reset
complexity. We find some bounds on the reset complexity in terms of the state
complexity of a given language. We also compare the reset complexity and the
state complexity for languages related to slowly synchronizing automata and
study uniqueness question for automata yielding the minimum of reset
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2819</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2819</id><created>2014-04-10</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author></authors><title>Decoding of Quasi-Cyclic Codes up to A New Lower Bound on the Minimum
  Distance</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><journal-ref>IEEE International Symposium on Information Theory (ISIT 2014)
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new lower bound on the minimum Hamming distance of linear quasi-cyclic
codes over finite fields is proposed. It is based on spectral analysis and
generalizes the Semenov- Trifonov bound in a similar way as the Hartmann-Tzeng
bound extends the BCH approach for cyclic codes. Furthermore, a syndrome-based
algebraic decoding algorithm is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2820</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2820</id><created>2014-04-10</created><authors><author><keyname>Nia</keyname><forenames>Mehran Alidoost</forenames></author><author><keyname>Sajedi</keyname><forenames>Ali</forenames></author><author><keyname>Jamshidpey</keyname><forenames>Aryo</forenames></author></authors><title>An Introduction to Digital Signature Schemes</title><categories>cs.CR</categories><comments>In Proceeding of National Conference on Information Retrieval, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, all types of digital signature schemes emphasis on secure and best
verification methods. Different digital signature schemes are used in order for
the websites, security organizations, banks and so on to verify user's
validity. Digital signature schemes are categorized to several types such as
proxy, on-time, batch and so on. In this paper, different types of schemes are
compared based on security level, efficiency, difficulty of algorithm and so
on. Results show that best scheme depends on security, complexity and other
important parameters. We tried simply to define the schemes and review them in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2824</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2824</id><created>2014-04-01</created><authors><author><keyname>Burcsi</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Lipt&#xe1;k</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Ruskey</keyname><forenames>Frank</forenames></author><author><keyname>Sawada</keyname><forenames>Joe</forenames></author></authors><title>Normal, Abby Normal, Prefix Normal</title><categories>cs.FL cs.DM cs.DS math.CO</categories><comments>Accepted at FUN '14</comments><journal-ref>LNCS 8496, pages 74-88 (2014)</journal-ref><doi>10.1007/978-3-319-07890-8_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prefix normal word is a binary word with the property that no substring has
more 1s than the prefix of the same length. This class of words is important in
the context of binary jumbled pattern matching. In this paper we present
results about the number $pnw(n)$ of prefix normal words of length $n$, showing
that $pnw(n) =\Omega\left(2^{n - c\sqrt{n\ln n}}\right)$ for some $c$ and
$pnw(n) = O \left(\frac{2^n (\ln n)^2}{n}\right)$. We introduce efficient
algorithms for testing the prefix normal property and a &quot;mechanical algorithm&quot;
for computing prefix normal forms. We also include games which can be played
with prefix normal words. In these games Alice wishes to stay normal but Bob
wants to drive her &quot;abnormal&quot; -- we discuss which parameter settings allow
Alice to succeed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2825</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2825</id><created>2014-04-09</created><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author></authors><title>Asymptotics of Fingerprinting and Group Testing: Capacity-Achieving
  Log-Likelihood Decoders</title><categories>cs.IT cs.CR math.IT math.ST stat.TH</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the large-coalition asymptotics of fingerprinting and group testing,
and derive explicit decoders that provably achieve capacity for many of the
considered models. We do this both for simple decoders (fast but suboptimal)
and for joint decoders (slow but optimal), and both for informed and uninformed
settings.
  For fingerprinting, we show that if the pirate strategy is known, the
Neyman-Pearson-based log-likelihood decoders provably achieve capacity,
regardless of the strategy. The decoder built against the interleaving attack
is further shown to be a universal decoder, able to deal with arbitrary attacks
and achieving the uninformed capacity. This universal decoder is shown to be
closely related to the Lagrange-optimized decoder of Oosterwijk et al. and the
empirical mutual information decoder of Moulin. Joint decoders are also
proposed, and we conjecture that these also achieve the corresponding joint
capacities.
  For group testing, the simple decoder for the classical model is shown to be
more efficient than the one of Chan et al. and it provably achieves the simple
group testing capacity. For generalizations of this model such as noisy group
testing, the resulting simple decoders also achieve the corresponding simple
capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2827</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2827</id><created>2014-04-10</created><authors><author><keyname>Abasi</keyname><forenames>Hasan</forenames></author><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author></authors><title>A Simple Algorithm for Hamiltonicity</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new algebraic technique that solves the following problem: Given
a black box that contains an arithmetic circuit $f$ over a field of
characteristic $2$ of degree~$d$. Decide whether $f$, expressed as an
equivalent multivariate polynomial, contains a multilinear monomial of degree
$d$.
  This problem was solved by Williams \cite{W} and Bj\&quot;orklund et. al.
\cite{BHKK} for a white box (the circuit is given as an input) that contains
arithmetic circuit. We show a simple black box algorithm that solves the
problem with the same time complexity.
  This gives a simple randomized algorithm for the simple $k$-path problem for
directed graphs of the same time complexity\footnote{$O^*(f(k))$ is
$O(poly(n)\cdot f(k))$} $O^*(2^k)$ as in \cite{W} and with reusing the same
ideas from \cite{BHKK} with the above gives another algorithm (probably not
simpler) for undirected graphs of the same time complexity $O^*(1.657^k)$ as in
\cite{B10,BHKK}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2832</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2832</id><created>2014-04-10</created><updated>2015-10-13</updated><authors><author><keyname>Giannakopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Bounding the Optimal Revenue of Selling Multiple Goods</title><categories>cs.GT</categories><journal-ref>Theoretical Computer Science 581 (2015) 83-96</journal-ref><doi>10.1016/j.tcs.2015.03.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using duality theory techniques we derive simple, closed-form formulas for
bounding the optimal revenue of a monopolist selling many heterogeneous goods,
in the case where the buyer's valuations for the items come i.i.d. from a
uniform distribution and in the case where they follow independent (but not
necessarily identical) exponential distributions. We apply this in order to get
in both these settings specific performance guarantees, as functions of the
number of items $m$, for the simple deterministic selling mechanisms studied by
Hart and Nisan [EC 2012], namely the one that sells the items separately and
the one that offers them all in a single bundle.
  We also propose and study the performance of a natural randomized mechanism
for exponential valuations, called Proportional. As an interesting corollary,
for the special case where the exponential distributions are also identical, we
can derive that offering the goods in a single full bundle is the optimal
selling mechanism for any number of items. To our knowledge, this is the first
result of its kind: finding a revenue-maximizing auction in an additive setting
with arbitrarily many goods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2842</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2842</id><created>2014-04-10</created><updated>2014-04-19</updated><authors><author><keyname>Jin</keyname><forenames>Xibo</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Hu</keyname><forenames>Songlin</forenames></author><author><keyname>Zhou</keyname><forenames>Biyu</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>A Joint Optimization of Operational Cost and Performance Interference in
  Cloud Data Centers</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual machine (VM) scheduling is an important technique to efficiently
operate the computing resources in a data center. Previous work has mainly
focused on consolidating VMs to improve resource utilization and thus to
optimize energy consumption. However, the interference between collocated VMs
is usually ignored, which can result in very worse performance degradation to
the applications running in those VMs due to the contention of the shared
resources. Based on this observation, we aim at designing efficient VM
assignment and scheduling strategies where we consider optimizing both the
operational cost of the data center and the performance degradation of running
applications and then, we propose a general model which captures the inherent
tradeoff between the two contradictory objectives. We present offline and
online solutions for this problem by exploiting the spatial and temporal
information of VMs where VM scheduling is done by jointly consider the
combinations and the life-cycle overlapping of the VMs. Evaluation results show
that the proposed methods can generate efficient schedules for VMs, achieving
low operational cost while significantly reducing the performance degradation
of applications in cloud data centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2843</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2843</id><created>2014-04-10</created><authors><author><keyname>Aswani</keyname><forenames>Anil</forenames></author><author><keyname>Bouffard</keyname><forenames>Patrick</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaojing</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire</forenames></author></authors><title>Practical Comparison of Optimization Algorithms for Learning-Based MPC
  with Linear Models</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning-based control methods are an attractive approach for addressing
performance and efficiency challenges in robotics and automation systems. One
such technique that has found application in these domains is learning-based
model predictive control (LBMPC). An important novelty of LBMPC lies in the
fact that its robustness and stability properties are independent of the type
of online learning used. This allows the use of advanced statistical or machine
learning methods to provide the adaptation for the controller. This paper is
concerned with providing practical comparisons of different optimization
algorithms for implementing the LBMPC method, for the special case where the
dynamic model of the system is linear and the online learning provides linear
updates to the dynamic model. For comparison purposes, we have implemented a
primal-dual infeasible start interior point method that exploits the sparsity
structure of LBMPC. Our open source implementation (called LBmpcIPM) is
available through a BSD license and is provided freely to enable the rapid
implementation of LBMPC on other platforms. This solver is compared to the
dense active set solvers LSSOL and qpOASES using a quadrotor helicopter
platform. Two scenarios are considered: The first is a simulation comparing
hovering control for the quadrotor, and the second is on-board control
experiments of dynamic quadrotor flight. Though the LBmpcIPM method has better
asymptotic computational complexity than LSSOL and qpOASES, we find that for
certain integrated systems (like our quadrotor testbed) these methods can
outperform LBmpcIPM. This suggests that actual benchmarks should be used when
choosing which algorithm is used to implement LBMPC on practical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2861</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2861</id><created>2014-04-10</created><updated>2015-07-05</updated><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>Distributed Signaling Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recurring theme in recent computer science literature is that proper design
of signaling schemes is a crucial aspect of effective mechanisms aiming to
optimize social welfare or revenue. One of the research endeavors of this line
of work is understanding the algorithmic and computational complexity of
designing efficient signaling schemes. In reality, however, information is
typically not held by a central authority, but is distributed among multiple
sources (third-party &quot;mediators&quot;), a fact that dramatically changes the
strategic and combinatorial nature of the signaling problem, making it a game
between information providers, as opposed to a traditional mechanism design
problem.
  In this paper we introduce {\em distributed signaling games}, while using
display advertising as a canonical example for introducing this foundational
framework. A distributed signaling game may be a pure coordination game (i.e.,
a distributed optimization task), or a non-cooperative game. In the context of
pure coordination games, we show a wide gap between the computational
complexity of the centralized and distributed signaling problems. On the other
hand, we show that if the information structure of each mediator is assumed to
be &quot;local&quot;, then there is an efficient algorithm that finds a near-optimal
($5$-approximation) distributed signaling scheme.
  In the context of non-cooperative games, the outcome generated by the
mediators' signals may have different value to each (due to the auctioneer's
desire to align the incentives of the mediators with his own by relative
compensations). We design a mechanism for this problem via a novel application
of Shapley's value, and show that it possesses some interesting properties, in
particular, it always admits a pure Nash equilibrium, and it never decreases
the revenue of the auctioneer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2862</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2862</id><created>2014-04-10</created><updated>2015-03-03</updated><authors><author><keyname>Carmi</keyname><forenames>Avishy Y.</forenames></author><author><keyname>Moskovich</keyname><forenames>Daniel</forenames></author></authors><title>Tangle Machines</title><categories>cs.IT cs.SY math.GT math.IT quant-ph</categories><comments>29 pages, 37 figures. Major revision. Introduction rewritten.
  Definitions simplified, with the Gauss diagram definition pushed to an
  appendix. Previous appendix removed</comments><msc-class>94A15, 81P68, 57M99</msc-class><acm-class>H.1.1; F.0</acm-class><journal-ref>Proc. R. Soc. A 2015 471 20150111</journal-ref><doi>10.1098/rspa.2015.0111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tangle machines are topologically inspired diagrammatic models. Their novel
feature is their natural notion of equivalence. Equivalent tangle machines may
differ locally, but globally they are considered to share the same information
content. The goal of tangle machine equivalence is to provide a
context-independent method to select, from among many ways to perform a task,
the `best' way to perform the task. The concept of equivalent tangle machines
is illustrated through examples in which they represent recursive computations,
networks of adiabatic quantum computations, and networks of distributed
information processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2863</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2863</id><created>2014-04-10</created><authors><author><keyname>Carmi</keyname><forenames>Avishy Y.</forenames></author><author><keyname>Moskovich</keyname><forenames>Daniel</forenames></author></authors><title>Tangle Machines II: Invariants</title><categories>cs.IT cs.SY math.GT math.IT quant-ph</categories><comments>26 pages, 30 figures</comments><msc-class>94A15, 81P68, 57M99</msc-class><acm-class>H.1.1; F.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The preceding paper constructed tangle machines as diagrammatic models, and
illustrated their utility with a number of examples. The information content of
a tangle machine is contained in characteristic quantities associated to
equivalence classes of tangle machines, which are called invariants. This paper
constructs invariants of tangle machines. Chief among these are the prime
factorizations of a machine, which are essentially unique. This is proven using
low dimensional topology, through representing a colour-suppressed machine as a
diagram for a network of jointly embedded spheres and intervals in 4-space. The
complexity of a tangle machine is defined as its number of prime factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2864</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2864</id><created>2014-04-10</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Ricciutelli</keyname><forenames>Giacomo</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>LDPC coded transmissions over the Gaussian broadcast channel with
  confidential messages</title><categories>cs.IT cs.CR math.IT</categories><comments>5 pages, 5 figures, to be presented at IEEE ICT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design and assess some practical low-density parity-check (LDPC) coded
transmission schemes for the Gaussian broadcast channel with confidential
messages (BCC). This channel model is different from the classical wiretap
channel model as the unauthorized receiver (Eve) must be able to decode some
part of the information. Hence, the reliability and security targets are
different from those of the wiretap channel. In order to design and assess
practical coding schemes, we use the error rate as a metric of the performance
achieved by the authorized receiver (Bob) and the unauthorized receiver (Eve).
We study the system feasibility, and show that two different levels of
protection against noise are required on the public and the secret messages.
This can be achieved in two ways: i) by using LDPC codes with unequal error
protection (UEP) of the transmitted information bits or ii) by using two
classical non-UEP LDPC codes with different rates. We compare these two
approaches and show that, for the considered examples, the solution exploiting
UEP LDPC codes is more efficient than that using non-UEP LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2872</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2872</id><created>2014-04-10</created><authors><author><keyname>Mahmud</keyname><forenames>Md Pavel</forenames></author><author><keyname>Schliep</keyname><forenames>Alexander</forenames></author></authors><title>TreQ-CG: Clustering Accelerates High-Throughput Sequencing Read Mapping</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As high-throughput sequencers become standard equipment outside of sequencing
centers, there is an increasing need for efficient methods for pre-processing
and primary analysis. While a vast literature proposes methods for HTS data
analysis, we argue that significant improvements can still be gained by
exploiting expensive pre-processing steps which can be amortized with savings
from later stages. We propose a method to accelerate and improve read mapping
based on an initial clustering of possibly billions of high-throughput
sequencing reads, yielding clusters of high stringency and a high degree of
overlap. This clustering improves on the state-of-the-art in running time for
small datasets and, for the first time, makes clustering high-coverage human
libraries feasible. Given the efficiently computed clusters, only one
representative read from each cluster needs to be mapped using a traditional
readmapper such as BWA, instead of individually mapping all reads. On human
reads, all processing steps, including clustering and mapping, only require
11%-59% of the time for individually mapping all reads, achieving speed-ups for
all readmappers, while minimally affecting mapping quality. This accelerates a
highly sensitive readmapper such as Stampy to be competitive with a fast
readmapper such as BWA on unclustered reads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2878</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2878</id><created>2014-04-10</created><authors><author><keyname>Bijal</keyname><forenames>Dalwadi</forenames></author><author><keyname>Sanket</keyname><forenames>Suthar</forenames></author></authors><title>Overview of Stemming Algorithms for Indian and Non-Indian Languages</title><categories>cs.CL</categories><journal-ref>International Journal of Computer Science and Information
  Technologies, Vol. 5 (2) , 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stemming is a pre-processing step in Text Mining applications as well as a
very common requirement of Natural Language processing functions. Stemming is
the process for reducing inflected words to their stem. The main purpose of
stemming is to reduce different grammatical forms / word forms of a word like
its noun, adjective, verb, adverb etc. to its root form. Stemming is widely
uses in Information Retrieval system and reduces the size of index files. We
can say that the goal of stemming is to reduce inflectional forms and sometimes
derivationally related forms of a word to a common base form. In this paper we
have discussed different stemming algorithm for non-Indian and Indian language,
methods of stemming, accuracy and errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2885</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2885</id><created>2014-04-08</created><authors><author><keyname>Jiang</keyname><forenames>Tian-Shun</forenames></author><author><keyname>Polizzi</keyname><forenames>Zachary</forenames></author><author><keyname>Yuan</keyname><forenames>Christopher</forenames></author></authors><title>A Networks and Machine Learning Approach to Determine the Best College
  Coaches of the 20th-21st Centuries</title><categories>stat.AP cs.LG cs.SI</categories><comments>18 pages, Submitted to the 2014 Mathematical Contest in Modeling
  (MCM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our objective is to find the five best college sports coaches of past century
for three different sports. We decided to look at men's basketball, football,
and baseball. We wanted to use an approach that could definitively determine
team skill from the games played, and then use a machine-learning algorithm to
calculate the correct coach skills for each team in a given year. We created a
networks-based model to calculate team skill from historical game data. A
digraph was created for each year in each sport. Nodes represented teams, and
edges represented a game played between two teams. The arrowhead pointed
towards the losing team. We calculated the team skill of each graph using a
right-hand eigenvector centrality measure. This way, teams that beat good teams
will be ranked higher than teams that beat mediocre teams. The eigenvector
centrality rankings for most years were well correlated with tournament
performance and poll-based rankings. We assumed that the relationship between
coach skill $C_s$, player skill $P_s$, and team skill $T_s$ was $C_s \cdot P_s
= T_s$. We then created a function to describe the probability that a given
score difference would occur based on player skill and coach skill. We
multiplied the probabilities of all edges in the network together to find the
probability that the correct network would occur with any given player skill
and coach skill matrix. We was able to determine player skill as a function of
team skill and coach skill, eliminating the need to optimize two unknown
matrices. The top five coaches in each year were noted, and the top coach of
all time was calculated by dividing the number of times that coach ranked in
the yearly top five by the years said coach had been active.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2888</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2888</id><created>2014-04-10</created><authors><author><keyname>Feham</keyname><forenames>Bouchra Rahali Mohammed</forenames></author></authors><title>Substrate integrated waveguide power divider, circulator and coupler in
  [10-15] GHz band</title><categories>cs.OH</categories><journal-ref>International Journal of Information Sciences and Techniques
  (IJIST) Vol.4, No.1/2, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Substrate Integrated Waveguide (SIW) technology is an attractive approach
for the design of high performance microwave and millimeter wave components, as
it combines the advantages of planar technology, such as low fabrication costs,
with the low loss inherent to the waveguide solution. In this study, a
substrate integrated waveguide power divider, circulator and coupler are
conceived and optimized in [10-15] GHz band by Ansoft HFSS code. Thus, results
of this modeling are presented, discussed and allow to integrate these devices
in planar circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2889</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2889</id><created>2014-04-10</created><authors><author><keyname>Abduljalil</keyname><forenames>Fekri M.</forenames></author></authors><title>A Novel Real-Time Video and Data Capture of Vehicular Accident in
  Intelligent Transportation Systems</title><categories>cs.NI</categories><doi>10.5121/ijcnc.2014.6205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel real-time video and data capture of vehicle accident
is proposed in Intelligent Transportation System (ITS). The proposed scheme
solves the problem of huge storage needed for recording vehicle accident in the
smart vehicle and in the remote ITS server. It works efficiently with small
amount of storage size and guarantee saving accident video in secondary
storage. It enables user to capture real-time video and data of running
vehicle. It enables user to get vehicle accident video and data anytime
anywhere. The scheme is implemented using testbed and its performance is
evaluated. The results show that the proposed scheme guarantees record the
vehicle accident in the ITS server. The proposed scheme has better results in
comparison with full time video recording scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2891</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2891</id><created>2014-04-10</created><authors><author><keyname>Tang</keyname><forenames>Ping Tak Peter</forenames></author><author><keyname>Kestyn</keyname><forenames>James</forenames></author><author><keyname>Polizzi</keyname><forenames>Eric</forenames></author></authors><title>A New Highly Parallel Non-Hermitian Eigensolver</title><categories>math.NA cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating portions of eigenvalues and eigenvectors of matrices or matrix
pencils has many applications. An approach to this calculation for Hermitian
problems based on a density matrix has been proposed in 2009 and a software
package called FEAST has been developed. The density-matrix approach allows
FEAST's implementation to exploit a key strength of modern computer
architectures, namely, multiple levels of parallelism. Consequently, the
software package has been well received and subsequently commercialized. A
detailed theoretical analysis of Hermitian FEAST has also been established very
recently. This paper generalizes the FEAST algorithm and theory, for the first
time, to tackle non-Hermitian problems. Fundamentally, the new algorithm is
basic subspace iteration or Bauer bi-iteration, except applied with a novel
accelerator based on Cauchy integrals. The resulting algorithm retains the
multi-level parallelism of Hermitian FEAST, making it a valuable new tool for
large-scale computational science and engineering problems on leading-edge
computing platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2892</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2892</id><created>2014-04-10</created><authors><author><keyname>Pashazadeh</keyname><forenames>Saeid</forenames></author><author><keyname>Saeedvand</keyname><forenames>Saeed</forenames></author></authors><title>Modelling of Walking Humanoid Robot With Capability of Floor Detection
  and Dynamic Balancing Using Colored Petri Net</title><categories>cs.RO</categories><comments>10 Pages, 5 Figures</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol. 4, No. 2, pp. 1-10, March 2014</journal-ref><doi>10.5121/ijfcst.2014.4201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most humanoid robots have highly complicated structure and design of robots
that are very similar to human is extremely difficult. In this paper, modelling
of a general and comprehensive algorithm for control of humanoid robots is
presented using Colored Petri Nets. For keeping dynamic balance of the robot,
combination of Gyroscope and Accelerometer sensors are used in algorithm. Image
processing is used to identify two fundamental issues: first, detection of
target or an object which robot must follow; second, detecting surface of the
ground so that walking robot could maintain its balance just like a human and
shows its best performance. Presented model gives high-level view of humanoid
robot's operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2903</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2903</id><created>2014-04-02</created><authors><author><keyname>Leordeanu</keyname><forenames>Marius</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author></authors><title>Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep
  Object Recognition</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general multi-class visual recognition model, termed the
Classifier Graph, which aims to generalize and integrate ideas from many of
today's successful hierarchical recognition approaches. Our graph-based model
has the advantage of enabling rich interactions between classes from different
levels of interpretation and abstraction. The proposed multi-class system is
efficiently learned using step by step updates. The structure consists of
simple logistic linear layers with inputs from features that are automatically
selected from a large pool. Each newly learned classifier becomes a potential
new feature. Thus, our feature pool can consist both of initial manually
designed features as well as learned classifiers from previous steps (graph
nodes), each copied many times at different scales and locations. In this
manner we can learn and grow both a deep, complex graph of classifiers and a
rich pool of features at different levels of abstraction and interpretation.
Our proposed graph of classifiers becomes a multi-class system with a recursive
structure, suitable for deep detection and recognition of several classes
simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2904</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2904</id><created>2014-04-10</created><updated>2014-12-12</updated><authors><author><keyname>Kositwattanarerk</keyname><forenames>Wittawat</forenames></author><author><keyname>Ong</keyname><forenames>Soon Sheng</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>Construction A of Lattices over Number Fields and Block Fading Wiretap
  Coding</title><categories>cs.IT math.IT math.NT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a lattice construction from totally real and CM fields, which
naturally generalizes the Construction A of lattices from $p$-ary codes
obtained from the cyclotomic field $\mathbb{Q}(\zeta_p)$, $p$ a prime, which in
turn contains the so-called Construction A of lattices from binary codes as a
particular case. We focus on the maximal totally real subfield
$\mathbb{Q}(\zeta_{p^r}+\zeta_{p}^{-r})$ of the cyclotomic field
$\mathbb{Q}(\zeta_{p^r})$, $r\geq 1$. Our construction has applications to
coset encoding of algebraic lattice codes, and we detail the case of coset
encoding of block fading wiretap codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2921</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2921</id><created>2014-04-10</created><authors><author><keyname>Wei</keyname><forenames>Xing</forenames></author><author><keyname>Aurzada</keyname><forenames>Frank</forenames></author><author><keyname>McGarry</keyname><forenames>Michael P.</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author></authors><title>DyCaPPON: Dynamic Circuit and Packet Passive Optical Network (Extended
  Version)</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic circuits are well suited for applications that require predictable
service with a constant bit rate for a prescribed period of time, such as cloud
computing and e-science applications. Past research on upstream transmission in
passive optical networks (PONs) has mainly considered packet-switched traffic
and has focused on optimizing packet-level performance metrics, such as
reducing mean delay. This study proposes and evaluates a dynamic circuit and
packet PON (DyCaPPON) that provides dynamic circuits along with packet-switched
service. DyCaPPON provides $(i)$ flexible packet-switched service through
dynamic bandwidth allocation in periodic polling cycles, and $(ii)$ consistent
circuit service by allocating each active circuit a fixed-duration upstream
transmission window during each fixed-duration polling cycle. We analyze
circuit-level performance metrics, including the blocking probability of
dynamic circuit requests in DyCaPPON through a stochastic knapsack-based
analysis. Through this analysis we also determine the bandwidth occupied by
admitted circuits. The remaining bandwidth is available for packet traffic and
we conduct an approximate analysis of the resulting mean delay of packet
traffic. Through extensive numerical evaluations and verifying simulations we
demonstrate the circuit blocking and packet delay trade-offs in DyCaPPON.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2923</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2923</id><created>2014-04-10</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Self-organization towards optimally interdependent networks by means of
  coevolution</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>14 pages, 4 figures; accepted for publication in New Journal of
  Physics</comments><journal-ref>New J. Phys. 16 (2014) 033041</journal-ref><doi>10.1088/1367-2630/16/3/033041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coevolution between strategy and network structure is established as a means
to arrive at optimal conditions for resolving social dilemmas. Yet recent
research highlights that the interdependence between networks may be just as
important as the structure of an individual network. We therefore introduce
coevolution of strategy and network interdependence to study whether it can
give rise to elevated levels of cooperation in the prisoner's dilemma game. We
show that the interdependence between networks self-organizes so as to yield
optimal conditions for the evolution of cooperation. Even under extremely
adverse conditions cooperators can prevail where on isolated networks they
would perish. This is due to the spontaneous emergence of a two-class society,
with only the upper class being allowed to control and take advantage of the
interdependence. Spatial patterns reveal that cooperators, once arriving to the
upper class, are much more competent than defectors in sustaining compact
clusters of followers. Indeed, the asymmetric exploitation of interdependence
confers to them a strong evolutionary advantage that may resolve even the
toughest of social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2939</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2939</id><created>2014-04-10</created><authors><author><keyname>Pashazadeh</keyname><forenames>Saied</forenames></author><author><keyname>Niyari</keyname><forenames>Elham Abdolrahimi</forenames></author></authors><title>Modeling Enterprise Architecture Using Timed Colored PETRI Net: Single
  Processor Scheduling</title><categories>cs.SE</categories><comments>13 pages, 5 figures</comments><journal-ref>International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT), Vol. 5, No. 1, March 2014</journal-ref><doi>10.5121/ijmpict.2014.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of modeling enterprise architecture and analysis of it is to ease
decision making about architecture of information systems. Planning is one of
the most important tasks in an organization and has a major role in increasing
the productivity of it. Scope of this paper is scheduling processes in the
enterprise architecture. Scheduling is decision making on execution start time
of processes that are used in manufacturing and service systems. Different
methods and tools have been proposed for modeling enterprise architecture.
Colored Petri net is extension of traditional Petri net that its modeling
capability has grown dramatically. A developed model with Colored Petri net is
suitable for verification of operational aspects and performance evaluation of
information systems. With having ability of hierarchical modeling, colored
Petri nets permits that using predesigned modules for smaller parts of the
system and with a general algorithm, any kind of enterprise architecture can be
modeled. A two level hierarchical model is presented as a building block for
modeling architecture of Transaction Processing Systems (TPS) in this paper.
This model schedules and runs processes based on a predetermined non-preemptive
scheduling method. The model can be used for scheduling of processes with four
non-preemptive methods named, priority based (PR), shortest job first (SJF),
first come first served (FCFS) and highest response ratio next (HRRN). The
presented model is designed such can be used as one of the main components in
modeling any type of enterprise architecture. Most enterprise architectures can
be modeled by putting together appropriate number of these modules and proper
composition of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2943</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2943</id><created>2014-04-10</created><updated>2015-01-07</updated><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Lehmann</keyname><forenames>Sebastian</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Orthogonal Graph Drawing with Inflexible Edges</title><categories>cs.DS cs.DM</categories><comments>23 pages, 5 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of creating plane orthogonal drawings of 4-planar
graphs (planar graphs with maximum degree 4) with constraints on the number of
bends per edge. More precisely, we have a flexibility function assigning to
each edge $e$ a natural number $\mathrm{flex}(e)$, its flexibility. The problem
FlexDraw asks whether there exists an orthogonal drawing such that each edge
$e$ has at most $\mathrm{flex}(e)$ bends. It is known that FlexDraw is NP-hard
if $\mathrm{flex}(e) = 0$ for every edge $e$. On the other hand, FlexDraw can
be solved efficiently if $\mathrm{flex}(e) \ge 1$ and is trivial if
$\mathrm{flex}(e) \ge 2$ for every edge $e$.
  To close the gap between the NP-hardness for $\mathrm{flex}(e) = 0$ and the
efficient algorithm for $\mathrm{flex}(e) \ge 1$, we investigate the
computational complexity of FlexDraw in case only few edges are inflexible
(i.e., have flexibility~$0$). We show that for any $\varepsilon &gt; 0$ FlexDraw
is NP-complete for instances with $O(n^\varepsilon)$ inflexible edges with
pairwise distance $\Omega(n^{1-\varepsilon})$ (including the case where they
induce a matching). On the other hand, we give an FPT-algorithm with running
time $O(2^k\cdot n \cdot T_{\mathrm{flow}}(n))$, where $T_{\mathrm{flow}}(n)$
is the time necessary to compute a maximum flow in a planar flow network with
multiple sources and sinks, and $k$ is the number of inflexible edges having at
least one endpoint of degree 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2946</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2946</id><created>2014-04-10</created><authors><author><keyname>Aslanidis</keyname><forenames>Timotheos</forenames></author><author><keyname>Kogias</keyname><forenames>Marios-Evangelos</forenames></author></authors><title>Algorithms for Packet Routing in Switching Networks with Reconfiguration
  Overhead</title><categories>cs.NI</categories><comments>7 pages, 2 figures, CCNET 2014 Conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Given a set of messages to be transmitted in packages from a set of sending
stations to a set of receiving stations, we are required to schedule the
packages so as to achieve the minimum possible time from the moment the 1st
transmission initiates to the concluding of the last. Preempting packets in
order to reroute message remains, as part of some other packet to be
transmitted at a later time would be a great means to achieve our goal, if not
for the fact that each preemption will come with a reconfiguration cost that
will delay our entire effort. The problem has been extensively studied in the
past and various algorithms have been proposed to handle many variations of the
problem. In this paper we propose an improved algorithm that we call the
Split-Graph Algorithm (SGA). To establish its efficiency we compare it, to two
of the algorithms developed in the past. These two are the best presented in
bibliography so far, one in terms of approximation ratio and one in terms of
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2948</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2948</id><created>2014-04-10</created><authors><author><keyname>Wang</keyname><forenames>Bo</forenames></author><author><keyname>Goldenberg</keyname><forenames>Anna</forenames></author></authors><title>Gradient-based Laplacian Feature Selection</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of high dimensional noisy data is of essence across a variety of
research fields. Feature selection techniques are designed to find the relevant
feature subset that can facilitate classification or pattern detection.
Traditional (supervised) feature selection methods utilize label information to
guide the identification of relevant feature subsets. In this paper, however,
we consider the unsupervised feature selection problem. Without the label
information, it is particularly difficult to identify a small set of relevant
features due to the noisy nature of real-world data which corrupts the
intrinsic structure of the data. Our Gradient-based Laplacian Feature Selection
(GLFS) selects important features by minimizing the variance of the Laplacian
regularized least squares regression model. With $\ell_1$ relaxation, GLFS can
find a sparse subset of features that is relevant to the Laplacian manifolds.
Extensive experiments on simulated, three real-world object recognition and two
computational biology datasets, have illustrated the power and superior
performance of our approach over multiple state-of-the-art unsupervised feature
selection methods. Additionally, we show that GLFS selects a sparser set of
more relevant features in a supervised setting outperforming the popular
elastic net methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2952</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2952</id><created>2014-04-10</created><authors><author><keyname>Oussama</keyname><forenames>Noui</forenames></author><author><keyname>Lemnouar</keyname><forenames>Noui</forenames></author></authors><title>A blind robust watermarking scheme based on svd and circulant matrices</title><categories>cs.MM cs.CR</categories><comments>12 pages, 6 figures, 5 tables, Second International Conference on
  Computational Science &amp; Engineering (CSE - 2014)</comments><journal-ref>Second International Conference on Computational Science and
  Engineering (CSE-2014) Dubai, UAE, April 04 ~ 05 - 2014 Volume Editors:
  Dhinaharan Nagamalai, Sundarapandian Vaidyanathan ISBN : 978-1-921987-30-4 pp
  65 - 77</journal-ref><doi>10.5121/csit.2014.4406</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Multimedia security has been the aim point of considerable research activity
because of its wide application area. The major technology to achieve copyright
protection, content authentication, access control and multimedia security is
watermarking which is the process of embedding data into a multimedia element
such as image or audio, this embedded data can later be extracted from, or
detected in the embedded element for different purposes. In this work, a blind
watermarking algorithm based on SVD and circulant matrices has been presented.
Every circulant matrix is associated with a matrix for which the SVD
decomposition coincides with the spectral decomposition. This leads to improve
the Chandra algorithm [1], our presentation will include a discussion on the
data hiding capacity, watermark transparency and robustness against a wide
range of common image processing attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2954</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2954</id><created>2014-04-10</created><authors><author><keyname>Rahwan</keyname><forenames>Talal</forenames></author><author><keyname>Michalak</keyname><forenames>Tomasz</forenames></author><author><keyname>Wooldridge</keyname><forenames>Michael</forenames></author></authors><title>A Measure of Synergy in Coalitions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the performance of a team of agents exceeds our expectations or fall
short of them, we often explain this by saying that there was some synergy in
the team---either positive (the team exceeded our expectations) or negative
(they fell short). Our aim in this article is to develop a formal and
principled way of measuring synergies, both positive and negative. Using
characteristic function cooperative games as our underlying model, we present a
formal measure of synergy, based on the idea that a synergy is exhibited when
the performance of a team deviates from the norm. We then show that our synergy
value is the only possible such measure that satisfies certain intuitive
properties. We then investigate some alternative characterisations of this
measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2959</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2959</id><created>2014-04-10</created><authors><author><keyname>Klasen</keyname><forenames>Bernd</forenames></author></authors><title>SocioAware Content Distribution using P2P solutions in Hybrid Networks</title><categories>cs.SI cs.NI</categories><comments>accepted at The 18th IEEE Symposium on Computers and Communications
  (ISCC 2013) but has not been presented and published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing online traffic that is bringing the infrastructure to its limits
induces an urgent demand for an efficient content delivery model. Capitalizing
social networks and using advanced delivery networks potentially can help to
solve this problem. However, due to the complex nature of the involved networks
such a model is difficult to assess. In this paper we use a simulative approach
to analyze how the SatTorrent P2P protocol supported by social networks can
improve content delivery by means of reduced download duration and traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2962</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2962</id><created>2014-04-10</created><authors><author><keyname>Johnsen</keyname><forenames>Aleck C.</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Computing Minimum Tile Sets to Self-Assemble Colors Patterns</title><categories>cs.CC</categories><acm-class>F.1.1</acm-class><journal-ref>Proceedings from the 24th International Symposium on Agorithms and
  Computation, ISAAC 2013, Hong Kong, China. Springer-Verlag Berlin Heidelberg.
  699-710</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patterned self-assembly tile set synthesis (PATS) aims at finding a minimum
tile set to uniquely self-assemble a given rectangular color pattern. For $k
\ge 1$, $k$-PATS is a variant of PATS that restricts input patterns to those
with at most $k$ colors. We prove the {\bf NP}-hardness of 29-PATS, where the
best known is that of 60-PATS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2964</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2964</id><created>2014-04-10</created><authors><author><keyname>Chen</keyname><forenames>Richard Li-Yang</forenames></author><author><keyname>Fan</keyname><forenames>Neng</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Watson</keyname><forenames>Jean-Paul</forenames></author></authors><title>Contingency-Constrained Unit Commitment with Post-Contingency Corrective
  Recourse</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing costs in the generation unit commitment
problem, a cornerstone in electric power system operations, while enforcing an
N-k-e reliability criterion. This reliability criterion is a generalization of
the well-known $N$-$k$ criterion, and dictates that at least $(1-e_ j)$
fraction of the total system demand must be met following the failures of $k$
or fewer system components. We refer to this problem as the
Contingency-Constrained Unit Commitment problem, or CCUC. We present a
mixed-integer programming formulation of the CCUC that accounts for both
transmission and generation element failures. We propose novel cutting plane
algorithms that avoid the need to explicitly consider an exponential number of
contingencies. Computational studies are performed on several IEEE test systems
and a simplified model of the Western US interconnection network, which
demonstrate the effectiveness of our proposed methods relative to current
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2983</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2983</id><created>2014-04-10</created><authors><author><keyname>Prattico</keyname><forenames>Flavio</forenames></author><author><keyname>Dzahir</keyname><forenames>Mohd Azuwan Mat</forenames></author><author><keyname>Yamamoto</keyname><forenames>Shin-ichiroh</forenames></author></authors><title>Couple Control Model Implementation on Antagonistic Mono- and
  Bi-Articular Actuators</title><categories>physics.med-ph cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, robot assisted therapy devices are increasingly used for spinal
cord injury (SCI) rehabilitation in assisting handicapped patients to regain
their impaired movements. Assistive robotic systems may not be able to cure or
fully compensate impairments, but it should be able to assist certain impaired
functions and ease movements. In this study, a couple control model for
lower-limb orthosis of a body weight support gait training system is proposed.
The developed leg orthosis implements the use of pneumatic artificial muscle as
an actuation system. The pneumatic muscle was arranged antagonistically to form
two pair of mono-articular muscles (i.e., hip and knee joints), and a pair of
bi-articular actuators (i.e., rectus femoris and hamstring). The results of the
proposed couple control model showed that, it was able to simultaneously
control the antagonistic mono- and bi-articular actuators and sufficiently
performed walking motion of the leg orthosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2984</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2984</id><created>2014-04-10</created><authors><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Fremont</keyname><forenames>Daniel J.</forenames></author><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>Distribution-Aware Sampling and Weighted Model Counting for SAT</title><categories>cs.AI cs.DS</categories><comments>This is a full version of AAAI 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a CNF formula and a weight for each assignment of values to variables,
two natural problems are weighted model counting and distribution-aware
sampling of satisfying assignments. Both problems have a wide variety of
important applications. Due to the inherent complexity of the exact versions of
the problems, interest has focused on solving them approximately. Prior work in
this area scaled only to small problems in practice, or failed to provide
strong theoretical guarantees, or employed a computationally-expensive maximum
a posteriori probability (MAP) oracle that assumes prior knowledge of a
factored representation of the weight distribution. We present a novel approach
that works with a black-box oracle for weights of assignments and requires only
an {\NP}-oracle (in practice, a SAT-solver) to solve both the counting and
sampling problems. Our approach works under mild assumptions on the
distribution of weights of satisfying assignments, provides strong theoretical
guarantees, and scales to problems involving several thousand variables. We
also show that the assumptions can be significantly relaxed while improving
computational efficiency if a factored representation of the weights is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2986</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2986</id><created>2014-04-10</created><authors><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author></authors><title>A Tutorial on Independent Component Analysis</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent component analysis (ICA) has become a standard data analysis
technique applied to an array of problems in signal processing and machine
learning. This tutorial provides an introduction to ICA based on linear algebra
formulating an intuition for ICA from first principles. The goal of this
tutorial is to provide a solid foundation on this advanced topic so that one
might learn the motivation behind ICA, learn why and when to apply this
technique and in the process gain an introduction to this exciting field of
active research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2989</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2989</id><created>2014-04-10</created><authors><author><keyname>Gunawan</keyname><forenames>David</forenames></author><author><keyname>Budiono</keyname><forenames>Karno</forenames></author></authors><title>Comparative Analysis On Some Possible Partnership Schemes of Global IP
  Exchange Providers</title><categories>cs.NI</categories><comments>12 pages, 8 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.2, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IP eXchange or IPX is GSMA proposal for IP interconnection model which
supports multi services to offer end-to-end QoS, security, interoperability,
SLAs through a dedicated connection. There are some possible partnership
schemes between IPX providers such as peering mode, semi-hosted mode,
full-hosted mode, or combination between these modes. The implementation of the
schemes will be case-by-case basis with some considerations based on, but not
limited to, IPX Providers network asset, coverage, services, features offer,
commercial offer, and customers. For an IPX provider to become competitive in
IPX business and become a global IPX hub, some value added should be considered
such as cost efficiency and great network performance. To achieve it, an IPX
provider could implement some strategies such as build network sinergy between
them and partners to develop IPX Service as single offering, offer their
customers with bundled access network and services. An IPX provider should also
consider their existing customer-based that can be a benefit to their
bargaining position to other potential IPX provider partners to determine price
and business scheme for partnership.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2993</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2993</id><created>2014-04-10</created><authors><author><keyname>Yu</keyname><forenames>L.</forenames></author><author><keyname>Liu</keyname><forenames>H.</forenames></author><author><keyname>Zheng</keyname><forenames>D.</forenames></author></authors><title>On More Bent Functions From Dillon Exponents</title><categories>cs.IT math.IT</categories><comments>16 papges</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain a new class of $p$-ary binomial bent functions which
are determined by Kloosterman sums. The bentness of another three classes of
functions is characterized by some exponential sums and some results in
\cite{Linian2013} are generalized. Furthermore we obtain, in some special
cases, some bent functions are determined by Kloosterman sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2997</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2997</id><created>2014-04-11</created><authors><author><keyname>Ganascia</keyname><forenames>Jean-Gabriel</forenames><affiliation>LIP6</affiliation></author><author><keyname>Glaudes</keyname><forenames>Pierre</forenames><affiliation>CELFF XVI-XXI</affiliation></author><author><keyname>Del Lungo</keyname><forenames>Andrea</forenames><affiliation>ALITHILA</affiliation></author></authors><title>Automatic Detection of Reuses and Citations in Literary Texts</title><categories>cs.CL cs.DL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For more than forty years now, modern theories of literature (Compagnon,
1979) insist on the role of paraphrases, rewritings, citations, reciprocal
borrowings and mutual contributions of any kinds. The notions of
intertextuality, transtextuality, hypertextuality/hypotextuality, were
introduced in the seventies and eighties to approach these phenomena. The
careful analysis of these references is of particular interest in evaluating
the distance that the creator voluntarily introduces with his/her masters.
Phoebus is collaborative project that makes computer scientists from the
University Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary
teams of Paris-Sorbonne University with the aim to develop efficient tools for
literary studies that take advantage of modern computer science techniques. In
this context, we have developed a piece of software that automatically detects
and explores networks of textual reuses in classical literature. This paper
describes the principles on which is based this program, the significant
results that have already been obtained and the perspectives for the near
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.2999</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.2999</id><created>2014-04-11</created><authors><author><keyname>Shi</keyname><forenames>Tianlin</forenames></author><author><keyname>Ming</keyname><forenames>Liang</forenames></author><author><keyname>Hu</keyname><forenames>Xiaolin</forenames></author></authors><title>A Reverse Hierarchy Model for Predicting Eye Fixations</title><categories>cs.CV</categories><comments>CVPR 2014, 27th IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR). CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of psychological and physiological evidences suggest that early
visual attention works in a coarse-to-fine way, which lays a basis for the
reverse hierarchy theory (RHT). This theory states that attention propagates
from the top level of the visual hierarchy that processes gist and abstract
information of input, to the bottom level that processes local details.
Inspired by the theory, we develop a computational model for saliency detection
in images. First, the original image is downsampled to different scales to
constitute a pyramid. Then, saliency on each layer is obtained by image
super-resolution reconstruction from the layer above, which is defined as
unpredictability from this coarse-to-fine reconstruction. Finally, saliency on
each layer of the pyramid is fused into stochastic fixations through a
probabilistic model, where attention initiates from the top layer and
propagates downward through the pyramid. Extensive experiments on two standard
eye-tracking datasets show that the proposed method can achieve competitive
results with state-of-the-art models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3001</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3001</id><created>2014-04-11</created><authors><author><keyname>Wang</keyname><forenames>Runxin</forenames></author><author><keyname>Liu</keyname><forenames>Rongke</forenames></author><author><keyname>Hou</keyname><forenames>Yi</forenames></author></authors><title>Joint Successive Cancellation Decoding of Polar Codes over Intersymbol
  Interference Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are a class of capacity-achieving codes for the binary-input
discrete memoryless channels (B-DMCs). However, when applied in channels with
intersymbol interference (ISI), the codes may perform poorly with BCJR
equalization and conventional decoding methods. To deal with the ISI problem,
in this paper a new joint successive cancellation (SC) decoding algorithm is
proposed for polar codes in ISI channels, which combines the equalization and
conventional decoding. The initialization information of the decoding method is
the likelihood functions of ISI codeword symbols rather than the codeword
symbols. The decoding adopts recursion formulas like conventional SC decoding
and is without iterations. This is in contrast to the conventional iterative
algorithm which performs iterations between the equalizer and decoder. In
addition, the proposed SC trellis decoding can be easily extended to list
decoding which can further improve the performance. Simulation shows that the
proposed scheme significantly outperforms the conventional decoding schemes in
ISI channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3002</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3002</id><created>2014-04-11</created><authors><author><keyname>Kodge</keyname><forenames>B. G.</forenames></author><author><keyname>Hiremath</keyname><forenames>P. S.</forenames></author></authors><title>Elevation Contour Analysis and Water body Extraction for Finding Water
  Scarcity Locations using DEM</title><categories>cs.OH</categories><comments>6 pages, 7 figures</comments><journal-ref>World Journal of Science and Technology 2011, 1(12): 29-34 World
  Journal of Science and Technology 2011, 1(12)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presents study was aimed to create new methods for extraction and
analysis of land elevation contour lines, automatic extraction of water bodies
(river basins and lakes), from the digital elevation models (DEM) of a test
area. And extraction of villages which are fell under critical water scarcity
regions for agriculture and drinking water with respect to their elevation data
and available natural water resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3010</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3010</id><created>2014-04-11</created><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>On the Energy-Spectral Efficiency Trade-off of the MRC Receiver in
  Massive MIMO Systems with Transceiver Power Consumption</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, submitted to IEEE Globecom 2014. arXiv admin
  note: text overlap with arXiv:1401.4907</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the uplink of a multiuser massive MIMO system wherein a base
station (BS) having $M$ antennas communicates coherently with $K$ single
antenna user terminals (UTs). We study the energy efficiency of this system
while taking the transceiver power consumption at the UTs and the BS into
consideration. For a given spectral efficiency $R$ and fixed transceiver power
consumption parameters, we propose and analyze the problem of maximizing the
energy efficiency as a function of $(M,K)$. For the maximum ratio combining
(MRC) detector at the BS we show that with increasing $R$, $(M,K)$ can be
adaptively increased in such a way that the energy efficiency converges to a
positive constant as $R \rightarrow \infty$ ($(M,K)$ is increased in such a way
that a constant per-user spectral efficiency $R/K$ is maintained). This is in
contrast to the fixed $(M,K)$ scenario where the energy efficiency is known to
converge to zero as $R \rightarrow \infty$. We also observe that for large $R$,
the optimal $(M,K)$ maximizing the energy efficiency is such that, the total
power consumed by the power amplifiers (PA) in all the $K$ UTs is a small
fraction of the total system power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3011</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3011</id><created>2014-04-11</created><authors><author><keyname>Abid</keyname><forenames>Sohail</forenames></author><author><keyname>Khan</keyname><forenames>Shahab</forenames></author></authors><title>Improving Performance of Routing Protocols Using MRP Framework</title><categories>cs.NI</categories><comments>8 pages, 8 figures and 1 table</comments><journal-ref>International Journal of Ambient Systems and Applications (IJASA)
  Vol.2, No.1, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These days MANET is an amazing remarkably altering or rising technology, for
the reason that of its elite nature of scattered mobile devices and self
motivated network topology. The mobile adhoc routing protocol follows several
principles in wireless MANETs. The up to date and novel applications based on
wireless technology are being produced in the private as well as commercial
sectors. A lot of challenges which are facing wireless MANETs like network
stability, security, energy efficiency and performance analysis etc. At present
wireless adhoc network get much more attention because of its accessibility
everywhere. As a result researchers produce several routing protocols. In this
paper first of all we analyzed the performance investigation of wireless
routing protocols on the basis of ROH, throughput, end to end delay and PDR.
After that we proposed an Mixed Routing Protocol framework which improve
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3012</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3012</id><created>2014-04-11</created><updated>2014-08-18</updated><authors><author><keyname>Tanaka</keyname><forenames>Kazuyuki</forenames></author><author><keyname>Kataoka</keyname><forenames>Shun</forenames></author><author><keyname>Yasuda</keyname><forenames>Muneki</forenames></author><author><keyname>Waizumi</keyname><forenames>Yuji</forenames></author><author><keyname>Hsu</keyname><forenames>Chiou-Ting</forenames></author></authors><title>Bayesian image segmentations by Potts prior and loopy belief propagation</title><categories>cs.CV cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><journal-ref>Journal of the Physical Society of Japan 83 (2014) 124002</journal-ref><doi>10.7566/JPSJ.83.124002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Bayesian image segmentation model based on Potts prior
and loopy belief propagation. The proposed Bayesian model involves several
terms, including the pairwise interactions of Potts models, and the average
vectors and covariant matrices of Gauss distributions in color image modeling.
These terms are often referred to as hyperparameters in statistical machine
learning theory. In order to determine these hyperparameters, we propose a new
scheme for hyperparameter estimation based on conditional maximization of
entropy in the Potts prior. The algorithm is given based on loopy belief
propagation. In addition, we compare our conditional maximum entropy framework
with the conventional maximum likelihood framework, and also clarify how the
first order phase transitions in LBP's for Potts models influence our
hyperparameter estimation procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3017</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3017</id><created>2014-04-11</created><authors><author><keyname>Barta</keyname><forenames>Gergo</forenames></author></authors><title>A Link-based Approach to Entity Resolution in Social Networks</title><categories>cs.IR cs.DS cs.SI</categories><comments>10 pages, 5 figures, 2 tables, Second International Conference of
  Database and Data Mining (DBDM 2014)</comments><msc-class>62-07</msc-class><doi>10.5121/csit.2014.4409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks initially had been places for people to contact each other,
find friends or new acquaintances. As such they ever proved interesting for
machine aided analysis. Recent developments, however, pivoted social networks
to being among the main fields of information exchange, opinion expression and
debate. As a result there is growing interest in both analyzing and integrating
social network services. In this environment efficient information retrieval is
hindered by the vast amount and varying quality of the user-generated content.
Guiding users to relevant information is a valuable service and also a
difficult task, where a crucial part of the process is accurately resolving
duplicate entities to real-world ones. In this paper we propose a novel
approach that utilizes the principles of link mining to successfully extend the
methodology of entity resolution to multitype problems. The proposed method is
presented using an illustrative social network-based real-world example and
validated by comprehensive evaluation of the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3018</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3018</id><created>2014-04-11</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Jin</keyname><forenames>Yichao</forenames></author><author><keyname>Zhang</keyname><forenames>Weiwen</forenames></author><author><keyname>Wen</keyname><forenames>Yonggang</forenames></author></authors><title>Enhancing User Experience for Multi-Screen Social TV Streaming over
  Wireless Networks</title><categories>cs.MM</categories><comments>submitted to IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, multi-screen cloud social TV is invented to transform TV into
social experience. People watching the same content on social TV may come from
different locations, while freely interact with each other through text, image,
audio and video. This crucial virtual living-room experience adds social
aspects into existing performance metrics. In this paper, we parse social TV
user experience into three elements (i.e., inter-user delay, video quality of
experience (QoE), and resource efficiency), and provide a joint analytical
framework to enhance user experience. Specifically, we propose a cloud-based
optimal playback rate allocation scheme to maximize the overall QoE while upper
bounding inter-user delay. Experiment results show that our algorithm achieves
near-optimal tradeoff between inter-user delay and video quality, and
demonstrates resilient performance even under very fast wireless channel
fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3020</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3020</id><created>2014-04-11</created><authors><author><keyname>M</keyname><forenames>Kumaraswamy</forenames></author><author><keyname>K</keyname><forenames>Shaila</forenames></author><author><keyname>V</keyname><forenames>Tejaswi</forenames></author><author><keyname>R</keyname><forenames>Venugopal K</forenames></author><author><keyname>Iyengar</keyname><forenames>S S</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>QoS group based optimal retransmission medium access protocol for
  wireless sensor networks</title><categories>cs.NI</categories><comments>9 pages in IEEE format and 6 figures</comments><journal-ref>IJCNC, Vol.6, No.2, March 2014</journal-ref><doi>10.5121/ijcnc.2014.6206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents, a Group Based Optimal Retransmission Medium Access
(GORMA) Protocol is designed that combines protocol of Collision Avoidance (CA)
and energy management for low-cost, short-range, low-data rate and low-energy
sensor nodes applications in environment monitoring, agriculture, industrial
plants etc. In this paper, the GORMA protocol focuses on efficient MAC protocol
to provide autonomous Quality of Service (QoS) to the sensor nodes in one-hop
QoS retransmission group and two QoS groups in WSNs where the source nodes do
not have receiver circuits. Hence, they can only transmit data to a sink node,
but cannot receive acknowledgement control signals from the sink node. The
proposed protocol GORMA provides QoS to the nodes which work independently on
predefined time by allowing them to transmit each packet an optimal number of
times within a given period. Our simulation results shows that the performance
of GORMA protocol, which maximize the delivery probability of one-hop QoS group
and two QoS groups and minimize the energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3022</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3022</id><created>2014-04-11</created><authors><author><keyname>Nielsen</keyname><forenames>Johan Sebastian Rosenkilde</forenames><affiliation>INT - University of Ulm.</affiliation></author><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author></authors><title>Multi-Trial Guruswami-Sudan Decoding for Generalised Reed--Solomon Codes</title><categories>cs.IT math.IT</categories><comments>Design Codes and Cryptography (2014)</comments><proxy>ccsd</proxy><report-no>DCC2014</report-no><doi>10.1007/s10623-014-9951-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An iterated refinement procedure for the Guruswami-Sudan list decoding
algorithm for Generalised Reed-Solomon codes based on Alekhnovich's module
minimisation is proposed. The method is parametrisable and allows variants of
the usual list decoding approach. In particular, finding the list of closest
codewords within an intermediate radius can be performed with improved
average-case complexity while retaining the worst-case complexity. We provide a
detailed description of the module minimisation, reanalysing the
Mulders-Storjohann algorithm and drawing new connections to both Alekhnovich's
algorithm and Lee-O'Sullivan's. Furthermore, we show how to incorporate the
re-encoding technique of K\&quot;otter and Vardy into our iterative algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3023</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3023</id><created>2014-04-11</created><updated>2014-12-06</updated><authors><author><keyname>Chotard</keyname><forenames>Alexandre</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Markov Chain Analysis of Evolution Strategies on a Linear Constraint
  Optimization Problem</title><categories>cs.NE math.OC</categories><comments>Amir Hussain; Zhigang Zeng; Nian Zhang. IEEE Congress on Evolutionary
  Computation, Jul 2014, Beijing, China</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses a $(1,\lambda)$-Evolution Strategy, a randomised
comparison-based adaptive search algorithm, on a simple constraint optimisation
problem. The algorithm uses resampling to handle the constraint and optimizes a
linear function with a linear constraint. Two cases are investigated: first the
case where the step-size is constant, and second the case where the step-size
is adapted using path length control. We exhibit for each case a Markov chain
whose stability analysis would allow us to deduce the divergence of the
algorithm depending on its internal parameters. We show divergence at a
constant rate when the step-size is constant. We sketch that with step-size
adaptation geometric divergence takes place. Our results complement previous
studies where stability was assumed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3026</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3026</id><created>2014-04-11</created><authors><author><keyname>Bodnar</keyname><forenames>Todd</forenames></author><author><keyname>Barclay</keyname><forenames>Victoria C</forenames></author><author><keyname>Ram</keyname><forenames>Nilam</forenames></author><author><keyname>Tucker</keyname><forenames>Conrad S</forenames></author><author><keyname>Salath&#xe9;</keyname><forenames>Marcel</forenames></author></authors><title>On the Ground Validation of Online Diagnosis with Twitter and Medical
  Records</title><categories>cs.SI cs.CL cs.LG</categories><comments>Presented at of WWW2014. WWW'14 Companion, April 7-11, 2014, Seoul,
  Korea</comments><acm-class>I.2.1</acm-class><doi>10.1145/2567948.2579272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media has been considered as a data source for tracking disease.
However, most analyses are based on models that prioritize strong correlation
with population-level disease rates over determining whether or not specific
individual users are actually sick. Taking a different approach, we develop a
novel system for social-media based disease detection at the individual level
using a sample of professionally diagnosed individuals. Specifically, we
develop a system for making an accurate influenza diagnosis based on an
individual's publicly available Twitter data. We find that about half (17/35 =
48.57%) of the users in our sample that were sick explicitly discuss their
disease on Twitter. By developing a meta classifier that combines text
analysis, anomaly detection, and social network analysis, we are able to
diagnose an individual with greater than 99% accuracy even if she does not
discuss her health.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3033</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3033</id><created>2014-04-11</created><updated>2015-02-15</updated><authors><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author><author><keyname>Cordasco</keyname><forenames>Gennaro</forenames></author><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author><author><keyname>Peters</keyname><forenames>Joseph</forenames></author><author><keyname>Vaccaro</keyname><forenames>Ugo</forenames></author></authors><title>How to go Viral: Cheaply and Quickly</title><categories>cs.SI cs.DS math.CO</categories><comments>An extended abstract of this paper will appear in Proceedings of
  Seventh International conference on Fun with Algorithms (FUN 2014), Lectures
  Notes in Computer Science, Springer</comments><journal-ref>7th International Conference, FUN 2014, Lipari Island, Sicily,
  Italy, July 1-3, 2014. Proceedings ISBN 978-3-319-07889-2</journal-ref><doi>10.1007/978-3-319-07890-8_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a social network represented by a graph $G$, we consider the problem of
finding a bounded cardinality set of nodes $S$ with the property that the
influence spreading from $S$ in $G$ is as large as possible. The dynamics that
govern the spread of influence is the following: initially only elements in $S$
are influenced; subsequently at each round, the set of influenced elements is
augmented by all nodes in the network that have a sufficiently large number of
already influenced neighbors. While it is known that the general problem is
hard to solve --- even in the approximate sense --- we present exact polynomial
time algorithms for trees, paths, cycles, and complete graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3034</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3034</id><created>2014-04-11</created><updated>2014-06-05</updated><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames><affiliation>School of Computing, University of Dundee, UK</affiliation></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames><affiliation>School of Computing, University of Dundee, UK</affiliation></author></authors><title>ACL2(ml): Machine-Learning for ACL2</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 61-75</journal-ref><doi>10.4204/EPTCS.152.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ACL2(ml) is an extension for the Emacs interface of ACL2. This tool uses
machine-learning to help the ACL2 user during the proof-development. Namely,
ACL2(ml) gives hints to the user in the form of families of similar theorems,
and generates auxiliary lemmas automatically. In this paper, we present the two
most recent extensions for ACL2(ml). First, ACL2(ml) can suggest now families
of similar function definitions, in addition to the families of similar
theorems. Second, the lemma generation tool implemented in ACL2(ml) has been
improved with a method to generate preconditions using the guard mechanism of
ACL2. The user of ACL2(ml) can also invoke directly the latter extension to
obtain preconditions for his own conjectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3041</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3041</id><created>2014-04-11</created><updated>2014-10-29</updated><authors><author><keyname>Garc&#xed;a-Fern&#xe1;ndez</keyname><forenames>&#xc1;ngel F.</forenames></author><author><keyname>Morelande</keyname><forenames>Mark R.</forenames></author><author><keyname>Grajal</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>Labelled OSPA metric for fixed and known number of targets</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of multiple target tracking algorithms with labelled sets can
be done using the labelled optimal subpattern assignment (LOSPA) metric. In
this paper, we provide the expression of the same metric for fixed and known
number of targets when vector notation is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3056</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3056</id><created>2014-04-11</created><updated>2014-09-29</updated><authors><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Principles of Antifragile Software</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to study and define the concept of &quot;antifragile
software&quot;. For this, I start from Taleb's statement that antifragile systems
love errors, and discuss whether traditional software dependability fits into
this class. The answer is somewhat negative, although adaptive fault tolerance
is antifragile: the system learns something when an error happens, and always
imrpoves. Automatic runtime bug fixing is changing the code in response to
errors, fault injection in production means injecting errors in business
critical software. I claim that both correspond to antifragility. Finally, I
hypothesize that antifragile development processes are better at producing
antifragile software systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3063</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3063</id><created>2014-04-11</created><authors><author><keyname>Gupta</keyname><forenames>Richa</forenames></author><author><keyname>Gupta</keyname><forenames>Sunny</forenames></author><author><keyname>Singhal</keyname><forenames>Anuradha</forenames></author></authors><title>Importance and Techniques of Information Hiding : A Review</title><categories>cs.CR</categories><comments>6 pages, 6 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  9(5):1-6, March 2013</journal-ref><doi>10.14445/22312803/IJCTT-V9P149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information or data is very crucial resource to us. Thus securing the
information becomes all the more necessary. The communication media through
which we send data does not provide data security, so other methods of securing
data are required. Information hiding plays a very crucial role today. It
provided methods for encrypting the information so that it becomes unreadable
for any unintended user. This paper reviews the techniques that exist for data
hiding and how can these be combined to provide another level of security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3075</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3075</id><created>2014-04-11</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Ricciutelli</keyname><forenames>Giacomo</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Practical LDPC coded modulation schemes for the fading broadcast channel
  with confidential messages</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 4 figures, to be presented at IEEE ICC'14 - Workshop on
  Wireless Physical Layer Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcast channel with confidential messages is a well studied scenario
from the theoretical standpoint, but there is still lack of practical schemes
able to achieve some fixed level of reliability and security over such a
channel. In this paper, we consider a quasi-static fading channel in which both
public and private messages must be sent from the transmitter to the receivers,
and we aim at designing suitable coding and modulation schemes to achieve such
a target. For this purpose, we adopt the error rate as a metric, by considering
that reliability (security) is achieved when a sufficiently low (high) error
rate is experienced at the receiving side. We show that some conditions exist
on the system feasibility, and that some outage probability must be tolerated
to cope with the fading nature of the channel. The proposed solution exploits
low-density parity-check codes with unequal error protection, which are able to
guarantee two different levels of protection against noise for the public and
the private information, in conjunction with different modulation schemes for
the public and the private message bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3078</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3078</id><created>2014-04-11</created><updated>2015-03-31</updated><authors><author><keyname>Lindberg</keyname><forenames>Christopher</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author></authors><title>Distributed Compressed Sensing for Sensor Networks with Packet Erasures</title><categories>cs.IT math.IT</categories><comments>Paper accepted to GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two approaches to distributed compressed sensing for in-network data
compression and signal reconstruction at a sink in a wireless sensor network
where sensors are placed on a straight line. Communication to the sink is
considered to be bandwidth-constrained due to the large number of devices. By
using distributed compressed sensing for compression of the data in the
network, the communication cost (bandwith usage) to the sink can be decreased
at the expense of delay induced by the local communication necessary for
compression. We investigate the relation between cost and delay given a certain
reconstruction performance requirement when using basis pursuit denoising for
reconstruction. Moreover, we analyze and compare the performance degradation
due to erased packets sent to the sink of the two approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3082</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3082</id><created>2014-04-11</created><updated>2016-02-17</updated><authors><author><keyname>Lauri</keyname><forenames>Juho</forenames></author></authors><title>Further Hardness Results on Rainbow and Strong Rainbow Connectivity</title><categories>cs.CC cs.DM</categories><comments>13 pages, 4 figures</comments><msc-class>68Q25</msc-class><journal-ref>Discrete Applied Mathematics 201: 191-200 (2016)</journal-ref><doi>10.1016/j.dam.2015.07.041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A path in an edge-colored graph is \textit{rainbow} if no two edges of it are
colored the same. The graph is said to be \textit{rainbow connected} if there
is a rainbow path between every pair of vertices. If there is a rainbow
shortest path between every pair of vertices, the graph is \textit{strong
rainbow connected}. We consider the complexity of the problem of deciding if a
given edge-colored graph is rainbow or strong rainbow connected. These problems
are called \textsc{Rainbow connectivity} and \textsc{Strong rainbow
connectivity}, respectively. We prove both problems remain $\NP$\hyp{}complete
on interval outerplanar graphs and $k$-regular graphs for $k \geq 3$.
Previously, no graph class was known where the complexity of the two problems
would differ. We show that for block graphs, which form a subclass of chordal
graphs, \textsc{Rainbow connectivity} is $\NP$\hyp{}complete while
\textsc{Strong rainbow connectivity} is in $\P$. We conclude by considering
some tractable special cases, and show for instance that both problems are in
$\XP$ when parameterized by tree-depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3084</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3084</id><created>2014-04-11</created><authors><author><keyname>Havemann</keyname><forenames>Frank</forenames></author><author><keyname>Larsen</keyname><forenames>Birger</forenames></author></authors><title>Bibliometric Indicators of Young Authors in Astrophysics: Can Later
  Stars be Predicted?</title><categories>cs.DL astro-ph.IM physics.soc-ph</categories><comments>14 pages, 10 figures</comments><journal-ref>Scientometrics, 30. November 2014, 1-25 p</journal-ref><doi>10.1007/s11192-014-1476-3</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We test 16 bibliometric indicators with respect to their validity at the
level of the individual researcher by estimating their power to predict later
successful researchers. We compare the indicators of a sample of astrophysics
researchers who later co-authored highly cited papers before their first
landmark paper with the distributions of these indicators over a random control
group of young authors in astronomy and astrophysics. We find that field and
citation-window normalisation substantially improves the predicting power of
citation indicators. The two indicators of total influence based on citation
numbers normalised with expected citation numbers are the only indicators which
show differences between later stars and random authors significant on a 1%
level. Indicators of paper output are not very useful to predict later stars.
The famous $h$-index makes no difference at all between later stars and the
random control group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3099</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3099</id><created>2014-04-11</created><authors><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author></authors><title>Distance Distributions for Real Cellular Networks</title><categories>cs.NI</categories><comments>2 pages, 1 figure, IEEE Conference on Computer Communications
  (INFOCOM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the general distribution for the distance between a
mobile user and any base station (BS). We show that a random variable
proportional to the distance squared is Gamma distributed. In the case of the
nearest BS, it can be reduced to the well established result of the distance
being Rayleigh distributed. We validate our results using a random node
simulation and real Vodafone 3G network data, and go on to show how the
distribution is tractable by deriving the average aggregate interference power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3104</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3104</id><created>2014-04-11</created><authors><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author></authors><title>Transmit Pulse Shaping for Molecular Communications</title><categories>cs.ET</categories><comments>2 pages, 1 figure, IEEE Conference on Computer Communications
  (INFOCOM)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a method for shaping the transmit pulse of a molecular
signal such that the diffusion channel's response is a sharp pulse. The impulse
response of a diffusion channel is typically characterised as having an
infinitely long transient response. This can cause severe
inter-symbol-interference, and reduce the achievable reliable bit rate. We
achieve the desired chemical channel response by poisoning the channel with a
secondary compound, such that it chemically cancels aspects of the primary
information signal. We use two independent methods to show that the chemical
concentration of the \emph{information signal} should be $\propto \delta(t)$
and that of the \emph{poison signal} should be $\propto t^{-3/2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3109</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3109</id><created>2014-04-11</created><updated>2014-10-22</updated><authors><author><keyname>Karrasch</keyname><forenames>Daniel</forenames></author><author><keyname>Huhn</keyname><forenames>Florian</forenames></author><author><keyname>Haller</keyname><forenames>George</forenames></author></authors><title>Automated detection of coherent Lagrangian vortices in two-dimensional
  unsteady flows</title><categories>math.DS cs.GR physics.ao-ph physics.flu-dyn</categories><comments>17 pages, 6 figures, 1 table, accepted for publication in Proc. R.
  Soc. Lond., Ser. A</comments><journal-ref>Proceedings of the Royal Society A, 471, 20140639, 2015</journal-ref><doi>10.1098/rspa.2014.0639</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coherent boundaries of Lagrangian vortices in fluid flows have recently been
identified as closed orbits of line fields associated with the Cauchy-Green
strain tensor. Here we develop a fully automated procedure for the detection of
such closed orbits in large-scale velocity data sets. We illustrate the power
of our method on ocean surface velocities derived from satellite altimetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3111</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3111</id><created>2014-04-11</created><authors><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Khirallah</keyname><forenames>Chadi</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author><author><keyname>Thompson</keyname><forenames>John</forenames></author></authors><title>Interference Allocation Scheduler for Green Multimedia Delivery</title><categories>cs.NI</categories><comments>29 pages, 9 figures</comments><journal-ref>IEEE Transactions on Vehicular Technology, volume 63, issue 5, pp
  1--12, 2014</journal-ref><doi>10.1109/TVT.2014.2312373</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in wireless networking is how to allocate the
available radio resources in order to maximise key service delivery parameters
such as the aggregate throughput and the multimedia quality of experience
(QoE). We propose a novel and effective scheduling policy that allocates
resource blocks, such that interference power is shifted towards
capacity-saturated users, while improving the throughput of unsaturated users.
The highlight of the research is that the proposed scheme can dramatically
improve the performance of cells that have a high discrepancy in its
signal-to-noise-ratio (SNR) distribution, which is typical in urban areas. The
results show that a \emph{free-lunch} solution is possible, whereby for a
negligible performance degradation in the saturated users, a large improvement
in the non-saturated users can be obtained. However, on average, the number of
free-lunch user pairings are low. By relaxing the degradation constraints, the
\emph{non-free-lunch} solution can yield a greater multi-user throughput gain.
Motivated by surge in mobile multimedia traffic, we further demonstrate that
the proposed scheduling may have a profound impact on both energy efficiency
and QoE of multimedia service delivery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3114</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3114</id><created>2014-04-11</created><updated>2014-05-27</updated><authors><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Makse</keyname><forenames>Hern&#xe1;n A.</forenames></author></authors><title>Conditions for viral influence spreading through multiplex correlated
  social networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>45 pages, 11 figures</comments><journal-ref>Phys. Rev. X 4, 021031 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in network science is to predict how certain
individuals are able to initiate new networks to spring up &quot;new ideas&quot;.
Frequently, these changes in trends are triggered by a few innovators who
rapidly impose their ideas through &quot;viral&quot; influence spreading producing
cascades of followers fragmenting an old network to create a new one. Typical
examples include the raise of scientific ideas or abrupt changes in social
media, like the raise of Facebook.com to the detriment of Myspace.com. How this
process arises in practice has not been conclusively demonstrated. Here, we
show that a condition for sustaining a viral spreading process is the existence
of a multiplex correlated graph with hidden &quot;influence links&quot;. Analytical
solutions predict percolation phase transitions, either abrupt or continuous,
where networks are disintegrated through viral cascades of followers as in
empirical data. Our modeling predicts the strict conditions to sustain a large
viral spreading via a scaling form of the local correlation function between
multilayers, which we also confirm empirically. Ultimately, the theory predicts
the conditions for viral cascading in a large class of multiplex networks
ranging from social to financial systems and markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3131</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3131</id><created>2014-04-11</created><updated>2014-07-22</updated><authors><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author></authors><title>The Possibility Problem for Probabilistic XML (Extended Version)</title><categories>cs.DB cs.CC cs.LO</categories><comments>20 pages, 1 table, 2 figures. This is the complete version (including
  proofs) of work initially submitted as an extended abstract (without proofs)
  at the AMW 2014 workshop and subsequently submitted (with proofs) at the BDA
  2014 conference (no formal proceedings). This version integrates the feedback
  from both rounds of reviews</comments><acm-class>H.2.3; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the possibility problem of determining if a document is a
possible world of a probabilistic document, in the setting of probabilistic
XML. This basic question is a special case of query answering or tree automata
evaluation, but it has specific practical uses, such as checking whether an
user-provided probabilistic document outcome is possible or sufficiently
likely. In this paper, we study the complexity of the possibility problem for
probabilistic XML models of varying expressiveness. We show that the decision
problem is often tractable in the absence of long-distance dependencies, but
that its computation variant is intractable on unordered documents. We also
introduce an explicit matches variant to generalize practical situations where
node labels are unambiguous; this ensures tractability of the possibility
problem, even under long-distance dependencies, provided event conjunctions are
disallowed. Our results entirely classify the tractability boundary over all
considered problem variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3141</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3141</id><created>2014-04-11</created><authors><author><keyname>Kaminski</keyname><forenames>Mark</forenames></author><author><keyname>Nenov</keyname><forenames>Yavor</forenames></author><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author></authors><title>Datalog Rewritability of Disjunctive Datalog Programs and its
  Applications to Ontology Reasoning</title><categories>cs.AI cs.LO</categories><comments>14 pages. To appear at AAAI-14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of rewriting a disjunctive datalog program into plain
datalog. We show that a disjunctive program is rewritable if and only if it is
equivalent to a linear disjunctive program, thus providing a novel
characterisation of datalog rewritability. Motivated by this result, we propose
weakly linear disjunctive datalog---a novel rule-based KR language that extends
both datalog and linear disjunctive datalog and for which reasoning is
tractable in data complexity. We then explore applications of weakly linear
programs to ontology reasoning and propose a tractable extension of OWL 2 RL
with disjunctive axioms. Our empirical results suggest that many non-Horn
ontologies can be reduced to weakly linear programs and that query answering
over such ontologies using a datalog engine is feasible in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3145</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3145</id><created>2014-04-11</created><authors><author><keyname>Leng</keyname><forenames>Mei</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Shin</keyname><forenames>Hyundong</forenames></author></authors><title>Distributed Local Linear Parameter Estimation using Gaussian SPAWN</title><categories>cs.MA cs.SY</categories><doi>10.1109/TSP.2014.2373311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating local sensor parameters, where the
local parameters and sensor observations are related through linear stochastic
models. Sensors exchange messages and cooperate with each other to estimate
their own local parameters iteratively. We study the Gaussian Sum-Product
Algorithm over a Wireless Network (gSPAWN) procedure, which is based on belief
propagation, but uses fixed size broadcast messages at each sensor instead.
Compared with the popular diffusion strategies for performing network parameter
estimation, whose communication cost at each sensor increases with increasing
network density, the gSPAWN algorithm allows sensors to broadcast a message
whose size does not depend on the network size or density, making it more
suitable for applications in wireless sensor networks. We show that the gSPAWN
algorithm converges in mean and has mean-square stability under some technical
sufficient conditions, and we describe an application of the gSPAWN algorithm
to a network localization problem in non-line-of-sight environments. Numerical
results suggest that gSPAWN converges much faster in general than the diffusion
method, and has lower communication costs, with comparable root mean square
errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3146</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3146</id><created>2014-04-11</created><authors><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author><author><keyname>Bertschinger</keyname><forenames>Nils</forenames></author><author><keyname>Olbrich</keyname><forenames>Eckehard</forenames></author><author><keyname>Jost</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Reconsidering unique information: Towards a multivariate information
  decomposition</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, submitted to ISIT 2014</comments><msc-class>94A15, 94A17</msc-class><doi>10.1109/ISIT.2014.6875230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information that two random variables $Y$, $Z$ contain about a third
random variable $X$ can have aspects of shared information (contained in both
$Y$ and $Z$), of complementary information (only available from $(Y,Z)$
together) and of unique information (contained exclusively in either $Y$ or
$Z$). Here, we study measures $\widetilde{SI}$ of shared, $\widetilde{UI}$
unique and $\widetilde{CI}$ complementary information introduced by
Bertschinger et al., which are motivated from a decision theoretic perspective.
We find that in most cases the intuitive rule that more variables contain more
information applies, with the exception that $\widetilde{SI}$ and
$\widetilde{CI}$ information are not monotone in the target variable $X$.
Additionally, we show that it is not possible to extend the bivariate
information decomposition into $\widetilde{SI}$, $\widetilde{UI}$ and
$\widetilde{CI}$ to a non-negative decomposition on the partial information
lattice of Williams and Beer. Nevertheless, the quantities $\widetilde{UI}$,
$\widetilde{SI}$ and $\widetilde{CI}$ have a well-defined interpretation, even
in the multivariate setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3152</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3152</id><created>2014-04-11</created><authors><author><keyname>Atia</keyname><forenames>George</forenames></author></authors><title>Change Detection with Compressive Measurements</title><categories>cs.IT math.IT math.ST stat.TH</categories><doi>10.1109/LSP.2014.2352116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quickest change point detection is concerned with the detection of
statistical change(s) in sequences while minimizing the detection delay subject
to false alarm constraints. In this paper, the problem of change point
detection is studied when the decision maker only has access to compressive
measurements. First, an expression for the average detection delay of
Shiryaev's procedure with compressive measurements is derived in the asymptotic
regime where the probability of false alarm goes to zero. Second, the
dependence of the delay on the compression ratio and the signal to noise ratio
is explicitly quantified. The ratio of delays with and without compression is
studied under various sensing matrix constructions, including Gaussian
ensembles and random projections. For a target ratio of the delays after and
before compression, a sufficient condition on the number of measurements
required to meet this objective with prespecified probability is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3162</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3162</id><created>2014-04-11</created><authors><author><keyname>Kr&#xf6;ll</keyname><forenames>Harald</forenames></author><author><keyname>Zwicky</keyname><forenames>Stefan</forenames></author><author><keyname>Odermatt</keyname><forenames>Reto</forenames></author><author><keyname>Bruderer</keyname><forenames>Lukas</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author><author><keyname>Huang</keyname><forenames>Qiuting</forenames></author></authors><title>A Signal Processor for Gaussian Message Passing</title><categories>cs.AR</categories><comments>accepted to the IEEE IEEE International Symposium on Circuits and
  Systems (ISCAS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel signal processing unit built upon the
theory of factor graphs, which is able to address a wide range of signal
processing algorithms. More specifically, the demonstrated factor graph
processor (FGP) is tailored to Gaussian message passing algorithms. We show how
to use a highly configurable systolic array to solve the message update
equations of nodes in a factor graph efficiently. A proper instruction set and
compilation procedure is presented. In a recursive least squares channel
estimation example we show that the FGP can compute a message update faster
than a state-ofthe- art DSP. The results demonstrate the usabilty of the FGP
architecture as a flexible HW accelerator for signal-processing and
communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3165</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3165</id><created>2014-04-11</created><updated>2014-04-28</updated><authors><author><keyname>Ozcan</keyname><forenames>Gozde</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author></authors><title>Energy-Efficient Power Adaptation for Cognitive Radio Systems under
  Imperfect Channel Sensing</title><categories>cs.IT math.IT</categories><comments>To Appear at 2014 IEEE INFOCOM Workshop on Green Cognitive
  Communications and Computing Networks. Some typos are fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, energy efficient power adaptation is considered in
sensing-based spectrum sharing cognitive radio systems in which secondary users
first perform channel sensing and then initiate data transmission with two
power levels based on the sensing decisions (e.g., idle or busy). It is assumed
that spectrum sensing is performed by the cognitive secondary users, albeit
with possible errors. In this setting, the optimization problem of maximizing
the energy efficiency (EE) subject to peak/average transmission power
constraints and average interference constraints is considered. The circuit
power is taken into account for total power consumption. By exploiting the
quasiconcave property of the EE maximization problem, the original problem is
transformed into an equivalent parameterized concave problem and Dinkelbach's
method-based iterative power adaptation algorithm is proposed. The impact of
sensing performance, peak/average transmit power constraints and average
interference constraint on the energy efficiency of cognitive radio systems is
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3166</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3166</id><created>2014-04-11</created><updated>2015-06-15</updated><authors><author><keyname>Brijder</keyname><forenames>Robert</forenames></author></authors><title>Minimal Output Unstable Configurations in Chemical Reaction Networks and
  Deciders</title><categories>cs.CC cs.DC</categories><comments>14 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the set of output stable configurations of chemical reaction
deciders (CRDs). It turns out that CRDs with only bimolecular reactions (which
are almost equivalent to population protocols) have a special structure that
allows for an algorithm to efficiently compute their finite set of minimal
output unstable configurations. As a consequence, a relatively large set of
configurations may be efficiently checked for output stability.
  We also provide a number of observations regarding the semilinearity result
of Angluin et al. [Distrib. Comput., 2007] from the context of population
protocols (which is a central result for output stable CRDs). In particular, we
observe that the computation-friendly class of totally stable CRDs has equal
expressive power as the larger class of output stable CRDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3181</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3181</id><created>2014-04-11</created><updated>2014-08-21</updated><authors><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>FAST-PPR: Scaling Personalized PageRank Estimation for Large Graphs</title><categories>cs.DS cs.SI</categories><comments>KDD 2014</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm, FAST-PPR, for estimating personalized PageRank:
given start node $s$ and target node $t$ in a directed graph, and given a
threshold $\delta$, FAST-PPR estimates the Personalized PageRank $\pi_s(t)$
from $s$ to $t$, guaranteeing a small relative error as long $\pi_s(t)&gt;\delta$.
Existing algorithms for this problem have a running-time of $\Omega(1/\delta)$;
in comparison, FAST-PPR has a provable average running-time guarantee of
${O}(\sqrt{d/\delta})$ (where $d$ is the average in-degree of the graph). This
is a significant improvement, since $\delta$ is often $O(1/n)$ (where $n$ is
the number of nodes) for applications. We also complement the algorithm with an
$\Omega(1/\sqrt{\delta})$ lower bound for PageRank estimation, showing that the
dependence on $\delta$ cannot be improved.
  We perform a detailed empirical study on numerous massive graphs, showing
that FAST-PPR dramatically outperforms existing algorithms. For example, on the
2010 Twitter graph with 1.5 billion edges, for target nodes sampled by
popularity, FAST-PPR has a $20$ factor speedup over the state of the art.
Furthermore, an enhanced version of FAST-PPR has a $160$ factor speedup on the
Twitter graph, and is at least $20$ times faster on all our candidate graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3184</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3184</id><created>2014-04-11</created><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Decreasing Weighted Sorted $\ell_1$ Regularization</title><categories>cs.CV cs.IT cs.LG math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a new family of regularizers, termed {\it weighted sorted
$\ell_1$ norms} (WSL1), which generalizes the recently introduced {\it
octagonal shrinkage and clustering algorithm for regression} (OSCAR) and also
contains the $\ell_1$ and $\ell_{\infty}$ norms as particular instances. We
focus on a special case of the WSL1, the {\sl decreasing WSL1} (DWSL1), where
the elements of the argument vector are sorted in non-increasing order and the
weights are also non-increasing. In this paper, after showing that the DWSL1 is
indeed a norm, we derive two key tools for its use as a regularizer: the dual
norm and the Moreau proximity operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3186</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3186</id><created>2014-04-11</created><authors><author><keyname>Demarco</keyname><forenames>Favio</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Xuan</keyname><forenames>Jifeng</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Berre</keyname><forenames>Daniel Le</forenames><affiliation>CRIL</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Automatic Repair of Buggy If Conditions and Missing Preconditions with
  SMT</title><categories>cs.SE</categories><comments>CSTVA'2014, India (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Nopol, an approach for automatically repairing buggy if conditions
and missing preconditions. As input, it takes a program and a test suite which
contains passing test cases modeling the expected behavior of the program and
at least one failing test case embodying the bug to be repaired. It consists of
collecting data from multiple instrumented test suite executions, transforming
this data into a Satisfiability Modulo Theory (SMT) problem, and translating
the SMT result -- if there exists one -- into a source code patch. Nopol
repairs object oriented code and allows the patches to contain nullness checks
as well as specific method calls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3190</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3190</id><created>2014-04-11</created><authors><author><keyname>Li</keyname><forenames>Cong</forenames></author><author><keyname>Georgiopoulos</keyname><forenames>Michael</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Georgios C.</forenames></author></authors><title>Pareto-Path Multi-Task Multiple Kernel Learning</title><categories>cs.LG</categories><comments>Accepted by IEEE Transactions on Neural Networks and Learning Systems</comments><doi>10.1109/TNNLS.2014.2309939</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A traditional and intuitively appealing Multi-Task Multiple Kernel Learning
(MT-MKL) method is to optimize the sum (thus, the average) of objective
functions with (partially) shared kernel function, which allows information
sharing amongst tasks. We point out that the obtained solution corresponds to a
single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO)
problem, which considers the concurrent optimization of all task objectives
involved in the Multi-Task Learning (MTL) problem. Motivated by this last
observation and arguing that the former approach is heuristic, we propose a
novel Support Vector Machine (SVM) MT-MKL framework, that considers an
implicitly-defined set of conic combinations of task objectives. We show that
solving our framework produces solutions along a path on the aforementioned PF
and that it subsumes the optimization of the average of objective functions as
a special case. Using algorithms we derived, we demonstrate through a series of
experimental results that the framework is capable of achieving better
classification performance, when compared to other similar MTL approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3200</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3200</id><created>2014-04-11</created><updated>2015-03-26</updated><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author></authors><title>Decentralized Computation Offloading Game For Mobile Cloud Computing</title><categories>cs.NI</categories><comments>The paper has been accepted by IEEE Transactions on Parallel and
  Distributed Systems (TPDS) Vol. 26, No. 4, pp. 974 - 983, March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile cloud computing is envisioned as a promising approach to augment
computation capabilities of mobile devices for emerging resource-hungry mobile
applications. In this paper, we propose a game theoretic approach for achieving
efficient computation offloading for mobile cloud computing. We formulate the
decentralized computation offloading decision making problem among mobile
device users as a decentralized computation offloading game. We analyze the
structural property of the game and show that the game always admits a Nash
equilibrium. We then design a decentralized computation offloading mechanism
that can achieve a Nash equilibrium of the game and quantify its efficiency
ratio over the centralized optimal solution. Numerical results demonstrate that
the proposed mechanism can achieve efficient computation offloading performance
and scale well as the system size increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3203</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3203</id><created>2014-04-11</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Compressive classification and the rare eclipse problem</title><categories>cs.LG cs.IT math.IT math.ST stat.TH</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper addresses the fundamental question of when convex sets remain
disjoint after random projection. We provide an analysis using ideas from
high-dimensional convex geometry. For ellipsoids, we provide a bound in terms
of the distance between these ellipsoids and simple functions of their
polynomial coefficients. As an application, this theorem provides bounds for
compressive classification of convex sets. Rather than assuming that the data
to be classified is sparse, our results show that the data can be acquired via
very few measurements yet will remain linearly separable. We demonstrate the
feasibility of this approach in the context of hyperspectral imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3221</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3221</id><created>2014-04-11</created><authors><author><keyname>Cao</keyname><forenames>Yongcan</forenames></author></authors><title>UAV Circumnavigating an Unknown Target Under a GPS-denied Environment
  with Range-only Measurements</title><categories>cs.SY cs.RO math.OC</categories><comments>A preliminary version of this work will be presented at the 2014
  American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One typical application of unmanned aerial vehicles is the intelligence,
surveillance, and reconnaissance mission, where the objective is to improve
situation awareness through information acquisition. For examples, an efficient
way to gather information regarding a target is to deploy UAV in such a way
that it orbits around this target at a desired distance. Such a UAV motion is
called circumnavigation. The objective of the paper is to design a UAV control
algorithm such that this circumnavigation mission is achieved under a
GPS-denied environment using range-only measurement. The control algorithm is
constructed in two steps. The first step is to design a UAV control algorithm
by assuming the availability of both range and range rate measurements, where
the associated control input is always bounded. The second step is to further
eliminate the use of range rate measurement by using an estimated range rate,
obtained via a sliding-mode estimator using range measurement, to replace
actual range rate measurement. Such a controller design technique is applicable
in the control design of other UAV navigation and control missions under a
GPS-denied environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3233</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3233</id><created>2014-04-11</created><authors><author><keyname>Hailpern</keyname><forenames>Joshua</forenames></author><author><keyname>Venkata</keyname><forenames>Niranjan Damera</forenames></author><author><keyname>Danilevsky</keyname><forenames>Marina</forenames></author></authors><title>Pagination: It's what you say, not how long it takes to say it</title><categories>cs.CL cs.IR</categories><comments>10 pages, Submitted to DOCENG'14</comments><acm-class>I.7.2; I.7.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pagination - the process of determining where to break an article across
pages in a multi-article layout is a common layout challenge for most
commercially printed newspapers and magazines. To date, no one has created an
algorithm that determines a minimal pagination break point based on the content
of the article. Existing approaches for automatic multi-article layout focus
exclusively on maximizing content (number of articles) and optimizing aesthetic
presentation (e.g., spacing between articles). However, disregarding the
semantic information within the article can lead to overly aggressive cutting,
thereby eliminating key content and potentially confusing the reader, or
setting too generous of a break point, thereby leaving in superfluous content
and making automatic layout more difficult. This is one of the remaining
challenges on the path from manual layouts to fully automated processes that
still ensure article content quality. In this work, we present a new approach
to calculating a document minimal break point for the task of pagination. Our
approach uses a statistical language model to predict minimal break points
based on the semantic content of an article. We then compare 4 novel candidate
approaches, and 4 baselines (currently in use by layout algorithms). Results
from this experiment show that one of our approaches strongly outperforms the
baselines and alternatives. Results from a second study suggest that humans are
not able to agree on a single &quot;best&quot; break point. Therefore, this work shows
that a semantic-based lower bound break point prediction is necessary for ideal
automated document synthesis within a real-world context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3238</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3238</id><created>2014-04-11</created><updated>2014-10-15</updated><authors><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Bounds on Distance Estimation via Diffusive Molecular Communication</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures, 1 table. Will be presented at the 2014 IEEE
  Global Communications Conference (GLOBECOM) in Austin, TX, USA, on December
  9, 2014</comments><doi>10.1109/GLOCOM.2014.7037234</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies distance estimation for diffusive molecular communication.
The Cramer-Rao lower bound on the variance of the distance estimation error is
derived. The lower bound is derived for a physically unbounded environment with
molecule degradation and steady uniform flow. The maximum likelihood distance
estimator is derived and its accuracy is shown via simulation to perform very
close to the Cramer-Rao lower bound. An existing protocol is shown to be
equivalent to the maximum likelihood distance estimator if only one observation
is made. Simulation results also show the accuracy of existing protocols with
respect to the Cramer-Rao lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3248</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3248</id><created>2014-04-11</created><updated>2015-01-21</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author></authors><title>Optimization Problems with Diseconomies of Scale via Decoupling</title><categories>cs.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new framework for solving optimization problems with a
diseconomy of scale. In such problems, our goal is to minimize the cost of
resources used to perform a certain task. The cost of resources grows
superlinearly, as $x^q$, $q\ge 1$, with the amount $x$ of resources used. We
define a novel linear programming relaxation for such problems, and then show
that the integrality gap of the relaxation is $A_q$, where $A_q$ is the $q$-th
moment of the Poisson random variable with parameter 1. Using our framework, we
obtain approximation algorithms for the Minimum Energy Efficient Routing,
Minimum Degree Balanced Spanning Tree, Load Balancing on Unrelated Parallel
Machines, and Unrelated Parallel Machine Scheduling with Nonlinear Functions of
Completion Times problems.
  Our analysis relies on the decoupling inequality for nonnegative random
variables. The inequality states that $$\big \|\sum_{i=1}^n X_i\big\|_{q} \leq
C_q \,\big \|\sum_{i=1}^n Y_i\big\|_{q},$$ where $X_i$ are independent
nonnegative random variables, $Y_i$ are possibly dependent nonnegative random
variable, and each $Y_i$ has the same distribution as $X_i$. The inequality was
proved by de la Pe\~na in 1990. De la Pe\~na, Ibragimov, and Sharakhmetov
(2003) showed that $C_q\leq 2$ for $q\in (1,2)$ and $C_q\leq A_q^{1/q}$ for
$q\geq 2$. We show that the optimal constant is $C_q=A_q^{1/q}$ for any $q\geq
1$. We then prove a more general inequality: For every convex function
$\varphi$, $$\mathbb{E}[\varphi\Big(\sum_{i=1}^n X_i\Big)]\leq
\mathbb{E}[\varphi\Big(P\sum_{i=1}^n Y_i\Big)],$$ and, for every concave
function $\psi$, $$\mathbb{E}[\psi\Big(\sum_{i=1}^n X_i\Big)] \geq
\mathbb{E}[\psi\Big(P\sum_{i=1}^n Y_i\Big)],$$ where $P$ is a Poisson random
variable with parameter 1 independent of the random variables $Y_i$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3250</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3250</id><created>2014-04-11</created><authors><author><keyname>Salmond</keyname><forenames>Daniel</forenames></author><author><keyname>Grant</keyname><forenames>Alex</forenames></author><author><keyname>Grivell</keyname><forenames>Ian</forenames></author><author><keyname>Chan</keyname><forenames>Terence</forenames></author></authors><title>On the rank of random matrices over finite fields</title><categories>cs.IT math.IT</categories><comments>8 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel lower bound is introduced for the full rank probability of random
finite field matrices, where a number of elements with known location are
identically zero, and remaining elements are chosen independently of each
other, uniformly over the field. The main ingredient is a result showing that
constraining additional elements to be zero cannot result in a higher
probability of full rank. The bound then follows by &quot;zeroing&quot; elements to
produce a block-diagonal matrix, whose full rank probability can be computed
exactly. The bound is shown to be at least as tight and can be strictly tighter
than existing bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3263</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3263</id><created>2014-04-12</created><updated>2014-07-22</updated><authors><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Varaiya</keyname><forenames>Pravin P.</forenames></author></authors><title>Compressive Origin-Destination Matrix Estimation</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an approach to estimate Origin-Destination (OD) flows and
their path splits, based on traffic counts on links in the network. The
approach called Compressive Origin-Destination Estimation (CODE) is inspired by
Compressive Sensing (CS) techniques. Even though the estimation problem is
underdetermined, CODE recovers the unknown variables exactly when the number of
alternative paths for each OD pair is small. Noiseless, noisy, and weighted
versions of CODE are illustrated for synthetic networks, and with real data for
a small region in East Providence. CODE's versatility is suggested by its use
to estimate the number of vehicles and the Vehicle-Miles Traveled (VMT) using
link counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3265</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3265</id><created>2014-04-12</created><authors><author><keyname>Tan</keyname><forenames>Haisheng</forenames></author><author><keyname>Yu</keyname><forenames>Jiajun</forenames></author><author><keyname>Liang</keyname><forenames>Hongyu</forenames></author><author><keyname>Lou</keyname><forenames>Tiancheng</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>Optimal Rendezvous Strategies for Different Environments in Cognitive
  Radio Networks</title><categories>cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cognitive Radio Networks (CRNs), the secondary users (SUs) are allowed to
access the licensed channels opportunistically. A fundamental and essential
operation for SUs is to establish communication through choosing a common
channel at the same time slot, which is referred to as rendezvous problem. In
this paper, we study strategies to achieve fast rendezvous for two secondary
users.
  The channel availability for secondary nodes is subject to temporal and
spatial variation. Moreover, in a distributed system, one user is oblivious of
the other user's channel status. Therefore, a fast rendezvous is not trivial.
Recently, a number of rendezvous strategies have been proposed for different
system settings, but rarely have they taken the temporal variation of the
channels into account. In this work, we first derive a time-adaptive strategy
with optimal expected time-to-rendezvous (TTR) for synchronous systems in
stable environments, where channel availability is assumed to be static over
time. Next, in dynamic environments, which better represent temporally dynamic
channel availability in CRNs, we first derive optimal strategies for two
special cases, and then prove that our strategy is still asymptotically optimal
in general dynamic cases.
  Numerous simulations are conducted to demonstrate the performance of our
strategies, and validate the theoretical analysis. The impacts of different
parameters on the TTR are also investigated, such as the number of channels,
the channel open possibilities, the extent of the environment being dynamic,
and the existence of an intruder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3272</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3272</id><created>2014-04-12</created><updated>2014-05-09</updated><authors><author><keyname>Chatterjee</keyname><forenames>Bapi</forenames></author><author><keyname>Nguyen</keyname><forenames>Nhan</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>Efficient Lock-free Binary Search Trees</title><categories>cs.DC</categories><comments>15 pages, 3 figures, submitted to PODC</comments><report-no>TR - 2014:05, ISSN 1652-926X</report-no><acm-class>E.1; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel algorithm for concurrent lock-free internal
binary search trees (BST) and implement a Set abstract data type (ADT) based on
that. We show that in the presented lock-free BST algorithm the amortized step
complexity of each set operation - {\sc Add}, {\sc Remove} and {\sc Contains} -
is $O(H(n) + c)$, where, $H(n)$ is the height of BST with $n$ number of nodes
and $c$ is the contention during the execution. Our algorithm adapts to
contention measures according to read-write load. If the situation is
read-heavy, the operations avoid helping pending concurrent {\sc Remove}
operations during traversal, and, adapt to interval contention. However, for
write-heavy situations we let an operation help pending {\sc Remove}, even
though it is not obstructed, and so adapt to tighter point contention. It uses
single-word compare-and-swap (\texttt{CAS}) operations. We show that our
algorithm has improved disjoint-access-parallelism compared to similar existing
algorithms. We prove that the presented algorithm is linearizable. To the best
of our knowledge this is the first algorithm for any concurrent tree data
structure in which the modify operations are performed with an additive term of
contention measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3280</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3280</id><created>2014-04-12</created><authors><author><keyname>Guermah</keyname><forenames>Hatim</forenames></author><author><keyname>Fissaa</keyname><forenames>Tarik</forenames></author><author><keyname>Hafiddi</keyname><forenames>Hatim</forenames></author><author><keyname>Nassar</keyname><forenames>Mahmoud</forenames></author><author><keyname>Kriouile</keyname><forenames>Abdelaziz</forenames></author></authors><title>An Ontology Oriented Architecture for Context Aware Services Adaptation</title><categories>cs.SE</categories><comments>10 pages, 5 figures, IJCSI (International Journal of Computer Science
  Issues)</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 2, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of ubiquitous computing, a class of applications called
context-aware services attracted great interest especially since the emergence
of wireless technologies and mobile devices. Context-aware application can
dynamically capture a range of information from its environment and this
information represents a context, the application adapts its execution
according to this context. An important challenge in ubiquitous computing is
dealing with context. Ontologies presents the most promising instrument for
context modeling and managing due to their high and formal expressiveness and
the possibilities for applying ontology reasoning techniques. In this paper, we
present an ontology based approach for the development of context aware
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3285</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3285</id><created>2014-04-12</created><authors><author><keyname>Moeini</keyname><forenames>Mahdi</forenames></author><author><keyname>Jemai</keyname><forenames>Zied</forenames></author><author><keyname>Sahin</keyname><forenames>Evren</forenames></author></authors><title>An Integer Programming Model for the Dynamic Location and Relocation of
  Emergency Vehicles: A Case Study</title><categories>cs.AI</categories><comments>Proceedings of the 12th International Symposium on Operational
  Research (SOR'2013), Slovenia, September 2013, pp. 343-350, (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the dynamic Emergency Medical Service (EMS)
systems. A dynamic location model is presented that tries to locate and
relocate the ambulances. The proposed model controls the movements and
locations of ambulances in order to provide a better coverage of the demand
points under different fluctuation patterns that may happen during a given
period of time. Some numerical experiments have been carried out by using some
real-world data sets that have been collected through the French EMS system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3286</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3286</id><created>2014-04-12</created><authors><author><keyname>Moeini</keyname><forenames>Mahdi</forenames></author></authors><title>A Continuous Optimization Approach for the Financial Portfolio Selection
  under Discrete Asset Choice Constraints</title><categories>cs.CE</categories><comments>Proceedings of the 12th International Symposium on Operational
  Research (SOR'2013), Slovenia, September 2013, pp. 89-95, (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a generalization of the Markowitz's Mean-Variance
model under linear transaction costs and cardinality constraints. The
cardinality constraints are used to limit the number of assets in the optimal
portfolio. The generalized model is formulated as a mixed integer quadratic
programming (MIP) problem. The purpose of this paper is to investigate a
continuous approach based on difference of convex functions (DC) programming
for solving the MIP model. The preliminary comparative results of the proposed
approach versus CPLEX are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3290</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3290</id><created>2014-04-12</created><authors><author><keyname>Dar</keyname><forenames>Yehuda</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author></authors><title>Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and
  Analysis</title><categories>cs.MM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block-based motion estimation (ME) and compensation (MC) techniques are
widely used in modern video processing algorithms and compression systems. The
great variety of video applications and devices results in numerous compression
specifications. Specifically, there is a diversity of frame-rates and
bit-rates. In this paper, we study the effect of frame-rate and compression
bit-rate on block-based ME and MC as commonly utilized in inter-frame coding
and frame-rate up conversion (FRUC). This joint examination yields a
comprehensive foundation for comparing MC procedures in coding and FRUC. First,
the video signal is modeled as a noisy translational motion of an image. Then,
we theoretically model the motion-compensated prediction of an available and
absent frames as in coding and FRUC applications, respectively. The theoretic
MC-prediction error is further analyzed and its autocorrelation function is
calculated for coding and FRUC applications. We show a linear relation between
the variance of the MC-prediction error and temporal-distance. While the
affecting distance in MC-coding is between the predicted and reference frames,
MC-FRUC is affected by the distance between the available frames used for the
interpolation. Moreover, the dependency in temporal-distance implies an inverse
effect of the frame-rate. FRUC performance analysis considers the prediction
error variance, since it equals to the mean-squared-error of the interpolation.
However, MC-coding analysis requires the entire autocorrelation function of the
error; hence, analytic simplicity is beneficial. Therefore, we propose two
constructions of a separable autocorrelation function for prediction error in
MC-coding. We conclude by comparing our estimations with experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3291</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3291</id><created>2014-04-12</created><authors><author><keyname>Wilber</keyname><forenames>Michael J.</forenames></author><author><keyname>Kwak</keyname><forenames>Iljung S.</forenames></author><author><keyname>Belongie</keyname><forenames>Serge J.</forenames></author></authors><title>Cost-Effective HITs for Relative Similarity Comparisons</title><categories>cs.CV cs.LG</categories><comments>7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity comparisons of the form &quot;Is object a more similar to b than to c?&quot;
are useful for computer vision and machine learning applications.
Unfortunately, an embedding of $n$ points is specified by $n^3$ triplets,
making collecting every triplet an expensive task. In noticing this difficulty,
other researchers have investigated more intelligent triplet sampling
techniques, but they do not study their effectiveness or their potential
drawbacks. Although it is important to reduce the number of collected triplets,
it is also important to understand how best to display a triplet collection
task to a user. In this work we explore an alternative display for collecting
triplets and analyze the monetary cost and speed of the display. We propose
best practices for creating cost effective human intelligence tasks for
collecting triplets. We show that rather than changing the sampling algorithm,
simple changes to the crowdsourcing UI can lead to much higher quality
embeddings. We also provide a dataset as well as the labels collected from
crowd workers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3301</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3301</id><created>2014-04-12</created><authors><author><keyname>Wang</keyname><forenames>William Yang</forenames></author><author><keyname>Mazaitis</keyname><forenames>Kathryn</forenames></author><author><keyname>Lao</keyname><forenames>Ni</forenames></author><author><keyname>Mitchell</keyname><forenames>Tom</forenames></author><author><keyname>Cohen</keyname><forenames>William W.</forenames></author></authors><title>Efficient Inference and Learning in a Large Knowledge Base: Reasoning
  with Extracted Information using a Locally Groundable First-Order
  Probabilistic Logic</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1305.2254</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important challenge for probabilistic logics is reasoning with very large
knowledge bases (KBs) of imperfect information, such as those produced by
modern web-scale information extraction systems. One scalability problem shared
by many probabilistic logics is that answering queries involves &quot;grounding&quot; the
query---i.e., mapping it to a propositional representation---and the size of a
&quot;grounding&quot; grows with database size. To address this bottleneck, we present a
first-order probabilistic language called ProPPR in which that approximate
&quot;local groundings&quot; can be constructed in time independent of database size.
Technically, ProPPR is an extension to stochastic logic programs (SLPs) that is
biased towards short derivations; it is also closely related to an earlier
relational learning algorithm called the path ranking algorithm (PRA). We show
that the problem of constructing proofs for this logic is related to
computation of personalized PageRank (PPR) on a linearized version of the proof
space, and using on this connection, we develop a proveably-correct approximate
grounding scheme, based on the PageRank-Nibble algorithm. Building on this, we
develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In
experiments, we show that learning for ProPPR is orders magnitude faster than
learning for Markov logic networks; that allowing mutual recursion (joint
learning) in KB inference leads to improvements in performance; and that ProPPR
can learn weights for a mutually recursive program with hundreds of clauses,
which define scores of interrelated predicates, over a KB containing one
million entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3311</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3311</id><created>2014-04-12</created><updated>2014-04-17</updated><authors><author><keyname>Kisielewicz</keyname><forenames>Andrzej</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Synchronizing Automata with Large Reset Lengths</title><categories>cs.FL</categories><comments>19 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study synchronizing automata with the shortest reset words of relatively
large length. First, we refine the Frankl-Pin result on the length of the
shortest words of rank $m$, and the B\'eal, Perrin, and Steinberg results on
the length of the shortest reset words in one-cluster automata. The obtained
results are applied to computation aimed in extending the class of small
automata for which the \v{C}ern\'y conjecture is verified and discovering new
automata with special properties regarding synchronization. In particular, a
new class of slowly synchronizing automata on a ternary alphabet is constructed
and a conjecture on $cn$-extendable sets in synchronizing automata is
disproved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3312</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3312</id><created>2014-04-12</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>Shrinkage Optimized Directed Information using Pictorial Structures for
  Action Recognition</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose a novel action recognition framework. The method
uses pictorial structures and shrinkage optimized directed information
assessment (SODA) coupled with Markov Random Fields called SODA+MRF to model
the directional temporal dependency and bidirectional spatial dependency. As a
variant of mutual information, directional information captures the directional
information flow and temporal structure of video sequences across frames.
Meanwhile, within each frame, Markov random fields are utilized to model the
spatial relations among different parts of a human body and the body parts of
different people. The proposed SODA+MRF model is robust to view point
transformations and detect complex interactions accurately. We compare the
proposed method against several baseline methods to highlight the effectiveness
of the SODA+MRF model. We demonstrate that our algorithm has superior action
recognition performance on the UCF action recognition dataset, the Olympic
sports dataset and the collective activity dataset over several
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3316</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3316</id><created>2014-04-12</created><authors><author><keyname>Cortinhas</keyname><forenames>Luiz</forenames></author><author><keyname>Monteiro</keyname><forenames>Patrick</forenames></author><author><keyname>Zahlan</keyname><forenames>Amir</forenames></author><author><keyname>Vianna</keyname><forenames>Gabriel</forenames></author><author><keyname>Moscoso</keyname><forenames>Marcio</forenames></author></authors><title>Embed System for Robotic Arm with 3 Degree of Freedom Controller using
  Computational Vision on Real-Time</title><categories>cs.RO cs.SY</categories><comments>8 pages,9 figures, published on AIFL 2014 conference (AIFL-2014
  Submission 20)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Paper deals with robotic arm embed controller system, with distributed
system based on protocol communication between one server supporting multiple
points and mobile applications trough sockets .The proposed system utilizes
hand with glove gesture in three-dimensional recognition using fuzzy
implementation to set x,y,z coordinates. This approach present all
implementation over: two raspberry PI arm based computer running client
program, x64 PC running server program, and one robot arm controlled by
ATmega328p based board.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3318</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3318</id><created>2014-04-12</created><authors><author><keyname>Bilardi</keyname><forenames>Gianfranco</forenames></author><author><keyname>Pietracaprina</keyname><forenames>Andrea</forenames></author><author><keyname>Pucci</keyname><forenames>Geppino</forenames></author><author><keyname>Scquizzato</keyname><forenames>Michele</forenames></author><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author></authors><title>Network-Oblivious Algorithms</title><categories>cs.DS cs.DC</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is proposed for the design and analysis of
\emph{network-oblivious algorithms}, namely, algorithms that can run unchanged,
yet efficiently, on a variety of machines characterized by different degrees of
parallelism and communication capabilities. The framework prescribes that a
network-oblivious algorithm be specified on a parallel model of computation
where the only parameter is the problem's input size, and then evaluated on a
model with two parameters, capturing parallelism granularity and communication
latency. It is shown that, for a wide class of network-oblivious algorithms,
optimality in the latter model implies optimality in the Decomposable BSP
model, which is known to effectively describe a wide and significant class of
parallel platforms. The proposed framework can be regarded as an attempt to
port the notion of obliviousness, well established in the context of cache
hierarchies, to the realm of parallel computation. Its effectiveness is
illustrated by providing optimal network-oblivious algorithms for a number of
key problems. Some limitations of the oblivious approach are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3320</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3320</id><created>2014-04-12</created><authors><author><keyname>Adler</keyname><forenames>Ilan</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>On Simplex Pivoting Rules and Complexity Theory</title><categories>cs.CC cs.DS</categories><comments>To appear in IPCO 2014</comments><acm-class>G.1.6; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there are simplex pivoting rules for which it is PSPACE-complete
to tell if a particular basis will appear on the algorithm's path. Such rules
cannot be the basis of a strongly polynomial algorithm, unless P = PSPACE. We
conjecture that the same can be shown for most known variants of the simplex
method. However, we also point out that Dantzig's shadow vertex algorithm has a
polynomial path problem. Finally, we discuss in the same context randomized
pivoting rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3321</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3321</id><created>2014-04-12</created><updated>2014-08-05</updated><authors><author><keyname>Schliephake</keyname><forenames>Michael</forenames></author><author><keyname>Laure</keyname><forenames>Erwin</forenames></author></authors><title>Performance Analysis of Irregular Collective Communication with the
  Crystal Router Algorithm</title><categories>cs.DC</categories><comments>Conference EASC2014: Solving Software Challenges for Exascale</comments><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to achieve exascale performance it is important to detect potential
bottlenecks and identify strategies to overcome them. For this, both
applications and system software must be analysed and potentially improved. The
EU FP7 project Collaborative Research into Exascale Systemware, Tools &amp;
Applications (CRESTA) chose the approach to co-design advanced simulation
applications and system software as well as development tools. In this paper,
we present the results of a co-design activity focused on the simulation code
NEK5000 that aims at performance improvements of collective communication
operations. We have analysed the algorithms that form the core of NEK5000's
communication module in order to assess its viability on recent computer
architectures before starting to improve its performance. Our results show that
the crystal router algorithm performs well in sparse, irregular collective
operations for medium and large processor number but improvements for even
larger system sizes of the future will be needed. We sketch the needed
improvements, which will make the communication algorithms also beneficial for
other applications that need to implement latency-dominated communication
schemes with short messages. The latency-optimised communication operations
will also become used in a runtime-system providing dynamic load balancing,
under development within CRESTA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3325</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3325</id><created>2014-04-12</created><updated>2014-10-31</updated><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>A Weighted Correlation Index for Rankings with Ties</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the correlation between two different scores for the same set
of items is a common problem in information retrieval, and the most commonly
used statistics that quantifies this correlation is Kendall's $\tau$. However,
the standard definition fails to capture that discordances between items with
high rank are more important than those between items with low rank. Recently,
a new measure of correlation based on average precision has been proposed to
solve this problem, but like many alternative proposals in the literature it
assumes that there are no ties in the scores. This is a major deficiency in a
number of contexts, and in particular while comparing centrality scores on
large graphs, as the obvious baseline, indegree, has a very large number of
ties in web and social graphs. We propose to extend Kendall's definition in a
natural way to take into account weights in the presence of ties. We prove a
number of interesting mathematical properties of our generalization and
describe an $O(n\log n)$ algorithm for its computation. We also validate the
usefulness of our weighted measure of correlation using experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3327</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3327</id><created>2014-04-12</created><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Supremum-Norm Convergence for Step-Asynchronous Successive
  Overrelaxation on M-matrices</title><categories>cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Step-asynchronous successive overrelaxation updates the values contained in a
single vector using the usual Gau\ss-Seidel-like weighted rule, but arbitrarily
mixing old and new values, the only constraint being temporal coherence: you
cannot use a value before it has been computed. We show that given a
nonnegative real matrix $A$, a $\sigma\geq\rho(A)$ and a vector $\boldsymbol
w&gt;0$ such that $A\boldsymbol w\leq\sigma\boldsymbol w$, every iteration of
step-asynchronous successive overrelaxation for the problem $(sI- A)\boldsymbol
x=\boldsymbol b$, with $s &gt;\sigma$, reduces geometrically the $\boldsymbol
w$-norm of the current error by a factor that we can compute explicitly. Then,
we show that given a $\sigma&gt;\rho(A)$ it is in principle always possible to
compute such a $\boldsymbol w$. This property makes it possible to estimate the
supremum norm of the absolute error at each iteration without any additional
hypothesis on $A$, even when $A$ is so large that computing the product
$A\boldsymbol x$ is feasible, but estimating the supremum norm of $(sI-A)^{-1}$
is not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3329</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3329</id><created>2014-04-12</created><authors><author><keyname>Thi</keyname><forenames>Hoai An Le</forenames></author><author><keyname>Moeini</keyname><forenames>Mahdi</forenames></author></authors><title>Portfolio Selection Under Buy-In Threshold Constraints Using DC
  Programming and DCA</title><categories>cs.CE</categories><comments>Proceedings of third International Conference on Service Systems and
  Service Management (SSSM'06/IEEE), Troyes, Oct. 2006, pp. 296-300 (2006).
  arXiv admin note: text overlap with arXiv:cs/0501005 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In matter of Portfolio selection, we consider a generalization of the
Markowitz Mean-Variance model which includes buy-in threshold constraints.
These constraints limit the amount of capital to be invested in each asset and
prevent very small investments in any asset. The new model can be converted
into a NP-hard mixed integer quadratic programming problem. The purpose of this
paper is to investigate a continuous approach based on DC programming and DCA
for solving this new model. DCA is a local continuous approach to solve a wide
variety of nonconvex programs for which it provided quite often a global
solution and proved to be more robust and efficient than standard methods.
Preliminary comparative results of DCA and a classical Branch-and-Bound
algorithm will be presented. These results show that DCA is an efficient and
promising approach for the considered portfolio selection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3330</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3330</id><created>2014-04-12</created><authors><author><keyname>Moeini</keyname><forenames>Mahdi</forenames></author><author><keyname>Thi</keyname><forenames>Hoai An Le</forenames></author></authors><title>A DC programming approach for constrained two-dimensional non-guillotine
  cutting problem</title><categories>cs.CE</categories><comments>Proceedings of the International Conference on Industrial Engineering
  and Systems Management (IESM 2011), Metz, May 2011, pp. 212-221, (2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a new application of Difference of Convex functions
programming and DCA in solving the constrained two-dimensional non-guillotine
cutting problem. This problem consists of cutting a number of rectangular
pieces from a large rectangular object. The cuts are done under some
constraints and the objective is to maximize the total value of the pieces cut.
We reformulate this problem as a DC program and solve it by DCA. The
performance of the approach is compared with the standard solver CPLEX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3341</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3341</id><created>2014-04-12</created><authors><author><keyname>Ballatore</keyname><forenames>Andrea</forenames></author></authors><title>Defacing the map: Cartographic vandalism in the digital commons</title><categories>cs.CY</categories><comments>24 pages, 4 figures, 1 table.The Cartographic Journal, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the emergent phenomenon of carto-vandalism, the
intentional defacement of collaborative cartographic digital artefacts in the
context of volunteered geographic information. Through a qualitative analysis
of reported incidents in WikiMapia and OpenStreetMap, a typology of this kind
of vandalism is outlined, including play, ideological, fantasy, artistic, and
industrial carto-vandalism, as well as carto-spam. Two families of
counter-strategies deployed in amateur mapping communities are discussed.
First, the contributors organise forms of policing, based on volunteered
community involvement, patrolling the maps and reporting incidents. Second, the
detection of carto-vandalism can be supported by automated tools, based either
on explicit rules or on machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3349</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3349</id><created>2014-04-13</created><updated>2014-05-20</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author></authors><title>Computer Simulation Codes for the Quine-McCluskey Method of Logic
  Minimization</title><categories>cs.OH</categories><comments>45 pages, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quine-McCluskey method is useful in minimizing logic expressions for
larger number of variables when compared with minimization by Karnaugh Map or
Boolean algebra. In this paper, we have tried to put together all of the
computer codes which are available on the internet, edited and modified them as
well as rewritten some parts of those collected codes our self, which are used
in the implementation of the Quine- McCluskey method. A brief introduction and
the logic of this method are discussed following which the codes have been
provided. The Quine-McCluskey Method has been implemented using computer
languages like C and C++ using some amount of variations. Our effort is to list
them all, so that the readers well versed in any of the particular computer
language will find it easy to follow the code written in that particular
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3363</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3363</id><created>2014-04-13</created><updated>2015-05-07</updated><authors><author><keyname>Fuchs</keyname><forenames>Franz G.</forenames></author><author><keyname>Hjelmervik</keyname><forenames>Jon M.</forenames></author></authors><title>Interactive Isogeometric Volume Visualization with Pixel-Accurate
  Geometry</title><categories>cs.GR</categories><acm-class>I.6.9.h; G.1.1.e; G.1.5; G.1.7; G.1.0.g</acm-class><doi>10.1109/TVCG.2015.2430337</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent development, called isogeometric analysis, provides a unified
approach for design, analysis and optimization of functional products in
industry. Traditional volume rendering methods for inspecting the results from
the numerical simulations cannot be applied directly to isogeometric models. We
present a novel approach for interactive visualization of isogeometric analysis
results, ensuring correct, i.e., pixel-accurate geometry of the volume
including its bounding surfaces. The entire OpenGL pipeline is used in a
multi-stage algorithm leveraging techniques from surface rendering,
order-independent transparency, as well as theory and numerical methods for
ordinary differential equations. We showcase the efficiency of our approach on
different models relevant to industry, ranging from quality inspection of the
parametrization of the geometry, to stress analysis in linear elasticity, to
visualization of computational fluid dynamics results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3366</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3366</id><created>2014-04-13</created><updated>2014-04-27</updated><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author></authors><title>Learning Deep Convolutional Features for MRI Based Alzheimer's Disease
  Classification</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to an error in the
  MRI data used in the experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective and accurate diagnosis of Alzheimer's disease (AD) or mild
cognitive impairment (MCI) can be critical for early treatment and thus has
attracted more and more attention nowadays. Since first introduced, machine
learning methods have been gaining increasing popularity for AD related
research. Among the various identified biomarkers, magnetic resonance imaging
(MRI) are widely used for the prediction of AD or MCI. However, before a
machine learning algorithm can be applied, image features need to be extracted
to represent the MRI images. While good representations can be pivotal to the
classification performance, almost all the previous studies typically rely on
human labelling to find the regions of interest (ROI) which may be correlated
to AD, such as hippocampus, amygdala, precuneus, etc. This procedure requires
domain knowledge and is costly and tedious.
  Instead of relying on extraction of ROI features, it is more promising to
remove manual ROI labelling from the pipeline and directly work on the raw MRI
images. In other words, we can let the machine learning methods to figure out
these informative and discriminative image structures for AD classification. In
this work, we propose to learn deep convolutional image features using
unsupervised and supervised learning. Deep learning has emerged as a powerful
tool in the machine learning community and has been successfully applied to
various tasks. We thus propose to exploit deep features of MRI images based on
a pre-trained large convolutional neural network (CNN) for AD and MCI
classification, which spares the effort of manual ROI annotation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3368</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3368</id><created>2014-04-13</created><updated>2014-12-05</updated><authors><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Nisnevitch</keyname><forenames>Pinhas</forenames></author></authors><title>Near-optimal sample compression for nearest neighbors</title><categories>cs.LG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first sample compression algorithm for nearest neighbors with
non-trivial performance guarantees. We complement these guarantees by
demonstrating almost matching hardness lower bounds, which show that our bound
is nearly optimal. Our result yields new insight into margin-based nearest
neighbor classification in metric spaces and allows us to significantly sharpen
and simplify existing bounds. Some encouraging empirical results are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3370</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3370</id><created>2014-04-13</created><authors><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Distance function of D numbers</title><categories>cs.AI</categories><comments>29 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer theory is widely applied in uncertainty modelling and
knowledge reasoning due to its ability of expressing uncertain information. A
distance between two basic probability assignments(BPAs) presents a measure of
performance for identification algorithms based on the evidential theory of
Dempster-Shafer. However, some conditions lead to limitations in practical
application for Dempster-Shafer theory, such as exclusiveness hypothesis and
completeness constraint. To overcome these shortcomings, a novel theory called
D numbers theory is proposed. A distance function of D numbers is proposed to
measure the distance between two D numbers. The distance function of D numbers
is an generalization of distance between two BPAs, which inherits the advantage
of Dempster-Shafer theory and strengthens the capability of uncertainty
modeling. An illustrative case is provided to demonstrate the effectiveness of
the proposed function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3377</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3377</id><created>2014-04-13</created><authors><author><keyname>Pickhardt</keyname><forenames>Rene</forenames></author><author><keyname>Gottron</keyname><forenames>Thomas</forenames></author><author><keyname>K&#xf6;rner</keyname><forenames>Martin</forenames></author><author><keyname>Wagner</keyname><forenames>Paul Georg</forenames></author><author><keyname>Speicher</keyname><forenames>Till</forenames></author><author><keyname>Staab</keyname><forenames>Steffen</forenames></author></authors><title>A Generalized Language Model as the Combination of Skipped n-grams and
  Modified Kneser-Ney Smoothing</title><categories>cs.CL</categories><comments>13 pages, 2 figures, ACL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel approach for building language models based on a
systematic, recursive exploration of skip n-gram models which are interpolated
using modified Kneser-Ney smoothing. Our approach generalizes language models
as it contains the classical interpolation with lower order models as a special
case. In this paper we motivate, formalize and present our approach. In an
extensive empirical experiment over English text corpora we demonstrate that
our generalized language models lead to a substantial reduction of perplexity
between 3.1% and 12.7% in comparison to traditional language models using
modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over
three other languages and a domain specific corpus where we observed consistent
improvements. Finally, we also show that the strength of our approach lies in
its ability to cope in particular with sparse training data. Using a very small
training data set of only 736 KB text we yield improvements of even 25.7%
reduction of perplexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3378</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3378</id><created>2014-04-13</created><updated>2014-11-04</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Shalev-Shwatz</keyname><forenames>Shai</forenames></author></authors><title>Complexity theoretic limitations on learning DNF's</title><categories>cs.LG cs.CC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.2272</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the recently developed framework of [Daniely et al, 2014], we show that
under a natural assumption on the complexity of refuting random K-SAT formulas,
learning DNF formulas is hard. Furthermore, the same assumption implies the
hardness of learning intersections of $\omega(\log(n))$ halfspaces,
agnostically learning conjunctions, as well as virtually all (distribution
free) learning problems that were previously shown hard (under complexity
assumptions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3381</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3381</id><created>2014-04-13</created><authors><author><keyname>De Vogeleer</keyname><forenames>Karel</forenames></author><author><keyname>Memmi</keyname><forenames>Gerard</forenames></author><author><keyname>Jouvelot</keyname><forenames>Pierre</forenames></author><author><keyname>Coelho</keyname><forenames>Fabien</forenames></author></authors><title>Modeling the Temperature Bias of Power Consumption for Nanometer-Scale
  CPUs in Application Processors</title><categories>cs.AR</categories><comments>Submitted to SAMOS 2014; International Conference on Embedded
  Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and experimentally validate a new macro-level model of the CPU
temperature/power relationship within nanometer-scale application processors or
system-on-chips. By adopting a holistic view, this model is able to take into
account many of the physical effects that occur within such systems. Together
with two algorithms described in the paper, our results can be used, for
instance by engineers designing power or thermal management units, to cancel
the temperature-induced bias on power measurements. This will help them gather
temperature-neutral power data while running multiple instance of their
benchmarks. Also power requirements and system failure rates can be decreased
by controlling the CPU's thermal behavior.
  Even though it is usually assumed that the temperature/power relationship is
exponentially related, there is however a lack of publicly available physical
temperature/power measurements to back up this assumption, something our paper
corrects. Via measurements on two pertinent platforms sporting nanometer-scale
application processors, we show that the power/temperature relationship is
indeed very likely exponential over a 20{\deg}C to 85{\deg}C temperature range.
Our data suggest that, for application processors operating between 20{\deg}C
and 50{\deg}C, a quadratic model is still accurate and a linear approximation
is acceptable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3382</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3382</id><created>2014-04-13</created><authors><author><keyname>Ray</keyname><forenames>Abhishek</forenames></author><author><keyname>Mishra</keyname><forenames>Siba</forenames></author><author><keyname>Mohapatra</keyname><forenames>Durga Prasad</forenames></author></authors><title>An Approach for Computing Dynamic Slice of Concurrent Aspect-Oriented
  Programs</title><categories>cs.SE</categories><comments>20 pages. arXiv admin note: text overlap with arXiv:1403.0100</comments><journal-ref>International Journal of Software Engineering and Its
  Applications, Vol. 7, No. 1, January, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dynamic slicing algorithm to compute the slice of concurrent
aspect-oriented programs. We use a dependence based intermediate program
representation called Concurrent Aspect-oriented System Dependence Graph
(CASDG) to represent a concurrent aspect-oriented program. The CASDG of an
aspect-oriented program consists of a system dependence graph (SDG) for the
non-aspect code, a group of dependence graphs for aspect code and some
additional dependence edges used to connect the system dependence graph for the
non-aspect code to dependence graph for aspect code. The proposed dynamic
slicing al-gorithm is an extended version of NMDS algorithm for concurrent
object-oriented programs, which is based on marking and unmarking of the
executed nodes in CASDG appropriately during run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3389</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3389</id><created>2014-04-13</created><authors><author><keyname>Bauso</keyname><forenames>Dario</forenames></author><author><keyname>Dia</keyname><forenames>Ben Mansour</forenames></author><author><keyname>Djehiche</keyname><forenames>Boualem</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Tempone</keyname><forenames>Raul</forenames></author></authors><title>Mean-Field Games for Marriage</title><categories>math.OC cs.GT cs.SY math.DS math.PR</categories><comments>22 figures. Accepted and to appear in PLoS One</comments><doi>10.1371/journal.pone.0094933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines mean-field games for marriage. The results support the
argument that optimizing the long-term well-being through effort and social
feeling state distribution (mean-field) will help to stabilize marriage.
However, if the cost of effort is very high, the couple fluctuates in a bad
feeling state or the marriage breaks down. We then examine the influence of
society on a couple using mean field sentimental games. We show that, in
mean-field equilibrium, the optimal effort is always higher than the one-shot
optimal effort. We illustrate numerically the influence of the couple's network
on their feeling states and their well-being.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3391</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3391</id><created>2014-04-13</created><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>Adjacency labeling schemes and induced-universal graphs</title><categories>cs.DS cs.DM math.CO</categories><acm-class>G.2.2; E.1; E.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a way of assigning labels to the vertices of any undirected graph
on up to $n$ vertices, each composed of $n/2+O(1)$ bits, such that given the
labels of two vertices, and no other information regarding the graph, it is
possible to decide whether or not the vertices are adjacent in the graph. This
is optimal, up to an additive constant, and constitutes the first improvement
in almost 50 years of an $n/2+O(\log n)$ bound of Moon. As a consequence, we
obtain an induced-universal graph for $n$-vertex graphs containing only
$O(2^{n/2})$ vertices, which is optimal up to a multiplicative constant,
solving an open problem of Vizing from 1968. We obtain similar tight results
for directed graphs, tournaments and bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3394</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3394</id><created>2014-04-13</created><updated>2016-01-18</updated><authors><author><keyname>Li</keyname><forenames>Gang</forenames></author><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Decentralized and Collaborative Subspace Pursuit: A
  Communication-Efficient Algorithm for Joint Sparsity Pattern Recovery with
  Sensor Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 64, no. 3, pp.
  556-566, Feb. 2016</journal-ref><doi>10.1109/TSP.2015.2483482</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of joint sparsity pattern recovery in
a distributed sensor network. The sparse multiple measurement vector signals
(MMVs) observed by all the nodes are assumed to have a common (but unknown)
sparsity pattern. To accurately recover the common sparsity pattern in a
decentralized manner with a low communication overhead of the network, we
develop an algorithm named decentralized and collaborative subspace pursuit
(DCSP). In DCSP, each node is required to perform three kinds of operations per
iteration: 1) estimate the local sparsity pattern by finding the subspace that
its measurement vector most probably lies in; 2) share its local sparsity
pattern estimate with one-hop neighboring nodes; and 3) update the final
sparsity pattern estimate by majority vote based fusion of all the local
sparsity pattern estimates obtained in its neighborhood. The convergence of
DCSP is proved and its communication overhead is quantitatively analyzed. We
also propose another decentralized algorithm named generalized DCSP (GDCSP) by
allowing more information exchange among neighboring nodes to further improve
the accuracy of sparsity pattern recovery at the cost of increased
communication overhead. Experimental results show that, 1) compared with
existing decentralized algorithms, DCSP provides much better accuracy of
sparsity pattern recovery at a comparable communication cost; and 2) the
accuracy of GDCSP is very close to that of centralized processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3396</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3396</id><created>2014-04-13</created><updated>2015-03-28</updated><authors><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author><author><keyname>Hatami</keyname><forenames>Hamed</forenames></author><author><keyname>Keller</keyname><forenames>Nathan</forenames></author><author><keyname>Lifshitz</keyname><forenames>Noam</forenames></author></authors><title>On the sum of the L1 influences of bounded functions</title><categories>cs.CC</categories><comments>16 pages; accepted for publication in the Israel Journal of
  Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f\colon \{-1,1\}^n \to [-1,1]$ have degree $d$ as a multilinear
polynomial. It is well-known that the total influence of $f$ is at most $d$.
Aaronson and Ambainis asked whether the total $L_1$ influence of $f$ can also
be bounded as a function of $d$. Ba\v{c}kurs and Bavarian answered this
question in the affirmative, providing a bound of $O(d^3)$ for general
functions and $O(d^2)$ for homogeneous functions. We improve on their results
by providing a bound of $d^2$ for general functions and $O(d\log d)$ for
homogeneous functions. In addition, we prove a bound of $d/(2 \pi)+o(d)$ for
monotone functions, and provide a matching example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3403</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3403</id><created>2014-04-13</created><updated>2016-01-01</updated><authors><author><keyname>Chang</keyname><forenames>Hsien-Chih</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>From Proximity to Utility: A Voronoi Partition of Pareto Optima</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extension of Voronoi diagrams where not only the distance to
the site is taken into account when considering which site the client is going
to use, but additional attributes (i.e., prices) are also considered. A cell in
this diagram is then the locus of all points (i.e. clients) that consider the
same set of sites to be relevant. In particular, the precise site a client
might use from this candidate set depends on parameters that might change
between usages, and the candidate set lists all of the relevant sites. The
resulting diagram is significantly more expressive than Voronoi diagrams, but
naturally has the drawback that its complexity, even in the plane, might be
quite high.
  Nevertheless, we show that if the attributes of the sites are drawn from the
same distribution (note that the locations are fixed), then the expected
complexity of the candidate diagram is near linear. To derive this result, we
derive several new technical results, which are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3406</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3406</id><created>2014-04-13</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames></author></authors><title>Knowledge-Based Automatic Generation of Linear Algebra Algorithms and
  Code</title><categories>cs.MS</categories><comments>Dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation focuses on the design and the implementation of
domain-specific compilers for linear algebra matrix equations. The development
of efficient libraries for such equations, which lie at the heart of most
software for scientific computing, is a complex process that requires expertise
in a variety of areas, including the application domain, algorithms, numerical
analysis and high-performance computing. Moreover, the process involves the
collaboration of several people for a considerable amount of time. With our
compilers, we aim to relieve the developers from both designing algorithms and
writing code, and to generate routines that match or even surpass the
performance of those written by human experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3407</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3407</id><created>2014-04-13</created><authors><author><keyname>Shevchenko</keyname><forenames>Ruslan</forenames></author></authors><title>Annotated imports</title><categories>cs.PL</categories><acm-class>D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presented simple extensions to scala language related to import statements:
exported imports, which provide ability to reuse sequence of import clauses in
composable form and default rewriters, which provide mechanism for pluggable
macro-based AST transformation of overall compilation unit, activated by import
of library object. Using these facilities not only allows more compact code, it
prevents application programmer from producing certain type of errors too and
allows to implement local language extension as libraries on top of standard
compiler. Part of discussed extensions is submitted to scala language committee
as pre-sip \cite{ai-presip} and can be used as first step for refining imports
semantics in the future version of scala language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3411</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3411</id><created>2014-04-13</created><authors><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Laurenti</keyname><forenames>Nicola</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author></authors><title>Achievable Secrecy Rates over MIMOME Gaussian Channels with GMM Signals
  in Low-Noise Regime</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, to be presented at Global Wireless Summit 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wiretap multiple-input multiple-output multiple-eavesdropper
(MIMOME) channel, where agent Alice aims at transmitting a secret message to
agent Bob, while leaking no information on it to an eavesdropper agent Eve. We
assume that Alice has more antennas than both Bob and Eve, and that she has
only statistical knowledge of the channel towards Eve. We focus on the
low-noise regime, and assess the secrecy rates that are achievable when the
secret message determines the distribution of a multivariate Gaussian mixture
model (GMM) from which a realization is generated and transmitted over the
channel. In particular, we show that if Eve has fewer antennas than Bob, secret
transmission is always possible at low-noise. Moreover, we show that in the
low-noise limit the secrecy capacity of our scheme coincides with its
unconstrained capacity, by providing a class of covariance matrices that allow
to attain such limit without the need of wiretap coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3415</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3415</id><created>2014-04-13</created><updated>2014-04-15</updated><authors><author><keyname>Abramov</keyname><forenames>E. G.</forenames></author><author><keyname>Komissarov</keyname><forenames>A. B.</forenames></author><author><keyname>Kornyakov</keyname><forenames>D. A.</forenames></author></authors><title>Generalized version of the support vector machine for binary
  classification problems: supporting hyperplane machine</title><categories>cs.LG stat.ML</categories><comments>22 pages with 3 figures, 1 Octave script</comments><acm-class>F.1.1; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper there is proposed a generalized version of the SVM for binary
classification problems in the case of using an arbitrary transformation x -&gt;
y. An approach similar to the classic SVM method is used. The problem is widely
explained. Various formulations of primal and dual problems are proposed. For
one of the most important cases the formulae are derived in detail. A simple
computational example is demonstrated. The algorithm and its implementation is
presented in Octave language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3418</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3418</id><created>2014-04-13</created><authors><author><keyname>Vats</keyname><forenames>Divyanshu</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Active Learning for Undirected Graphical Model Selection</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><comments>AISTATS 2014</comments><journal-ref>Proceedings of the 17th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. JMLR: W&amp;CP
  volume 33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies graphical model selection, i.e., the problem of estimating
a graph of statistical relationships among a collection of random variables.
Conventional graphical model selection algorithms are passive, i.e., they
require all the measurements to have been collected before processing begins.
We propose an active learning algorithm that uses junction tree representations
to adapt future measurements based on the information gathered from prior
measurements. We prove that, under certain conditions, our active learning
algorithm requires fewer scalar measurements than any passive algorithm to
reliably estimate a graph. A range of numerical results validate our theory and
demonstrates the benefits of active learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3435</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3435</id><created>2014-04-13</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author></authors><title>Web Search of New Linearized Medical Drug Leads</title><categories>cs.IR</categories><comments>8 pages, 9 figures, reprint of paper in SKY 2011 Workshop, Paris,
  France, October 2011, SciTePress Digital Library</comments><acm-class>H.3.3</acm-class><doi>10.5220/0003705401080115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web is a potentially huge source of medical drug leads. But despite the
significant amount of multi- dimensional information about drugs, currently
commercial search engines accept only linear keyword strings as inputs. This
work uses linearized fragments of molecular structures as knowledge
representation units to serve as inputs to search engines. It is shown that
quite arbitrary fragments are surprisingly free of ambiguity, obtaining
relatively small result sets, which are both manageable and rich in novel
potential drug leads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3438</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3438</id><created>2014-04-13</created><updated>2014-09-22</updated><authors><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Srinivasan</keyname><forenames>Kannan</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Constant Delay and Constant Feedback Moving Window Network Coding for
  Wireless Multicast: Design and Asymptotic Analysis</title><categories>cs.IT math.IT</categories><comments>18 pages, 11 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge of wireless multicast is to be able to support a large
number of users while simultaneously maintaining low delay and low feedback
overhead. In this paper, we develop a joint coding and feedback scheme named
Moving Window Network Coding with Anonymous Feedback (MWNC-AF) that
successfully addresses this challenge. In particular, we show that our scheme
simultaneously achieves both a constant decoding delay and a constant feedback
overhead, irrespective of the number of receivers $n$, without sacrificing
either throughput or reliability. We explicitly characterize the asymptotic
decay rate of the tail of the delay distribution, and prove that transmitting a
fixed amount of information bits into the MWNC-AF encoder buffer in each
time-slot (called &quot;constant data injection process&quot;) achieves the fastest decay
rate, thus showing how to obtain delay optimality in a large deviation sense.
We then investigate the average decoding delay of MWNC-AF, and show that when
the traffic load approaches the capacity, the average decoding delay under the
constant injection process is at most one half of that under a Bernoulli
injection process. In addition, we prove that the per-packet encoding and
decoding complexity of MWNC-AF both scale as $O(\log n)$, with the number of
receivers $n$. Our simulations further underscore the performance of our scheme
through comparisons with other schemes and show that the delay, encoding and
decoding complexity are low even for a large number of receivers, demonstrating
the efficiency, scalability, and ease of implementability of MWNC-AF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3439</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3439</id><created>2014-04-13</created><authors><author><keyname>Arslan</keyname><forenames>Omur</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>Anytime Hierarchical Clustering</title><categories>stat.ML cs.IR cs.LG</categories><comments>13 pages, 6 figures, 5 tables, in preparation for submission to a
  conference</comments><acm-class>H.3.3; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new anytime hierarchical clustering method that iteratively
transforms an arbitrary initial hierarchy on the configuration of measurements
along a sequence of trees we prove for a fixed data set must terminate in a
chain of nested partitions that satisfies a natural homogeneity requirement.
Each recursive step re-edits the tree so as to improve a local measure of
cluster homogeneity that is compatible with a number of commonly used (e.g.,
single, average, complete) linkage functions. As an alternative to the standard
batch algorithms, we present numerical evidence to suggest that appropriate
adaptations of this method can yield decentralized, scalable algorithms
suitable for distributed/parallel computation of clustering hierarchies and
online tracking of clustering trees applicable to large, dynamically changing
databases and anomaly detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3442</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3442</id><created>2014-04-13</created><authors><author><keyname>Etesami</keyname><forenames>Seyed Rasoul</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Pure Nash Equilibrium in Capacitated Selfish Replication (CSR) Game</title><categories>cs.GT cs.DM cs.SY math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the capacitated selfish replication game with binary preferences.
We first devise an algorithm which can reach a pure Nash equilibrium on trees
after n steps where n is the number of nodes (players) in the game. Introducing
an ordinal potential function for such a game, we determine the convergence
rate of the best response dynamics to a pure Nash equilibrium based on the
radii of the updating agents. Using this potential function, we propose a least
best response dynamics. We prove polynomial time convergence of such dynamics
to a pure Nash equilibrium for general graphs when the number of resources is
limited or their edge density is high enough with respect to the number of
resources. This partially answers a conjecture on the existence of PLS-hard
algorithms for finding the equilibrium when the number of resources is more
than 2. We show that the least best response dynamics improve the convergence
speed of an earlier algorithm [1] in the case of two resources by a factor of
at least n^2. We extend the results for the capacitated selfish replication
games with arbitrary cache size and provide an effective allocation rule for
extra capacities in the network in order to accelerate the least best response
dynamics to an equilibrium in polynomial time. Finally, we make a comparison
between graph coloring and the set of equilibria of the capacitated selfish
replication game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3447</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3447</id><created>2014-04-13</created><authors><author><keyname>Guo</keyname><forenames>Alan</forenames></author></authors><title>Group homomorphisms as error correcting codes</title><categories>cs.IT math.GR math.IT</categories><comments>13 pages</comments><msc-class>20D10, 20D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the minimum distance of the error correcting code formed by
the homomorphisms between two finite groups $G$ and $H$. We prove some general
structural results on how the distance behaves with respect to natural group
operations, such as passing to subgroups and quotients, and taking products.
Our main result is a general formula for the distance when $G$ is solvable or
$H$ is nilpotent, in terms of the normal subgroup structure of $G$ as well as
the prime divisors of $|G|$ and $|H|$. In particular, we show that in the above
case, the distance is independent of the subgroup structure of $H$. We
complement this by showing that, in general, the distance depends on the
subgroup structure $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3448</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3448</id><created>2014-04-13</created><updated>2014-10-27</updated><authors><author><keyname>Zhong</keyname><forenames>YuKun</forenames></author><author><keyname>Wang</keyname><forenames>BaoQiu</forenames></author><author><keyname>Lin</keyname><forenames>JianBiao</forenames></author><author><keyname>Tao</keyname><forenames>Chen</forenames></author><author><keyname>Nian</keyname><forenames>Che</forenames></author><author><keyname>Wen</keyname><forenames>Xie</forenames></author></authors><title>Solving The Longest Overlap Region Problem for Noncoding DNA Sequences
  with GPU</title><categories>cs.DC cs.CE</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early hardware limitations of GPU (lack of synchronization primitives and
limited memory caching mechanisms) can make GPU-based computation inefficient.
Now Bio-technologies bring more chances to Bioinformatics and Biological
Engineering. Our paper introduces a way to solve the longest overlap region of
non-coding DNA sequences on using the Compute Unified Device Architecture
(CUDA) platform Intel(R) Core(TM) i3- 3110m quad-core. Compared to standard CPU
implementation, CUDA performance proves the method of the longest overlap
region recognition of noncoding DNA is an efficient approach to
high-performance bioinformatics applications. Studies show the fact that
efficiency of GPU performance is more than 20 times speedup than that of CPU
serial implementation. We believe our method gives a cost-efficient solution to
the bioinformatics community for solving longest overlap region recognition
problem and other related fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3451</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3451</id><created>2014-04-13</created><authors><author><keyname>Lai</keyname><forenames>Jun</forenames></author><author><keyname>Ambikasaran</keyname><forenames>Sivaram</forenames></author><author><keyname>Greengard</keyname><forenames>Leslie F.</forenames></author></authors><title>A fast direct solver for high frequency scattering from a large cavity
  in two dimensions</title><categories>math.NA cs.NA</categories><comments>15 pages, 9 figures. Contact author for animation</comments><msc-class>97N40, 65F05, 45B05, 45A05</msc-class><acm-class>G.1; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast direct solver for the simulation of electromagnetic
scattering from an arbitrarily-shaped, large, empty cavity embedded in an
infinite perfectly conducting half space. The governing Maxwell equations are
reformulated as a well-conditioned second kind integral equation and the
resulting linear system is solved in nearly linear time using a hierarchical
matrix factorization technique. We illustrate the performance of the scheme
with several numerical examples for complex cavity shapes over a wide range of
frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3454</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3454</id><created>2014-04-13</created><updated>2014-05-08</updated><authors><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Hung</keyname><forenames>Hui-Ju</forenames></author><author><keyname>Lin</keyname><forenames>Chih-Chung</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author></authors><title>Scalable Steiner Tree for Multicast Communications in Software-Defined
  Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-Defined Networking (SDN) enables flexible network resource
allocations for traffic engineering, but at the same time the scalability
problem becomes more serious since traffic is more difficult to be aggregated.
Those crucial issues in SDN have been studied for unicast but have not been
explored for multicast traffic, and addressing those issues for multicast is
more challenging since the identities and the number of members in a multicast
group can be arbitrary. In this paper, therefore, we propose a new multicast
tree for SDN, named Branch-aware Steiner Tree (BST). The BST problem is
difficult since it needs to jointly minimize the numbers of the edges and the
branch nodes in a tree, and we prove that it is NP-Hard and inapproximable
within $k$, which denotes the number of group members. We further design an
approximation algorithm, called Branch Aware Edge Reduction Algorithm (BAERA),
to solve the problem. Simulation results demonstrate that the trees obtained by
BAERA are more bandwidth-efficient and scalable than the shortest-path trees
and traditional Steiner trees. Most importantly, BAERA is computation-efficient
to be deployed in SDN since it can generate a tree on massive networks in small
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3456</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3456</id><created>2014-04-13</created><authors><author><keyname>Zhong</keyname><forenames>Yukun</forenames></author><author><keyname>He</keyname><forenames>ZhiWei</forenames></author><author><keyname>Wang</keyname><forenames>XianHong</forenames></author><author><keyname>Cao</keyname><forenames>XiongBin</forenames></author></authors><title>A Way For Accelerating The DNA Sequence Reconstruction Problem By CUDA</title><categories>cs.DC cs.CE</categories><comments>7 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, we usually utilize the method of shotgun to cut a DNA sequence
into pieces and we have to reconstruct the original DNA sequence from the
pieces, those are widely used method for DNA assembly. Emerging DNA sequence
technologies open up more opportunities for molecular biology. This paper
introduce a new method to improve the efficiency of reconstructing DNA sequence
using suffix array based on CUDA programming model. The experimental result
show the construction of suffix array using GPU is an more efficient approach
on Intel(R) Core(TM) i3-3110K quad-core and NVIDIA GeForce 610M GPU, and study
show the performance of our method is more than 20 times than that of CPU
serial implementation. We believe our method give a cost-efficient solution to
the bioinformatics community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3458</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3458</id><created>2014-04-14</created><updated>2014-07-24</updated><authors><author><keyname>Lin</keyname><forenames>Sian-Jheng</forenames></author><author><keyname>Chung</keyname><forenames>Wei-Ho</forenames></author><author><keyname>Han</keyname><forenames>Yunghsiang S.</forenames></author></authors><title>Novel Polynomial Basis and Its Application to Reed-Solomon Erasure Codes</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a new basis of polynomial over finite fields of
characteristic two and then apply it to the encoding/decoding of Reed-Solomon
erasure codes. The proposed polynomial basis allows that $h$-point polynomial
evaluation can be computed in $O(h\log_2(h))$ finite field operations with
small leading constant. As compared with the canonical polynomial basis, the
proposed basis improves the arithmetic complexity of addition, multiplication,
and the determination of polynomial degree from $O(h\log_2(h)\log_2\log_2(h))$
to $O(h\log_2(h))$. Based on this basis, we then develop the encoding and
erasure decoding algorithms for the $(n=2^r,k)$ Reed-Solomon codes. Thanks to
the efficiency of transform based on the polynomial basis, the encoding can be
completed in $O(n\log_2(k))$ finite field operations, and the erasure decoding
in $O(n\log_2(n))$ finite field operations. To the best of our knowledge, this
is the first approach supporting Reed-Solomon erasure codes over
characteristic-2 finite fields while achieving a complexity of $O(n\log_2(n))$,
in both additive and multiplicative complexities. As the complexity leading
factor is small, the algorithms are advantageous in practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3461</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3461</id><created>2014-04-14</created><authors><author><keyname>Lu</keyname><forenames>Xiaolu</forenames></author><author><keyname>Li</keyname><forenames>Dongxu</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Feng</keyname><forenames>Ling</forenames></author></authors><title>A 2D based Partition Strategy for Solving Ranking under Team Context
  (RTP)</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a 2D based partition method for solving the problem
of Ranking under Team Context(RTC) on datasets without a priori. We first map
the data into 2D space using its minimum and maximum value among all
dimensions. Then we construct window queries with consideration of current team
context. Besides, during the query mapping procedure, we can pre-prune some
tuples which are not top ranked ones. This pre-classified step will defer
processing those tuples and can save cost while providing solutions for the
problem. Experiments show that our algorithm performs well especially on large
datasets with correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3465</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3465</id><created>2014-04-14</created><authors><author><keyname>LeMay</keyname><forenames>Michael</forenames></author><author><keyname>Gunter</keyname><forenames>Carl A.</forenames></author></authors><title>Network-on-Chip Firewall: Countering Defective and Malicious
  System-on-Chip Hardware</title><categories>cs.CR</categories><comments>33 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices are in roles where the integrity and confidentiality of their
apps and data are of paramount importance. They usually contain a
System-on-Chip (SoC), which integrates microprocessors and peripheral
Intellectual Property (IP) connected by a Network-on-Chip (NoC). Malicious IP
or software could compromise critical data. Some types of attacks can be
blocked by controlling data transfers on the NoC using Memory Management Units
(MMUs) and other access control mechanisms. However, commodity processors do
not provide strong assurances regarding the correctness of such mechanisms, and
it is challenging to verify that all access control mechanisms in the system
are correctly configured. We propose a NoC Firewall (NoCF) that provides a
single locus of control and is amenable to formal analysis. We demonstrate an
initial analysis of its ability to resist malformed NoC commands, which we
believe is the first effort to detect vulnerabilities that arise from NoC
protocol violations perpetrated by erroneous or malicious IP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3482</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3482</id><created>2014-04-14</created><authors><author><keyname>Philippe</keyname><forenames>Gaborit</forenames></author><author><keyname>Gilles</keyname><forenames>Zemor</forenames></author></authors><title>On the hardness of the decoding and the minimum distance problems for
  rank codes</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a randomized reduction for the Rank Syndrome Decoding
problem and Rank Minimum Distance problem for rank codes. Our results are based
on an embedding from linear codes equipped with Hamming distance unto linear
codes over an extension field equipped with the rank metric. We prove that if
both previous problems for rank metric are in ZPP = RP$\cap$coRP, then we would
have NP=ZPP. We also give complexity results for the respective approximation
problems in rank metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3497</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3497</id><created>2014-04-14</created><authors><author><keyname>Thomsen</keyname><forenames>Henning</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Using Wireless Network Coding to Replace a Wired with Wireless Backhaul</title><categories>cs.IT math.IT</categories><comments>6 pages (double-column), 5 figures, submitted to Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular networks are evolving towards dense deployment of small cells. This
in turn demands flexible and efficient backhauling solutions. A viable solution
that reuses the same spectrum is wireless backhaul where the Small Base Station
(SBS) acts as a relay. In this paper we consider a reference system that uses
wired backhaul and each Mobile Station (MS) in the small cell has its uplink
and downlink rates defined. The central question is: if we remove the wired
backhaul, how much extra power should the wireless backhaul use in order to
support the same uplink/downlink rates? We introduce the idea of
wireless-emulated wire (WEW), based on two-way relaying and network coding.
Furthermore, in a scenario where two SBSs are served simultaneously, WEW gives
rise to new communication strategies, partially inspired by the private/public
messages from the Han-Kobayashi scheme for interference channel. We formulate
and solve the associated optimization problems. The proposed approach provides
a convincing argument that two-way communication is the proper context to
design and optimize wireless backhauling solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3501</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3501</id><created>2014-04-14</created><updated>2014-07-08</updated><authors><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Limouzy</keyname><forenames>Vincent</forenames></author><author><keyname>Mary</keyname><forenames>Arnaud</forenames></author><author><keyname>Nourine</keyname><forenames>Lhouari</forenames></author><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author></authors><title>Polynomial Delay Algorithm for Listing Minimal Edge Dominating sets in
  Graphs</title><categories>cs.DS</categories><comments>proofs simplified from previous version, 12 pages, 2 figures</comments><msc-class>68R05, 68R10, 05C30, 05C69, 05C76, 05C85</msc-class><acm-class>F.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Transversal problem, i.e, the enumeration of all the minimal transversals
of a hypergraph in output-polynomial time, i.e, in time polynomial in its size
and the cumulated size of all its minimal transversals, is a fifty years old
open problem, and up to now there are few examples of hypergraph classes where
the problem is solved. A minimal dominating set in a graph is a subset of its
vertex set that has a non empty intersection with the closed neighborhood of
every vertex. It is proved in [M. M. Kant\'e, V. Limouzy, A. Mary, L. Nourine,
On the Enumeration of Minimal Dominating Sets and Related Notions, In Revision
2014] that the enumeration of minimal dominating sets in graphs and the
enumeration of minimal transversals in hypergraphs are two equivalent problems.
Hoping this equivalence can help to get new insights in the Transversal
problem, it is natural to look inside graph classes. It is proved independently
and with different techniques in [Golovach et al. - ICALP 2013] and [Kant\'e et
al. - ISAAC 2012] that minimal edge dominating sets in graphs (i.e, minimal
dominating sets in line graphs) can be enumerated in incremental
output-polynomial time. We provide the first polynomial delay and polynomial
space algorithm that lists all the minimal edge dominating sets in graphs,
answering an open problem of [Golovach et al. - ICALP 2013]. Besides the
result, we hope the used techniques that are a mix of a modification of the
well-known Berge's algorithm and a strong use of the structure of line graphs,
are of great interest and could be used to get new output-polynomial time
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3520</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3520</id><created>2014-04-14</created><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Mitavskiy</keyname><forenames>Boris</forenames></author><author><keyname>Zhou</keyname><forenames>Yuren</forenames></author></authors><title>A Theoretical Assessment of Solution Quality in Evolutionary Algorithms
  for the Knapsack Problem</title><categories>cs.NE</categories><doi>10.1109/CEC.2014.6900442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms are well suited for solving the knapsack problem.
Some empirical studies claim that evolutionary algorithms can produce good
solutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigations
address the quality of solutions that evolutionary algorithms may produce for
the knapsack problem. The current paper focuses on a theoretical investigation
of three types of (N+1) evolutionary algorithms that exploit bitwise mutation,
truncation selection, plus different repair methods for the 0-1 knapsack
problem. It assesses the solution quality in terms of the approximation ratio.
Our work indicates that the solution produced by pure strategy and mixed
strategy evolutionary algorithms is arbitrarily bad. Nevertheless, the
evolutionary algorithm using helper objectives may produce 1/2-approximation
solutions to the 0-1 knapsack problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3525</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3525</id><created>2014-04-14</created><updated>2014-09-09</updated><authors><author><keyname>Wesemann</keyname><forenames>Stefan</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Distributed Asynchronous Optimization Framework for the MISO
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Signal Processing, accepted for publication, 16
  pages, 8 figures</comments><doi>10.1109/TSP.2014.2359631</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distributed optimization of transmit strategies in a
multiple-input, single-output (MISO) interference channel (IFC). Existing
distributed algorithms rely on stricly synchronized update steps by the
individual users. They require a global synchronization mechanism and
potentially suffer from the synchronization penalty caused by e.g., backhaul
communication delays and fixed update sequences. We establish a general
optimization framework that allows asynchronous update steps. The users perform
their computations at arbitrary instants of time, and do not wait for
information that has been sent to them. Based on certain bounds on the amount
of asynchronism that is present in the execution of the algorithm, we are able
to characterize its convergence. As illustrated by our numerical results, the
proposed algorithm can alleviate communication overloads and is not excessively
slowed down by neither communication delays, nor by differences in the
computation intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3537</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3537</id><created>2014-04-14</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz</forenames></author></authors><title>BeSpaceD: Towards a Tool Framework and Methodology for the Specification
  and Verification of Spatial Behavior of Distributed Software Component
  Systems</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we present work towards a framework for modeling and checking
behavior of spatially distributed component systems. Design goals of our
framework are the ability to model spatial behavior in a component oriented,
simple and intuitive way, the possibility to automatically analyse and verify
systems and integration possibilities with other modeling and verification
tools. We present examples and the verification steps necessary to prove
properties such as range coverage or the absence of collisions between
components and technical details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3538</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3538</id><created>2014-04-14</created><updated>2014-04-30</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph</forenames></author><author><keyname>Morvant</keyname><forenames>Emilie</forenames></author><author><keyname>Takhanov</keyname><forenames>Rustem</forenames></author></authors><title>Proceedings of The 38th Annual Workshop of the Austrian Association for
  Pattern Recognition (\&quot;OAGM), 2014</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 38th Annual Workshop of the Austrian Association for Pattern Recognition
(\&quot;OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop provides
a platform for researchers and industry to discuss traditional and new areas of
computer vision. This year the main topic is: Pattern Recognition:
interdisciplinary challenges and opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3543</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3543</id><created>2014-04-14</created><updated>2014-04-16</updated><authors><author><keyname>Zhu</keyname><forenames>Zhenyao</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Recover Canonical-View Faces in the Wild with Deep Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face images in the wild undergo large intra-personal variations, such as
poses, illuminations, occlusions, and low resolutions, which cause great
challenges to face-related applications. This paper addresses this challenge by
proposing a new deep learning framework that can recover the canonical view of
face images. It dramatically reduces the intra-person variances, while
maintaining the inter-person discriminativeness. Unlike the existing face
reconstruction methods that were either evaluated in controlled 2D environment
or employed 3D information, our approach directly learns the transformation
from the face images with a complex set of variations to their canonical views.
At the training stage, to avoid the costly process of labeling canonical-view
images from the training set by hand, we have devised a new measurement to
automatically select or synthesize a canonical-view image for each identity. As
an application, this face recovery approach is used for face verification.
Facial features are learned from the recovered canonical-view face images by
using a facial component-based convolutional neural network. Our approach
achieves the state-of-the-art performance on the LFW dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3577</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3577</id><created>2014-04-14</created><authors><author><keyname>Jurkiewicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick</forenames></author></authors><title>Cache-Oblivious VAT-Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The VAT-model (virtual address translation model) extends the EM-model
(external memory model) and takes the cost of address translation in virtual
memories into account. In this model, the cost of a single memory access may be
logarithmic in the largest address used. We show that the VAT-cost of
cache-oblivious algorithms is only by a constant factor larger than their
EM-cost; this requires a somewhat more stringent tall cache assumption as for
the EM-model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3580</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3580</id><created>2014-04-14</created><authors><author><keyname>Atanasov</keyname><forenames>Nikolay A.</forenames></author><author><keyname>Tron</keyname><forenames>Roberto</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Joint Estimation and Localization in Sensor Networks</title><categories>cs.MA cs.NI cs.RO cs.SY</categories><comments>9 pages (two-column); 5 figures; Manuscript submitted to the 2014
  IEEE Conference on Decision and Control (CDC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of collaborative tracking of dynamic targets
in wireless sensor networks. A novel distributed linear estimator, which is a
version of a distributed Kalman filter, is derived. We prove that the filter is
mean square consistent in the case of static target estimation. When large
sensor networks are deployed, it is common that the sensors do not have good
knowledge of their locations, which affects the target estimation procedure.
Unlike most existing approaches for target tracking, we investigate the
performance of our filter when the sensor poses need to be estimated by an
auxiliary localization procedure. The sensors are localized via a distributed
Jacobi algorithm from noisy relative measurements. We prove strong convergence
guarantees for the localization method and in turn for the joint localization
and target estimation approach. The performance of our algorithms is
demonstrated in simulation on environmental monitoring and target tracking
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3581</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3581</id><created>2014-04-14</created><updated>2014-09-29</updated><authors><author><keyname>Joly</keyname><forenames>Arnaud</forenames></author><author><keyname>Geurts</keyname><forenames>Pierre</forenames></author><author><keyname>Wehenkel</keyname><forenames>Louis</forenames></author></authors><title>Random forests with random projections of the output space for high
  dimensional multi-label classification</title><categories>stat.ML cs.LG</categories><journal-ref>Machine Learning and Knowledge Discovery in Databases, 2014, Part
  I, pp 607-622</journal-ref><doi>10.1007/978-3-662-44848-9_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adapt the idea of random projections applied to the output space, so as to
enhance tree-based ensemble methods in the context of multi-label
classification. We show how learning time complexity can be reduced without
affecting computational complexity and accuracy of predictions. We also show
that random output space projections may be used in order to reach different
bias-variance tradeoffs, over a broad panel of benchmark problems, and that
this may lead to improved accuracy while reducing significantly the
computational burden of the learning stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3589</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3589</id><created>2014-04-14</created><updated>2015-07-07</updated><authors><author><keyname>Michel</keyname><forenames>Mathieu</forenames></author><author><keyname>Quoitin</keyname><forenames>Bruno</forenames></author></authors><title>Technical Report : ContikiMAC vs X-MAC performance analysis</title><categories>cs.NI</categories><comments>Version submitted at IJSN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper try to better understand the performance of ContikiMAC compared to
X-MAC. ContikiMAC achieves a transmission by repeatedly transmitting a data
packet until the reception of an ACK from the destination. While X-MAC uses a
stream of small size strobes to advertise the destination of the incoming
transmission. A priori, X-MAC is then less bandwidth consumptive. To better
understand the efficiency of ContikiMAC, despite an intuitively more
consumptive transmitting procedure, we have compared this protocol to an
updated version of X-MAC which has been designed with the goal to obtain a more
objective comparison. At our best knowledge, this kind of &quot;objective&quot; and
detailed analysis has never been produced. Both protocols performances are
evaluated in terms of number of retransmissions, latency, packet delivery ratio
(PDR) and duty-cycle. Our study reveals that the combination of the fast-sleep
optimization shortening the wake-up period with a transmission procedure more
efficient helps ContikiMAC to obtain a better PDR at the cost of a reduced
latency and energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3591</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3591</id><created>2014-04-14</created><updated>2014-04-15</updated><authors><author><keyname>Argyriou</keyname><forenames>Andreas</forenames></author><author><keyname>Signoretto</keyname><forenames>Marco</forenames></author><author><keyname>Suykens</keyname><forenames>Johan</forenames></author></authors><title>Hybrid Conditional Gradient - Smoothing Algorithms with Applications to
  Sparse and Low Rank Regularization</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a hybrid conditional gradient - smoothing algorithm (HCGS) for
solving composite convex optimization problems which contain several terms over
a bounded set. Examples of these include regularization problems with several
norms as penalties and a norm constraint. HCGS extends conditional gradient
methods to cases with multiple nonsmooth terms, in which standard conditional
gradient methods may be difficult to apply. The HCGS algorithm borrows
techniques from smoothing proximal methods and requires first-order
computations (subgradients and proximity operations). Unlike proximal methods,
HCGS benefits from the advantages of conditional gradient methods, which render
it more efficient on certain large scale optimization problems. We demonstrate
these advantages with simulations on two matrix optimization problems:
regularization of matrices with combined $\ell_1$ and trace norm penalties; and
a convex relaxation of sparse PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3596</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3596</id><created>2014-04-14</created><updated>2015-11-03</updated><authors><author><keyname>Barbu</keyname><forenames>Adrian</forenames></author><author><keyname>Lay</keyname><forenames>Nathan</forenames></author><author><keyname>Gramajo</keyname><forenames>Gary</forenames></author></authors><title>Face Detection with a 3D Model</title><categories>cs.CV</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a part-based face detection approach where the spatial
relationship between the face parts is represented by a hidden 3D model with
six parameters. The computational complexity of the search in the six
dimensional pose space is addressed by proposing meaningful 3D pose candidates
by image-based regression from detected face keypoint locations. The 3D pose
candidates are evaluated using a parameter sensitive classifier based on
difference features relative to the 3D pose. A compatible subset of candidates
is then obtained by non-maximal suppression. Experiments on two standard face
detection datasets show that the proposed 3D model based approach obtains
results comparable to or better than state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3600</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3600</id><created>2014-04-14</created><updated>2015-07-13</updated><authors><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Fan</keyname><forenames>Hua</forenames></author><author><keyname>Xie</keyname><forenames>Eric Yong</forenames></author><author><keyname>Cheng</keyname><forenames>Ge</forenames></author><author><keyname>Li</keyname><forenames>Chengqing</forenames></author></authors><title>Deciphering a novel image cipher based on mixed transformed Logistic
  maps</title><categories>cs.CR</categories><comments>9 pages</comments><doi>10.1142/S0218127415501886</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since John von Neumann suggested utilizing Logistic map as a random number
generator in 1947, a great number of encryption schemes based on Logistic map
and/or its variants have been proposed. This paper re-evaluates the security of
an image cipher based on transformed logistic maps and proves that the image
cipher can be deciphered efficiently under two different conditions: 1) two
pairs of known plain-images and the corresponding cipher-images with
computational complexity of $O(2^{18}+L)$; 2) two pairs of chosen plain-images
and the corresponding cipher-images with computational complexity of $O(L)$,
where $L$ is the number of pixels in the plain-image. In contrast, the required
condition in the previous deciphering method is eighty-seven pairs of chosen
plain-images and the corresponding cipher-images with computational complexity
of $O(2^{7}+L)$. In addition, three other security flaws existing in most
Logistic-map-based ciphers are also reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3606</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3606</id><created>2014-04-14</created><updated>2014-08-28</updated><authors><author><keyname>Chan</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Jia</keyname><forenames>Kui</forenames></author><author><keyname>Gao</keyname><forenames>Shenghua</forenames></author><author><keyname>Lu</keyname><forenames>Jiwen</forenames></author><author><keyname>Zeng</keyname><forenames>Zinan</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>PCANet: A Simple Deep Learning Baseline for Image Classification?</title><categories>cs.CV cs.LG cs.NE</categories><doi>10.1109/TIP.2015.2475625</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a very simple deep learning network for image
classification which comprises only the very basic data processing components:
cascaded principal component analysis (PCA), binary hashing, and block-wise
histograms. In the proposed architecture, PCA is employed to learn multistage
filter banks. It is followed by simple binary hashing and block histograms for
indexing and pooling. This architecture is thus named as a PCA network (PCANet)
and can be designed and learned extremely easily and efficiently. For
comparison and better understanding, we also introduce and study two simple
variations to the PCANet, namely the RandNet and LDANet. They share the same
topology of PCANet but their cascaded filters are either selected randomly or
learned from LDA. We have tested these basic networks extensively on many
benchmark visual datasets for different tasks, such as LFW for face
verification, MultiPIE, Extended Yale B, AR, FERET datasets for face
recognition, as well as MNIST for hand-written digits recognition.
Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with
the state of the art features, either prefixed, highly hand-crafted or
carefully learned (by DNNs). Even more surprisingly, it sets new records for
many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST
variations. Additional experiments on other public datasets also demonstrate
the potential of the PCANet serving as a simple but highly competitive baseline
for texture classification and object recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3610</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3610</id><created>2014-04-10</created><authors><author><keyname>Adrover</keyname><forenames>Cosme</forenames></author><author><keyname>Bodnar</keyname><forenames>Todd</forenames></author><author><keyname>Salathe</keyname><forenames>Marcel</forenames></author></authors><title>Targeting HIV-related Medication Side Effects and Sentiment Using
  Twitter Data</title><categories>cs.SI cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a descriptive analysis of Twitter data. Our study focuses on
extracting the main side effects associated with HIV treatments. The crux of
our work was the identification of personal tweets referring to HIV. We
summarize our results in an infographic aimed at the general public. In
addition, we present a measure of user sentiment based on hand-rated tweets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3614</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3614</id><created>2014-04-14</created><updated>2015-04-17</updated><authors><author><keyname>Vond&#x159;ejc</keyname><forenames>Jaroslav</forenames></author><author><keyname>Zeman</keyname><forenames>Jan</forenames></author><author><keyname>Marek</keyname><forenames>Ivo</forenames></author></authors><title>Guaranteed upper-lower bounds on homogenized properties by FFT-based
  Galerkin method</title><categories>cs.NA math.NA physics.comp-ph</categories><comments>37 pages, 20 figures</comments><journal-ref>Computer Methods in Applied Mechanics and Engineering, 297, pp.
  258-291, 2015</journal-ref><doi>10.1016/j.cma.2015.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guaranteed upper-lower bounds on homogenized coefficients, arising from the
periodic cell problem, are calculated in a scalar elliptic setting. Our
approach builds on the recent variational reformulation of the Moulinec-Suquet
(1994) Fast Fourier Transform (FFT) homogenization scheme by Vond\v{r}ejc et
al. (2014), which is based on the conforming Galerkin approximation with
trigonometric polynomials. Upper-lower bounds are obtained by adjusting the
primal-dual finite element framework developed independently by Dvo\v{r}\'{a}k
(1993) and Wieckowski (1995) to the FFT-based Galerkin setting. We show that
the discretization procedure differs for odd and non-odd number of grid points.
Thanks to the Helmholtz decomposition inherited from the continuous
formulation, the duality structure is fully preserved for the odd
discretizations. In the latter case, a more complex primal-dual structure is
observed due to presence of the trigonometric polynomials associated with the
Nyquist frequencies. These theoretical findings are confirmed with numerical
examples. To conclude, the main advantage of the FFT-based approach over
conventional finite-element schemes is that the primal and the dual problems
are treated on the same basis, and this property can be extended beyond the
scalar elliptic setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3626</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3626</id><created>2014-04-14</created><updated>2014-11-20</updated><authors><author><keyname>Ghaddar</keyname><forenames>Bissan</forenames></author><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author><author><keyname>Mevissen</keyname><forenames>Martin</forenames></author></authors><title>Optimal Power Flow as a Polynomial Optimization Problem</title><categories>math.OC cs.SY</categories><comments>A more concise version, which also fixes a number of minor issues</comments><doi>10.1109/TPWRS.2015.2390037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formulating the alternating current optimal power flow (ACOPF) as a
polynomial optimization problem makes it possible to solve large instances in
practice and to guarantee asymptotic convergence in theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3633</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3633</id><created>2014-04-14</created><updated>2014-12-29</updated><authors><author><keyname>de Witt</keyname><forenames>Christian Schr&#xf6;der</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Zamdzhiev</keyname><forenames>Vladimir</forenames><affiliation>University of Oxford</affiliation></author></authors><title>The ZX-calculus is incomplete for quantum mechanics</title><categories>cs.LO math.CT quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 285-292</journal-ref><doi>10.4204/EPTCS.172.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the ZX-calculus is incomplete for quantum mechanics. We suggest
the addition of a new 'color-swap' rule, of which currently no analytical
formulation is known and which we suspect may be necessary, but not sufficient
to make the ZX-calculus complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3637</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3637</id><created>2014-04-14</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>A Game-Theoretic Framework for Decentralized Cooperative Data Exchange
  using Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a game theoretic framework for studying the
problem of minimizing the delay of instantly decodable network coding (IDNC)
for cooperative data exchange (CDE) in decentralized wireless network. In this
configuration, clients cooperate with each other to recover the erased packets
without a central controller. Game theory is employed herein as a tool for
improving the distributed solution by overcoming the need for a central
controller or additional signaling in the system. We model the session by
self-interested players in a non-cooperative potential game. The utility
functions are designed such that increasing individual payoff results in a
collective behavior achieving both a desirable system performance in a shared
network environment and the Nash bargaining solution. Three games are
developed: the first aims to reduce the completion time, the second to reduce
the maximum decoding delay and the third the sum decoding delay. We improve
these formulations to include punishment policy upon collision occurrence and
achieve the Nash bargaining solution. Through extensive simulations, our
framework is tested against the best performance that could be found in the
conventional point-to-multipoint (PMP) recovery process in numerous cases:
first we simulate the problem with complete information. We, then, simulate
with incomplete information and finally we test it in lossy feedback scenario.
Numerical results show that our formulation with complete information largely
outperforms the conventional PMP scheme in most situations and achieves a lower
delay. They also show that the completion time formulation with incomplete
information also outperforms the conventional PMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3638</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3638</id><created>2014-04-14</created><updated>2015-06-24</updated><authors><author><keyname>Pishdad</keyname><forenames>Leila</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Approximate MMSE Estimator for Linear Dynamic Systems with Gaussian
  Mixture Noise</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose an approximate Minimum Mean-Square Error (MMSE)
filter for linear dynamic systems with Gaussian Mixture noise. The proposed
estimator tracks each component of the Gaussian Mixture (GM) posterior with an
individual filter and minimizes the trace of the covariance matrix of the bank
of filters, as opposed to minimizing the MSE of individual filters in the
commonly used Gaussian sum filter (GSF). Hence, the spread of means in the
proposed method is smaller than that of GSF which makes it more robust to
removing components. Consequently, lower complexity reduction schemes can be
used with the proposed filter without losing estimation accuracy and precision.
This is supported through simulations on synthetic data as well as experimental
data related to an indoor localization system. Additionally, we show that in
two limit cases the state estimation provided by our proposed method converges
to that of GSF, and we provide simulation results supporting this in other
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3653</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3653</id><created>2014-04-14</created><updated>2014-06-16</updated><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author></authors><title>Maximum Persistency in Energy Minimization</title><categories>cs.DM</categories><comments>Extended technical report for the CVPR 2014 paper. Update: correction
  to the proof of characterization theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider discrete pairwise energy minimization problem (weighted
constraint satisfaction, max-sum labeling) and methods that identify a globally
optimal partial assignment of variables. When finding a complete optimal
assignment is intractable, determining optimal values for a part of variables
is an interesting possibility. Existing methods are based on different
sufficient conditions. We propose a new sufficient condition for partial
optimality which is: (1) verifiable in polynomial time (2) invariant to
reparametrization of the problem and permutation of labels and (3) includes
many existing sufficient conditions as special cases. We pose the problem of
finding the maximum optimal partial assignment identifiable by the new
sufficient condition. A polynomial method is proposed which is guaranteed to
assign same or larger part of variables than several existing approaches. The
core of the method is a specially constructed linear program that identifies
persistent assignments in an arbitrary multi-label setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3656</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3656</id><created>2014-04-14</created><authors><author><keyname>Raman</keyname><forenames>Karthik</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Methods for Ordinal Peer Grading</title><categories>cs.LG cs.IR</categories><comments>Submitted to KDD 2014</comments><acm-class>H.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MOOCs have the potential to revolutionize higher education with their wide
outreach and accessibility, but they require instructors to come up with
scalable alternates to traditional student evaluation. Peer grading -- having
students assess each other -- is a promising approach to tackling the problem
of evaluation at scale, since the number of &quot;graders&quot; naturally scales with the
number of students. However, students are not trained in grading, which means
that one cannot expect the same level of grading skills as in traditional
settings. Drawing on broad evidence that ordinal feedback is easier to provide
and more reliable than cardinal feedback, it is therefore desirable to allow
peer graders to make ordinal statements (e.g. &quot;project X is better than project
Y&quot;) and not require them to make cardinal statements (e.g. &quot;project X is a
B-&quot;). Thus, in this paper we study the problem of automatically inferring
student grades from ordinal peer feedback, as opposed to existing methods that
require cardinal peer feedback. We formulate the ordinal peer grading problem
as a type of rank aggregation problem, and explore several probabilistic models
under which to estimate student grades and grader reliability. We study the
applicability of these methods using peer grading data collected from a real
class -- with instructor and TA grades as a baseline -- and demonstrate the
efficacy of ordinal feedback techniques in comparison to existing cardinal peer
grading methods. Finally, we compare these peer-grading techniques to
traditional evaluation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3659</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3659</id><created>2014-04-10</created><authors><author><keyname>Konigsberg</keyname><forenames>Amir</forenames></author></authors><title>Avoiding Undesired Choices Using Intelligent Adaptive Systems</title><categories>cs.AI</categories><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 2, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a number of heuristics that can be used for identifying when
intransitive choice behaviour is likely to occur in choice situations. We also
suggest two methods for avoiding undesired choice behaviour, namely transparent
communication and adaptive choice-set generation. We believe that these two
ways can contribute to the avoidance of decision biases in choice situations
that may often be regretted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3660</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3660</id><created>2014-04-14</created><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Hartung</keyname><forenames>Sepp</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>Constant-factor approximations for Capacitated Arc Routing without
  triangle inequality</title><categories>cs.DS cs.DM math.CO</categories><acm-class>F.2.2; I.2.8; G.2.1; G.2.2</acm-class><journal-ref>Operations Research Letters 42(4):290--292, 2014</journal-ref><doi>10.1016/j.orl.2014.05.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph with edge costs and edge demands, the Capacitated
Arc Routing problem (CARP) asks for minimum-cost routes for equal-capacity
vehicles so as to satisfy all demands. Constant-factor polynomial-time
approximation algorithms were proposed for CARP with triangle inequality, while
CARP was claimed to be NP-hard to approximate within any constant factor in
general. Correcting this claim, we show that any factor {\alpha} approximation
for CARP with triangle inequality yields a factor {\alpha} approximation for
the general CARP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3666</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3666</id><created>2014-04-14</created><authors><author><keyname>He</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Z. Jane</forenames></author><author><keyname>Leung</keyname><forenames>Victor Leung C. M.</forenames></author></authors><title>Unitary Query for the $M \times L \times N$ MIMO Backscatter RFID
  Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A MIMO backscatter RFID system consists of three operational ends: the query
end (with $M$ reader transmitting antennas), the tag end (with $L$ tag
antennas) and the receiving end (with $N$ reader receiving antennas). Such an
$M \times L \times N$ setting in RFID can bring spatial diversity and has been
studied for STC at the tag end. Current understanding of the query end is that
it is only an energy provider for the tag and query signal designs cannot
improve the performance. However, we propose a novel \textit{unitary query}
scheme, which creates time diversity \emph{within channel coherent time} and
can yield \emph{significant} performance improvements. To overcome the
difficulty of evaluating the performance when the unitary query is employed at
the query end and STC is employed at the tag end, we derive a new measure based
on the ranks of certain carefully constructed matrices. The measure implies
that the unitary query has superior performance. Simulations show that the
unitary query can bring $5-10$ dB gain in mid SNR regimes. In addition, the
unitary query can also improve the performance of single-antenna tags
significantly, allowing employing low complex and small-size single-antenna
tags for high performance. This improvement is unachievable for single-antenna
tags when the conventional uniform query is employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3675</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3675</id><created>2014-04-14</created><updated>2014-10-10</updated><authors><author><keyname>Carbonnel</keyname><forenames>Clement</forenames></author><author><keyname>Cooper</keyname><forenames>Martin C.</forenames></author><author><keyname>Hebrard</keyname><forenames>Emmanuel</forenames></author></authors><title>On Backdoors To Tractable Constraint Languages</title><categories>cs.AI cs.CC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of CSPs, a strong backdoor is a subset of variables such that
every complete assignment yields a residual instance guaranteed to have a
specified property. If the property allows efficient solving, then a small
strong backdoor provides a reasonable decomposition of the original instance
into easy instances. An important challenge is the design of algorithms that
can find quickly a small strong backdoor if one exists. We present a systematic
study of the parameterized complexity of backdoor detection when the target
property is a restricted type of constraint language defined by means of a
family of polymorphisms. In particular, we show that under the weak assumption
that the polymorphisms are idempotent, the problem is unlikely to be FPT when
the parameter is either r (the constraint arity) or k (the size of the
backdoor) unless P = NP or FPT = W[2]. When the parameter is k+r, however, we
are able to identify large classes of languages for which the problem of
finding a small backdoor is FPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3677</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3677</id><created>2014-04-14</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Decoding Delay Controlled Reduction of Completion Time in Instantly
  Decodable Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several years, the completion time and the decoding delay problems in
Instantly Decodable Network Coding (IDNC) were considered separately and were
thought to completely act against each other. Recently, some works aimed to
balance the effects of these two important IDNC metrics but none of them
studied a further optimization of one by controlling the other. In this paper,
we study the effect of controlling the decoding delay to reduce the completion
time below its currently best known solution in persistent erasure channels. We
first derive the decoding-delay-dependent expressions of the users' and overall
completion times. Although using such expressions to find the optimal overall
completion time is NP-hard, we design two novel heuristics that minimizes the
probability of increasing the maximum of these decoding-delay-dependent
completion time expressions after each transmission through a layered control
of their decoding delays. We, then, extend our study to the limited feedback
scenario. Simulation results show that our new algorithms achieves both a lower
mean completion time and mean decoding delay compared to the best known
heuristic for completion time reduction. The gap in performance becomes
significant for harsh erasure scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3684</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3684</id><created>2014-04-14</created><authors><author><keyname>Canale</keyname><forenames>Eduardo</forenames></author><author><keyname>Romero</keyname><forenames>Pablo</forenames></author></authors><title>Diameter Constrained Reliability: Computational Complexity in terms of
  the diameter and number of terminals</title><categories>cs.CC</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a simple graph with $|V|=n$ nodes and $|E|=m$ links, a
subset $K \subseteq V$ of \emph{terminals}, a vector $p=(p_1,\ldots,p_m) \in
[0,1]^m$ and a positive integer $d$, called \emph{diameter}. We assume nodes
are perfect but links fail stochastically and independently, with probabilities
$q_i=1-p_i$. The \emph{diameter-constrained reliability} (DCR for short), is
the probability that the terminals of the resulting subgraph remain connected
by paths composed by $d$ links, or less. This number is denoted by
$R_{K,G}^{d}(p)$.
  The general DCR computation is inside the class of
$\mathcal{N}\mathcal{P}$-Hard problems, since is subsumes the complexity that a
random graph is connected. In this paper, the computational complexity of
DCR-subproblems is discussed in terms of the number of terminal nodes $k=|K|$
and diameter $d$. Either when $d=1$ or when $d=2$ and $k$ is fixed, the DCR is
inside the class $\mathcal{P}$ of polynomial-time problems. The DCR turns
$\mathcal{N}\mathcal{P}$-Hard when $k \geq 2$ is a fixed input parameter and
$d\geq 3$.
  The case where $k=n$ and $d \geq 2$ is fixed are not studied in prior
literature. Here, the $\mathcal{N}\mathcal{P}$-Hardness of this case is
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3697</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3697</id><created>2014-04-14</created><updated>2014-07-29</updated><authors><author><keyname>Sagarra</keyname><forenames>Oleguer</forenames></author><author><keyname>Font-Clos</keyname><forenames>Francesc</forenames></author><author><keyname>P&#xe9;rez-Vicente</keyname><forenames>Conrad J.</forenames></author><author><keyname>D&#xed;az-Guilera</keyname><forenames>Albert</forenames></author></authors><title>The configuration multi-edge model: Assessing the effect of fixing node
  strengths on weighted network magnitudes</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages. 4 figures</comments><journal-ref>O. Sagarra et al 2014 EPL 107 38002</journal-ref><doi>10.1209/0295-5075/107/38002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks grow subject to structural constraints which affect their
measurable properties. Assessing the effect that such constraints impose on
their observables is thus a crucial aspect to be taken into account in their
analysis. To this end,we examine the effect of fixing the strength sequence in
multi-edge networks on several network observables such as degrees, disparity,
average neighbor properties and weight distribution using an ensemble approach.
We provide a general method to calculate any desired weighted network metric
and we show that several features detected in real data could be explained
solely by structural constraints. We thus justify the need of analytical null
models to be used as basis to assess the relevance of features found in real
data represented in weighted network form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3702</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3702</id><created>2014-04-14</created><authors><author><keyname>Sabic</keyname><forenames>Deni</forenames></author><author><keyname>Strbac</keyname><forenames>Damir</forenames></author><author><keyname>Lemes</keyname><forenames>Samir</forenames></author><author><keyname>Cabaravdic</keyname><forenames>Malik</forenames></author></authors><title>Upgrade of A Robot Workstation for Positioning of Measuring Objects on
  CMM</title><categories>cs.RO</categories><comments>Proceedings of the 17th International Research/Expert Conference
  &quot;Trends in the Development of Machinery and Associated Technology&quot; TMT 2013,
  (S. Ekinovic, J. Vivancos Calvet, S. Yalcin, editors), pp 393-396, ISSN
  1840-4944, Istanbul, Turkey, 10-11 September 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to decrease the measuring cycle time on the coordinate measuring
machine (CMM) a robot workstation for the positioning of measuring objects was
created. The application of a simple 5-axis industrial robot enables the
positioning of the objects within the working space of CMM and measuring of
different surfaces on the same object without human intervention. In this
article an upgrade of an existing robot workstation through different design
measures is shown. The main goal of this upgrade is to improve the measuring
accuracy of the complex robot-CMM system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3706</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3706</id><created>2014-04-14</created><authors><author><keyname>Lemes</keyname><forenames>Samir</forenames></author><author><keyname>Strbac</keyname><forenames>Damir</forenames></author><author><keyname>Cabaravdic</keyname><forenames>Malik</forenames></author></authors><title>Using industrial robot to manipulate the measured object in CMM</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems: Industrial
  Robots, Antonio Visioli (Ed.), Vol. 10, 281: 2013, IF 0.82, ISSN 1729-8806</journal-ref><doi>10.5772/56585</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Coordinate measuring machines (CMMs) are widely used to check dimensions of
manufactured parts, especially in automotive industry. The major obstacles in
automation of these measurements are fixturing and clamping assemblies, which
are required in order to position the measured object within the CMM. This
paper describes how an industrial robot can be used to manipulate the measured
object within the CMM work space, in order to enable automation of complex
geometry measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3707</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3707</id><created>2014-04-14</created><authors><author><keyname>Lemes</keyname><forenames>Samir</forenames></author></authors><title>Information Security Management of Web Portals Based on Joomla CMS</title><categories>cs.CR</categories><comments>Proceedings of the 15th International Research/Expert Conference
  &quot;Trends in the Development of Machinery and Associated Technology&quot; TMT 2011,
  (S. Ekinovic, J. Vivancos, E. Tacer, editors), pp 509-512, ISSN 1840-4944,
  Prague, Czech Republic, 12-18 September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information is the key asset of all organizations and can exist in many
forms. It can be printed or written on paper, stored electronically,
transmitted by mail or by electronic means, shown in films, or spoken in
conversation. In today's competitive business environment, such information is
constantly under threat from many sources, which can be internal, external,
accidental, or malicious. Joomla is a very popular Content Management System
(CMS) used for web page maintenance. This highly versatile software has found
itself in both large corporate web portals, and simple web pages such as blogs.
Such popularity increases its vulnerability to potential attacks and therefore
needs an appropriate security management. ISO (the International Organization
for Standardization) and IEC (the International Electrotechnical Commission)
created the series of standards aimed at providing a model for establishing,
implementing, operating, monitoring, reviewing, maintaining and improving an
Information Security Management System (ISMS). This paper shows how principles
set in ISO/IEC 27000 series of standards can be used to improve security of
Joomla based web portals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3708</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3708</id><created>2014-04-14</created><updated>2015-04-14</updated><authors><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh</forenames></author><author><keyname>Lou</keyname><forenames>Tiancheng</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Bai</forenames></author></authors><title>Inferring Social Status and Rich Club Effects in Enterprise
  Communication Networks</title><categories>cs.SI cs.AI physics.soc-ph</categories><comments>13 pages, 4 figures</comments><journal-ref>PLoS ONE 10(3): e0119446. 2015</journal-ref><doi>10.1371/journal.pone.0119446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social status, defined as the relative rank or position that an individual
holds in a social hierarchy, is known to be among the most important motivating
forces in social behaviors. In this paper, we consider the notion of status
from the perspective of a position or title held by a person in an enterprise.
We study the intersection of social status and social networks in an
enterprise. We study whether enterprise communication logs can help reveal how
social interactions and individual status manifest themselves in social
networks. To that end, we use two enterprise datasets with three communication
channels --- voice call, short message, and email --- to demonstrate the
social-behavioral differences among individuals with different status. We have
several interesting findings and based on these findings we also develop a
model to predict social status. On the individual level, high-status
individuals are more likely to be spanned as structural holes by linking to
people in parts of the enterprise networks that are otherwise not well
connected to one another. On the community level, the principle of homophily,
social balance and clique theory generally indicate a &quot;rich club&quot; maintained by
high-status individuals, in the sense that this community is much more
connected, balanced and dense. Our model can predict social status of
individuals with 93% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3720</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3720</id><created>2014-04-12</created><authors><author><keyname>Williams</keyname><forenames>Richard</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>The substantive and practical significance of citation impact
  differences between institutions: Guidelines for the analysis of percentiles
  using effect sizes and confidence intervals</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our chapter we address the statistical analysis of percentiles: How should
the citation impact of institutions be compared? In educational and
psychological testing, percentiles are already used widely as a standard to
evaluate an individual's test scores - intelligence tests for example - by
comparing them with the percentiles of a calibrated sample. Percentiles, or
percentile rank classes, are also a very suitable method for bibliometrics to
normalize citations of publications in terms of the subject category and the
publication year and, unlike the mean-based indicators (the relative citation
rates), percentiles are scarcely affected by skewed distributions of citations.
The percentile of a certain publication provides information about the citation
impact this publication has achieved in comparison to other similar
publications in the same subject category and publication year. Analyses of
percentiles, however, have not always been presented in the most effective and
meaningful way. New APA guidelines (American Psychological Association, 2010)
suggest a lesser emphasis on significance tests and a greater emphasis on the
substantive and practical significance of findings. Drawing on work by Cumming
(2012) we show how examinations of effect sizes (e.g. Cohen's d statistic) and
confidence intervals can lead to a clear understanding of citation impact
differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3721</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3721</id><created>2014-04-14</created><updated>2014-05-09</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Wagner</keyname><forenames>Caroline</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>BRICS countries and scientific excellence: A bibliometric analysis of
  most frequently-cited papers</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The BRICS countries (Brazil, Russia, India, and China, and South Africa) are
noted for their increasing participation in science and technology. The
governments of these countries have been boosting their investments in research
and development to become part of the group of nations doing research at a
world-class level. This study investigates the development of the BRICS
countries in the domain of top-cited papers (top 10% and 1% most frequently
cited papers) between 1990 and 2010. To assess the extent to which these
countries have become important players on the top level, we compare the BRICS
countries with the top-performing countries worldwide. As the analyses of the
(annual) growth rates show, with the exception of Russia, the BRICS countries
have increased their output in terms of most frequently-cited papers at a
higher rate than the top-cited countries worldwide. In a further step of
analysis for this study, we generate co-authorship networks among authors of
highly cited papers for four time points to view changes in BRICS participation
(1995, 2000, 2005, and 2010). Here, the results show that all BRICS countries
succeeded in becoming part of this network, whereby the Chinese collaboration
activities focus on the USA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3722</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3722</id><created>2014-04-14</created><updated>2015-11-20</updated><authors><author><keyname>Haney</keyname><forenames>Samuel</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Ding</keyname><forenames>Bolin</forenames></author></authors><title>Design of Policy-Aware Differentially Private Algorithms</title><categories>cs.DB cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of designing error optimal differentially private algorithms is
well studied. Recent work applying differential privacy to real world settings
have used variants of differential privacy that appropriately modify the notion
of neighboring databases. The problem of designing error optimal algorithms for
such variants of differential privacy is open. In this paper, we show a novel
transformational equivalence result that can turn the problem of query
answering under differential privacy with a modified notion of neighbors to one
of query answering under standard differential privacy, for a large class of
neighbor definitions.
  We utilize the Blowfish privacy framework that generalizes differential
privacy. Blowfish uses a {\em policy graph} to instantiate different notions of
neighboring databases. We show that the error incurred when answering a
workload $\mathbf{W}$ on a database $\mathbf{x}$ under a Blowfish policy graph
$G$ is identical to the error required to answer a transformed workload
$f_G(\mathbf{W})$ on database $g_G(\mathbf{x})$ under standard differential
privacy, where $f_G$ and $g_G$ are linear transformations based on $G$. Using
this result, we develop error efficient algorithms for releasing histograms and
multidimensional range queries under different Blowfish policies. We believe
the tools we develop will be useful for finding mechanisms to answer many other
classes of queries with low error under other policy graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3733</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3733</id><created>2014-04-14</created><authors><author><keyname>Touchette</keyname><forenames>Dave</forenames></author></authors><title>Quantum Information Complexity and Amortized Communication</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>v1, 38 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new notion of information cost for quantum protocols, and a
corresponding notion of quantum information complexity for bipartite quantum
channels, and then investigate the properties of such quantities. These are the
fully quantum generalizations of the analogous quantities for bipartite
classical functions that have found many applications recently, in particular
for proving communication complexity lower bounds. Our definition is strongly
tied to the quantum state redistribution task.
  Previous attempts have been made to define such a quantity for quantum
protocols, with particular applications in mind; our notion differs from these
in many respects. First, it directly provides a lower bound on the quantum
communication cost, independent of the number of rounds of the underlying
protocol. Secondly, we provide an operational interpretation for quantum
information complexity: we show that it is exactly equal to the amortized
quantum communication complexity of a bipartite channel on a given state. This
generalizes a result of Braverman and Rao to quantum protocols, and even
strengthens the classical result in a bounded round scenario. Also, this
provides an analogue of the Schumacher source compression theorem for
interactive quantum protocols, and answers a question raised by Braverman.
  We also discuss some potential applications to quantum communication
complexity lower bounds by specializing our definition for classical functions
and inputs. Building on work of Jain, Radhakrishnan and Sen, we provide new
evidence suggesting that the bounded round quantum communication complexity of
the disjointness function is \Omega (n/M + M), for M-message protocols. This
would match the best known upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3747</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3747</id><created>2014-04-14</created><updated>2014-05-02</updated><authors><author><keyname>Tomita</keyname><forenames>Yu</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Low-distance Surface Codes under Realistic Quantum Noise</title><categories>quant-ph cs.ET</categories><comments>15 pages, 15 figures, 4 tables, comments welcome</comments><journal-ref>Phys.Rev. A.90, 062320 (2014)</journal-ref><doi>10.1103/PhysRevA.90.062320</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of distance-three surface code layouts under
realistic multi-parameter noise models. We first calculate their thresholds
under depolarizing noise. We then compare a Pauli-twirl approximation of
amplitude and phase damping to amplitude and phase damping. We find the
approximate channel results in a pessimistic estimate of the logical error
rate, indicating the realistic threshold may be higher than previously
estimated. From Monte-Carlo simulations, we identify experimental parameters
for which these layouts admit reliable computation. Due to its low resource
cost and superior performance, we conclude that the 17-qubit layout should be
targeted in early experimental implementations of the surface code. We find
that architectures with gate times in the 5-40 ns range and T1 times of at
least 1-2 us range will exhibit improved logical error rates with a 17-qubit
surface code encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3757</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3757</id><created>2014-04-14</created><updated>2014-10-25</updated><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Inheritance patterns in citation networks reveal scientific memes</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>8 two-column pages, 5 figures; accepted for publication in Physical
  Review X</comments><journal-ref>Phys. Rev. X 4 (2014) 041036</journal-ref><doi>10.1103/PhysRevX.4.041036</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Memes are the cultural equivalent of genes that spread across human culture
by means of imitation. What makes a meme and what distinguishes it from other
forms of information, however, is still poorly understood. Our analysis of
memes in the scientific literature reveals that they are governed by a
surprisingly simple relationship between frequency of occurrence and the degree
to which they propagate along the citation graph. We propose a simple
formalization of this pattern and we validate it with data from close to 50
million publication records from the Web of Science, PubMed Central, and the
American Physical Society. Evaluations relying on human annotators, citation
network randomizations, and comparisons with several alternative approaches
confirm that our formula is accurate and effective, without a dependence on
linguistic or ontological knowledge and without the application of arbitrary
thresholds or filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3759</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3759</id><created>2014-04-14</created><authors><author><keyname>Babych</keyname><forenames>Bogdan</forenames></author><author><keyname>Hartley</keyname><forenames>Anthony</forenames></author></authors><title>Meta-evaluation of comparability metrics using parallel corpora</title><categories>cs.CL</categories><comments>10 pages, 3 figures, 12th International Conference on Intelligent
  Text Processing and Computational Linguistics CICLing 2011. February 20 to
  26, 2011, Tokyo, Japan. International Journal of Computational Linguistics
  and Applications, Proceedings volume of CICLing-2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metrics for measuring the comparability of corpora or texts need to be
developed and evaluated systematically. Applications based on a corpus, such as
training Statistical MT systems in specialised narrow domains, require finding
a reasonable balance between the size of the corpus and its consistency, with
controlled and benchmarked levels of comparability for any newly added
sections. In this article we propose a method that can meta-evaluate
comparability metrics by calculating monolingual comparability scores
separately on the 'source' and 'target' sides of parallel corpora. The range of
scores on the source side is then correlated (using Pearson's r coefficient)
with the range of 'target' scores; the higher the correlation - the more
reliable is the metric. The intuition is that a good metric should yield the
same distance between different domains in different languages. Our method
gives consistent results for the same metrics on different data sets, which
indicates that it is reliable and can be used for metric comparison or for
optimising settings of parametrised metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3766</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3766</id><created>2014-04-14</created><updated>2014-05-22</updated><authors><author><keyname>Han</keyname><forenames>Puxiao</forenames></author><author><keyname>Niu</keyname><forenames>Ruixin</forenames></author><author><keyname>Ren</keyname><forenames>Mengqi</forenames></author></authors><title>Distributed Approximate Message Passing for Compressed Sensing</title><categories>cs.DC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an efficient distributed approach for implementing the
approximate message passing (AMP) algorithm, named distributed AMP (DAMP), is
developed for compressed sensing (CS) recovery in sensor networks with the
sparsity K unknown. In the proposed DAMP, distributed sensors do not have to
use or know the entire global sensing matrix, and the burden of computation and
storage for each sensor is reduced. To reduce communications among the sensors,
a new data query algorithm, called global computation for AMP (GCAMP), is
proposed. The proposed GCAMP based DAMP approach has exactly the same recovery
solution as the centralized AMP algorithm, which is proved theoretically in the
paper. The performance of the DAMP approach is evaluated in terms of the
communication cost saved by using GCAMP. For comparison purpose, thresholding
algorithm (TA), a well known distributed Top-K algorithm, is modified so that
it also leads to the same recovery solution as the centralized AMP. Numerical
results demonstrate that the GCAMP based DAMP outperforms the Modified TA based
DAMP, and reduces the communication cost significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3767</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3767</id><created>2014-04-14</created><authors><author><keyname>R&#xf3;th</keyname><forenames>&#xc1;goston</forenames></author></authors><title>Control point based exact description of higher dimensional
  trigonometric and hyperbolic curves and multivariate surfaces</title><categories>math.NA cs.GR</categories><comments>25 pages, 16 figures</comments><msc-class>65D17, 68U07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the normalized B-bases of vector spaces of trigonometric and hyperbolic
polynomials of finite order, we specify control point configurations for the
exact description of higher dimensional (rational) curves and (hybrid)
multivariate surfaces determined by coordinate functions that are exclusively
given either by traditional trigonometric or hyperbolic polynomials in each of
their variables. The usefulness and applicability of theoretical results and
proposed algorithms are illustrated by many examples that also comprise the
control point based exact description of several famous curves (like epi- and
hypocycloids, foliums, torus knots, Bernoulli's lemniscate, hyperbolas),
surfaces (such as pure trigonometric or hybrid surfaces of revolution like tori
and hyperboloids, respectively) and 3-dimensional volumes. The core of the
proposed modeling methods relies on basis transformation matrices with entries
that can be efficiently obtained by order elevation. Providing subdivision
formulae for curves described by convex combinations of these normalized
B-basis functions and control points, we also ensure the possible incorporation
of all proposed techniques into today's CAD systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3768</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3768</id><created>2014-04-14</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author><author><keyname>Wieder</keyname><forenames>Udi</forenames></author></authors><title>Changing Bases: Multistage Optimization for Matroids and Matchings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is motivated by the fact that many systems need to be maintained
continually while the underlying costs change over time. The challenge is to
continually maintain near-optimal solutions to the underlying optimization
problems, without creating too much churn in the solution itself. We model this
as a multistage combinatorial optimization problem where the input is a
sequence of cost functions (one for each time step); while we can change the
solution from step to step, we incur an additional cost for every such change.
We study the multistage matroid maintenance problem, where we need to maintain
a base of a matroid in each time step under the changing cost functions and
acquisition costs for adding new elements. The online version of this problem
generalizes online paging. E.g., given a graph, we need to maintain a spanning
tree $T_t$ at each step: we pay $c_t(T_t)$ for the cost of the tree at time
$t$, and also $| T_t\setminus T_{t-1} |$ for the number of edges changed at
this step. Our main result is an $O(\log m \log r)$-approximation, where $m$ is
the number of elements/edges and $r$ is the rank of the matroid. We also give
an $O(\log m)$ approximation for the offline version of the problem. These
bounds hold when the acquisition costs are non-uniform, in which caseboth these
results are the best possible unless P=NP.
  We also study the perfect matching version of the problem, where we must
maintain a perfect matching at each step under changing cost functions and
costs for adding new elements. Surprisingly, the hardness drastically
increases: for any constant $\epsilon&gt;0$, there is no
$O(n^{1-\epsilon})$-approximation to the multistage matching maintenance
problem, even in the offline case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3776</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3776</id><created>2014-04-14</created><authors><author><keyname>Bandyapadhyay</keyname><forenames>Sayan</forenames></author><author><keyname>Bhowmick</keyname><forenames>Santanu</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author></authors><title>Approximation Schemes for Partitioning: Convex Decomposition and Surface
  Approximation</title><categories>cs.CG</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit two NP-hard geometric partitioning problems - convex decomposition
and surface approximation. Building on recent developments in geometric
separators, we present quasi-polynomial time algorithms for these problems with
improved approximation guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3780</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3780</id><created>2014-04-14</created><authors><author><keyname>Mendes</keyname><forenames>Caio de Andrade</forenames></author><author><keyname>Mariano</keyname><forenames>Hugo Luiz</forenames></author></authors><title>Towards a good notion of categories of logics</title><categories>math.CT cs.LO</categories><comments>10 pages</comments><msc-class>18C99, 03B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider (fi?nitary, propositional) logics through the original use of
Category Theory: the study of the &quot;sociology of mathematical objects&quot;, aligning
us with a recent, and growing, trend of study logics through its relations with
other logics (e.g. process of combinations of logics as ?bring [Gab] and
possible translation semantics [Car]). So will be objects of study the classes
of logics, i.e. categories whose objects are logical systems (i.e., a signature
with a Tarskian consequence relation) and the morphisms are related to (some
concept of) translations between these systems. The present work provides the
fi?rst steps of a project of considering categories of logical systems
satisfying simultaneously certain natural requirements: it seems that in the
literature ([AFLM1], [AFLM2], [AFLM3], [BC], [BCC1], [BCC2], [CG], [FC]) this
is achieved only partially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3782</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3782</id><created>2014-04-14</created><updated>2014-04-23</updated><authors><author><keyname>de Ara&#xfa;jo</keyname><forenames>Anderson</forenames></author></authors><title>A quantitative approach to semantic informativity</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article shows a form of measuring semantic informativity of deductions.
Dynamic concepts of complexity and relevancy are presented according to
explicit definitions of insertion and deletion on databases. Hence, with
respect to finite databases, it solves Bar-Hillel-Carnap paradox and Hintikka'
scandal of deduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3785</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3785</id><created>2014-04-14</created><authors><author><keyname>Coleman</keyname><forenames>David</forenames></author><author><keyname>Sucan</keyname><forenames>Ioan</forenames></author><author><keyname>Chitta</keyname><forenames>Sachin</forenames></author><author><keyname>Correll</keyname><forenames>Nikolaus</forenames></author></authors><title>Reducing the Barrier to Entry of Complex Robotic Software: a MoveIt!
  Case Study</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing robot agnostic software frameworks involves synthesizing the
disparate fields of robotic theory and software engineering while
simultaneously accounting for a large variability in hardware designs and
control paradigms. As the capabilities of robotic software frameworks increase,
the setup difficulty and learning curve for new users also increase. If the
entry barriers for configuring and using the software on robots is too high,
even the most powerful of frameworks are useless. A growing need exists in
robotic software engineering to aid users in getting started with, and
customizing, the software framework as necessary for particular robotic
applications. In this paper a case study is presented for the best practices
found for lowering the barrier of entry in the MoveIt! framework, an
open-source tool for mobile manipulation in ROS, that allows users to 1)
quickly get basic motion planning functionality with minimal initial setup, 2)
automate its configuration and optimization, and 3) easily customize its
components. A graphical interface that assists the user in configuring MoveIt!
is the cornerstone of our approach, coupled with the use of an existing
standardized robot model for input, automatically generated robot-specific
configuration files, and a plugin-based architecture for extensibility. These
best practices are summarized into a set of barrier to entry design principles
applicable to other robotic software. The approaches for lowering the entry
barrier are evaluated by usage statistics, a user survey, and compared against
our design objectives for their effectiveness to users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3788</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3788</id><created>2014-04-14</created><authors><author><keyname>Zhang</keyname><forenames>Changhun</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author></authors><title>Data Modeling with Large Random Matrices in a Cognitive Radio Network
  Testbed: Initial Experimental Demonstrations with 70 Nodes</title><categories>cs.IT math.IT</categories><comments>4 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper reports some initial experimental demonstrations of the
theoretical framework: the massive amount of data in the large-scale cognitive
radio network can be naturally modeled as (large) random matrices. In
particular, using experimental data we will demonstrate that the empirical
spectral distribution of the large sample covariance matrix---a Hermitian
random matrix---agree with its theoretical distribution (Marchenko-Pastur law).
On the other hand, the eigenvalues of the large data matrix ---a non-Hermitian
random matrix---are experimentally found to follow the single ring law, a
theoretical result that has been discovered relatively recently. To our best
knowledge, our paper is the first such attempt, in the context of large-scale
wireless network, to compare theoretical predictions with experimental
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3789</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3789</id><created>2014-04-14</created><updated>2014-12-23</updated><authors><author><keyname>Barcelo</keyname><forenames>Helene</forenames></author><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>Group size effect on cooperation in social dilemmas</title><categories>cs.GT q-bio.PE</categories><comments>Forthcoming in Scientific Reports</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social dilemmas are central to human society. Depletion of natural resources,
climate protection, security of energy supply, and workplace collaborations are
all examples of social dilemmas. Since cooperative behaviour in a social
dilemma is individually costly, Nash equilibrium predicts that humans should
not cooperate. Yet experimental studies show that people do cooperate even in
anonymous one-shot interactions. In spite of the large number of participants
in many modern social dilemmas, little is known about the effect of group size
on cooperation. Does larger group size favour or prevent cooperation? We
address this problem both experimentally and theoretically. Experimentally, we
find that there is no general answer: it depends on the strategic situation.
Specifically, we find that larger groups are more cooperative in the Public
Goods game, but less cooperative in the N-person Prisoner's dilemma.
Theoretically, we show that this behaviour is not consistent with either the
Fehr &amp; Schmidt model or (a one-parameter version of) the Charness &amp; Rabin
model, but it is consistent with the cooperative equilibrium model introduced
by the second author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3801</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3801</id><created>2014-04-14</created><updated>2014-05-26</updated><authors><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Nishimura</keyname><forenames>Naomi</forenames></author><author><keyname>Pathak</keyname><forenames>Vinayak</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author></authors><title>Shortest reconfiguration paths in the solution space of Boolean formulas</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a Boolean formula and a satisfying assignment, a flip is an operation
that changes the value of a variable in the assignment so that the resulting
assignment remains satisfying. We study the problem of computing the shortest
sequence of flips (if one exists) that transforms a given satisfying assignment
$s$ to another satisfying assignment $t$ of a Boolean formula. Earlier work
characterized the complexity of finding any (not necessarily the shortest)
sequence of flips from one satisfying assignment to another using Schaefer's
framework for classification of Boolean formulas. We build on it to provide a
trichotomy for the complexity of finding the shortest sequence of flips and
show that it is either in P, NP-complete, or PSPACE-complete.
  Our result adds to the small set of complexity results known for shortest
reconfiguration sequence problems by providing an example where the shortest
sequence can be found in polynomial time even though its length is not equal to
the symmetric difference of the values of the variables in $s$ and $t$. This is
in contrast to all reconfiguration problems studied so far, where polynomial
time algorithms for computing the shortest path were known only for cases where
the path modified the symmetric difference only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3808</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3808</id><created>2014-04-14</created><authors><author><keyname>Rehman</keyname><forenames>Obaid Ur</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Robust Dynamic State Feedback Guaranteed Cost Control of Nonlinear
  Systems using Copies of Plant Nonlinearities</title><categories>cs.SY</categories><comments>4 pages, technical note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a systematic approach to the design of a robust dynamic
state feedback controller using copies of the plant nonlinearities, which is
based on the use of IQCs and minimax LQR control. The approach combines a
linear state feedback guaranteed cost controller and copies of the plant
nonlinearities to form a robust nonlinear controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3811</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3811</id><created>2014-04-14</created><authors><author><keyname>Voroninski</keyname><forenames>Vladislav</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>A strong restricted isometry property, with an application to phaseless
  compressed sensing</title><categories>cs.IT math.IT math.NA</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The many variants of the restricted isometry property (RIP) have proven to be
crucial theoretical tools in the fields of compressed sensing and matrix
completion. The study of extending compressed sensing to accommodate phaseless
measurements naturally motivates a strong notion of restricted isometry
property (SRIP), which we develop in this paper. We show that if $A \in
\mathbb{R}^{m\times n}$ satisfies SRIP and phaseless measurements $|Ax_0| = b$
are observed about a $k$-sparse signal $x_0 \in \mathbb{R}^n$, then minimizing
the $\ell_1$ norm subject to $ |Ax| = b $ recovers $x_0$ up to multiplication
by a global sign. Moreover, we establish that the SRIP holds for the random
Gaussian matrices typically used for standard compressed sensing, implying that
phaseless compressed sensing is possible from $O(k \log (n/k))$ measurements
with these matrices via $\ell_1$ minimization over $|Ax| = b$. Our analysis
also yields an erasure robust version of the Johnson-Lindenstrauss Lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3816</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3816</id><created>2014-04-15</created><authors><author><keyname>Li</keyname><forenames>Judith Y.</forenames></author><author><keyname>Ambikasaran</keyname><forenames>Sivaram</forenames></author><author><keyname>Darve</keyname><forenames>Eric F.</forenames></author><author><keyname>Kitanidis</keyname><forenames>Peter K.</forenames></author></authors><title>A Kalman filter powered by $\mathcal{H}^2$-matrices for quasi-continuous
  data assimilation problems</title><categories>math.NA cs.NA stat.CO</categories><comments>18 pages, 7 figures. Water Resources Research, 2014</comments><acm-class>I.4.4; I.4.5; I.4.10; G.1.3; I.1.2</acm-class><doi>10.1002/2013WR014607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuously tracking the movement of a fluid or a plume in the subsurface is
a challenge that is often encountered in applications, such as tracking a plume
of injected CO$_2$ or of a hazardous substance. Advances in monitoring
techniques have made it possible to collect measurements at a high frequency
while the plume moves, which has the potential advantage of providing
continuous high-resolution images of fluid flow with the aid of data
processing. However, the applicability of this approach is limited by the high
computational cost associated with having to analyze large data sets within the
time constraints imposed by real-time monitoring. Existing data assimilation
methods have computational requirements that increase super-linearly with the
size of the unknowns $m$. In this paper, we present the HiKF, a new Kalman
filter (KF) variant powered by the hierarchical matrix approach that
dramatically reduces the computational and storage cost of the standard KF from
$\mathcal{O}(m^2)$ to $\mathcal{O}(m)$, while producing practically the same
results. The version of HiKF that is presented here takes advantage of the
so-called random walk dynamical model, which is tailored to a class of data
assimilation problems in which measurements are collected quasi-continuously.
The proposed method has been applied to a realistic CO$_2$ injection model and
compared with the ensemble Kalman filter (EnKF). Numerical results show that
HiKF can provide estimates that are more accurate than EnKF, and also
demonstrate the usefulness of modeling the system dynamics as a random walk in
this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3820</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3820</id><created>2014-04-15</created><authors><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author></authors><title>Circuit complexity, proof complexity, and polynomial identity testing</title><categories>cs.CC cs.LO math.LO</categories><acm-class>F.2.2; F.4.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new algebraic proof system, which has tight connections to
(algebraic) circuit complexity. In particular, we show that any
super-polynomial lower bound on any Boolean tautology in our proof system
implies that the permanent does not have polynomial-size algebraic circuits
(VNP is not equal to VP). As a corollary to the proof, we also show that
super-polynomial lower bounds on the number of lines in Polynomial Calculus
proofs (as opposed to the usual measure of number of monomials) imply the
Permanent versus Determinant Conjecture. Note that, prior to our work, there
was no proof system for which lower bounds on an arbitrary tautology implied
any computational lower bound.
  Our proof system helps clarify the relationships between previous algebraic
proof systems, and begins to shed light on why proof complexity lower bounds
for various proof systems have been so much harder than lower bounds on the
corresponding circuit classes. In doing so, we highlight the importance of
polynomial identity testing (PIT) for understanding proof complexity.
  More specifically, we introduce certain propositional axioms satisfied by any
Boolean circuit computing PIT. We use these PIT axioms to shed light on
AC^0[p]-Frege lower bounds, which have been open for nearly 30 years, with no
satisfactory explanation as to their apparent difficulty. We show that either:
a) Proving super-polynomial lower bounds on AC^0[p]-Frege implies VNP does not
have polynomial-size circuits of depth d - a notoriously open question for d at
least 4 - thus explaining the difficulty of lower bounds on AC^0[p]-Frege, or
b) AC^0[p]-Frege cannot efficiently prove the depth d PIT axioms, and hence we
have a lower bound on AC^0[p]-Frege.
  Using the algebraic structure of our proof system, we propose a novel way to
extend techniques from algebraic circuit complexity to prove lower bounds in
proof complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3828</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3828</id><created>2014-04-15</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>He</keyname><forenames>Di</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Tao</keyname><forenames>Yixin</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author></authors><title>Generalized Second Price Auction with Probabilistic Broad Match</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized Second Price (GSP) auctions are widely used by search engines
today to sell their ad slots. Most search engines have supported broad match
between queries and bid keywords when executing GSP auctions, however, it has
been revealed that GSP auction with the standard broad-match mechanism they are
currently using (denoted as SBM-GSP) has several theoretical drawbacks (e.g.,
its theoretical properties are known only for the single-slot case and
full-information setting, and even in this simple setting, the corresponding
worst-case social welfare can be rather bad). To address this issue, we propose
a novel broad-match mechanism, which we call the Probabilistic Broad-Match
(PBM) mechanism. Different from SBM that puts together the ads bidding on all
the keywords matched to a given query for the GSP auction, the GSP with PBM
(denoted as PBM-GSP) randomly samples a keyword according to a predefined
probability distribution and only runs the GSP auction for the ads bidding on
this sampled keyword. We perform a comprehensive study on the theoretical
properties of the PBM-GSP. Specifically, we study its social welfare in the
worst equilibrium, in both full-information and Bayesian settings. The results
show that PBM-GSP can generate larger welfare than SBM-GSP under mild
conditions. Furthermore, we also study the revenue guarantee for PBM-GSP in
Bayesian setting. To the best of our knowledge, this is the first work on
broad-match mechanisms for GSP that goes beyond the single-slot case and the
full-information setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3839</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3839</id><created>2014-04-15</created><updated>2014-08-21</updated><authors><author><keyname>Hosseinmardi</keyname><forenames>Homa</forenames></author><author><keyname>Ghasemianlangroodi</keyname><forenames>Amir</forenames></author><author><keyname>Han</keyname><forenames>Richard</forenames></author><author><keyname>Lv</keyname><forenames>Qin</forenames></author><author><keyname>Mishra</keyname><forenames>Shivakant</forenames></author></authors><title>Towards Understanding Cyberbullying Behavior in a Semi-Anonymous Social
  Network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyberbullying has emerged as an important and growing social problem, wherein
people use online social networks and mobile phones to bully victims with
offensive text, images, audio and video on a 247 basis. This paper studies
negative user behavior in the Ask.fm social network, a popular new site that
has led to many cases of cyberbullying, some leading to suicidal behavior.We
examine the occurrence of negative words in Ask.fms question+answer profiles
along with the social network of likes of questions+answers. We also examine
properties of users with cutting behavior in this social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3840</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3840</id><created>2014-04-15</created><updated>2014-12-19</updated><authors><author><keyname>Lu</keyname><forenames>Chaochao</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Surpassing Human-Level Face Verification Performance on LFW with
  GaussianFace</title><categories>cs.CV cs.LG stat.ML</categories><comments>Appearing in Proceedings of the 29th AAAI Conference on Artificial
  Intelligence (AAAI-15), Oral Presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face verification remains a challenging problem in very complex conditions
with large variations such as pose, illumination, expression, and occlusions.
This problem is exacerbated when we rely unrealistically on a single training
data source, which is often insufficient to cover the intrinsically complex
face variations. This paper proposes a principled multi-task learning approach
based on Discriminative Gaussian Process Latent Variable Model, named
GaussianFace, to enrich the diversity of training data. In comparison to
existing methods, our model exploits additional data from multiple
source-domains to improve the generalization performance of face verification
in an unknown target-domain. Importantly, our model can adapt automatically to
complex data distributions, and therefore can well capture complex face
variations inherent in multiple sources. Extensive experiments demonstrate the
effectiveness of the proposed model in learning from diverse data sources and
generalize to unseen domain. Specifically, the accuracy of our algorithm
achieves an impressive accuracy rate of 98.52% on the well-known and
challenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the
human-level performance in face verification (97.53%) on LFW is surpassed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3861</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3861</id><created>2014-04-15</created><updated>2015-02-01</updated><authors><author><keyname>Martella</keyname><forenames>Claudio</forenames></author><author><keyname>Logothetis</keyname><forenames>Dionysios</forenames></author><author><keyname>Loukas</keyname><forenames>Andreas</forenames></author><author><keyname>Siganos</keyname><forenames>Georgos</forenames></author></authors><title>Spinner: Scalable Graph Partitioning in the Cloud</title><categories>cs.DC</categories><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several organizations, like social networks, store and routinely analyze
large graphs as part of their daily operation. Such graphs are typically
distributed across multiple servers, and graph partitioning is critical for
efficient graph management. Existing partitioning algorithms focus on finding
graph partitions with good locality, but disregard the pragmatic challenges of
integrating partitioning into large-scale graph management systems deployed on
the cloud, such as dealing with the scale and dynamicity of the graph and the
compute environment.
  In this paper, we propose Spinner, a scalable and adaptive graph partitioning
algorithm based on label propagation designed on top of the Pregel model.
Spinner scales to massive graphs, produces partitions with locality and balance
comparable to the state-of-the-art and efficiently adapts the partitioning upon
changes. We describe our algorithm and its implementation in the Pregel
programming model that makes it possible to partition billion-vertex graphs. We
evaluate Spinner with a variety of synthetic and real graphs and show that it
can compute partitions with quality comparable to the state-of-the art. In
fact, by using Spinner in conjunction with the Giraph graph processing engine,
we speed up different applications by a factor of 2 relative to standard hash
partitioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3862</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3862</id><created>2014-04-15</created><updated>2014-11-22</updated><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Glassner</keyname><forenames>Yonatan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Optimizing the CVaR via Sampling</title><categories>stat.ML cs.AI cs.LG</categories><comments>To appear in AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Value at Risk (CVaR) is a prominent risk measure that is being
used extensively in various domains. We develop a new formula for the gradient
of the CVaR in the form of a conditional expectation. Based on this formula, we
propose a novel sampling-based estimator for the CVaR gradient, in the spirit
of the likelihood-ratio method. We analyze the bias of the estimator, and prove
the convergence of a corresponding stochastic gradient descent algorithm to a
local CVaR optimum. Our method allows to consider CVaR optimization in new
domains. As an example, we consider a reinforcement learning application, and
learn a risk-sensitive controller for the game of Tetris.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3867</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3867</id><created>2014-04-15</created><authors><author><keyname>Murase</keyname><forenames>Yohsuke</forenames></author><author><keyname>Uchitane</keyname><forenames>Takeshi</forenames></author><author><keyname>Ito</keyname><forenames>Nobuyasu</forenames></author></authors><title>A tool for parameter-space explorations</title><categories>physics.comp-ph cs.DC</categories><comments>4 pages, 5 figures, CSP 2014 conference</comments><journal-ref>Physics Procedia 57C pp. 73-76 (2014)</journal-ref><doi>10.1016/j.phpro.2014.08.134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A software for managing simulation jobs and results, named &quot;OACIS&quot;, is
presented. It controls a large number of simulation jobs executed in various
remote servers, keeps these results in an organized way, and manages the
analyses on these results. The software has a web browser front end, and users
can submit various jobs to appropriate remote hosts from a web browser easily.
After these jobs are finished, all the result files are automatically
downloaded from the computational hosts and stored in a traceable way together
with the logs of the date, host, and elapsed time of the jobs. Some
visualization functions are also provided so that users can easily grasp the
overview of the results distributed in a high-dimensional parameter space.
Thus, OACIS is especially beneficial for the complex simulation models having
many parameters for which a lot of parameter searches are required. By using
API of OACIS, it is easy to write a code that automates parameter selection
depending on the previous simulation results. A few examples of the automated
parameter selection are also demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3875</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3875</id><created>2014-04-15</created><updated>2014-04-28</updated><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>Boltzmann samplers for random generation of lambda terms</title><categories>cs.DS cs.LO cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomly generating structured objects is important in testing and optimizing
functional programs, whereas generating random $'l$-terms is more specifically
needed for testing and optimizing compilers. For that a tool called QuickCheck
has been proposed, but in this tool the control of the random generation is
left to the programmer. Ten years ago, a method called Boltzmann samplers has
been proposed to generate combinatorial structures. In this paper, we show how
Boltzmann samplers can be developed to generate lambda-terms, but also other
data structures like trees. These samplers rely on a critical value which
parameters the main random selector and which is exhibited here with
explanations on how it is computed. Haskell programs are proposed to show how
samplers are actually implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3877</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3877</id><created>2014-04-15</created><authors><author><keyname>Pal</keyname><forenames>Chandrajit</forenames></author><author><keyname>Kotal</keyname><forenames>Avik</forenames></author><author><keyname>Samanta</keyname><forenames>Asit</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>Design space exploration for image processing architectures on FPGA
  targets</title><categories>cs.AR</categories><comments>Proc.International Doctoral Symposium on Applied Computation and
  Security Systems(ACSS) pp. 1-19, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the emergence of embedded applications in image and video processing,
communication and cryptography, improvement of pictorial information for better
human perception like deblurring, denoising in several fields such as satellite
imaging, medical imaging, mobile applications etc. are gaining importance for
renewed research. Behind such developments, the primary responsibility lies
with the advancement of semiconductor technology leading to FPGA based
programmable logic devices, which combines the advantages of both custom
hardware and dedicated DSP resources. In addition, FPGA provides powerful
reconfiguration feature and hence is an ideal target for rapid prototyping. We
have endeavoured to exploit exceptional features of FPGA technology in respect
to hardware parallelism leading to higher computational density and throughput,
and have observed better performances than those one can get just merely
porting the image processing software algorithms to hardware. In this paper, we
intend to present an elaborate review, based on our expertise and experiences,
on undertaking necessary transformation to an image processing software
algorithm including the optimization techniques that makes its operation in
hardware comparatively faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3881</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3881</id><created>2014-04-15</created><authors><author><keyname>Ramezani</keyname><forenames>Hamid</forenames></author><author><keyname>Fazel</keyname><forenames>Fatemeh</forenames></author><author><keyname>Stojanovic</keyname><forenames>Milica</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Collision Tolerant Packet Scheduling for Underwater Acoustic
  Localization</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the joint problem of packet scheduling and
self-localization in an underwater acoustic sensor network where sensor nodes
are distributed randomly in an operating area. In terms of packet scheduling,
our goal is to minimize the localization time, and to do so we consider two
packet transmission schemes, namely a collision-free scheme (CFS), and a
collision-tolerant scheme (CTS). The required localization time is formulated
for these schemes, and through analytical results and numerical examples their
performances are shown to be generally comparable. However, when the packet
duration is short (as is the case for a localization packet), and the operating
area is large (above 3km in at least one dimension), the collision-tolerant
scheme requires a smaller localization time than the collision-free scheme.
After gathering enough measurements, an iterative Gauss-Newton algorithm is
employed by each sensor node for self-localization, and the Cramer Rao lower
bound is evaluated as a benchmark. Although CTS consumes more energy for packet
transmission, it provides a better localization accuracy. Additionally, in this
scheme the anchor nodes work independently of each other, and can operate
asynchronously which leads to a simplified implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3882</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3882</id><created>2014-04-15</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Liedloff</keyname><forenames>Mathieu</forenames></author><author><keyname>Montealegre</keyname><forenames>Pedro</forenames></author><author><keyname>Todinca</keyname><forenames>Ioan</forenames></author></authors><title>Algorithms parameterized by vertex cover and modular width, through
  potential maximal cliques</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give upper bounds on the number of minimal separators and
potential maximal cliques of graphs w.r.t. two graph parameters, namely vertex
cover ($\operatorname{vc}$) and modular width ($\operatorname{mw}$). We prove
that for any graph, the number of minimal separators is
$\mathcal{O}^*(3^{\operatorname{vc}})$ and
$\mathcal{O}^*(1.6181^{\operatorname{mw}})$, and the number of potential
maximal cliques is $\mathcal{O}^*(4^{\operatorname{vc}})$ and
$\mathcal{O}^*(1.7347^{\operatorname{mw}})$, and these objects can be listed
within the same running times. (The $\mathcal{O}^*$ notation suppresses
polynomial factors in the size of the input.) Combined with known results, we
deduce that a large family of problems, e.g., Treewidth, Minimum Fill-in,
Longest Induced Path, Feedback vertex set and many others, can be solved in
time $\mathcal{O}^*(4^{\operatorname{vc}})$ or
$\mathcal{O}^*(1.7347^{\operatorname{mw}})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3884</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3884</id><created>2014-04-15</created><authors><author><keyname>Xiang</keyname><forenames>Chengdi</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author></authors><title>Performance Analysis and Coherent Guaranteed Cost Control for Uncertain
  Quantum Systems</title><categories>quant-ph cs.SY</categories><comments>8 pages, 1 figure, a shortened version is to appear in the
  proceedings of the 2014 European Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents several results on performance analysis for a class of
uncertain linear quantum systems subject to either quadratic or non-quadratic
perturbations in the system Hamiltonian. Also, coherent guaranteed cost
controllers are designed for the uncertain quantum systems to achieve improved
control performance. The coherent controller is realized by adding a control
Hamiltonian to the quantum system and its performance is demonstrated by an
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3891</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3891</id><created>2014-04-15</created><updated>2015-07-02</updated><authors><author><keyname>Qin</keyname><forenames>Yang</forenames></author><author><keyname>Zhong</keyname><forenames>Xiaoxiong</forenames></author><author><keyname>Yang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Li</keyname><forenames>Yanlin</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author></authors><title>Joint Channel Assignment and Opportunistic Routing for Maximizing
  Throughput in Cognitive Radio Networks</title><categories>cs.NI cs.PF</categories><comments>5 pages, 4 figures, to appear in Proc. of IEEE GlobeCom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the joint opportunistic routing and channel
assignment problem in multi-channel multi-radio (MCMR) cognitive radio networks
(CRNs) for improving aggregate throughput of the secondary users. We first
present the nonlinear programming optimization model for this joint problem,
taking into account the feature of CRNs-channel uncertainty. Then considering
the queue state of a node, we propose a new scheme to select proper forwarding
candidates for opportunistic routing. Furthermore, a new algorithm for
calculating the forwarding probability of any packet at a node is proposed,
which is used to calculate how many packets a forwarder should send, so that
the duplicate transmission can be reduced compared with MAC-independent
opportunistic routing &amp; encoding (MORE) [11]. Our numerical results show that
the proposed scheme performs significantly better that traditional routing and
opportunistic routing in which channel assignment strategy is employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3899</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3899</id><created>2014-04-15</created><updated>2014-06-16</updated><authors><author><keyname>Ramsdell</keyname><forenames>John D.</forenames></author><author><keyname>Dougherty</keyname><forenames>Daniel J.</forenames></author><author><keyname>Guttman</keyname><forenames>Joshua D.</forenames></author><author><keyname>Rowe</keyname><forenames>Paul D.</forenames></author></authors><title>A Hybrid Analysis for Security Protocols with State</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptographic protocols rely on message-passing to coordinate activity among
principals. Each principal maintains local state in individual local sessions
only as needed to complete that session. However, in some protocols a principal
also uses state to coordinate its different local sessions. Sometimes the
non-local, mutable state is used as a means, for example with smart cards or
Trusted Platform Modules. Sometimes it is the purpose of running the protocol,
for example in commercial transactions.
  Many richly developed tools and techniques, based on well-understood
foundations, are available for design and analysis of pure message-passing
protocols. But the presence of cross-session state poses difficulties for these
techniques.
  In this paper we provide a framework for modeling stateful protocols. We
define a hybrid analysis method. It leverages theorem-proving---in this
instance, the PVS prover---for reasoning about computations over state. It
combines that with an &quot;enrich-by-need&quot; approach---embodied by CPSA---that
focuses on the message-passing part. As a case study we give a full analysis of
the Envelope Protocol, due to Mark Ryan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3905</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3905</id><created>2014-04-15</created><updated>2014-11-03</updated><authors><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Schneider</keyname><forenames>Reinhold</forenames></author><author><keyname>Stojanac</keyname><forenames>Zeljka</forenames></author></authors><title>Tensor completion in hierarchical tensor representations</title><categories>math.NA cs.IT math.IT</categories><comments>revised version, to be published in Compressed Sensing and Its
  Applications (edited by H. Boche, R. Calderbank, G. Kutyniok, J. Vybiral)</comments><msc-class>15A69, 65F99, 94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing extends from the recovery of sparse vectors from
undersampled measurements via efficient algorithms to the recovery of matrices
of low rank from incomplete information. Here we consider a further extension
to the reconstruction of tensors of low multi-linear rank in recently
introduced hierarchical tensor formats from a small number of measurements.
Hierarchical tensors are a flexible generalization of the well-known Tucker
representation, which have the advantage that the number of degrees of freedom
of a low rank tensor does not scale exponentially with the order of the tensor.
While corresponding tensor decompositions can be computed efficiently via
successive applications of (matrix) singular value decompositions, some
important properties of the singular value decomposition do not extend from the
matrix to the tensor case. This results in major computational and theoretical
difficulties in designing and analyzing algorithms for low rank tensor
recovery. For instance, a canonical analogue of the tensor nuclear norm is
NP-hard to compute in general, which is in stark contrast to the matrix case.
In this book chapter we consider versions of iterative hard thresholding
schemes adapted to hierarchical tensor formats. A variant builds on methods
from Riemannian optimization and uses a retraction mapping from the tangent
space of the manifold of low rank tensors back to this manifold. We provide
first partial convergence results based on a tensor version of the restricted
isometry property (TRIP) of the measurement map. Moreover, an estimate of the
number of measurements is provided that ensures the TRIP of a given tensor rank
with high probability for Gaussian measurement maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3913</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3913</id><created>2014-04-15</created><authors><author><keyname>Beaumont</keyname><forenames>Olivier</forenames></author><author><keyname>Marchal</keyname><forenames>Loris</forenames></author></authors><title>Analysis of Dynamic Scheduling Strategies for Matrix Multiplication on
  Heterogeneous Platforms</title><categories>cs.DC</categories><comments>Accepted for publication in HPDC 2014</comments><acm-class>F.2.0</acm-class><doi>10.1145/2600212.2600223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tremendous increase in the size and heterogeneity of supercomputers makes
it very difficult to predict the performance of a scheduling algorithm.
Therefore, dynamic solutions, where scheduling decisions are made at runtime
have overpassed static allocation strategies. The simplicity and efficiency of
dynamic schedulers such as Hadoop are a key of the success of the MapReduce
framework. Dynamic schedulers such as StarPU, PaRSEC or StarSs are also
developed for more constrained computations, e.g. task graphs coming from
linear algebra. To make their decisions, these runtime systems make use of some
static information, such as the distance of tasks to the critical path or the
affinity between tasks and computing resources (CPU, GPU,...) and of dynamic
information, such as where input data are actually located. In this paper, we
concentrate on two elementary linear algebra kernels, namely the outer product
and the matrix multiplication. For each problem, we propose several dynamic
strategies that can be used at runtime and we provide an analytic study of
their theoretical performance. We prove that the theoretical analysis provides
very good estimate of the amount of communications induced by a dynamic
strategy and can be used in order to efficiently determine thresholds used in
dynamic scheduler, thus enabling to choose among them for a given problem and
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3918</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3918</id><created>2014-04-15</created><authors><author><keyname>Vu</keyname><forenames>Van</forenames></author></authors><title>A simple SVD algorithm for finding hidden partitions</title><categories>math.CO cs.DS</categories><msc-class>68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a hidden partition in a random environment is a general and important
problem, which contains as subproblems many famous questions, such as finding a
hidden clique, finding a hidden coloring, finding a hidden bipartition etc.
  In this paper, we provide a simple SVD algorithm for this purpose, answering
a question of McSherry. This algorithm is very easy to implement and works for
sparse graphs with optimal density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3919</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3919</id><created>2014-04-15</created><updated>2015-02-04</updated><authors><author><keyname>Bernasconi</keyname><forenames>Anna</forenames></author><author><keyname>Ciriani</keyname><forenames>Valentina</forenames></author><author><keyname>Lago</keyname><forenames>Lorenzo</forenames></author></authors><title>On the Error Resilience of Ordered Binary Decision Diagrams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordered Binary Decision Diagrams (OBDDs) are a data structure that is used in
an increasing number of fields of Computer Science (e.g., logic synthesis,
program verification, data mining, bioinformatics, and data protection) for
representing and manipulating discrete structures and Boolean functions. The
purpose of this paper is to study the error resilience of OBDDs and to design a
resilient version of this data structure, i.e., a self-repairing OBDD. In
particular, we describe some strategies that make reduced ordered OBDDs
resilient to errors in the indexes, that are associated to the input variables,
or in the pointers (i.e., OBDD edges) of the nodes. These strategies exploit
the inherent redundancy of the data structure, as well as the redundancy
introduced by its efficient implementations. The solutions we propose allow the
exact restoring of the original OBDD and are suitable to be applied to
classical software packages for the manipulation of OBDDs currently in use.
Another result of the paper is the definition of a new canonical OBDD model,
called {\em Index-resilient Reduced OBDD}, which guarantees that a node with a
faulty index has a reconstruction cost $O(k)$, where $k$ is the number of nodes
with corrupted index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3920</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3920</id><created>2014-04-14</created><authors><author><keyname>Jonker</keyname><forenames>Catholijn</forenames></author><author><keyname>Broekens</keyname><forenames>Joost</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author></authors><title>Virtual Reflexes</title><categories>cs.CY cs.HC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Virtual Reality is used successfully to treat people for regular phobias. A
new challenge is to develop Virtual Reality Exposure Training for social
skills. Virtual actors in such systems have to show appropriate social behavior
including emotions, gaze, and keeping distance. The behavior must be realistic
and real-time. Current approaches consist of four steps: 1) trainee social
signal detection, 2) cognitive-affective interpretation, 3) determination of
the appropriate bodily responses, and 4) actuation. The &quot;cognitive&quot; detour of
such approaches does not match the directness of human bodily reflexes and
causes unrealistic responses and delay. Instead, we propose virtual reflexes as
concurrent sensory-motor processes to control virtual actors. Here we present a
virtual reflexes architecture, explain how emotion and cognitive modulation are
embedded, detail its workings, and give an example description of an aggression
training application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3925</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3925</id><created>2014-04-13</created><updated>2014-12-29</updated><authors><author><keyname>Delpeuch</keyname><forenames>Antonin</forenames><affiliation>&#xc9;cole Normale Sup&#xe9;rieure, Paris</affiliation></author></authors><title>Complexity of Grammar Induction for Quantum Types</title><categories>cs.CL math.CT</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 236-248</journal-ref><doi>10.4204/EPTCS.172.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most categorical models of meaning use a functor from the syntactic category
to the semantic category. When semantic information is available, the problem
of grammar induction can therefore be defined as finding preimages of the
semantic types under this forgetful functor, lifting the information flow from
the semantic level to a valid reduction at the syntactic level. We study the
complexity of grammar induction, and show that for a variety of type systems,
including pivotal and compact closed categories, the grammar induction problem
is NP-complete. Our approach could be extended to linguistic type systems such
as autonomous or bi-closed categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3933</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3933</id><created>2014-04-15</created><authors><author><keyname>Lee</keyname><forenames>Philip G.</forenames></author><author><keyname>Wu</keyname><forenames>Ying</forenames></author></authors><title>Scalable Matting: A Sub-linear Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural image matting, which separates foreground from background, is a very
important intermediate step in recent computer vision algorithms. However, it
is severely underconstrained and difficult to solve. State-of-the-art
approaches include matting by graph Laplacian, which significantly improves the
underconstrained nature by reducing the solution space. However, matting by
graph Laplacian is still very difficult to solve and gets much harder as the
image size grows: current iterative methods slow down as $\mathcal{O}\left(n^2
\right)$ in the resolution $n$. This creates uncomfortable practical limits on
the resolution of images that we can matte. Current literature mitigates the
problem, but they all remain super-linear in complexity. We expose properties
of the problem that remain heretofore unexploited, demonstrating that an
optimization technique originally intended to solve PDEs can be adapted to take
advantage of this knowledge to solve the matting problem, not heuristically,
but exactly and with sub-linear complexity. This makes ours the most efficient
matting solver currently known by a very wide margin and allows matting finally
to be practical and scalable in the future as consumer photos exceed many
dozens of megapixels, and also relieves matting from being a bottleneck for
vision algorithms that depend on it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3945</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3945</id><created>2014-04-14</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>A Game Theoretic Approach to Minimize the Completion Time of Network
  Coded Cooperative Data Exchange</title><categories>cs.IT cs.GT math.IT</categories><comments>conference version of arXiv:1404.3637</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a game theoretic framework for studying the
problem of minimizing the completion time of instantly decodable network coding
(IDNC) for cooperative data exchange (CDE) in decentralized wireless network.
In this configuration, clients cooperate with each other to recover the erased
packets without a central controller. Game theory is employed herein as a tool
for improving the distributed solution by overcoming the need for a central
controller or additional signaling in the system. We model the session by
self-interested players in a non-cooperative potential game. The utility
function is designed such that increasing individual payoff results in a
collective behavior achieving both a desirable system performance in a shared
network environment and the Pareto optimal solution. Through extensive
simulations, our approach is compared to the best performance that could be
found in the conventional point-to-multipoint (PMP) recovery process. Numerical
results show that our formulation largely outperforms the conventional PMP
scheme in most practical situations and achieves a lower delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3958</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3958</id><created>2014-04-15</created><authors><author><keyname>Nakaizumi</keyname><forenames>Chisaki</forenames></author><author><keyname>Matsui</keyname><forenames>Toshie</forenames></author><author><keyname>Mori</keyname><forenames>Koichi</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Head-related Impulse Response-based Spatial Auditory Brain-computer
  Interface</title><categories>q-bio.NC cs.HC</categories><comments>5 pages, 1 figure, submitted to 6th International Brain-Computer
  Interface Conference 2014, Graz, Austria</comments><doi>10.3217/978-3-85125-378-8-20</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This study provides a comprehensive test of the head-related impulse response
(HRIR) to an auditory spatial speller brain-computer interface (BCI) paradigm,
including a comparison with a conventional virtual headphone-based spatial
auditory modality. Five BCI-naive users participated in an experiment based on
five Japanese vowels. The auditory evoked potentials obtained produced
encouragingly good and stable P300-responses in online BCI experiments. Our
case study indicates that the auditory HRIR spatial sound paradigm reproduced
with headphones could be a viable alternative to established multi-loudspeaker
surround sound BCI-speller applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3959</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3959</id><created>2014-04-15</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Pianesi</keyname><forenames>Fabio</forenames></author><author><keyname>Stock</keyname><forenames>Oliviero</forenames></author></authors><title>Is it morally acceptable for a system to lie to persuade me?</title><categories>cs.CY cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the fast rise of increasingly autonomous artificial agents and robots,
a key acceptability criterion will be the possible moral implications of their
actions. In particular, intelligent persuasive systems (systems designed to
influence humans via communication) constitute a highly sensitive topic because
of their intrinsically social nature. Still, ethical studies in this area are
rare and tend to focus on the output of the required action. Instead, this work
focuses on the persuasive acts themselves (e.g. &quot;is it morally acceptable that
a machine lies or appeals to the emotions of a person to persuade her, even if
for a good end?&quot;). Exploiting a behavioral approach, based on human assessment
of moral dilemmas -- i.e. without any prior assumption of underlying ethical
theories -- this paper reports on a set of experiments. These experiments
address the type of persuader (human or machine), the strategies adopted
(purely argumentative, appeal to positive emotions, appeal to negative
emotions, lie) and the circumstances. Findings display no differences due to
the agent, mild acceptability for persuasion and reveal that truth-conditional
reasoning (i.e. argument validity) is a significant dimension affecting
subjects' judgment. Some implications for the design of intelligent persuasive
systems are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3984</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3984</id><created>2014-04-15</created><updated>2014-06-14</updated><authors><author><keyname>Pan</keyname><forenames>Yunpeng</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos</forenames></author></authors><title>Nonparametric Infinite Horizon Kullback-Leibler Stochastic Control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two nonparametric approaches to Kullback-Leibler (KL) control, or
linearly-solvable Markov decision problem (LMDP) based on Gaussian processes
(GP) and Nystr\&quot;{o}m approximation. Compared to recently developed parametric
methods, the proposed data-driven frameworks feature accurate function
approximation and efficient on-line operations. Theoretically, we derive the
mathematical connection of KL control based on dynamic programming with earlier
work in control theory which relies on information theoretic dualities for the
infinite time horizon case. Algorithmically, we give explicit optimal control
policies in nonparametric forms, and propose on-line update schemes with
budgeted computational costs. Numerical results demonstrate the effectiveness
and usefulness of the proposed frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3990</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3990</id><created>2014-04-15</created><authors><author><keyname>Dosa</keyname><forenames>Gyorgy</forenames></author><author><keyname>Epstein</keyname><forenames>Leah</forenames></author></authors><title>Colorful bin packing</title><categories>cs.DS</categories><comments>To appear in SWAT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a variant of online bin packing, called colorful bin packing. In
this problem, items that are presented one by one are to be packed into bins of
size 1. Each item i has a size s_i \in [0,1] and a color c_i \in C, where C is
a set of colors (that is not necessarily known in advance). The total size of
items packed into a bin cannot exceed its size, thus an item i can always be
packed into a new bin, but an item cannot be packed into a non-empty bin if the
previous item packed into that bin has the same color, or if the occupied space
in it is larger than 1-s_i. This problem generalizes standard online bin
packing and online black and white bin packing (where |C|=2). We prove that
colorful bin packing is harder than black and white bin packing in the sense
that an online algorithm for zero size items that packs the input into the
smallest possible number of bins cannot exist for C \geq 3, while it is known
that such an algorithm exists for |C|=2. We show that natural generalizations
of classic algorithms for bin packing fail to work for the case |C| \geq 3 and
moreover, algorithms that perform well for black and white bin packing do not
perform well either, already for the case |C|=3. Our main results are a new
algorithm for colorful bin packing that we design and analyze, whose absolute
competitive ratio is 4, and a new lower bound of 2 on the asymptotic
competitive ratio of any algorithm, that is valid even for black and white bin
packing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3991</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3991</id><created>2014-04-15</created><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Spiralet Sparse Representation</title><categories>cs.CV</categories><comments>10 pages, Working Paper Number: WP-RFM-14-01</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the first report on Working Paper WP-RFM-14-01. The potential and
capability of sparse representations is well-known. However, their
(multivariate variable) vectorial form, which is completely fine in many fields
and disciplines, results in removal and filtering of important &quot;spatial&quot;
relations that are implicitly carried by two-dimensional [or multi-dimensional]
objects, such as images. In this paper, a new approach, called spiralet sparse
representation, is proposed in order to develop an augmented representation and
therefore a modified sparse representation and theory, which is capable to
preserve the data associated to the spatial relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3992</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3992</id><created>2014-04-15</created><authors><author><keyname>Kalyani</keyname><forenames>Aditi</forenames></author><author><keyname>Kumud</keyname><forenames>Hemant</forenames></author><author><keyname>Singh</keyname><forenames>Shashi Pal</forenames></author><author><keyname>Kumar</keyname><forenames>Ajai</forenames></author></authors><title>Assessing the Quality of MT Systems for Hindi to English Translation</title><categories>cs.CL</categories><journal-ref>International Journal of Computer Applications, Volume 89, No 15,
  March 2014</journal-ref><doi>10.5120/15711-4629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation plays a vital role in checking the quality of MT output. It is
done either manually or automatically. Manual evaluation is very time consuming
and subjective, hence use of automatic metrics is done most of the times. This
paper evaluates the translation quality of different MT Engines for
Hindi-English (Hindi data is provided as input and English is obtained as
output) using various automatic metrics like BLEU, METEOR etc. Further the
comparison automatic evaluation results with Human ranking have also been
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.3997</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.3997</id><created>2014-04-15</created><authors><author><keyname>Sabag</keyname><forenames>Oron</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author></authors><title>Lossless Coding of Correlated Sources with Actions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the problem of distributed compression of correlated
sources with an action-dependent joint distribution. This class of problems is,
in fact, an extension of the Slepian-Wolf model, but where cost-constrained
actions taken by the encoder or the decoder affect the generation of one of the
sources. The purpose of this work is to study the implications of actions on
the achievable rates.
  In particular, two cases where transmission occurs over a rate-limited link
are studied; case A for actions taken at the decoder and case B where actions
are taken at the encoder. A complete single-letter characterization of the set
of achievable rates is given in both cases. Furthermore, a network coding setup
is investigated for the case where actions are taken at the encoder. The
sources are generated at different nodes of the network and are required at a
set of terminal nodes, yet transmission occurs over a general, acyclic,
directed network. For this setup, generalized cut-set bounds are derived, and a
full characterization of the set of achievable rates using single-letter
expressions is provided. For this scenario, random linear network coding is
proved to be optimal, even though this is not a classical multicast problem.
Additionally, two binary examples are investigated and demonstrate how actions
taken at different nodes of the system have a significant affect on the
achievable rate region in comparison to a naive time-sharing strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4004</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4004</id><created>2014-04-15</created><authors><author><keyname>Hella</keyname><forenames>Lauri</forenames></author><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author></authors><title>One-dimensional fragment of first-order logic</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel decidable fragment of first-order logic. The fragment is
one-dimensional in the sense that quantification is limited to applications of
blocks of existential (universal) quantifiers such that at most one variable
remains free in the quantified formula. The fragment is closed under Boolean
operations, but additional restrictions (called uniformity conditions) apply to
combinations of atomic formulae with two or more variables. We argue that the
notions of one-dimensionality and uniformity together offer a novel perspective
on the robust decidability of modal logics. We also establish that minor
modifications to the restrictions of the syntax of the one-dimensional fragment
lead to undecidable formalisms. Namely, the two-dimensional and non-uniform
one-dimensional fragments are shown undecidable. Finally, we prove that with
regard to expressivity, the one-dimensional fragment is incomparable with both
the guarded negation fragment and two-variable logic with counting. Our proof
of the decidability of the one-dimensional fragment is based on a technique
involving a direct reduction to the monadic class of first-order logic. The
novel technique is itself of an independent mathematical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4020</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4020</id><created>2014-04-15</created><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Williams</keyname><forenames>Tyson</forenames></author></authors><title>The Complexity of Counting Edge Colorings and a Dichotomy for Some
  Higher Domain Holant Problems</title><categories>cs.CC</categories><comments>75 pages, 29 figures, 4 tables</comments><msc-class>68Q17</msc-class><acm-class>F.1.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that an effective version of Siegel's Theorem on finiteness of
integer solutions and an application of elementary Galois theory are key
ingredients in a complexity classification of some Holant problems. These
Holant problems, denoted by Holant(f), are defined by a symmetric ternary
function f that is invariant under any permutation of the k &gt;= 3 domain
elements. We prove that Holant(f) exhibits a complexity dichotomy. This
dichotomy holds even when restricted to planar graphs. A special case of this
result is that counting edge k-colorings is #P-hard over planar 3-regular
graphs for k &gt;= 3. In fact, we prove that counting edge k-colorings is #P-hard
over planar r-regular graphs for all k &gt;= r &gt;= 3. The problem is
polynomial-time computable in all other parameter settings. The proof of the
dichotomy theorem for Holant(f) depends on the fact that a specific polynomial
p(x,y) has an explicitly listed finite set of integer solutions, and the
determination of the Galois groups of some specific polynomials. In the
process, we also encounter the Tutte polynomial, medial graphs, Eulerian
partitions, Puiseux series, and a certain lattice condition on the (logarithm
of) the roots of polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4021</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4021</id><created>2014-04-15</created><authors><author><keyname>Labb&#xe9;</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Reutenauer</keyname><forenames>Christophe</forenames></author></authors><title>A d-dimensional extension of Christoffel words</title><categories>cs.DM math.CO</categories><comments>26 pages, 14 figures</comments><msc-class>05C75 (Primary) 52C35, 68R15 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this article, we extend the definition of Christoffel words to directed
subgraphs of the hypercubic lattice in arbitrary dimension that we call
Christoffel graphs. Christoffel graphs when $d=2$ correspond to well-known
Christoffel words. Due to periodicity, the $d$-dimensional Christoffel graph
can be embedded in a $(d-1)$-torus (a parallelogram when $d=3$). We show that
Christoffel graphs have similar properties to those of Christoffel words:
symmetry of their central part and conjugation with their reversal. Our main
result extends Pirillo's theorem (characterization of Christoffel words which
asserts that a word $amb$ is a Christoffel word if and only if it is conjugate
to $bma$) in arbitrary dimension. In the generalization, the map $amb\mapsto
bma$ is seen as a flip operation on graphs embedded in $\mathbb{Z}^d$ and the
conjugation is a translation. We show that a fully periodic subgraph of the
hypercubic lattice is a translate of its flip if and only if it is a
Christoffel graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4026</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4026</id><created>2014-04-15</created><updated>2015-04-24</updated><authors><author><keyname>Dar</keyname><forenames>Yehuda</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author></authors><title>Improving Low Bit-Rate Video Coding using Spatio-Temporal Down-Scaling</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good quality video coding for low bit-rate applications is important for
transmission over narrow-bandwidth channels and for storage with limited memory
capacity. In this work, we develop a previous analysis for image compression at
low bit-rates to adapt it to video signals. Improving compression using
down-scaling in the spatial and temporal dimensions is examined. We show, both
theoretically and experimentally, that at low bit-rates, we benefit from
applying spatio-temporal scaling. The proposed method includes down-scaling
before the compression and a corresponding up-scaling afterwards, while the
codec itself is left unmodified. We propose analytic models for low bit-rate
compression and spatio-temporal scaling operations. Specifically, we use
theoretic models of motion-compensated prediction of available and absent
frames as in coding and frame-rate up-conversion (FRUC) applications,
respectively. The proposed models are designed for multi-resolution analysis.
In addition, we formulate a bit-allocation procedure and propose a method for
estimating good down-scaling factors of a given video based on its second-order
statistics and the given bit-budget. We validate our model with experimental
results of H.264 compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4032</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4032</id><created>2014-04-15</created><updated>2014-07-16</updated><authors><author><keyname>Liu</keyname><forenames>Guangcan</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Recovery of Coherent Data via Low-Rank Dictionary Pursuit</title><categories>stat.ME cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently established RPCA method provides us a convenient way to restore
low-rank matrices from grossly corrupted observations. While elegant in theory
and powerful in reality, RPCA may be not an ultimate solution to the low-rank
matrix recovery problem. Indeed, its performance may not be perfect even when
data are strictly low-rank. This is because conventional RPCA ignores the
clustering structures of the data which are ubiquitous in modern applications.
As the number of cluster grows, the coherence of data keeps increasing, and
accordingly, the recovery performance of RPCA degrades. We show that the
challenges raised by coherent data (i.e., the data with high coherence) could
be alleviated by Low-Rank Representation (LRR), provided that the dictionary in
LRR is configured appropriately. More precisely, we mathematically prove that
if the dictionary itself is low-rank then LRR is immune to the coherence
parameter which increases with the underlying cluster number. This provides an
elementary principle for dealing with coherent data. Subsequently, we devise a
practical algorithm to obtain proper dictionaries in unsupervised environments.
Our extensive experiments on randomly generated matrices verify our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4038</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4038</id><created>2014-04-15</created><updated>2014-04-17</updated><authors><author><keyname>Papagiannopoulou</keyname><forenames>Christina</forenames></author><author><keyname>Tsoumakas</keyname><forenames>Grigorios</forenames></author><author><keyname>Tsamardinos</keyname><forenames>Ioannis</forenames></author></authors><title>Discovering and Exploiting Entailment Relationships in Multi-Label
  Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a sound probabilistic method for enforcing adherence of
the marginal probabilities of a multi-label model to automatically discovered
deterministic relationships among labels. In particular we focus on discovering
two kinds of relationships among the labels. The first one concerns pairwise
positive entailement: pairs of labels, where the presence of one implies the
presence of the other in all instances of a dataset. The second concerns
exclusion: sets of labels that do not coexist in the same instances of the
dataset. These relationships are represented with a Bayesian network. Marginal
probabilities are entered as soft evidence in the network and adjusted through
probabilistic inference. Our approach offers robust improvements in mean
average precision compared to the standard binary relavance approach across all
12 datasets involved in our experiments. The discovery process helps
interesting implicit knowledge to emerge, which could be useful in itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4067</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4067</id><created>2014-04-15</created><authors><author><keyname>Ghosh</keyname><forenames>Tamal</forenames></author><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Dan</keyname><forenames>Pranab K</forenames></author></authors><title>An effective AHP-based metaheuristic approach to solve supplier
  selection problem</title><categories>cs.NE</categories><journal-ref>International Journal of Procurement Management, Vol. 5, No. 2,
  2012</journal-ref><doi>10.1504/IJPM.2012.045647</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The supplier selection problem is based on electing the best supplier from a
group of pre-specified candidates, is identified as a Multi Criteria Decision
Making (MCDM), is proportionately significant in terms of qualitative and
quantitative attributes. It is a fundamental issue to achieve a trade-off
between such quantifiable and unquantifiable attributes with an aim to
accomplish the best solution to the abovementioned problem. This article
portrays a metaheuristic based optimization model to solve this NP-Complete
problem. Initially the Analytic Hierarchy Process (AHP) is implemented to
generate an initial feasible solution of the problem. Thereafter a Simulated
Annealing (SA) algorithm is exploited to improve the quality of the obtained
solution. The Taguchi robust design method is exploited to solve the critical
issues on the subject of the parameter selection of the SA technique. In order
to verify the proposed methodology the numerical results are demonstrated based
on tangible industry data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4078</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4078</id><created>2014-04-15</created><authors><author><keyname>Li</keyname><forenames>Xia</forenames></author><author><keyname>Lin</keyname><forenames>Feng</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author></authors><title>Modeling Massive Amount of Experimental Data with Large Random Matrices
  in a Real-Time UWB-MIMO System</title><categories>cs.IT math.IT</categories><comments>4 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to study data modeling for massive datasets. Large
random matrices are used to model the massive amount of data collected from our
experimental testbed. This testbed was developed for a real-time
ultra-wideband, multiple input multiple output (UWB-MIMO) system. Empirical
spectral density is the relevant information we seek for. After we treat this
UWB-MIMO system as a black box, we aim to model the output of the black box as
a large statistical system, whose outputs can be described by (large) random
matrices. This model is extremely general to allow for the study of non-linear
and non-Gaussian phenomenon. The good agreements between the theoretical
predictions and the empirical findings validate the correctness of the our
suggested data model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4088</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4088</id><created>2014-04-15</created><authors><author><keyname>Rahman</keyname><forenames>Akhlaqur</forenames></author><author><keyname>Tasnim</keyname><forenames>Sumaira</forenames></author></authors><title>Ensemble Classifiers and Their Applications: A Review</title><categories>cs.LG</categories><comments>Published with International Journal of Computer Trends and
  Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  10(1):31-35, April 2014</journal-ref><doi>10.14445/22312803/IJCTT-V10P107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble classifier refers to a group of individual classifiers that are
cooperatively trained on data set in a supervised classification problem. In
this paper we present a review of commonly used ensemble classifiers in the
literature. Some ensemble classifiers are also developed targeting specific
applications. We also present some application driven ensemble classifiers in
this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4089</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4089</id><created>2014-04-15</created><authors><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>On the Role of Canonicity in Bottom-up Knowledge Compilation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of bottom-up compilation of knowledge bases, which is
usually predicated on the existence of a polytime function for combining
compilations using Boolean operators (usually called an Apply function). While
such a polytime Apply function is known to exist for certain languages (e.g.,
OBDDs) and not exist for others (e.g., DNNF), its existence for certain
languages remains unknown. Among the latter is the recently introduced language
of Sentential Decision Diagrams (SDDs), for which a polytime Apply function
exists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical
SDDs). We resolve this open question in this paper and consider some of its
theoretical and practical implications. Some of the findings we report question
the common wisdom on the relationship between bottom-up compilation, language
canonicity and the complexity of the Apply function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4095</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4095</id><created>2014-04-15</created><updated>2014-05-18</updated><authors><author><keyname>Mills</keyname><forenames>Peter</forenames></author></authors><title>Multi-borders classification</title><categories>stat.ML cs.LG</categories><comments>Corrected error in equations: second and third equations were not
  linearly independent. Corrected figure to match. &quot;Hierarchical&quot; scheme is a
  decision tree</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of possible methods of generalizing binary classification to
multi-class classification increases exponentially with the number of class
labels. Often, the best method of doing so will be highly problem dependent.
Here we present classification software in which the partitioning of
multi-class classification problems into binary classification problems is
specified using a recursive control language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4100</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4100</id><created>2014-04-15</created><authors><author><keyname>Gong</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Hongyu</forenames></author><author><keyname>Seo</keyname><forenames>Hyunmin</forenames></author><author><keyname>Kim</keyname><forenames>Sunghun</forenames></author></authors><title>Locating Crashing Faults based on Crash Stack Traces</title><categories>cs.SE</categories><comments>11 pages, this works is done in 2011 when the first author(Liang
  Gong) was a master student in Tsinghua University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software crashes due to its increasing complexity. Once a crash happens, a
crash report could be sent to software developers for investigation upon user
permission. Because of the large number of crash reports and limited
information, debugging for crashes is often a tedious and labor-intensive task.
In this paper, we propose a statistical fault localization framework to help
developers locate functions that contain crashing faults. We generate the
execution traces for the failing traces based on the crash stack, and the
passing traces from normal executions. We form program spectra by combining
generated passing and failing trace, and then apply statistical fault
localization techniques such as Ochiai to locate the crashing faults. We also
propose two heuristics to improve the fault localization performance. We
evaluate our approach using the real-world Firefox crash report data. The
results show that the performance of our method is promising. Our approach
permits developers to locate 63.9% crashing faults by examining only 5% Firefox
3.6 functions in the spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4104</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4104</id><created>2014-04-15</created><authors><author><keyname>Shi</keyname><forenames>Jianing V.</forenames></author><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Sparse Bilinear Logistic Regression</title><categories>math.OC cs.CV cs.LG</categories><comments>27 pages, 5 figures</comments><msc-class>65K10, 68W40, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the concept of sparse bilinear logistic
regression for decision problems involving explanatory variables that are
two-dimensional matrices. Such problems are common in computer vision,
brain-computer interfaces, style/content factorization, and parallel factor
analysis. The underlying optimization problem is bi-convex; we study its
solution and develop an efficient algorithm based on block coordinate descent.
We provide a theoretical guarantee for global convergence and estimate the
asymptotical convergence rate using the Kurdyka-{\L}ojasiewicz inequality. A
range of experiments with simulated and real data demonstrate that sparse
bilinear logistic regression outperforms current techniques in several
important applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4105</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4105</id><created>2014-04-15</created><authors><author><keyname>Shi</keyname><forenames>Yuan</forenames></author><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>Sparse Compositional Metric Learning</title><categories>cs.LG stat.ML</categories><comments>18 pages. To be published in Proceedings of the 27th AAAI Conference
  on Artificial Intelligence (AAAI 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for metric learning by framing it as learning a
sparse combination of locally discriminative metrics that are inexpensive to
generate from the training data. This flexible framework allows us to naturally
derive formulations for global, multi-task and local metric learning. The
resulting algorithms have several advantages over existing methods in the
literature: a much smaller number of parameters to be estimated and a
principled way to generalize learned metrics to new testing data points. To
analyze the approach theoretically, we derive a generalization bound that
justifies the sparse combination. Empirically, we evaluate our algorithms on
several datasets against state-of-the-art metric learning methods. The results
are consistent with our theoretical findings and demonstrate the superiority of
our approach in terms of classification performance and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4106</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4106</id><created>2014-04-15</created><authors><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author><author><keyname>Rocco</keyname><forenames>Marco</forenames></author><author><keyname>Ceppi</keyname><forenames>Sofia</forenames></author><author><keyname>Gerding</keyname><forenames>Enrico H.</forenames></author></authors><title>Mechanism Design for Mobile Geo-Location Advertising</title><categories>cs.GT</categories><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile geo-location advertising, where mobile ads are targeted based on a
user's location, has been identified as a key growth factor for the mobile
market. As with online advertising, a crucial ingredient for their success is
the development of effective economic mechanisms. An important difference is
that mobile ads are shown sequentially over time and information about the user
can be learned based on their movements. Furthermore, ads need to be shown
selectively to prevent ad fatigue. To this end, we introduce, for the first
time, a user model and suitable economic mechanisms which take these factors
into account. Specifically, we design two truthful mechanisms which produce an
advertisement plan based on the user's movements. One mechanism is allocatively
efficient, but requires exponential compute time in the worst case. The other
requires polynomial time, but is not allocatively efficient. Finally, we
experimentally evaluate the tradeoff between compute time and efficiency of our
mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4107</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4107</id><created>2014-02-08</created><authors><author><keyname>Srikanth</keyname><forenames>Vasireddy</forenames></author><author><keyname>Ramesh</keyname><forenames>Pillem</forenames></author></authors><title>Invisibility System Using Image Processing and Optical Camouflage
  Technology</title><categories>cs.OH</categories><comments>IJETT, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invisible persons are seen in fiction stories only, but in the real world it
is proved that invisibility is possible. This paper describes the creation of
invisibility with the help of technologies like Optical camouflage; Image based
rendering and Retro reflective projection. The object that needs to be made
transparent or invisible is painted or covered with retro reflective material.
Then a projector projects the background image on it making the masking object
virtually transparent. Capturing the background image requires a video camera,
which sits behind the person wearing the cloak. The video from the camera must
be in a digital format so it can be sent to a computer for image processing
using image based rendering technical. There are some useful applications for
this simple but astonishing technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4108</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4108</id><created>2014-02-24</created><updated>2014-07-09</updated><authors><author><keyname>Alsharif</keyname><forenames>Ouais</forenames></author><author><keyname>Bachman</keyname><forenames>Philip</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Representation as a Service</title><categories>cs.LG</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a Machine Learning Service Provider (MLSP) designed to rapidly
create highly accurate learners for a never-ending stream of new tasks. The
challenge is to produce task-specific learners that can be trained from few
labeled samples, even if tasks are not uniquely identified, and the number of
tasks and input dimensionality are large. In this paper, we argue that the MLSP
should exploit knowledge from previous tasks to build a good representation of
the environment it is in, and more precisely, that useful representations for
such a service are ones that minimize generalization error for a new hypothesis
trained on a new task. We formalize this intuition with a novel method that
minimizes an empirical proxy of the intra-task small-sample generalization
error. We present several empirical results showing state-of-the art
performance on single-task transfer, multitask learning, and the full lifelong
learning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4114</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4114</id><created>2014-04-15</created><updated>2014-11-25</updated><authors><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Structured Stochastic Variational Inference</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic variational inference makes it possible to approximate posterior
distributions induced by large datasets quickly using stochastic optimization.
The algorithm relies on the use of fully factorized variational distributions.
However, this &quot;mean-field&quot; independence approximation limits the fidelity of
the posterior approximation, and introduces local optima. We show how to relax
the mean-field approximation to allow arbitrary dependencies between global
parameters and local hidden variables, producing better parameter estimates by
reducing bias, sensitivity to local optima, and sensitivity to hyperparameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4120</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4120</id><created>2014-04-15</created><updated>2015-03-01</updated><authors><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Rebelatto</keyname><forenames>Joao Luiz</forenames></author><author><keyname>Uchoa-Filhoand</keyname><forenames>Bartolomeu F.</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Harvest-Then-Cooperate: Wireless-Powered Cooperative Communications</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Trans. Signal Processing</comments><doi>10.1109/TSP.2015.2396009</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we consider a wireless-powered cooperative communication
network consisting of one hybrid access-point (AP), one source, and one relay.
In contrast to conventional cooperative networks, the source and relay in the
considered network have no embedded energy supply. They need to rely on the
energy harvested from the signals broadcasted by the AP for their cooperative
information transmission. Based on this three-node reference model, we propose
a harvest-then-cooperate (HTC) protocol, in which the source and relay harvest
energy from the AP in the downlink and work cooperatively in the uplink for the
source's information transmission. Considering a delay-limited transmission
mode, the approximate closed-form expression for the average throughput of the
proposed protocol is derived over Rayleigh fading channels. Subsequently, this
analysis is extended to the multi-relay scenario, where the approximate
throughput of the HTC protocol with two popular relay selection schemes is
derived. The asymptotic analyses for the throughput performance of the
considered schemes at high signal-to-noise radio are also provided. All
theoretical results are validated by numerical simulations. The impacts of the
system parameters, such as time allocation, relay number, and relay position,
on the throughput performance are extensively investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4123</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4123</id><created>2014-04-15</created><authors><author><keyname>Fukunaga</keyname><forenames>Takuro</forenames></author></authors><title>Covering problems in edge- and node-weighted graphs</title><categories>cs.DS</categories><comments>To appear in SWAT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the graph covering problem in which a set of edges in an
edge- and node-weighted graph is chosen to satisfy some covering constraints
while minimizing the sum of the weights. In this problem, because of the large
integrality gap of a natural linear programming (LP) relaxation, LP rounding
algorithms based on the relaxation yield poor performance. Here we propose a
stronger LP relaxation for the graph covering problem. The proposed relaxation
is applied to designing primal-dual algorithms for two fundamental graph
covering problems: the prize-collecting edge dominating set problem and the
multicut problem in trees. Our algorithms are an exact polynomial-time
algorithm for the former problem, and a 2-approximation algorithm for the
latter problem, respectively. These results match the currently known best
results for purely edge-weighted graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4132</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4132</id><created>2014-04-15</created><updated>2015-03-29</updated><authors><author><keyname>Hager</keyname><forenames>William W.</forenames></author><author><keyname>Zhu</keyname><forenames>Jiajie</forenames></author></authors><title>Projection algorithms for non-convex minimization, with application to
  sparse principal component analysis</title><categories>cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider concave minimization problems over non-convex sets.Optimization
problems with this structure arise in sparse principal component analysis. We
analyze both a gradient projection algorithm and an approximate Newton
algorithm where the Hessian approximation is a multiple of the identity.
Convergence results are established. In numerical experiments arising in sparse
principal component analysis, it is seen that the performance of the gradient
projection algorithm is very similar to that of the truncated power method and
the generalized power method. In some cases, the approximate Newton algorithm
with a Barzilai-Borwein (BB) Hessian approximation can be substantially faster
than the other algorithms, and can converge to a better solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4136</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4136</id><created>2014-04-16</created><authors><author><keyname>Gupta</keyname><forenames>Richa</forenames></author><author><keyname>Gupta</keyname><forenames>Sunny</forenames></author><author><keyname>Singhal</keyname><forenames>Anuradha</forenames></author></authors><title>Big Data: Overview</title><categories>cs.OH</categories><comments>3 pages, 1 figure</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  9(5):1-3, March 2014</journal-ref><doi>10.14445/22312803/IJCTT-V9150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data is data that exceeds the processing capacity of traditional
databases. The data is too big to be processed by a single machine. New and
innovative methods are required to process and store such large volumes of
data. This paper provides an overview on big data, its importance in our live
and some technologies to handle big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4139</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4139</id><created>2014-04-16</created><authors><author><keyname>Gupta</keyname><forenames>Richa</forenames></author></authors><title>Web Mining using Artificial Ant Colonies: A Survey</title><categories>cs.OH</categories><comments>6 pages, 3 figures</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  10(1): 1-6, April 2014</journal-ref><doi>10.14445/22312803/IJCTT-V10P103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web mining has been very crucial to any organization as it provides useful
insights to business patterns. It helps the company to understand its customers
better. As the web is growing in pace, so is its importance and hence it
becomes all the more necessary to find useful patterns. Here in this paper, web
mining using ant colony optimization has been reviewed with some of its
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4140</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4140</id><created>2014-04-16</created><authors><author><keyname>Gupta</keyname><forenames>Richa</forenames></author></authors><title>Journey from Data Mining to Web Mining to Big Data</title><categories>cs.OH</categories><comments>3 pages</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  10(1): 1-3, April 2014. Published by Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V10P104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the journey of big data starting from data mining to web
mining to big data. It discusses each of this method in brief and also provides
their applications. It states the importance of mining big data today using
fast and novel approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4141</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4141</id><created>2014-04-16</created><authors><author><keyname>Gupta</keyname><forenames>Richa</forenames></author></authors><title>Information Hiding and Attacks : Review</title><categories>cs.CR</categories><comments>4 pages. arXiv admin note: substantial text overlap with
  arXiv:1404.3063</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  10(1): 1-4, April 2014. Published by Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V10P105.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Hiding is considered very important part of our lives. There
exist many techniques for securing the information. This paper briefs on the
techniques for information hiding and the potential threats to those methods.
This paper briefs about cryptanalysis and stegananlysis, two methods for
breaching into the security methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4152</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4152</id><created>2014-04-16</created><updated>2014-04-17</updated><authors><author><keyname>Liu</keyname><forenames>Yongchao</forenames></author><author><keyname>Schmidt</keyname><forenames>Bertil</forenames></author></authors><title>SWAPHI: Smith-Waterman Protein Database Search on Xeon Phi Coprocessors</title><categories>cs.DC</categories><comments>A short version of this paper has been accepted by the IEEE ASAP 2014
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximal sensitivity of the Smith-Waterman (SW) algorithm has enabled its
wide use in biological sequence database search. Unfortunately, the high
sensitivity comes at the expense of quadratic time complexity, which makes the
algorithm computationally demanding for big databases. In this paper, we
present SWAPHI, the first parallelized algorithm employing Xeon Phi
coprocessors to accelerate SW protein database search. SWAPHI is designed based
on the scale-and-vectorize approach, i.e. it boosts alignment speed by
effectively utilizing both the coarse-grained parallelism from the many
co-processing cores (scale) and the fine-grained parallelism from the 512-bit
wide single instruction, multiple data (SIMD) vectors within each core
(vectorize). By searching against the large UniProtKB/TrEMBL protein database,
SWAPHI achieves a performance of up to 58.8 billion cell updates per second
(GCUPS) on one coprocessor and up to 228.4 GCUPS on four coprocessors.
Furthermore, it demonstrates good parallel scalability on varying number of
coprocessors, and is also superior to both SWIPE on 16 high-end CPU cores and
BLAST+ on 8 cores when using four coprocessors, with the maximum speedup of
1.52 and 1.86, respectively. SWAPHI is written in C++ language (with a set of
SIMD intrinsics), and is freely available at http://swaphi.sourceforge.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4157</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4157</id><created>2014-04-16</created><updated>2014-09-29</updated><authors><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author><author><keyname>Boutros</keyname><forenames>Joseph Jean</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author></authors><title>Phase Precoding for the Compute-and-Forward Protocol</title><categories>cs.IT math.IT</categories><comments>Submitted with 28 Pages and 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compute-and-forward (CoF) is a relaying protocol, which uses algebraic
structured codes to harness the interference and remove the noise in wireless
networks. We propose the use of phase precoders at the transmitters of a
network, where relays apply CoF strategy. We define the {\em phase precoded
computation rate} and show that it is greater than the original computation
rate of CoF protocol. We further give a new low-complexity method for finding
network equations. We finally show that the proposed precoding scheme increases
the degrees-of-freedom (DoF) of CoF protocol. This overcomes the limitations on
the DoF of the CoF protocol, recently presented by Niesen and Whiting. Using
tools from Diophantine approximation and algebraic geometry, we prove the
existence of a phase precoder that approaches the maximum DoF when the number
of transmitters tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4161</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4161</id><created>2014-04-16</created><updated>2014-07-06</updated><authors><author><keyname>Berljafa</keyname><forenames>Mario</forenames><affiliation>The University of Manchester</affiliation></author><author><keyname>Wortmann</keyname><forenames>Daniel</forenames><affiliation>Forschungszentrum Juelich</affiliation></author><author><keyname>Di Napoli</keyname><forenames>Edoardo</forenames><affiliation>Forschungszentrum Juelich</affiliation><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>An Optimized and Scalable Eigensolver for Sequences of Eigenvalue
  Problems</title><categories>cs.MS cs.DC physics.comp-ph</categories><comments>23 Pages, 6 figures. First revision of an invited submission to
  special issue of Concurrency and Computation: Practice and Experience</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many scientific applications the solution of non-linear differential
equations are obtained through the set-up and solution of a number of
successive eigenproblems. These eigenproblems can be regarded as a sequence
whenever the solution of one problem fosters the initialization of the next. In
addition, in some eigenproblem sequences there is a connection between the
solutions of adjacent eigenproblems. Whenever it is possible to unravel the
existence of such a connection, the eigenproblem sequence is said to be
correlated. When facing with a sequence of correlated eigenproblems the current
strategy amounts to solving each eigenproblem in isolation. We propose a
alternative approach which exploits such correlation through the use of an
eigensolver based on subspace iteration and accelerated with Chebyshev
polynomials (ChFSI). The resulting eigensolver is optimized by minimizing the
number of matrix-vector multiplications and parallelized using the Elemental
library framework. Numerical results show that ChFSI achieves excellent
scalability and is competitive with current dense linear algebra parallel
eigensolvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4163</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4163</id><created>2014-04-16</created><updated>2014-04-21</updated><authors><author><keyname>Avramopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Multiplicative weights in monotropic games</title><categories>cs.GT cs.MA math.OC</categories><comments>This paper has been withdrawn by the author due a crucial error in
  the proof of the main result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of population games that we call monotropic; these
are games characterized by the presence of a unique globally neutrally stable
Nash equilibrium. Monotropic games generalize strictly concave potential games
and zero sum games with a unique minimax solution. Within the class of
monotropic games, we study a multiplicative weights dynamic. We show that,
depending on a parameter called the learning rate, multiplicative weights are
interior globally convergent to the unique equilibrium of monotropic games, but
may also induce chaotic behavior if the learning rate is not carefully chosen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4164</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4164</id><created>2014-04-16</created><updated>2014-07-11</updated><authors><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Foggi</keyname><forenames>Tommaso</forenames></author></authors><title>Time-Frequency Packing for High Capacity Coherent Optical Links</title><categories>cs.IT math.IT</categories><comments>10 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:1406.5685 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider realistic long-haul optical links, with linear and nonlinear
impairments, and investigate the application of time-frequency packing with
low-order constellations as a possible solution to increase the spectral
efficiency. A detailed comparison with available techniques from the literature
will be also performed. We will see that this technique represents a feasible
solution to overcome the relevant theoretical and technological issues related
to this spectral efficiency increase and could be more effective than the
simple adoption of high-order modulation formats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4171</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4171</id><created>2014-04-16</created><authors><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Dropout Training for Support Vector Machines</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout and other feature noising schemes have shown promising results in
controlling over-fitting by artificially corrupting the training data. Though
extensive theoretical and empirical studies have been performed for generalized
linear models, little work has been done for support vector machines (SVMs),
one of the most successful approaches for supervised learning. This paper
presents dropout training for linear SVMs. To deal with the intractable
expectation of the non-smooth hinge loss under corrupting distributions, we
develop an iteratively re-weighted least square (IRLS) algorithm by exploring
data augmentation techniques. Our algorithm iteratively minimizes the
expectation of a re-weighted least square problem, where the re-weights have
closed-form solutions. The similar ideas are applied to develop a new IRLS
algorithm for the expected logistic loss under corrupting distributions. Our
algorithms offer insights on the connection and difference between the hinge
loss and logistic loss in dropout training. Empirical results on several real
datasets demonstrate the effectiveness of dropout training on significantly
boosting the classification accuracy of linear SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4173</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4173</id><created>2014-04-16</created><authors><author><keyname>Lautenschlaeger</keyname><forenames>Wolfram</forenames></author></authors><title>A Deterministic TCP Bandwidth Sharing Model</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally TCP bandwidth sharing has been investigated mainly by
stochastic approaches due to its seemingly chaotic nature. Even though of great
generality, the theories deal mainly with expectation values, which is prone to
misinterpretation with respect to the Quality-of-Experience (QoE). We
disassemble TCP operating conditions into dominating scenarios and show that
bandwidth sharing alone follows mostly deterministic rules. From the analysis
we derive significant root causes of well-known TCP aspects like unequal
sharing, burstiness of losses, global synchronization, and on buffer sizing. We
base our model on a detailed analysis of bandwidth sharing experiments with
subsequent mathematical reproduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4175</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4175</id><created>2014-04-16</created><authors><author><keyname>Olivetti</keyname><forenames>Emanuele</forenames></author><author><keyname>Kia</keyname><forenames>Seyed Mostafa</forenames></author><author><keyname>Avesani</keyname><forenames>Paolo</forenames></author></authors><title>MEG Decoding Across Subjects</title><categories>stat.ML cs.LG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain decoding is a data analysis paradigm for neuroimaging experiments that
is based on predicting the stimulus presented to the subject from the
concurrent brain activity. In order to make inference at the group level, a
straightforward but sometimes unsuccessful approach is to train a classifier on
the trials of a group of subjects and then to test it on unseen trials from new
subjects. The extreme difficulty is related to the structural and functional
variability across the subjects. We call this approach &quot;decoding across
subjects&quot;. In this work, we address the problem of decoding across subjects for
magnetoencephalographic (MEG) experiments and we provide the following
contributions: first, we formally describe the problem and show that it belongs
to a machine learning sub-field called transductive transfer learning (TTL).
Second, we propose to use a simple TTL technique that accounts for the
differences between train data and test data. Third, we propose the use of
ensemble learning, and specifically of stacked generalization, to address the
variability across subjects within train data, with the aim of producing more
stable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, we
compare the standard approach of not modelling the differences across subjects,
to the proposed one of combining TTL and ensemble learning. We show that the
proposed approach is consistently more accurate than the standard one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4181</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4181</id><created>2014-04-16</created><authors><author><keyname>Moinard</keyname><forenames>Matthieu</forenames></author><author><keyname>Amonou</keyname><forenames>Isabelle</forenames></author><author><keyname>Duhamel</keyname><forenames>Pierre</forenames></author><author><keyname>Brault</keyname><forenames>Patrice</forenames></author></authors><title>Prediction of Transformed (DCT) Video Coding Residual for Video
  Compression</title><categories>cs.IT cs.MM math.IT</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video compression has been investigated by means of analysis-synthesis, and
more particularly by means of inpainting. The first part of our approach has
been to develop the inpainting of DCT coefficients in an image. This has shown
good results for image compression without overpassing todays compression
standards like JPEG. We then looked at integrating the same approach in a video
coder, and in particular in the widely used H264 AVC standard coder, but the
same approach can be used in the framework of HEVC. The originality of this
work consists in cancelling at the coder, then automatically restoring, at the
decoder, some well chosen DCT residual coefficients. For this purpose, we have
developed a restoration model of transformed coefficients. By using a total
variation based model, we derive conditions for the reconstruction of
transformed coefficients that have been suppressed or altered. The main purpose
here, in a video coding context, is to improve the ratedistortion performance
of existing coders. To this end DCT restoration is used as an additional
prediction step to the spatial prediction of the transformed coefficients,
based on an image regularization process. The method has been successfully
tested with the H.264 AVC video codec standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4184</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4184</id><created>2014-04-16</created><authors><author><keyname>Hamada</keyname><forenames>Katsuhiko</forenames></author><author><keyname>Mori</keyname><forenames>Hiromu</forenames></author><author><keyname>Shinoda</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Airborne Ultrasonic Tactile Display Brain-computer Interface Paradigm</title><categories>q-bio.NC cs.HC</categories><comments>5 pages, 3 figures, submitted to 6th International Brain-Computer
  Interface Conference 2014, Graz, Austria</comments><doi>10.3217/978-3-85125-378-8-18</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We study the extent to which contact-less and airborne ultrasonic tactile
display (AUTD) stimuli delivered to the palms of a user can serve as a platform
for a brain computer interface (BCI) paradigm. Six palm positions are used to
evoke combined somatosensory brain responses, in order to define a novel
contact-less tactile BCI. A comparison is made with classical attached
vibrotactile transducers. Experiment results of subjects performing online
experiments validate the novel BCI paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4189</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4189</id><created>2014-04-16</created><authors><author><keyname>Berth&#xe9;</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Labb&#xe9;</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Factor Complexity of S-adic sequences generated by the
  Arnoux-Rauzy-Poincar\'e Algorithm</title><categories>cs.DM math.DS</categories><comments>36 pages, 16 figures</comments><msc-class>37B10 (Primary) 68R15 (Secondary)</msc-class><journal-ref>Advances in Applied Mathematics 63 (2015) 90-130</journal-ref><doi>10.1016/j.aam.2014.11.001</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Arnoux-Rauzy-Poincar\'e multidimensional continued fraction algorithm is
obtained by combining the Arnoux-Rauzy and Poincar\'e algorithms. It is a
generalized Euclidean algorithm. Its three-dimensional linear version consists
in subtracting the sum of the two smallest entries to the largest if possible
(Arnoux-Rauzy step), and otherwise, in subtracting the smallest entry to the
median and the median to the largest (the Poincar\'e step), and by performing
when possible Arnoux-Rauzy steps in priority. After renormalization it provides
a piecewise fractional map of the standard $2$-simplex. We study here the
factor complexity of its associated symbolic dynamical system, defined as an
$S$-adic system. It is made of infinite words generated by the composition of
sequences of finitely many substitutions, together with some restrictions
concerning the allowed sequences of substitutions expressed in terms of a
regular language. Here, the substitutions are provided by the matrices of the
linear version of the algorithm. We give an upper bound for the linear growth
of the factor complexity. We then deduce the convergence of the associated
algorithm by unique ergodicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4191</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4191</id><created>2014-04-16</created><authors><author><keyname>Tadic</keyname><forenames>Bosiljka</forenames></author><author><keyname>Gligorijevic</keyname><forenames>Vladimir</forenames></author><author><keyname>Skowron</keyname><forenames>Marcin</forenames></author><author><keyname>Suvakov</keyname><forenames>Milovan</forenames></author></authors><title>The Dynamics of Emotional Chats with Bots: Experiment and Agent-Based
  Simulations</title><categories>cs.SI physics.soc-ph</categories><comments>16 pages, 11 figures</comments><journal-ref>ScienceJet 2014, 3: 50</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative research of emotions in psychology and machine-learning methods
for extracting emotion components from text messages open an avenue for
physical science to explore the nature of stochastic processes in which
emotions play a role, e.g., in human dynamics online. Here, we investigate the
occurrence of collective behavior of users that is induced by chats with
emotional Bots. The Bots, designed in an experimental environment, are
considered. Furthermore, using the agent-based modeling approach, the activity
of these experimental Bots is simulated within a social network of interacting
emotional agents. Quantitative analysis of time series carrying emotional
messages by agents suggests temporal correlations and persistent fluctuations
with clustering according to emotion similarity. {All data used in this study
are fully anonymized.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4199</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4199</id><created>2014-04-16</created><authors><author><keyname>Arnault</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Amblard</keyname><forenames>Zo&#xe9;</forenames></author></authors><title>A qutrit Quantum Key Distribution protocol with better noise resistance</title><categories>quant-ph cs.CR</categories><comments>11 pages</comments><msc-class>81P94</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ekert quantum key distribution protocol uses pairs of entangled qubits
and performs checks based on a Bell inequality to detect eavesdropping. The
3DEB protocol uses instead pairs of entangled qutrits to achieve better noise
resistance than the Ekert protocol. It performs checks based on a Bell
inequality for qutrits named CHSH-3. In this paper, we present a new protocol,
which also uses pairs of entangled qutrits, but achieves even better noise
resistance than 3DEB. This gain of performance is obtained by using another
inequality called here hCHSH-3. As the hCHSH3 inequality involve products of
observables which become incompatible when using quantum states, we show how
the parties running the protocol can measure the violation of hCHSH3 in the
presence of noise, to ensure the secrecy of the key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4208</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4208</id><created>2014-04-16</created><updated>2015-01-07</updated><authors><author><keyname>Gyarmati</keyname><forenames>Laszlo</forenames></author><author><keyname>Laoutaris</keyname><forenames>Nikolaos</forenames></author><author><keyname>Sdrolias</keyname><forenames>Kostas</forenames></author><author><keyname>Rodriguez</keyname><forenames>Pablo</forenames></author><author><keyname>Courcoubetis</keyname><forenames>Costas</forenames></author></authors><title>From advertising profits to bandwidth prices-A quantitative methodology
  for negotiating premium peering</title><categories>cs.NI</categories><comments>16 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed a first of its kind methodology for deriving bandwidth
prices for premium direct peering between Access ISPs (A-ISPs) and Content and
Service Providers (CSPs) that want to deliver content and services in premium
quality. Our methodology establishes a direct link between service
profitability, e.g., from advertising, user- and subscriber-loyalty,
interconnection costs, and finally bandwidth price for peering. Unlike existing
work in both the networking and economics literature, our resulting
computational model built around Nash bargaining, can be used for deriving
quantitative results comparable to actual market prices. We analyze the US
market and derive prices for video that compare favorably with existing prices
for transit and paid peering. We also observe that the fair prices returned by
the model for high-profit/low-volume services such as search, are orders of
magnitude higher than current bandwidth prices. This implies that resolving
existing (fierce) interconnection tussles may require per service, instead of
wholesale, peering between A-ISPs and CSPs. Our model can be used for deriving
initial benchmark prices for such negotiations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4226</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4226</id><created>2014-04-16</created><authors><author><keyname>Kodama</keyname><forenames>Takumi</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Spatial Tactile Brain-Computer Interface Paradigm Applying Vibration
  Stimuli to Large Areas of User's Back</title><categories>q-bio.NC cs.HC</categories><comments>5 pages, 4 figures, submitted to 6th International Brain-Computer
  Interface Conference 2014, Graz, Austria</comments><doi>10.3217/978-3-85125-378-8-32</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We aim at an augmentation of communication abilities of amyotrophic lateral
sclerosis (ALS) patients by creating a brain-computer interface (BCI) which can
control a computer or other device by using only brain activity. As a method,
we use a stimulus-driven BCI based on vibration stimuli delivered via a gaming
pad to the user's back. We identify P300 responses from brain activity data in
response to the vibration stimuli. The user's intentions are classified
according to the P300 responses recorded in the EEG. From the results of the
psychophysical and online BCI experiments, we are able to classify the P300
responses very accurately, which proves the effectiveness of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4239</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4239</id><created>2014-04-16</created><updated>2015-06-10</updated><authors><author><keyname>Adiprasito</keyname><forenames>Karim A.</forenames></author><author><keyname>Benedetti</keyname><forenames>Bruno</forenames></author><author><keyname>Lutz</keyname><forenames>Frank H.</forenames></author></authors><title>Extremal examples of collapsible complexes and random discrete Morse
  theory</title><categories>math.CO cs.CG math.AT math.GT</categories><comments>24 pages, 9 figures, 2 tables; improved presentation of proofs,
  details and figures added</comments><report-no>CPH-SYM-DNRF92</report-no><msc-class>57Q15, 57Q05, 57N10, 57N13, 52B70, 52B05, 52B22, 55N35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present extremal constructions connected with the property of simplicial
collapsibility.
  (1) For each $d \ge 2$, there are collapsible (and shellable) simplicial
$d$-complexes with only one free face. Also, there are non-evasive
$d$-complexes with only two free faces. (Both results are optimal in all
dimensions.)
  (2) Optimal discrete Morse vectors need not be unique. We explicitly
construct a contractible, but non-collapsible $3$-dimensional simplicial
complex with face vector $f=(106,596,1064,573)$ that admits two distinct
optimal discrete Morse vectors, $(1,1,1,0)$ and $(1,0,1,1)$. Indeed, we show
that in every dimension $d\geq 3$ there are contractible, non-collapsible
simplicial $d$-complexes that have $(1,0,\dots,0,1,1,0)$ and
$(1,0,\dots,0,0,1,1)$ as distinct optimal discrete Morse vectors.
  (3) We give a first explicit example of a (non-PL) $5$-manifold, with face
vector $f=(5013,72300,290944,$ $495912,383136,110880)$, that is collapsible but
not homeomorphic to a ball.
  Furthermore, we discuss possible improvements and drawbacks of random
approaches to collapsibility and discrete Morse theory. We will introduce
randomized versions \texttt{random-lex-first} and \texttt{random-lex-last} of
the \texttt{lex-first} and \texttt{lex-last} discrete Morse strategies of
\cite{BenedettiLutz2014}, respectively --- and we will see that in many
instances the \texttt{random-lex-last} strategy works significantly better than
Benedetti--Lutz's (uniform) \texttt{random} strategy.
  On the theoretical side, we prove that after repeated barycentric
subdivisions, the discrete Morse vectors found by randomized algorithms have,
on average, an exponential (in the number of barycentric subdivisions) number
of critical cells asymptotically almost surely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4246</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4246</id><created>2014-04-16</created><authors><author><keyname>Stulova</keyname><forenames>Nataliia</forenames></author><author><keyname>Morales</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>An Approach to Assertion-based Debugging of Higher-Order (C)LP Programs</title><categories>cs.PL</categories><comments>24 pages, 1 figure. A 2-page extended abstract to be published as a
  technical communication in the on-line addendum of the special issue(s) of
  the TPLP journal for ICLP14</comments><report-no>CLIP-1/2014.0</report-no><acm-class>D.1.6; D.2.4; D.3.3; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order constructs extend the expressiveness of first-order (Constraint)
Logic Programming ((C)LP) both syntactically and semantically. At the same time
assertions have been in use for some time in (C)LP systems helping programmers
detect errors and validate programs. However, these assertion-based extensions
to (C)LP have not been integrated well with higher-order to date. This paper
contributes to filling this gap by extending the assertion-based approach to
error detection and program validation to the higher-order context within
(C)LP. We propose an extension of properties and assertions as used in (C)LP in
order to be able to fully describe arguments that are predicates. The extension
makes the full power of the assertion language available when describing
higher-order arguments. We provide syntax and semantics for (higher-order)
properties and assertions, as well as for programs which contain such
assertions, including the notions of error and partial correctness and provide
some formal results. We also discuss several alternatives for performing
run-time checking of such programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4249</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4249</id><created>2014-04-16</created><authors><author><keyname>Berger</keyname><forenames>Annabell</forenames></author><author><keyname>Rechner</keyname><forenames>Steffen</forenames></author></authors><title>Broder's Chain Is Not Rapidly Mixing</title><categories>cs.DM</categories><comments>Keywords: sampling of matchings, rapidly mixing Markov chains,
  permanent of a matrix, random generation, monomer-dimer systems, Markov chain
  Monte Carlo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that Broder's Markov chain for approximate sampling near-perfect and
perfect matchings is not rapidly mixing for Hamiltonian, regular, threshold and
planar bipartite graphs, filling a gap in the literature. In the second part we
experimentally compare Broder's chain with the Markov chain by Jerrum, Sinclair
and Vigoda from 2004. For the first time, we provide a systematic experimental
investigation of mixing time bounds for these Markov chains. We observe that
the exact total mixing time is in many cases significantly lower than known
upper bounds using canonical path or multicommodity flow methods, even if the
structure of an underlying state graph is known. In contrast we observe
comparatively tighter upper bounds using spectral gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4250</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4250</id><created>2014-04-16</created><updated>2015-11-30</updated><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Witness structures and immediate snapshot complexes</title><categories>cs.DC</categories><comments>27 pages, full paper version of the 1st part of the preprint
  arXiv:1402.4707</comments><msc-class>57-XX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and study a new family of combinatorial simplicial
complexes, which we call immediate snapshot complexes. Our construction and
terminology is strongly motivated by theoretical distributed computing, as
these complexes are combinatorial models of the standard protocol complexes
associated to immediate snapshot read/write shared memory communication model.
In order to define the immediate snapshot complexes we need a new combinatorial
object, which we call a witness structure. These objects are indexing the
simplices in the immediate snapshot complexes, while a special operation on
them, called ghosting, describes the combinatorics of taking simplicial
boundary. In general, we develop the theory of witness structures and use it to
prove several combinatorial as well as topological properties of the immediate
snapshot complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4258</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4258</id><created>2014-04-16</created><updated>2014-04-24</updated><authors><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author><author><keyname>Geer</keyname><forenames>Connor</forenames></author><author><keyname>Piekut</keyname><forenames>David</forenames></author></authors><title>An Analysis of State-Relevance Weights and Sampling Distributions on
  L1-Regularized Approximate Linear Programming Approximation Accuracy</title><categories>cs.AI</categories><comments>Identical to the ICML 2014 paper of the same name, but with full
  proofs. Please cite the ICML paper</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Recent interest in the use of $L_1$ regularization in the use of value
function approximation includes Petrik et al.'s introduction of
$L_1$-Regularized Approximate Linear Programming (RALP). RALP is unique among
$L_1$-regularized approaches in that it approximates the optimal value function
using off-policy samples. Additionally, it produces policies which outperform
those of previous methods, such as LSPI. RALP's value function approximation
quality is affected heavily by the choice of state-relevance weights in the
objective function of the linear program, and by the distribution from which
samples are drawn; however, there has been no discussion of these
considerations in the previous literature. In this paper, we discuss and
explain the effects of choices in the state-relevance weights and sampling
distribution on approximation quality, using both theoretical and experimental
illustrations. The results provide insight not only onto these effects, but
also provide intuition into the types of MDPs which are especially well suited
for approximation with RALP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4273</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4273</id><created>2014-04-16</created><authors><author><keyname>Guo</keyname><forenames>Alan</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>List decoding group homomorphisms between supersolvable groups</title><categories>cs.IT cs.CC math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the set of homomorphisms between two supersolvable groups can be
locally list decoded up to the minimum distance of the code, extending the
results of Dinur et al who studied the case where the groups are abelian.
Moreover, when specialized to the abelian case, our proof is more streamlined
and gives a better constant in the exponent of the list size. The constant is
improved from about 3.5 million to 105.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4274</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4274</id><created>2014-04-16</created><updated>2014-05-29</updated><authors><author><keyname>Ahmetaj</keyname><forenames>Shqiponja</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Ortiz</keyname><forenames>Magdalena</forenames></author><author><keyname>Simkus</keyname><forenames>Mantas</forenames></author></authors><title>Managing Change in Graph-structured Data Using Description Logics (long
  version with appendix)</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the setting of graph-structured data that evolves
as a result of operations carried out by users or applications. We study
different reasoning problems, which range from ensuring the satisfaction of a
given set of integrity constraints after a given sequence of updates, to
deciding the (non-)existence of a sequence of actions that would take the data
to an (un)desirable state, starting either from a specific data instance or
from an incomplete description of it. We consider an action language in which
actions are finite sequences of conditional insertions and deletions of nodes
and labels, and use Description Logics for describing integrity constraints and
(partial) states of the data. We then formalize the above data management
problems as a static verification problem and several planning problems. We
provide algorithms and tight complexity bounds for the formalized problems,
both for an expressive DL and for a variant of DL-Lite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4275</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4275</id><created>2014-04-15</created><updated>2014-06-20</updated><authors><author><keyname>Xiaochao</keyname><forenames>Qian</forenames></author></authors><title>A Bitcoin system with no mining and no history transactions: Build a
  compact Bitcoin system</title><categories>cs.CE cs.CR q-fin.GN</categories><comments>Add some details about the client load, an explicit definition of
  decentralization, APPENDIX B</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are some alternative Cryptocurrency systems which claim that they are
based on PoS are actually based on PoSTW which denotes the Proof of
Stake(coin), Time(day) and Work(hashing), while the other pure PoS
Cryptocurrency systems are actually centralized. In this paper we propose a new
framework of Cryptocurrency system. The major parts what we have changed
include, a fast transparent distribution solution which can avoid deceptions
between the sponsor and the audience, removing the bloated history transactions
from data synchronization, no mining, no blockchain, it's environmentally
friendly, no checkpoint, no exchange hub needed, it's truly decentralized and
purely based on proof of stake. The logic is very simple and intuitive, 51% of
stakes talk. The highlight of this paper is a proposal of a new concise data
synchronization mechanism named &quot;Converged Consensus&quot; which ensures the system
reaches a consistent distributed consensus. We think the famous blockchain
mechanism based on PoW is no longer an essential element of a Cryptocurrency
system. In aspect of security, we propose TILP &amp; SSS strategies to secure our
system. At the end, we try to give an explicit definition of decentralization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4282</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4282</id><created>2014-04-16</created><updated>2016-01-19</updated><authors><author><keyname>Bossy</keyname><forenames>Mireille</forenames></author><author><keyname>Espina</keyname><forenames>Jose</forenames></author><author><keyname>Morice</keyname><forenames>Jacques</forenames></author><author><keyname>Paris</keyname><forenames>Cristian</forenames></author><author><keyname>Rousseau</keyname><forenames>Antoine</forenames></author></authors><title>Modeling the wind circulation around mills with a Lagrangian stochastic
  approach</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims at introducing model methodology and numerical studies related
to a Lagrangian stochastic approach applied to the computation of the wind
circulation around mills. We adapt the Lagrangian stochastic downscaling method
that we have introduced in [3] and [4] to the atmospheric boundary layer and we
introduce here a Lagrangian version of the actuator disc methods to take
account of the mills. We present our numerical method and numerical experiments
in the case of non rotating and rotating actuator disc models. We also present
some features of our numerical method, in particular the computation of the
probability distribution of the wind in the wake zone, as a byproduct of the
fluid particle model and the associated PDF method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4286</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4286</id><created>2014-04-16</created><authors><author><keyname>Bidgoli</keyname><forenames>Behrouz Minaei</forenames></author><author><keyname>Nazaridoust</keyname><forenames>Maryam</forenames></author></authors><title>Case study: Data Mining of Associate Degree Accepted Candidates by
  Modular Method</title><categories>cs.DB</categories><comments>8 pages, 8 figures, 2 tabales. http://www.SciRP.org/journal/cn</comments><journal-ref>Communications and Network, 2012, 4, 189-268</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since about 10 years ago, University of Applied Science and Technology (UAST)
in Iran has admitted students in discontinuous associate degree by modular
method, so that almost 100,000 students are accepted every year. Although the
first aim of holding such courses was to improve scientific and skill level of
employees, over time a considerable group of unemployed people have been
interested to participate in these courses. According to this fact, in this
paper, we mine and analyze a sample data of accepted candidates in modular 2008
and 2009 courses by using unsupervised and supervised learning paradigms. In
the first step, by using unsupervised paradigm, we grouped (clustered) set of
modular accepted candidates based on their student status and labeled data sets
by three classes so that each class somehow shows educational and student
status of modular accepted candidates. In the second step, by using supervised
and unsupervised algorithms, we generated predicting models in 2008 data sets.
Then, by making a comparison between performances of generated models, we
selected predicting model of association rules through which some rules were
extracted. Finally, this model is executed for Test set which includes accepted
candidates of next course then by evaluation of results, the percentage of
correctness and confidentiality of obtained results can be viewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4304</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4304</id><created>2014-04-16</created><authors><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Otepka</keyname><forenames>Johannes</forenames></author><author><keyname>Pfeifer</keyname><forenames>Norbert</forenames></author><author><keyname>Ghuffar</keyname><forenames>Sajid</forenames></author><author><keyname>Korzeniowska</keyname><forenames>Karolina</forenames></author><author><keyname>Wagner</keyname><forenames>Gerald</forenames></author></authors><title>Automated Classification of Airborne Laser Scanning Point Clouds</title><categories>cs.CE cs.AI</categories><journal-ref>In: Solving Computationally Expensive Engineering Problems.
  Springer Proceedings in Mathematics &amp; Statistics Volume 97: 269-292. 2014</journal-ref><doi>10.1007/978-3-319-08985-0_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making sense of the physical world has always been at the core of mapping. Up
until recently, this has always dependent on using the human eye. Using
airborne lasers, it has become possible to quickly &quot;see&quot; more of the world in
many more dimensions. The resulting enormous point clouds serve as data sources
for applications far beyond the original mapping purposes ranging from flooding
protection and forestry to threat mitigation. In order to process these large
quantities of data, novel methods are required. In this contribution, we
develop models to automatically classify ground cover and soil types. Using the
logic of machine learning, we critically review the advantages of supervised
and unsupervised methods. Focusing on decision trees, we improve accuracy by
including beam vector components and using a genetic algorithm. We find that
our approach delivers consistently high quality classifications, surpassing
classical methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4312</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4312</id><created>2014-04-16</created><updated>2014-04-20</updated><authors><author><keyname>Du</keyname><forenames>Dong</forenames></author></authors><title>On Level persistence (Relevant level persistence numbers)</title><categories>cs.CG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1210.3092; and
  text overlap with arXiv:1104.5646 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this note is to describe of a new set of numerical invariants,
the relevant level persistence numbers, and make explicit their relationship
with the four types of bar codes, a more familiar set of complete invariants
for level persistence. The paper provides the opportunity to compare level
persistence with the persistence introduced by Edelsbrunner-
Letscher-Zomorodian called in this paper, as sub-level persistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4314</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4314</id><created>2014-04-16</created><authors><author><keyname>Kong</keyname><forenames>Lingpeng</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>An Empirical Comparison of Parsing Methods for Stanford Dependencies</title><categories>cs.CL</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stanford typed dependencies are a widely desired representation of natural
language sentences, but parsing is one of the major computational bottlenecks
in text analysis systems. In light of the evolving definition of the Stanford
dependencies and developments in statistical dependency parsing algorithms,
this paper revisits the question of Cer et al. (2010): what is the tradeoff
between accuracy and speed in obtaining Stanford dependencies in particular? We
also explore the effects of input representations on this tradeoff:
part-of-speech tags, the novel use of an alternative dependency representation
as input, and distributional representaions of words. We find that direct
dependency parsing is a more viable solution than it was found to be in the
past. An accompanying software release can be found at:
http://www.ark.cs.cmu.edu/TBSD
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4316</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4316</id><created>2014-04-16</created><authors><author><keyname>Zou</keyname><forenames>Will Y.</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Sun</keyname><forenames>Miao</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author></authors><title>Generic Object Detection With Dense Neural Patterns and Regionlets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the challenge of establishing a bridge between deep
convolutional neural networks and conventional object detection frameworks for
accurate and efficient generic object detection. We introduce Dense Neural
Patterns, short for DNPs, which are dense local features derived from
discriminatively trained deep convolutional neural networks. DNPs can be easily
plugged into conventional detection frameworks in the same way as other dense
local features(like HOG or LBP). The effectiveness of the proposed approach is
demonstrated with the Regionlets object detection framework. It achieved 46.1%
mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL
VOC 2010 dataset, which dramatically improves the original Regionlets approach
without DNPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4326</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4326</id><created>2014-04-16</created><authors><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Usunier</keyname><forenames>Nicolas</forenames></author></authors><title>Open Question Answering with Weakly Supervised Embedding Models</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building computers able to answer questions on any subject is a long standing
goal of artificial intelligence. Promising progress has recently been achieved
by methods that learn to map questions to logical forms or database queries.
Such approaches can be effective but at the cost of either large amounts of
human-labeled data or by defining lexicons and grammars tailored by
practitioners. In this paper, we instead take the radical approach of learning
to map questions to vectorial feature representations. By mapping answers into
the same space one can query any knowledge base independent of its schema,
without requiring any grammar or lexicon. Our method is trained with a new
optimization procedure combining stochastic gradient descent followed by a
fine-tuning step using the weak supervision provided by blending automatically
and collaboratively generated resources. We empirically demonstrate that our
model can capture meaningful signals from its noisy supervision leading to
major improvements over paralex, the only existing method able to be trained on
similar weakly labeled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4334</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4334</id><created>2014-04-15</created><authors><author><keyname>Marzilli</keyname><forenames>Colleen</forenames></author><author><keyname>Delello</keyname><forenames>Julie</forenames></author><author><keyname>Marmion</keyname><forenames>Shelly</forenames></author><author><keyname>McWhorter</keyname><forenames>Rochell</forenames></author><author><keyname>Roberts</keyname><forenames>Paul</forenames></author><author><keyname>Marzilli</keyname><forenames>T. Scott</forenames></author></authors><title>Faculty Attitudes Towards Integrating Technology and Innovation</title><categories>cs.CY</categories><comments>20 pages</comments><journal-ref>International Journal on Integrating Technology in Innovation,
  3(1) 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Technological innovation is an important aspect of teaching and learning in
the 21st century. This article examines faculty attitudes toward technology use
in the classroom at one regional public university in the United States.
Building on a faculty-led initiative to develop a Community of Practice for
improving education, this study used a mixed-method approach of a
faculty-developed, electronic survey to assess this topic. Findings from 72
faculty members revealed an overall positive stance toward technology in the
classroom and the average faculty member utilized about six technology tools in
their courses. The opportunities, barriers and future uses for technologies in
the higher education classroom emerged from the open-ended questions on the
survey. One finding of particular concern is that faculty are fearful that
technology causes a loss of the humanistic perspective in education. The
university is redesigning ten of its most popular courses to increase
flexibility, accessibility and student success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4344</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4344</id><created>2014-04-16</created><updated>2015-02-22</updated><authors><author><keyname>Berenbrink</keyname><forenames>Petra</forenames><affiliation>SFU.ca</affiliation></author><author><keyname>Klasing</keyname><forenames>Ralf</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>INRIA Paris-Rocquencourt, LIAFA</affiliation></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames><affiliation>SFU.ca, ENS Paris</affiliation></author><author><keyname>Uznanski</keyname><forenames>Przemyslaw</forenames></author></authors><title>Improved Analysis of Deterministic Load-Balancing Schemes</title><categories>cs.DS</categories><comments>minor corrections; updated literature overview</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of deterministic load balancing of tokens in the
discrete model. A set of $n$ processors is connected into a $d$-regular
undirected network. In every time step, each processor exchanges some of its
tokens with each of its neighbors in the network. The goal is to minimize the
discrepancy between the number of tokens on the most-loaded and the
least-loaded processor as quickly as possible.
  Rabani et al. (1998) present a general technique for the analysis of a wide
class of discrete load balancing algorithms. Their approach is to characterize
the deviation between the actual loads of a discrete balancing algorithm with
the distribution generated by a related Markov chain. The Markov chain can also
be regarded as the underlying model of a continuous diffusion algorithm. Rabani
et al. showed that after time $T = O(\log (Kn)/\mu)$, any algorithm of their
class achieves a discrepancy of $O(d\log n/\mu)$, where $\mu$ is the spectral
gap of the transition matrix of the graph, and $K$ is the initial load
discrepancy in the system.
  In this work we identify some natural additional conditions on deterministic
balancing algorithms, resulting in a class of algorithms reaching a smaller
discrepancy. This class contains well-known algorithms, eg., the Rotor-Router.
Specifically, we introduce the notion of cumulatively fair load-balancing
algorithms where in any interval of consecutive time steps, the total number of
tokens sent out over an edge by a node is the same (up to constants) for all
adjacent edges. We prove that algorithms which are cumulatively fair and where
every node retains a sufficient part of its load in each step, achieve a
discrepancy of $O(\min\{d\sqrt{\log n/\mu},d\sqrt{n}\})$ in time $O(T)$. We
also show that in general neither of these assumptions may be omitted without
increasing discrepancy. We then show by a combinatorial potential reduction
argument that any cumulatively fair scheme satisfying some additional
assumptions achieves a discrepancy of $O(d)$ almost as quickly as the
continuous diffusion process. This positive result applies to some of the
simplest and most natural discrete load balancing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4350</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4350</id><created>2014-04-16</created><updated>2015-05-12</updated><authors><author><keyname>Gattami</keyname><forenames>Ather</forenames></author></authors><title>Kalman meets Shannon</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communicating the state of a dynamical system via
a Shannon Gaussian channel. The receiver, which acts as both a decoder and
estimator, observes the noisy measurement of the channel output and makes an
optimal estimate of the state of the dynamical system in the minimum mean
square sense. The transmitter observes a possibly noisy measurement of the
state of the dynamical system. These measurements are then used to encode the
message to be transmitted over a noisy Gaussian channel, where a per sample
power constraint is imposed on the transmitted message. Thus, we get a mixed
problem of Shannon's source-channel coding problem and a sort of Kalman
filtering problem. We first consider the problem of communication with full
state measurements at the transmitter and show that optimal linear encoders
don't need to have memory and the optimal linear decoders have an order of at
most that of the state dimension. We also give explicitly the structure of the
optimal linear filters. For the case where the transmitter has access to noisy
measurements of the state, we derive a separation principle for the optimal
communication scheme, where the transmitter needs a filter with an order of at
most the dimension of the state of the dynamical system. The results are
derived for first order linear dynamical systems, but may be extended to MIMO
systems with arbitrary order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4351</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4351</id><created>2014-04-16</created><authors><author><keyname>Misra</keyname><forenames>Navodit</forenames></author><author><keyname>Kuruoglu</keyname><forenames>Ercan E.</forenames></author></authors><title>Stable Graphical Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stable random variables are motivated by the central limit theorem for
densities with (potentially) unbounded variance and can be thought of as
natural generalizations of the Gaussian distribution to skewed and heavy-tailed
phenomenon. In this paper, we introduce stable graphical (SG) models, a class
of multivariate stable densities that can also be represented as Bayesian
networks whose edges encode linear dependencies between random variables. One
major hurdle to the extensive use of stable distributions is the lack of a
closed-form analytical expression for their densities. This makes penalized
maximum-likelihood based learning computationally demanding. We establish
theoretically that the Bayesian information criterion (BIC) can asymptotically
be reduced to the computationally more tractable minimum dispersion criterion
(MDC) and develop StabLe, a structure learning algorithm based on MDC. We use
simulated datasets for five benchmark network topologies to empirically
demonstrate how StabLe improves upon ordinary least squares (OLS) regression.
We also apply StabLe to microarray gene expression data for lymphoblastoid
cells from 727 individuals belonging to eight global population groups. We
establish that StabLe improves test set performance relative to OLS via
ten-fold cross-validation. Finally, we develop SGEX, a method for quantifying
differential expression of genes between different population groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4356</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4356</id><created>2014-04-16</created><updated>2014-04-30</updated><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author></authors><title>Phase transition in kinetic exchange opinion models with independence</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>10 pages, 4 figures, accepted for publication in Phys. Lett. A</comments><journal-ref>Phys. Lett. A 378, 1683 (2014)</journal-ref><doi>10.1016/j.physleta.2014.04.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the critical behavior of a three-state ($+1$, $-1$,
$0$) opinion model with independence. Each agent has a probability $q$ to act
as independent, i.e., he/she can choose his/her opinion independently of the
opinions of the other agents. On the other hand, with the complementary
probability $1-q$ the agent interacts with a randomly chosen individual through
a kinetic exchange. Our analytical and numerical results show that the
independence mechanism acts as a noise that induces an order-disorder
transition at critical points $q_{c}$ that depend on the individuals'
flexibility. For a special value of this flexibility the system undergoes a
transition to an absorbing state with all opinions $0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4384</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4384</id><created>2014-04-16</created><authors><author><keyname>Bariran</keyname><forenames>S. E. S.</forenames></author><author><keyname>Sahari</keyname><forenames>K. S. M.</forenames></author><author><keyname>Yunus</keyname><forenames>B.</forenames></author></authors><title>A novel interactive OBE approach in SCM pedagogy using beer game
  simulation theory</title><categories>cs.CY</categories><comments>Special Issue: International Conference on Teaching and Learning in
  Education, 2013</comments><journal-ref>International Journal of Asian Social Science, 2013,
  3(9):2034-2040</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary challenge in SCM pedagogy is the learners interaction with the
dynamic nature of supply chain transactions. Once achieved, it is also required
to evaluate learners learning experience based on their performance. In this
paper, a combination of outcome-based education (OBE) and simulation-based
education is proposed focusing on beer game theory. The analysis is based on
336 runs of beer game simulation within a target group of 56 participants
divided into 14 subgroups (SG1-SG14).The purpose of the study is mainly to
investigate the effect of mutual interactions on students learning process
using supply chain total cost and ordering fluctuations as critical measurement
criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4386</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4386</id><created>2014-04-16</created><authors><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Mehta</keyname><forenames>Prashant G.</forenames></author></authors><title>Probabilistic Data Association-Feedback Particle Filter for Multiple
  Target Tracking Applications</title><categories>math.PR cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the problem of tracking single or multiple
targets with multiple non-target specific observations (measurements). For such
filtering problems with data association uncertainty, a novel feedback
control-based particle filter algorithm is introduced. The algorithm is
referred to as the probabilistic data association-feedback particle filter
(PDA-FPF). The proposed filter is shown to represent a generalization to the
nonlinear non-Gaussian case of the classical Kalman filter-based probabilistic
data association filter (PDAF). One remarkable conclusion is that the proposed
PDA-FPF algorithm retains the innovation error-based feedback structure of the
classical PDAF algorithm, even in the nonlinear non-Gaussian case. The
theoretical results are illustrated with the aid of numerical examples
motivated by multiple target tracking applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4388</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4388</id><created>2014-04-16</created><authors><author><keyname>Chang</keyname><forenames>Yanling</forenames></author><author><keyname>Erera</keyname><forenames>Alan L.</forenames></author><author><keyname>White</keyname><forenames>Chelsea C.</forenames><suffix>III</suffix></author></authors><title>Partially Observed, Multi-objective Markov Games</title><categories>math.OC cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intent of this research is to generate a set of non-dominated policies
from which one of two agents (the leader) can select a most preferred policy to
control a dynamic system that is also affected by the control decisions of the
other agent (the follower). The problem is described by an infinite horizon,
partially observed Markov game (POMG). At each decision epoch, each agent
knows: its past and present states, its past actions, and noise corrupted
observations of the other agent's past and present states. The actions of each
agent are determined at each decision epoch based on these data. The leader
considers multiple objectives in selecting its policy. The follower considers a
single objective in selecting its policy with complete knowledge of and in
response to the policy selected by the leader. This leader-follower assumption
allows the POMG to be transformed into a specially structured, partially
observed Markov decision process (POMDP). This POMDP is used to determine the
follower's best response policy. A multi-objective genetic algorithm (MOGA) is
used to create the next generation of leader policies based on the fitness
measures of each leader policy in the current generation. Computing a fitness
measure for a leader policy requires a value determination calculation, given
the leader policy and the follower's best response policy. The policies from
which the leader can select a most preferred policy are the non-dominated
policies of the final generation of leader policies created by the MOGA. An
example is presented that illustrates how these results can be used to support
a manager of a liquid egg production process (the leader) in selecting a
sequence of actions to best control this process over time, given that there is
an attacker (the follower) who seeks to contaminate the liquid egg production
process with a chemical or biological toxin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4391</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4391</id><created>2014-04-16</created><authors><author><keyname>Zhang</keyname><forenames>Rick</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Control of Robotic Mobility-On-Demand Systems: a Queueing-Theoretical
  Perspective</title><categories>cs.RO cs.MA</categories><comments>10 pages, To appear at RSS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present and analyze a queueing-theoretical model for
autonomous mobility-on-demand (MOD) systems where robotic, self-driving
vehicles transport customers within an urban environment and rebalance
themselves to ensure acceptable quality of service throughout the entire
network. We cast an autonomous MOD system within a closed Jackson network model
with passenger loss. It is shown that an optimal rebalancing algorithm
minimizing the number of (autonomously) rebalancing vehicles and keeping
vehicles availabilities balanced throughout the network can be found by solving
a linear program. The theoretical insights are used to design a robust,
real-time rebalancing algorithm, which is applied to a case study of New York
City. The case study shows that the current taxi demand in Manhattan can be met
with about 8,000 robotic vehicles (roughly 60% of the size of the current taxi
fleet). Finally, we extend our queueing-theoretical setup to include congestion
effects, and we study the impact of autonomously rebalancing vehicles on
overall congestion. Collectively, this paper provides a rigorous approach to
the problem of system-wide coordination of autonomously driving vehicles, and
provides one of the first characterizations of the sustainability benefits of
robotic transportation networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4400</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4400</id><created>2014-04-16</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Farrell</keyname><forenames>Brendan</forenames></author></authors><title>Strong Divergence of Reconstruction Procedures for the Paley-Wiener
  Space $\mathcal{PW}^1_\pi$ and the Hardy Space $\mathcal{H}^1$</title><categories>cs.IT math.IT</categories><comments>Revision of a paper to appear in the Journal of Approximation Theory</comments><msc-class>94A20, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous results on certain sampling series have left open if divergence only
occurs for certain subsequences or, in fact, in the limit. Here we prove that
divergence occurs in the limit.
  We consider three canonical reconstruction methods for functions in the
Paley-Wiener space $\mathcal{PW}^1_\pi$. For each of these we prove an instance
when the reconstruction diverges in the limit. This is a much stronger
statement than previous results that provide only $\limsup$ divergence. We also
address reconstruction for functions in the Hardy space $\mathcal{H}^1$ and
show that for any subsequence of the natural numbers there exists a function in
$\mathcal{H}^1$ for which reconstruction diverges in $\limsup$. For two of
these sampling series we show that when divergence occurs, the sampling series
has strong oscillations so that the maximum and the minimum tend to positive
and negative infinity. Our results are of interest in functional analysis
because they go beyond the type of result that can be obtained using the
Banach-Steinhaus Theorem. We discuss practical implications of this work; in
particular the work shows that methods using specially chosen subsequences of
reconstructions cannot yield convergence for the Paley-Wiener Space
$\mathcal{PW}^1_\pi$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4410</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4410</id><created>2014-04-16</created><updated>2016-01-04</updated><authors><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>Lewis</keyname><forenames>Robert Y.</forenames></author><author><keyname>Roux</keyname><forenames>Cody</forenames></author></authors><title>A heuristic prover for real inequalities</title><categories>cs.MS cs.LO</categories><acm-class>I.1.2; G.4; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a general method for verifying inequalities between real-valued
expressions, especially the kinds of straightforward inferences that arise in
interactive theorem proving. In contrast to approaches that aim to be complete
with respect to a particular language or class of formulas, our method
establishes claims that require heterogeneous forms of reasoning, relying on a
Nelson-Oppen-style architecture in which special-purpose modules collaborate
and share information. The framework is thus modular and extensible. A
prototype implementation shows that the method works well on a variety of
examples, and complements techniques that are used by contemporary interactive
provers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4412</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4412</id><created>2014-04-16</created><updated>2015-09-16</updated><authors><author><keyname>Zhou</keyname><forenames>Guoxu</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author></authors><title>Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness</title><categories>cs.LG cs.CV stat.ML</categories><comments>appears in IEEE Transactions on Image Processing, 2015</comments><doi>10.1109/TIP.2015.2478396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction
of nonnegative parts-based and physically meaningful latent components from
high-dimensional tensor data while preserving the natural multilinear structure
of data. However, as the data tensor often has multiple modes and is
large-scale, existing NTD algorithms suffer from a very high computational
complexity in terms of both storage and computation time, which has been one
major obstacle for practical applications of NTD. To overcome these
disadvantages, we show how low (multilinear) rank approximation (LRA) of
tensors is able to significantly simplify the computation of the gradients of
the cost function, upon which a family of efficient first-order NTD algorithms
are developed. Besides dramatically reducing the storage complexity and running
time, the new algorithms are quite flexible and robust to noise because any
well-established LRA approaches can be applied. We also show how nonnegativity
incorporating sparsity substantially improves the uniqueness property and
partially alleviates the curse of dimensionality of the Tucker decompositions.
Simulation results on synthetic and real-world data justify the validity and
high efficiency of the proposed NTD algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4416</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4416</id><created>2014-04-16</created><authors><author><keyname>Kamae</keyname><forenames>Teturo</forenames></author><author><keyname>Kim</keyname><forenames>Dong Han</forenames></author></authors><title>A characterization of eventually periodicity</title><categories>cs.CC math.DS</categories><comments>11 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we show that the Kamae-Xue complexity function for an
infinite sequence classifies eventual periodicity completely. We prove that an
infinite binary word $x_1x_2 \cdots $ is eventually periodic if and only if
$\Sigma(x_1x_2\cdots x_n)/n^3$ has a positive limit, where $\Sigma(x_1x_2\cdots
x_n)$ is the sum of the squares of all the numbers of appearance of finite
words in $x_1 x_2 \cdots x_n$, which was introduced by Kamae-Xue as a criterion
of randomness in the sense that $x_1x_2\cdots x_n$ is more random if
$\Sigma(x_1x_2\cdots x_n)$ is smaller. In fact, it is known that the lower
limit of $\Sigma(x_1x_2\cdots x_n) /n^2 $ is at least 3/2 for any sequence
$x_1x_2 \cdots$, while the limit exists as 3/2 almost surely for the
$(1/2,1/2)$ product measure. For the other extreme, the upper limit of
$\Sigma(x_1x_2\cdots x_n)/n^3$ is bounded by 1/3. There are sequences which are
not eventually periodic but the lower limit of $\Sigma(x_1x_2\cdots x_n)/n^3$
is positive, while the limit does not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4420</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4420</id><created>2014-04-16</created><updated>2014-07-16</updated><authors><author><keyname>Diaz</keyname><forenames>Mario</forenames></author><author><keyname>P&#xe9;rez-Abreu</keyname><forenames>V&#xed;ctor</forenames></author></authors><title>Random Matrix Systems with Block-Based Behavior and Operator-Valued
  Models</title><categories>math.PR cs.IT math.IT</categories><comments>29 pages, 3 figures, title changed and some points raised by
  colleagues were addressed. New proof in Appendix B</comments><msc-class>60C20, 46l53</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model to estimate the asymptotic isotropic mutual information of a
multiantenna channel is considered. Using a block-based dynamics and the angle
diversity of the system, we derived what may be thought of as the
operator-valued version of the Kronecker correlation model. This model turns
out to be more flexible than the classical version, as it incorporates both an
arbitrary channel correlation and the correlation produced by the asymptotic
antenna patterns. A method to calculate the asymptotic isotropic mutual
information of the system is established using operator-valued free probability
tools. A particular case is considered in which we start with explicit Cauchy
transforms and all the computations are done with diagonal matrices, which make
the implementation simpler and more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4435</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4435</id><created>2014-04-17</created><updated>2014-04-18</updated><authors><author><keyname>Ranganathan</keyname><forenames>Aanjhan</forenames></author><author><keyname>Danev</keyname><forenames>Boris</forenames></author><author><keyname>Capkun</keyname><forenames>Srdjan</forenames></author></authors><title>Low-power Distance Bounding</title><categories>cs.CR</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distance bounding system guarantees an upper bound on the physical distance
between a verifier and a prover. However, in contrast to a conventional
wireless communication system, distance bounding systems introduce tight
requirements on the processing delay at the prover and require high distance
measurement precision making their practical realization challenging. Prior
proposals of distance bounding systems focused primarily on building provers
with minimal processing delays but did not consider the power limitations of
provers and verifiers. However, in a wide range of applications (e.g., physical
access control), provers are expected to be fully or semi-passive introducing
additional constraints on the design and implementation of distance bounding
systems.
  In this work, we propose a new physical layer scheme for distance bounding
and leverage this scheme to implement a distance bounding system with a
low-power prover. Our physical layer combines frequency modulated continuous
wave (FMCW) and backscatter communication. The use of backscatter communication
enables low power consumption at the prover which is critical for a number of
distance bounding applications. By using the FMCW-based physical layer, we
further decouple the physical distance estimation from the processing delay at
the prover, thereby enabling the realization of the majority of distance
bounding protocols developed in prior art. We evaluate our system under various
attack scenarios and show that it offers strong security guarantees against
distance, mafia and terrorist frauds. Additionally, we validate the
communication and distance measurement characteristics of our system through
simulations and experiments and show that it is well suited for short-range
physical access control and payment applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4443</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4443</id><created>2014-04-17</created><authors><author><keyname>Abu-Shaban</keyname><forenames>Zohair</forenames></author><author><keyname>R</keyname><forenames>Bhavani Shankar M.</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Enhanced List-Based Group-Wise Overloaded Receiver with Application to
  Satellite Reception</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The market trends towards the use of smaller dish antennas for TV satellite
receivers, as well as the growing density of broadcasting satellites in orbit
require the application of robust adjacent satellite interference (ASI)
cancellation algorithms at the receivers. The wider beamwidth of a small size
dish and the growing number of satellites in orbit impose an overloaded
scenario, i.e., a scenario where the number of transmitting satellites exceeds
the number of receiving antennas. For such a scenario, we present a two stage
receiver to enhance signal detection from the satellite of interest, i.e., the
satellite that the dish is pointing to, while reducing interference from
neighboring satellites. Towards this objective, we propose an enhanced
List-based Group-wise Search Detection (LGSD) receiver architecture that takes
into account the spatially correlated additive noise and uses the
signal-to-interference-plus noise ratio (SINR) maximization criterion to
improve detection performance. Simulations show that the proposed receiver
structure enhances the performance of satellite systems in the presence of ASI
when compared to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4448</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4448</id><created>2014-04-17</created><authors><author><keyname>Abu-Shaban</keyname><forenames>Zohair</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Grotz</keyname><forenames>Joel</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Overloaded Satellite Receiver Using SIC with Hybrid Beamforming and ML
  Detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new receiver structure that is intended to detect the
signals from multiple adjacent satellites in the presence of other interfering
satellites is proposed. We tackle the worst case interference conditions, i.e.,
it is assumed that uncoded signals that fully overlap in frequency arrive at a
multiple-element small-size parabolic antenna in a spatially correlated noise
environment. The proposed successive interference cancellation (SIC) receiver,
denoted by SIC Hy/ML, employs hybrid beamforming and disjoint maximum
likelihood (ML) detection. Depending on the individual signals spatial
position, the proposed SIC Hy/ML scheme takes advantage of two types of
beamformers: a maximum ratio combining (MRC) beamformer and a compromised array
response (CAR) beamformer. The performance of the proposed receiver is compared
to an SIC receiver that uses only MRC beamforming scheme with ML detection for
all signals, a joint ML detector, and a minimum mean square error detector. It
is found that SIC Hy/ML outperforms the other schemes by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4453</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4453</id><created>2014-04-17</created><authors><author><keyname>Mejri</keyname><forenames>Asma</forenames></author><author><keyname>Othman</keyname><forenames>Ghaya Rekaya-Ben</forenames></author></authors><title>Efficient Decoding Algorithms for the Compute-and-Forward Strategy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address in this paper decoding aspects of the Compute-and-Forward (CF)
physical-layer network coding strategy. It is known that the original decoder
for the CF is asymptotically optimal. However, its performance gap to optimal
decoders in practical settings are still not known. In this work, we develop
and assess the performance of novel decoding algorithms for the CF operating in
the multiple access channel. For the fading channel, we analyze the ML decoder
and develop a novel diophantine approximation-based decoding algorithm showed
numerically to outperform the original CF decoder. For the Gaussian channel, we
investigate the maximum a posteriori (MAP) decoder. We derive a novel MAP
decoding metric and develop practical decoding algorithms proved numerically to
outperform the original one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4465</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4465</id><created>2014-04-17</created><authors><author><keyname>Merz</keyname><forenames>Florian</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author></authors><title>PReaCH: A Fast Lightweight Reachability Index using Pruning and
  Contraction Hierarchies</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the data structure PReaCH (for Pruned Reachability Contraction
Hierarchies) which supports reachability queries in a directed graph, i.e., it
supports queries that ask whether two nodes in the graph are connected by a
directed path. PReaCH adapts the contraction hierarchy speedup techniques for
shortest path queries to the reachability setting. The resulting approach is
surprisingly simple and guarantees linear space and near linear preprocessing
time. Orthogonally to that, we improve existing pruning techniques for the
search by gathering more information from a single DFS-traversal of the graph.
PReaCH-indices significantly outperform previous data structures with
comparable preprocessing cost. Methods with faster queries need significantly
more preprocessing time in particular for the most difficult instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4467</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4467</id><created>2014-04-17</created><updated>2014-07-22</updated><authors><author><keyname>Schwarzenberg</keyname><forenames>Robert</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author></authors><title>Cube-Cut: Vertebral Body Segmentation in MRI-Data through Cubic-Shaped
  Divergences</title><categories>cs.CV</categories><comments>23 figures, 2 tables, 43 references, PLoS ONE 9(4): e93389</comments><doi>10.1371/journal.pone.0093389</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a graph-based method using a cubic template for
volumetric segmentation of vertebrae in magnetic resonance imaging (MRI)
acquisitions. The user can define the degree of deviation from a regular cube
via a smoothness value Delta. The Cube-Cut algorithm generates a directed graph
with two terminal nodes (s-t-network), where the nodes of the graph correspond
to a cubic-shaped subset of the image's voxels. The weightings of the graph's
terminal edges, which connect every node with a virtual source s or a virtual
sink t, represent the affinity of a voxel to the vertebra (source) and to the
background (sink). Furthermore, a set of infinite weighted and non-terminal
edges implements the smoothness term. After graph construction, a minimal
s-t-cut is calculated within polynomial computation time, which splits the
nodes into two disjoint units. Subsequently, the segmentation result is
determined out of the source-set. A quantitative evaluation of a C++
implementation of the algorithm resulted in an average Dice Similarity
Coefficient (DSC) of 81.33% and a running time of less than a minute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4468</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4468</id><created>2014-04-17</created><authors><author><keyname>Hannula</keyname><forenames>Miika</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>Link</keyname><forenames>Sebastian</forenames></author></authors><title>On Independence Atoms and Keys</title><categories>cs.DB cs.LO</categories><msc-class>68P15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uniqueness and independence are two fundamental properties of data. Their
enforcement in database systems can lead to higher quality data, faster data
service response time, better data-driven decision making and knowledge
discovery from data. The applications can be effectively unlocked by providing
efficient solutions to the underlying implication problems of keys and
independence atoms. Indeed, for the sole class of keys and the sole class of
independence atoms the associated finite and general implication problems
coincide and enjoy simple axiomatizations. However, the situation changes
drastically when keys and independence atoms are combined. We show that the
finite and the general implication problems are already different for keys and
unary independence atoms. Furthermore, we establish a finite axiomatization for
the general implication problem, and show that the finite implication problem
does not enjoy a k-ary axiomatization for any k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4473</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4473</id><created>2014-04-17</created><updated>2014-07-04</updated><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>A Simple $O(\log\log(\mathrm{rank}))$-Competitive Algorithm for the
  Matroid Secretary Problem</title><categories>cs.DS</categories><comments>17 pages, 3 figures</comments><msc-class>68W27</msc-class><acm-class>G.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Only recently progress has been made in obtaining
$o(\log(\mathrm{rank}))$-competitive algorithms for the matroid secretary
problem. More precisely, Chakraborty and Lachish (2012) presented a
$O(\sqrt{\log(\mathrm{rank})})$-competitive procedure, and Lachish (2014) later
presented a $O(\log\log(\mathrm{rank}))$-competitive algorithm. Both these
algorithms and their analyses are very involved, which is also reflected in the
extremely high constants in their competitive ratios.
  Using different tools, we present a considerably simpler
$O(\log\log(\mathrm{rank}))$-competitive algorithm for the matroid secretary
problem. Our algorithm can be interpreted as a distribution over a simple type
of matroid secretary algorithms which are easy to analyze. Due to the
simplicity of our procedure, we are also able to vastly improve on the hidden
constant in the competitive ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4478</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4478</id><created>2014-04-17</created><authors><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author><author><keyname>Tesa&#x159;</keyname><forenames>Marek</forenames></author></authors><title>Rainbow Colouring of Split Graphs</title><categories>cs.DM cs.CC math.CO</categories><comments>This is the full version of a paper to be presented at ICGT 2014.
  This complements the results in arXiv:1205.1670 (which were presented in
  COCOON 2013), and both will be merged into a single journal submission</comments><msc-class>O5C15, 05C85 (Primary), 05C40 (Secondary)</msc-class><acm-class>G.2.2; F.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rainbow path in an edge coloured graph is a path in which no two edges are
coloured the same. A rainbow colouring of a connected graph G is a colouring of
the edges of G such that every pair of vertices in G is connected by at least
one rainbow path. The minimum number of colours required to rainbow colour G is
called its rainbow connection number. Between them, Chakraborty et al. [J.
Comb. Optim., 2011] and Ananth et al. [FSTTCS, 2012] have shown that for every
integer k, k \geq 2, it is NP-complete to decide whether a given graph can be
rainbow coloured using k colours.
  A split graph is a graph whose vertex set can be partitioned into a clique
and an independent set. Chandran and Rajendraprasad have shown that the problem
of deciding whether a given split graph G can be rainbow coloured using 3
colours is NP-complete and further have described a linear time algorithm to
rainbow colour any split graph using at most one colour more than the optimum
[COCOON, 2012]. In this article, we settle the computational complexity of the
problem on split graphs and thereby discover an interesting dichotomy.
Specifically, we show that the problem of deciding whether a given split graph
can be rainbow coloured using k colours is NP-complete for k \in {2,3}, but can
be solved in polynomial time for all other values of k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4484</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4484</id><created>2014-04-17</created><authors><author><keyname>Basavaraju</keyname><forenames>Manu</forenames></author><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Mathew</keyname><forenames>Rogers</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author></authors><title>Separation dimension of sparse graphs</title><categories>math.CO cs.DM</categories><comments>This is the full version of a paper to be presented at ICGT 2014.
  This is a subset of the results in arXiv:1212.6756</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The separation dimension of a graph $G$ is the smallest natural number $k$
for which the vertices of $G$ can be embedded in $\mathbb{R}^k$ such that any
pair of disjoint edges in $G$ can be separated by a hyperplane normal to one of
the axes. Equivalently, it is the smallest possible cardinality of a family
$\mathcal{F}$ of permutations of the vertices of $G$ such that for any two
disjoint edges of $G$, there exists at least one permutation in $\mathcal{F}$
in which all the vertices in one edge precede those in the other. In general,
the maximum separation dimension of a graph on $n$ vertices is $\Theta(\log
n)$. In this article, we focus on sparse graphs and show that the maximum
separation dimension of a $k$-degenerate graph on $n$ vertices is $O(k \log\log
n)$ and that there exists a family of $2$-degenerate graphs with separation
dimension $\Omega(\log\log n)$. We also show that the separation dimension of
the graph $G^{1/2}$ obtained by subdividing once every edge of another graph
$G$ is at most $(1 + o(1)) \log\log \chi(G)$ where $\chi(G)$ is the chromatic
number of the original graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4486</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4486</id><created>2014-04-17</created><updated>2014-04-18</updated><authors><author><keyname>Basavaraju</keyname><forenames>Manu</forenames></author><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Golumbic</keyname><forenames>Martin Charles</forenames></author><author><keyname>Mathew</keyname><forenames>Rogers</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author></authors><title>Boxicity and separation dimension</title><categories>math.CO cs.DM</categories><comments>This is the full version of a paper by the same name submitted to
  WG-2014. Some results proved in this paper are also present in
  arXiv:1212.6756. arXiv admin note: substantial text overlap with
  arXiv:1212.6756</comments><msc-class>05C65, 05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family $\mathcal{F}$ of permutations of the vertices of a hypergraph $H$ is
called 'pairwise suitable' for $H$ if, for every pair of disjoint edges in $H$,
there exists a permutation in $\mathcal{F}$ in which all the vertices in one
edge precede those in the other. The cardinality of a smallest such family of
permutations for $H$ is called the 'separation dimension' of $H$ and is denoted
by $\pi(H)$. Equivalently, $\pi(H)$ is the smallest natural number $k$ so that
the vertices of $H$ can be embedded in $\mathbb{R}^k$ such that any two
disjoint edges of $H$ can be separated by a hyperplane normal to one of the
axes. We show that the separation dimension of a hypergraph $H$ is equal to the
'boxicity' of the line graph of $H$. This connection helps us in borrowing
results and techniques from the extensive literature on boxicity to study the
concept of separation dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4495</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4495</id><created>2014-04-17</created><updated>2014-07-09</updated><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>Jir&#xe1;skov&#xe1;</keyname><forenames>Galina</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>On Upper and Lower Bounds on the Length of Alternating Towers</title><categories>cs.FL</categories><journal-ref>MFCS 2014, LNCS 8634, pp. 315-326</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tower between two regular languages is a sequence of strings such that all
strings on odd positions belong to one of the languages, all strings on even
positions belong to the other language, and each string can be embedded into
the next string in the sequence. It is known that if there are towers of any
length, then there also exists an infinite tower. We investigate upper and
lower bounds on the length of finite towers between two regular languages with
respect to the size of the automata representing the languages in the case
there is no infinite tower. This problem is relevant to the separation problem
of regular languages by piecewise testable languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4496</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4496</id><created>2014-04-17</created><authors><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Heren</keyname><forenames>Akif Cem</forenames></author><author><keyname>Tugcu</keyname><forenames>Tuna</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>3-D Channel Characteristics for Molecular Communications with an
  Absorbing Receiver</title><categories>cs.IT math.IT q-bio.MN</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the domain of molecular communications, researchers mimic the
techniques in nature to come up with alternative communication methods for
collaborating nanomachines. This work investigates the channel transfer
function for molecular communication via diffusion. In nature,
information-carrying molecules are generally absorbed by the target node via
receptors. Using the concentration function, without considering the absorption
process, as the channel transfer function implicitly assumes that the receiver
node does not affect the system. In this letter, we propose a solid analytical
formulation and analyze the signal metrics (attenuation and propagation delay)
for molecular communication via diffusion channel with an absorbing receiver in
a 3-D environment. The proposed model and the formulation match well with the
simulations without any normalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4502</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4502</id><created>2014-04-17</created><updated>2014-04-29</updated><authors><author><keyname>Nguyen</keyname><forenames>Thi-Van-Anh</forenames></author><author><keyname>Lallouet</keyname><forenames>Arnaud</forenames></author></authors><title>A Complete Solver for Constraint Games</title><categories>cs.GT cs.AI</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game Theory studies situations in which multiple agents having conflicting
objectives have to reach a collective decision. The question of a compact
representation language for agents utility function is of crucial importance
since the classical representation of a $n$-players game is given by a
$n$-dimensional matrix of exponential size for each player. In this paper we
use the framework of Constraint Games in which CSP are used to represent
utilities. Constraint Programming --including global constraints-- allows to
easily give a compact and elegant model to many useful games. Constraint Games
come in two flavors: Constraint Satisfaction Games and Constraint Optimization
Games, the first one using satisfaction to define boolean utilities. In
addition to multimatrix games, it is also possible to model more complex games
where hard constraints forbid certain situations. In this paper we study
complete search techniques and show that our solver using the compact
representation of Constraint Games is faster than the classical game solver
Gambit by one to two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4504</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4504</id><created>2014-04-17</created><authors><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Lidick&#xfd;</keyname><forenames>Bernard</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author><author><keyname>Valla</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>On the Tree Search Problem with Non-uniform Costs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching in partially ordered structures has been considered in the context
of information retrieval and efficient tree-like indexes, as well as in
hierarchy based knowledge representation. In this paper we focus on tree-like
partial orders and consider the problem of identifying an initially unknown
vertex in a tree by asking edge queries: an edge query $e$ returns the
component of $T-e$ containing the vertex sought for, while incurring some known
cost $c(e)$.
  The Tree Search Problem with Non-Uniform Cost is: given a tree $T$ where each
edge has an associated cost, construct a strategy that minimizes the total cost
of the identification in the worst case.
  Finding the strategy guaranteeing the minimum possible cost is an NP-complete
problem already for input tree of degree 3 or diameter 6. The best known
approximation guarantee is the $O(\log n/\log \log \log n)$-approximation
algorithm of [Cicalese et al. TCS 2012].
  We improve upon the above results both from the algorithmic and the
computational complexity point of view: We provide a novel algorithm that
provides an $O(\frac{\log n}{\log \log n})$-approximation of the cost of the
optimal strategy. In addition, we show that finding an optimal strategy is
NP-complete even when the input tree is a spider, i.e., at most one vertex has
degree larger than 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4506</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4506</id><created>2014-04-17</created><authors><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Misra</keyname><forenames>Pranabendu</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Deterministic Truncation of Linear Matroids</title><categories>cs.DS cs.DM</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $M=(E,{\cal I})$ be a matroid. A {\em $k$-truncation} of $M$ is a matroid
{$M'=(E,{\cal I}')$} such that for any $A\subseteq E$, $A\in {\cal I}'$ if and
only if $|A|\leq k$ and $A\in {\cal I}$. Given a linear representation of $M$
we consider the problem of finding a linear representation of the
$k$-truncation of this matroid. This problem can be abstracted out to the
following problem on matrices. Let $M$ be a $n\times m$ matrix over a field
$\mathbb{F}$. A {\em rank $k$-truncation} of the matrix $M$ is a $k\times m$
matrix $M_k$ (over $\mathbb{F}$ or a related field) such that for every subset
$I\subseteq \{1,\ldots,m\}$ of size at most $k$, the set of columns
corresponding to $I$ in $M$ has rank $|I|$ if and only of the corresponding set
of columns in $M_k$ has rank $|I|$. Finding rank $k$-truncation of matrices is
a common way to obtain a linear representation of $k$-truncation of linear
matroids, which has many algorithmic applications. A common way to compute a
rank $k$-truncation of a $n \times m$ matrix is to multiply the matrix with a
random $k\times n$ matrix (with the entries from a field of an appropriate
size), yielding a simple randomized algorithm. So a natural question is whether
it possible to obtain a rank $k$-truncations of a matrix, {\em
deterministically}. In this paper we settle this question for matrices over any
finite field or the field of rationals ($\mathbb Q$). We show that given a
matrix $M$ over a field $\mathbb{F}$ we can compute a $k$-truncation $M_k$ over
the ring $\mathbb{F}[X]$ in deterministic polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4519</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4519</id><created>2014-04-17</created><authors><author><keyname>Kari</keyname><forenames>Jarkko</forenames></author><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Trace Complexity of Chaotic Reversible Cellular Automata</title><categories>math.DS cs.FL</categories><comments>12 pages + 1 page appendix, 4 figures. Accepted to Reversible
  Computation 2014 (proceedings published by Springer)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delvenne, K\r{u}rka and Blondel have defined new notions of computational
complexity for arbitrary symbolic systems, and shown examples of effective
systems that are computationally universal in this sense. The notion is defined
in terms of the trace function of the system, and aims to capture its dynamics.
We present a Devaney-chaotic reversible cellular automaton that is universal in
their sense, answering a question that they explicitly left open. We also
discuss some implications and limitations of the construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4526</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4526</id><created>2014-04-17</created><authors><author><keyname>Kamali</keyname><forenames>Shahin</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author></authors><title>An All-Around Near-Optimal Solution for the Classic Bin Packing Problem</title><categories>cs.DS</categories><comments>20 pages (including the abstract and references), 3 Figures, 2 tables</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the first algorithm with optimal average-case and
close-to-best known worst-case performance for the classic on-line problem of
bin packing. It has long been observed that known bin packing algorithms with
optimal average-case performance were not optimal in the worst-case sense. In
particular First Fit and Best Fit had optimal average-case ratio of 1 but a
worst-case competitive ratio of 1.7. The wasted space of First Fit and Best Fit
for a uniform random sequence of length $n$ is expected to be $\Theta(n^{2/3})$
and $\Theta(\sqrt{n} \log ^{3/4} n)$, respectively. The competitive ratio can
be improved to 1.691 using the Harmonic algorithm; further variations of this
algorithm can push down the competitive ratio to 1.588. However, Harmonic and
its variations have poor performance on average; in particular, Harmonic has
average-case ratio of around 1.27. In this paper, first we introduce a simple
algorithm which we term Harmonic Match. This algorithm performs as well as Best
Fit on average, i.e., it has an average-case ratio of 1 and expected wasted
space of $\Theta(\sqrt{n} \log ^{3/4} n)$. Moreover, the competitive ratio of
the algorithm is as good as Harmonic, i.e., it converges to $ 1.691$ which is
an improvement over 1.7 of Best Fit and First Fit. We also introduce a
different algorithm, termed as Refined Harmonic Match, which achieves an
improved competitive ratio of $1.636$ while maintaining the good average-case
performance of Harmonic Match and Best Fit. Finally, our extensive experimental
evaluation of the studied bin packing algorithms shows that our proposed
algorithms have comparable average-case performance with Best Fit and First
Fit, and this holds also for sequences that follow distributions other than the
uniform distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4528</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4528</id><created>2014-04-17</created><authors><author><keyname>de Arruda</keyname><forenames>Guilherme Ferraz</forenames></author><author><keyname>Barbieri</keyname><forenames>Andr&#xe9; Luiz</forenames></author><author><keyname>Rodriguez</keyname><forenames>Pablo Mart&#xed;n</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco Aparecido</forenames></author></authors><title>The role of centrality for the identification of influential spreaders
  in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 11 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of the most influential spreaders in networks is important
to control and understand the spreading capabilities of the system as well as
to ensure an efficient information diffusion such as in rumor-like dynamics.
Recent works have suggested that the identification of influential spreaders is
not independent of the dynamics being studied. For instance, the key disease
spreaders might not necessarily be so when it comes to analyze social contagion
or rumor propagation. Additionally, it has been shown that different metrics
(degree, coreness, etc) might identify different influential nodes even for the
same dynamical processes with diverse degree of accuracy. In this paper, we
investigate how nine centrality measures correlate with the disease and rumor
spreading capabilities of the nodes that made up different synthetic and
real-world (both spatial and non-spatial) networks. We also propose a
generalization of the random walk accessibility as a new centrality measure and
derive analytical expressions for the latter measure for simple network
configurations. Our results show that for non-spatial networks, the $k$-core
and degree centralities are most correlated to epidemic spreading, whereas the
average neighborhood degree, the closeness centrality and accessibility are
most related to rumor dynamics. On the contrary, for spatial networks, the
accessibility measure outperforms the rest of centrality metrics in almost all
cases regardless of the kind of dynamics considered. Therefore, an important
consequence of our analysis is that previous studies performed in synthetic
random networks cannot be generalized to the case of spatial networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4533</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4533</id><created>2014-04-17</created><authors><author><keyname>Tran</keyname><forenames>Minh-Dung</forenames></author><author><keyname>Acs</keyname><forenames>Gergely</forenames></author><author><keyname>Castelluccia</keyname><forenames>Claude</forenames></author></authors><title>Retargeting Without Tracking</title><categories>cs.CR cs.CY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retargeting ads are increasingly prevalent on the Internet as their
effectiveness has been shown to outperform conventional targeted ads.
Retargeting ads are not only based on users' interests, but also on their
intents, i.e. commercial products users have shown interest in. Existing
retargeting systems heavily rely on tracking, as retargeting companies need to
know not only the websites a user has visited but also the exact products on
these sites. They are therefore very intrusive, and privacy threatening.
Furthermore, these schemes are still sub-optimal since tracking is partial, and
they often deliver ads that are obsolete (because, for example, the targeted
user has already bought the advertised product).
  This paper presents the first privacy-preserving retargeting ads system. In
the proposed scheme, the retargeting algorithm is distributed between the user
and the advertiser such that no systematic tracking is necessary, more control
and transparency is provided to users, but still a lot of targeting flexibility
is provided to advertisers. We show that our scheme, that relies on homomorphic
encryption, can be efficiently implemented and trivially solves many problems
of existing schemes, such as frequency capping and ads freshness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4540</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4540</id><created>2014-04-17</created><authors><author><keyname>C&#xf3;rdoba</keyname><forenames>Antonio</forenames></author><author><keyname>Aguilar-Hidalgo</keyname><forenames>Daniel</forenames></author><author><keyname>Lemos</keyname><forenames>M. Carmen</forenames></author></authors><title>Collective computation in a network with distributed information</title><categories>cs.SI cs.DC physics.soc-ph</categories><comments>11 pages, 4 figures</comments><msc-class>68M14, 05C82</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a distributed information network in which each node has access to
the information contained in a limited set of nodes (its neighborhood) at a
given time. A collective computation is carried out in which each node
calculates a value that implies all information contained in the network (in
our case, the average value of a variable that can take different values in
each network node). The neighborhoods can change dynamically by exchanging
neighbors with other nodes. The results of this collective calculation show
rapid convergence and good scalability with the network size. These results are
compared with those of a fixed network arranged as a square lattice, in which
the number of rounds to achieve a given accuracy is very high when the size of
the network increases. The results for the evolving networks are interpreted in
light of the properties of complex networks and are directly relevant to the
diameter and characteristic path length of the networks, which seem to express
&quot;small world&quot; properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4543</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4543</id><created>2014-04-17</created><authors><author><keyname>Haghi</keyname><forenames>Hadi Restgou</forenames></author><author><keyname>Kangavari</keyname><forenames>Mohammadreza</forenames></author><author><keyname>QasemiZadeh</keyname><forenames>Behrang</forenames></author></authors><title>A Novel Approach for Video Temporal Annotation</title><categories>cs.MM</categories><comments>Published in a Local Confrence, 2006</comments><msc-class>68Uxx</msc-class><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in computing, communication, and data storage have led to an
increasing number of large digital libraries publicly available on the
Internet. Main problem of content-based video retrieval is inferring semantics
from raw video data. Video data play an important role in these libraries.
Instead of words, a video retrieval system deals with collections of video
records. Therefore, the system is confronted with the problem of video
understanding. Because machine understanding of the video data is still an
unsolved research problem, text annotations are usually used to describe the
content of video data according to the annotator's understanding and the
purpose of that video data. Most of proposed systems for video annotation are
domain dependent. In addition, in many of these systems, an important feature
of video data, temporality, is disregarded. In this paper, we proposed a
framework for video temporal annotation. The proposed system uses domain
knowledge and a time ontology to perform temporal annotation of input video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4547</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4547</id><created>2014-04-17</created><updated>2015-01-19</updated><authors><author><keyname>Anselmi</keyname><forenames>Jonatha</forenames></author><author><keyname>Gaujal</keyname><forenames>Bruno</forenames></author><author><keyname>Nesti</keyname><forenames>Tommaso</forenames></author></authors><title>Control of parallel non-observable queues: asymptotic equivalence and
  optimality of periodic policies</title><categories>cs.PF math.OC math.PR</categories><comments>17 pages, 1 figure</comments><msc-class>60K25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a queueing system composed of a dispatcher that routes
deterministically jobs to a set of non-observable queues working in parallel.
In this setting, the fundamental problem is which policy should the dispatcher
implement to minimize the stationary mean waiting time of the incoming jobs. We
present a structural property that holds in the classic scaling of the system
where the network demand (arrival rate of jobs) grows proportionally with the
number of queues. Assuming that each queue of type $r$ is replicated $k$ times,
we consider a set of policies that are periodic with period $k \sum_r p_r$ and
such that exactly $p_r$ jobs are sent in a period to each queue of type $r$.
When $k\to\infty$, our main result shows that all the policies in this set are
equivalent, in the sense that they yield the same mean stationary waiting time,
and optimal, in the sense that no other policy having the same aggregate
arrival rate to \emph{all} queues of a given type can do better in minimizing
the stationary mean waiting time. This property holds in a strong probabilistic
sense. Furthermore, the limiting mean waiting time achieved by our policies is
a convex function of the arrival rate in each queue, which facilitates the
development of a further optimization aimed at solving the fundamental problem
above for large systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4553</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4553</id><created>2014-04-17</created><updated>2014-09-01</updated><authors><author><keyname>Wu</keyname><forenames>Daoyuan</forenames></author><author><keyname>Chang</keyname><forenames>Rocky K. C.</forenames></author></authors><title>Analyzing Android Browser Apps for file:// Vulnerabilities</title><categories>cs.CR</categories><comments>The paper has been accepted by ISC'14 as a regular paper (see
  https://daoyuan14.github.io/). This is a Technical Report version for
  reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Securing browsers in mobile devices is very challenging, because these
browser apps usually provide browsing services to other apps in the same
device. A malicious app installed in a device can potentially obtain sensitive
information through a browser app. In this paper, we identify four types of
attacks in Android, collectively known as FileCross, that exploits the
vulnerable file:// to obtain users' private files, such as cookies, bookmarks,
and browsing histories. We design an automated system to dynamically test 115
browser apps collected from Google Play and find that 64 of them are vulnerable
to the attacks. Among them are the popular Firefox, Baidu and Maxthon browsers,
and the more application-specific ones, including UC Browser HD for tablet
users, Wikipedia Browser, and Kids Safe Browser. A detailed analysis of these
browsers further shows that 26 browsers (23%) expose their browsing interfaces
unintentionally. In response to our reports, the developers concerned promptly
patched their browsers by forbidding file:// access to private file zones,
disabling JavaScript execution in file:// URLs, or even blocking external
file:// URLs. We employ the same system to validate the ten patches received
from the developers and find one still failing to block the vulnerability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4560</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4560</id><created>2014-04-17</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author></authors><title>A Control Dichotomy for Pure Scoring Rules</title><categories>cs.GT cs.CC cs.MA</categories><comments>A shorter version of this paper will appear in the proceedings of the
  Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI 2014)</comments><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scoring systems are an extremely important class of election systems. A
length-$m$ (so-called) scoring vector applies only to $m$-candidate elections.
To handle general elections, one must use a family of vectors, one per length.
The most elegant approach to making sure such families are &quot;family-like&quot; is the
recently introduced notion of (polynomial-time uniform) pure scoring rules
[Betzler and Dorn 2010], where each scoring vector is obtained from its
precursor by adding one new coefficient. We obtain the first dichotomy theorem
for pure scoring rules for a control problem. In particular, for constructive
control by adding voters (CCAV), we show that CCAV is solvable in polynomial
time for $k$-approval with $k \leq 3$, $k$-veto with $k \leq 2$, every pure
scoring rule in which only the two top-rated candidates gain nonzero scores,
and a particular rule that is a &quot;hybrid&quot; of 1-approval and 1-veto. For all
other pure scoring rules, CCAV is NP-complete. We also investigate the
descriptive richness of different models for defining pure scoring rules,
proving how more rule-generation time gives more rules, proving that rationals
give more rules than do the natural numbers, and proving that some restrictions
previously thought to be &quot;w.l.o.g.&quot; in fact do lose generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4572</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4572</id><created>2014-04-17</created><authors><author><keyname>Qasemizadeh</keyname><forenames>Behrang</forenames></author><author><keyname>Rahimi</keyname><forenames>Saeed</forenames></author><author><keyname>Bakhtiari</keyname><forenames>Behrooz Mahmoodi</forenames></author></authors><title>The First Parallel Multilingual Corpus of Persian: Toward a Persian
  BLARK</title><categories>cs.CL</categories><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we have introduced the first parallel corpus of Persian with
more than 10 other European languages. This article describes primary steps
toward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,
we have proposed morphosyntactic specification of Persian based on
EAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article
introduces Persian Language, with emphasis on its orthography and
morphosyntactic features, then a new Part-of-Speech categorization and
orthography for Persian in digital environments is proposed. Finally, the
corpus and related statistic will be analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4575</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4575</id><created>2014-04-17</created><authors><author><keyname>Louis</keyname><forenames>Anand</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author></authors><title>Approximation Algorithms for Hypergraph Small Set Expansion and Small
  Set Vertex Expansion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expansion of a hypergraph, a natural extension of the notion of expansion
in graphs, is defined as the minimum over all cuts in the hypergraph of the
ratio of the number of the hyperedges cut to the size of the smaller side of
the cut. We study the Hypergraph Small Set Expansion problem, which, for a
parameter $\delta \in (0,1/2]$, asks to compute the cut having the least
expansion while having at most $\delta$ fraction of the vertices on the smaller
side of the cut. We present two algorithms. Our first algorithm gives an
$\tilde O(\delta^{-1} \sqrt{\log n})$ approximation. The second algorithm finds
a set with expansion $\tilde O(\delta^{-1}(\sqrt{d_{\text{max}}r^{-1}\log r\,
\phi^*} + \phi^*))$ in a $r$--uniform hypergraph with maximum degree
$d_{\text{max}}$ (where $\phi^*$ is the expansion of the optimal solution).
Using these results, we also obtain algorithms for the Small Set Vertex
Expansion problem: we get an $\tilde O(\delta^{-1} \sqrt{\log n})$
approximation algorithm and an algorithm that finds a set with vertex expansion
$O\left(\delta^{-1}\sqrt{\phi^V \log d_{\text{max}} } + \delta^{-1}
\phi^V\right)$ (where $\phi^V$ is the vertex expansion of the optimal
solution).
  For $\delta=1/2$, Hypergraph Small Set Expansion is equivalent to the
hypergraph expansion problem. In this case, our approximation factor of
$O(\sqrt{\log n})$ for expansion in hypergraphs matches the corresponding
approximation factor for expansion in graphs due to ARV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4592</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4592</id><created>2014-04-17</created><authors><author><keyname>Raeesi</keyname><forenames>Mohsen</forenames></author><author><keyname>Morid</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Shajari</keyname><forenames>Mehdi</forenames></author></authors><title>Trust Evaluation using an Improved Context Similarity Measurement</title><categories>cs.OH</categories><comments>19 pages, 13 figures</comments><journal-ref>International Journal of Business Information Systems Strategies
  (IJBISS), Volume 3, Number 1, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In context-aware trust evaluation, using ontology tree is a popular approach
to represent the relation between contexts. Usually, similarity between two
contexts is computed using these trees. Therefore, the performance of trust
evaluation highly depends on the quality of ontology trees. Fairness or
granularity consistency is one of the major limitations affecting the quality
of ontology tree. This limitation refers to inequality of semantic similarity
in the most ontology trees. In other words, semantic similarity of every two
adjacent nodes is unequal in these trees. It deteriorates the performance of
contexts similarity computation. We overcome this limitation by weighting tree
edges based on their semantic similarity. Weight of each edge is computed using
Normalized Similarity Score (NSS) method. This method is based on frequencies
of concepts (words) co-occurrences in the pages indexed by search engines. Our
experiments represent the better performance of the proposed approach in
comparison with established trust evaluation approaches. The suggested approach
can enhance efficiency of any solution which models semantic relations by
ontology tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4598</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4598</id><created>2014-04-17</created><authors><author><keyname>Pourmiri</keyname><forenames>Ali</forenames></author><author><keyname>Sauerwald</keyname><forenames>Thomas</forenames></author></authors><title>Cutoff Phenomenon for Random Walks on Kneser Graphs</title><categories>math.CO cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cutoff phenomenon for an ergodic Markov chain describes a sharp
transition in the convergence to its stationary distribution, over a negligible
period of time, known as cutoff window. We study the cutoff phenomenon for
simple random walks on Kneser graphs, which is a family of ergodic Markov
chains. Given two integers $n$ and $k$, the Kneser graph $K(2n+k,n)$ is defined
as the graph with vertex set being all subsets of $\{1,\ldots,2n+k\}$ of size
$n$ and two vertices $A$ and $B$ being connected by an edge if $A\cap B
=\emptyset$. We show that for any $k=O(n)$, the random walk on $K(2n+k,n)$
exhibits a cutoff at $\frac{1}{2}\log_{1+k/n}{(2n+k)}$ with a window of size
$O(\frac{n}{k})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4599</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4599</id><created>2014-04-17</created><updated>2015-10-15</updated><authors><author><keyname>Otto</keyname><forenames>Martin</forenames></author></authors><title>Finite Groupoids, Finite Coverings and Symmetries in Finite Structures</title><categories>math.CO cs.LO math.LO</categories><comments>This paper extends and supersedes earlier expositions in LICS 2013
  and arXiv:1211.5656. Version (v2) of this paper fixes a false claim in Lemma
  2.9 of the original version. Version (v4) eliminates confusion around
  &quot;covering of A&quot; vs &quot;realisation of H(A)&quot; (Definition 3.14 and adaptation of
  Lemma 3.16) and corrects a mistake in the &quot;excursion&quot; on Herwig's thm in
  section 4.4</comments><msc-class>05E18, 05C65, 05C25, 05C38, 20F05, 57M12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel construction of finite hypergraphs and relational
structures that is based on reduced products with Cayley graphs of groupoids.
To this end we construct groupoids whose Cayley graphs have large girth not
just in the usual sense, but with respect to a discounted distance measure that
contracts arbitrarily long sequences of edges within the same sub-groupoid
(coset) and only counts transitions between cosets. Reduced products with such
groupoids are sufficiently generic to be applicable to various constructions
that are specified in terms of local glueing operations and require global
finite closure. We here examine hypergraph coverings and extension tasks that
lift local symmetries to global automorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4606</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4606</id><created>2014-04-16</created><updated>2014-06-19</updated><authors><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>O'Callaghan</keyname><forenames>Derek</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>How Many Topics? Stability Analysis for Topic Models</title><categories>cs.LG cs.CL cs.IR</categories><comments>Improve readability of plots. Add minor clarifications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modeling refers to the task of discovering the underlying thematic
structure in a text corpus, where the output is commonly presented as a report
of the top terms appearing in each topic. Despite the diversity of topic
modeling algorithms that have been proposed, a common challenge in successfully
applying these techniques is the selection of an appropriate number of topics
for a given corpus. Choosing too few topics will produce results that are
overly broad, while choosing too many will result in the &quot;over-clustering&quot; of a
corpus into many small, highly-similar topics. In this paper, we propose a
term-centric stability analysis strategy to address this issue, the idea being
that a model with an appropriate number of topics will be more robust to
perturbations in the data. Using a topic modeling approach based on matrix
factorization, evaluations performed on a range of corpora show that this
strategy can successfully guide the model selection process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4607</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4607</id><created>2014-04-16</created><authors><author><keyname>Aubert</keyname><forenames>Olivier</forenames></author><author><keyname>Pri&#xe9;</keyname><forenames>Yannick</forenames></author><author><keyname>Canellas</keyname><forenames>Camila</forenames></author></authors><title>Leveraging video annotations in video-based e-learning</title><categories>cs.CY cs.HC cs.MM</categories><comments>7th International Conference on Computer Supported Education (CSEDU),
  Barcelone : Spain (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The e-learning community has been producing and using video content for a
long time, and in the last years, the advent of MOOCs greatly relied on video
recordings of teacher courses. Video annotations are information pieces that
can be anchored in the temporality of the video so as to sustain various
processes ranging from active reading to rich media editing. In this position
paper we study how video annotations can be used in an e-learning context -
especially MOOCs - from the triple point of view of pedagogical processes,
current technical platforms functionalities, and current challenges. Our
analysis is that there is still plenty of room for leveraging video annotations
in MOOCs beyond simple active reading, namely live annotation, performance
annotation and annotation for assignment; and that new developments are needed
to accompany this evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4622</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4622</id><created>2014-04-17</created><authors><author><keyname>Beideman</keyname><forenames>Calvin</forenames></author><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author></authors><title>Set Families with Low Pairwise Intersection</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $\left(n,\ell,\gamma\right)$-sharing set family of size $m$ is a family of
sets $S_1,\ldots,S_m\subseteq [n]$ s.t. each set has size $\ell$ and each pair
of sets shares at most $\gamma$ elements. We let $m\left(n,\ell,\gamma\right)$
denote the maximum size of any such set family and we consider the following
question: How large can $m\left(n,\ell,\gamma\right)$ be?
$\left(n,\ell,\gamma\right)$-sharing set families have a rich set of
applications including the construction of pseudorandom number generators and
usable and secure password management schemes. We analyze the explicit
construction of Blocki et al using recent bounds on the value of the $t$'th
Ramanujan prime. We show that this explicit construction produces a
$\left(4\ell^2\ln 4\ell,\ell,\gamma\right)$-sharing set family of size $\left(2
\ell \ln 2\ell\right)^{\gamma+1}$ for any $\ell\geq \gamma$. We also show that
the construction of Blocki et al can be used to obtain a weak
$\left(n,\ell,\gamma\right)$-sharing set family of size $m$ for any $m &gt;0$.
These results are competitive with the inexplicit construction of Raz et al for
weak $\left(n,\ell,\gamma\right)$-sharing families. We show that our explicit
construction of weak $\left(n,\ell,\gamma\right)$-sharing set families can be
used to obtain a parallelizable pseudorandom number generator with a low memory
footprint by using the pseudorandom number generator of Nisan and Wigderson. We
also prove that $m\left(n,n/c_1,c_2n\right)$ must be a constant whenever $c_2
\leq \frac{2}{c_1^3+c_1^2}$. We show that this bound is nearly tight as
$m\left(n,n/c_1,c_2n\right)$ grows exponentially fast whenever $c_2 &gt;
c_1^{-2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4629</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4629</id><created>2014-04-17</created><updated>2014-04-18</updated><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author><author><keyname>Vetter</keyname><forenames>Jeffrey S.</forenames></author></authors><title>A Survey of Methods For Analyzing and Improving GPU Energy Efficiency</title><categories>cs.AR</categories><comments>Accepted with minor revision in ACM Computing Survey Journal (impact
  factor 3.85, five year impact of 7.85)</comments><acm-class>A.1; I.3.1; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed a phenomenal growth in the computational
capabilities and applications of GPUs. However, this trend has also led to
dramatic increase in their power consumption. This paper surveys research works
on analyzing and improving energy efficiency of GPUs. It also provides a
classification of these techniques on the basis of their main research idea.
Further, it attempts to synthesize research works which compare energy
efficiency of GPUs with other computing systems, e.g. FPGAs and CPUs. The aim
of this survey is to provide researchers with knowledge of state-of-the-art in
GPU power management and motivate them to architect highly energy-efficient
GPUs of tomorrow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4639</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4639</id><created>2014-04-17</created><updated>2014-08-26</updated><authors><author><keyname>Gharaibeh</keyname><forenames>Ammar</forenames></author><author><keyname>Khreishah</keyname><forenames>Abdallah</forenames></author><author><keyname>Khalil</keyname><forenames>Issa</forenames></author><author><keyname>Wu</keyname><forenames>Jie</forenames></author></authors><title>Asymptotically-Optimal Incentive-Based En-Route Caching Scheme</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content caching at intermediate nodes is a very effective way to optimize the
operations of Computer networks, so that future requests can be served without
going back to the origin of the content. Several caching techniques have been
proposed since the emergence of the concept, including techniques that require
major changes to the Internet architecture such as Content Centric Networking.
Few of these techniques consider providing caching incentives for the nodes or
quality of service guarantees for content owners. In this work, we present a
low complexity, distributed, and online algorithm for making caching decisions
based on content popularity, while taking into account the aforementioned
issues. Our algorithm performs en-route caching. Therefore, it can be
integrated with the current TCP/IP model. In order to measure the performance
of any online caching algorithm, we define the competitive ratio as the ratio
of the performance of the online algorithm in terms of traffic savings to the
performance of the optimal offline algorithm that has a complete knowledge of
the future. We show that under our settings, no online algorithm can achieve a
better competitive ratio than $\Omega(\log n)$, where $n$ is the number of
nodes in the network. Furthermore, we show that under realistic scenarios, our
algorithm has an asymptotically optimal competitive ratio in terms of the
number of nodes in the network. We also study an extension to the basic
algorithm and show its effectiveness through extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4641</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4641</id><created>2014-04-17</created><authors><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Multilingual Models for Compositional Distributed Semantics</title><categories>cs.CL</categories><comments>Proceedings of ACL 2014 (Long papers)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel technique for learning semantic representations, which
extends the distributional hypothesis to multilingual data and joint-space
embeddings. Our models leverage parallel data and learn to strongly align the
embeddings of semantically equivalent sentences, while maintaining sufficient
distance between those of dissimilar sentences. The models do not rely on word
alignments or any syntactic information and are successfully applied to a
number of diverse languages. We extend our approach to learn semantic
representations at the document level, too. We evaluate these models on two
cross-lingual document classification tasks, outperforming the prior state of
the art. Through qualitative analysis and the study of pivoting effects we
demonstrate that our representations are semantically plausible and can capture
semantic relationships across languages without parallel data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4644</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4644</id><created>2014-04-17</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>A New Space for Comparing Graphs</title><categories>stat.ME cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a new mathematical representations for graph, which allows direct
comparison between different graph structures, is an open-ended research
direction. Having such a representation is the first prerequisite for a variety
of machine learning algorithms like classification, clustering, etc., over
graph datasets. In this paper, we propose a symmetric positive semidefinite
matrix with the $(i,j)$-{th} entry equal to the covariance between normalized
vectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representation
for graph with adjacency matrix $A$. We show that the proposed matrix
representation encodes the spectrum of the underlying adjacency matrix and it
also contains information about the counts of small sub-structures present in
the graph such as triangles and small paths. In addition, we show that this
matrix is a \emph{&quot;graph invariant&quot;}. All these properties make the proposed
matrix a suitable object for representing graphs.
  The representation, being a covariance matrix in a fixed dimensional metric
space, gives a mathematical embedding for graphs. This naturally leads to a
measure of similarity on graph objects. We define similarity between two given
graphs as a Bhattacharya similarity measure between their corresponding
covariance matrix representations. As shown in our experimental study on the
task of social network classification, such a similarity measure outperforms
other widely used state-of-the-art methodologies. Our proposed method is also
computationally efficient. The computation of both the matrix representation
and the similarity value can be performed in operations linear in the number of
edges. This makes our method scalable in practice.
  We believe our theoretical and empirical results provide evidence for
studying truncated power iterations, of the adjacency matrix, to characterize
social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4646</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4646</id><created>2014-04-17</created><updated>2014-07-16</updated><authors><author><keyname>Liu</keyname><forenames>Guangcan</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Advancing Matrix Completion by Modeling Extra Structures beyond
  Low-Rankness</title><categories>stat.ME cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>arXiv admin note: text overlap with arXiv:1404.4032</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-known method for completing low-rank matrices based on convex
optimization has been established by Cand{\`e}s and Recht. Although
theoretically complete, the method may not entirely solve the low-rank matrix
completion problem. This is because the method captures only the low-rankness
property which gives merely a rough constraint that the data points locate on
some low-dimensional subspace, but generally ignores the extra structures which
specify in more detail how the data points locate on the subspace. Whenever the
geometric distribution of the data points is not uniform, the coherence
parameters of data might be large and, accordingly, the method might fail even
if the latent matrix we want to recover is fairly low-rank. To better handle
non-uniform data, in this paper we propose a method termed Low-Rank Factor
Decomposition (LRFD), which imposes an additional restriction that the data
points must be represented as linear combinations of the bases in a dictionary
constructed or learnt in advance. We show that LRFD can well handle non-uniform
data, provided that the dictionary is configured properly: We mathematically
prove that if the dictionary itself is low-rank then LRFD is immune to the
coherence parameters which might be large on non-uniform data. This provides an
elementary principle for learning the dictionary in LRFD and, naturally, leads
to a practical algorithm for advancing matrix completion. Extensive experiments
on randomly generated matrices and motion datasets show encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4653</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4653</id><created>2014-04-17</created><authors><author><keyname>Kambhampati</keyname><forenames>Sundeep</forenames></author><author><keyname>Stewart</keyname><forenames>Christopher</forenames></author></authors><title>An Efficient and Balanced Platform for Data-Parallel Subsampling
  Workloads</title><categories>cs.DC</categories><comments>70 Pages, 16 figures,Thesis, The Ohio State University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of internet services, data started growing faster than it can
be processed. To personalize user experience, this enormous data has to be
processed in real time, in interactive fashion. In order to achieve faster data
processing often a statistical method called subsampling. Subsampling workloads
compute statistics from a random subset of sample data (i.e., a subsample).
Data-parallel platforms group these samples into tasks; each task subsamples
its data in parallel.
  Current, state-of-the-art platforms such as Hadoop are built for large tasks
that run for long periods of time, but applications with smaller average task
sizes suffer large overheads on these platforms. Tasks in subsampling workloads
are sized to minimize the number of overall cache misses, and these tasks can
complete in seconds. This technique can reduce the overall length of a
map-reduce job, but only when the savings from the cache miss rate reduction
are not eclipsed by the platform overhead of task creation and data
distribution.
  In this thesis, we propose a data-parallel platform with an efficient data
distribution component that breaks data-parallel subsampling workloads into
compute clusters with tiny tasks. Each tiny task completes in few hundreds of
milliseconds to seconds. Tiny tasks reduce processor cache misses caused by
random subsampling, which speeds up per-task running time. However, they cause
significant scheduling overheads and data distribution challenges. We propose a
task knee-pointing algorithm and a dynamic scheduler that schedules the tasks
to worker nodes based on the availability and response times of the data nodes.
We compare our framework against various configurations of BashReduce and
Hadoop. A detailed discussion of tiny task approach on two workloads, EAGLET
and Netflix movie rating is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4655</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4655</id><created>2014-04-17</created><authors><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author><author><keyname>M&#xe9;moli</keyname><forenames>Facundo</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author></authors><title>Hierarchical Quasi-Clustering Methods for Asymmetric Networks</title><categories>cs.LG stat.ML</categories><comments>Accepted to the 31st International Conference on Machine Learning
  (ICML), Beijing, China, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces hierarchical quasi-clustering methods, a generalization
of hierarchical clustering for asymmetric networks where the output structure
preserves the asymmetry of the input data. We show that this output structure
is equivalent to a finite quasi-ultrametric space and study admissibility with
respect to two desirable properties. We prove that a modified version of single
linkage is the only admissible quasi-clustering method. Moreover, we show
stability of the proposed method and we establish invariance properties
fulfilled by it. Algorithms are further developed and the value of
quasi-clustering analysis is illustrated with a study of internal migration
within United States.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4661</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4661</id><created>2014-04-17</created><authors><author><keyname>Wang</keyname><forenames>Jiang</forenames></author><author><keyname>song</keyname><forenames>Yang</forenames></author><author><keyname>Leung</keyname><forenames>Thomas</forenames></author><author><keyname>Rosenberg</keyname><forenames>Chuck</forenames></author><author><keyname>Wang</keyname><forenames>Jinbin</forenames></author><author><keyname>Philbin</keyname><forenames>James</forenames></author><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Wu</keyname><forenames>Ying</forenames></author></authors><title>Learning Fine-grained Image Similarity with Deep Ranking</title><categories>cs.CV</categories><comments>CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning fine-grained image similarity is a challenging task. It needs to
capture between-class and within-class image differences. This paper proposes a
deep ranking model that employs deep learning techniques to learn similarity
metric directly from images.It has higher learning capability than models based
on hand-crafted features. A novel multiscale network structure has been
developed to describe the images effectively. An efficient triplet sampling
algorithm is proposed to learn the model with distributed asynchronized
stochastic gradient. Extensive experiments show that the proposed algorithm
outperforms models based on hand-crafted visual features and deep
classification models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4664</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4664</id><created>2014-04-17</created><updated>2014-04-24</updated><authors><author><keyname>Chen</keyname><forenames>Hsien-Pu</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author><author><keyname>Schmera</keyname><forenames>Gabor</forenames></author></authors><title>Do electromagnetic waves exist in a short cable at low frequencies? What
  does physics say?</title><categories>cs.CR</categories><comments>version after Galley Proof corrections in Fluctuation and Noise
  Letters</comments><journal-ref>Fluctuation and Noise Letters 13 (2014) 1450016 (13 pages)</journal-ref><doi>10.1142/S0219477514500163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We refute a physical model, recently proposed by Gunn, Allison and Abbott
(GAA) [arXiv:1402.2709v2], to utilize electromagnetic waves for eavesdropping
on the Kirchhoff-law-Johnson-noise (KLJN) secure key distribution. Their model,
and its theoretical underpinnings, is found to be fundamentally flawed because
their assumption of electromagnetic waves violates not only the wave equation
but also the Second Law of Thermodynamics, the Principle of Detailed Balance,
Boltzmann's Energy Equipartition Theorem, and Planck's formula by implying
infinitely strong blackbody radiation. We deduce the correct mathematical model
of the GAA scheme, which is based on impedances at the quasi-static limit.
Mathematical analysis and simulation results confirm our approach and prove
that GAA's experimental interpretation is incorrect too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4666</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4666</id><created>2014-04-17</created><authors><author><keyname>Givelberg</keyname><forenames>Edward</forenames></author></authors><title>Object-Oriented Parallel Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an object-oriented framework for parallel programming, which is
based on the observation that programming objects can be naturally interpreted
as processes. A parallel program consists of a collection of persistent
processes that communicate by executing remote methods. We discuss code
parallelization and process persistence, and explain the main ideas in the
context of computations with very large data objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4667</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4667</id><created>2014-04-17</created><authors><author><keyname>Mardani</keyname><forenames>Morteza</forenames></author><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Subspace Learning and Imputation for Streaming Big Data Matrices and
  Tensors</title><categories>stat.ML cs.IT cs.LG math.IT</categories><doi>10.1109/TSP.2015.2417491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting latent low-dimensional structure from high-dimensional data is of
paramount importance in timely inference tasks encountered with `Big Data'
analytics. However, increasingly noisy, heterogeneous, and incomplete datasets
as well as the need for {\em real-time} processing of streaming data pose major
challenges to this end. In this context, the present paper permeates benefits
from rank minimization to scalable imputation of missing data, via tracking
low-dimensional subspaces and unraveling latent (possibly multi-way) structure
from \emph{incomplete streaming} data. For low-rank matrix data, a subspace
estimator is proposed based on an exponentially-weighted least-squares
criterion regularized with the nuclear norm. After recasting the non-separable
nuclear norm into a form amenable to online optimization, real-time algorithms
with complementary strengths are developed and their convergence is established
under simplifying technical assumptions. In a stationary setting, the
asymptotic estimates obtained offer the well-documented performance guarantees
of the {\em batch} nuclear-norm regularized estimator. Under the same unifying
framework, a novel online (adaptive) algorithm is developed to obtain multi-way
decompositions of \emph{low-rank tensors} with missing entries, and perform
imputation as a byproduct. Simulated tests with both synthetic as well as real
Internet and cardiac magnetic resonance imagery (MRI) data confirm the efficacy
of the proposed algorithms, and their superior performance relative to
state-of-the-art alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4676</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4676</id><created>2014-04-17</created><authors><author><keyname>Chen</keyname><forenames>Xujin</forenames></author><author><keyname>Hu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wang</keyname><forenames>Changjun</forenames></author></authors><title>Approximability of the Minimum Weighted Doubly Resolving Set Problem</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locating source of diffusion in networks is crucial for controlling and
preventing epidemic risks. It has been studied under various probabilistic
models. In this paper, we study source location from a deterministic point of
view by modeling it as the minimum weighted doubly resolving set (DRS) problem,
which is a strengthening of the well-known metric dimension problem.
  Let $G$ be a vertex weighted undirected graph on $n$ vertices. A vertex
subset $S$ of $G$ is DRS of $G$ if for every pair of vertices $u,v$ in $G$,
there exist $x,y\in S$ such that the difference of distances (in terms of
number of edges) between $u$ and $x,y$ is not equal to the difference of
distances between $v$ and $x,y$. The minimum weighted DRS problem consists of
finding a DRS in $G$ with minimum total weight. We establish $\Theta(\ln n)$
approximability of the minimum DRS problem on general graphs for both weighted
and unweighted versions. This is the first work providing explicit
approximation lower and upper bounds for minimum (weighted) DRS problem, which
are nearly tight. Moreover, we design first known strongly polynomial time
algorithms for the minimum weighted DRS problem on general wheels and trees
with additional constant $k\ge0$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4679</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4679</id><created>2014-04-17</created><updated>2014-04-28</updated><authors><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Koutra</keyname><forenames>Danai</forenames></author></authors><title>Graph-based Anomaly Detection and Description: A Survey</title><categories>cs.SI cs.CR</categories><comments>Survey on Graph-Based Anomaly Detection. Algorithms and Applications.
  Static Graphs. Dynamic Graphs. Graph Anomaly Description. 49 pages (without
  citations)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting anomalies in data is a vital task, with numerous high-impact
applications in areas such as security, finance, health care, and law
enforcement. While numerous techniques have been developed in past years for
spotting outliers and anomalies in unstructured collections of
multi-dimensional points, with graph data becoming ubiquitous, techniques for
structured {\em graph} data have been of focus recently. As objects in graphs
have long-range correlations, a suite of novel technology has been developed
for anomaly detection in graph data.
  This survey aims to provide a general, comprehensive, and structured overview
of the state-of-the-art methods for anomaly detection in data represented as
graphs. As a key contribution, we provide a comprehensive exploration of both
data mining and machine learning algorithms for these {\em detection} tasks. we
give a general framework for the algorithms categorized under various settings:
unsupervised vs. (semi-)supervised approaches, for static vs. dynamic graphs,
for attributed vs. plain graphs. We highlight the effectiveness, scalability,
generality, and robustness aspects of the methods. What is more, we stress the
importance of anomaly {\em attribution} and highlight the major techniques that
facilitate digging out the root cause, or the `why', of the detected anomalies
for further analysis and sense-making. Finally, we present several real-world
applications of graph-based anomaly detection in diverse domains, including
financial, auction, computer traffic, and social networks. We conclude our
survey with a discussion on open theoretical and practical challenges in the
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4685</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4685</id><created>2014-04-17</created><updated>2014-06-17</updated><authors><author><keyname>Sahoo</keyname><forenames>B. P. S.</forenames></author><author><keyname>Puthal</keyname><forenames>Deepak</forenames></author></authors><title>DRUG: An Energy-Efficient Data-Centric Routing Protocol for Wireless
  Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, sensor nodes are deployed in left unattended area. In such
situation feeding energy to the batteries or replacing the batteries is
difficult or even sometimes impossible too. Therefore, prolonging the network
lifetime is an important optimization goal in this aspect. In this paper, we
propose a new Energy-efficient Datacentric RoUtinG protocol called DRUG. In
this paper, we propose an adaptive Data centric approach to find an optimal
routing path from source to sink when the sensor nodes are deployed randomly
deployed in a restricted service area with single sink. Using the NS-2
Simulator, we compare the performance of DRUG with that of the FLOODING and
SPIN protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4688</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4688</id><created>2014-04-18</created><updated>2014-04-21</updated><authors><author><keyname>Meir</keyname><forenames>Reshef</forenames></author><author><keyname>Lev</keyname><forenames>Omer</forenames></author><author><keyname>Rosenschein</keyname><forenames>Jeffrey S.</forenames></author></authors><title>A Local-Dominance Theory of Voting Equilibria</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that no reasonable voting rule is strategyproof. Moreover,
the common Plurality rule is particularly prone to strategic behavior of the
voters and empirical studies show that people often vote strategically in
practice. Multiple game-theoretic models have been proposed to better
understand and predict such behavior and the outcomes it induces. However,
these models often make unrealistic assumptions regarding voters' behavior and
the information on which they base their vote.
  We suggest a new model for strategic voting that takes into account voters'
bounded rationality, as well as their limited access to reliable information.
We introduce a simple behavioral heuristic based on \emph{local dominance},
where each voter considers a set of possible world states without assigning
probabilities to them. This set is constructed based on prospective candidates'
scores (e.g., available from an inaccurate poll). In a \emph{voting
equilibrium}, all voters vote for candidates not dominated within the set of
possible states.
  We prove that these voting equilibria exist in the Plurality rule for a broad
class of local dominance relations (that is, different ways to decide which
states are possible). Furthermore, we show that in an iterative setting where
voters may repeatedly change their vote, local dominance-based dynamics quickly
converge to an equilibrium if voters start from the truthful state. Weaker
convergence guarantees in more general settings are also provided.
  Using extensive simulations of strategic voting on generated and real
preference profiles, we show that convergence is fast and robust, that emerging
equilibria are consistent across various starting conditions, and that they
replicate widely known patterns of human voting behavior such as Duverger's
law. Further, strategic voting generally improves the quality of the winner
compared to truthful voting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4692</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4692</id><created>2014-04-18</created><authors><author><keyname>Doshi</keyname><forenames>Nishant</forenames></author></authors><title>Approximation for the Path Complexity of Binary Search Tree</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of an algorithm is an important parameter to determine its
effi-ciency. They are of different types viz. Time complexity, Space
complexity, etc. However, none of them consider the execution path as a
complexity measure. Ashok et al, firstly proposed the notion of the Path
Complexity of a pro-gram/algorithm, which defined based on the number of
execution paths as a function of the input size. However, the notion of path
complexity of the pro-gram, cannot apply to the object-oriented environment.
Therefore, Anupam et al, has extended the notion of path complexity to the
class as follows. The notion of the state of the class is defined based on
structural representation (aka state) of the class. The class contains data
members and data operations. It considers only those data operations that
change the state of the class. The path complexity of the class is defined to
be the number of valid input sequences, each of them con-taining valid data
operations. Anupam et al, had applied this notion to the class Stack. However,
the stack is basic and simple data structures. Therefore, in this research we
have used a more complex class to understand the path complexity behavior in
the object oriented environment. Binary Search Tree (BST) is one of the well
known (and more complex too) data structure, which is useful in sorting,
searching, Traffic Engineering and many more applications. We have analyzed the
path complexity of the class BST based on the algorithms for insert and delete
operations. Additionally, we have modified the delete operation to minimize the
path complexity for the class BST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4693</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4693</id><created>2014-04-18</created><authors><author><keyname>Kutzkov</keyname><forenames>Konstantin</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>Consistent Subset Sampling</title><categories>cs.DS</categories><comments>To appear in SWAT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistent sampling is a technique for specifying, in small space, a subset
$S$ of a potentially large universe $U$ such that the elements in $S$ satisfy a
suitably chosen sampling condition. Given a subset $\mathcal{I}\subseteq U$ it
should be possible to quickly compute $\mathcal{I}\cap S$, i.e., the elements
in $\mathcal{I}$ satisfying the sampling condition. Consistent sampling has
important applications in similarity estimation, and estimation of the number
of distinct items in a data stream.
  In this paper we generalize consistent sampling to the setting where we are
interested in sampling size-$k$ subsets occurring in some set in a collection
of sets of bounded size $b$, where $k$ is a small integer. This can be done by
applying standard consistent sampling to the $k$-subsets of each set, but that
approach requires time $\Theta(b^k)$. Using a carefully designed hash function,
for a given sampling probability $p \in (0,1]$, we show how to improve the time
complexity to $\Theta(b^{\lceil k/2\rceil}\log \log b + pb^k)$ in expectation,
while maintaining strong concentration bounds for the sample. The space usage
of our method is $\Theta(b^{\lceil k/4\rceil})$.
  We demonstrate the utility of our technique by applying it to several
well-studied data mining problems. We show how to efficiently estimate the
number of frequent $k$-itemsets in a stream of transactions and the number of
bipartite cliques in a graph given as incidence stream. Further, building upon
a recent work by Campagna et al., we show that our approach can be applied to
frequent itemset mining in a parallel or distributed setting. We also present
applications in graph stream mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4696</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4696</id><created>2014-04-18</created><updated>2015-07-14</updated><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Kutzkov</keyname><forenames>Konstantin</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>Triangle counting in dynamic graph streams</title><categories>cs.DS</categories><comments>New version of a SWAT 2014 paper with improved results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the number of triangles in graph streams using a limited amount of
memory has become a popular topic in the last decade. Different variations of
the problem have been studied, depending on whether the graph edges are
provided in an arbitrary order or as incidence lists. However, with a few
exceptions, the algorithms have considered {\em insert-only} streams. We
present a new algorithm estimating the number of triangles in {\em dynamic}
graph streams where edges can be both inserted and deleted. We show that our
algorithm achieves better time and space complexity than previous solutions for
various graph classes, for example sparse graphs with a relatively small number
of triangles. Also, for graphs with constant transitivity coefficient, a common
situation in real graphs, this is the first algorithm achieving constant
processing time per edge. The result is achieved by a novel approach combining
sampling of vertex triples and sparsification of the input graph. In the course
of the analysis of the algorithm we present a lower bound on the number of
pairwise independent 2-paths in general graphs which might be of independent
interest. At the end of the paper we discuss lower bounds on the space
complexity of triangle counting algorithms that make no assumptions on the
structure of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4699</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4699</id><created>2014-04-18</created><updated>2014-12-16</updated><authors><author><keyname>Claeys</keyname><forenames>Mathieu</forenames><affiliation>CRAN, IUF</affiliation></author><author><keyname>Daafouz</keyname><forenames>Jamal</forenames><affiliation>CRAN, IUF</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames></author></authors><title>Modal occupation measures and LMI relaxations for nonlinear switched
  systems control</title><categories>math.OC cs.SY</categories><comments>New section 4 on reconstruction of a trajectory from its approximate
  moments.New examples in section 6</comments><proxy>ccsd</proxy><report-no>Rapport LAAS n{\textdegree} 14138</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a linear programming approach for the optimal control of
nonlinear switched systems where the control is the switching sequence. This is
done by introducing modal occupation measures, which allow to relax the problem
as a primal linear programming (LP) problem. Its dual linear program of
Hamilton-Jacobi-Bellman inequalities is also characterized. The LPs are then
solved numerically with a converging hierarchy of primal-dual
moment-sum-of-squares (SOS) linear matrix inequalities (LMI). Because of the
special structure of switched systems, we obtain a much more efficient method
than could be achieved by applying standard moment/SOS LMI hierarchies for
general optimal control problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4702</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4702</id><created>2014-04-18</created><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author><author><keyname>Vondr&#xe1;k</keyname><forenames>Jan</forenames></author></authors><title>Nearly Tight Bounds on $\ell_1$ Approximation of Self-Bounding Functions</title><categories>cs.LG cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of learning and approximation of self-bounding
functions over the uniform distribution on the Boolean hypercube ${0,1}^n$.
Informally, a function $f:{0,1}^n \rightarrow \mathbb{R}$ is self-bounding if
for every $x \in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginal
decreases in the value of the function at $x$. Self-bounding functions include
such well-known classes of functions as submodular and fractionally-subadditive
(XOS) functions. They were introduced by Boucheron et al in the context of
concentration of measure inequalities. Our main result is a nearly tight
$\ell_1$-approximation of self-bounding functions by low-degree juntas.
Specifically, all self-bounding functions can be $\epsilon$-approximated in
$\ell_1$ by a polynomial of degree $\tilde{O}(1/\epsilon)$ over
$2^{\tilde{O}(1/\epsilon)}$ variables. Both the degree and junta-size are
optimal up to logarithmic terms. Previously, the best known bound was
$O(1/\epsilon^{2})$ on the degree and $2^{O(1/\epsilon^2)}$ on the number of
variables (Feldman and Vondr \'{a}k 2013). These results lead to improved and
in several cases almost tight bounds for PAC and agnostic learning of
submodular, XOS and self-bounding functions. In particular, assuming hardness
of learning juntas, we show that PAC and agnostic learning of self-bounding
functions have complexity of $n^{\tilde{\Theta}(1/\epsilon)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4711</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4711</id><created>2014-04-18</created><authors><author><keyname>Moretti</keyname><forenames>Marco</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Resource Allocation for Power Minimization in the Downlink of THP-based
  Spatial Multiplexing MIMO-OFDMA Systems</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures, IEEE Trans. Veh. Technol</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we deal with resource allocation in the downlink of spatial
multiplexing MIMO-OFDMA systems. In particular, we concentrate on the problem
of jointly optimizing the transmit and receive processing matrices, the channel
assignment and the power allocation with the objective of minimizing the total
power consumption while satisfying different quality-of-service requirements. A
layered architecture is used in which users are first partitioned in different
groups on the basis of their channel quality and then channel assignment and
transceiver design are sequentially addressed starting from the group of users
with most adverse channel conditions. The multi-user interference among users
belonging to different groups is removed at the base station using a
Tomlinson-Harashima pre-coder operating at user level. Numerical results are
used to highlight the effectiveness of the proposed solution and to make
comparisons with existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4713</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4713</id><created>2014-04-18</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author><author><keyname>Alfia</keyname><forenames>Avinoam</forenames></author></authors><title>Knowledge-Driven Game Design by Non-Programmers</title><categories>cs.SE</categories><comments>10 pages, 4 figures, reprint of paper in SKY 2013 Workshop,
  Vilamoura, Portugal, September 2013, SCITEPRESS Digital Library</comments><acm-class>D.2.2</acm-class><doi>10.5220/0004640000470054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game extension is an entertaining activity that offers an opportunity to test
new design approaches by non-programmers. The real challenge is to enable this
activity by means of a suitable infrastructure. We propose a knowledge-driven
approach with natural game-player concepts. These concepts, found in game
ontologies, include game abstractions and rules for game moves. The approach
has been implemented and tested for board games. These include tic-tac-toe as a
simplest example, enabling extensions of tic-tac- toe, say to a four-by-four
board and Sudoku, a single player game of a very different nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4714</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4714</id><created>2014-04-18</created><authors><author><keyname>Sun</keyname><forenames>Yaming</forenames></author><author><keyname>Lin</keyname><forenames>Lei</forenames></author><author><keyname>Tang</keyname><forenames>Duyu</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author><author><keyname>Ji</keyname><forenames>Zhenzhou</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author></authors><title>Radical-Enhanced Chinese Character Embedding</title><categories>cs.CL</categories><comments>8 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a method to leverage radical for learning Chinese character
embedding. Radical is a semantic and phonetic component of Chinese character.
It plays an important role as characters with the same radical usually have
similar semantic meaning and grammatical usage. However, existing Chinese
processing algorithms typically regard word or character as the basic unit but
ignore the crucial radical information. In this paper, we fill this gap by
leveraging radical for learning continuous representation of Chinese character.
We develop a dedicated neural architecture to effectively learn character
embedding and apply it on Chinese character similarity judgement and Chinese
word segmentation. Experiment results show that our radical-enhanced method
outperforms existing embedding learning algorithms on both tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4718</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4718</id><created>2014-04-18</created><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Sekar</keyname><forenames>Shreyas</forenames></author></authors><title>Approximate Equilibrium and Incentivizing Social Coordination</title><categories>cs.GT</categories><comments>A preliminary version of this work will appear in AAAI-14:
  Twenty-Eighth Conference on Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study techniques to incentivize self-interested agents to form socially
desirable solutions in scenarios where they benefit from mutual coordination.
Towards this end, we consider coordination games where agents have different
intrinsic preferences but they stand to gain if others choose the same strategy
as them. For non-trivial versions of our game, stable solutions like Nash
Equilibrium may not exist, or may be socially inefficient even when they do
exist. This motivates us to focus on designing efficient algorithms to compute
(almost) stable solutions like Approximate Equilibrium that can be realized if
agents are provided some additional incentives. Our results apply in many
settings like adoption of new products, project selection, and group formation,
where a central authority can direct agents towards a strategy but agents may
defect if they have better alternatives. We show that for any given instance,
we can either compute a high quality approximate equilibrium or a near-optimal
solution that can be stabilized by providing small payments to some players. We
then generalize our model to encompass situations where player relationships
may exhibit complementarities and present an algorithm to compute an
Approximate Equilibrium whose stability factor is linear in the degree of
complementarity. Our results imply that a little influence is necessary in
order to ensure that selfish players coordinate and form socially efficient
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4731</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4731</id><created>2014-04-18</created><authors><author><keyname>Cibulka</keyname><forenames>Josef</forenames></author><author><keyname>Matou&#x161;ek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Pat&#xe1;k</keyname><forenames>Pavel</forenames></author></authors><title>Three-monotone interpolation</title><categories>cs.CG</categories><msc-class>26B25 (Primary) 90C22, 52A41 (Secondary)</msc-class><journal-ref>Discrete and Computational Geometry 54 (1): 3-21 (2015)</journal-ref><doi>10.1007/s00454-015-9695-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function $f\colon\mathbb R\to\mathbb R$ is called \emph{$k$-monotone} if it
is $(k-2)$-times differentiable and its $(k-2)$nd derivative is convex. A point
set $P\subset\mathbb R^2$ is \emph{$k$-monotone interpolable} if it lies on a
graph of a $k$-monotone function. These notions have been studied in analysis,
approximation theory etc. since the 1940s.
  We show that 3-monotone interpolability is very non-local: we exhibit an
arbitrarily large finite $P$ for which every proper subset is $3$-monotone
interpolable but $P$ itself is not. On the other hand, we prove a Ramsey-type
result: for every $n$ there exists $N$ such that every $N$-point $P$ with
distinct $x$-coordinates contains an $n$-point $Q$ such that $Q$ or its
vertical mirror reflection are $3$-monotone interpolable. The analogs for
$k$-monotone interpolability with $k=1$ and $k=2$ are classical theorems of
Erd\H{o}s and Szekeres, while the cases with $k\ge4$ remain open.
  We also investigate the computational complexity of deciding $3$-monotone
interpolability of a given point set. Using a known characterization, this
decision problem can be stated as an instance of polynomial optimization and
reformulated as a semidefinite program. We exhibit an example for which this
semidefinite program has only doubly exponentially large feasible solutions,
and thus known algorithms cannot solve it in polynomial time. While such
phenomena have been well known for semidefinite programming in general, ours
seems to be the first such example in polynomial optimization, and it involves
only univariate quadratic polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4738</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4738</id><created>2014-04-18</created><authors><author><keyname>Kaushik</keyname><forenames>Ankit</forenames></author><author><keyname>Raza</keyname><forenames>M. Rehan</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>On the Deployment of Cognitive Relay as Underlay Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, 4 tables, accepted in Proceedings of CrownCom
  2014, Oulu (Finland), June 2-4, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to extend the idea of Cognitive Relay (CR).
CR, as a secondary user, follows an underlay paradigm to endorse secondary
usage of the spectrum to the indoor devices. To seek a spatial opportunity,
i.e., deciding its transmission over the primary user channels, CR models its
deployment scenario and the movements of the primary receivers and indoor
devices. Modeling is beneficial for theoretical analysis, however it is also
important to ensure the performance of CR in a real scenario. We consider
briefly, the challenges involved while deploying a hardware prototype of such a
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4740</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4740</id><created>2014-04-18</created><authors><author><keyname>QasemiZadeh</keyname><forenames>Behrang</forenames></author><author><keyname>Rahimi</keyname><forenames>Saeed</forenames></author><author><keyname>Ghalati</keyname><forenames>Mehdi Safaee</forenames></author></authors><title>Challenges in Persian Electronic Text Analysis</title><categories>cs.CL</categories><comments>Appeared in a Local conference 2006, available for the first time</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Farsi, also known as Persian, is the official language of Iran and Tajikistan
and one of the two main languages spoken in Afghanistan. Farsi enjoys a unified
Arabic script as its writing system. In this paper we briefly introduce the
writing standards of Farsi and highlight problems one would face when analyzing
Farsi electronic texts, especially during development of Farsi corpora
regarding to transcription and encoding of Farsi e-texts. The pointes mentioned
may sounds easy but they are crucial when developing and processing written
corpora of Farsi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4744</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4744</id><created>2014-04-18</created><authors><author><keyname>Bohli</keyname><forenames>Jens Mathias</forenames></author><author><keyname>Dobre</keyname><forenames>Dan</forenames></author><author><keyname>Karame</keyname><forenames>Ghassan O.</forenames></author><author><keyname>Li</keyname><forenames>Wenting</forenames></author></authors><title>PrivLoc: Preventing Location Tracking in Geofencing Services</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-based services are increasingly used in our daily activities. In
current services, users however have to give up their location privacy in order
to acquire the service. The literature features a large number of contributions
which aim at enhancing user privacy in location-based services. Most of these
contributions obfuscate the locations of users using spatial and/or temporal
cloaking in order to provide k-anonymity. Although such schemes can indeed
strengthen the location privacy of users, they often decrease the service
quality and do not necessarily prevent the possible tracking of user movements
(i.e., direction, trajectory, velocity). With the rise of Geofencing
applications, tracking of movements becomes more evident since, in these
settings, the service provider is not only requesting a single location of the
user, but requires the movement vectors of users to determine whether the user
has entered/exited a Geofence of interest. In this paper, we propose a novel
solution, PrivLoc, which enables the privacy-preserving outsourcing of
Geofencing and location-based services to the cloud without leaking any
meaningful information about the location, trajectory, and velocity of the
users. Notably, PrivLoc enables an efficient and privacy-preserving
intersection of movement vectors with any polygon of interest, leveraging
functionality from existing Geofencing services or spatial databases. We
analyze the security and privacy provisions of PrivLoc and we evaluate the
performance of our scheme by means of implementation. Our results show that the
performance overhead introduced by PrivLoc can be largely tolerated in
realistic deployment settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4748</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4748</id><created>2014-04-18</created><authors><author><keyname>Shai</keyname><forenames>Saray</forenames></author><author><keyname>Kenett</keyname><forenames>Dror Y.</forenames></author><author><keyname>Kenett</keyname><forenames>Yoed N.</forenames></author><author><keyname>Faust</keyname><forenames>Miriam</forenames></author><author><keyname>Dobson</keyname><forenames>Simon</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Resilience of modular complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks often have a modular structure, where a number of tightly-
connected groups of nodes (modules) have relatively few interconnections.
Modularity had been shown to have an important effect on the evolution and
stability of biological networks, on the scalability and efficiency of
large-scale infrastructure, and the development of economic and social systems.
An analytical framework for understanding modularity and its effects on network
vulnerability is still missing. Through recent advances in the understanding of
multilayer networks, however, it is now possible to develop a theoretical
framework to systematically study this critical issue. Here we study,
analytically and numerically, the resilience of modular networks under attacks
on interconnected nodes, which exhibit high betweenness values and are often
more exposed to failure. Our model provides new understandings into the
feedback between structure and function in real world systems, and consequently
has important implications as diverse as developing efficient immunization
strategies, designing robust large-scale infrastructure, and understanding
brain function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4749</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4749</id><created>2014-04-18</created><updated>2014-11-04</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Bracher</keyname><forenames>Annina</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Decoding binary node labels from censored edge measurements: Phase
  transition and efficient recovery</title><categories>cs.IT cs.DS math.IT</categories><comments>will appear in the IEEE Transactions on Network Science and
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of clustering a graph $G$ into two communities by
observing a subset of the vertex correlations. Specifically, we consider the
inverse problem with observed variables $Y=B_G x \oplus Z$, where $B_G$ is the
incidence matrix of a graph $G$, $x$ is the vector of unknown vertex variables
(with a uniform prior) and $Z$ is a noise vector with Bernoulli$(\varepsilon)$
i.i.d. entries. All variables and operations are Boolean. This model is
motivated by coding, synchronization, and community detection problems. In
particular, it corresponds to a stochastic block model or a correlation
clustering problem with two communities and censored edges. Without noise,
exact recovery (up to global flip) of $x$ is possible if and only the graph $G$
is connected, with a sharp threshold at the edge probability $\log(n)/n$ for
Erd\H{o}s-R\'enyi random graphs. The first goal of this paper is to determine
how the edge probability $p$ needs to scale to allow exact recovery in the
presence of noise. Defining the degree (oversampling) rate of the graph by
$\alpha =np/\log(n)$, it is shown that exact recovery is possible if and only
if $\alpha &gt;2/(1-2\varepsilon)^2+ o(1/(1-2\varepsilon)^2)$. In other words,
$2/(1-2\varepsilon)^2$ is the information theoretic threshold for exact
recovery at low-SNR. In addition, an efficient recovery algorithm based on
semidefinite programming is proposed and shown to succeed in the threshold
regime up to twice the optimal rate. For a deterministic graph $G$, defining
the degree rate as $\alpha=d/\log(n)$, where $d$ is the minimum degree of the
graph, it is shown that the proposed method achieves the rate $\alpha&gt;
4((1+\lambda)/(1-\lambda)^2)/(1-2\varepsilon)^2+ o(1/(1-2\varepsilon)^2)$,
where $1-\lambda$ is the spectral gap of the graph $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4754</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4754</id><created>2014-04-18</created><authors><author><keyname>Siris</keyname><forenames>Vasilios A.</forenames></author><author><keyname>Vasilakos</keyname><forenames>Xenofon</forenames></author><author><keyname>Polyzos</keyname><forenames>George C.</forenames></author></authors><title>Efficient Proactive Caching for Supporting Seamless Mobility</title><categories>cs.NI</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed proactive caching approach that exploits user
mobility information to decide where to proactively cache data to support
seamless mobility, while efficiently utilizing cache storage using a congestion
pricing scheme. The proposed approach is applicable to the case where objects
have different sizes and to a two-level cache hierarchy, for both of which the
proactive caching problem is hard. Additionally, our modeling framework
considers the case where the delay is independent of the requested data object
size and the case where the delay is a function of the object size. Our
evaluation results show how various system parameters influence the delay gains
of the proposed approach, which achieves robust and good performance relative
to an oracle and an optimal scheme for a flat cache structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4757</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4757</id><created>2014-04-18</created><authors><author><keyname>D&#xed;az</keyname><forenames>Josep</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>Perarnau</keyname><forenames>Guillem</forenames></author><author><keyname>P&#xe9;rez-Gim&#xe9;nez</keyname><forenames>Xavier</forenames></author></authors><title>On the relation between graph distance and Euclidean distance in random
  geometric graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given any two vertices u, v of a random geometric graph, denote by d_E(u,v)
their Euclidean distance and by d_G(u,v) their graph distance. The problem of
finding upper bounds on d_G(u,v) in terms of d_E(u,v) has received a lot of
attention in the literature. In this paper, we improve these upper bounds for
values of r=omega(sqrt(log n)) (i.e. for r above the connectivity threshold).
Our result also improves the best-known estimates on the diameter of random
geometric graphs. We also provide a lower bound on d_G(u,v) in terms of
d_E(u,v).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4761</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4761</id><created>2014-04-18</created><updated>2014-04-26</updated><authors><author><keyname>Zewail</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Mohasseb</keyname><forenames>Yahya</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>Using Network Coding to Achieve the Capacity of Deterministic Relay
  Networks with Relay Messages</title><categories>cs.IT math.IT</categories><comments>12 pages, 5 figures, submitted to IEEE JSAC Network coding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive the capacity of the deterministic relay networks
with relay messages. We consider a network which consists of five nodes, four
of which can only communicate via the fifth one. However, the fifth node is not
merely a relay as it may exchange private messages with the other network
nodes. First, we develop an upper bound on the capacity region based on the
notion of a single sided genie. In the course of the achievability proof, we
also derive the deterministic capacity of a 4-user relay network (without
private messages at the relay). The capacity achieving schemes use a
combination of two network coding techniques: the Simple Ordering Scheme (SOS)
and Detour Schemes (DS). In the SOS, we order the transmitted bits at each user
such that the bi-directional messages will be received at the same channel
level at the relay, while the basic idea behind the DS is that some parts of
the message follow an indirect path to their respective destinations. This
paper, therefore, serves to show that user cooperation and network coding can
enhance throughput, even when the users are not directly connected to each
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4766</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4766</id><created>2014-04-18</created><authors><author><keyname>Feuerstein</keyname><forenames>Esteban</forenames></author><author><keyname>Marchetti-Spaccamela</keyname><forenames>Alberto</forenames></author><author><keyname>Schalekamp</keyname><forenames>Frans</forenames></author><author><keyname>Sitters</keyname><forenames>Rene</forenames></author><author><keyname>van der Ster</keyname><forenames>Suzanne</forenames></author><author><keyname>Stougie</keyname><forenames>Leen</forenames></author><author><keyname>van Zuylen</keyname><forenames>Anke</forenames></author></authors><title>Scheduling over Scenarios on Two Machines</title><categories>cs.DS</categories><comments>To appear in COCOON 2014. The final publication is available at
  link.springer.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider scheduling problems over scenarios where the goal is to find a
single assignment of the jobs to the machines which performs well over all
possible scenarios. Each scenario is a subset of jobs that must be executed in
that scenario and all scenarios are given explicitly. The two objectives that
we consider are minimizing the maximum makespan over all scenarios and
minimizing the sum of the makespans of all scenarios. For both versions, we
give several approximation algorithms and lower bounds on their
approximability. With this research into optimization problems over scenarios,
we have opened a new and rich field of interesting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4767</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4767</id><created>2014-04-18</created><authors><author><keyname>Elango</keyname><forenames>Venmugil</forenames><affiliation>CSE</affiliation></author><author><keyname>Rastello</keyname><forenames>Fabrice</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Pouchet</keyname><forenames>Louis-No&#xeb;l</forenames><affiliation>UCLA-CS</affiliation></author><author><keyname>Ramanujam</keyname><forenames>J.</forenames><affiliation>ECE</affiliation></author><author><keyname>Sadayappan</keyname><forenames>P.</forenames><affiliation>CSE</affiliation></author></authors><title>On Characterizing the Data Movement Complexity of Computational DAGs for
  Parallel Execution</title><categories>cs.DC cs.DS</categories><proxy>ccsd</proxy><report-no>RR-8522</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology trends are making the cost of data movement increasingly dominant,
both in terms of energy and time, over the cost of performing arithmetic
operations in computer systems. The fundamental ratio of aggregate data
movement bandwidth to the total computational power (also referred to the
machine balance parameter) in parallel computer systems is decreasing. It is
there- fore of considerable importance to characterize the inherent data
movement requirements of parallel algorithms, so that the minimal architectural
balance parameters required to support it on future systems can be well
understood. In this paper, we develop an extension of the well-known red-blue
pebble game to develop lower bounds on the data movement complexity for the
parallel execution of computational directed acyclic graphs (CDAGs) on parallel
systems. We model multi-node multi-core parallel systems, with the total
physical memory distributed across the nodes (that are connected through some
interconnection network) and in a multi-level shared cache hierarchy for
processors within a node. We also develop new techniques for lower bound
characterization of non-homogeneous CDAGs. We demonstrate the use of the
methodology by analyzing the CDAGs of several numerical algorithms, to develop
lower bounds on data movement for their parallel execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4768</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4768</id><created>2014-04-18</created><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author></authors><title>Nearly Optimal Computations with Structured Matrices</title><categories>cs.SC</categories><comments>(2014-04-10)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We estimate the Boolean complexity of multiplication of structured matrices
by a vector and the solution of nonsingular linear systems of equations with
these matrices. We study four basic most popular classes, that is, Toeplitz,
Hankel, Cauchy and Van-der-monde matrices, for which the cited computational
problems are equivalent to the task of polynomial multiplication and division
and polynomial and rational multipoint evaluation and interpolation. The
Boolean cost estimates for the latter problems have been obtained by Kirrinnis
in \cite{kirrinnis-joc-1998}, except for rational interpolation, which we
supply now. All known Boolean cost estimates for these problems rely on using
Kronecker product. This implies the $d$-fold precision increase for the $d$-th
degree output, but we avoid such an increase by relying on distinct techniques
based on employing FFT. Furthermore we simplify the analysis and make it more
transparent by combining the representation of our tasks and algorithms in
terms of both structured matrices and polynomials and rational functions. This
also enables further extensions of our estimates to cover Trummer's important
problem and computations with the popular classes of structured matrices that
generalize the four cited basic matrix classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4772</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4772</id><created>2014-04-18</created><updated>2014-06-16</updated><authors><author><keyname>Magron</keyname><forenames>Victor</forenames><affiliation>LAAS</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS, CTU/FEE</affiliation></author><author><keyname>Lasserre</keyname><forenames>Jean-Bernard</forenames><affiliation>LAAS</affiliation></author></authors><title>Approximating Pareto Curves using Semidefinite Relaxations</title><categories>math.OC cs.RO</categories><proxy>ccsd</proxy><report-no>Rapport LAAS n&amp;deg; 14137</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing an approximation of the Pareto curve
associated with the multiobjective optimization problem $\min_{\mathbf{x} \in
\mathbf{S}}\{ (f_1(\mathbf{x}), f_2(\mathbf{x})) \}$, where $f_1$ and $f_2$ are
two conflicting polynomial criteria and $\mathbf{S} \subset \mathbb{R}^n$ is a
compact basic semialgebraic set. We provide a systematic numerical scheme to
approximate the Pareto curve. We start by reducing the initial problem into a
scalarized polynomial optimization problem (POP). Three scalarization methods
lead to consider different parametric POPs, namely (a) a weighted convex sum
approximation, (b) a weighted Chebyshev approximation, and (c) a parametric
sublevel set approximation. For each case, we have to solve a semidefinite
programming (SDP) hierarchy parametrized by the number of moments or
equivalently the degree of a polynomial sums of squares approximation of the
Pareto curve. When the degree of the polynomial approximation tends to
infinity, we provide guarantees of convergence to the Pareto curve in
$L^2$-norm for methods (a) and (b), and $L^1$-norm for method (c).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4774</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4774</id><created>2014-04-18</created><updated>2014-10-22</updated><authors><author><keyname>Jing</keyname><forenames>Wang</forenames></author><author><keyname>Zhong-Qiu</keyname><forenames>Zhao</forenames></author><author><keyname>Xuegang</keyname><forenames>Hu</forenames></author><author><keyname>Yiu-ming</keyname><forenames>Cheung</forenames></author><author><keyname>Meng</keyname><forenames>Wang</forenames></author><author><keyname>Xindong</keyname><forenames>Wu</forenames></author></authors><title>Online Group Feature Selection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online feature selection with dynamic features has become an active research
area in recent years. However, in some real-world applications such as image
analysis and email spam filtering, features may arrive by groups. Existing
online feature selection methods evaluate features individually, while existing
group feature selection methods cannot handle online processing. Motivated by
this, we formulate the online group feature selection problem, and propose a
novel selection approach for this problem. Our proposed approach consists of
two stages: online intra-group selection and online inter-group selection. In
the intra-group selection, we use spectral analysis to select discriminative
features in each group when it arrives. In the inter-group selection, we use
Lasso to select a globally optimal subset of features. This 2-stage procedure
continues until there are no more features to come or some predefined stopping
conditions are met. Extensive experiments conducted on benchmark and real-world
data sets demonstrate that our proposed approach outperforms other
state-of-the-art online feature selection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4775</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4775</id><created>2014-04-18</created><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author></authors><title>Accelerated Approximation of the Complex Roots of a Univariate
  Polynomial (Extended Abstract)</title><categories>cs.SC</categories><comments>(10/04/2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly efficient and even nearly optimal algorithms have been developed for
the classical problem of univariate polynomial root-finding (see, e.g.,
\cite{P95}, \cite{P02}, \cite{MNP13}, and the bibliography therein), but this
is still an area of active research. By combining some powerful techniques
developed in this area we devise new nearly optimal algorithms, whose
substantial merit is their simplicity, important for the implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4780</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4780</id><created>2014-04-18</created><authors><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Li</keyname><forenames>Peipei</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Hu</keyname><forenames>Xuegang</forenames></author></authors><title>Robust Face Recognition via Adaptive Sparse Representation</title><categories>cs.CV</categories><doi>10.1109/TCYB.2014.2307067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Representation (or coding) based Classification (SRC) has gained great
success in face recognition in recent years. However, SRC emphasizes the
sparsity too much and overlooks the correlation information which has been
demonstrated to be critical in real-world face recognition problems. Besides,
some work considers the correlation but overlooks the discriminative ability of
sparsity. Different from these existing techniques, in this paper, we propose a
framework called Adaptive Sparse Representation based Classification (ASRC) in
which sparsity and correlation are jointly considered. Specifically, when the
samples are of low correlation, ASRC selects the most discriminative samples
for representation, like SRC; when the training samples are highly correlated,
ASRC selects most of the correlated and discriminative samples for
representation, rather than choosing some related samples randomly. In general,
the representation model is adaptive to the correlation structure, which
benefits from both $\ell_1$-norm and $\ell_2$-norm.
  Extensive experiments conducted on publicly available data sets verify the
effectiveness and robustness of the proposed algorithm by comparing it with
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4785</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4785</id><created>2014-04-18</created><authors><author><keyname>Verhodubs</keyname><forenames>Olegs</forenames></author></authors><title>Ontology as a Source for Rule Generation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discloses the potential of OWL (Web Ontology Language) ontologies
for generation of rules. The main purpose of this paper is to identify new
types of rules, which may be generated from OWL ontologies. Rules, generated
from OWL ontologies, are necessary for the functioning of the Semantic Web
Expert System. It is expected that the Semantic Web Expert System (SWES) will
be able to process ontologies from the Web with the purpose to supplement or
even to develop its knowledge base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4789</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4789</id><created>2014-04-18</created><authors><author><keyname>Mo</keyname><forenames>Hongming</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A new combination approach based on improved evidence distance</title><categories>cs.AI</categories><comments>14 pages, 1 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer evidence theory is a powerful tool in information fusion.
When the evidence are highly conflicting, the counter-intuitive results will be
presented. To adress this open issue, a new method based on evidence distance
of Jousselme and Hausdorff distance is proposed. Weight of each evidence can be
computed, preprocess the original evidence to generate a new evidence. The
Dempster's combination rule is used to combine the new evidence. Comparing with
the existing methods, the new proposed method is efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4791</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4791</id><created>2014-03-20</created><authors><author><keyname>Pourghasem</keyname><forenames>Mona</forenames></author><author><keyname>Sheikhloo</keyname><forenames>Elham Ghare</forenames></author><author><keyname>Atani</keyname><forenames>Reza Ebrahimi</forenames></author></authors><title>Light Weight Implementation of Stream Ciphers for M-Commerce
  Applications</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world the use of computer and telecommunications networking is
essential for human life. Among these, mobile tools and devices due to
availability, have found a special impact on everyone life. This feature
addition to providing sample facilities such as financial transactions at any
place and time has raised the Sensitivities about security of these devices. In
order to provide security, numerous techniques have been proposed which due to
the limitations of mobile devices; an algorithm should be taken that have the
ability to function for light weight ubiquitous computing. In this paper, four
eSTREAM candidates from software profile were taken into account and analyzed
and implemented by using J2ME technology. Then these algorithms were
implemented on a variety of mobile phones and are compared with each other in
terms of execution time and finally the obtained Results are expressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4797</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4797</id><created>2014-04-18</created><updated>2015-01-26</updated><authors><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Parallel Graph Partitioning for Complex Networks</title><categories>cs.DC cs.DS cs.NE cs.SI physics.soc-ph</categories><comments>Review article. Parallelization of our previous approach
  arXiv:1402.3281</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processing large complex networks like social networks or web graphs has
recently attracted considerable interest. In order to do this in parallel, we
need to partition them into pieces of about equal size. Unfortunately, previous
parallel graph partitioners originally developed for more regular mesh-like
networks do not work well for these networks. This paper addresses this problem
by parallelizing and adapting the label propagation technique originally
developed for graph clustering. By introducing size constraints, label
propagation becomes applicable for both the coarsening and the refinement phase
of multilevel graph partitioning. We obtain very high quality by applying a
highly parallel evolutionary algorithm to the coarsened graph. The resulting
system is both more scalable and achieves higher quality than state-of-the-art
systems like ParMetis or PT-Scotch. For large complex networks the performance
differences are very big. For example, our algorithm can partition a web graph
with 3.3 billion edges in less than sixteen seconds using 512 cores of a high
performance cluster while producing a high quality partition -- none of the
competing systems can handle this graph on our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4800</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4800</id><created>2014-04-16</created><authors><author><keyname>Sinha</keyname><forenames>Ayushi</forenames></author><author><keyname>Roncal</keyname><forenames>William Gray</forenames></author><author><keyname>Kasthuri</keyname><forenames>Narayanan</forenames></author><author><keyname>Chuang</keyname><forenames>Ming</forenames></author><author><keyname>Manavalan</keyname><forenames>Priya</forenames></author><author><keyname>Kleissas</keyname><forenames>Dean M.</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Lichtman</keyname><forenames>Jeff W.</forenames></author><author><keyname>Kazhdan</keyname><forenames>Michael</forenames></author></authors><title>Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes</title><categories>cs.CV</categories><comments>2 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new pipeline which automatically identifies and
annotates axoplasmic reticula, which are small subcellular structures present
only in axons. We run our algorithm on the Kasthuri11 dataset, which was color
corrected using gradient-domain techniques to adjust contrast. We use a
bilateral filter to smooth out the noise in this data while preserving edges,
which highlights axoplasmic reticula. These axoplasmic reticula are then
annotated using a morphological region growing algorithm. Additionally, we
perform Laplacian sharpening on the bilaterally filtered data to enhance edges,
and repeat the morphological region growing algorithm to annotate more
axoplasmic reticula. We track our annotations through the slices to improve
precision, and to create long objects to aid in segment merging. This method
annotates axoplasmic reticula with high precision. Our algorithm can easily be
adapted to annotate axoplasmic reticula in different sets of brain data by
changing a few thresholds. The contribution of this work is the introduction of
a straightforward and robust pipeline which annotates axoplasmic reticula with
high precision, contributing towards advancements in automatic feature
annotations in neural EM data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4801</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4801</id><created>2014-04-17</created><authors><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Generalized Evidence Theory</title><categories>cs.AI</categories><comments>39 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conflict management is still an open issue in the application of Dempster
Shafer evidence theory. A lot of works have been presented to address this
issue. In this paper, a new theory, called as generalized evidence theory
(GET), is proposed. Compared with existing methods, GET assumes that the
general situation is in open world due to the uncertainty and incomplete
knowledge. The conflicting evidence is handled under the framework of GET. It
is shown that the new theory can explain and deal with the conflicting evidence
in a more reasonable way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4805</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4805</id><created>2014-04-18</created><authors><author><keyname>Ochs</keyname><forenames>Peter</forenames></author><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>iPiano: Inertial Proximal Algorithm for Non-Convex Optimization</title><categories>cs.CV math.OC</categories><comments>32pages, 7 figures, to appear in SIAM Journal on Imaging Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study an algorithm for solving a minimization problem
composed of a differentiable (possibly non-convex) and a convex (possibly
non-differentiable) function. The algorithm iPiano combines forward-backward
splitting with an inertial force. It can be seen as a non-smooth split version
of the Heavy-ball method from Polyak. A rigorous analysis of the algorithm for
the proposed class of problems yields global convergence of the function values
and the arguments. This makes the algorithm robust for usage on non-convex
problems. The convergence result is obtained based on the \KL inequality. This
is a very weak restriction, which was used to prove convergence for several
other gradient methods. First, an abstract convergence theorem for a generic
algorithm is proved, and, then iPiano is shown to satisfy the requirements of
this theorem. Furthermore, a convergence rate is established for the general
problem class. We demonstrate iPiano on computer vision problems: image
denoising with learned priors and diffusion based image compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4806</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4806</id><created>2014-04-18</created><updated>2014-09-18</updated><authors><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author><author><keyname>Ventre</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Prete</keyname><forenames>Luca</forenames></author><author><keyname>Siracusano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Gerola</keyname><forenames>Matteo</forenames></author><author><keyname>Salvadori</keyname><forenames>Elio</forenames></author></authors><title>OSHI - Open Source Hybrid IP/SDN networking (and its emulation on
  Mininet and on distributed SDN testbeds)</title><categories>cs.NI</categories><comments>Final version (Last updated August, 2014)</comments><journal-ref>EWSDN 2014, 3rd European Workshop on Software Defined Networks,
  1-3 September 2014, Budapest, Hungary</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of SDN in IP backbones requires the coexistence of regular
IP forwarding and SDN based forwarding. The former is typically applied to best
effort Internet traffic, the latter can be used for different types of advanced
services (VPNs, Virtual Leased Lines, Traffic Engineering...). In this paper we
first introduce the architecture and the services of an &quot;hybrid&quot; IP/SDN
networking scenario. Then we describe the design and implementation of an Open
Source Hybrid IP/SDN (OSHI) node. It combines Quagga for OSPF routing and Open
vSwitch for OpenFlow based switching on Linux. The availability of tools for
experimental validation and performance evaluation of SDN solutions is
fundamental for the evolution of SDN. We provide a set of open source tools
that allow to facilitate the design of hybrid IP/SDN experimental networks,
their deployment on Mininet or on distributed SDN research testbeds and their
test. Finally, using the provided tools, we evaluate key performance aspects of
the proposed solutions. The OSHI development and test environment is available
in a VirtualBox VM image that can be downloaded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4814</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4814</id><created>2014-04-18</created><updated>2014-05-09</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Gog</keyname><forenames>Simon</forenames></author><author><keyname>Manzini</keyname><forenames>Giovanni</forenames></author><author><keyname>Sir&#xe9;n</keyname><forenames>Jouni</forenames></author></authors><title>Reusing an FM-index</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitively, if two strings $S_1$ and $S_2$ are sufficiently similar and we
already have an FM-index for $S_1$ then, by storing a little extra information,
we should be able to reuse parts of that index in an FM-index for $S_2$. We
formalize this intuition and show that it can lead to significant space savings
in practice, as well as to some interesting theoretical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4818</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4818</id><created>2014-04-18</created><authors><author><keyname>Jagerman</keyname><forenames>Rolf</forenames><affiliation>course supervisor</affiliation></author><author><keyname>Sab&#xe9;e</keyname><forenames>Wendo</forenames><affiliation>course supervisor</affiliation></author><author><keyname>Versluis</keyname><forenames>Laurens</forenames><affiliation>course supervisor</affiliation></author><author><keyname>de Vos</keyname><forenames>Martijn</forenames><affiliation>course supervisor</affiliation></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames><affiliation>course supervisor</affiliation></author></authors><title>The fifteen year struggle of decentralizing privacy-enhancing technology</title><categories>cs.CY cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ever since the introduction of the internet, it has been void of any privacy.
The majority of internet traffic currently is and always has been unencrypted.
A number of anonymous communication overlay networks exist whose aim it is to
provide privacy to its users. However, due to the nature of the internet, there
is major difficulty in getting these networks to become both decentralized and
anonymous. We list reasons for having anonymous networks, discern the problems
in achieving decentralization and sum up the biggest initiatives in the field
and their current status. To do so, we use one exemplary network, the Tor
network. We explain how Tor works, what vulnerabilities this network currently
has, and possible attacks that could be used to violate privacy and anonymity.
The Tor network is used as a key comparison network in the main part of the
report: a tabular overview of the major anonymous networking technologies in
use today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4820</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4820</id><created>2014-04-17</created><authors><author><keyname>Guo</keyname><forenames>Xu</forenames></author><author><keyname>Zhang</keyname><forenames>Weisheng</forenames></author><author><keyname>Zhong</keyname><forenames>Wenliang</forenames></author></authors><title>Topology optimization based on moving deformable components: A new
  computational framework</title><categories>cs.CE physics.comp-ph</categories><comments>46 pages, 22 figures</comments><report-no>DLUT-DEM-2014-18</report-no><journal-ref>Journal of Applied Mechanics Vol.81 (2014) 081009</journal-ref><doi>10.1115/1.4027609</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present work, a new computational framework for structural topology
optimization based on the concept of moving deformable components is proposed.
Compared with the traditional pixel or node point-based solution framework, the
proposed solution paradigm can incorporate more geometry and mechanical
information into topology optimization directly and therefore render the
solution process more flexible. It also has the great potential to reduce the
computational burden associated with topology optimization substantially. Some
representative examples are presented to illustrate the effectiveness of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4821</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4821</id><created>2014-04-18</created><authors><author><keyname>Kovalchuk</keyname><forenames>Sergey V.</forenames></author><author><keyname>Zakharchuk</keyname><forenames>Artem V.</forenames></author><author><keyname>Liao</keyname><forenames>Jiaqi</forenames></author><author><keyname>Ivanov</keyname><forenames>Sergey V.</forenames></author><author><keyname>Boukhanovsky</keyname><forenames>Alexander V.</forenames></author></authors><title>A Technology for BigData Analysis Task Description using Domain-Specific
  Languages</title><categories>cs.DC cs.DB cs.PL</categories><comments>To appear in Proceedings of the International Conference on
  Computational Science (ICCS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents a technology for dynamic knowledge-based building of
Domain-Specific Languages (DSL) to describe data-intensive scientific discovery
tasks using BigData technology. The proposed technology supports high level
abstract definition of analytic and simulation parts of the task as well as
integration into the composite scientific solutions. Automatic translation of
the abstract task definition enables seamless integration of various data
sources within single solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4822</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4822</id><created>2014-04-18</created><updated>2014-08-06</updated><authors><author><keyname>Flint</keyname><forenames>Ian</forenames></author><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Privault</keyname><forenames>Nicolas</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author></authors><title>Performance Analysis of Ambient RF Energy Harvesting: A Stochastic
  Geometry Approach</title><categories>cs.IT math.IT</categories><comments>(to be presented in IEEE Globecom 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ambient RF (Radio Frequency) energy harvesting technique has recently been
proposed as a potential solution to provide proactive energy replenishment for
wireless devices. This paper aims to analyze the performance of a battery-free
wireless sensor powered by ambient RF energy harvesting using a
stochastic-geometry approach. Specifically, we consider a random network model
in which ambient RF sources are distributed as a Ginibre alpha?-determinantal
point process which recovers the Poisson point process when alpha? approaches
zero. We characterize the expected RF energy harvesting rate.We also perform a
worst-case study which derives the upper bounds of both power outage and
transmission outage probabilities. Numerical results show that our upper bounds
are accurate and that better performance is achieved when the distribution of
ambient sources exhibits stronger repulsion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4827</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4827</id><created>2014-04-18</created><authors><author><keyname>Colcolmbet</keyname><forenames>Thomas</forenames></author><author><keyname>Manuel</keyname><forenames>Amaldev</forenames></author></authors><title>$\mu$-calculus on data words</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the decidability and expressiveness issues of $\mu$-calculus on data
words and data $\omega$-words. It is shown that the full logic as well as the
fragment which uses only the least fixpoints are undecidable, while the
fragment containing only greatest fixpoints is decidable. Two subclasses,
namely BMA and BR, obtained by limiting the compositions of formulas and their
automata characterizations are exhibited. Furthermore, Data-LTL and
two-variable first-order logic are expressed as unary alternation-free fragment
of BMA. Finally basic inclusions of the fragments are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4834</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4834</id><created>2014-04-18</created><updated>2014-05-18</updated><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>de Oliveira</keyname><forenames>Rafael Mendes</forenames></author></authors><title>Factors of Sparse Polynomials are Sparse</title><categories>cs.CC cs.DM math.AG math.CO</categories><comments>This paper was removed due to an error in the proof (Claim 4.12 as
  stated is not true)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper was removed due to an error in the proof (Claim 4.12 as stated is
not true). The authors would like to thank Ilya Volkovich for pointing out a
counterexample to this paper's main result in positive characteristic: If $F$
is a field with prime characteristic $p$, then the polynomial $x_1^p + x_2^p +
\ldots + x^n^p$ has the following factor: $(x_1+x_2+ \ldots + x_n)^{p-1}$,
which has sparsity $n^p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4851</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4851</id><created>2014-04-18</created><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Rubin</keyname><forenames>Natan</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Kinetic Voronoi Diagrams and Delaunay Triangulations under Polygonal
  Distance Functions</title><categories>cs.CG cs.DS math.MG</categories><acm-class>F.2.2; G.2.1; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points and $Q$ a convex $k$-gon in ${\mathbb R}^2$.
We analyze in detail the topological (or discrete) changes in the structure of
the Voronoi diagram and the Delaunay triangulation of $P$, under the convex
distance function defined by $Q$, as the points of $P$ move along prespecified
continuous trajectories. Assuming that each point of $P$ moves along an
algebraic trajectory of bounded degree, we establish an upper bound of
$O(k^4n\lambda_r(n))$ on the number of topological changes experienced by the
diagrams throughout the motion; here $\lambda_r(n)$ is the maximum length of an
$(n,r)$-Davenport-Schinzel sequence, and $r$ is a constant depending on the
algebraic degree of the motion of the points. Finally, we describe an algorithm
for efficiently maintaining the above structures, using the kinetic data
structure (KDS) framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4852</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4852</id><created>2014-04-18</created><updated>2014-08-18</updated><authors><author><keyname>Surowka</keyname><forenames>Robert L.</forenames></author><author><keyname>Regan</keyname><forenames>Kenneth W.</forenames></author></authors><title>Polynomials Modulo Composite Numbers: Ax-Katz type theorems for the
  structure of their solution sets</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the Ax-Katz theorem for a single polynomial from finite fields to
the rings Z_m with m composite. This extension not only yields the analogous
result, but gives significantly higher divisibility bounds. We conjecture what
computer runs suggest is the optimal result for any m, and prove a special case
of it. The special case is for m = 2^r and polynomials of degree 2. Our results
also yield further properties of the solution spaces. Polynomials modulo
composites are the focus of some computational complexity lower bound
frontiers, while those modulo 2^r arise in the simulation of quantum circuits.
We give some prospective applications of this research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4856</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4856</id><created>2014-04-18</created><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Quantitative games with interval objectives</title><categories>cs.LO cs.GT</categories><comments>Full version of CONCUR submission</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally quantitative games such as mean-payoff games and discount sum
games have two players -- one trying to maximize the payoff, the other trying
to minimize it. The associated decision problem, &quot;Can Eve (the maximizer)
achieve, for example, a positive payoff?&quot; can be thought of as one player
trying to attain a payoff in the interval $(0,\infty)$. In this paper we
consider the more general problem of determining if a player can attain a
payoff in a finite union of arbitrary intervals for various payoff functions
(liminf, mean-payoff, discount sum, total sum). In particular this includes the
interesting exact-value problem, &quot;Can Eve achieve a payoff of exactly (e.g.)
0?&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4859</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4859</id><created>2014-04-18</created><authors><author><keyname>Accisano</keyname><forenames>Paul</forenames></author><author><keyname>&#xdc;ng&#xf6;r</keyname><forenames>Alper</forenames></author></authors><title>Matching Curves to Imprecise Point Sets using Fr\'echet Distance</title><categories>cs.CG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a polygonal curve in $\mathbb{R}^d$ of length $n$, and $S$ be a
point-set of size $k$. The Curve/Point Set Matching problem consists of finding
a polygonal curve $Q$ on $S$ such that the Fr\'echet distance from $P$ is less
than a given $\varepsilon$. We consider eight variations of the problem based
on the distance metric used and the omittability or repeatability of the
points. We provide closure to a recent series of complexity results for the
case where $S$ consists of precise points. More importantly, we formulate a
more realistic version of the problem that takes into account measurement
errors. This new problem is posed as the matching of a given curve to a set of
imprecise points. We prove that all three variations of the problem that are in
P when $S$ consists of precise points become NP-complete when $S$ consists of
imprecise points. We also discuss approximation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4865</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4865</id><created>2014-04-18</created><updated>2014-04-21</updated><authors><author><keyname>Wang</keyname><forenames>Huangxin</forenames></author><author><keyname>Zhang</keyname><forenames>Jean X.</forenames></author><author><keyname>Li</keyname><forenames>Fei</forenames></author></authors><title>On Time-Sensitive Revenue Management and Energy Scheduling in Green Data
  Centers</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design an analytically and experimentally better online
energy and job scheduling algorithm with the objective of maximizing net profit
for a service provider in green data centers. We first study the previously
known algorithms and conclude that these online algorithms have provable poor
performance against their worst-case scenarios. To guarantee an online
algorithm's performance in hindsight, we design a randomized algorithm to
schedule energy and jobs in the data centers and prove the algorithm's expected
competitive ratio in various settings. Our algorithm is theoretical-sound and
it outperforms the previously known algorithms in many settings using both real
traces and simulated data. An optimal offline algorithm is also implemented as
an empirical benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4880</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4880</id><created>2014-04-18</created><authors><author><keyname>Nascimento</keyname><forenames>Abra&#xe3;o D. C.</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author><author><keyname>Cintra</keyname><forenames>Renato J.</forenames></author></authors><title>Bias Correction and Modified Profile Likelihood under the Wishart
  Complex Distribution</title><categories>cs.CV stat.ME</categories><journal-ref>IEEE Transactions on Geoscience and Remote Sensing, vol. 52, issue
  8, August, pages 4932--4941, 2014</journal-ref><doi>10.1109/TGRS.2013.2285927</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes improved methods for the maximum likelihood (ML)
estimation of the equivalent number of looks $L$. This parameter has a
meaningful interpretation in the context of polarimetric synthetic aperture
radar (PolSAR) images. Due to the presence of coherent illumination in their
processing, PolSAR systems generate images which present a granular noise
called speckle. As a potential solution for reducing such interference, the
parameter $L$ controls the signal-noise ratio. Thus, the proposal of efficient
estimation methodologies for $L$ has been sought. To that end, we consider
firstly that a PolSAR image is well described by the scaled complex Wishart
distribution. In recent years, Anfinsen et al. derived and analyzed estimation
methods based on the ML and on trace statistical moments for obtaining the
parameter $L$ of the unscaled version of such probability law. This paper
generalizes that approach. We present the second-order bias expression proposed
by Cox and Snell for the ML estimator of this parameter. Moreover, the formula
of the profile likelihood modified by Barndorff-Nielsen in terms of $L$ is
discussed. Such derivations yield two new ML estimators for the parameter $L$,
which are compared to the estimators proposed by Anfinsen et al. The
performance of these estimators is assessed by means of Monte Carlo
experiments, adopting three statistical measures as comparison criterion: the
mean square error, the bias, and the coefficient of variation. Equivalently to
the simulation study, an application to actual PolSAR data concludes that the
proposed estimators outperform all the others in homogeneous scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4884</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4884</id><created>2014-04-18</created><authors><author><keyname>Eubanks</keyname><forenames>David A.</forenames></author></authors><title>Causal Interfaces</title><categories>cs.AI math.ST stat.TH</categories><comments>20 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interaction of two binary variables, assumed to be empirical
observations, has three degrees of freedom when expressed as a matrix of
frequencies. Usually, the size of causal influence of one variable on the other
is calculated as a single value, as increase in recovery rate for a medical
treatment, for example. We examine what is lost in this simplification, and
propose using two interface constants to represent positive and negative
implications separately. Given certain assumptions about non-causal outcomes,
the set of resulting epistemologies is a continuum. We derive a variety of
particular measures and contrast them with the one-dimensional index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4887</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4887</id><created>2014-04-18</created><updated>2014-09-22</updated><authors><author><keyname>Akhremtsev</keyname><forenames>Yaroslav</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>(Semi-)External Algorithms for Graph Partitioning and Clustering</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop semi-external and external memory algorithms for
graph partitioning and clustering problems. Graph partitioning and clustering
are key tools for processing and analyzing large complex networks. We address
both problems in the (semi-)external model by adapting the size-constrained
label propagation technique. Our (semi-)external size-constrained label
propagation algorithm can be used to compute graph clusterings and is a
prerequisite for the (semi-)external graph partitioning algorithm. The
algorithm is then used for both the coarsening and the refinement phase of a
multilevel algorithm to compute graph partitions. Our algorithm is able to
partition and cluster huge complex networks with billions of edges on cheap
commodity machines. Experiments demonstrate that the semi-external graph
partitioning algorithm is scalable and can compute high quality partitions in
time that is comparable to the running time of an efficient internal memory
implementation. A parallelization of the algorithm in the semi-external model
further reduces running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4888</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4888</id><created>2014-04-18</created><updated>2015-05-27</updated><authors><author><keyname>Nun</keyname><forenames>Isadora</forenames></author><author><keyname>Pichara</keyname><forenames>Karim</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author><author><keyname>Kim</keyname><forenames>Dae-Won</forenames></author></authors><title>Supervised detection of anomalous light-curves in massive astronomical
  catalogs</title><categories>cs.CE astro-ph.IM cs.LG</categories><comments>16 pages, 18 figures, published in The Astrophysical Journal</comments><journal-ref>2014, ApJ, 793, 23</journal-ref><doi>10.1088/0004-637X/793/1/23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of synoptic sky surveys has led to a massive amount of data
for which resources needed for analysis are beyond human capabilities. To
process this information and to extract all possible knowledge, machine
learning techniques become necessary. Here we present a new method to
automatically discover unknown variable objects in large astronomical catalogs.
With the aim of taking full advantage of all the information we have about
known objects, our method is based on a supervised algorithm. In particular, we
train a random forest classifier using known variability classes of objects and
obtain votes for each of the objects in the training set. We then model this
voting distribution with a Bayesian network and obtain the joint voting
distribution among the training objects. Consequently, an unknown object is
considered as an outlier insofar it has a low joint probability. Our method is
suitable for exploring massive datasets given that the training process is
performed offline. We tested our algorithm on 20 millions light-curves from the
MACHO catalog and generated a list of anomalous candidates. We divided the
candidates into two main classes of outliers: artifacts and intrinsic outliers.
Artifacts were principally due to air mass variation, seasonal variation, bad
calibration or instrumental errors and were consequently removed from our
outlier list and added to the training set. After retraining, we selected about
4000 objects, which we passed to a post analysis stage by perfoming a
cross-match with all publicly available catalogs. Within these candidates we
identified certain known but rare objects such as eclipsing Cepheids, blue
variables, cataclysmic variables and X-ray sources. For some outliers there
were no additional information. Among them we identified three unknown
variability types and few individual outliers that will be followed up for a
deeper analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4893</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4893</id><created>2014-04-18</created><authors><author><keyname>Codecasa</keyname><forenames>Daniele</forenames></author><author><keyname>Stella</keyname><forenames>Fabio</forenames></author></authors><title>CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit</title><categories>cs.AI cs.LG cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous time Bayesian network classifiers are designed for temporal
classification of multivariate streaming data when time duration of events
matters and the class does not change over time. This paper introduces the
CTBNCToolkit: an open source Java toolkit which provides a stand-alone
application for temporal classification and a library for continuous time
Bayesian network classifiers. CTBNCToolkit implements the inference algorithm,
the parameter learning algorithm, and the structural learning algorithm for
continuous time Bayesian network classifiers. The structural learning algorithm
is based on scoring functions: the marginal log-likelihood score and the
conditional log-likelihood score are provided. CTBNCToolkit provides also an
implementation of the expectation maximization algorithm for clustering
purpose. The paper introduces continuous time Bayesian network classifiers. How
to use the CTBNToolkit from the command line is described in a specific
section. Tutorial examples are included to facilitate users to understand how
the toolkit must be used. A section dedicate to the Java library is proposed to
help further code extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4895</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4895</id><created>2014-04-18</created><authors><author><keyname>Kramer</keyname><forenames>Raphael</forenames></author><author><keyname>Subramanian</keyname><forenames>Anand</forenames></author><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author><author><keyname>Cabral</keyname><forenames>Luc&#xed;dio dos Anjos Formiga</forenames></author></authors><title>A matheuristic approach for the Pollution-Routing Problem</title><categories>cs.DS</categories><comments>Working Paper -- UFPB, 26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the Pollution-Routing Problem (PRP), a Vehicle Routing
Problem (VRP) with environmental considerations, recently introduced in the
literature by [Bektas and Laporte (2011), Transport. Res. B-Meth. 45 (8),
1232-1250]. The objective is to minimize operational and environmental costs
while respecting capacity constraints and service time windows. Costs are based
on driver wages and fuel consumption, which depends on many factors, such as
travel distance and vehicle load. The vehicle speeds are considered as decision
variables. They complement routing decisions, impacting the total cost, the
travel time between locations, and thus the set of feasible routes. We propose
a method which combines a local search-based metaheuristic with an integer
programming approach over a set covering formulation and a recursive
speed-optimization algorithm. This hybridization enables to integrate more
tightly route and speed decisions. Moreover, two other &quot;green&quot; VRP variants,
the Fuel Consumption VRP (FCVRP) and the Energy Minimizing VRP (EMVRP), are
addressed. The proposed method compares very favorably with previous algorithms
from the literature and many new improved solutions are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4904</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4904</id><created>2014-04-18</created><authors><author><keyname>Bensman</keyname><forenames>Stephen J.</forenames></author><author><keyname>Smolinsky</keyname><forenames>Lawrence J.</forenames></author><author><keyname>Sage</keyname><forenames>Daniel S.</forenames></author></authors><title>Comparison of the Research Effectiveness of Chemistry Nobelists and
  Fields Medalist Mathematicians with Google Scholar: the Yule-Simon Model</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses the Yule-Simon model to estimate to what extent the work of
chemistry Nobelists and Fields medalist mathematicians is incorporated into the
knowledge corpus of their disciplines as measured by Google Scholar inlinks.
Due to differences in the disciplines and prizes, it finds that the work of
chemistry Nobelists is better incorporated than that of Fields medalists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4909</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4909</id><created>2014-04-18</created><updated>2014-06-30</updated><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Sir&#xe9;n</keyname><forenames>Jouni</forenames></author></authors><title>Document Retrieval on Repetitive Collections</title><categories>cs.DS cs.IR</categories><comments>Accepted to ESA 2014. Implementation and experiments at
  http://www.cs.helsinki.fi/group/suds/rlcsa/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document retrieval aims at finding the most important documents where a
pattern appears in a collection of strings. Traditional pattern-matching
techniques yield brute-force document retrieval solutions, which has motivated
the research on tailored indexes that offer near-optimal performance. However,
an experimental study establishing which alternatives are actually better than
brute force, and which perform best depending on the collection
characteristics, has not been carried out. In this paper we address this
shortcoming by exploring the relationship between the nature of the underlying
collection and the performance of current methods. Via extensive experiments we
show that established solutions are often beaten in practice by brute-force
alternatives. We also design new methods that offer superior time/space
trade-offs, particularly on repetitive collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4910</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4910</id><created>2014-04-18</created><authors><author><keyname>Mukherjee</keyname><forenames>Arko Provo</forenames></author><author><keyname>Tirthapura</keyname><forenames>Srikanta</forenames></author></authors><title>Enumerating Maximal Bicliques from a Large Graph using MapReduce</title><categories>cs.DC</categories><comments>A preliminary version of the paper was accepted at the Proceedings of
  the 3rd IEEE International Congress on Big Data 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the enumeration of maximal bipartite cliques (bicliques) from a
large graph, a task central to many practical data mining problems in social
network analysis and bioinformatics. We present novel parallel algorithms for
the MapReduce platform, and an experimental evaluation using Hadoop MapReduce.
Our algorithm is based on clustering the input graph into smaller sized
subgraphs, followed by processing different subgraphs in parallel. Our
algorithm uses two ideas that enable it to scale to large graphs: (1) the
redundancy in work between different subgraph explorations is minimized through
a careful pruning of the search space, and (2) the load on different reducers
is balanced through the use of an appropriate total order among the vertices.
Our evaluation shows that the algorithm scales to large graphs with millions of
edges and tens of mil- lions of maximal bicliques. To our knowledge, this is
the first work on maximal biclique enumeration for graphs of this scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4911</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4911</id><created>2014-04-18</created><updated>2015-09-20</updated><authors><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author></authors><title>Communication Delay Co-Design in $\mathcal{H}_2$ Distributed Control
  Using Atomic Norm Minimization</title><categories>math.OC cs.SY</categories><comments>IEEE Transactions on Control of Network Systems, Conditionally
  Accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When designing distributed controllers for large-scale systems, the
actuation, sensing and communication architectures of the controller can no
longer be taken as given. In particular, controllers implemented using dense
architectures typically outperform controllers implemented using simpler ones
-- however, it is also desirable to minimize the cost of building the
architecture used to implement a controller. The recently introduced
Regularization for Design (RFD) framework poses the controller
architecture/control law co-design problem as one of jointly optimizing the
competing metrics of controller architecture cost and closed loop performance,
and shows that this task can be accomplished by augmenting the variational
solution to an optimal control problem with a suitable atomic norm penalty.
Although explicit constructions for atomic norms useful for the design of
actuation, sensing and joint actuation/sensing architectures are introduced, no
such construction is given for atomic norms used to design communication
architectures. This paper describes an atomic norm that can be used to design
communication architectures for which the resulting distributed optimal
controller is specified by the solution to a convex program. Using this atomic
norm we then show that in the context of $\mathcal{H}_2$ distributed optimal
control, the communication architecture/control law co-design task can be
performed through the use of finite dimensional second order cone programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4923</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4923</id><created>2014-04-19</created><updated>2014-09-22</updated><authors><author><keyname>Shen</keyname><forenames>Jie</forenames></author><author><keyname>Liu</keyname><forenames>Guangcan</forenames></author><author><keyname>Chen</keyname><forenames>Jia</forenames></author><author><keyname>Fang</keyname><forenames>Yuqiang</forenames></author><author><keyname>Xie</keyname><forenames>Jianbin</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Unified Structured Learning for Simultaneous Human Pose Estimation and
  Garment Attribute Classification</title><categories>cs.CV</categories><comments>Accepted to IEEE Trans. on Image Processing</comments><doi>10.1109/TIP.2014.2358082</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we utilize structured learning to simultaneously address two
intertwined problems: human pose estimation (HPE) and garment attribute
classification (GAC), which are valuable for a variety of computer vision and
multimedia applications. Unlike previous works that usually handle the two
problems separately, our approach aims to produce a jointly optimal estimation
for both HPE and GAC via a unified inference procedure. To this end, we adopt a
preprocessing step to detect potential human parts from each image (i.e., a set
of &quot;candidates&quot;) that allows us to have a manageable input space. In this way,
the simultaneous inference of HPE and GAC is converted to a structured learning
problem, where the inputs are the collections of candidate ensembles, the
outputs are the joint labels of human parts and garment attributes, and the
joint feature representation involves various cues such as pose-specific
features, garment-specific features, and cross-task features that encode
correlations between human parts and garment attributes. Furthermore, we
explore the &quot;strong edge&quot; evidence around the potential human parts so as to
derive more powerful representations for oriented human parts. Such evidences
can be seamlessly integrated into our structured learning model as a kind of
energy function, and the learning process could be performed by standard
structured Support Vector Machines (SVM) algorithm. However, the joint
structure of the two problems is a cyclic graph, which hinders efficient
inference. To resolve this issue, we compute instead approximate optima by
using an iterative procedure, where in each iteration the variables of one
problem are fixed. In this way, satisfactory solutions can be efficiently
computed by dynamic programming. Experimental results on two benchmark datasets
show the state-of-the-art performance of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4925</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4925</id><created>2014-04-19</created><authors><author><keyname>Ahmad</keyname><forenames>Iftikhar</forenames></author><author><keyname>Ashraf</keyname><forenames>Uzma</forenames></author><author><keyname>Anum</keyname><forenames>Sadia</forenames></author><author><keyname>Tahir</keyname><forenames>Hira</forenames></author></authors><title>Enhanced aodv route discovery and route establishment for qos provision
  for real time transmission in manet</title><categories>cs.NI</categories><comments>9 pages Published In IJCNC journal 3 figures and two tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.2, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MANET is a temporary connection of mobile nodes via wireless links having no
centralized base station. We developed a protocol with an enhanced route
discovery mechanism that avoids the pre-transmission delay. When a source node
wants to communicate with another node, it broadcast RREQ. EAODV give priority
to the source node of real time transmission. When RREQ packet send to neighbor
node, for real time transmission it accept route request on priority basis and
the drop ratio of packets decreased, then throughput increases by receiving
more packets at destination and delivery ratio also increased through these QOS
improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4927</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4927</id><created>2014-04-19</created><updated>2014-05-17</updated><authors><author><keyname>Satpathi</keyname><forenames>Siddhartha</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mrityunjoy</forenames></author></authors><title>On the number of iterations for convergence of CoSaMP and SP algorithm</title><categories>cs.IT math.IT</categories><comments>7 pages, 1 figure, 2 tables, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sampling Matching Pursuit(CoSaMP) and Subspace Pursuit(SP) are
popular compressive sensing greedy recovery algorithms. In this letter, we
demonstrate that the CoSaMP algorithm can successfully reconstruct a $K$-sparse
signal from a compressed measurement ${\bf y}={\bf A x}$ by a maximum of $5K$
iterations if the sensing matrix ${\bf A}$ satisfies the Restricted Isometry
Constant (RIC) of $\delta_{4K} &lt; \frac {1}{\sqrt{5}}$ and SP algorithm can
reconstruct within $6K$ iteration when RIC of $\delta_{3K} &lt; \frac
{1}{\sqrt{5}}$ is satisfied. The proposed bound in convergence with respect to
number of iterations shows improvement over the existing bounds for Subspace
Pursuit and provides new results for CoSaMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4935</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4935</id><created>2014-04-19</created><authors><author><keyname>Sharma</keyname><forenames>Richa</forenames></author><author><keyname>Nigam</keyname><forenames>Shweta</forenames></author><author><keyname>Jain</keyname><forenames>Rekha</forenames></author></authors><title>Opinion Mining In Hindi Language: A Survey</title><categories>cs.IR cs.CL</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST) International Journal in Foundations of Computer Science
  &amp; Technology (IJFCST), Vol.4, No.2, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinions are very important in the life of human beings. These Opinions
helped the humans to carry out the decisions. As the impact of the Web is
increasing day by day, Web documents can be seen as a new source of opinion for
human beings. Web contains a huge amount of information generated by the users
through blogs, forum entries, and social networking websites and so on To
analyze this large amount of information it is required to develop a method
that automatically classifies the information available on the Web. This domain
is called Sentiment Analysis and Opinion Mining. Opinion Mining or Sentiment
Analysis is a natural language processing task that mine information from
various text forms such as reviews, news, and blogs and classify them on the
basis of their polarity as positive, negative or neutral. But, from the last
few years, enormous increase has been seen in Hindi language on the Web.
Research in opinion mining mostly carried out in English language but it is
very important to perform the opinion mining in Hindi language also as large
amount of information in Hindi is also available on the Web. This paper gives
an overview of the work that has been done Hindi language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4936</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4936</id><created>2014-04-19</created><authors><author><keyname>Liu</keyname><forenames>Jin-Hu</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Li</keyname><forenames>Wei-Min</forenames></author></authors><title>Promoting cold-start items in recommender systems</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>6 pages, 6 figures</comments><doi>10.1371/journal.pone.0113457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of major challenges, cold-start problem plagues nearly all recommender
systems. In particular, new items will be overlooked, impeding the development
of new products online. Given limited resources, how to utilize the knowledge
of recommender systems and design efficient marketing strategy for new items is
extremely important. In this paper, we convert this ticklish issue into a clear
mathematical problem based on a bipartite network representation. Under the
most widely used algorithm in real e-commerce recommender systems, so-called
the item-based collaborative filtering, we show that to simply push new items
to active users is not a good strategy. To our surprise, experiments on real
recommender systems indicate that to connect new items with some less active
users will statistically yield better performance, namely these new items will
have more chance to appear in other users' recommendation lists. Further
analysis suggests that the disassortative nature of recommender systems
contributes to such observation. In a word, getting in-depth understanding on
recommender systems could pave the way for the owners to popularize their
cold-start products with low costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4939</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4939</id><created>2014-04-19</created><authors><author><keyname>Lu</keyname><forenames>Weizhi</forenames></author><author><keyname>Kpalma</keyname><forenames>Kidiyo</forenames></author><author><keyname>Ronsin</keyname><forenames>Joseph</forenames></author></authors><title>Bipartite Graph based Construction of Compressed Sensing Matrices</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes an efficient method to construct the bipartite graph with
as many edges as possible while without introducing the shortest cycles of
length equal to 4. The binary matrix associated with the bipartite graph
described above presents comparable and even better phase transitions than
Gaussian random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4942</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4942</id><created>2014-04-19</created><authors><author><keyname>Holzmann</keyname><forenames>Thomas</forenames></author><author><keyname>Hoppe</keyname><forenames>Christof</forenames></author><author><keyname>Kluckner</keyname><forenames>Stefan</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author></authors><title>Geometric Abstraction from Noisy Image-Based 3D Reconstructions</title><categories>cs.CV</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating geometric abstracted models from image-based scene reconstructions
is difficult due to noise and irregularities in the reconstructed model. In
this paper, we present a geometric modeling method for noisy reconstructions
dominated by planar horizontal and orthogonal vertical structures. We partition
the scene into horizontal slices and create an inside/outside labeling
represented by a floor plan for each slice by solving an energy minimization
problem. Consecutively, we create an irregular discretization of the volume
according to the individual floor plans and again label each cell as
inside/outside by minimizing an energy function. By adjusting the smoothness
parameter, we introduce different levels of detail. In our experiments, we show
results with varying regularization levels using synthetically generated and
real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4944</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4944</id><created>2014-04-19</created><authors><author><keyname>Pedroso</keyname><forenames>Jo&#xe3;o Pedro</forenames></author><author><keyname>Kubo</keyname><forenames>Mikio</forenames></author><author><keyname>Viana</keyname><forenames>Ana</forenames></author></authors><title>Unit commitment with valve-point loading effect</title><categories>math.OC cs.CE</categories><report-no>DCC-2014-05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valve-point loading affects the input-output characteristics of generating
units, bringing the fuel costs nonlinear and nonsmooth. This has been
considered in the solution of load dispatch problems, but not in the planning
phase of unit commitment. This paper presents a mathematical optimization model
for the thermal unit commitment problem considering valve-point loading. The
formulation is based on a careful linearization of the fuel cost function,
which is modeled with great detail on power regions being used in the current
solution, and roughly on other regions. A set of benchmark instances for this
problem is used for analyzing the method, with recourse to a general-purpose
mixed-integer optimization solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4960</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4960</id><created>2014-04-19</created><updated>2014-07-11</updated><authors><author><keyname>Tian</keyname><forenames>Fei</forenames></author><author><keyname>Li</keyname><forenames>Haifang</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Agent Behavior Prediction and Its Generalization Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning algorithms have been applied to predict agent behaviors in
real-world dynamic systems, such as advertiser behaviors in sponsored search
and worker behaviors in crowdsourcing. The behavior data in these systems are
generated by live agents: once the systems change due to the adoption of the
prediction models learnt from the behavior data, agents will observe and
respond to these changes by changing their own behaviors accordingly. As a
result, the behavior data will evolve and will not be identically and
independently distributed, posing great challenges to the theoretical analysis
on the machine learning algorithms for behavior prediction. To tackle this
challenge, in this paper, we propose to use Markov Chain in Random Environments
(MCRE) to describe the behavior data, and perform generalization analysis of
the machine learning algorithms on its basis. Since the one-step transition
probability matrix of MCRE depends on both previous states and the random
environment, conventional techniques for generalization analysis cannot be
directly applied. To address this issue, we propose a novel technique that
transforms the original MCRE into a higher-dimensional time-homogeneous Markov
chain. The new Markov chain involves more variables but is more regular, and
thus easier to deal with. We prove the convergence of the new Markov chain when
time approaches infinity. Then we prove a generalization bound for the machine
learning algorithms on the behavior data generated by the new Markov chain,
which depends on both the Markovian parameters and the covering number of the
function class compounded by the loss function for behavior prediction and the
behavior prediction model. To the best of our knowledge, this is the first work
that performs the generalization analysis on data generated by complex
processes in real-world dynamic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4963</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4963</id><created>2014-04-19</created><updated>2014-05-15</updated><authors><author><keyname>Badia</keyname><forenames>Antonio</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author></authors><title>Functional dependencies with null markers</title><categories>cs.DB</categories><comments>accepted at the Computer Journal (April 2014)</comments><journal-ref>Computer Journal 58 (5), 2015</journal-ref><doi>10.1093/comjnl/bxu039</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Functional dependencies are an integral part of database design. However,
they are only defined when we exclude null markers. Yet we commonly use null
markers in practice. To bridge this gap between theory and practice,
researchers have proposed definitions of functional dependencies over relations
with null markers. Though sound, these definitions lack some qualities that we
find desirable. For example, some fail to satisfy Armstrong's axioms---while
these axioms are part of the foundation of common database methodologies. We
propose a set of properties that any extension of functional dependencies over
relations with null markers should possess. We then propose two new extensions
having these properties. These extensions attempt to allow null markers where
they make sense to practitioners.
  They both support Armstrong's axioms and provide realizable null markers: at
any time, some or all of the null markers can be replaced by actual values
without causing an anomaly. Our proposals may improve database designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4970</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4970</id><created>2014-04-19</created><authors><author><keyname>Rashid</keyname><forenames>Ekbal</forenames></author><author><keyname>Patnaik</keyname><forenames>Srikanta</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Vandana</forenames></author></authors><title>Prediction of rate of improvement of software quality and development
  effort on the basis of Degreeof excellence with respect to number of lines of
  code</title><categories>cs.SE</categories><comments>8 Pages. International Journal of Computer Engineering and
  Applications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this research work is to improve the degree of excellence by
removing the number of exceptions from the software. The modern age is more
concerned with the quality of software. Extensive research is being carried out
in this direction. The rate of improvement of quality of software largely
depends on the development time. This development time is chiefly calculated in
clock hours. However development time does not reflect the effort put in by the
developer. A better parameter can be the rate of improvement of quality level
or the rate of improvement of the degree of excellence with respect to time.
Now this parameter needs the prediction of error level and degree of excellence
at a particular stage of development of the software. This paper explores an
attempt to develop a system to predict rate of improvement of the software
quality at a particular point of time with respect to the number of lines of
code present in the software. Having calculated the error level and degree of
excellence at two points in time, we can move forward towards the estimation of
the rate of improvement of the software quality with respect to time. This
parameter can estimate the effort put in while development of the software and
can add a new dimension to the understanding of software quality in software
engineering domain. In order to obtain the results we have used an indigenous
tool for software quality prediction and for graphical representation of data,
we have used Microsoft office 2007 graphical chart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4972</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4972</id><created>2014-04-19</created><updated>2014-04-27</updated><authors><author><keyname>Takabatake</keyname><forenames>Yoshimasa</forenames></author><author><keyname>Tabei</keyname><forenames>Yasuo</forenames></author><author><keyname>Sakamoto</keyname><forenames>Hiroshi</forenames></author></authors><title>Improved ESP-index: a practical self-index for highly repetitive texts</title><categories>cs.DS</categories><comments>This is the full version of a proceeding accepted to the 11th
  International Symposium on Experimental Algorithms (SEA2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While several self-indexes for highly repetitive texts exist, developing a
practical self-index applicable to real world repetitive texts remains a
challenge. ESP-index is a grammar-based self-index on the notion of
edit-sensitive parsing (ESP), an efficient parsing algorithm that guarantees
upper bounds of parsing discrepancies between different appearances of the same
subtexts in a text. Although ESP-index performs efficient top-down searches of
query texts, it has a serious issue on binary searches for finding appearances
of variables for a query text, which resulted in slowing down the query
searches. We present an improved ESP-index (ESP-index-I) by leveraging the idea
behind succinct data structures for large alphabets. While ESP-index-I keeps
the same types of efficiencies as ESP-index about the top-down searches, it
avoid the binary searches using fast rank/select operations. We experimentally
test ESP-index-I on the ability to search query texts and extract subtexts from
real world repetitive texts on a large-scale, and we show that ESP-index-I
performs better that other possible approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4975</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4975</id><created>2014-04-19</created><updated>2014-08-05</updated><authors><author><keyname>Xiang</keyname><forenames>Yu</forenames></author><author><keyname>Lan</keyname><forenames>Tian</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Chen</keyname><forenames>Yih-Farn R</forenames></author></authors><title>Joint Latency and Cost Optimization for Erasure-coded Data Center
  Storage</title><categories>cs.DC cs.IT math.IT math.OC</categories><comments>14 pages, presented in part at IFIP Performance, Oct 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern distributed storage systems offer large capacity to satisfy the
exponentially increasing need of storage space. They often use erasure codes to
protect against disk and node failures to increase reliability, while trying to
meet the latency requirements of the applications and clients. This paper
provides an insightful upper bound on the average service delay of such
erasure-coded storage with arbitrary service time distribution and consisting
of multiple heterogeneous files. Not only does the result supersede known delay
bounds that only work for a single file or homogeneous files, it also enables a
novel problem of joint latency and storage cost minimization over three
dimensions: selecting the erasure code, placement of encoded chunks, and
optimizing scheduling policy. The problem is efficiently solved via the
computation of a sequence of convex approximations with provable convergence.
We further prototype our solution in an open-source, cloud storage deployment
over three geographically distributed data centers. Experimental results
validate our theoretical delay analysis and show significant latency reduction,
providing valuable insights into the proposed latency-cost tradeoff in
erasure-coded storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4982</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4982</id><created>2014-04-19</created><authors><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>Rotbart</keyname><forenames>Noy</forenames></author></authors><title>Dynamic and Multi-functional Labeling Schemes</title><categories>cs.DS cs.DC</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate labeling schemes supporting adjacency, ancestry, sibling, and
connectivity queries in forests. In the course of more than 20 years, the
existence of $\log n + O(\log \log)$ labeling schemes supporting each of these
functions was proven, with the most recent being ancestry [Fraigniaud and
Korman, STOC '10]. Several multi-functional labeling schemes also enjoy lower
or upper bounds of $\log n + \Omega(\log \log n)$ or $\log n + O(\log \log n)$
respectively. Notably an upper bound of $\log n + 5\log \log n$ for
adjacency+siblings and a lower bound of $\log n + \log \log n$ for each of the
functions siblings, ancestry, and connectivity [Alstrup et al., SODA '03]. We
improve the constants hidden in the $O$-notation. In particular we show a $\log
n + 2\log \log n$ lower bound for connectivity+ancestry and
connectivity+siblings, as well as an upper bound of $\log n + 3\log \log n +
O(\log \log \log n)$ for connectivity+adjacency+siblings by altering existing
methods.
  In the context of dynamic labeling schemes it is known that ancestry requires
$\Omega(n)$ bits [Cohen, et al. PODS '02]. In contrast, we show upper and lower
bounds on the label size for adjacency, siblings, and connectivity of $2\log n$
bits, and $3 \log n$ to support all three functions. There exist efficient
adjacency labeling schemes for planar, bounded treewidth, bounded arboricity
and interval graphs. In a dynamic setting, we show a lower bound of $\Omega(n)$
for each of those families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4983</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4983</id><created>2014-04-19</created><authors><author><keyname>Mathur</keyname><forenames>Iti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Darbari</keyname><forenames>Hemant</forenames></author><author><keyname>Kumar</keyname><forenames>Ajai</forenames></author></authors><title>Shiva++: An Enhanced Graph based Ontology Matcher</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1403.7465</comments><journal-ref>International Journal of Computer Applications 92(16):30-34, April
  2014</journal-ref><doi>10.5120/16095-5393</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the web getting bigger and assimilating knowledge about different
concepts and domains, it is becoming very difficult for simple database driven
applications to capture the data for a domain. Thus developers have come out
with ontology based systems which can store large amount of information and can
apply reasoning and produce timely information. Thus facilitating effective
knowledge management. Though this approach has made our lives easier, but at
the same time has given rise to another problem. Two different ontologies
assimilating same knowledge tend to use different terms for the same concepts.
This creates confusion among knowledge engineers and workers, as they do not
know which is a better term then the other. Thus we need to merge ontologies
working on same domain so that the engineers can develop a better application
over it. This paper shows the development of one such matcher which merges the
concepts available in two ontologies at two levels; 1) at string level and 2)
at semantic level; thus producing better merged ontologies. We have used a
graph matching technique which works at the core of the system. We have also
evaluated the system and have tested its performance with its predecessor which
works only on string matching. Thus current approach produces better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4984</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4984</id><created>2014-04-19</created><authors><author><keyname>Steiner</keyname><forenames>Fabian</forenames></author><author><keyname>Mezghani</keyname><forenames>Amine</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>Information Theoretic Analysis of Concurrent Information Transfer and
  Power Gain</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><journal-ref>Circuits and Systems (ISCAS), 2012 IEEE International Symposium on
  , pp.548,551, 20-23 May 2012</journal-ref><doi>10.1109/ISCAS.2012.6272088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the fundamental trade-off between information
transfer and power gain by means of an information-theoretic framework in
communications circuits. This analysis is of interest as many of today's
applications require that maximum information and maximum signal power are
extracted (or transferred) through the circuit at the same time for further
processing so that a compromise concerning the signal spectral shape as well as
the matching network has to be found. To this end, the optimization framework
is applied to a two-port circuit, which is used as an abstraction for a
broadband amplifier. Thereby, we characterize the involved Pareto bound by
considering different optimization problems. The first one aims at optimizing
the input power spectral density (PSD) as well as the source and load
admittances, whereas the second approach assumes the PSD to be fixed and
uniformly distributed within a fixed bandwidth and optimizes the source and
load admittances only. Moreover, we will show that additional matching networks
may help to improve the trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4985</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4985</id><created>2014-04-19</created><authors><author><keyname>Ishowo-Oloko</keyname><forenames>Fatimah</forenames></author><author><keyname>Crandall</keyname><forenames>Jacob</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Abdallah</keyname><forenames>Sherief</forenames></author><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author></authors><title>Learning in Repeated Games: Human Versus Machine</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Artificial Intelligence has successfully outperformed humans in complex
combinatorial games (such as chess and checkers), humans have retained their
supremacy in social interactions that require intuition and adaptation, such as
cooperation and coordination games. Despite significant advances in learning
algorithms, most algorithms adapt at times scales which are not relevant for
interactions with humans, and therefore the advances in AI on this front have
remained of a more theoretical nature. This has also hindered the experimental
evaluation of how these algorithms perform against humans, as the length of
experiments needed to evaluate them is beyond what humans are reasonably
expected to endure (max 100 repetitions). This scenario is rapidly changing, as
recent algorithms are able to converge to their functional regimes in shorter
time-scales. Additionally, this shift opens up possibilities for experimental
investigation: where do humans stand compared with these new algorithms? We
evaluate humans experimentally against a representative element of these
fast-converging algorithms. Our results indicate that the performance of at
least one of these algorithms is comparable to, and even exceeds, the
performance of people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4995</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4995</id><created>2014-04-19</created><authors><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>A Generalized Cut-Set Bound for Deterministic Multi-Flow Networks and
  its Applications</title><categories>cs.IT math.IT</categories><comments>A shorter version of this paper will appear in the Proceedings of
  ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new outer bound for the sum capacity of general multi-unicast
deterministic networks. Intuitively, this bound can be understood as applying
the cut-set bound to concatenated copies of the original network with a special
restriction on the allowed transmit signal distributions. We first study
applications to finite-field networks, where we obtain a general outer-bound
expression in terms of ranks of the transfer matrices. We then show that, even
though our outer bound is for deterministic networks, a recent result relating
the capacity of AWGN KxKxK networks and the capacity of a deterministic
counterpart allows us to establish an outer bound to the DoF of KxKxK wireless
networks with general connectivity. This bound is tight in the case of the
&quot;adjacent-cell interference&quot; topology, and yields graph-theoretic necessary and
sufficient conditions for K DoF to be achievable in general topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.4997</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.4997</id><created>2014-04-19</created><updated>2015-05-17</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author></authors><title>Tight bounds for learning a mixture of two gaussians</title><categories>cs.LG cs.DS stat.ML</categories><comments>STOC 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider the problem of identifying the parameters of an unknown mixture
of two arbitrary $d$-dimensional gaussians from a sequence of independent
random samples. Our main results are upper and lower bounds giving a
computationally efficient moment-based estimator with an optimal convergence
rate, thus resolving a problem introduced by Pearson (1894). Denoting by
$\sigma^2$ the variance of the unknown mixture, we prove that
$\Theta(\sigma^{12})$ samples are necessary and sufficient to estimate each
parameter up to constant additive error when $d=1.$ Our upper bound extends to
arbitrary dimension $d&gt;1$ up to a (provably necessary) logarithmic loss in $d$
using a novel---yet simple---dimensionality reduction technique. We further
identify several interesting special cases where the sample complexity is
notably smaller than our optimal worst-case bound. For instance, if the means
of the two components are separated by $\Omega(\sigma)$ the sample complexity
reduces to $O(\sigma^2)$ and this is again optimal.
  Our results also apply to learning each component of the mixture up to small
error in total variation distance, where our algorithm gives strong
improvements in sample complexity over previous work. We also extend our lower
bound to mixtures of $k$ Gaussians, showing that $\Omega(\sigma^{6k-2})$
samples are necessary to estimate each parameter up to constant additive error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5000</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5000</id><created>2014-04-19</created><authors><author><keyname>Goel</keyname><forenames>Gagan</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author></authors><title>Clinching Auctions Beyond Hard Budget Constraints</title><categories>cs.GT</categories><comments>Accepted to EC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraints on agent's ability to pay play a major role in auction design for
any setting where the magnitude of financial transactions is sufficiently
large. Those constraints have been traditionally modeled in mechanism design as
\emph{hard budget}, i.e., mechanism is not allowed to charge agents more than a
certain amount. Yet, real auction systems (such as Google AdWords) allow more
sophisticated constraints on agents' ability to pay, such as \emph{average
budgets}. In this work, we investigate the design of Pareto optimal and
incentive compatible auctions for agents with \emph{constrained quasi-linear
utilities}, which captures more realistic models of liquidity constraints that
the agents may have. Our result applies to a very general class of allocation
constraints known as polymatroidal environments, encompassing many settings of
interest such as multi-unit auctions, matching markets, video-on-demand and
advertisement systems.
  Our design is based Ausubel's \emph{clinching framework}. Incentive
compatibility and feasibility with respect to ability-to-pay constraints are
direct consequences of the clinching framework. Pareto-optimality, on the other
hand, is considerably more challenging, since the no-trade condition that
characterizes it depends not only on whether agents have their budgets
exhausted or not, but also on prices {at} which the goods are allocated. In
order to get a handle on those prices, we introduce novel concepts of dropping
prices and saturation. These concepts lead to our main structural result which
is a characterization of the tight sets in the clinching auction outcome and
its relation to dropping prices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5002</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5002</id><created>2014-04-19</created><authors><author><keyname>Ajwani</keyname><forenames>Deepak</forenames></author><author><keyname>Kennedy</keyname><forenames>W. Sean</forenames></author><author><keyname>Sala</keyname><forenames>Alessandra</forenames></author><author><keyname>Saniee</keyname><forenames>Iraj</forenames></author></authors><title>A Geometric Distance Oracle for Large Real-World Graphs</title><categories>cs.SI cs.DS</categories><comments>15 pages, 9 figures, 3 tables</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Many graph processing algorithms require determination of shortest-path
distances between arbitrary numbers of node pairs. Since computation of exact
distances between all node-pairs of a large graph, e.g., 10M nodes and up, is
prohibitively expensive both in computational time and storage space, distance
approximation is often used in place of exact computation. In this paper, we
present a novel and scalable distance oracle that leverages the hyperbolic core
of real-world large graphs for fast and scalable distance approximation. We
show empirically that the proposed oracle significantly outperforms prior
oracles on a random set of test cases drawn from public domain graph libraries.
There are two sets of prior work against which we benchmark our approach. The
first set, which often outperforms other oracles, employs embedding of the
graph into low dimensional Euclidean spaces with carefully constructed
hyperbolic distances, but provides no guarantees on the distance estimation
error. The second set leverages Gromov-type tree contraction of the graph with
the additive error guaranteed not to exceed $2\delta\log{n}$, where $\delta$ is
the hyperbolic constant of the graph. We show that our proposed oracle 1) is
significantly faster than those oracles that use hyperbolic embedding (first
set) with similar approximation error and, perhaps surprisingly, 2) exhibits
substantially lower average estimation error compared to Gromov-like tree
contractions (second set). We substantiate our claims through numerical
computations on a collection of a dozen real world networks and synthetic test
cases from multiple domains, ranging in size from 10s of thousand to 10s of
millions of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5007</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5007</id><created>2014-04-20</created><updated>2014-09-19</updated><authors><author><keyname>Amir</keyname><forenames>Mohamed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Elfouly</keyname><forenames>Tarek</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>On the degrees of freedom of MIMO Multiple access channel with multiple
  eavesdroppers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-transmitter Gaussian multiple access wiretap channel with multiple
antennas at the transmitters, legitimate receiver and eavesdroppers is studied.
The existence of unknown number of eavesdroppers is assumed but with maximum
number of antennas at any eavesdropper limited to a known value $N_E$. The
channel matrices between the transmitters and the receiver is available
everywhere, while the legitimate the legitimate transmitters and the legitimate
receiver have no information about the eavesdroppers channels. A new upperbound
is established and A new achievable DoF bound is provided and meets the
upperbound. It is important to note that the same problem has been studied
recently with arbitrarily varying eavesdropper channels and an upperbound has
been derived. However, our achievable sum secure DoF exceeds their previously
derived upperbound. Consequently, we revisited the uppperbound derivation and
we re-derived a new mathematically robust upperbound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5009</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5009</id><created>2014-04-20</created><updated>2015-09-09</updated><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Torr</keyname><forenames>Philip</forenames></author></authors><title>Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference</title><categories>cs.CV cs.LG cs.NA</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Branch-and-Cut (B&amp;C) method for solving general MAP-MRF
inference problems. The core of our method is a very efficient bounding
procedure, which combines scalable semidefinite programming (SDP) and a
cutting-plane method for seeking violated constraints. In order to further
speed up the computation, several strategies have been exploited, including
model reduction, warm start and removal of inactive constraints.
  We analyze the performance of the proposed method under different settings,
and demonstrate that our method either outperforms or performs on par with
state-of-the-art approaches. Especially when the connectivities are dense or
when the relative magnitudes of the unary costs are low, we achieve the best
reported results. Experiments show that the proposed algorithm achieves better
approximation than the state-of-the-art methods within a variety of time
budgets on challenging non-submodular MAP-MRF inference problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5012</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5012</id><created>2014-04-20</created><updated>2014-05-06</updated><authors><author><keyname>Lai</keyname><forenames>Ching-Yi</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Lu</keyname><forenames>Hsiao-feng</forenames></author></authors><title>On the MacWilliams Identity for Classical and Quantum Convolutional
  Codes</title><categories>cs.IT math.IT quant-ph</categories><comments>Part of this work will appear in Proceedings of 2014 IEEE Intl. Symp.
  Inf. Theory. We included additional results (Subsections III.D and III.E) in
  the second version. 23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usual weight generating functions associating with a convolutional code
(CC) are based on state space realizations or the weight adjacency matrices
(WAMs). The MacWilliams identity for CCs on the WAMs was first established by
Gluesing-Luerssen and Schneider in the case of minimal encoders, and
generalized by Forney using the normal factor graph theorem. We define the dual
of a convolutional code in the viewpoint of constraint codes and obtain a
simple and direct proof of the MacWilliams identity for CCs. By considering the
weight enumeration functions over infinite stages, i.e. all codewords of a CC,
we establish additional relations between a CC and its dual. Relations between
various notions of weight generating function are also clarified, and the
reason that no MacWilliams identity exists for free-distance enumerators is
clear now. Hence the MacWilliams theorem for CCs can be considered complete.
For our purpose, we choose a different representation for the exact weight
generating function (EWGF) of a block code, by defining it as a linear
combination of orthonormal vectors in Dirac bra-ket notation, rather than the
standard polynomial representation. Within this framework, the MacWilliams
identity for the EWGFs can be derived with simply a Fourier transform. This
representation provides great flexibility so that various notions of weight
generating functions and their MacWilliams identities can be easily obtained
from the MacWilliams identity for the EWGFs. As a result, we also obtain the
MacWilliams identity for the input-output weight adjacency matrices (IOWAMs) of
a systematic convolutional code and its dual, which cannot be obtained from
previous approaches. Finally, paralleling the development of the classical
case, we establish the MacWilliams identity for quantum convolutional codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5020</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5020</id><created>2014-04-20</created><updated>2014-08-06</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Son</keyname><forenames>Jae Hyun</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Trayer</keyname><forenames>Mark</forenames></author><author><keyname>Pi</keyname><forenames>Zhouyue</forenames></author><author><keyname>Hwang</keyname><forenames>Dong Yoon</forenames></author><author><keyname>Moon</keyname><forenames>Joong Ki</forenames></author></authors><title>Training-Free Non-Intrusive Load Monitoring of Electric Vehicle Charging
  with Low Sampling Rate</title><categories>cs.OH</categories><comments>Accepted by The 40th Annual Conference of the IEEE Industrial
  Electronics Society (IECON 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-intrusive load monitoring (NILM) is an important topic in smart-grid and
smart-home. Many energy disaggregation algorithms have been proposed to detect
various individual appliances from one aggregated signal observation. However,
few works studied the energy disaggregation of plug-in electric vehicle (EV)
charging in the residential environment since EVs charging at home has emerged
only recently. Recent studies showed that EV charging has a large impact on
smart-grid especially in summer. Therefore, EV charging monitoring has become a
more important and urgent missing piece in energy disaggregation. In this
paper, we present a novel method to disaggregate EV charging signals from
aggregated real power signals. The proposed method can effectively mitigate
interference coming from air-conditioner (AC), enabling accurate EV charging
detection and energy estimation under the presence of AC power signals.
Besides, the proposed algorithm requires no training, demands a light
computational load, delivers high estimation accuracy, and works well for data
recorded at the low sampling rate 1/60 Hz. When the algorithm is tested on
real-world data recorded from 11 houses over about a whole year (total 125
months worth of data), the averaged error in estimating energy consumption of
EV charging is 15.7 kwh/month (while the true averaged energy consumption of EV
charging is 208.5 kwh/month), and the averaged normalized mean square error in
disaggregating EV charging load signals is 0.19.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5021</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5021</id><created>2014-04-20</created><authors><author><keyname>Horovitz</keyname><forenames>Michal</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Local Rank Modulation for Flash Memories II</title><categories>cs.IT math.IT</categories><comments>submitted to ITW2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local rank modulation scheme was suggested recently for representing
information in flash memories in order to overcome drawbacks of rank
modulation. For $0 &lt; s\leq t\leq n$ with $s$ divides $n$, an $(s,t,n)$-LRM
scheme is a local rank modulation scheme where the $n$ cells are locally viewed
cyclically through a sliding window of size $t$ resulting in a sequence of
small permutations which requires less comparisons and less distinct values.
The gap between two such windows equals to $s$. In this work, encoding,
decoding, and asymptotic enumeration of the $(1,3,n)$-LRM scheme is studied.
The techniques which are suggested have some generalizations for $(1,t,n)$-LRM,
$t &gt; 3$, but the proofs will become more complicated. The enumeration problem
is presented also as a purely combinatorial problem. Finally, we prove the
conjecture that the size of a constant weight $(1,2,n)$-LRM Gray code with
weight two is at most $2n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5029</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5029</id><created>2014-04-20</created><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames><affiliation>Angela</affiliation></author><author><keyname>Jun</keyname><forenames>Ying</forenames><affiliation>Angela</affiliation></author><author><keyname>Zhang</keyname></author></authors><title>Using Covert Topological Information for Defense Against Malicious
  Attacks on DC State Estimation</title><categories>cs.CR</categories><comments>Accepted for publication by Journal of Selected Areas in
  Communications (JSAC). arXiv admin note: substantial text overlap with
  arXiv:1304.4151</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Accurate state estimation is of paramount importance to maintain the power
system operating in a secure and efficient state. The recently identified
coordinated data injection attacks to meter measurements can bypass the current
security system and introduce errors to the state estimates. The conventional
wisdom to mitigate such attacks is by securing meter measurements to evade
malicious injections. In this paper, we provide a novel alternative to defend
against false-data injection attacks using covert power network topological
information. By keeping the exact reactance of a set of transmission lines from
attackers, no false data injection attack can be launched to compromise any set
of state variables. We first investigate from the attackers' perspective the
necessary condition to perform injection attack. Based on the arguments, we
characterize the optimal protection problem, which protects the state variables
with minimum cost, as a well-studied Steiner tree problem in a graph. Besides,
we also propose a mixed defending strategy that jointly considers the use of
covert topological information and secure meter measurements when either method
alone is costly or unable to achieve the protection objective. A mixed integer
linear programming (MILP) formulation is introduced to obtain the optimal mixed
defending strategy. To tackle the NP-hardness of the problem, a tree
pruning-based heuristic is further presented to produce an approximate solution
in polynomial time. The advantageous performance of the proposed defending
mechanisms is verified in IEEE standard power system testcases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5034</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5034</id><created>2014-04-20</created><updated>2014-06-10</updated><authors><author><keyname>Khan</keyname><forenames>Parvez Mahmood</forenames></author><author><keyname>Beg</keyname><forenames>M M Sufyan</forenames></author><author><keyname>Ahmad</keyname><forenames>Musheer</forenames></author></authors><title>Sustaining IT PMOs during Cycles of Global Recession</title><categories>cs.SE</categories><journal-ref>European Journal of Scientific Research, vol. 114, no. 3, pp.
  376-385, November 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Growth in the number of PMOs established by the industry over last decade and
ever growing body of literature on PMO related research in academia is a clear
indication that there is very clear interest of researchers, practitioners and
industries across the globe to understand and explore value propositions of
PMO. However, there is still a lack of consensus on many critical aspects of
PMOs. While there are many PMOs being established, but there are also many
being closed and disbanded, which is definitely a matter of concern. In
industry environment, a narrow majority of PMOs are well-regarded by their
organizations and are seen as contributing business value, many of the others
are still struggling to show value for money and some are failing, causing a
high mortality rate among PMOs. This paper is the result of a study undertaken
to get a deeper understanding of factors that may be causing mortality and
failure of PMOs. Post Implementation Reviews of 4-failed &amp; 3-challenged PMOs in
IT-Industry were carried out with concerned Project Managers &amp; PMO-staff, using
grounded theory research method, with support from the concerned enterprise
from IT-Industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5037</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5037</id><created>2014-04-20</created><authors><author><keyname>Pesenson</keyname><forenames>Isaac Z.</forenames></author></authors><title>Multiresolution analysis on compact Riemannian manifolds</title><categories>cs.IT math.IT</categories><comments>published in &quot;Multiscale Analysis and Nonlinear Dynamics: From Genes
  to the Brain&quot;, First Edition. Edited by Misha Meyer Pesenson. 2013 Wiley-VCH
  Verlag GmbH &amp; Co. KGaA. Published 2013 by Wiley-VCH Verlag GmbH &amp; Co. KGaA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the chapter &quot;Multiresolution Analysis on Compact Riemannian Manifolds&quot;
Isaac Pesenson describes multiscale analysis, sampling, interpolation and
approximation of functions defined on manifolds. His main achievements are:
construction on manifolds of bandlimited and space-localized frames which have
Parseval property and construction of variational splines on manifolds. Such
frames and splines enable multiscale analysis on arbitrary compact manifolds,
and they already found a number of important applications (statistics, CMB,
crystallography) related to such manifolds as two-dimensional sphere and group
of its rotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5043</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5043</id><created>2014-04-20</created><authors><author><keyname>Lomadze</keyname><forenames>Vakhtang</forenames></author></authors><title>The predictable degree property, column reducedness, and minimality in
  multidimensional convolutional coding</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-dimensional analogs of the predictable degree property and column
reducedness are defined, and it is proved that the two properties are
equivalent. It is shown that every multidimensional convolutional code has,
what is called, a minimal reduced polynomial resolution. It is uniquely
determined (up to isomorphism) and leads to a number of important integer
invariants of the code generalizing classical Forney's indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5055</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5055</id><created>2014-04-20</created><authors><author><keyname>Budkuley</keyname><forenames>Amitalok J.</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Correlated Jamming in a Joint Source Channel Communication System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study correlated jamming in joint source-channel communication systems. An
i.i.d. source is to be communicated over a memoryless channel in the presence
of a correlated jammer with non-causal knowledge of user transmission. This
user-jammer interaction is modeled as a zero sum game. A set of conditions on
the source and the channel is provided for the existence of a Nash equilibrium
for this game, where the user strategy is uncoded transmission and the jammer
strategy is i.i.d jamming. This generalizes a well-known example of uncoded
communication of a Gaussian sources over Gaussian channels with additive
jamming. Another example, of a Binary Symmetric source over a Binary Symmetric
channel with jamming, is provided as a validation of this result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5060</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5060</id><created>2014-04-20</created><authors><author><keyname>Budkuley</keyname><forenames>Amitalok J.</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Writing on a Dirty Paper in the presence of Jamming</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of writing on a dirty paper in the presence of
jamming is examined. We consider an AWGN channel with an additive white
Gaussian state and an additive adversarial jammer. The state is assumed to be
known non-causally to the encoder and the jammer but not to the decoder. The
capacity of the channel in the presence of a jammer is determined. A surprising
result that this capacity is equal to the capacity of a relaxed version of the
problem, where the state is also known non-causally to the decoder, is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5062</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5062</id><created>2014-04-20</created><authors><author><keyname>Zaimovic-Uzunovic</keyname><forenames>Nermina</forenames></author><author><keyname>Lemes</keyname><forenames>Samir</forenames></author><author><keyname>Curic</keyname><forenames>Damir</forenames></author><author><keyname>Topcic</keyname><forenames>Alan</forenames></author></authors><title>Rapid prototyping for sling design optimization</title><categories>cs.CE</categories><comments>ISSN 1854-6250</comments><journal-ref>Advances in Production, Engineering &amp; Management 6(2011)4, 271-280</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with combination of two modern engineering methods in order
to optimise the shape of a representative casting product. The product being
analysed is a sling, which is used to attach pulling rope in timber
transportation. The first step was 3D modelling and static stress/strain
analysis using CAD/CAE software NX4. The slinger shape optimization was
performed using Traction method, by means of software Optishape-TS. To define
constraints for shape optimization, FEA software FEMAP was used. The mould
pattern with optimized 3D shape was then prepared using Fused Deposition
Modelling (FDM) Rapid prototyping method. The sling mass decreased by 20%,
while signifficantly better stress distribution was achieved, with maximum
stress 3.5 times less than initial value. The future researches should use 3D
scanning technology in order to provide more accurate 3D model of initial part.
Results of this research can be used by toolmakers in order to engage FEA/RP
technology to design and manufacture lighter products with acceptable stress
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5065</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5065</id><created>2014-04-20</created><authors><author><keyname>Tsoumakas</keyname><forenames>Grigorios</forenames></author><author><keyname>Spyromitros-Xioufis</keyname><forenames>Eleftherios</forenames></author><author><keyname>Vrekou</keyname><forenames>Aikaterini</forenames></author><author><keyname>Vlahavas</keyname><forenames>Ioannis</forenames></author></authors><title>Multi-Target Regression via Random Linear Target Combinations</title><categories>cs.LG</categories><journal-ref>ECML PKDD Proceedings, Part III (2014) 225-240</journal-ref><doi>10.1007/978-3-662-44845-8_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-target regression is concerned with the simultaneous prediction of
multiple continuous target variables based on the same set of input variables.
It arises in several interesting industrial and environmental application
domains, such as ecological modelling and energy forecasting. This paper
presents an ensemble method for multi-target regression that constructs new
target variables via random linear combinations of existing targets. We discuss
the connection of our approach with multi-label classification algorithms, in
particular RA$k$EL, which originally inspired this work, and a family of recent
multi-label classification algorithms that involve output coding. Experimental
results on 12 multi-target datasets show that it performs significantly better
than a strong baseline that learns a single model for each target using
gradient boosting and compares favourably to multi-objective random forest
approach, which is a state-of-the-art approach. The experiments further show
that our approach improves more when stronger unconditional dependencies exist
among the targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5068</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5068</id><created>2014-04-20</created><updated>2015-04-21</updated><authors><author><keyname>Barati</keyname><forenames>C. Nicolas</forenames></author><author><keyname>Hosseini</keyname><forenames>S. Amir</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Liu</keyname><forenames>Pei</forenames></author><author><keyname>Korakis</keyname><forenames>Thanasis</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra S.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>Directional Cell Discovery in Millimeter Wave Cellular Networks</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The acute disparity between increasing bandwidth demand and available
spectrum, has brought millimeter wave (mmW) bands to the forefront of candidate
solutions for the next-generation cellular networks. Highly directional
transmissions are essential for cellular communication in these frequencies to
compensate for high isotropic path loss. This reliance on directional
beamforming, however, complicates initial cell search since the mobile and base
station must jointly search over a potentially large angular directional space
to locate a suitable path to initiate communication. To address this problem,
this paper proposes a directional cell discovery procedure where base stations
periodically transmit synchronization signals, potentially in time-varying
random directions, to scan the angular space. Detectors for these signals are
derived based on a Generalized Likelihood Ratio Test (GLRT) under various
signal and receiver assumptions. The detectors are then simulated under
realistic design parameters and channels based on actual experimental
measurements at 28~GHz in New York City. The study reveals two key findings:
(i) digital beamforming can significantly outperform analog beamforming even
when the digital beamforming uses very low quantization to compensate for the
additional power requirements; and (ii) omni-directional transmissions of the
synchronization signals from the base station generally outperforms random
directional scanning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5069</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5069</id><created>2014-04-20</created><updated>2015-08-31</updated><authors><author><keyname>Lairez</keyname><forenames>Pierre</forenames></author></authors><title>Computing periods of rational integrals</title><categories>cs.SC math.AG</categories><comments>To appear in Math. comp. Supplementary material at
  http://pierre.lairez.fr/supp/periods/</comments><msc-class>Primary 68W30, secondary 14K20, 14F40, 33F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A period of a rational integral is the result of integrating, with respect to
one or several variables, a rational function over a closed path. This work
focuses particularly on periods depending on a parameter: in this case the
period under consideration satisfies a linear differential equation, the
Picard-Fuchs equation. I give a reduction algorithm that extends the
Griffiths-Dwork reduction and apply it to the computation of Picard-Fuchs
equations. The resulting algorithm is elementary and has been successfully
applied to problems that were previously out of reach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5078</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5078</id><created>2014-04-20</created><authors><author><keyname>Petuchowski</keyname><forenames>Ethan</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author></authors><title>TurKPF: TurKontrol as a Particle Filter</title><categories>cs.AI</categories><comments>8 pages, 6 figures, formula appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TurKontrol, and algorithm presented in (Dai et al. 2010), uses a POMDP to
model and control an iterative workflow for crowdsourced work. Here, TurKontrol
is re-implemented as &quot;TurKPF,&quot; which uses a Particle Filter to reduce
computation time &amp; memory usage. Most importantly, in our experimental
environment with default parameter settings, the action is chosen nearly
instantaneously. Through a series of experiments we see that TurKPF and
TurKontrol perform similarly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5083</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5083</id><created>2014-04-20</created><updated>2014-07-30</updated><authors><author><keyname>Hanif</keyname><forenames>Muhammad</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Transmit Antenna Selection in Underlay Cognitive Radio Environment</title><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio (CR) technology addresses the problem of spectrum
under-utilization. In underlay CR mode, the secondary users are allowed to
communicate provided that their transmission is not detrimental to primary user
communication. Transmit antenna selection is one of the low-complexity methods
to increase the capacity of wireless communication systems. In this article, we
propose and analyze the performance benefit of a transmit antenna selection
scheme for underlay secondary system that ensures the instantaneous
interference caused by the secondary transmitter to the primary receiver is
below a predetermined level. Closed-form expressions of the outage probability,
amount of fading, and ergodic capacity for the secondary network are derived.
Monte-carlo simulations are also carried out to confirm various mathematical
results presented in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5084</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5084</id><created>2014-04-20</created><updated>2014-05-09</updated><authors><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Probabilistic Bisimulation: Naturally on Distributions</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to the usual understanding of probabilistic systems as stochastic
processes, recently these systems have also been regarded as transformers of
probabilities. In this paper, we give a natural definition of strong
bisimulation for probabilistic systems corresponding to this view that treats
probability distributions as first-class citizens. Our definition applies in
the same way to discrete systems as well as to systems with uncountable state
and action spaces. Several examples demonstrate that our definition refines the
understanding of behavioural equivalences of probabilistic systems. In
particular, it solves a long-standing open problem concerning the
representation of memoryless continuous time by memory-full continuous time.
Finally, we give algorithms for computing this bisimulation not only for finite
but also for classes of uncountably infinite systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5121</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5121</id><created>2014-04-21</created><authors><author><keyname>Liu</keyname><forenames>Yanpei</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Kim</keyname><forenames>Nam Sung</forenames></author></authors><title>SleepScale: Runtime Joint Speed Scaling and Sleep States Management for
  Power Efficient Data Centers</title><categories>cs.PF cs.SY</categories><comments>Accepted by ACM/IEEE International Symposium on Computer Architecture
  (ISCA) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power consumption in data centers has been growing significantly in recent
years. To reduce power, servers are being equipped with increasingly
sophisticated power management mechanisms. Different mechanisms offer
dramatically different trade-offs between power savings and performance
penalties. Considering the complexity, variety, and temporally varying nature
of the applications hosted in a typical data center, intelligently determining
which power management policy to use and when is a complicated task.
  In this paper we analyze a system model featuring both performance scaling
and low-power states. We reveal the interplay between performance scaling and
low-power states via intensive simulation and analytic verification. Based on
the observations, we present SleepScale, a runtime power management tool
designed to efficiently exploit existing power control mechanisms. At run time,
SleepScale characterizes power consumption and quality-of-service (QoS) for
each low-power state and frequency setting, and selects the best policy for a
given QoS constraint. We evaluate SleepScale using workload traces from data
centers and achieve significant power savings relative to conventional power
management strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5122</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5122</id><created>2014-04-21</created><updated>2014-11-14</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Jung</keyname><forenames>Tzyy-Ping</forenames></author><author><keyname>Makeig</keyname><forenames>Scott</forenames></author><author><keyname>Pi</keyname><forenames>Zhouyue</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Spatiotemporal Sparse Bayesian Learning with Applications to Compressed
  Sensing of Multichannel Physiological Signals</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Codes are available at:
  https://sites.google.com/site/researchbyzhang/stsbl</comments><journal-ref>IEEE Transactions On Neural Systems And Rehabilitation
  Engineering, Vol. 22, No. 6, pp. 1186-1197, November 2014</journal-ref><doi>10.1109/TNSRE.2014.2319334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy consumption is an important issue in continuous wireless
telemonitoring of physiological signals. Compressed sensing (CS) is a promising
framework to address it, due to its energy-efficient data compression
procedure. However, most CS algorithms have difficulty in data recovery due to
non-sparsity characteristic of many physiological signals. Block sparse
Bayesian learning (BSBL) is an effective approach to recover such signals with
satisfactory recovery quality. However, it is time-consuming in recovering
multichannel signals, since its computational load almost linearly increases
with the number of channels.
  This work proposes a spatiotemporal sparse Bayesian learning algorithm to
recover multichannel signals simultaneously. It not only exploits temporal
correlation within each channel signal, but also exploits inter-channel
correlation among different channel signals. Furthermore, its computational
load is not significantly affected by the number of channels. The proposed
algorithm was applied to brain computer interface (BCI) and EEG-based driver's
drowsiness estimation. Results showed that the algorithm had both better
recovery performance and much higher speed than BSBL. Particularly, the
proposed algorithm ensured that the BCI classification and the drowsiness
estimation had little degradation even when data were compressed by 80%, making
it very suitable for continuous wireless telemonitoring of multichannel
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5123</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5123</id><created>2014-04-21</created><authors><author><keyname>Ebrahim</keyname><forenames>Mansoor</forenames></author><author><keyname>Khan</keyname><forenames>Shujaat</forenames></author><author><keyname>Khalid</keyname><forenames>UmerBin</forenames></author></authors><title>Security Risk Analysis in Peer 2 Peer System; An Approach towards
  Surmounting Security Challenges</title><categories>cs.CR</categories><journal-ref>Asian Journal of Engineering Science and Technology AJEST 2 (2)
  2.2 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P2P networking has become a promising technology and has achieved popularity
as a mechanism for users to share files without the need for centralized
servers. The rapid growth of P2P networks beginning with Kaza, Lime wire,
Napsters, E-donkey, Gnutella etc makes them an attractive target to the
creators of viruses and other security threats. This paper describes the major
security issues on P2P networks (Viruses and worms) and presents the study of
propagation mechanisms. In particular, the paper explores different P2P viruses
and worms, their propagation methodology, outlines the challenges, and
evaluates how P2P worms affect the network. The experimental results obtained
will provide new direction in surmounting the security concerns in P2P Networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5127</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5127</id><created>2014-04-21</created><authors><author><keyname>Bachrach</keyname><forenames>Yoram</forenames></author><author><keyname>Ceppi</keyname><forenames>Sofia</forenames></author><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author><author><keyname>Key</keyname><forenames>Peter</forenames></author><author><keyname>Kurokawa</keyname><forenames>David</forenames></author></authors><title>Optimising Trade-offs Among Stakeholders in Ad Auctions</title><categories>cs.GT</categories><comments>18 pages, 10 figures, ACM Conference on Economics and Computation
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine trade-offs among stakeholders in ad auctions. Our metrics are the
revenue for the utility of the auctioneer, the number of clicks for the utility
of the users and the welfare for the utility of the advertisers. We show how to
optimize linear combinations of the stakeholder utilities, showing that these
can be tackled through a GSP auction with a per-click reserve price. We then
examine constrained optimization of stakeholder utilities.
  We use simulations and analysis of real-world sponsored search auction data
to demonstrate the feasible trade-offs, examining the effect of changing the
allowed number of ads on the utilities of the stakeholders. We investigate both
short term effects, when the players do not have the time to modify their
behavior, and long term equilibrium conditions.
  Finally, we examine a combinatorially richer constrained optimization
problem, where there are several possible allowed configurations (templates) of
ad formats. This model captures richer ad formats, which allow using the
available screen real estate in various ways. We show that two natural
generalizations of the GSP auction rules to this domain are poorly behaved,
resulting in not having a symmetric Nash equilibrium or having one with poor
welfare. We also provide positive results for restricted cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5144</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5144</id><created>2014-04-21</created><authors><author><keyname>Konomi</keyname><forenames>M.</forenames></author><author><keyname>Sacha</keyname><forenames>G. M.</forenames></author></authors><title>Influence of the learning method in the performance of feedforward
  neural networks when the activity of neurons is modified</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method that allows us to give a different treatment to any neuron inside
feedforward neural networks is presented. The algorithm has been implemented
with two very different learning methods: a standard Back-propagation (BP)
procedure and an evolutionary algorithm. First, we have demonstrated that the
EA training method converges faster and gives more accurate results than BP.
Then we have made a full analysis of the effects of turning off different
combinations of neurons after the training phase. We demonstrate that EA is
much more robust than BP for all the cases under study. Even in the case when
two hidden neurons are lost, EA training is still able to give good average
results. This difference implies that we must be very careful when pruning or
redundancy effects are being studied since the network performance when losing
neurons strongly depends on the training method. Moreover, the influence of the
individual inputs will also depend on the training algorithm. Since EA keeps a
good classification performance when units are lost, this method could be a
good way to simulate biological learning systems since they must be robust
against deficient neuron performance. Although biological systems are much more
complex than the simulations shown in this article, we propose that a smart
training strategy such as the one shown here could be considered as a first
protection against the losing of a certain number of neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5155</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5155</id><created>2014-04-21</created><authors><author><keyname>Xia</keyname><forenames>Yingce</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Yu</keyname><forenames>Nenghai</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Incentivizing High-quality Content from Heterogeneous Users: On the
  Existence of Nash Equilibrium</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence of pure Nash equilibrium (PNE) for the mechanisms used
in Internet services (e.g., online reviews and question-answer websites) to
incentivize users to generate high-quality content. Most existing work assumes
that users are homogeneous and have the same ability. However, real-world users
are heterogeneous and their abilities can be very different from each other due
to their diverse background, culture, and profession. In this work, we consider
heterogeneous users with the following framework: (1) the users are
heterogeneous and each of them has a private type indicating the best quality
of the content she can generate; (2) there is a fixed amount of reward to
allocate to the participated users. Under this framework, we study the
existence of pure Nash equilibrium of several mechanisms composed by different
allocation rules, action spaces, and information settings. We prove the
existence of PNE for some mechanisms and the non-existence of PNE for some
mechanisms. We also discuss how to find a PNE for those mechanisms with PNE
either through a constructive way or a search algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5157</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5157</id><created>2014-04-21</created><updated>2015-04-19</updated><authors><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Trace Inclusion for One-Counter Nets Revisited</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-Counter nets (OCN) consist of a nondeterministic finite control and a
single integer counter that cannot be fully tested for zero. They form a
natural subclass of both One-Counter Automata, which allow zero-tests and Petri
Nets/VASS, which allow multiple such weak counters.
  The trace inclusion problem has recently been shown to be undecidable for
OCN. In this paper, we contrast the complexity of two natural restrictions
which imply decidability.
  First, we show that trace inclusion between an OCN and a deterministic OCN is
NL-complete, even with arbitrary binary-encoded initial counter-values as part
of the input. Secondly, we show Ackermannian completeness of for the trace
universality problem of nondeterministic OCN. This problem is equivalent to
checking trace inclusion between a finite and a OCN-process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5165</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5165</id><created>2014-04-21</created><updated>2014-04-22</updated><authors><author><keyname>Xu</keyname><forenames>Nuo</forenames></author><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Lim</keyname><forenames>Keng Kiat</forenames></author><author><keyname>Ozgul</keyname><forenames>Etkin Baris</forenames></author></authors><title>GP-Localize: Persistent Mobile Robot Localization using Online Sparse
  Gaussian Process Observation Model</title><categories>cs.RO cs.LG stat.ML</categories><comments>28th AAAI Conference on Artificial Intelligence (AAAI 2014), Extended
  version with proofs, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Central to robot exploration and mapping is the task of persistent
localization in environmental fields characterized by spatially correlated
measurements. This paper presents a Gaussian process localization (GP-Localize)
algorithm that, in contrast to existing works, can exploit the spatially
correlated field measurements taken during a robot's exploration (instead of
relying on prior training data) for efficiently and scalably learning the GP
observation model online through our proposed novel online sparse GP. As a
result, GP-Localize is capable of achieving constant time and memory (i.e.,
independent of the size of the data) per filtering step, which demonstrates the
practical feasibility of using GPs for persistent robot localization and
autonomy. Empirical evaluation via simulated experiments with real-world
datasets and a real robot experiment shows that GP-Localize outperforms
existing GP localization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5169</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5169</id><created>2014-04-21</created><authors><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author></authors><title>On Uniform Reductions between Direct Product and XOR Lemmas</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a close connection between Direct Product and XOR lemmas in the
sense that in many settings, we can prove one given the other. The known
reductions that are used for the above purpose are either in the non-uniform
setting or give non-matching parameters. By non-matching parameter we mean that
$k$-wise Direct Product lemma implies $k'$-wise XOR lemma (and vice versa) for
$k \neq k'$. In this work, we discuss reductions between $k$-wise Direct
Product and $k$-wise XOR lemmas. That is, we show that if the $k$-wise direct
product lemma holds, then so does the $k$-wise XOR lemma and vice versa. We
show that even though there is a perfectly uniform reduction in one direction,
the reduction in the other direction requires some amount of non-uniformity. We
give reductions in both directions matching information-theoretic bounds up to
polynomial factors. Our techniques also give a small quantitative improvement
over the known results about proving $k$-wise XOR lemma using $2k$-wise Direct
Product lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5173</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5173</id><created>2014-04-21</created><updated>2014-05-10</updated><authors><author><keyname>Steiner</keyname><forenames>Fabian</forenames></author><author><keyname>Dempfle</keyname><forenames>Steffen</forenames></author><author><keyname>Ingber</keyname><forenames>Amir</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Compression for Quadratic Similarity Queries: Finite Blocklength and
  Practical Schemes</title><categories>cs.IT math.IT</categories><comments>minor clarifications and wording updates compared to v1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of compression for the purpose of similarity
identification, where similarity is measured by the mean square Euclidean
distance between vectors. While the asymptotical fundamental limits of the
problem - the minimal compression rate and the error exponent - were found in a
previous work, in this paper we focus on the nonasymptotic domain and on
practical, implementable schemes. We first present a finite blocklength
achievability bound based on shape-gain quantization: The gain (amplitude) of
the vector is compressed via scalar quantization and the shape (the projection
on the unit sphere) is quantized using a spherical code. The results are
numerically evaluated and they converge to the asymptotic values as predicted
by the error exponent. We then give a nonasymptotic lower bound on the
performance of any compression scheme, and compare to the upper (achievability)
bound. For a practical implementation of such a scheme, we use wrapped
spherical codes, studied by Hamkins and Zeger, and use the Leech lattice as an
example for an underlying lattice. As a side result, we obtain a bound on the
covering angle of any wrapped spherical code, as a function of the covering
radius of the underlying lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5184</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5184</id><created>2014-04-21</created><updated>2015-02-01</updated><authors><author><keyname>J&#xe4;rvinen</keyname><forenames>Jouni</forenames></author><author><keyname>Radeleczki</keyname><forenames>S&#xe1;ndor</forenames></author></authors><title>Tolerances induced by irredundant coverings</title><categories>math.RA cs.DM</categories><comments>12 pages, 2 figures</comments><msc-class>03E20, 05C69 (Primary), 68T37, 06B23 (Secondary)</msc-class><journal-ref>Fundamenta Informaticae 137 (2015) 341-353</journal-ref><doi>10.3233/FI-2015-1183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider tolerances induced by irredundant coverings. Each
tolerance $R$ on $U$ determines a quasiorder $\lesssim_R$ by setting $x
\lesssim_R y$ if and only if $R(x) \subseteq R(y)$. We prove that for a
tolerance $R$ induced by a covering $\mathcal{H}$ of $U$, the covering
$\mathcal{H}$ is irredundant if and only if the quasiordered set $(U,
\lesssim_R)$ is bounded by minimal elements and the tolerance $R$ coincides
with the product ${\gtrsim_R} \circ {\lesssim_R}$. We also show that in such a
case $\mathcal{H} = \{ {\uparrow}m \mid \text{$m$ is minimal in
$(U,\lesssim_R)$} \}$, and for each minimal $m$, we have $R(m) = {\uparrow} m$.
Additionally, this irredundant covering $\mathcal{H}$ inducing $R$ consists of
some blocks of the tolerance $R$. We give necessary and sufficient conditions
under which $\mathcal{H}$ and the set of $R$-blocks coincide. These results are
established by applying the notion of Helly numbers of quasiordered sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5187</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5187</id><created>2014-04-21</created><updated>2014-12-10</updated><authors><author><keyname>Nokleby</keyname><forenames>Matthew</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Discrimination on the Grassmann Manifold: Fundamental Limits of Subspace
  Classifiers</title><categories>cs.IT math.IT</categories><comments>19 pages, 4 figures. Revised submission to IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present fundamental limits on the reliable classification of linear and
affine subspaces from noisy, linear features. Drawing an analogy between
discrimination among subspaces and communication over vector wireless channels,
we propose two Shannon-inspired measures to characterize asymptotic classifier
performance. First, we define the classification capacity, which characterizes
necessary and sufficient conditions for the misclassification probability to
vanish as the signal dimension, the number of features, and the number of
subspaces to be discerned all approach infinity. Second, we define the
diversity-discrimination tradeoff which, by analogy with the
diversity-multiplexing tradeoff of fading vector channels, characterizes
relationships between the number of discernible subspaces and the
misclassification probability as the noise power approaches zero. We derive
upper and lower bounds on these measures which are tight in many regimes.
Numerical results, including a face recognition application, validate the
results in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5190</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5190</id><created>2014-04-18</created><updated>2014-08-08</updated><authors><author><keyname>Khamis</keyname><forenames>Mahmoud Abo</forenames></author><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>Sparse Approximation, List Decoding, and Uncertainty Principles</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider list versions of sparse approximation problems, where unlike the
existing results in sparse approximation that consider situations with unique
solutions, we are interested in multiple solutions. We introduce these problems
and present the first combinatorial results on the output list size. These
generalize and enhance some of the existing results on threshold phenomenon and
uncertainty principles in sparse approximations. Our definitions and results
are inspired by similar results in list decoding. We also present lower bound
examples that bolster our results and show they are of the appropriate size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5199</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5199</id><created>2014-04-21</created><authors><author><keyname>Wang</keyname><forenames>Zhijian</forenames></author><author><keyname>Xu</keyname><forenames>Bin</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Social cycling and conditional responses in the Rock-Paper-Scissors game</title><categories>physics.soc-ph cs.GT</categories><comments>7 pages + 14 pages supplementary information</comments><journal-ref>Scientific Reports 4, 5830 (2014)</journal-ref><doi>10.1038/srep05830</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How humans make decisions in non-cooperative strategic interactions is a
challenging question. For the fundamental model system of Rock-Paper-Scissors
(RPS) game, classic game theory of infinite rationality predicts the Nash
equilibrium (NE) state with every player randomizing her choices to avoid being
exploited, while evolutionary game theory of bounded rationality in general
predicts persistent cyclic motions, especially for finite populations. However,
as empirical studies on human subjects have been relatively sparse, it is still
a controversial issue as to which theoretical framework is more appropriate to
describe decision making of human subjects. Here we observe population-level
cyclic motions in a laboratory experiment of the discrete-time iterated RPS
game under the traditional random pairwise-matching protocol. The cycling
direction and frequency are not sensitive to the payoff parameter a. This
collective behavior contradicts with the NE theory but it is quantitatively
explained by a microscopic model of win-lose-tie conditional response without
any adjustable parameter. Our theoretical calculations reveal that this new
strategy may offer higher payoffs to individual players in comparison with the
NE mixed strategy, suggesting that high social efficiency is achievable through
optimized conditional response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5214</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5214</id><created>2014-04-21</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Graph Kernels via Functional Embedding</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a representation of graph as a functional object derived from the
power iteration of the underlying adjacency matrix. The proposed functional
representation is a graph invariant, i.e., the functional remains unchanged
under any reordering of the vertices. This property eliminates the difficulty
of handling exponentially many isomorphic forms. Bhattacharyya kernel
constructed between these functionals significantly outperforms the
state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph
classification datasets, demonstrating the superiority of our approach. The
proposed methodology is simple and runs in time linear in the number of edges,
which makes our kernel more efficient and scalable compared to many widely
adopted graph kernels with running time cubic in the number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5236</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5236</id><created>2014-04-21</created><updated>2014-05-27</updated><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Sum-of-squares proofs and the quest toward optimal algorithms</title><categories>cs.DS cs.CC cs.LG math.OC</categories><comments>Survey. To appear in proceedings of ICM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to obtain the best-known guarantees, algorithms are traditionally
tailored to the particular problem we want to solve. Two recent developments,
the Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,
surprisingly suggest that this tailoring is not necessary and that a single
efficient algorithm could achieve best possible guarantees for a wide range of
different problems.
  The Unique Games Conjecture (UGC) is a tantalizing conjecture in
computational complexity, which, if true, will shed light on the complexity of
a great many problems. In particular this conjecture predicts that a single
concrete algorithm provides optimal guarantees among all efficient algorithms
for a large class of computational problems.
  The Sum-of-Squares (SOS) method is a general approach for solving systems of
polynomial constraints. This approach is studied in several scientific
disciplines, including real algebraic geometry, proof complexity, control
theory, and mathematical programming, and has found applications in fields as
diverse as quantum information theory, formal verification, game theory and
many others.
  We survey some connections that were recently uncovered between the Unique
Games Conjecture and the Sum-of-Squares method. In particular, we discuss new
tools to rigorously bound the running time of the SOS method for obtaining
approximate solutions to hard optimization problems, and how these tools give
the potential for the sum-of-squares method to provide new guarantees for many
problems of interest, and possibly to even refute the UGC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5239</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5239</id><created>2014-04-21</created><authors><author><keyname>Razis</keyname><forenames>Gerasimos</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>InfluenceTracker: Rating the impact of a Twitter account</title><categories>cs.SI physics.soc-ph</categories><comments>Draft version of paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a methodology of rating the influence of a Twitter ac-count in
this famous microblogging service. We then evaluate it over real ac-counts,
under the belief that influence is not only a matter of quantity (amount of
followers), but also a mixture of quality measures that reflect interaction,
awareness, and visibility in the social sphere. The authors of this paper have
created InfluenceTracker, a publicly available website where anyone can rate
and compare the recent activity of any Twitter account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5244</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5244</id><created>2014-04-21</created><updated>2015-01-30</updated><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author><author><keyname>Rubinchik</keyname><forenames>Mikhail</forenames></author><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author></authors><title>$\mathrm{Pal}^k$ Is Linear Recognizable Online</title><categories>cs.FL cs.DS</categories><comments>18 pages, 5 figures, presented in SOFSEM 2015</comments><journal-ref>Proc. SOFSEM 2015. Springer, 2015. LNCS Vol. 8939, 289-301</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a language $L$ that is online recognizable in linear time and space, we
construct a linear time and space online recognition algorithm for the language
$L\cdot\mathrm{Pal}$, where $\mathrm{Pal}$ is the language of all nonempty
palindromes. Hence for every fixed positive $k$, $\mathrm{Pal}^k$ is online
recognizable in linear time and space. Thus we solve an open problem posed by
Galil and Seiferas in 1978.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5245</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5245</id><created>2014-04-21</created><updated>2015-02-26</updated><authors><author><keyname>Krysta</keyname><forenames>Piotr</forenames></author><author><keyname>Manlove</keyname><forenames>David</forenames></author><author><keyname>Rastegari</keyname><forenames>Baharak</forenames></author><author><keyname>Zhang</keyname><forenames>Jinshan</forenames></author></authors><title>Size versus truthfulness in the House Allocation problem</title><categories>cs.GT</categories><comments>Appeared in the Proceedings of EC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the House Allocation problem (also known as the Assignment problem),
i.e., the problem of allocating a set of objects among a set of agents, where
each agent has ordinal preferences (possibly involving ties) over a subset of
the objects. We focus on truthful mechanisms without monetary transfers for
finding large Pareto optimal matchings. It is straightforward to show that no
deterministic truthful mechanism can approximate a maximum cardinality Pareto
optimal matching with ratio better than 2. We thus consider randomized
mechanisms. We give a natural and explicit extension of the classical Random
Serial Dictatorship Mechanism (RSDM) specifically for the House Allocation
problem where preference lists can include ties. We thus obtain a universally
truthful randomized mechanism for finding a Pareto optimal matching and show
that it achieves an approximation ratio of $\frac{e}{e-1}$. The same bound
holds even when agents have priorities (weights) and our goal is to find a
maximum weight (as opposed to maximum cardinality) Pareto optimal matching. On
the other hand we give a lower bound of $\frac{18}{13}$ on the approximation
ratio of any universally truthful Pareto optimal mechanism in settings with
strict preferences. In the case that the mechanism must additionally be
non-bossy, an improved lower bound of $\frac{e}{e-1}$ holds. This lower bound
is tight given that RSDM for strict preference lists is non-bossy. We moreover
interpret our problem in terms of the classical secretary problem and prove
that our mechanism provides the best randomized strategy of the administrator
who interviews the applicants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5248</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5248</id><created>2014-04-21</created><authors><author><keyname>Meddeb</keyname><forenames>M.</forenames></author><author><keyname>Karray</keyname><forenames>H.</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>Intelligent Remote Control for TV Program based on Emotion in Arabic
  Speech</title><categories>cs.HC</categories><comments>6 pages, 3 figures</comments><journal-ref>International Journal of Scientific Research &amp; Engineering
  Technology (IJSET), ISSN: (2277-1581) volume 1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems for TV program have been studied for the realization of
personalized TV Electronic Program Guides. In this paper, we propose automatic
emotion Arabic speech recognition in order to achieve an intelligent remote
control. In addition, the TV can estimate our interests and preferences by
observing our behavior to watch and have a conversation on topics that might be
interesting to us.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5254</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5254</id><created>2014-04-21</created><authors><author><keyname>Haber</keyname><forenames>Eldad</forenames></author><author><keyname>Chung</keyname><forenames>Mathias</forenames></author></authors><title>Simultaneous Source for non-uniform data variance and missing data</title><categories>cs.CE</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of simultaneous sources in geophysical inverse problems has
revolutionized the ability to deal with large scale data sets that are obtained
from multiple source experiments. However, the technique breaks when the data
has non-uniform standard deviation or when some data are missing. In this paper
we develop, study, and compare a number of techniques that enable to utilize
advantages of the simultaneous source framework for these cases. We show that
the inverse problem can still be solved efficiently by using these new
techniques. We demonstrate our new approaches on the Direct Current Resistivity
inverse problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5267</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5267</id><created>2014-04-21</created><updated>2015-11-01</updated><authors><author><keyname>Brengos</keyname><forenames>Tomasz</forenames></author></authors><title>Lax functors and coalgebraic weak bisimulation</title><categories>cs.LO</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the work by Soboci\'nski on relational presheaves and their
connection with weak (bi)simulation for labelled transistion systems to a
coalgebraic setting. We show that the coalgebraic notion of saturation studied
in our previous work can be expressed in the language of lax functors in terms
of existence of a certain adjunction between categories of lax functors. This
observation allows us to generalize the notion of the coalgebraic weak
bisimulation to lax functors. We instantiate this definition on two examples of
timed systems and show that it coincides with their time-abstract behavioural
equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5278</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5278</id><created>2014-04-21</created><authors><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author><author><keyname>Clark</keyname><forenames>Stephen</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>The Frobenius anatomy of word meanings I: subject and object relative
  pronouns</title><categories>cs.CL</categories><comments>31 pages</comments><journal-ref>Journal of Logic and Computation, Special Issue: The Incomputable,
  an Isaac Newton Institute Workshop, 23(6), pp.1293-1317, 2013</journal-ref><doi>10.1093/logcom/ext044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a compositional vector-based semantics of subject and
object relative pronouns within a categorical framework. Frobenius algebras are
used to formalise the operations required to model the semantics of relative
pronouns, including passing information between the relative clause and the
modified noun phrase, as well as copying, combining, and discarding parts of
the relative clause. We develop two instantiations of the abstract semantics,
one based on a truth-theoretic approach and one based on corpus statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5287</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5287</id><created>2014-04-21</created><updated>2014-05-04</updated><authors><author><keyname>Lin</keyname><forenames>Chien-Hao</forenames></author><author><keyname>Ho</keyname><forenames>Yew Kam</forenames></author></authors><title>Quantification of entanglement entropy in helium by the Schmidt-Slater
  decomposition method</title><categories>quant-ph cs.IT math.IT physics.atom-ph physics.chem-ph</categories><comments>Final version to be published in Few-Body Systems (accepted on May
  2nd, 2014)</comments><journal-ref>Few-Body Systems 55, 1141 (2014)</journal-ref><doi>10.1007/s00601-014-0900-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present an investigation on the spatial entanglement
entropies in the helium atom by using highly correlated Hylleraas functions to
represent the S-wave states. Singlet-spin 1sns 1Se states (with n = 1 to 6) and
triplet-spin 1sns 3Se states (with n = 2 to 6) are investigated. As a measure
on the spatial entanglement, von Neumann entropy and linear entropy are
calculated. Furthermore, we apply the Schmidt-Slater decomposition method on
the two-electron wave functions, and obtain eigenvalues of the one-particle
reduced density matrix, from which the linear entropy and von Neumann entropy
can be determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5320</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5320</id><created>2014-04-21</created><updated>2014-09-19</updated><authors><author><keyname>Bocharov</keyname><forenames>Alex</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Efficient synthesis of universal Repeat-Until-Success circuits</title><categories>quant-ph cs.ET</categories><comments>15 pages, 10 figures; reformatted and minor edits; added Fig. 2 to
  visualize the density of z-rotations implementable via RUS protocols</comments><journal-ref>Phys. Rev. Lett. 114, 080502 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.080502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it was shown that Repeat-Until-Success (RUS) circuits can achieve a
$2.5$ times reduction in expected $T$-count over ancilla-free techniques for
single-qubit unitary decomposition. However, the previously best known
algorithm to synthesize RUS circuits requires exponential classical runtime. In
this paper we present an algorithm to synthesize an RUS circuit to approximate
any given single-qubit unitary within precision $\varepsilon$ in
probabilistically polynomial classical runtime. Our synthesis approach uses the
Clifford+$T$ basis, plus one ancilla qubit and measurement. We provide
numerical evidence that our RUS circuits have an expected $T$-count on average
$2.5$ times lower than the theoretical lower bound of $3 \log_2
(1/\varepsilon)$ for ancilla-free single-qubit circuit decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5322</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5322</id><created>2014-04-21</created><authors><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>CitNetExplorer: A new software tool for analyzing and visualizing
  citation networks</title><categories>cs.DL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present CitNetExplorer, a new software tool for analyzing and visualizing
citation networks of scientific publications. CitNetExplorer can for instance
be used to study the development of a research field, to delineate the
literature on a research topic, and to support literature reviewing. We first
introduce the main concepts that need to be understood when working with
CitNetExplorer. We then demonstrate CitNetExplorer by using the tool to analyze
the scientometric literature and the literature on community detection in
networks. Finally, we discuss some technical details on the construction,
visualization, and analysis of citation networks in CitNetExplorer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5331</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5331</id><created>2014-04-21</created><updated>2014-04-24</updated><authors><author><keyname>Lin</keyname><forenames>Feng</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author><author><keyname>Browning</keyname><forenames>James P.</forenames></author></authors><title>Spectrum Sensing with Small-Sized Datasets in Cognitive Radio:
  Algorithms and Analysis</title><categories>cs.NI</categories><comments>11 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sensing is a fundamental component of cognitive radio. How to
promptly sense the presence of primary users is a key issue to a cognitive
radio network. The time requirement is critical in that violating it will cause
harmful interference to the primary user, leading to a system-wide failure. The
motivation of our work is to provide an effective spectrum sensing method to
detect primary users as soon as possible. In the language of streaming based
real-time data processing, short-time means small-sized data. In this paper, we
propose a cumulative spectrum sensing method dealing with limited sized data. A
novel method of covariance matrix estimation is utilized to approximate the
true covariance matrix. The theoretical analysis is derived based on
concentration inequalities and random matrix theory to support the claims of
detection performance. Comparisons between the proposed method and other
traditional approaches, judged by the simulation using a captured digital TV
signal, show that this proposed method can operate either using smaller-sized
data or working under lower SNR environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5344</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5344</id><created>2014-04-21</created><updated>2014-07-07</updated><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Feng</keyname><forenames>Wensen</forenames></author><author><keyname>Ranftl</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Qiao</keyname><forenames>Hong</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>A higher-order MRF based variational model for multiplicative noise
  reduction</title><categories>cs.CV</categories><comments>5 pages, 5 figures, to appear in IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2014.2337274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fields of Experts (FoE) image prior model, a filter-based higher-order
Markov Random Fields (MRF) model, has been shown to be effective for many image
restoration problems. Motivated by the successes of FoE-based approaches, in
this letter, we propose a novel variational model for multiplicative noise
reduction based on the FoE image prior model. The resulted model corresponds to
a non-convex minimization problem, which can be solved by a recently published
non-convex optimization algorithm. Experimental results based on synthetic
speckle noise and real synthetic aperture radar (SAR) images suggest that the
performance of our proposed method is on par with the best published
despeckling algorithm. Besides, our proposed model comes along with an
additional advantage, that the inference is extremely efficient. {Our GPU based
implementation takes less than 1s to produce state-of-the-art despeckling
performance.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5351</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5351</id><created>2014-04-21</created><authors><author><keyname>Hamid</keyname><forenames>Raffay</forenames></author><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>DeCoste</keyname><forenames>Dennis</forenames></author><author><keyname>Sundaresan</keyname><forenames>Neel</forenames></author></authors><title>Fast Approximate Matching of Cell-Phone Videos for Robust Background
  Subtraction</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We identify a novel instance of the background subtraction problem that
focuses on extracting near-field foreground objects captured using handheld
cameras. Given two user-generated videos of a scene, one with and the other
without the foreground object(s), our goal is to efficiently generate an output
video with only the foreground object(s) present in it. We cast this challenge
as a spatio-temporal frame matching problem, and propose an efficient solution
for it that exploits the temporal smoothness of the video sequences. We present
theoretical analyses for the error bounds of our approach, and validate our
findings using a detailed set of simulation experiments. Finally, we present
the results of our approach tested on multiple real videos captured using
handheld cameras, and compare them to several alternate foreground extraction
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5352</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5352</id><created>2014-04-21</created><updated>2014-04-26</updated><authors><author><keyname>Hassin</keyname><forenames>Dan</forenames></author><author><keyname>Scrivener</keyname><forenames>Adam</forenames></author><author><keyname>Zhou</keyname><forenames>Yibo</forenames></author></authors><title>Critique of J. Kim's &quot;P is not equal to NP by Modus Tollens&quot;</title><categories>cs.CC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a critique of version three of Joonmo Kim's paper entitled &quot;P
is not equal to NP by Modus Tollens. [arXiv:1403.4143v3]&quot; After summarizing
Kim's proof, we note that the logic that Kim uses is inconsistent, which
provides evidence that the proof is invalid. To show this, we will consider two
reasonable interpretations of Kim's definitions, and show that &quot;P is not equal
to NP&quot; does not seem to follow in an obvious way using any of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5356</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5356</id><created>2014-04-21</created><authors><author><keyname>Janssen</keyname><forenames>Jeannette</forenames></author><author><keyname>Vautour</keyname><forenames>Celeste</forenames></author></authors><title>Finding safe strategies for competitive diffusion on trees</title><categories>cs.DM cs.GR cs.SI</categories><comments>2 figures, colour</comments><journal-ref>Internet Mathematics 11:232-252, 2015</journal-ref><doi>10.1080/15427951.2014.977407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the two-player safe game of Competitive Diffusion, a game-theoretic
model for the diffusion of technologies or influence through a social network.
In game theory, safe strategies are mixed strategies with a minimal expected
gain against unknown strategies of the opponents. Safe strategies for
competitive diffusion lead to maximum spread of influence in the presence of
uncertainty about the other players. We study the safe game on two specific
classes of trees, spiders and complete trees, and give tight bounds on the
minimal expected gain. We then use these results to give an algorithm which
suggests a safe strategy for a player on any tree. We test this algorithm on
randomly generated trees, and show that it finds strategies that are close to
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5357</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5357</id><created>2014-04-21</created><authors><author><keyname>Kalita</keyname><forenames>Nayan Jyoti</forenames></author><author><keyname>Saharia</keyname><forenames>Navanath</forenames></author><author><keyname>Sinha</keyname><forenames>Smriti Kumar</forenames></author></authors><title>Morphological Analysis of the Bishnupriya Manipuri Language using Finite
  State Transducers</title><categories>cs.CL</categories><journal-ref>Computational Linguistics and Intelligent Text Processing,
  vol.8403, series: Lecture Notes in Computer Science, pp 206-213, (2014)</journal-ref><doi>10.1007/978-3-642-54906-9_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a morphological analysis of Bishnupriya Manipuri
language, an Indo-Aryan language spoken in the north eastern India. As of now,
there is no computational work available for the language. Finite state
morphology is one of the successful approaches applied in a wide variety of
languages over the year. Therefore we adapted the finite state approach to
analyse morphology of the Bishnupriya Manipuri language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5363</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5363</id><created>2014-04-21</created><updated>2015-01-28</updated><authors><author><keyname>Owen</keyname><forenames>Art B.</forenames></author></authors><title>A constraint on extensible quadrature rules</title><categories>math.NA cs.NA</categories><comments>7 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the worst case integration error in a family of functions decays as
$n^{-\alpha}$ for some $\alpha&gt;1$ and simple averages along an extensible
sequence match that rate at a set of sample sizes $n_1&lt;n_2&lt;\dots&lt;\infty$, then
these sample sizes must grow at least geometrically. More precisely,
$n_{k+1}/n_k\ge \rho$ must hold for a value $1&lt;\rho&lt;2$ that increases with
$\alpha$. This result always rules out arithmetic sequences but never rules out
sample size doubling. The same constraint holds in a root mean square setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5367</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5367</id><created>2014-04-21</created><authors><author><keyname>Passos</keyname><forenames>Alexandre</forenames></author><author><keyname>Kumar</keyname><forenames>Vineet</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Lexicon Infused Phrase Embeddings for Named Entity Resolution</title><categories>cs.CL</categories><comments>Accepted in CoNLL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art approaches for named-entity recognition (NER) use semi
supervised information in the form of word clusters and lexicons. Recently
neural network-based language models have been explored, as they as a byproduct
generate highly informative vector representations for words, known as word
embeddings. In this paper we present two contributions: a new form of learning
word embeddings that can leverage information from relevant lexicons to improve
the representations, and the first system to use neural word embeddings to
achieve state-of-the-art results on named-entity recognition in both CoNLL and
Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for
CoNLL 2003---significantly better than any previous system trained on public
data, and matching a system employing massive private industrial query-log
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5372</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5372</id><created>2014-04-21</created><authors><author><keyname>Ballatore</keyname><forenames>Andrea</forenames></author><author><keyname>Bertolotto</keyname><forenames>Michela</forenames></author><author><keyname>Wilson</keyname><forenames>David C.</forenames></author></authors><title>Linking Geographic Vocabularies through WordNet</title><categories>cs.IR cs.CL</categories><comments>21 pages, 1 figure</comments><journal-ref>Annals of GIS, 20 (2), 2014, 73-84</journal-ref><doi>10.1080/19475683.2014.904440</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linked open data (LOD) paradigm has emerged as a promising approach to
structuring and sharing geospatial information. One of the major obstacles to
this vision lies in the difficulties found in the automatic integration between
heterogeneous vocabularies and ontologies that provides the semantic backbone
of the growing constellation of open geo-knowledge bases. In this article, we
show how to utilize WordNet as a semantic hub to increase the integration of
LOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mapping
technique between a given vocabulary and WordNet, combining intensional and
extensional aspects of the geographic terms. Voc2WordNet is evaluated against a
sample of human-generated alignments with the OpenStreetMap (OSM) Semantic
Network, a crowdsourced geospatial resource, and the GeoNames ontology, the
vocabulary of a large digital gazetteer. These empirical results indicate that
the approach can obtain high precision and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5385</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5385</id><created>2014-04-22</created><authors><author><keyname>Baba-Ahmed</keyname><forenames>Mohammed Zakarya</forenames><affiliation>LTT</affiliation></author><author><keyname>Benmammar</keyname><forenames>Badr</forenames><affiliation>LTT</affiliation></author><author><keyname>Bendimerad</keyname><forenames>Fethi Tarik</forenames><affiliation>LTT</affiliation></author></authors><title>Vers l'auto-gestion d'un r\'eseau de radio cognitive</title><categories>cs.NI</categories><comments>in French. International Congress on Telecommunication and
  Application'14 (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio (CR) operates in different fields as varied, one of these is
cognitive radio networks. In this paper, we propose a new approach used CR,
which aims to manage potential failures of computer systems and applications
through the introduction of two aspects of autonomous networks to make systems
capable of managing themselves with minimum human intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5393</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5393</id><created>2014-04-22</created><authors><author><keyname>K</keyname><forenames>Sujathakumari</forenames></author><author><keyname>S</keyname><forenames>Dersanambika K.</forenames></author></authors><title>Cooperating distributed context-free hexagonal array grammar systems
  with permitting contexts</title><categories>cs.FL</categories><comments>16 pages, 6 figures, Published with International Journal of
  Mathematics Trends and Technology (IJMTT)</comments><journal-ref>International Journal of Mathematical Trends and Technology
  (IJMTT). V7:156-171 March 2014</journal-ref><doi>10.14445/22315373/IJMTT-V7P520</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we associate permitting symbols with rules of Grammars in the
components of cooperating distributed context free hexagonal array grammar
systems as a control mechanism and investigating the generative power of the
resulting systems in the terminal mode. This feature of associating permitting
symbols with rules when extended to patterns in the form of connected arrays
also requires checking of symbols, but this is simpler than usual pattern
matching. The benefit of allowing permitting symbols is that it enables us to
reduce the number of components required in a cooperating distributed hexagonal
array grammar system for generating a set of picture arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5398</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5398</id><created>2014-04-22</created><authors><author><keyname>Reingold</keyname><forenames>Omer</forenames></author><author><keyname>Vardi</keyname><forenames>Shai</forenames></author></authors><title>New Techniques and Tighter Bounds for Local Computation Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an input $x$, and a search problem $F$, local computation algorithms
(LCAs) implement access to specified locations of $y$ in a legal output $y \in
F(x)$, using polylogarithmic time and space. Mansour et al., (2012), had
previously shown how to convert certain online algorithms to LCAs. In this
work, we expand on that line of work and develop new techniques for designing
LCAs and bounding their space and time complexity. Our contributions are
fourfold: (1) We significantly improve the running times and space requirements
of LCAs for previous results, (2) we expand and better define the family of
online algorithms which can be converted to LCAs using our techniques, (3) we
show that our results apply to a larger family of graphs than that of previous
results, and (4) our proofs are simpler and more concise than the previous
proof methods.
  For example, we show how to construct LCAs that require
$O(\log{n}\log\log{n})$ space and $O(\log^2{n})$ time (and expected time
$O(\log\log{n})$) for problems such as maximal matching on a large family of
graphs, as opposed to the henceforth best results that required $O(\log^3{n})$
space and $O(\log^4{n})$ time, and applied to a smaller family of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5406</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5406</id><created>2014-04-22</created><authors><author><keyname>Saxena</keyname><forenames>Avinash</forenames></author><author><keyname>Rao</keyname><forenames>Shrisha</forenames></author></authors><title>Degradation Analysis of Probabilistic Parallel Choice Systems</title><categories>cs.PF math.ST stat.TH</categories><msc-class>90B25, 60K10</msc-class><journal-ref>International Journal of Reliability, Quality and Safety
  Engineering, vol. 21(3), June 2014</journal-ref><doi>10.1142/S0218539314500120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Degradation analysis is used to analyze the useful lifetimes of systems,
their failure rates, and various other system parameters like mean time to
failure (MTTF), mean time between failures (MTBF), and the system failure rate
(SFR). In many systems, certain possible parallel paths of execution that have
greater chances of success are preferred over others. Thus we introduce here
the concept of probabilistic parallel choice. We use binary and $n$-ary
probabilistic choice operators in describing the selections of parallel paths.
These binary and $n$-ary probabilistic choice operators are considered so as to
represent the complete system (described as a series-parallel system) in terms
of the probabilities of selection of parallel paths and their relevant
parameters. Our approach allows us to derive new and generalized formulae for
system parameters like MTTF, MTBF, and SFR. We use a generalized exponential
distribution, allowing distinct installation times for individual components,
and use this model to derive expressions for such system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5410</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5410</id><created>2014-04-22</created><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Angel</forenames></author></authors><title>Dynamics to equilibrium in Network Games: individual behavior and global
  response</title><categories>physics.soc-ph cs.GT</categories><journal-ref>PLoS ONE 10(3): e0120343 (2015)</journal-ref><doi>10.1371/journal.pone.0120343</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various social contexts ranging from public goods provision to information
collection can be depicted as games of strategic interactions, where a player's
well-being depends on her own action as well as on the actions taken by her
neighbors. Whereas much attention has been devoted to the identification and
characterization of Bayes-Nash equilibria of such games, in this work we look
at strategic interactions from an evolutionary perspective. Starting from a
recent mean-field analysis of the evolutionary dynamics in these games, here we
present results of numerical simulations designed to find out whether Nash
equilibria are accessible by adaptation of players' strategies, and in general
to find the attractors of the evolution. Simulations allow us to go beyond a
global characterization of the cooperativeness of the equilibria and probe into
the individual behavior. We find that when players imitate each other, the
evolution does not reach Nash equilibria and, worse, leads to very unfavorable
states in terms of welfare. On the contrary, when players update their behavior
rationally, they self-organize into a rich variety of Nash equilibria, where
individual behavior and payoffs are shaped by the nature of the game, the
structure of the social network and the players' position within the topology.
Our results allow us to assess the validity of the mean-field approaches and
also show qualitative agreement with theoretical predictions for equilibria in
the context of one-shot games under incomplete information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5412</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5412</id><created>2014-04-22</created><authors><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Analytical Assessment of Coordinated Overlay D2D Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, analytical assessment of overlay-inband device-to-device (D2D)
communications is investigated, under cellular-network-assisted (coordinated)
scheduling. To this end, a simple scheduling scheme is assumed that takes into
account only local (per cell) topological information of the D2D links.
Stochastic geometry tools are utilized in order to obtain analytical
expressions for the interferers density as well as the D2D link
signal-to-interference-ratio distribution. The analytical results accuracy is
validated by comparison with simulations. In addition, the analytical
expressions are employed for efficiently optimizing the parameters of a
cellular system with overlay D2D communications. It is shown that coordinated
scheduling of D2D transmissions enhances system performance both in terms of
average user rate as well as maximum allowable D2D link distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5417</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5417</id><created>2014-04-22</created><authors><author><keyname>Gros</keyname><forenames>Claudius</forenames></author><author><keyname>Linkerhand</keyname><forenames>Mathias</forenames></author><author><keyname>Walther</keyname><forenames>Valentin</forenames></author></authors><title>Attractor Metadynamics in Adapting Neural Networks</title><categories>q-bio.NC cond-mat.dis-nn cs.NE</categories><journal-ref>Artificial Neural Networks and Machine Learning-ICANN 2014 , S.
  Wermter et al. (Eds), pp. 65-72. Springer (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slow adaption processes, like synaptic and intrinsic plasticity, abound in
the brain and shape the landscape for the neural dynamics occurring on
substantially faster timescales. At any given time the network is characterized
by a set of internal parameters, which are adapting continuously, albeit
slowly. This set of parameters defines the number and the location of the
respective adiabatic attractors. The slow evolution of network parameters hence
induces an evolving attractor landscape, a process which we term attractor
metadynamics. We study the nature of the metadynamics of the attractor
landscape for several continuous-time autonomous model networks. We find both
first- and second-order changes in the location of adiabatic attractors and
argue that the study of the continuously evolving attractor landscape
constitutes a powerful tool for understanding the overall development of the
neural dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5421</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5421</id><created>2014-04-22</created><authors><author><keyname>Avner</keyname><forenames>Orly</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Concurrent bandits and cognitive radio networks</title><categories>cs.LG cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of multiple users targeting the arms of a single
multi-armed stochastic bandit. The motivation for this problem comes from
cognitive radio networks, where selfish users need to coexist without any side
communication between them, implicit cooperation or common control. Even the
number of users may be unknown and can vary as users join or leave the network.
We propose an algorithm that combines an $\epsilon$-greedy learning rule with a
collision avoidance mechanism. We analyze its regret with respect to the
system-wide optimum and show that sub-linear regret can be obtained in this
setting. Experiments show dramatic improvement compared to other algorithms for
this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5424</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5424</id><created>2014-04-22</created><authors><author><keyname>Bellenguez-Morineau</keyname><forenames>Odile</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Prot</keyname><forenames>Damien</forenames></author></authors><title>A Note on NP-Hardness of Preemptive Mean Flow-Time Scheduling for
  Parallel Machines</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper &quot;The complexity of mean flow time scheduling problems with
release times&quot;, by Baptiste, Brucker, Chrobak, D\&quot;urr, Kravchenko and Sourd,
the authors claimed to prove strong NP-hardness of the scheduling problem
$P|pmtn,r_j|\sum C_j$, namely multiprocessor preemptive scheduling where the
objective is to minimize the mean flow time. We point out a serious error in
their proof and give a new proof of strong NP-hardness for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5428</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5428</id><created>2014-04-22</created><authors><author><keyname>Huang</keyname><forenames>Bojun</forenames></author></authors><title>Sequential Resource Allocation with Positional Costs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing the total cost to run a sequence of $n$
tasks in the given order by $k$ agents under the positional cost model. The
cost to run a task not only depends on the intrinsic cost of the task itself,
but also monotonically related to the position this task is in the working list
of the agent assigned. Such a positional effect can naturally arise from the
classic sum-of-completion-time minimization problems, and is also well
motivated by the varying efficiency when an agent works in reality (such as due
to the learning effects or deteriorating effects). Also, it can be seen as a
deterministic variant of the classic Baysian sequential decision making
problems. This paper presents a simple and practical algorithm that runs in
$O(k^2 n)$ time and minimizes the total cost of any problem instance consisting
of two task types. The algorithm works by making greedy decision for each task
sequentially based on some stopping thresholds in a &quot;greedy-like&quot; allocation
simulation -- a working style coinciding with Gittins' optimal-stopping based
algorithm for the classic Baysian multi-armed bandit problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5432</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5432</id><created>2014-04-22</created><updated>2014-05-07</updated><authors><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Win-Win Kernelization for Degree Sequence Completion Problems</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the kernelizability of a class of NP-hard graph modification
problems based on vertex degree properties. Our main positive results refer to
NP-hard graph completion (that is, edge addition) cases while we show that
there is no hope to achieve analogous results for the corresponding vertex or
edge deletion versions. Our algorithms are based on a method that transforms
graph completion problems into efficiently solvable number problems and
exploits f-factor computations for translating the results back into the graph
setting. Indeed, our core observation is that we encounter a win-win situation
in the sense that either the number of edge additions is small (and thus faster
to find) or the problem is polynomial- time solvable. This approach helps in
answering an open question by Mathieson and Szeider [JCSS 2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5433</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5433</id><created>2014-04-22</created><updated>2015-07-14</updated><authors><author><keyname>Grandi</keyname><forenames>Umberto</forenames></author><author><keyname>Grossi</keyname><forenames>Davide</forenames></author><author><keyname>Turrini</keyname><forenames>Paolo</forenames></author></authors><title>Equilibrium Refinement through Negotiation in Binary Voting</title><categories>cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study voting games on binary issues, where voters might hold an objective
over some issues at stake, while willing to strike deals on the remaining ones,
and can influence one another's voting decision before the vote takes place. We
analyse voters' rational behaviour in the resulting two-phase game, showing
under what conditions undesirable equilibria can be removed as an effect of the
pre-vote phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5439</identifier>
 <datestamp>2014-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5439</id><created>2014-04-22</created><authors><author><keyname>De Maria</keyname><forenames>Elisabetta</forenames><affiliation>INRIA and CNRS</affiliation></author><author><keyname>Despeyroux</keyname><forenames>Joelle</forenames><affiliation>INRIA and CNRS</affiliation></author><author><keyname>Felty</keyname><forenames>Amy</forenames><affiliation>EECS, Ottawa</affiliation></author></authors><title>A Logical Framework for Systems Biology</title><categories>cs.LO q-bio.OT</categories><comments>(2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for the formal verification of biological systems
based on the use of a modal linear logic. We show how such a logic can be used,
with worlds as instants of time, as an unified framework to encode both
biological systems and temporal properties of their dynamic behaviour. To
illustrate our methodology, we consider a model of the P53/Mdm2 DNA-damage
repair mechanism. We prove several properties that are important for such a
model to satisfy and serve to illustrate the promise of our approach. We
formalize the proofs of these properties in the Coq Proof Assistant, with the
help of a Lambda Prolog prover for partial automation of the proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5448</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5448</id><created>2014-04-22</created><authors><author><keyname>Arumugam</keyname><forenames>Guru Prakash</forenames></author><author><keyname>Augustine</keyname><forenames>John</forenames></author><author><keyname>Golin</keyname><forenames>Mordecai J.</forenames></author><author><keyname>Srikanthan</keyname><forenames>Prashanth</forenames></author></authors><title>A Polynomial Time Algorithm for Minimax-Regret Evacuation on a Dynamic
  Path</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dynamic path network is an undirected path with evacuees situated at each
vertex. To evacuate the path, evacuees travel towards a designated sink
(doorway) to exit. Each edge has a capacity, the number of evacuees that can
enter the edge in unit time. Congestion occurs if an evacuee has to wait at a
vertex for other evacuees to leave first. The basic problem is to place k sinks
on the line, with an associated evacuation strategy, so as to minimize the
total time needed to evacuate everyone. The minmax-regret version introduces
uncertainty into the input, with the number of evacuees at vertices only being
specified to within a range. The problem is to find a universal solution whose
regret (difference from optimal for a given input) is minimized over all legal
inputs. The previously best known algorithms for the minmax regret version
problem ran in time exponential in k. In this paper, we derive new prop- erties
of solutions that yield the first polynomial time algorithms for solving the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5453</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5453</id><created>2014-04-22</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author></authors><title>Games with a Weak Adversary</title><categories>cs.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-player graph games with partial-observation and parity
objective. While the decision problem for three-player games with a coalition
of the first and second players against the third player is undecidable, we
present a decidability result for partial-observation games where the first and
third player are in a coalition against the second player, thus where the
second player is adversarial but weaker due to partial-observation. We
establish tight complexity bounds in the case where player 1 is less informed
than player 2, namely 2-EXPTIME-completeness for parity objectives. The
symmetric case of player 1 more informed than player 2 is much more
complicated, and we show that already in the case where player 1 has perfect
observation, memory of size non-elementary is necessary in general for
reachability objectives, and the problem is decidable for safety and
reachability objectives. Our results have tight connections with
partial-observation stochastic games for which we derive new complexity
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5454</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5454</id><created>2014-04-22</created><authors><author><keyname>Singla</keyname><forenames>Adish</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author><author><keyname>Kamar</keyname><forenames>Ece</forenames></author><author><keyname>White</keyname><forenames>Ryen</forenames></author></authors><title>Stochastic Privacy</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online services such as web search and e-commerce applications typically rely
on the collection of data about users, including details of their activities on
the web. Such personal data is used to enhance the quality of service via
personalization of content and to maximize revenues via better targeting of
advertisements and deeper engagement of users on sites. To date, service
providers have largely followed the approach of either requiring or requesting
consent for opting-in to share their data. Users may be willing to share
private information in return for better quality of service or for incentives,
or in return for assurances about the nature and extend of the logging of data.
We introduce \emph{stochastic privacy}, a new approach to privacy centering on
a simple concept: A guarantee is provided to users about the upper-bound on the
probability that their personal data will be used. Such a probability, which we
refer to as \emph{privacy risk}, can be assessed by users as a preference or
communicated as a policy by a service provider. Service providers can work to
personalize and to optimize revenues in accordance with preferences about
privacy risk. We present procedures, proofs, and an overall system for
maximizing the quality of services, while respecting bounds on allowable or
communicated privacy risk. We demonstrate the methodology with a case study and
evaluation of the procedures applied to web search personalization. We show how
we can achieve near-optimal utility of accessing information with provable
guarantees on the probability of sharing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5458</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5458</id><created>2014-04-22</created><authors><author><keyname>Gordienko</keyname><forenames>Yuri</forenames></author><author><keyname>Bekenov</keyname><forenames>Lev</forenames></author><author><keyname>Gatsenko</keyname><forenames>Olexandr</forenames></author><author><keyname>Zasimchuk</keyname><forenames>Elena</forenames></author><author><keyname>Tatarenko</keyname><forenames>Valentin</forenames></author></authors><title>Complex Workflow Management and Integration of Distributed Computing
  Resources by Science Gateway Portal for Molecular Dynamics Simulations in
  Materials Science</title><categories>cs.CE cond-mat.mtrl-sci cs.DC</categories><comments>8 pages, 8 figures; Proc. of Third International Conference &quot;High
  Performance Computing&quot; HPC-UA 2013 (Ukraine, Kyiv, October 7-11, 2013)
  (http://hpc-ua.org/hpc-ua-13/proceedings). ISBN 978-966-7690-16-8 (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;IMP Science Gateway Portal&quot; (http://scigate.imp.kiev.ua) for complex
workflow management and integration of distributed computing resources (like
clusters, service grids, desktop grids, clouds) is presented. It is created on
the basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for
science workflow operation and gUSE - for smooth integration of available
resources for parallel and distributed computing in various heterogeneous
distributed computing infrastructures (DCI). The typical scientific workflow
with possible scenarios of its preparation and usage is considered. Several
typical science applications (scientific workflows) are considered for
molecular dynamics (MD) simulations of complex behavior of various
nanostructures (nanoindentation of graphene layers, defect system relaxation in
metal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The
advantages and drawbacks of the solution are shortly analyzed in the context of
its practical applications for MD simulations in materials science, physics and
nanotechnologies with available heterogeneous DCIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5468</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5468</id><created>2014-04-22</created><authors><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Intersection Graphs: An Introduction</title><categories>cs.DM</categories><comments>49 pages</comments><msc-class>05C62, 68Q22, 68Q25, 68R10</msc-class><journal-ref>Annals of Pure and Applied Mathematics, vol. 4, No. 1, 2013, 43-91</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intersection graphs are very important in both theoretical as well as
application point of view. Depending on the geometrical representation,
different type of intersection graphs are defined. Among them interval,
circular-arc, permutation, trapezoid, chordal, disk, circle graphs are more
important. In this article, a brief introduction of each of these intersection
graphs is given. Some basic properties and algorithmic status of few problems
on these graphs are cited. This article will help to the beginners to start
work in this direction. Since the article contains a lot of information in a
compact form it is also useful for the expert researchers too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5475</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5475</id><created>2014-04-22</created><updated>2014-11-01</updated><authors><author><keyname>Takhanov</keyname><forenames>Rustem</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author></authors><title>Combining pattern-based CRFs and weighted context-free grammars</title><categories>cs.FL cs.DS cs.LG</categories><comments>11 pages</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two models for the sequence labeling (tagging) problem. The first
one is a {\em Pattern-Based Conditional Random Field }(\PB), in which the
energy of a string (chain labeling) $x=x_1\ldots x_n\in D^n$ is a sum of terms
over intervals $[i,j]$ where each term is non-zero only if the substring
$x_i\ldots x_j$ equals a prespecified word $w\in \Lambda$. The second model is
a {\em Weighted Context-Free Grammar }(\WCFG) frequently used for natural
language processing. \PB and \WCFG encode local and non-local interactions
respectively, and thus can be viewed as complementary.
  We propose a {\em Grammatical Pattern-Based CRF model }(\GPB) that combines
the two in a natural way. We argue that it has certain advantages over existing
approaches such as the {\em Hybrid model} of Bened{\'i} and Sanchez that
combines {\em $\mbox{$N$-grams}$} and \WCFGs. The focus of this paper is to
analyze the complexity of inference tasks in a \GPB such as computing MAP. We
present a polynomial-time algorithm for general \GPBs and a faster version for
a special case that we call {\em Interaction Grammars}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5478</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5478</id><created>2014-04-22</created><authors><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Optimal control of information epidemics modeled as Maki Thompson rumors</title><categories>cs.SY cs.SI math.OC</categories><comments>Accepted for publication in Communications in Nonlinear Science and
  Numerical Simulation</comments><journal-ref>Communications in Nonlinear Science and Numerical Simulation, Vol.
  19, Issue 12, Dec. 2014, pp. 4135-4147</journal-ref><doi>10.1016/j.cnsns.2014.04.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model the spread of information in a homogeneously mixed population using
the Maki Thompson rumor model. We formulate an optimal control problem, from
the perspective of single campaigner, to maximize the spread of information
when the campaign budget is fixed. Control signals, such as advertising in the
mass media, attempt to convert ignorants and stiflers into spreaders. We show
the existence of a solution to the optimal control problem when the campaigning
incurs non-linear costs under the isoperimetric budget constraint. The solution
employs Pontryagin's Minimum Principle and a modified version of forward
backward sweep technique for numerical computation to accommodate the
isoperimetric budget constraint. The techniques developed in this paper are
general and can be applied to similar optimal control problems in other areas.
  We have allowed the spreading rate of the information epidemic to vary over
the campaign duration to model practical situations when the interest level of
the population in the subject of the campaign changes with time. The shape of
the optimal control signal is studied for different model parameters and
spreading rate profiles. We have also studied the variation of the optimal
campaigning costs with respect to various model parameters. Results indicate
that, for some model parameters, significant improvements can be achieved by
the optimal strategy compared to the static control strategy. The static
strategy respects the same budget constraint as the optimal strategy and has a
constant value throughout the campaign horizon. This work finds application in
election and social awareness campaigns, product advertising, movie promotion
and crowdfunding campaigns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5479</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5479</id><created>2014-04-22</created><authors><author><keyname>Huschenbett</keyname><forenames>Martin</forenames></author><author><keyname>Kuske</keyname><forenames>Dietrich</forenames></author><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>The monoid of queue actions</title><categories>cs.FL math.GR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We investigate the monoid of transformations that are induced by sequences of
writing to and reading from a queue storage. We describe this monoid by means
of a confluent and terminating semi-Thue system and study some of its basic
algebraic properties, e.g., conjugacy. Moreover, we show that while several
properties concerning its rational subsets are undecidable, their uniform
membership problem is NL-complete. Furthermore, we present an algebraic
characterization of this monoid's recognizable subsets. Finally, we prove that
it is not Thurston-automatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5480</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5480</id><created>2014-04-22</created><updated>2014-10-25</updated><authors><author><keyname>Dohmen</keyname><forenames>Klaus</forenames></author><author><keyname>Trinks</keyname><forenames>Martin</forenames></author></authors><title>An Abstraction of Whitney's Broken Circuit Theorem</title><categories>math.CO cs.DM math.NT</categories><comments>18 pages</comments><msc-class>05A15, 05C30, 05C31, 06A07, 11A25, 52A01</msc-class><journal-ref>The Electronic Journal of Combinatorics 21 (2014) #P4.32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a broad generalization of Whitney's broken circuit theorem on
the chromatic polynomial of a graph to sums of type $\sum_{A\subseteq S} f(A)$
where $S$ is a finite set and $f$ is a mapping from the power set of $S$ into
an abelian group. We give applications to the domination polynomial and the
subgraph component polynomial of a graph, the chromatic polynomial of a
hypergraph, the characteristic polynomial and Crapo's beta invariant of a
matroid, and the principle of inclusion-exclusion. Thus, we discover several
known and new results in a concise and unified way. As further applications of
our main result, we derive a new generalization of the maximums-minimums
identity and of a theorem due to Blass and Sagan on the M\&quot;obius function of a
finite lattice, which generalizes Rota's crosscut theorem. For the classical
M\&quot;obius function, both Euler's totient function and its Dirichlet inverse, and
the reciprocal of the Riemann zeta function we obtain new expansions involving
the greatest common divisor resp. least common multiple. We finally establish
an even broader generalization of Whitney's broken circuit theorem in the
context of convex geometries (antimatroids).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5481</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5481</id><created>2014-04-22</created><authors><author><keyname>Hours</keyname><forenames>Hadrien</forenames></author><author><keyname>Biersack</keyname><forenames>Ernst</forenames></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames></author></authors><title>Causal study of Network Performance</title><categories>cs.NI</categories><comments>ALGOTEL 2014 -- 16\`emes Rencontres Francophones sur les Aspects
  Algorithmiques des T\'el\'ecommunications, Le Bois-Plage-en-R\'e : France
  (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of Internet in the every day life has pushed its evolution in a very
fast way. The heterogeneity of the equipments supporting its networks, as well
as the different devices from which it can be accessed, have participated in
increasing the complexity of understanding its global behavior and performance.
In our study we propose a new method for studying the performance of TCP
protocol based on causal graphs. Causal graphs offer models easy to interpret
and use. They highlight the structural model of the system they represent and
give us access to the causal dependences between the different parameters of
the system. One of the major contribution of causal graphs is their ability to
predict the effects of an intervention from observations made before this
intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5501</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5501</id><created>2014-04-22</created><authors><author><keyname>Ye</keyname><forenames>Min</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>Polar Codes for Distributed Hierarchical Source Coding</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><journal-ref>Advances in Mathematics of Communication, 9, no.1, 2015, 87-103</journal-ref><doi>10.3934/amc.2015.9.87</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that polar codes can be used to achieve the rate-distortion functions
in the problem of hierarchical source coding also known as the successive
refinement problem. We also analyze the distributed version of this problem,
constructing a polar coding scheme that achieves the rate distortion functions
for successive refinement with side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5507</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5507</id><created>2014-04-22</created><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Strong Converse and Second-Order Asymptotics of Channel Resolvability</title><categories>cs.IT math.IT</categories><comments>7 pages, a shorter version will appear in ISIT 2014, this version
  includes the proofs of technical lemmas in appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of channel resolvability for fixed i.i.d. input
distributions and discrete memoryless channels (DMCs), and derive the strong
converse theorem for any DMCs that are not necessarily full rank. We also
derive the optimal second-order rate under a condition. Furthermore, under the
condition that a DMC has the unique capacity achieving input distribution, we
derive the optimal second-order rate of channel resolvability for the worst
input distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5510</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5510</id><created>2014-04-22</created><authors><author><keyname>Ghodselahi</keyname><forenames>Abdolhamid</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>Serving Online Demands with Movable Centers</title><categories>cs.DS</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an online problem in which a set of mobile resources or centers have
to be moved in order to optimally serve a set of demands. There is a set of $n$
nodes and a set of $k$ resources that are placed at some of the nodes. Each
node can potentially host several resources and the resources can be moved
between the nodes. At the nodes, demands arrive in an online fashion, and the
cost for serving the demands is a function of the number of demands and
resources at the different nodes. An online algorithm has to move resources in
order to keep the service cost low, however as moving resources is expensive,
the objective of an online algorithm is to minimize the total number of
movements.
  Specifically, we give a deterministic online algorithm, which for parameters
$\alpha\geq 1$ and $\beta\geq 0$ guarantees that at all times, the service cost
is within a multiplicative factor $\alpha$ and an additive term $\beta$ of the
optimal service cost and where also the movement cost is within a small
multiplicative factor and an additive term of the optimal overall cost.
Roughly, for $\alpha=1$ and $\beta=\Omega(k)$, we show that the movement cost
by time $t$ is upper bounded by the optimal service cost at time $t$ plus an
additive term of order $O(k\log k)$. For $\alpha&gt;1$ and arbitrary $\beta$, we
show that the movement cost by time $t$ is only linear in $k\log k$ and
logarithmic in the optimal service cost at time $t$. We also show that both
bounds are almost asymptotically tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5511</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5511</id><created>2014-04-18</created><authors><author><keyname>Goetschalckx</keyname><forenames>Robby</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author><author><keyname>Tadepalli</keyname><forenames>Prasad</forenames></author></authors><title>Coactive Learning for Locally Optimal Problem Solving</title><categories>cs.LG</categories><comments>AAAI 2014 paper, including appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coactive learning is an online problem solving setting where the solutions
provided by a solver are interactively improved by a domain expert, which in
turn drives learning. In this paper we extend the study of coactive learning to
problems where obtaining a globally optimal or near-optimal solution may be
intractable or where an expert can only be expected to make small, local
improvements to a candidate solution. The goal of learning in this new setting
is to minimize the cost as measured by the expert effort over time. We first
establish theoretical bounds on the average cost of the existing coactive
Perceptron algorithm. In addition, we consider new online algorithms that use
cost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved
theoretical bounds. We provide an empirical evaluation of the learners in
various domains, which show that the Perceptron based algorithms are quite
effective and that unlike the case for online classification, the PA algorithms
do not yield significant performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5513</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5513</id><created>2014-04-19</created><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Hetterich</keyname><forenames>Samuel</forenames></author><author><keyname>Rassmann</keyname><forenames>Felicia</forenames></author><author><keyname>Vilenchik</keyname><forenames>Dan</forenames></author></authors><title>The condensation phase transition in random graph coloring</title><categories>cs.DM math.CO math.PR</categories><msc-class>05C80</msc-class><doi>10.1007/s00220-015-2464-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a non-rigorous formalism called the &quot;cavity method&quot;, physicists have
put forward intriguing predictions on phase transitions in discrete structures.
One of the most remarkable ones is that in problems such as random $k$-SAT or
random graph $k$-coloring, very shortly before the threshold for the existence
of solutions there occurs another phase transition called &quot;condensation&quot;
[Krzakala et al., PNAS 2007]. The existence of this phase transition appears to
be intimately related to the difficulty of proving precise results on, e.g.,
the $k$-colorability threshold as well as to the performance of message passing
algorithms. In random graph $k$-coloring, there is a precise conjecture as to
the location of the condensation phase transition in terms of a distributional
fixed point problem. In this paper we prove this conjecture for $k$ exceeding a
certain constant $k_0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5520</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5520</id><created>2014-04-21</created><authors><author><keyname>Loshchilov</keyname><forenames>Ilya</forenames><affiliation>LIS</affiliation></author></authors><title>A Computationally Efficient Limited Memory CMA-ES for Large Scale
  Optimization</title><categories>cs.NE</categories><comments>Genetic and Evolutionary Computation Conference (GECCO'2014) (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a computationally efficient limited memory Covariance Matrix
Adaptation Evolution Strategy for large scale optimization, which we call the
LM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for
numerical optimization of non-linear, non-convex optimization problems in
continuous domain. Inspired by the limited memory BFGS method of Liu and
Nocedal (1989), the LM-CMA-ES samples candidate solutions according to a
covariance matrix reproduced from $m$ direction vectors selected during the
optimization process. The decomposition of the covariance matrix into Cholesky
factors allows to reduce the time and memory complexity of the sampling to
$O(mn)$, where $n$ is the number of decision variables. When $n$ is large
(e.g., $n$ &gt; 1000), even relatively small values of $m$ (e.g., $m=20,30$) are
sufficient to efficiently solve fully non-separable problems and to reduce the
overall run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5521</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5521</id><created>2014-04-22</created><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author></authors><title>Together we stand, Together we fall, Together we win: Dynamic Team
  Formation in Massive Open Online Courses</title><categories>cs.SI cs.CY cs.LG cs.MA</categories><comments>In Proceedings of 5th IEEE International Conference on Application of
  Digital Information &amp; Web Technologies (ICADIWT), India, February 2014 (6
  pages, 3 figures)</comments><doi>10.1109/ICADIWT.2014.6814694</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive Open Online Courses (MOOCs) offer a new scalable paradigm for
e-learning by providing students with global exposure and opportunities for
connecting and interacting with millions of people all around the world. Very
often, students work as teams to effectively accomplish course related tasks.
However, due to lack of face to face interaction, it becomes difficult for MOOC
students to collaborate. Additionally, the instructor also faces challenges in
manually organizing students into teams because students flock to these MOOCs
in huge numbers. Thus, the proposed research is aimed at developing a robust
methodology for dynamic team formation in MOOCs, the theoretical framework for
which is grounded at the confluence of organizational team theory, social
network analysis and machine learning. A prerequisite for such an undertaking
is that we understand the fact that, each and every informal tie established
among students offers the opportunities to influence and be influenced.
Therefore, we aim to extract value from the inherent connectedness of students
in the MOOC. These connections carry with them radical implications for the way
students understand each other in the networked learning community. Our
approach will enable course instructors to automatically group students in
teams that have fairly balanced social connections with their peers, well
defined in terms of appropriately selected qualitative and quantitative network
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5525</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5525</id><created>2014-04-17</created><authors><author><keyname>Hauenstein</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Pan</keyname><forenames>Victor</forenames></author><author><keyname>Szanto</keyname><forenames>Agnes</forenames></author></authors><title>Global Newton Iteration over Archimedean and non-Archimedean Fields</title><categories>cs.NA cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study iterative methods on the coefficients of the rational
univariate representation (RUR) of a given algebraic set, called global Newton
iteration. We compare two natural approaches to define locally quadratically
convergent iterations: the first one involves Newton iteration applied to the
approximate roots individually and then interpolation to find the RUR of these
approximate roots; the second one considers the coefficients in the exact RUR
as zeroes of a high dimensional map defined by polynomial reduction, and
applies Newton iteration on this map. We prove that over fields with a p-adic
valuation these two approaches give the same iteration function, but over
fields equipped with the usual Archimedean absolute value, they are not
equivalent. In the latter case, we give explicitly the iteration function for
both approaches. Finally, we analyze the parallel complexity of the different
versions of the global Newton iteration, compare them, and demonstrate that
they can be efficiently computed. The motivation for this study comes from the
certification of approximate roots of overdetermined and singular polynomial
systems via the recovery of an exact RUR from approximate numerical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5528</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5528</id><created>2014-04-22</created><authors><author><keyname>Javanmardi</keyname><forenames>Saeed</forenames></author><author><keyname>Shojafar</keyname><forenames>Mohammad</forenames></author><author><keyname>Amendola</keyname><forenames>Danilo</forenames></author><author><keyname>Cordeschi</keyname><forenames>Nicola</forenames></author><author><keyname>Liu</keyname><forenames>Hongbo</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Hybrid Genetic Algorithm for Cloud Computing Applications</title><categories>cs.DC cs.AI</categories><comments>10 Pages, 5 figures, 1 table, IBICA2014, Accepted to publish</comments><acm-class>C.2.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper with the aid of genetic algorithm and fuzzy theory, we present
a hybrid job scheduling approach, which considers the load balancing of the
system and reduces total execution time and execution cost. We try to modify
the standard Genetic algorithm and to reduce the iteration of creating
population with the aid of fuzzy theory. The main goal of this research is to
assign the jobs to the resources with considering the VM MIPS and length of
jobs. The new algorithm assigns the jobs to the resources with considering the
job length and resources capacities. We evaluate the performance of our
approach with some famous cloud scheduling models. The results of the
experiments show the efficiency of the proposed approach in term of execution
time, execution cost and average Degree of Imbalance (DI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5538</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5538</id><created>2014-04-22</created><updated>2014-10-09</updated><authors><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Analysis and Design of Two-Hop Diffusion-Based Molecular Communication
  Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, 1 table. Will be presented at the 2014 IEEE
  Global Communications Conference (GLOBECOM) in Austin, Texas, USA, on
  December 9, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a two-hop molecular communication network
consisting of one nanotransmitter, one nanoreceiver, and one nanotransceiver
acting as a relay. We consider two different schemes for relaying to improve
the range of diffusion-based molecular communication. In the first scheme, two
different types of messenger molecules are utilized at the relay node for
transmission and detection. In the second scheme, we assume that there is only
one type of molecule available to be used as an information carrier. We
identify self-interference as the performance-limiting effect for the second
relaying scheme. Self-interference occurs when the relay must detect the same
type of molecule that it also emits. Furthermore, we consider two relaying
modes analogous to those used in wireless communication systems, i.e.,
full-duplex and half-duplex. In particular, while our main focus is on
full-duplex relaying, half-duplex relaying is employed as a means to mitigate
self-interference. In addition, we propose the adaptation of the decision
threshold as an effective mechanism to mitigate self-interference at the relay
for full-duplex transmission. We derive closed-form expressions for the
expected error probability of the network for both considered relaying schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5539</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5539</id><created>2014-04-21</created><updated>2014-10-05</updated><authors><author><keyname>Rasouli</keyname><forenames>Mohammad</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Electricity Pooling Markets with Strategic Producers Possessing
  Asymmetric Information II: Inelastic Demand</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the restructured electricity industry, electricity pooling markets are an
oligopoly with strategic producers possessing private information (private
production cost function). We focus on pooling markets where aggregate demand
is represented by a non-strategic agent.
  Inelasticity of demand is a main difficulty in electricity markets which can
potentially result in market failure and high prices. We consider demand to be
inelastic.
  We propose a market mechanism that has the following features. (F1) It is
individually rational. (F2) It is budget balanced. (F3) It is price efficient,
that is, at equilibrium the price of electricity is equal to the marginal cost
of production. (F4) The energy production profile corresponding to every
non-zero Nash equilibrium of the game induced by the mechanism is a solution of
the corresponding centralized problem where the objective is the maximization
of the sum of the producers' and consumers' utilities.
  We identify some open problems associated with our approach to electricity
pooling markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5545</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5545</id><created>2014-04-22</created><updated>2014-04-30</updated><authors><author><keyname>Dixit</keyname><forenames>Kashyap</forenames></author></authors><title>$L_p$-Testers for Bounded Derivative Properties on Product Distributions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of $L_p$-testing of class of bounded derivative
properties over hypergrid domain with points distributed according to some
product distribution. This class includes monotonicity, the Lipschitz property,
$(\alpha,\beta)$-generalized Lipschitz and many more properties. Previous
results for $L_p$ testing on $[n]^d$ for this class were known for monotonicity
and $c$-Lipschitz properties over uniformly distributed domains. \medskip
  Our results imply testers that give the same upper bound for arbitrary
product distributions as the hitherto known testers, which use uniformly
randomly chosen samples from $[n]^d$, for monotonicity and Lipschitz testing.
Also, our testers are \emph{optimal} for a large class of bounded derivative
properties, that includes $(\alpha, \beta)$-generalized Lipschitz property,
over uniform distributions. Infact, each edge in $[n]^d$ is allowed to have
it's own left and right Lipschitz constants. The time complexity is \emph{same}
for arbitrary product distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5548</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5548</id><created>2014-04-22</created><updated>2014-12-03</updated><authors><author><keyname>B&#xf6;hm</keyname><forenames>Martin</forenames></author><author><keyname>Sgall</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Vesel&#xfd;</keyname><forenames>Pavel</forenames></author></authors><title>Online Colored Bin Packing</title><categories>cs.DS</categories><comments>Added lower bound of 2.5 for at least three colors, expanded some
  proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Colored Bin Packing problem a sequence of items of sizes up to $1$
arrives to be packed into bins of unit capacity. Each item has one of $c\geq 2$
colors and an additional constraint is that we cannot pack two items of the
same color next to each other in the same bin. The objective is to minimize the
number of bins.
  In the important special case when all items have size zero, we characterize
the optimal value to be equal to color discrepancy. As our main result, we give
an (asymptotically) 1.5-competitive algorithm which is optimal. In fact, the
algorithm always uses at most $\lceil1.5\cdot OPT\rceil$ bins and we show a
matching lower bound of $\lceil1.5\cdot OPT\rceil$ for any value of $OPT\geq
2$. In particular, the absolute ratio of our algorithm is $5/3$ and this is
optimal.
  For items of unrestricted sizes we give an asymptotically $3.5$-competitive
algorithm. When the items have sizes at most $1/d$ for a real $d \geq 2$ the
asymptotic competitive ratio is $1.5+d/(d-1)$. We also show that classical
algorithms First Fit, Best Fit and Worst Fit are not constant competitive,
which holds already for three colors and small items.
  In the case of two colors---the Black and White Bin Packing problem---we
prove that all Any Fit algorithms have absolute competitive ratio $3$. When the
items have sizes at most $1/d$ for a real $d \geq 2$ we show that the Worst Fit
algorithm is absolutely $(1+d/(d-1))$-competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5552</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5552</id><created>2014-04-22</created><authors><author><keyname>Elliott</keyname><forenames>James</forenames></author><author><keyname>Hoemmen</keyname><forenames>Mark</forenames></author><author><keyname>Mueller</keyname><forenames>Frank</forenames></author></authors><title>Tolerating Silent Data Corruption in Opaque Preconditioners</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate algorithm-based fault tolerance for silent, transient data
corruption in &quot;black-box&quot; preconditioners. We consider both additive Schwarz
domain decomposition with an ILU(k) subdomain solver, and algebraic multigrid,
both implemented in the Trilinos library. We evaluate faults that corrupt
preconditioner results in both single and multiple MPI ranks. We then analyze
how our approach behaves when then application is scaled. Our technique is
based on a Selective Reliability approach that performs most operations in an
unreliable mode, with only a few operations performed reliably. We also
investigate two responses to faults and discuss the performance overheads
imposed by each. For a non-symmetric problem solved using GMRES and ILU, we
show that at scale our fault tolerance approach incurs only 22% overhead for
the worst case. With detection techniques, we are able to reduce this overhead
to 1.8% in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5557</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5557</id><created>2014-04-22</created><updated>2016-02-10</updated><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Deledalle</keyname><forenames>Charles-Alban</forenames><affiliation>IMB</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal M.</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Dossal</keyname><forenames>Charles</forenames><affiliation>IMB</affiliation></author></authors><title>The Degrees of Freedom of Partly Smooth Regularizers</title><categories>math.ST cs.IT math.IT stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are concerned with regularized regression problems where
the prior regularizer is a proper lower semicontinuous and convex function
which is also partly smooth relative to a Riemannian submanifold. This
encompasses as special cases several known penalties such as the Lasso
($\ell^1$-norm), the group Lasso ($\ell^1-\ell^2$-norm), the
$\ell^\infty$-norm, and the nuclear norm. This also includes so-called
analysis-type priors, i.e. composition of the previously mentioned penalties
with linear operators, typical examples being the total variation or fused
Lasso penalties.We study the sensitivity of any regularized minimizer to
perturbations of the observations and provide its precise local
parameterization.Our main sensitivity analysis result shows that the predictor
moves locally stably along the same active submanifold as the observations
undergo small perturbations. This local stability is a consequence of the
smoothness of the regularizer when restricted to the active submanifold, which
in turn plays a pivotal role to get a closed form expression for the variations
of the predictor w.r.t. observations. We also show that, for a variety of
regularizers, including polyhedral ones or the group Lasso and its analysis
counterpart, this divergence formula holds Lebesgue almost everywhere.When the
perturbation is random (with an appropriate continuous distribution), this
allows us to derive an unbiased estimator of the degrees of freedom and of the
risk of the estimator prediction.Our results hold true without requiring the
design matrix to be full column rank.They generalize those already known in the
literature such as the Lasso problem, the general Lasso problem (analysis
$\ell^1$-penalty), or the group Lasso where existing results for the latter
assume that the design is full column rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5562</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5562</id><created>2014-04-22</created><updated>2015-01-23</updated><authors><author><keyname>Zhang</keyname><forenames>Sai</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Liu</keyname><forenames>Xue</forenames></author></authors><title>Characterizing Information Spreading in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks (OSNs) are changing the way in which the information
spreads throughout the Internet. A deep understanding of the information
spreading in OSNs leads to both social and commercial benefits. In this paper,
we characterize the dynamic of information spreading (e.g., how fast and widely
the information spreads against time) in OSNs by developing a general and
accurate model based on the Interactive Markov Chains (IMCs) and mean-field
theory. This model explicitly reveals the impacts of the network topology on
information spreading in OSNs. Further, we extend our model to feature the
time-varying user behaviors and the ever-changing information popularity. The
complicated dynamic patterns of information spreading are captured by our model
using six key parameters. Extensive tests based on Renren's dataset validate
the accuracy of our model, which demonstrate that it can characterize the
dynamic patterns of video sharing in Renren precisely and predict future
spreading tendency successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5565</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5565</id><created>2014-04-22</created><updated>2016-02-07</updated><authors><author><keyname>Oliveira</keyname><forenames>Mateus de Oliveira</forenames></author></authors><title>On the Satisfiability of Quantum Circuits of Small Treewidth</title><categories>cs.CC quant-ph</categories><comments>30 Pages. A preliminary version of this paper appeared at the 10th
  International Computer Science Symposium in Russia (CSR 2015). This version
  has been submitted to a journal and is currently under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been known for almost three decades that many $\mathrm{NP}$-hard
optimization problems can be solved in polynomial time when restricted to
structures of constant treewidth. In this work we provide the first extension
of such results to the quantum setting. We show that given a quantum circuit
$C$ with $n$ uninitialized inputs, $\mathit{poly}(n)$ gates, and treewidth $t$,
one can compute in time $(\frac{n}{\delta})^{\exp(O(t))}$ a classical
assignment $y\in \{0,1\}^n$ that maximizes the acceptance probability of $C$ up
to a $\delta$ additive factor. In particular, our algorithm runs in polynomial
time if $t$ is constant and $1/poly(n) &lt; \delta &lt; 1$. For unrestricted values
of $t$, this problem is known to be complete for the complexity class
$\mathrm{QCMA}$, a quantum generalization of MA. In contrast, we show that the
same problem is $\mathrm{NP}$-complete if $t=O(\log n)$ even when $\delta$ is
constant.
  On the other hand, we show that given a $n$-input quantum circuit $C$ of
treewidth $t=O(\log n)$, and a constant $\delta&lt;1/2$, it is
$\mathrm{QMA}$-complete to determine whether there exists a quantum state
$\mid\!\varphi\rangle \in (\mathbb{C}^d)^{\otimes n}$ such that the acceptance
probability of $C\mid\!\varphi\rangle$ is greater than $1-\delta$, or whether
for every such state $\mid\!\varphi\rangle$, the acceptance probability of
$C\mid\!\varphi\rangle$ is less than $\delta$. As a consequence, under the
widely believed assumption that $\mathrm{QMA} \neq \mathrm{NP}$, we have that
quantum witnesses are strictly more powerful than classical witnesses with
respect to Merlin-Arthur protocols in which the verifier is a quantum circuit
of logarithmic treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5566</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5566</id><created>2014-04-21</created><updated>2014-04-23</updated><authors><author><keyname>Agrawal</keyname><forenames>Akanksha</forenames></author><author><keyname>Govindarajan</keyname><forenames>Sathish</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author></authors><title>Vertex Cover Gets Faster and Harder on Low Degree Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding an optimal vertex cover in a graph is a classic
NP-complete problem, and is a special case of the hitting set question. On the
other hand, the hitting set problem, when asked in the context of induced
geometric objects, often turns out to be exactly the vertex cover problem on
restricted classes of graphs. In this work we explore a particular instance of
such a phenomenon. We consider the problem of hitting all axis-parallel slabs
induced by a point set P, and show that it is equivalent to the problem of
finding a vertex cover on a graph whose edge set is the union of two
Hamiltonian Paths. We show the latter problem to be NP-complete, and we also
give an algorithm to find a vertex cover of size at most k, on graphs of
maximum degree four, whose running time is 1.2637^k n^O(1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5568</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5568</id><created>2014-04-20</created><authors><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Tsur</keyname><forenames>Gilad</forenames></author></authors><title>The Power of an Example: Hidden Set Size Approximation Using Group
  Queries and Conditional Sampling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a basic problem of approximating the size of an unknown set $S$ in a
known universe $U$. We consider two versions of the problem. In both versions
the algorithm can specify subsets $T\subseteq U$. In the first version, which
we refer to as the group query or subset query version, the algorithm is told
whether $T\cap S$ is non-empty. In the second version, which we refer to as the
subset sampling version, if $T\cap S$ is non-empty, then the algorithm receives
a uniformly selected element from $T\cap S$. We study the difference between
these two versions under different conditions on the subsets that the algorithm
may query/sample, and in both the case that the algorithm is adaptive and the
case where it is non-adaptive. In particular we focus on a natural family of
allowed subsets, which correspond to intervals, as well as variants of this
family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5569</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5569</id><created>2014-04-22</created><updated>2016-02-01</updated><authors><author><keyname>B&#xf6;hm</keyname><forenames>Martin</forenames></author><author><keyname>Sgall</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>van Stee</keyname><forenames>Rob</forenames></author><author><keyname>Vesel&#xfd;</keyname><forenames>Pavel</forenames></author></authors><title>Online Bin Stretching with Three Bins</title><categories>cs.DS</categories><comments>Preprint of a journal version. See version 2 for the conference
  paper. Conference paper split into two journal submissions; see
  arXiv:1601.08111</comments><msc-class>68W27, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Bin Stretching is a semi-online variant of bin packing in which the
algorithm has to use the same number of bins as an optimal packing, but is
allowed to slightly overpack the bins. The goal is to minimize the amount of
overpacking, i.e., the maximum size packed into any bin.
  We give an algorithm for Online Bin Stretching with a stretching factor of
$11/8 = 1.375$ for three bins. Additionally, we present a lower bound of $45/33
= 1.\overline{36}$ for Online Bin Stretching on three bins and a lower bound of
$19/14$ for four and five bins that were discovered using a computer search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5581</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5581</id><created>2014-04-22</created><updated>2014-04-23</updated><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author></authors><title>Uniform and Piece-wise Uniform Fields in Memristor Models</title><categories>cs.ET cond-mat.mtrl-sci physics.chem-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Strukov model was the phenomenological model that accompanied the
announcement of the first recognised physical instantiation of the memristor
and, as such, it has been widely used. This model described the motion of a
boundary, $w$, between two types of inter-converting material,
$R_{\mathrm{off}}$ and $R_{\mathrm{on}}$, seemingly under a uniform field
across the entire device. In fact, what was intended was a field with a
discontinuity at $w$, that was uniform between $0&lt;x&lt;w$. In this paper we show
that the discontinuity is required for the Strukov model derivation to be
completed, and thus the derivation as given does not describe a situation with
a uniform field across the entire device. The discontinuity can be described as
a Heaviside function, $H$, located on $w$, for which there are three common
single-valued approximations for $H(w)$. The Strukov model as intended includes
an approximation for the Heaviside function (the field is taken to be the same
as that across the $R_{\mathrm{on}}$ part of the device). We compare
approximations and give solutions. We then extend the description of the field
to a more-realistic continuously varying sigmoidal transition between two
uniform fields and demonstrate that the centro-symmetric approximation model
(taking the field as being the average of the fields across $R_{\mathrm{on}}$
and $R_{\mathrm{off}}$) is a better single-point model of that situation: the
other two approximations over or underestimate the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5584</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5584</id><created>2014-04-22</created><authors><author><keyname>Reimers</keyname><forenames>Arne C.</forenames></author><author><keyname>Stougie</keyname><forenames>Leen</forenames></author></authors><title>A decomposition theory for vertex enumeration of convex polyhedra</title><categories>cs.CG cs.CC math.CO q-bio.MN</categories><comments>15 pages, 1 figure</comments><msc-class>52C45 (Primary), 52B40, 52B11, 68Q25 (Secondary)</msc-class><acm-class>F.2.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years the vertex enumeration problem of polyhedra has seen a
revival in the study of metabolic networks, which increased the demand for
efficient vertex enumeration algorithms for high-dimensional polyhedra given by
inequalities. In this paper we apply the concept of branch-decomposition to the
vertex enumeration problem of polyhedra $P = \{x : Sx = b, x \geq 0\}$.
Therefore, we introduce the concept of $k$-module and show how it relates to
the separators of the linear matroid generated by the columns of $S$. This then
translates structural properties of the matroidal branch-decomposition to the
context of polyhedra. We then use this to present a total polynomial time
algorithm for polytopes $P$ for which the branch-width of the linear matroid
generated by $S$ is bounded by a constant $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5585</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5585</id><created>2014-04-22</created><authors><author><keyname>Skala</keyname><forenames>Matthew</forenames></author></authors><title>A Structural Query System for Han Characters</title><categories>cs.CL cs.DB</categories><comments>28 pages, 5 figures, for submission to ACM Transactions on Asian
  Language Information Processing</comments><acm-class>H.3.1</acm-class><journal-ref>International Journal of Asian Language Processing 23(2) (2015)
  127-159</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IDSgrep structural query system for Han character dictionaries is
presented. This system includes a data model and syntax for describing the
spatial structure of Han characters using Extended Ideographic Description
Sequences (EIDSes) based on the Unicode IDS syntax; a language for querying
EIDS databases, designed to suit the needs of font developers and foreign
language learners; a bit vector index inspired by Bloom filters for faster
query operations; a freely available implementation; and format translation
from popular third-party IDS and XML character databases. Experimental results
are included, with a comparison to other software used for similar
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5588</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5588</id><created>2014-04-22</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Alzahrani</keyname><forenames>Majed</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>Large Margin Image Set Representation and Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel image set representation and classification
method by maximizing the margin of image sets. The margin of an image set is
defined as the difference of the distance to its nearest image set from
different classes and the distance to its nearest image set of the same class.
By modeling the image sets by using both their image samples and their affine
hull models, and maximizing the margins of the images sets, the image set
representation parameter learning problem is formulated as an minimization
problem, which is further optimized by an expectation -maximization (EM)
strategy with accelerated proximal gradient (APG) optimization in an iterative
algorithm. To classify a given test image set, we assign it to the class which
could provide the largest margin. Experiments on two applications of
video-sequence-based face recognition demonstrate that the proposed method
significantly outperforms state-of-the-art image set classification methods in
terms of both effectiveness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5605</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5605</id><created>2014-04-22</created><authors><author><keyname>Nikopour</keyname><forenames>Hosein</forenames></author><author><keyname>Yi</keyname><forenames>Eric</forenames></author><author><keyname>Bayesteh</keyname><forenames>Alireza</forenames></author><author><keyname>Au</keyname><forenames>Kelvin</forenames></author><author><keyname>Hawryluck</keyname><forenames>Mark</forenames></author><author><keyname>Baligh</keyname><forenames>Hadi</forenames></author><author><keyname>Ma</keyname><forenames>Jianglei</forenames></author></authors><title>SCMA for Downlink Multiple Access of 5G Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse code multiple access (SCMA) is a new frequency domain non-orthogonal
multiple-access technique which can improve spectral efficiency of wireless
radio access. With SCMA, different incoming data streams are directly mapped to
codewords of different multi-dimensional cookbooks, where each codeword
represents a spread transmission layer. Multiple SCMA layers share the same
time-frequency resources of OFDMA. The sparsity of codewords makes the
near-optimal detection feasible through iterative message passing algorithm
(MPA). Such low complexity of multi-layer detection allows excessive codeword
overloading in which the dimension of multiplexed layers exceeds the dimension
of codewords. Optimization of overloading factor along with modulation-coding
levels of layers provides a more flexible and efficient link-adaptation
mechanism. On the other hand, the signal spreading feature of SCMA can improve
link-adaptation as a result of less colored interference. In this paper a
technique is developed to enable multi-user SCMA (MU-SCMA) for downlink
wireless access. User pairing, power sharing, rate adjustment, and scheduling
algorithms are designed to improve the downlink throughput of a heavily loaded
network. The advantage of SCMA spreading for lightly loaded networks is also
evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5611</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5611</id><created>2014-04-22</created><authors><author><keyname>Gordienko</keyname><forenames>Yuri</forenames></author><author><keyname>Bekenev</keyname><forenames>Lev</forenames></author><author><keyname>Baskova</keyname><forenames>Olexandra</forenames></author><author><keyname>Gatsenko</keyname><forenames>Olexander</forenames></author><author><keyname>Zasimchuk</keyname><forenames>Elena</forenames></author><author><keyname>Stirenko</keyname><forenames>Sergii</forenames></author></authors><title>IMP Science Gateway: from the Portal to the Hub of Virtual Experimental
  Labs in Materials Science</title><categories>cs.CE cond-mat.mtrl-sci cs.DC</categories><comments>6 pages, 5 figures, 3 tables; 6th International Workshop on Science
  Gateways, IWSG-2014 (Dublin, Ireland, 3-5 June, 2014). arXiv admin note:
  substantial text overlap with arXiv:1404.5458</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Science gateway&quot; (SG) ideology means a user-friendly intuitive interface
between scientists (or scientific communities) and different software
components + various distributed computing infrastructures (DCIs) (like grids,
clouds, clusters), where researchers can focus on their scientific goals and
less on peculiarities of software/DCI. &quot;IMP Science Gateway Portal&quot;
(http://scigate.imp.kiev.ua) for complex workflow management and integration of
distributed computing resources (like clusters, service grids, desktop grids,
clouds) is presented. It is created on the basis of WS-PGRADE and gUSE
technologies, where WS-PGRADE is designed for science workflow operation and
gUSE - for smooth integration of available resources for parallel and
distributed computing in various heterogeneous distributed computing
infrastructures (DCI). The typical scientific workflows with possible scenarios
of its preparation and usage are presented. Several typical use cases for these
science applications (scientific workflows) are considered for molecular
dynamics (MD) simulations of complex behavior of various nanostructures
(nanoindentation of graphene layers, defect system relaxation in metal
nanocrystals, thermal stability of boron nitride nanotubes, etc.). The user
experience is analyzed in the context of its practical applications for MD
simulations in materials science, physics and nanotechnologies with available
heterogeneous DCIs. In conclusion, the &quot;science gateway&quot; approach - workflow
manager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity
to use the SG portal (like &quot;IMP Science Gateway Portal&quot;) in a very promising
way, namely, as a hub of various virtual experimental labs (different software
components + various requirements to resources) in the context of its practical
MD applications in materials science, physics, chemistry, biology, and
nanotechnologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5643</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5643</id><created>2014-04-22</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>A Formal Analysis of Required Cooperation in Multi-agent Planning</title><categories>cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on multi-agent planning has been popular in recent years. While
previous research has been motivated by the understanding that, through
cooperation, multi-agent systems can achieve tasks that are unachievable by
single-agent systems, there are no formal characterizations of situations where
cooperation is required to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a formal discussion from
the planning aspect. We first show that determining whether there is required
cooperation (RC) is intractable is general. Then, by dividing the problems that
require cooperation (referred to as RC problems) into two classes -- problems
with heterogeneous and homogeneous agents, we aim to identify all the
conditions that can cause RC in these two classes. We establish that when none
of these identified conditions hold, the problem is single-agent solvable.
Furthermore, with a few assumptions, we provide an upper bound on the minimum
number of agents required for RC problems with homogeneous agents. This study
not only provides new insights into multi-agent planning, but also has many
applications. For example, in human-robot teaming, when a robot cannot achieve
a task, it may be due to RC. In such cases, the human teammate should be
informed and, consequently, coordinate with other available robots for a
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5651</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5651</id><created>2014-04-22</created><updated>2014-11-19</updated><authors><author><keyname>Baccelli</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Biswas</keyname><forenames>Anup</forenames></author></authors><title>On Scaling Limits of Power Law Shot-noise Fields</title><categories>math.PR cs.IT math.IT</categories><comments>17 pages, Typos are corrected</comments><msc-class>60D05, 44A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the scaling limit of a class of shot-noise fields
defined on an independently marked stationary Poisson point process and with a
power law response function. Under appropriate conditions, it is shown that the
shot-noise field can be scaled suitably to have a $\alpha$-stable limit,
intensity of the underlying point process goes to infinity. It is also shown
that the finite dimensional distributions of the limiting random field have
i.i.d. stable random components. We hence propose to call this limte the
$\alpha$- stable white noise field. Analogous results are also obtained for the
extremal shot-noise field which converges to a Fr\'{e}chet white noise field.
Finally, these results are applied to the analysis of wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5653</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5653</id><created>2014-04-22</created><authors><author><keyname>Adalsteinsson</keyname><forenames>Gudmundur F.</forenames></author><author><keyname>Kevlahan</keyname><forenames>Nicholas K. -R.</forenames></author></authors><title>Compressive sampling for energy spectrum estimation of turbulent flows</title><categories>physics.flu-dyn cs.IT math.IT</categories><comments>20 pages, 11 figures, submitted to SIAM Journal on Scientific
  Computing</comments><msc-class>76F05 (Primary), 65F22, 65T60 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results from compressive sampling (CS) have demonstrated that accurate
reconstruction of sparse signals often requires far fewer samples than
suggested by the classical Nyquist--Shannon sampling theorem. Typically, signal
reconstruction errors are measured in the $\ell^2$ norm and the signal is
assumed to be sparse, compressible or having a prior distribution. Our spectrum
estimation by sparse optimization (SpESO) method uses prior information about
isotropic homogeneous turbulent flows with power law energy spectra and applies
the methods of CS to 1-D and 2-D turbulence signals to estimate their energy
spectra with small logarithmic errors. SpESO is distinct from existing energy
spectrum estimation methods which are based on sparse support of the signal in
Fourier space. SpESO approximates energy spectra with an order of magnitude
fewer samples than needed with Shannon sampling. Our results demonstrate that
SpESO performs much better than lumped orthogonal matching pursuit (LOMP), and
as well or better than wavelet-based best M-term or M/2-term methods, even
though these methods require complete sampling of the signal before
compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5660</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5660</id><created>2014-04-22</created><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Karloff</keyname><forenames>Howard</forenames></author></authors><title>Fast Algorithms for Constructing Maximum Entropy Summary Trees</title><categories>cs.DS</categories><comments>17 pages, 4 figures. Extended version of paper appearing in ICALP
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Karloff? and Shirley recently proposed summary trees as a new way to
visualize large rooted trees (Eurovis 2013) and gave algorithms for generating
a maximum-entropy k-node summary tree of an input n-node rooted tree. However,
the algorithm generating optimal summary trees was only pseudo-polynomial (and
worked only for integral weights); the authors left open existence of a
olynomial-time algorithm. In addition, the authors provided an additive
approximation algorithm and a greedy heuristic, both working on real weights.
This paper shows how to construct maximum entropy k-node summary trees in time
O(k^2 n + n log n) for real weights (indeed, as small as the time bound for the
greedy heuristic given previously); how to speed up the approximation algorithm
so that it runs in time O(n + (k^4/eps?) log(k/eps?)), and how to speed up the
greedy algorithm so as to run in time O(kn + n log n). Altogether, these
results make summary trees a much more practical tool than before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5665</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5665</id><created>2014-04-22</created><updated>2014-09-15</updated><authors><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author><author><keyname>Papavasileiou</keyname><forenames>Vasilis</forenames></author><author><keyname>Riedewald</keyname><forenames>Mirek</forenames></author></authors><title>ILP Modulo Data</title><categories>cs.LO</categories><comments>FMCAD 2014 final version plus proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast quantity of data generated and captured every day has led to a
pressing need for tools and processes to organize, analyze and interrelate this
data. Automated reasoning and optimization tools with inherent support for data
could enable advancements in a variety of contexts, from data-backed decision
making to data-intensive scientific research. To this end, we introduce a
decidable logic aimed at database analysis. Our logic extends quantifier-free
Linear Integer Arithmetic with operators from Relational Algebra, like
selection and cross product. We provide a scalable decision procedure that is
based on the BC(T) architecture for ILP Modulo Theories. Our decision procedure
makes use of database techniques. We also experimentally evaluate our approach,
and discuss potential applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5666</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5666</id><created>2014-04-22</created><updated>2015-02-13</updated><authors><author><keyname>Molkaraie</keyname><forenames>Mehdi</forenames></author></authors><title>An Importance Sampling Scheme on Dual Factor Graphs. II. Models with
  Strong Couplings</title><categories>stat.CO cs.IT math.IT physics.comp-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the partition function of the
two-dimensional ferromagnetic Ising and Potts models in an external magnetic
field. The estimation is done via importance sampling in the dual of the Forney
factor graph representing the models. We present importance sampling schemes
that can efficiently compute an estimate of the partition function in a wide
range of model parameters. Emphasis is on models in which a subset of the
coupling parameters is strong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5668</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5668</id><created>2014-04-22</created><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author><author><keyname>Lee</keyname><forenames>Daniel D.</forenames></author></authors><title>An Adversarial Interpretation of Information-Theoretic Bounded
  Rationality</title><categories>cs.AI</categories><comments>7 pages, 4 figures. Proceedings of AAAI-14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a growing interest in modeling planning with
information constraints. Accordingly, an agent maximizes a regularized expected
utility known as the free energy, where the regularizer is given by the
information divergence from a prior to a posterior policy. While this approach
can be justified in various ways, including from statistical mechanics and
information theory, it is still unclear how it relates to decision-making
against adversarial environments. This connection has previously been suggested
in work relating the free energy to risk-sensitive control and to extensive
form games. Here, we show that a single-agent free energy optimization is
equivalent to a game between the agent and an imaginary adversary. The
adversary can, by paying an exponential penalty, generate costs that diminish
the decision maker's payoffs. It turns out that the optimal strategy of the
adversary consists in choosing costs so as to render the decision maker
indifferent among its choices, which is a definining property of a Nash
equilibrium, thus tightening the connection between free energy optimization
and game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5683</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5683</id><created>2014-04-22</created><authors><author><keyname>Song</keyname><forenames>Eva C.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>The Likelihood Encoder for Lossy Source Compression</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a likelihood encoder is studied in the context of lossy source
compression. The analysis of the likelihood encoder is based on a soft-covering
lemma. It is demonstrated that the use of a likelihood encoder together with
the soft-covering lemma gives alternative achievability proofs for classical
source coding problems. The case of the rate-distortion function with side
information at the decoder (i.e. the Wyner-Ziv problem) is carefully examined
and an application of the likelihood encoder to the multi-terminal source
coding inner bound (i.e. the Berger-Tung region) is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5686</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5686</id><created>2014-04-22</created><updated>2014-07-09</updated><authors><author><keyname>Liu</keyname><forenames>Yue</forenames></author><author><keyname>Hu</keyname><forenames>Songlin</forenames></author><author><keyname>Rabl</keyname><forenames>Tilmann</forenames></author><author><keyname>Liu</keyname><forenames>Wantao</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author><author><keyname>Wu</keyname><forenames>Kaifeng</forenames></author><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Jintao</forenames></author></authors><title>DGFIndex for Smart Grid: Enhancing Hive with a Cost-Effective
  Multidimensional Range Index</title><categories>cs.DB cs.DC</categories><comments>12 pages, VLDB 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Smart Grid applications, as the number of deployed electric smart meters
increases, massive amounts of valuable meter data is generated and collected
every day. To enable reliable data collection and make business decisions fast,
high throughput storage and high-performance analysis of massive meter data
become crucial for grid companies. Considering the advantage of high
efficiency, fault tolerance, and price-performance of Hadoop and Hive systems,
they are frequently deployed as underlying platform for big data processing.
However, in real business use cases, these data analysis applications typically
involve multidimensional range queries (MDRQ) as well as batch reading and
statistics on the meter data. While Hive is high-performance at complex data
batch reading and analysis, it lacks efficient indexing techniques for MDRQ.
  In this paper, we propose DGFIndex, an index structure for Hive that
efficiently supports MDRQ for massive meter data. DGFIndex divides the data
space into cubes using the grid file technique. Unlike the existing indexes in
Hive, which stores all combinations of multiple dimensions, DGFIndex only
stores the information of cubes. This leads to smaller index size and faster
query processing. Furthermore, with pre-computing user-defined aggregations of
each cube, DGFIndex only needs to access the boundary region for aggregation
query. Our comprehensive experiments show that DGFIndex can save significant
disk space in comparison with the existing indexes in Hive and the query
performance with DGFIndex is 2-50 times faster than existing indexes in Hive
and HadoopDB for aggregation query, 2-5 times faster than both for
non-aggregation query, 2-75 times faster than scanning the whole table in
different query selectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5692</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5692</id><created>2014-04-22</created><updated>2015-07-22</updated><authors><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Shah</keyname><forenames>Parikshit</forenames></author><author><keyname>Wright</keyname><forenames>Stephen</forenames></author></authors><title>Forward - Backward Greedy Algorithms for Atomic Norm Regularization</title><categories>cs.DS cs.LG math.OC stat.ML</categories><comments>To appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2461515</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many signal processing applications, the aim is to reconstruct a signal
that has a simple representation with respect to a certain basis or frame.
Fundamental elements of the basis known as &quot;atoms&quot; allow us to define &quot;atomic
norms&quot; that can be used to formulate convex regularizations for the
reconstruction problem. Efficient algorithms are available to solve these
formulations in certain special cases, but an approach that works well for
general atomic norms, both in terms of speed and reconstruction accuracy,
remains to be found. This paper describes an optimization algorithm called
CoGEnT that produces solutions with succinct atomic representations for
reconstruction problems, generally formulated with atomic-norm constraints.
CoGEnT combines a greedy selection scheme based on the conditional gradient
approach with a backward (or &quot;truncation&quot;) step that exploits the quadratic
nature of the objective to reduce the basis size. We establish convergence
properties and validate the algorithm via extensive numerical experiments on a
suite of signal processing applications. Our algorithm and analysis also allow
for inexact forward steps and for occasional enhancements of the current
representation to be performed. CoGEnT can outperform the basic conditional
gradient method, and indeed many methods that are tailored to specific
applications, when the enhancement and truncation steps are defined
appropriately. We also introduce several novel applications that are enabled by
the atomic-norm framework, including tensor completion, moment problems in
signal processing, and graph deconvolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5699</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5699</id><created>2014-04-23</created><authors><author><keyname>Gough</keyname><forenames>John E.</forenames></author><author><keyname>James</keyname><forenames>Matthew R.</forenames></author><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author></authors><title>Quantum Trajectories for a Class of Continuous Matrix Product Input
  States</title><categories>quant-ph cs.SY math-ph math.MP math.OC</categories><comments>17 pages, 2 figures</comments><journal-ref>New J. Phys. 16, 075008 (2014)</journal-ref><doi>10.1088/1367-2630/16/7/075008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of continuous matrix product (CMP) states and
establish the stochastic master equations (quantum filters) for an arbitrary
quantum system probed by a bosonic input field in this class of states. We show
that this class of CMP states arise naturally as outputs of a Markovian model,
and that input fields in these states lead to master and filtering (quantum
trajectory) equations which are matrix-valued. Furthermore, it is shown that
this class of continuous matrix product states include the (continuous-mode)
single photon and time-ordered multi-photon states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5701</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5701</id><created>2014-04-23</created><updated>2015-05-08</updated><authors><author><keyname>Shah</keyname><forenames>Shahid Mehraj</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Achieving Shannon Capacity in a Wiretap Channel via Previous Messages</title><categories>cs.IT cs.CR math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a wiretap channel with a secret key buffer. We use
the coding scheme of [1] to enhance the secrecy rate to the capacity of the
main channel, while storing each securely transmitted message in the secret key
buffer. We use the oldest secret bits from the buffer to be used as a secret
key to transmit a message in a slot and then remove those bits. With this
scheme we are able to prove stronger results than those in [1]. i.e., not only
the message which is being transmitted currently, but all the messages
transmitted in last $N_1$ slots are secure with respect to all the information
that the eavesdropper possesses, where $N_1$ can be chosen arbitrarily large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5708</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5708</id><created>2014-04-23</created><authors><author><keyname>Xuan</keyname><forenames>Qi</forenames></author><author><keyname>Devanbu</keyname><forenames>Premkumar T</forenames></author><author><keyname>Filkov</keyname><forenames>Vladimir</forenames></author></authors><title>Converging Work-Talk Patterns in Online Task-Oriented Communities</title><categories>cs.SE cs.HC cs.SI physics.data-an</categories><acm-class>H.2.8; D.2.8; D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of what we do is accomplished by working collaboratively with others,
and a large portion of our lives are spent working and talking; the patterns
embodied in the alternation of working and talking can provide much useful
insight into task-oriented social behaviors. The available electronic traces of
the different kinds of human activities in online communities are an empirical
goldmine that can enable the holistic study and understanding of these social
systems. Open Source Software projects are prototypical examples of
collaborative, task-oriented communities, depending on volunteers for
high-quality work. Here, we use sequence analysis methods to identify the
work-talk patterns of software developers in these online communities.
  We find that software developers prefer to persist in same kinds of
activities, i.e., a string of work activities followed by a string of talk
activities and so forth, rather than switch them frequently; this tendency
strengthens with time, suggesting that developers become more efficient, and
can work longer with fewer interruptions. This process is accompanied by the
formation of community culture: developers' patterns in the same communities
get closer with time while different communities get relatively more different.
The emergence of community culture is apparently driven by both &quot;talk&quot; and
&quot;work&quot;. Finally, we also find that workers with good balance between &quot;work&quot; and
&quot;talk&quot; tend to produce just as much work as those that focus strongly on
&quot;work&quot;; however, the former appear to be more likely to continue to be active
contributors in the communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5709</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5709</id><created>2014-04-23</created><updated>2014-09-22</updated><authors><author><keyname>Karim</keyname><forenames>Mohammad S.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author></authors><title>In Order Packet Delivery in Instantly Decodable Network Coded Systems
  over Wireless Broadcast</title><categories>cs.NI</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study in-order packet delivery in instantly decodable
network coded systems for wireless broadcast networks. We are interested in
applications, in which the successful delivery of a packet depends on the
correct reception of this packet and all its preceding packets. We formulate
the problem of minimizing the number of undelivered packets to all receivers
over all transmissions until completion as a stochastic shortest path (SSP)
problem. Although finding the optimal packet selection policy using SSP is
computationally complex, it allows us to systematically exploit the problem
structure and draw guidelines for efficient packet selection policies that can
reduce the number of undelivered packets to all receivers over all
transmissions until completion. According to these guidelines, we design a
simple heuristic packet selection algorithm. Simulation results illustrate that
our proposed algorithm provides quicker packet delivery to the receivers
compared to the existing algorithms in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5711</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5711</id><created>2014-04-23</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author></authors><title>Modeling multi-stage decision optimization problems</title><categories>math.OC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-stage optimization under uncertainty techniques can be used to solve
long-term management problems. Although many optimization modeling language
extensions as well as computational environments have been proposed, the
acceptance of this technique is generally low, due to the inherent complexity
of the modeling and solution process. In this paper a simplification to
annotate multi-stage decision problems under uncertainty is presented - this
simplification contrasts with the common approach to create an extension on top
of an existing optimization modeling language. This leads to the definition of
meta models, which can be instanced in various programming languages. An
example using the statistical computing language R is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5715</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5715</id><created>2014-04-23</created><updated>2015-05-29</updated><authors><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Converses for Secret Key Agreement and Secure Computing</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider information theoretic secret key agreement and secure function
computation by multiple parties observing correlated data, with access to an
interactive public communication channel. Our main result is an upper bound on
the secret key length, which is derived using a reduction of binary hypothesis
testing to multiparty secret key agreement. Building on this basic result, we
derive new converses for multiparty secret key agreement. Furthermore, we
derive converse results for the oblivious transfer problem and the bit
commitment problem by relating them to secret key agreement. Finally, we derive
a necessary condition for the feasibility of secure computation by trusted
parties that seek to compute a function of their collective data, using an
interactive public communication that by itself does not give away the value of
the function. In many cases, we strengthen and improve upon previously known
converse bounds. Our results are single-shot and use only the given joint
distribution of the correlated observations. For the case when the correlated
observations consist of independent and identically distributed (in time)
sequences, we derive strong versions of previously known converses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5716</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5716</id><created>2014-04-23</created><authors><author><keyname>Kuijper</keyname><forenames>Margreta</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>List-Decoding Gabidulin Codes via Interpolation and the Euclidean
  Algorithm</title><categories>cs.IT math.IT</categories><comments>Submitted to ISITA 2014, IEICE copyright upon acceptance</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how Gabidulin codes can be list decoded by using a parametrization
approach. For this we consider a certain module in the ring of linearized
polynomials and find a minimal basis for this module using the Euclidean
algorithm with respect to composition of polynomials. For a given received
word, our decoding algorithm computes a list of all codewords that are closest
to the received word with respect to the rank metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5719</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5719</id><created>2014-04-23</created><updated>2015-05-25</updated><authors><author><keyname>Olmos</keyname><forenames>Pablo M.</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>A Scaling Law to Predict the Finite-Length Performance of
  Spatially-Coupled LDPC Codes</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, Volume 61 , Issue 6, June
  2015, Pages 3164 - 3184</journal-ref><doi>10.1109/TIT.2015.2422816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially-coupled LDPC codes are known to have excellent asymptotic
properties. Much less is known regarding their finite-length performance. We
propose a scaling law to predict the error probability of finite-length
spatially-coupled ensembles when transmission takes place over the binary
erasure channel. We discuss how the parameters of the scaling law are connected
to fundamental quantities appearing in the asymptotic analysis of these
ensembles and we verify that the predictions of the scaling law fit well to the
data derived from simulations over a wide range of parameters. The ultimate
goal of this line of research is to develop analytic tools for the design of
spatially-coupled LDPC codes under practical constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5734</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5734</id><created>2014-04-23</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author></authors><title>The Complexity of Ergodic Mean-payoff Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two-player (zero-sum) concurrent mean-payoff games played on a
finite-state graph. We focus on the important sub-class of ergodic games where
all states are visited infinitely often with probability 1. The algorithmic
study of ergodic games was initiated in a seminal work of Hoffman and Karp in
1966, but all basic complexity questions have remained unresolved. Our main
results for ergodic games are as follows: We establish (1) an optimal
exponential bound on the patience of stationary strategies (where patience of a
distribution is the inverse of the smallest positive probability and represents
a complexity measure of a stationary strategy); (2) the approximation problem
lie in FNP; (3) the approximation problem is at least as hard as the decision
problem for simple stochastic games (for which NP intersection coNP is the
long-standing best known bound). We present a variant of the strategy-iteration
algorithm by Hoffman and Karp; show that both our algorithm and the classical
value-iteration algorithm can approximate the value in exponential time; and
identify a subclass where the value-iteration algorithm is a FPTAS. We also
show that the exact value can be expressed in the existential theory of the
reals, and establish square-root sum hardness for a related class of games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5743</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5743</id><created>2014-04-23</created><updated>2014-04-28</updated><authors><author><keyname>Wang</keyname><forenames>Yaoyu</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Certificates in Data Structures</title><categories>cs.DS</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study certificates in static data structures. In the cell-probe model,
certificates are the cell probes which can uniquely identify the answer to the
query. As a natural notion of nondeterministic cell probes, lower bounds for
certificates in data structures immediately imply deterministic cell-probe
lower bounds. In spite of this extra power brought by nondeterminism, we prove
that two widely used tools for cell-probe lower bounds: richness lemma of
Miltersen et al. and direct-sum richness lemma of Patrascu and Thorup, both
hold for certificates in data structures with even better parameters. Applying
these lemmas and adopting existing reductions, we obtain certificate lower
bounds for a variety of static data structure problems. These certificate lower
bounds are at least as good as the highest known cell-probe lower bounds for
the respective problems. In particular, for approximate near neighbor (ANN)
problem in Hamming distance, our lower bound improves the state of the art.
When the space is strictly linear, our lower bound for ANN in d-dimensional
Hamming space becomes t=Omega(d), which along with the recent breakthrough for
polynomial evaluation of Larsen, are the only two t=Omega(d) lower bounds ever
proved for any problems in the cell-probe model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5756</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5756</id><created>2014-04-23</created><authors><author><keyname>Farina</keyname><forenames>R.</forenames></author><author><keyname>Dobricic</keyname><forenames>S.</forenames></author><author><keyname>Storto</keyname><forenames>A.</forenames></author><author><keyname>Masina</keyname><forenames>S.</forenames></author><author><keyname>Cuomo</keyname><forenames>S.</forenames></author></authors><title>A Revised Scheme to Compute Horizontal Covariances in an Oceanographic
  3D-VAR Assimilation System</title><categories>cs.NA cs.CE cs.DC math.NA</categories><comments>26 pages</comments><doi>10.1016/j.jcp.2015.01.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an improvement of an oceanographic three dimensional variational
assimilation scheme (3D-VAR), named OceanVar, by introducing a recursive filter
(RF) with the third order of accuracy (3rd-RF), instead of a RF with first
order of accuracy (1st-RF), to approximate horizontal Gaussian covariances. An
advantage of the proposed scheme is that the CPU's time can be substantially
reduced with benefits on the large scale applications. Experiments estimating
the impact of 3rd-RF are performed by assimilating oceanographic data in two
realistic oceanographic applications. The results evince benefits in terms of
assimilation process computational time, accuracy of the Gaussian correlation
modeling, and show that the 3rd-RF is a suitable tool for operational data
assimilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5764</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5764</id><created>2014-04-23</created><authors><author><keyname>Gatsenko</keyname><forenames>Olexander</forenames></author><author><keyname>Bekenev</keyname><forenames>Lev</forenames></author><author><keyname>Pavlov</keyname><forenames>Evgen</forenames></author><author><keyname>Gordienko</keyname><forenames>Yuri G.</forenames></author></authors><title>From Quantity to Quality: Massive Molecular Dynamics Simulation of
  Nanostructures under Plastic Deformation in Desktop and Service Grid
  Distributed Computing Infrastructure</title><categories>cs.CE cond-mat.mtrl-sci cs.DC</categories><comments>13 pages, 11 pages (http://journals.agh.edu.pl/csci/article/view/106)</comments><journal-ref>Computer Science 14 (1), 27-44 (2013)</journal-ref><doi>10.7494/csci.2013.14.1.27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed computing infrastructure (DCI) on the basis of BOINC and
EDGeS-bridge technologies for high-performance distributed computing is used
for porting the sequential molecular dynamics (MD) application to its parallel
version for DCI with Desktop Grids (DGs) and Service Grids (SGs). The actual
metrics of the working DG-SG DCI were measured, and the normal distribution of
host performances, and signs of log-normal distributions of other
characteristics (CPUs, RAM, and HDD per host) were found. The practical
feasibility and high efficiency of the MD simulations on the basis of DG-SG DCI
were demonstrated during the experiment with the massive MD simulations for the
large quantity of aluminum nanocrystals ($\sim10^2$-$10^3$). Statistical
analysis (Kolmogorov-Smirnov test, moment analysis, and bootstrapping analysis)
of the defect density distribution over the ensemble of nanocrystals had shown
that change of plastic deformation mode is followed by the qualitative change
of defect density distribution type over ensemble of nanocrystals. Some
limitations (fluctuating performance, unpredictable availability of resources,
etc.) of the typical DG-SG DCI were outlined, and some advantages (high
efficiency, high speedup, and low cost) were demonstrated. Deploying on DG DCI
allows to get new scientific $\it{quality}$ from the simulated $\it{quantity}$
of numerous configurations by harnessing sufficient computational power to
undertake MD simulations in a wider range of physical parameters
(configurations) in a much shorter timeframe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5765</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5765</id><created>2014-04-23</created><authors><author><keyname>Wolf</keyname><forenames>Daniel</forenames></author><author><keyname>Bajones</keyname><forenames>Markus</forenames></author><author><keyname>Prankl</keyname><forenames>Johann</forenames></author><author><keyname>Vincze</keyname><forenames>Markus</forenames></author></authors><title>Find my mug: Efficient object search with a mobile robot using semantic
  segmentation</title><categories>cs.CV cs.RO</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an efficient semantic segmentation framework for
indoor scenes, tailored to the application on a mobile robot. Semantic
segmentation can help robots to gain a reasonable understanding of their
environment, but to reach this goal, the algorithms not only need to be
accurate, but also fast and robust. Therefore, we developed an optimized 3D
point cloud processing framework based on a Randomized Decision Forest,
achieving competitive results at sufficiently high frame rates. We evaluate the
capabilities of our method on the popular NYU depth dataset and our own data
and demonstrate its feasibility by deploying it on a mobile service robot, for
which we could optimize an object search procedure using our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5767</identifier>
 <datestamp>2014-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5767</id><created>2014-04-23</created><authors><author><keyname>Richter</keyname><forenames>Hendrik</forenames></author></authors><title>Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates</title><categories>cs.NE</categories><journal-ref>In: Proc. IEEE Congress on Evolutionary Computation, IEEE CEC
  2014, (Ed.: C. A. Coello, Coello), IEEE Press, Piscataway, NJ, 2014,
  2692-2699</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coevolutionary minimal substrates are simple and abstract models that allow
studying the relationships and codynamics between objective and subjective
fitness. Using these models an approach is presented for defining and analyzing
fitness landscapes of coevolutionary problems. We devise similarity measures of
codynamic fitness landscapes and experimentally study minimal substrates of
test--based and compositional problems for both cooperative and competitive
interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5770</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5770</id><created>2014-04-23</created><authors><author><keyname>Erdweg</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Storm</keyname><forenames>Tijs</forenames></author><author><keyname>Dai</keyname><forenames>Yi</forenames></author></authors><title>Capture-Avoiding and Hygienic Program Transformations (incl. Proofs)</title><categories>cs.PL</categories><comments>In Proceedings of European Conference on Object-Oriented Programming
  (ECOOP) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program transformations in terms of abstract syntax trees compromise
referential integrity by introducing variable capture. Variable capture occurs
when in the generated program a variable declaration accidentally shadows the
intended target of a variable reference. Existing transformation systems either
do not guarantee the avoidance of variable capture or impair the implementation
of transformations.
  We present an algorithm called name-fix that automatically eliminates
variable capture from a generated program by systematically renaming variables.
name-fix is guided by a graph representation of the binding structure of a
program, and requires name-resolution algorithms for the source language and
the target language of a transformation. name-fix is generic and works for
arbitrary transformations in any transformation system that supports origin
tracking for names. We verify the correctness of name-fix and identify an
interesting class of transformations for which name-fix provides hygiene. We
demonstrate the applicability of name-fix for implementing capture-avoiding
substitution, inlining, lambda lifting, and compilers for two domain-specific
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5772</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5772</id><created>2014-04-23</created><updated>2014-07-28</updated><authors><author><keyname>Zhang</keyname><forenames>Yuyu</forenames></author><author><keyname>Dai</keyname><forenames>Hanjun</forenames></author><author><keyname>Xu</keyname><forenames>Chang</forenames></author><author><keyname>Feng</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Taifeng</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Sequential Click Prediction for Sponsored Search with Recurrent Neural
  Networks</title><categories>cs.IR cs.LG cs.NE</categories><comments>Accepted by AAAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Click prediction is one of the fundamental problems in sponsored search. Most
of existing studies took advantage of machine learning approaches to predict ad
click for each event of ad view independently. However, as observed in the
real-world sponsored search system, user's behaviors on ads yield high
dependency on how the user behaved along with the past time, especially in
terms of what queries she submitted, what ads she clicked or ignored, and how
long she spent on the landing pages of clicked ads, etc. Inspired by these
observations, we introduce a novel framework based on Recurrent Neural Networks
(RNN). Compared to traditional methods, this framework directly models the
dependency on user's sequential behaviors into the click prediction process
through the recurrent structure in RNN. Large scale evaluations on the
click-through logs from a commercial search engine demonstrate that our
approach can significantly improve the click prediction accuracy, compared to
sequence-independent approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5785</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5785</id><created>2014-04-23</created><authors><author><keyname>Dubois</keyname><forenames>Catherine</forenames><affiliation>ENSIIE - CEDRIC</affiliation></author><author><keyname>Giannakopoulou</keyname><forenames>Dimitra</forenames><affiliation>NASA - Ames</affiliation></author><author><keyname>M&#xe9;ry</keyname><forenames>Dominique</forenames><affiliation>Universit&#xe9; de Lorraine - LORIA</affiliation></author></authors><title>Proceedings 1st Workshop on Formal Integrated Development Environment</title><categories>cs.SE cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 149, 2014</journal-ref><doi>10.4204/EPTCS.149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of F-IDE 2014, the first international
workshop on Formal Integrated Development Environment, which was held as an
ETAPS 2014 satellite event, on April 6, 2014, in Grenoble (France). High levels
of safety, security and also privacy standards require the use of formal
methods to specify and develop compliant software (sub)systems. Any standard
comes with an assessment process, which requires a complete documentation of
the application in order to ease the justification of design choices and the
review of code and proofs. Thus tools are needed for handling specifications,
program constructs and verification artifacts. The aim of the F-IDE workshop is
to provide a forum for presenting and discussing research efforts as well as
experience returns on design, development and usage of formal IDE aiming at
making formal methods &quot;easier&quot; for both specialists and non-specialists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5813</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5813</id><created>2014-04-23</created><updated>2014-11-24</updated><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Topology of the immediate snapshot complexes</title><categories>cs.DC</categories><comments>final version as it appears in Topology and it Applications, Article
  number 5275. arXiv admin note: substantial text overlap with arXiv:1402.4707</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The immediate snapshot complexes were introduced as combinatorial models for
the protocol complexes in the context of theoretical distributed computing. In
the previous work we have developed a formal language of witness structures in
order to define and to analyze these complexes.
  In this paper, we study topology of immediate snapshot complexes. It was
known that these complexes are always pure and that they are pseudomanifolds.
Here we prove two further independent topological properties. First, we show
that immediate snapshot complexes are collapsible. Second, we show that these
complexes are homeomorphic to closed balls. Specifically, given any immediate
snapshot complex $P(\tr)$, we show that there exists a homeomorphism
$\varphi:\da^{|\supp\tr|-1}\ra P(\tr)$, such that $\varphi(\sigma)$ is a
subcomplex of $P(\tr)$, whenever $\sigma$ is a simplex in the simplicial
complex $\da^{|\supp\tr|-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5828</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5828</id><created>2014-04-23</created><updated>2014-10-13</updated><authors><author><keyname>Panagou</keyname><forenames>Dimitra</forenames></author></authors><title>Motion planning and Collision Avoidance using Non-Gradient Vector
  Fields. Technical Report</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel feedback method on the motion planning for
unicycle robots in environments with static obstacles, along with an extension
to the distributed planning and coordination in multi-robot systems. The method
employs a family of 2-dimensional analytic vector fields, whose integral curves
exhibit various patterns depending on the value of a parameter lambda. More
specifically, for an a priori known value of lambda, the vector field has a
unique singular point of dipole type and can be used to steer the unicycle to a
goal configuration. Furthermore, for the unique value of lambda that the vector
field has a continuum of singular points, the integral curves are used to
define flows around obstacles. An almost global feedback motion plan can then
be constructed by suitably blending attractive and repulsive vector fields in a
static obstacle environment. The method does not suffer from the appearance of
sinks (stable nodes) away from goal point. Compared to other similar methods
which are free of local minima, the proposed approach does not require any
parameter tuning to render the desired convergence properties. The paper also
addresses the extension of the method to the distributed coordination and
control of multiple robots, where each robot needs to navigate to a goal
configuration while avoiding collisions with the remaining robots, and while
using local information only. More specifically, based on the results which
apply to the single-robot case, a motion coordination protocol is presented
which guarantees the safety of the multi-robot system and the almost global
convergence of the robots to their goal configurations. The efficacy of the
proposed methodology is demonstrated via simulation results in static and
dynamic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5859</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5859</id><created>2014-04-23</created><updated>2015-01-06</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Holfeld</keyname><forenames>Bernd</forenames></author><author><keyname>Wirth</keyname><forenames>Thomas</forenames></author></authors><title>Distributed Channel Assignment in Cognitive Radio Networks: Stable
  Matching and Walrasian Equilibrium</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Wireless Communicaitons, 13 pages,
  10 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a set of secondary transmitter-receiver pairs in a cognitive
radio setting. Based on channel sensing and access performances, we consider
the problem of assigning channels orthogonally to secondary users through
distributed coordination and cooperation algorithms. Two economic models are
applied for this purpose: matching markets and competitive markets. In the
matching market model, secondary users and channels build two agent sets. We
implement a stable matching algorithm in which each secondary user, based on
his achievable rate, proposes to the coordinator to be matched with desirable
channels. The coordinator accepts or rejects the proposals based on the channel
preferences which depend on interference from the secondary user. The
coordination algorithm is of low complexity and can adapt to network dynamics.
In the competitive market model, channels are associated with prices and
secondary users are endowed with monetary budget. Each secondary user, based on
his utility function and current channel prices, demands a set of channels. A
Walrasian equilibrium maximizes the sum utility and equates the channel demand
to their supply. We prove the existence of Walrasian equilibrium and propose a
cooperative mechanism to reach it. The performance and complexity of the
proposed solutions are illustrated by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5869</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5869</id><created>2014-04-23</created><authors><author><keyname>Panda</keyname><forenames>Sanjaya Kumar</forenames></author><author><keyname>Bhoi</keyname><forenames>Sourav Kumar</forenames></author></authors><title>An Effective Round Robin Algorithm using Min-Max Dispersion Measure</title><categories>cs.OS</categories><comments>9 pages, 15 figures. International Journal on Computer Science and
  Engineering (IJCSE), 2012</comments><report-no>IJCSE12-04-01-124</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Round Robin (RR) scheduling algorithm is a preemptive scheduling algorithm.
It is designed especially for time sharing Operating System (OS). In RR
scheduling algorithm the CPU switches between the processes when the static
Time Quantum (TQ) expires. RR scheduling algorithm is considered as the most
widely used scheduling algorithm in research because the TQ is equally shared
among the processes. In this paper a newly proposed variant of RR algorithm
called Min-Max Round Robin (MMRR) scheduling algorithm is presented. The idea
of this MMRR is to make the TQ repeatedly adjusted using Min-Max dispersion
measure in accordance with remaining CPU burst time. Our experimental analysis
shows that MMRR performs much better than RR algorithm in terms of average
turnaround time, average waiting time and number of context switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5870</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5870</id><created>2014-04-22</created><authors><author><keyname>Syropoulos</keyname><forenames>Apostolos</forenames></author><author><keyname>Stavrianos</keyname><forenames>Athanasios</forenames></author></authors><title>Using Scripting Languages to Teach Programming</title><categories>cs.CY</categories><comments>13 pages</comments><msc-class>97Q60</msc-class><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, scripting programming languages like Python, Perl and Ruby are
widely used in system programming, scientific computing, etc. Although solving
a particular problem in these languages requires less time, less programming
effort, and less concepts to be taught to achieve the desired goal, still they
are not used as teaching tools. Therefore, the use of scripting languages as a
teaching vehicle for programming course is very promising. On the other hand,
GUI programming, when performed with such languages, is easy and rewarding,
since one sees the result of her work immediately. Thus, we are sure that
scripting languages combined with GUI toolkits will be the next big thing in
computer education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5874</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5874</id><created>2014-04-23</created><authors><author><keyname>Klymko</keyname><forenames>Christine</forenames></author><author><keyname>Gleich</keyname><forenames>David</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Using Triangles to Improve Community Detection in Directed Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a graph, a community may be loosely defined as a group of nodes that are
more closely connected to one another than to the rest of the graph. While
there are a variety of metrics that can be used to specify the quality of a
given community, one common theme is that flows tend to stay within
communities. Hence, we expect cycles to play an important role in community
detection. For undirected graphs, the importance of triangles -- an undirected
3-cycle -- has been known for a long time and can be used to improve community
detection. In directed graphs, the situation is more nuanced. The smallest
cycle is simply two nodes with a reciprocal connection, and using information
about reciprocation has proven to improve community detection. Our new idea is
based on the four types of directed triangles that contain cycles. To identify
communities in directed networks, then, we propose an undirected edge-weighting
scheme based on the type of the directed triangles in which edges are involved.
We also propose a new metric on quality of the communities that is based on the
number of 3-cycles that are split across communities. To demonstrate the impact
of our new weighting, we use the standard METIS graph partitioning tool to
determine communities and show experimentally that the resulting communities
result in fewer 3-cycles being cut. The magnitude of the effect varies between
a 10 and 50% reduction, and we also find evidence that this weighting scheme
improves a task where plausible ground-truth communities are known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5877</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5877</id><created>2014-04-23</created><updated>2015-09-01</updated><authors><author><keyname>Kalu&#x17e;a</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Density not realizable as the Jacobian determinant of a bilipschitz map</title><categories>math.MG cs.DM math.FA</categories><comments>11 pages, the proof section (section 3) was partly rewritten and
  clarified, 1 picture added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are every two separated nets in the plane bilipschitz equivalent? In the late
1990s, Burago and Kleiner and, independently, McMullen resolved this beautiful
question negatively. Both solutions are based on a construction of a density
function that is not realizable as the Jacobian determinant of a bilipschitz
map. McMullen's construction is simpler than the Burago-Kleiner one, and we
provide a full proof of its nonrealizability, which has not been available in
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5885</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5885</id><created>2014-04-22</created><authors><author><keyname>Rafique</keyname><forenames>Omar</forenames></author><author><keyname>L</keyname><forenames>Gangadharaiah S.</forenames></author></authors><title>Hardware Efficient WiMAX Deinterleaver Capable of Address Generation for
  Random Interleaving Depths</title><categories>cs.AR</categories><comments>This paper contains seven figures and four tables spreab over four
  pages</comments><journal-ref>IJETT, V10(4),187-190 April 2014.ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V10P235</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The variation in the prescribed modulation schemes and code rates for WiMAX
interleaver design, as defined by IEEE 802.16 standard, demands a plethora of
hardware if all the modulation schemes and code rates have to be unified into a
single electronic device. Add to this the complexities involved with the
algorithms and permutations of the WiMAX standard, invariably dependent on
floor function which is extremely hardware inefficient. This paper is an
attempt towards removing the complexities and excess hardware involvement in
the implementation of the permutations involved in Deinterleaver designs as
defined by IEEE 802.16
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5889</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5889</id><created>2014-04-23</created><updated>2015-12-18</updated><authors><author><keyname>Z&#xf6;rlein</keyname><forenames>Henning</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author></authors><title>Coherence Optimization and Best Complex Antipodal Spherical Codes</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 63, no. 24, pp. 6606
  - 6615, Dec. 2015</journal-ref><doi>10.1109/TSP.2015.2477052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector sets with optimal coherence according to the Welch bound cannot exist
for all pairs of dimension and cardinality. If such an optimal vector set
exists, it is an equiangular tight frame and represents the solution to a
Grassmannian line packing problem. Best Complex Antipodal Spherical Codes
(BCASCs) are the best vector sets with respect to the coherence. By extending
methods used to find best spherical codes in the real-valued Euclidean space,
the proposed approach aims to find BCASCs, and thereby, a complex-valued vector
set with minimal coherence. There are many applications demanding vector sets
with low coherence. Examples are not limited to several techniques in wireless
communication or to the field of compressed sensing. Within this contribution,
existing analytical and numerical approaches for coherence optimization of
complex-valued vector spaces are summarized and compared to the proposed
approach. The numerically obtained coherence values improve previously reported
results. The drawback of increased computational effort is addressed and a
faster approximation is proposed which may be an alternative for time critical
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5892</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5892</id><created>2014-04-23</created><updated>2014-04-24</updated><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author></authors><title>Straightening out planar poly-line drawings</title><categories>cs.CG cs.DM cs.DS</categories><comments>The main result turns out to be known (Pach &amp; Toth, J. Graph Theory
  2004, http://onlinelibrary.wiley.com/doi/10.1002/jgt.10168/pdf )</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any $y$-monotone poly-line drawing can be straightened out while
maintaining $y$-coordinates and height. The width may increase much, but we
also show that on some graphs exponential width is required if we do not want
to increase the height. Likewise $y$-monotonicity is required: there are
poly-line drawings (not $y$-monotone) that cannot be straightened out while
maintaining the height. We give some applications of our result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5894</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5894</id><created>2014-04-23</created><updated>2014-08-25</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Manasa</keyname><forenames>Lakshmi</forenames></author><author><keyname>Monmege</keyname><forenames>Benjamin</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Adding Negative Prices to Priced Timed Games</title><categories>cs.GT</categories><comments>Long version of a paper accepted for presentation at CONCUR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priced timed games (PTGs) are two-player zero-sum games played on the
infinite graph of configurations of priced timed automata where two players
take turns to choose transitions in order to optimize cost to reach target
states. Bouyer et al. and Alur, Bernadsky, and Madhusudan independently
proposed algorithms to solve PTGs with nonnegative prices under certain
divergence restriction over prices. Brihaye, Bruyere, and Raskin later provided
a justification for such a restriction by showing the undecidability of the
optimal strategy synthesis problem in the absence of this divergence
restriction. This problem for PTGs with one clock has long been conjectured to
be in polynomial time, however the current best known algorithm, by Hansen,
Ibsen-Jensen, and Miltersen, is exponential. We extend this picture by studying
PTGs with both negative and positive prices. We refine the undecidability
results for optimal strategy synthesis problem, and show undecidability for
several variants of optimal reachability cost objectives including reachability
cost, time-bounded reachability cost, and repeated reachability cost
objectives. We also identify a subclass with bi-valued price-rates and give a
pseudo-polynomial (polynomial when prices are nonnegative) algorithm to
partially answer the conjecture on the complexity of one-clock PTGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5899</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5899</id><created>2014-04-22</created><authors><author><keyname>Zhao</keyname><forenames>Ran</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Johansen</keyname><forenames>Christopher</forenames></author><author><keyname>Grenard</keyname><forenames>Jerry L.</forenames></author></authors><title>A Comparison of Clustering and Missing Data Methods for Health Sciences</title><categories>math.NA cs.LG</categories><msc-class>62H30, 91C20, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compare and analyze clustering methods with missing data in
health behavior research. In particular, we propose and analyze the use of
compressive sensing's matrix completion along with spectral clustering to
cluster health related data. The empirical tests and real data results show
that these methods can outperform standard methods like LPA and FIML, in terms
of lower misclassification rates in clustering and better matrix completion
performance in missing data problems. According to our examination, a possible
explanation of these improvements is that spectral clustering takes advantage
of high data dimension and compressive sensing methods utilize the
near-to-low-rank property of health data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5901</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5901</id><created>2014-04-23</created><authors><author><keyname>Hotz</keyname><forenames>Matthias</forenames></author><author><keyname>Vogel</keyname><forenames>Christian</forenames></author></authors><title>Linearization of Time-Varying Nonlinear Systems Using A Modified Linear
  Iterative Method</title><categories>cs.SY</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 62, no. 10, pp.
  2566-2579, May 2014</journal-ref><doi>10.1109/TSP.2014.2311965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linearization of nonlinear systems is an important digital enhancement
technique. In this paper, a real-time capable post- and pre-linearization
method for the widely applicable time-varying discrete-time Volterra series is
presented. To this end, an alternative view on the Volterra series is
established, which enables the utilization of certain modified linear iterative
methods for linearization. For one particular linear iterative method, the
Richardson iteration, the corresponding post- and pre-linearizers are discussed
in detail. It is motivated that the resulting algorithm can be regarded as a
generalization of some existing methods. Furthermore, a simply verifiable
condition for convergence is presented, which allows the straightforward
evaluation of applicability. The proposed method is demonstrated by means of
the linearization of a time-varying nonlinear amplifier, which highlights its
capability of linearizing significantly distorted signals, illustrates the
advantageous convergence behavior, and depicts its robustness against modeling
errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5903</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5903</id><created>2014-04-23</created><authors><author><keyname>Liu</keyname><forenames>Che-Yu</forenames></author><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Most Correlated Arms Identification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding the most mutually correlated arms among many
arms. We show that adaptive arms sampling strategies can have significant
advantages over the non-adaptive uniform sampling strategy. Our proposed
algorithms rely on a novel correlation estimator. The use of this accurate
estimator allows us to get improved results for a wide range of problem
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5905</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5905</id><created>2014-04-23</created><authors><author><keyname>Blackburn</keyname><forenames>Jeremy</forenames></author><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author></authors><title>STFU NOOB! Predicting Crowdsourced Decisions on Toxic Behavior in Online
  Games</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Proc. 23rd International World Wide Web Conference (WWW), 2014</comments><acm-class>K.4.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One problem facing players of competitive games is negative, or toxic,
behavior. League of Legends, the largest eSport game, uses a crowdsourcing
platform called the Tribunal to judge whether a reported toxic player should be
punished or not. The Tribunal is a two stage system requiring reports from
those players that directly observe toxic behavior, and human experts that
review aggregated reports. While this system has successfully dealt with the
vague nature of toxic behavior by majority rules based on many votes, it
naturally requires tremendous cost, time, and human efforts.
  In this paper, we propose a supervised learning approach for predicting
crowdsourced decisions on toxic behavior with large-scale labeled data
collections; over 10 million user reports involved in 1.46 million toxic
players and corresponding crowdsourced decisions. Our result shows good
performance in detecting overwhelmingly majority cases and predicting
crowdsourced decisions on them. We demonstrate good portability of our
classifier across regions. Finally, we estimate the practical implications of
our approach, potential cost savings and victim protection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5916</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5916</id><created>2014-04-23</created><authors><author><keyname>Heide</keyname><forenames>Felix</forenames></author><author><keyname>Gregson</keyname><forenames>James</forenames></author><author><keyname>Wetzstein</keyname><forenames>Gordon</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author><author><keyname>Heidrich</keyname><forenames>Wolfgang</forenames></author></authors><title>A Compressive Multi-Mode Superresolution Display</title><categories>cs.ET</categories><comments>Technical report</comments><acm-class>I.3.1</acm-class><doi>10.1364/OE.22.014981</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive displays are an emerging technology exploring the co-design of
new optical device configurations and compressive computation. Previously,
research has shown how to improve the dynamic range of displays and facilitate
high-quality light field or glasses-free 3D image synthesis. In this paper, we
introduce a new multi-mode compressive display architecture that supports
switching between 3D and high dynamic range (HDR) modes as well as a new
super-resolution mode. The proposed hardware consists of readily-available
components and is driven by a novel splitting algorithm that computes the pixel
states from a target high-resolution image. In effect, the display pixels
present a compressed representation of the target image that is perceived as a
single, high resolution image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5927</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5927</id><created>2014-04-23</created><updated>2014-11-05</updated><authors><author><keyname>Tsiligkaridis</keyname><forenames>Theodoros</forenames></author></authors><title>Secure MIMO Communications under Quantized Channel Feedback in the
  presence of Jamming</title><categories>cs.IT math.IT</categories><comments>11 pages, IEEE Transactions on Signal Processing</comments><journal-ref>Vol. 62, No. 23, December 1, 2014</journal-ref><doi>10.1109/TSP.2014.2362099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of secure communications in a MIMO setting in the
presence of an adversarial jammer equipped with $n_j$ transmit antennas and an
eavesdropper equipped with $n_e$ receive antennas. A multiantenna transmitter,
equipped with $n_t$ antennas, desires to secretly communicate a message to a
multiantenna receiver equipped with $n_r$ antennas. We propose a transmission
method based on artificial noise and linear precoding and a two-stage receiver
method employing beamforming. Under this strategy, we first characterize the
achievable secrecy rates of communication and prove that the achievable secure
degrees-of-freedom (SDoF) is given by $d_s = n_r - n_j$ in the perfect channel
state information (CSI) case. Second, we consider quantized CSI feedback using
Grassmannian quantization of a function of the direct channel matrix and derive
sufficient conditions for the quantization bit rate scaling as a function of
transmit power for maintaining the achievable SDoF $d_s$ with perfect CSI and
for having asymptotically zero secrecy rate loss due to quantization. Numerical
simulations are also provided to support the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5929</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5929</id><created>2014-04-23</created><authors><author><keyname>Franco</keyname><forenames>Maribell Sacanamboy</forenames></author><author><keyname>Guerrero</keyname><forenames>Fabio G.</forenames></author></authors><title>FPGA design of a cdma2000 turbo decoder</title><categories>cs.AR</categories><comments>In Spanish, 14 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the FPGA hardware design of a turbo decoder for the
cdma2000 standard. The work includes a study and mathematical analysis of the
turbo decoding process, based on the MAX-Log-MAP algorithm. Results of decoding
for a packet size of two hundred fifty bits are presented, as well as an
analysis of area versus performance, and the key variables for hardware design
in turbo decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5940</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5940</id><created>2014-04-23</created><authors><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author></authors><title>A strong converse for the quantum state merging protocol</title><categories>quant-ph cs.IT math.IT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Polyanskiy-Verd\'{u} paradigm provides an elegant way of using
generalized-divergences to obtain strong converses and thus far has remained
confined to protocols involving channels (classical or quantum). In this paper,
drawing inspirations from it, we provide strong converses for protocols
involving LOCC (local operations and classical communication). The key quantity
that we work with is the R\'{e}nyi relative entropy of entanglement. We provide
a strong converse for the quantum state merging protocol that gives an
exponential decay of the fidelity of the protocol for rates below the optimum
with the number of copies of the state and are provided both for entanglement
rate with LOCC as well as for classical communication with one-way LOCC. As an
aside, the developments also yield short strong converses for the
entanglement-concentration of pure states and the Schumacher compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5943</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5943</id><created>2014-04-23</created><updated>2014-06-09</updated><authors><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Hoy</keyname><forenames>Darrell</forenames></author><author><keyname>Taggart</keyname><forenames>Sam</forenames></author></authors><title>Price of Anarchy for Auction Revenue</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops tools for welfare and revenue analyses of Bayes-Nash
equilibria in asymmetric auctions with single-dimensional agents. We employ
these tools to derive price of anarchy results for social welfare and revenue.
Our approach separates the standard smoothness framework into two distinct
parts, isolating the analysis common to any auction from the analysis specific
to a given auction. The first part relates a bidder's contribution to welfare
in equilibrium to their contribution to welfare in the optimal auction using
the price the bidder faces for additional allocation. Intuitively, either an
agent's utility and hence contribution to welfare is high, or the price she has
to pay for additional allocation is high relative to her value. We call this
condition value covering; it holds in every Bayes-Nash equilibrium of any
auction. The second part, revenue covering, relates the prices bidders face for
additional allocation to the revenue of the auction, using an auction's rules
and feasibility constraints. Combining the two parts gives approximation
results to the optimal welfare, and, under the right conditions, the optimal
revenue. In mechanisms with reserve prices, our welfare results show
approximation with respect to the optimal mechanism with the same reserves.
  As a center-piece result, we analyze the single-item first-price auction with
individual monopoly reserves. When each distribution satisfies a regularity
condition the auction's revenue is at least a $2e/(e-1) \approx 3.16$
approximation to the revenue of the optimal auction. We also give bounds for
matroid auctions with first-price or all-pay semantics, and the generalized
first-price position auction. Finally, we give an extension theorem for
simultaneous composition, i.e., when multiple auctions are run simultaneously,
with single-valued, unit-demand agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5945</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5945</id><created>2014-04-23</created><authors><author><keyname>Shah</keyname><forenames>Shahid Mehraj</forenames></author><author><keyname>S</keyname><forenames>Parameswaran</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Previous Messages Provide the Key to Achieve Shannon Capacity in a
  Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>Accepted for IEEE International Conference on Communications Workshop
  (ICC) 2013, Budapest, Hungary. arXiv admin note: text overlap with
  arXiv:1404.5701</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wiretap channel and use previously transmitted messages to
generate a secret key which increases the secrecy capacity. This can be
bootstrapped to increase the secrecy capacity to the Shannon capacity without
using any feedback or extra channel while retaining the strong secrecy of the
wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5971</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5971</id><created>2014-04-23</created><updated>2014-06-09</updated><authors><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Nekipelov</keyname><forenames>Denis</forenames></author></authors><title>Mechanism Design for Data Science</title><categories>cs.GT</categories><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good economic mechanisms depend on the preferences of participants in the
mechanism. For example, the revenue-optimal auction for selling an item is
parameterized by a reserve price, and the appropriate reserve price depends on
how much the bidders are willing to pay. A mechanism designer can potentially
learn about the participants' preferences by observing historical data from the
mechanism; the designer could then update the mechanism in response to learned
preferences to improve its performance. The challenge of such an approach is
that the data corresponds to the actions of the participants and not their
preferences. Preferences can potentially be inferred from actions but the
degree of inference possible depends on the mechanism. In the optimal auction
example, it is impossible to learn anything about preferences of bidders who
are not willing to pay the reserve price. These bidders will not cast bids in
the auction and, from historical bid data, the auctioneer could never learn
that lowering the reserve price would give a higher revenue (even if it would).
To address this impossibility, the auctioneer could sacrifice revenue
optimality in the initial auction to obtain better inference properties so that
the auction's parameters can be adapted to changing preferences in the future.
This paper develops the theory for optimal mechanism design subject to good
inferability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5985</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5985</id><created>2014-04-23</created><updated>2015-03-11</updated><authors><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author></authors><title>Reflections on Tiles (in Self-Assembly)</title><categories>cs.CG cs.CC cs.ET</categories><comments>New results which classify the types of shapes which can
  self-assemble in the RTAM have been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the Reflexive Tile Assembly Model (RTAM), which is obtained from
the abstract Tile Assembly Model (aTAM) by allowing tiles to reflect across
their horizontal and/or vertical axes. We show that the class of directed
temperature-1 RTAM systems is not computationally universal, which is
conjectured but unproven for the aTAM, and like the aTAM, the RTAM is
computationally universal at temperature 2. We then show that at temperature 1,
when starting from a single tile seed, the RTAM is capable of assembling n x n
squares for n odd using only n tile types, but incapable of assembling n x n
squares for n even. Moreover, we show that n is a lower bound on the number of
tile types needed to assemble n x n squares for n odd in the temperature-1
RTAM. The conjectured lower bound for temperature-1 aTAM systems is 2n-1.
Finally, we give preliminary results toward the classification of which finite
connected shapes in Z^2 can be assembled (strictly or weakly) by a singly
seeded (i.e. seed of size 1) RTAM system, including a complete classification
of which finite connected shapes be strictly assembled by a &quot;mismatch-free&quot;
singly seeded RTAM system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5996</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5996</id><created>2014-04-23</created><authors><author><keyname>K&#xf6;hler</keyname><forenames>Ekkehard</forenames></author><author><keyname>Mouatadid</keyname><forenames>Lalla</forenames></author></authors><title>Linear Time LexDFS on Cocomparability Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexicographic depth first search (LexDFS) is a graph search protocol which
has already proved to be a powerful tool on cocomparability graphs.
Cocomparability graphs have been well studied by investigating their
complements (comparability graphs) and their corresponding posets. Recently
however LexDFS has led to a number of elegant polynomial and near linear time
algorithms on cocomparability graphs when used as a preprocessing step [2, 3,
11]. The nonlinear runtime of some of these results is a consequence of
complexity of this preprocessing step. We present the first linear time
algorithm to compute a LexDFS cocomparability ordering, therefore answering a
problem raised in [2] and helping achieve the first linear time algorithms for
the minimum path cover problem, and thus the Hamilton path problem, the maximum
independent set problem and the minimum clique cover for this graph family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.5997</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.5997</id><created>2014-04-23</created><updated>2014-04-26</updated><authors><author><keyname>Krizhevsky</keyname><forenames>Alex</forenames></author></authors><title>One weird trick for parallelizing convolutional neural networks</title><categories>cs.NE cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6000</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6000</id><created>2014-04-23</created><updated>2015-06-03</updated><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author></authors><title>Robust and computationally feasible community detection in the presence
  of arbitrary outlier nodes</title><categories>math.ST cs.IT math.IT math.OC stat.ML stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/14-AOS1290 in the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1290</report-no><journal-ref>Annals of Statistics 2015, Vol. 43, No. 3, 1027-1059</journal-ref><doi>10.1214/14-AOS1290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection, which aims to cluster $N$ nodes in a given graph into
$r$ distinct groups based on the observed undirected edges, is an important
problem in network data analysis. In this paper, the popular stochastic block
model (SBM) is extended to the generalized stochastic block model (GSBM) that
allows for adversarial outlier nodes, which are connected with the other nodes
in the graph in an arbitrary way. Under this model, we introduce a procedure
using convex optimization followed by $k$-means algorithm with $k=r$. Both
theoretical and numerical properties of the method are analyzed. A theoretical
guarantee is given for the procedure to accurately detect the communities with
small misclassification rate under the setting where the number of clusters can
grow with $N$. This theoretical result admits to the best-known result in the
literature of computationally feasible community detection in SBM without
outliers. Numerical results show that our method is both computationally fast
and robust to different kinds of outliers, while some popular computationally
fast community detection algorithms, such as spectral clustering applied to
adjacency matrices or graph Laplacians, may fail to retrieve the major clusters
due to a small portion of outliers. We apply a slight modification of our
method to a political blogs data set, showing that our method is competent in
practice and comparable to existing computationally feasible methods in the
literature. To the best of the authors' knowledge, our result is the first in
the literature in terms of clustering communities with fast growing numbers
under the GSBM where a portion of arbitrary outlier nodes exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6003</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6003</id><created>2014-04-23</created><authors><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author></authors><title>Buying Private Data without Verification</title><categories>cs.GT cs.CR cs.DS</categories><comments>Appears in EC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing a survey to aggregate non-verifiable
information from a privacy-sensitive population: an analyst wants to compute
some aggregate statistic from the private bits held by each member of a
population, but cannot verify the correctness of the bits reported by
participants in his survey. Individuals in the population are strategic agents
with a cost for privacy, \ie, they not only account for the payments they
expect to receive from the mechanism, but also their privacy costs from any
information revealed about them by the mechanism's outcome---the computed
statistic as well as the payments---to determine their utilities. How can the
analyst design payments to obtain an accurate estimate of the population
statistic when individuals strategically decide both whether to participate and
whether to truthfully report their sensitive information?
  We design a differentially private peer-prediction mechanism that supports
accurate estimation of the population statistic as a Bayes-Nash equilibrium in
settings where agents have explicit preferences for privacy. The mechanism
requires knowledge of the marginal prior distribution on bits $b_i$, but does
not need full knowledge of the marginal distribution on the costs $c_i$,
instead requiring only an approximate upper bound. Our mechanism guarantees
$\epsilon$-differential privacy to each agent $i$ against any adversary who can
observe the statistical estimate output by the mechanism, as well as the
payments made to the $n-1$ other agents $j\neq i$. Finally, we show that with
slightly more structured assumptions on the privacy cost functions of each
agent, the cost of running the survey goes to $0$ as the number of agents
diverges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6012</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6012</id><created>2014-04-23</created><updated>2014-10-25</updated><authors><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author></authors><title>Degrees of Freedom of Uplink-Downlink Multiantenna Cellular Networks</title><categories>cs.IT math.IT</categories><comments>22 pages, 11 figures, in revision for IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An uplink-downlink two-cell cellular network is studied in which the first
base station (BS) with $M_1$ antennas receives independent messages from its
$N_1$ serving users, while the second BS with $M_2$ antennas transmits
independent messages to its $N_2$ serving users. That is, the first and second
cells operate as uplink and downlink, respectively. Each user is assumed to
have a single antenna. Under this uplink-downlink setting, the sum degrees of
freedom (DoF) is completely characterized as the minimum of
$(N_1N_2+\min(M_1,N_1)(N_1-N_2)^++\min(M_2,N_2)(N_2-N_1)^+)/\max(N_1,N_2)$,
$M_1+N_2,M_2+N_1$, $\max(M_1,M_2)$, and $\max(N_1,N_2)$, where $a^+$ denotes
$\max(0,a)$. The result demonstrates that, for a broad class of network
configurations, operating one of the two cells as uplink and the other cell as
downlink can strictly improve the sum DoF compared to the conventional uplink
or downlink operation, in which both cells operate as either uplink or
downlink. The DoF gain from such uplink-downlink operation is further shown to
be achievable for heterogeneous cellular networks having hotspots and with
delayed channel state information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6013</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6013</id><created>2014-04-23</created><updated>2014-12-23</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>Service-Constraint Based Truthful Incentive Mechanisms for Crowd Sensing</title><categories>cs.NI cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.5677,
  arXiv:1404.2399 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd sensing is a new paradigm which leverages the pervasive smartphones to
efficiently collect and upload sensing data, enabling numerous novel
applications. To achieve good service quality for a crowd sensing application,
incentive mechanisms are necessary for attracting more user participation. Most
of existing mechanisms apply only for the budget-constraint scenario where the
platform (the crowd sensing organizer) has a budget limit. On the contrary, we
focus on a different scenario where the platform has a service limit. Based on
the offline and online auction model, we consider a general problem: users
submit their private profiles to the platform, and the platform aims at
selecting a subset of users before a specified deadline for minimizing the
total payment while a specific service can be completed. Specially, we design
offline and online service-constraint incentive mechanisms for the case where
the value function of selected users is monotone submodular. The mechanisms are
individual rationality, task feasibility, computational efficiency,
truthfulness, consumer sovereignty, constant frugality, and also performs well
in practice. Finally, we use extensive simulations to demonstrate the
theoretical properties of our mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6020</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6020</id><created>2014-04-23</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>A Fast Multiple Attractor Cellular Automata with Modified Clonal
  Classifier for Splicing Site Prediction in Human Genome</title><categories>cs.CE</categories><comments>Four Pages, Global Perspectives on Artificial Intelligence (GPAI)
  Volume 2, April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bioinformatics encompass storing, analyzing and interpreting the biological
data. Most of the challenges for Machine Learning methods like Cellular
Automata is to furnish the functional information with the corresponding
biological sequences. In eukaryotes DNA is divided into introns and exons. The
introns will be removed to make the coding region by a process called splicing.
By indentifying a splice site we can easily specify the DNA sequence category
(Donor/Accepter/Neither).Splicing sites play an important role in understanding
the genes. A class of CA which can handle fuzzy logic is employed with modified
clonal algorithm is proposed to identify the splicing site. This classifier is
tested with Irvine Primate Splice Junction Database. It is compared with
NNspIICE, GENIO, HSPL and SPIICE VIEW. The reported accuracy and efficiency of
prediction is quite promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6026</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6026</id><created>2014-04-24</created><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Sun</keyname><forenames>Tao</forenames></author><author><keyname>Cheng</keyname><forenames>Lizhi</forenames></author></authors><title>Proximal linearized iteratively reweighted least squares for a class of
  nonconvex and nonsmooth problems</title><categories>math.OC cs.IT math.IT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For solving a wide class of nonconvex and nonsmooth problems, we propose a
proximal linearized iteratively reweighted least squares (PL-IRLS) algorithm.
We ?rst approximate the original problem by smoothing methods, and second write
the approximated problem into an auxiliary problem by introducing new
variables. PL-IRLS is then built on solving the auxiliary problem by utilizing
the proximal linearization technique and the iteratively reweighted least
squares (IRLS) method, and has remarkable computation advantages. We show that
PL-IRLS can be extended to solve more general nonconvex and nonsmooth problems
via adjusting generalized parameters, and also to solve nonconvex and nonsmooth
problems with two or more blocks of variables. Theoretically, with the help of
the Kurdyka- Lojasiewicz property, we prove that each bounded sequence
generated by PL-IRLS globally converges to a critical point of the approximated
problem. To the best of our knowledge, this is the ?rst global convergence
result of applying IRLS idea to solve nonconvex and nonsmooth problems. At
last, we apply PL-IRLS to solve three representative nonconvex and nonsmooth
problems in sparse signal recovery and low-rank matrix recovery and obtain new
globally convergent algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6029</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6029</id><created>2014-04-24</created><authors><author><keyname>Moharana</keyname><forenames>B.</forenames></author><author><keyname>Gupta</keyname><forenames>Rakesh</forenames></author><author><keyname>Kushwaha</keyname><forenames>Bashishth Kumar</forenames></author></authors><title>Optimization and design of a laser-cutting machine using delta robot</title><categories>cs.RO</categories><comments>6 pages, 4 figures</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  - Volume 10 Number 4 - Apr 2014</journal-ref><doi>10.14445/22315381/IJETT-V10P233</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Industrial high speed laser operations the use of delta parallel robots
potentially offers many benefits due to their structural stiffness and limited
moving masses. This paper deals with a particular Delta, developed for high
speed laser cutting. Parallel delta robot has numerous advantages in comparison
with serial robots Higher stiffness and connected with that a lower mass of
links the possibility of transporting heavier loads, and higher accuracy. The
main drawback is however a smaller workspace. Hence there exists an interest
for the research concerning the workspace of robots.In industrial cutting tool
maximum do not have more prescribe measurement to cut so that in This paper is
oriented to parallel kinematic robots definition description of their specific
application of laser cutting comparison of robots made by different producers
and determination of velocity and acceleration parameters kinematic analysis
inverse and forward kinematic. It brings information about development of Delta
robot. The production of laser cutting machines began thirty years ago. The
progress was very fast and at present time every year over 3000 laser cutting
machines is installed in the world. Laser cutting is one of the largest
applications of lasers in metal working industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6031</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6031</id><created>2014-04-24</created><authors><author><keyname>Boddeti</keyname><forenames>Vishnu Naresh</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Maximum Margin Vector Correlation Filter</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation Filters (CFs) are a class of classifiers which are designed for
accurate pattern localization. Traditionally CFs have been used with scalar
features only, which limits their ability to be used with vector feature
representations like Gabor filter banks, SIFT, HOG, etc. In this paper we
present a new CF named Maximum Margin Vector Correlation Filter (MMVCF) which
extends the traditional CF designs to vector features. MMVCF further combines
the generalization capability of large margin based classifiers like Support
Vector Machines (SVMs) and the localization properties of CFs for better
robustness to outliers. We demonstrate the efficacy of MMVCF for object
detection and landmark localization on a variety of databases and demonstrate
that MMVCF consistently shows improved pattern localization capability in
comparison to SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6034</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6034</id><created>2014-04-24</created><authors><author><keyname>Gupta</keyname><forenames>Rakesh</forenames></author></authors><title>Design of a Low Voltage Class-AB CMOS Super Buffer Amplifier with Sub
  Threshold and Leakage Control</title><categories>cs.OH</categories><comments>5 pages, 6 figures, International Journal of Engineering Trends and
  Technology (IJETT)-Volume 7 Number 1 - Jan 2014</comments><doi>10.14445/22315381/IJETT-V7P219</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes a CMOS analogy voltage supper buffer designed to have
extremely low static current Consumption as well as high current drive
capability. A new technique is used to reduce the leakage power of class-AB
CMOS buffer circuits without affecting dynamic power dissipation. The name of
applied technique is TRANSISTOR GATING TECHNIQUE, which gives the high speed
buffer with the reduced low power dissipation (1.105%), low leakage and reduced
area (3.08%) also. The proposed buffer is simulated at 45nm CMOS technology and
the circuit is operated at 3.3V supply[11]. Consumption is comparable to the
switching component. Reports indicate that 40% or even higher percentage of the
total power consumption is due to the leakage of transistors. This percentage
will increase with technology scaling unless effective techniques are
introduced to bring leakage under control. This article focuses on circuit
optimization and Design automation techniques to accomplish this goal [9].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6036</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6036</id><created>2014-04-24</created><authors><author><keyname>Arisaka</keyname><forenames>Ryuta</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Gradual Classical Logic for Attributed Objects</title><categories>cs.AI cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is knowledge. There is belief. And there is tacit agreement.' 'We may
talk about objects. We may talk about attributes of the objects. Or we may talk
both about objects and their attributes.' This work inspects tacit agreements
on assumptions about the relation between objects and their attributes, and
studies a way of expressing them, presenting as the result what we term gradual
logic in which the sense of truth gradually shifts. It extends classical logic
instances with a new logical connective capturing the object-attribute
relation. A formal semantics is presented. Decidability is proved. Para-
consistent/epistemic/conditional/intensional/description/combined logics are
compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6037</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6037</id><created>2014-04-24</created><authors><author><keyname>Arisaka</keyname><forenames>Ryuta</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Structural Interactions and Absorption of Structural Rules in BI Sequent
  Calculus</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of a contraction-free BI sequent calculus, be it in the sense of
G3i or G4i, has not been successful in literature. We address the open problem
by presenting such a sequent system. In fact our calculus involves no
structural rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6039</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6039</id><created>2014-04-24</created><authors><author><keyname>Charlier</keyname><forenames>Benjamin</forenames><affiliation>UM2</affiliation></author><author><keyname>Charon</keyname><forenames>Nicolas</forenames><affiliation>DIKU, CMLA</affiliation></author><author><keyname>Trouv&#xe9;</keyname><forenames>Alain</forenames><affiliation>CMLA</affiliation></author></authors><title>The fshape framework for the variability analysis of functional shapes</title><categories>cs.CG cs.CV math.DG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a full mathematical and numerical framework for
treating functional shapes (or fshapes) following the landmarks of shape spaces
and shape analysis. Functional shapes can be described as signal functions
supported on varying geometrical supports. Analysing variability of fshapes'
ensembles require the modelling and quantification of joint variations in
geometry and signal, which have been treated separately in previous approaches.
Instead, building on the ideas of shape spaces for purely geometrical objects,
we propose the extended concept of fshape bundles and define Riemannian metrics
for fshape metamorphoses to model geometrico-functional transformations within
these bundles. We also generalize previous works on data attachment terms based
on the notion of varifolds and demonstrate the utility of these distances.
Based on these, we propose variational formulations of the atlas estimation
problem on populations of fshapes and prove existence of solutions for the
different models. The second part of the article examines the numerical
implementation of the models by detailing discrete expressions for the metrics
and gradients and proposing an optimization scheme for the atlas estimation
problem. We present a few results of the methodology on a synthetic dataset as
well as on a population of retinal membranes with thickness maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6040</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6040</id><created>2014-04-24</created><authors><author><keyname>Kumar</keyname><forenames>Rajiv</forenames></author><author><keyname>Gupta</keyname><forenames>Rakesh</forenames></author></authors><title>Design and considerations of ADC0808 as interleaved ADCs</title><categories>cs.OH</categories><comments>11 Pages, 13 figures, Internation Journal of Advanced Scientific and
  Technical Research Issue 4, Volume 1, January - February 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Here in this paper we are presenting a digital system background technique
for correcting the time offset error rate and gain mismatches in a time
interleaved analog to digital converter system for N channel communication
using 8 bit ADC0808 ICs. A time interleaved A to D converter system is an
effective way to implement a high sampling rate ADC with relatively slow
circuits. This paper analyses the benefits and derives an upper band on the
performance by considering kT/C noise and slewing requirement of the circuit
driving the system. In the system, several channel ADCs operate at interleaved
sampling times as if they were effectively a single ADC operating at a much
higher sampling rate. A timing mismatch calibration technique is proposed that
covers linear and non linear channel mismatches, unifies, and extends the
channel models. A novel foreground channel mismatch identification method has
been developed, which can be used to fully characterize dynamic linear
mismatches. A background identification method provides accurate timing
mismatch estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6041</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6041</id><created>2014-04-24</created><updated>2014-04-24</updated><authors><author><keyname>Kuo</keyname><forenames>Tung-Wei</forenames></author><author><keyname>Lee</keyname><forenames>Kuang-Che</forenames></author><author><keyname>Lin</keyname><forenames>Kate Ching-Ju</forenames></author><author><keyname>Tsai</keyname><forenames>Ming-Jer</forenames></author></authors><title>Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks</title><categories>cs.NI</categories><comments>Accepted on 12-Apr-2014 for publications at IEEE Transactions on
  Wireless Communications</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In multiuser MIMO (MU-MIMO) LANs, the achievable throughput of a client
depends on who are transmitting concurrently with it. Existing MU-MIMO MAC
protocols however enable clients to use the traditional 802.11 contention to
contend for concurrent transmission opportunities on the uplink. Such a
contention-based protocol not only wastes lots of channel time on multiple
rounds of contention, but also fails to maximally deliver the gain of MU-MIMO
because users randomly join concurrent transmissions without considering their
channel characteristics. To address such inefficiency, this paper introduces
MIMOMate, a leader-contention-based MU-MIMO MAC protocol that matches clients
as concurrent transmitters according to their channel characteristics to
maximally deliver the MU-MIMO gain, while ensuring all users to fairly share
concurrent transmission opportunities. Furthermore, MIMOMate elects the leader
of the matched users to contend for transmission opportunities using
traditional 802.11 CSMA/CA. It hence requires only a single contention overhead
for concurrent streams, and can be compatible with legacy 802.11 devices. A
prototype implementation in USRP-N200 shows that MIMOMate achieves an average
throughput gain of 1.42x and 1.52x over the traditional contention-based
protocol for 2-antenna and 3-antenna AP scenarios, respectively, and also
provides fairness for clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6044</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6044</id><created>2014-04-24</created><authors><author><keyname>Mishra</keyname><forenames>Shaunak</forenames></author><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Harnessing Bursty Interference in Multicarrier Systems with Feedback</title><categories>cs.IT math.IT</categories><comments>A shorter version of this work will appear in the proceedings of ISIT
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study parallel symmetric 2-user interference channels when the
interference is bursty and feedback is available from the respective receivers.
Presence of interference in each subcarrier is modeled as a memoryless
Bernoulli random state. The states across subcarriers are drawn from an
arbitrary joint distribution with the same marginal probability for each
subcarrier and instantiated i.i.d. over time. For the linear deterministic
setup, we give a complete characterization of the capacity region. For the
setup with Gaussian noise, we give outer bounds and a tight generalized degrees
of freedom characterization. We propose a novel helping mechanism which enables
subcarriers in very strong interference regime to help in recovering interfered
signals for subcarriers in strong and weak interference regimes. Depending on
the interference and burstiness regime, the inner bounds either employ the
proposed helping mechanism to code across subcarriers or treat the subcarriers
separately. The outer bounds demonstrate a connection to a subset entropy
inequality by Madiman and Tetali.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6048</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6048</id><created>2014-04-24</created><authors><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author></authors><title>List and Unique Error-Erasure Decoding of Interleaved Gabidulin Codes
  with Interpolation Techniques</title><categories>cs.IT math.IT</categories><comments>accepted for Designs, Codes and Cryptography; presented in part at
  WCC 2013, Bergen, Norway</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new interpolation-based decoding principle for interleaved Gabidulin codes
is presented. The approach consists of two steps: First, a multi-variate
linearized polynomial is constructed which interpolates the coefficients of the
received word and second, the roots of this polynomial have to be found. Due to
the specific structure of the interpolation polynomial, both steps
(interpolation and root-finding) can be accomplished by solving a linear system
of equations. This decoding principle can be applied as a list decoding
algorithm (where the list size is not necessarily bounded polynomially) as well
as an efficient probabilistic unique decoding algorithm. For the unique
decoder, we show a connection to known unique decoding approaches and give an
upper bound on the failure probability. Finally, we generalize our approach to
incorporate not only errors, but also row and column erasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6049</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6049</id><created>2014-04-24</created><authors><author><keyname>Zhao</keyname><forenames>Jin-Hua</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Statistical physics of hard combinatorial optimization: The vertex cover
  problem</title><categories>cond-mat.dis-nn cs.CC</categories><comments>17 pages. A mini-review to be published in Chinese Physics B</comments><journal-ref>Chinese Physics B 23: 078901 (2014)</journal-ref><doi>10.1088/1674-1056/23/7/078901</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical-case computation complexity is a research topic at the boundary of
computer science, applied mathematics, and statistical physics. In the last
twenty years the replica-symmetry-breaking mean field theory of spin glasses
and the associated message-passing algorithms have greatly deepened our
understanding of typical-case computation complexity. In this paper we use the
vertex cover problem, a basic nondeterministic-polynomial (NP)-complete
combinatorial optimization problem of wide application, as an example to
introduce the statistical physical methods and algorithms. We do not go into
the technical details but emphasize mainly the intuitive physical meanings of
the message-passing equations. A nonfamiliar reader shall be able to understand
to a large extent the physics behind the mean field approaches and to adjust
them in solving other optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6055</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6055</id><created>2014-04-24</created><updated>2014-05-13</updated><authors><author><keyname>Lu</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Ziqiang</forenames></author></authors><title>A General Homogeneous Matrix Formulation to 3D Rotation Geometric
  Transformations</title><categories>cs.CV</categories><comments>8 pages, 13 references, 1 table. arXiv admin note: substantial text
  overlap with arXiv:1307.0998</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algebraic projective geometry definitions of 3D rotations so as to
bridge a small gap between the applications and the definitions of 3D rotations
in homogeneous matrix form. A general homogeneous matrix formulation to 3D
rotation geometric transformations is proposed which suits for the cases when
the rotation axis is unnecessarily through the coordinate system origin given
their rotation axes and rotation angles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6059</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6059</id><created>2014-04-24</created><authors><author><keyname>Bora</keyname><forenames>Dibya Jyoti</forenames></author><author><keyname>Gupta</keyname><forenames>Dr. Anil Kumar</forenames></author></authors><title>A Comparative study Between Fuzzy Clustering Algorithm and Hard
  Clustering Algorithm</title><categories>cs.AI</categories><comments>Data Clustering,6 pages,6 figures,Published with International
  Journal of Computer Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V10(2):108-113, Apr 2014. ISSN:2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V10P119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data clustering is an important area of data mining. This is an unsupervised
study where data of similar types are put into one cluster while data of
another types are put into different cluster. Fuzzy C means is a very important
clustering technique based on fuzzy logic. Also we have some hard clustering
techniques available like K-means among the popular ones. In this paper a
comparative study is done between Fuzzy clustering algorithm and hard
clustering algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6064</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6064</id><created>2014-04-24</created><authors><author><keyname>Decker</keyname><forenames>Normann</forenames></author><author><keyname>Habermehl</keyname><forenames>Peter</forenames></author><author><keyname>Leucker</keyname><forenames>Martin</forenames></author><author><keyname>Thoma</keyname><forenames>Daniel</forenames></author></authors><title>Ordered Navigation on Multi-attributed Data Words</title><categories>cs.LO cs.FL</categories><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study temporal logics and automata on multi-attributed data words.
Recently, BD-LTL was introduced as a temporal logic on data words extending LTL
by navigation along positions of single data values. As allowing for navigation
wrt. tuples of data values renders the logic undecidable, we introduce ND-LTL,
an extension of BD-LTL by a restricted form of tuple-navigation. While complete
ND-LTL is still undecidable, the two natural fragments allowing for either
future or past navigation along data values are shown to be Ackermann-hard, yet
decidability is obtained by reduction to nested multi-counter systems. To this
end, we introduce and study nested variants of data automata as an intermediate
model simplifying the constructions. To complement these results we show that
imposing the same restrictions on BD-LTL yields two 2ExpSpace-complete
fragments while satisfiability for the full logic is known to be as hard as
reachability in Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6071</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6071</id><created>2014-04-24</created><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author></authors><title>Rough Clustering Based Unsupervised Image Change Detection</title><categories>cs.CV cs.AI</categories><comments>Proc. IEEE Conf. #30853, International Conference on Human Computer
  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013. (In Press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an unsupervised technique to detect the changed region
of multitemporal images on a same reference plane with the help of rough
clustering. The proposed technique is a soft-computing approach, based on the
concept of rough set with rough clustering and Pawlak's accuracy. It is less
noisy and avoids pre-deterministic knowledge about the distribution of the
changed and unchanged regions. To show the effectiveness, the proposed
technique is compared with some other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6074</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6074</id><created>2014-04-24</created><authors><author><keyname>Schrynemackers</keyname><forenames>Marie</forenames></author><author><keyname>Wehenkel</keyname><forenames>Louis</forenames></author><author><keyname>Babu</keyname><forenames>M. Madan</forenames></author><author><keyname>Geurts</keyname><forenames>Pierre</forenames></author></authors><title>Classifying pairs with trees for supervised biological network inference</title><categories>cs.LG stat.ML</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks are ubiquitous in biology and computational approaches have been
largely investigated for their inference. In particular, supervised machine
learning methods can be used to complete a partially known network by
integrating various measurements. Two main supervised frameworks have been
proposed: the local approach, which trains a separate model for each network
node, and the global approach, which trains a single model over pairs of nodes.
Here, we systematically investigate, theoretically and empirically, the
exploitation of tree-based ensemble methods in the context of these two
approaches for biological network inference. We first formalize the problem of
network inference as classification of pairs, unifying in the process
homogeneous and bipartite graphs and discussing two main sampling schemes. We
then present the global and the local approaches, extending the later for the
prediction of interactions between two unseen network nodes, and discuss their
specializations to tree-based ensemble methods, highlighting their
interpretability and drawing links with clustering techniques. Extensive
computational experiments are carried out with these methods on various
biological networks that clearly highlight that these methods are competitive
with existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6075</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6075</id><created>2014-04-24</created><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author></authors><title>Unsupervised Text Extraction from G-Maps</title><categories>cs.CV cs.AI</categories><comments>Proc. IEEE Conf. #30853, International Conference on Human Computer
  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper represents an text extraction method from Google maps, GIS
maps/images. Due to an unsupervised approach there is no requirement of any
prior knowledge or training set about the textual and non-textual parts. Fuzzy
CMeans clustering technique is used for image segmentation and Prewitt method
is used to detect the edges. Connected component analysis and gridding
technique enhance the correctness of the results. The proposed method reaches
98.5% accuracy level on the basis of experimental data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6087</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6087</id><created>2014-04-24</created><authors><author><keyname>Bhoi</keyname><forenames>Sourav Kumar</forenames></author><author><keyname>Panda</keyname><forenames>Sanjaya Kumar</forenames></author><author><keyname>Tarai</keyname><forenames>Debashee</forenames></author></authors><title>Enhancing CPU Performance using Subcontrary Mean Dynamic Round Robin
  (SMDRR) Scheduling Algorithm</title><categories>cs.OS</categories><comments>5 pages, 13 figures. Journal of Global Research in Computer Science
  2011. arXiv admin note: text overlap with arXiv:1103.3832 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Round Robin (RR) Algorithm is considered as optimal in time shared
environment because the static time is equally shared among the processes. If
the time quantum taken is static then it undergoes degradation of the CPU
performance and leads to so many context switches. In this paper, we have
proposed a new effective dynamic RR algorithm SMDRR (Subcontrary Mean Dynamic
Round Robin) based on dynamic time quantum where we use the subcontrary mean or
harmonic mean to find the time quantum. The idea of this approach is to make
the time quantum repeatedly adjusted according to the burst time of the
currently running processes. Our experimental analysis shows that SMDRR
performs better than RR algorithm in terms of reducing the number of context
switches, average turnaround time and average waiting time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6097</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6097</id><created>2014-04-24</created><authors><author><keyname>Righi</keyname><forenames>Simone</forenames></author><author><keyname>Tak&#xe1;cs</keyname><forenames>K&#xe1;roly</forenames></author></authors><title>Degree Variance and Emotional Strategies Catalyze Cooperation in Dynamic
  Signed Networks</title><categories>physics.soc-ph cs.SI</categories><comments>16 Pages, Proceeding of the European Conference on Modelling and
  Simumation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of the emergence of cooperation in dynamic signed
networks where agent strategies coevolve with relational signs and network
topology. Running simulations based on an agent-based model, we compare results
obtained in a regular lattice initialization with those obtained on a
comparable random network initialization. We show that the increased degree
heterogeneity at the outset enlarges the parametric conditions in which
cooperation survives in the long run. Furthermore, we show how the presence of
sign-dependent emotional strategies catalyze the evolution of cooperation with
both network topology initializations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6100</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6100</id><created>2014-04-24</created><updated>2014-10-13</updated><authors><author><keyname>Yi</keyname><forenames>Wentan</forenames></author><author><keyname>Chen</keyname><forenames>Shaozhen</forenames></author></authors><title>Multidimensional Zero-Correlation Linear Cryptanalysis of the Block
  Cipher KASUMI</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The block cipher KASUMI is widely used for security in many synchronous
wireless standards. It was proposed by ETSI SAGE for usage in 3GPP (3rd
Generation Partnership Project) ciphering algorthms in 2001. There are a great
deal of cryptanalytic results on KASUMI, however, its security evaluation
against the recent zero-correlation linear attacks is still lacking so far. In
this paper, we select some special input masks to refine the general 5-round
zero-correlation linear approximations combining with some observations on the
$FL$ functions and then propose the 6-round zero-correlation linear attack on
KASUMI. Moreover, zero-correlation linear attacks on the last 7-round KASUMI
are also introduced under some weak keys conditions. These weak keys take
$2^{-14}$ of the whole key space.
  The new zero-correlation linear attack on the 6-round needs about $2^{85}$
encryptions with $2^{62.8}$ known plaintexts. For the attack under weak keys
conditions on the last 7 round, the data complexity is about $2^{62.1}$ known
plaintexts and the time complexity $2^{110.5}$ encryptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6103</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6103</id><created>2014-04-24</created><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Romm</keyname><forenames>Assaf</forenames></author></authors><title>An Approximate &quot;Law of One Price&quot; in Random Assignment Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assignment games represent a tractable yet versatile model of two-sided
markets with transfers. We study the likely properties of the core of randomly
generated assignment games. If the joint productivities of every firm and
worker are i.i.d bounded random variables, then with high probability all
workers are paid roughly equal wages, and all firms make similar profits. This
implies that core allocations vary significantly in balanced markets, but that
there is core convergence in even slightly unbalanced markets. For the
benchmark case of uniform distribution, we provide a tight bound for the
workers' share of the surplus under the firm-optimal core allocation. We
present simulation results suggesting that the phenomena analyzed appear even
in medium-sized markets. Finally, we briefly discuss the effects of unbounded
distributions and the ways in which they may affect wage dispersion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6112</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6112</id><created>2014-04-24</created><authors><author><keyname>Ashour</keyname><forenames>Mahmoud</forenames></author><author><keyname>El-Sherif</keyname><forenames>Amr A.</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>Cognitive Radio Networks with Probabilistic Relaying: Stable Throughput
  and Delay Tradeoffs</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.1200</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study and analyze fundamental throughput and delay
tradeoffs in cooperative multiple access for cognitive radio systems. We focus
on the class of randomized cooperative policies, whereby the secondary user
(SU) serves either the queue of its own data or the queue of the primary user
(PU) relayed data with certain service probabilities. Moreover, admission
control is introduced at the relay queue, whereby a PU's packet is admitted to
the relay queue with an admission probability. The proposed policy introduces a
fundamental tradeoff between the delays of the PU and SU. Consequently, it
opens room for trading the PU delay for enhanced SU delay and vice versa. Thus,
the system could be tuned according to the demands of the intended application.
Towards this objective, stability conditions for the queues involved in the
system are derived. Furthermore, a moment generating function approach is
employed to derive closed-form expressions for the average delay encountered by
the packets of both users. The effect of varying the service and admission
probabilities on the system's throughput and delay is thoroughly investigated.
Results show that cooperation expands the stable throughput region. Moreover,
numerical simulation results assert the extreme accuracy of the analytically
derived delay expressions. In addition, we provide a criterion for the SU based
on which it decides whether cooperation is beneficial to the PU or not.
Furthermore, we show the impact of controlling the flow of data at the relay
queue using the admission probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6116</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6116</id><created>2014-04-24</created><authors><author><keyname>Chen</keyname><forenames>Xiaojun</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author></authors><title>Development of an open source software module for enhanced visualization
  during MR-guided interstitial gynecologic brachytherapy</title><categories>cs.SY cs.SE</categories><comments>9 pages, 6 figures</comments><journal-ref>Chen and Egger SpringerPlus 2014, 3:167</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2010, gynecologic malignancies were the 4th leading cause of death in U.S.
women and for patients with extensive primary or recurrent disease, treatment
with interstitial brachytherapy may be an option. However, brachytherapy
requires precise insertion of hollow catheters with introducers into the tumor
in order to eradicate the cancer. In this study, a software solution to assist
interstitial gynecologic brachytherapy has been investigated and the software
has been realized as an own module under (3D) Slicer, which is a free open
source software platform for (translational) biomedical research. The developed
research module allows on-time processing of intra-operative magnetic resonance
imaging (iMRI) data over a direct DICOM connection to a MR scanner. Afterwards
follows a multi-stage registration of CAD models of the medical brachytherapy
devices (template, obturator) to the patient's MR images, enabling the virtual
placement of interstitial needles to assist the physician during the
intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6150</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6150</id><created>2014-04-23</created><authors><author><keyname>Pierzchlewski</keyname><forenames>Jacek</forenames></author><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author></authors><title>Compressed Sensing Based Direct Conversion Receiver With Interference
  Reducing Sampling</title><categories>cs.IT math.IT</categories><comments>3 pages, 5 figures, submitted to IEEE International Conference On
  Sensing Communication and Networking 2014 (poster)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a direct conversion receiver applying compressed sensing
with the objective to relax the analog filtering requirements seen in the
traditional architecture. The analog filter is cumbersome in an \gls{IC} design
and relaxing its requirements is an advantage in terms of die area, performance
and robustness of the receiver. The objective is met by a selection of sampling
pattern matched to the prior knowledge of the frequency placement of the
desired and interfering signals. A simple numerical example demonstrates the
principle. The work is part of an ongoing research effort and the different
project phases are explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6151</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6151</id><created>2014-04-23</created><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Yang</keyname><forenames>Mingrui</forenames></author><author><keyname>Wark</keyname><forenames>Tim</forenames></author><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author></authors><title>SimpleTrack:Adaptive Trajectory Compression with Deterministic
  Projection Matrix for Mobile Sensor Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some mobile sensor network applications require the sensor nodes to transfer
their trajectories to a data sink. This paper proposes an adaptive trajectory
(lossy) compression algorithm based on compressive sensing. The algorithm has
two innovative elements. First, we propose a method to compute a deterministic
projection matrix from a learnt dictionary. Second, we propose a method for the
mobile nodes to adaptively predict the number of projections needed based on
the speed of the mobile nodes. Extensive evaluation of the proposed algorithm
using 6 datasets shows that our proposed algorithm can achieve sub-metre
accuracy. In addition, our method of computing projection matrices outperforms
two existing methods. Finally, comparison of our algorithm against a
state-of-the-art trajectory compression algorithm show that our algorithm can
reduce the error by 10-60 cm for the same compression ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6163</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6163</id><created>2014-04-24</created><updated>2014-04-27</updated><authors><author><keyname>Behmardi</keyname><forenames>Behrouz</forenames></author><author><keyname>Archambeau</keyname><forenames>Cedric</forenames></author><author><keyname>Bouchard</keyname><forenames>Guillaume</forenames></author></authors><title>Overlapping Trace Norms in Multi-View Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-view learning leverages correlations between different sources of data
to make predictions in one view based on observations in another view. A
popular approach is to assume that, both, the correlations between the views
and the view-specific covariances have a low-rank structure, leading to
inter-battery factor analysis, a model closely related to canonical correlation
analysis. We propose a convex relaxation of this model using structured norm
regularization. Further, we extend the convex formulation to a robust version
by adding an l1-penalized matrix to our estimator, similarly to convex robust
PCA. We develop and compare scalable algorithms for several convex multi-view
models. We show experimentally that the view-specific correlations are
improving data imputation performances, as well as labeling accuracy in
real-world multi-label prediction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6175</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6175</id><created>2014-04-24</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author></authors><title>Deepening the Relationship between SEFE and C-Planarity</title><categories>cs.CC cs.DS</categories><comments>8 pages, 3 figures, Extended version of 'SEFE = C-Planarity?' (9th
  International Colloquium on Graph Theory and Combinatorics - ICGT 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deepen the understanding of the connection between two
long-standing Graph Drawing open problems, that is, Simultaneous Embedding with
Fixed Edges (SEFE) and Clustered Planarity (C-PLANARITY). In his GD'12 paper
Marcus Schaefer presented a reduction from C-PLANARITY to SEFE of two planar
graphs (SEFE-2). We prove that a reduction exists also in the opposite
direction, if we consider instances of SEFE-2 in which the intersection graph
is connected. We pose as an open question whether the two problems are
polynomial-time equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6179</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6179</id><created>2014-04-24</created><authors><author><keyname>Schubotz</keyname><forenames>Moritz</forenames></author><author><keyname>Wicke</keyname><forenames>Gabriel</forenames></author></authors><title>Mathoid: Robust, Scalable, Fast and Accessible Math Rendering for
  Wikipedia</title><categories>cs.DL</categories><comments>12 pages, accepted at Conferences on Intelligent Computer Mathematics
  CICM2014</comments><doi>10.1007/978-3-319-08434-3_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia is the first address for scientists who want to recap basic
mathematical and physical laws and concepts. Today, formulae in those pages are
displayed as Portable Network Graphics images. Those images do not integrate
well into the text, can not be edited after copying, are inaccessible to screen
readers for people with special needs, do not support line breaks for small
screens and do not scale for high resolution devices. Mathoid improves this
situation and converts formulae specified by Wikipedia editors in a TeX-like
input format to MathML, with Scalable Vector Graphics images as a fallback
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6186</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6186</id><created>2014-04-24</created><authors><author><keyname>Obua</keyname><forenames>Steven</forenames></author><author><keyname>Fleuriot</keyname><forenames>Jacques</forenames></author><author><keyname>Scott</keyname><forenames>Phil</forenames></author><author><keyname>Aspinall</keyname><forenames>David</forenames></author></authors><title>ProofPeer: Collaborative Theorem Proving</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the concept of collaborative theorem proving and outline our plan
to make it a reality. We believe that a successful implementation of
collaborative theorem proving is a necessary prerequisite for the formal
verification of large systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6196</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6196</id><created>2014-04-24</created><updated>2014-06-20</updated><authors><author><keyname>Eguchi</keyname><forenames>Naohi</forenames></author></authors><title>Proving Termination of Unfolding Graph Rewriting for General Safe
  Recursion</title><categories>cs.LO cs.CC math.LO</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new termination proof and complexity analysis of
unfolding graph rewriting which is a specific kind of infinite graph rewriting
expressing the general form of safe recursion. We introduce a termination order
over sequences of terms together with an interpretation of term graphs into
sequences of terms. Unfolding graph rewrite rules expressing general safe
recursion can be successfully embedded into the termination order by the
interpretation, yielding the polynomial runtime complexity. Moreover,
generalising the definition of unfolding graph rewrite rules for general safe
recursion, we propose a new criterion for the polynomial runtime complexity of
infinite GRSs and for the polynomial size of normal forms in infinite GRSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6216</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6216</id><created>2014-04-24</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>CoRE Kernels</title><categories>stat.ML cs.DS cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term &quot;CoRE kernel&quot; stands for correlation-resemblance kernel. In many
applications (e.g., vision), the data are often high-dimensional, sparse, and
non-binary. We propose two types of (nonlinear) CoRE kernels for non-binary
sparse data and demonstrate the effectiveness of the new kernels through a
classification experiment. CoRE kernels are simple with no tuning parameters.
However, training nonlinear kernel SVM can be (very) costly in time and memory
and may not be suitable for truly large-scale industrial applications (e.g.
search). In order to make the proposed CoRE kernels more practical, we develop
basic probabilistic hashing algorithms which transform nonlinear kernels into
linear kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6218</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6218</id><created>2014-04-24</created><updated>2014-10-06</updated><authors><author><keyname>Tousimojarad</keyname><forenames>Ashkan</forenames></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>A Parallel Task-based Approach to Linear Algebra</title><categories>cs.DC cs.PF cs.PL</categories><comments>Final version as appeared in &quot;dx.doi.org/10.1109/ISPDC.2014.11&quot;</comments><journal-ref>Tousimojarad, A., Vanderbauwhede, W.: A parallel task-based
  approach to linear algebra. In: Parallel and Distributed Computing (ISPDC),
  2014 IEEE 13th International Symposium on. pp. 59-66. IEEE (2014)</journal-ref><doi>10.1109/ISPDC.2014.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processors with large numbers of cores are becoming commonplace. In order to
take advantage of the available resources in these systems, the programming
paradigm has to move towards increased parallelism. However, increasing the
level of concurrency in the program does not necessarily lead to better
performance. Parallel programming models have to provide flexible ways of
defining parallel tasks and at the same time, efficiently managing the created
tasks. OpenMP is a widely accepted programming model for shared-memory
architectures. In this paper we highlight some of the drawbacks in the OpenMP
tasking approach, and propose an alternative model based on the Glasgow
Parallel Reduction Machine (GPRM) programming framework. As the main focus of
this study, we deploy our model to solve a fundamental linear algebra problem,
LU factorisation of sparse matrices. We have used the SparseLU benchmark from
the BOTS benchmark suite, and compared the results obtained from our model to
those of the OpenMP tasking approach. The TILEPro64 system has been used to run
the experiments. The results are very promising, not only because of the
performance improvement for this particular problem, but also because they
verify the task management efficiency, stability, and flexibility of our model,
which can be applied to solve problems in future many-core systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6222</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6222</id><created>2014-04-24</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Pommereau</keyname><forenames>L&#xe9;onard</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Hachet</keyname><forenames>Martin</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>Assessing the Zone of Comfort in Stereoscopic Displays using EEG</title><categories>cs.HC</categories><proxy>ccsd</proxy><journal-ref>ACM SIGCHI Conference on Human Factors in Computing Systems (2014)</journal-ref><doi>10.1145/2559206.2581191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conflict between vergence (eye movement) and accommodation (crystalline
lens deformation) occurs in every stereoscopic display. It could cause
important stress outside the &quot;zone of comfort&quot;, when stereoscopic effect is too
strong. This conflict has already been studied using questionnaires, during
viewing sessions of several minutes. The present pilot study describes an
experimental protocol which compares two different comfort conditions using
electroencephalography (EEG) over short viewing sequences. Analyses showed
significant differences both in event-related potentials (ERP) and in frequency
bands power. An uncomfortable stereoscopy correlates with a weaker negative
component and a delayed positive component in ERP. It also induces a power
decrease in the alpha band and increases in theta and beta bands. With fast
responses to stimuli, EEG is likely to enable the conception of adaptive
systems, which could tune the stereoscopic experience according to each viewer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6228</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6228</id><created>2014-04-23</created><updated>2014-05-06</updated><authors><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Goossens</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Stainer</keyname><forenames>Am&#xe9;lie</forenames></author></authors><title>Synthesising Succinct Strategies in Safety Games</title><categories>cs.LO cs.GT</categories><comments>25 pags, 4 figures, 2 algorithms. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite turn-based safety games have been used for very different problems
such as the synthesis of linear temporal logic (LTL), the synthesis of
schedulers for computer systems running on multiprocessor platforms, and also
for the determinisation of timed automata. In these contexts, games are
implicitly defined, and their size is at least exponential in the size of the
input. Nevertheless, there are natural relations between states of arenas of
such games. We first formalise the properties that we expect on the relation
between states, thanks to the notion of alternating simulation. Then, we show
how such simulations can be exploited to (1) improve the running time of the
OTFUR algorithm to compute winning strategies and (2) obtain a succinct
representation of a winning strategy. We also show that our general theory
applies to the three applications mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6230</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6230</id><created>2014-04-24</created><updated>2014-06-09</updated><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Ensemble estimation of multivariate f-divergence</title><categories>cs.IT math.IT</categories><comments>14 pages, 6 figures, a condensed version of this paper was accepted
  to ISIT 2014, Version 2: Moved the proofs of the theorems from the main body
  to appendices at the end</comments><journal-ref>K.R. Moon and A.O. Hero III, &quot;Ensemble estimation of multivariate
  f-divergence,&quot; In Information Theory (ISIT), 2014 IEEE International
  Symposium on, pp. 356-360, 2014</journal-ref><doi>10.1109/ISIT.2014.6874854</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  f-divergence estimation is an important problem in the fields of information
theory, machine learning, and statistics. While several divergence estimators
exist, relatively few of their convergence rates are known. We derive the MSE
convergence rate for a density plug-in estimator of f-divergence. Then by
applying the theory of optimally weighted ensemble estimation, we derive a
divergence estimator with a convergence rate of O(1/T) that is simple to
implement and performs well in high dimensions. We validate our theoretical
results with experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6233</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6233</id><created>2014-04-24</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Towards Tight Bounds on Theta-Graphs</title><categories>cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1401.2127</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present improved upper and lower bounds on the spanning ratio of
$\theta$-graphs with at least six cones. Given a set of points in the plane, a
$\theta$-graph partitions the plane around each vertex into $m$ disjoint cones,
each having aperture $\theta=2\pi/m$, and adds an edge to the `closest' vertex
in each cone. We show that for any integer $k \geq 1$, $\theta$-graphs with
$4k+2$ cones have a spanning ratio of $1+2\sin(\theta/2)$ and we provide a
matching lower bound, showing that this spanning ratio tight.
  Next, we show that for any integer $k \geq 1$, $\theta$-graphs with $4k+4$
cones have spanning ratio at most
$1+2\sin(\theta/2)/(\cos(\theta/2)-\sin(\theta/2))$. We also show that
$\theta$-graphs with $4k+3$ and $4k+5$ cones have spanning ratio at most
$\cos(\theta/4)/(\cos(\theta/2)-\sin(3\theta/4))$. This is a significant
improvement on all families of $\theta$-graphs for which exact bounds are not
known. For example, the spanning ratio of the $\theta$-graph with 7 cones is
decreased from at most 7.5625 to at most 3.5132. These spanning proofs also
imply improved upper bounds on the competitiveness of the $\theta$-routing
algorithm. In particular, we show that the $\theta$-routing algorithm is
$(1+2\sin(\theta/2)/(\cos(\theta/2)-\sin(\theta/2)))$-competitive on
$\theta$-graphs with $4k+4$ cones and that this ratio is tight.
  Finally, we present improved lower bounds on the spanning ratio of these
graphs. Using these bounds, we provide a partial order on these families of
$\theta$-graphs. In particular, we show that $\theta$-graphs with $4k+4$ cones
have spanning ratio at least $1+2\tan(\theta/2)+2\tan^2(\theta/2)$. This is
somewhat surprising since, for equal values of $k$, the spanning ratio of
$\theta$-graphs with $4k+4$ cones is greater than that of $\theta$-graphs with
$4k+2$ cones, showing that increasing the number of cones can make the spanning
ratio worse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6245</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6245</id><created>2014-04-24</created><authors><author><keyname>Yamada</keyname><forenames>Akihisa</forenames></author><author><keyname>Kusakari</keyname><forenames>Keiichirou</forenames></author><author><keyname>Sakabe</keyname><forenames>Toshiki</forenames></author></authors><title>A Unified Ordering for Termination Proving</title><categories>cs.LO</categories><comments>38 pages, revised version submitted to SCP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a reduction order called the weighted path order (WPO) that
subsumes many existing reduction orders. WPO compares weights of terms as in
the Knuth-Bendix order (KBO), while WPO allows weights to be computed by a wide
class of interpretations. We investigate summations, polynomials and maximums
for such interpretations. We show that KBO is a restricted case of WPO induced
by summations, the polynomial order (POLO) is subsumed by WPO induced by
polynomials, and the lexicographic path order (LPO) is a restricted case of WPO
induced by maximums. By combining these interpretations, we obtain an instance
of WPO that unifies KBO, LPO and POLO. In order to fit WPO in the modern
dependency pair framework, we further provide a reduction pair based on WPO and
partial statuses. As a reduction pair, WPO also subsumes matrix
interpretations. We finally present SMT encodings of our techniques, and
demonstrate the significance of our work through experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6247</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6247</id><created>2014-04-24</created><authors><author><keyname>Deville</keyname><forenames>Pierre</forenames></author><author><keyname>Wang</keyname><forenames>Dashun</forenames></author><author><keyname>Sinatra</keyname><forenames>Roberta</forenames></author><author><keyname>Song</keyname><forenames>Chaoming</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author><author><keyname>Barabasi</keyname><forenames>Albert-Laszlo</forenames></author></authors><title>Career on the Move: Geography, Stratification, and Scientific Impact</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>Deville, P.et al. Career on the Move: Geography, Stratification,
  and Scientific Impact. Sci. Rep. 4, 4770; DOI:10.1038/srep04770 (2014)</journal-ref><doi>10.1038/srep04770</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Changing institutions is an integral part of an academic life. Yet little is
known about the mobility patterns of scientists at an institutional level and
how these career choices affect scientific outcomes. Here, we examine over
420,000 papers, to track the affiliation information of individual scientists,
allowing us to reconstruct their career trajectories over decades. We find that
career movements are not only temporally and spatially localized, but also
characterized by a high degree of stratification in institutional ranking. When
cross-group movement occurs, we find that while going from elite to lower-rank
institutions on average associates with modest decrease in scientific
performance, transitioning into elite institutions does not result in
subsequent performance gain. These results offer empirical evidence on
institutional level career choices and movements and have potential
implications for science policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6265</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6265</id><created>2014-04-24</created><authors><author><keyname>Guseva</keyname><forenames>E. V.</forenames></author><author><keyname>Mosiychuk</keyname><forenames>V. S.</forenames></author><author><keyname>Timoshenko</keyname><forenames>G. V.</forenames></author><author><keyname>Sharpan</keyname><forenames>O. B.</forenames></author></authors><title>Determination of the functional state of the fruits by parameters of the
  electric impedance</title><categories>cs.SY</categories><comments>In Ukrainian. Available at:
  http://radap.kpi.ua/index.php/radiotechnique/article/view/754. Visn. NTUU
  KPI, Ser. Radioteh. radioaparatobuduv. 56 (2014) 102-111</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Introduction. To assess the freshness of various products are often used
measuring impedance module. But due to the structure of plant foods diagnostic
value should have exactly a complex component of impedance. Article tasked with
developing criteria for assessing the functional state of the products subject
to a comprehensive component of the impedance. Research methodology. To
determine the functional status of the fruit were measured module and phase of
impedance at the three frequencies of 20, 100 and 500 kHz. Criteria for
recognition of functional status determined by the dynamics of changes in the
parameters of the complex impedance due to destructive processes caused by
dehydration and putrefaction processes. Data processing and analysis. On the
basis of experimental data obtained at three frequencies modeled frequency and
phase response and their changes during losing of freshness and appearance of
destructive processes. Discussion and conclusions. In fresh and stale fruit
modulus and phase of the impedance at low and high frequencies have
characteristic differences. But this is especially evident on the
phase-frequency characteristic, which can be seen that the value of the phase
with the loss of freshness at low frequency decreases and increases at high
more than twice during one week. Therefore, to assess the functional state of
fresh and stale products we suggest use phase portraits of phase response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6272</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6272</id><created>2014-04-24</created><authors><author><keyname>Wang</keyname><forenames>Zhaowen</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Brandt</keyname><forenames>Jonathan</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Huang</keyname><forenames>Thomas</forenames></author></authors><title>Scalable Similarity Learning using Large Margin Neighborhood Embedding</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying large-scale image data into object categories is an important
problem that has received increasing research attention. Given the huge amount
of data, non-parametric approaches such as nearest neighbor classifiers have
shown promising results, especially when they are underpinned by a learned
distance or similarity measurement. Although metric learning has been well
studied in the past decades, most existing algorithms are impractical to handle
large-scale data sets. In this paper, we present an image similarity learning
method that can scale well in both the number of images and the dimensionality
of image descriptors. To this end, similarity comparison is restricted to each
sample's local neighbors and a discriminative similarity measure is induced
from large margin neighborhood embedding. We also exploit the ensemble of
projections so that high-dimensional features can be processed in a set of
lower-dimensional subspaces in parallel without much performance compromise.
The similarity function is learned online using a stochastic gradient descent
algorithm in which the triplet sampling strategy is customized for quick
convergence of classification performance. The effectiveness of our proposed
model is validated on several data sets with scales varying from tens of
thousands to one million images. Recognition accuracies competitive with the
state-of-the-art performance are achieved with much higher efficiency and
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6277</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6277</id><created>2014-04-24</created><authors><author><keyname>Heunen</keyname><forenames>Chris</forenames></author></authors><title>Piecewise Boolean algebras and their domains</title><categories>cs.LO math.LO</categories><comments>11 pages</comments><msc-class>06E25, 06A11, 18F20</msc-class><acm-class>F.3.0</acm-class><journal-ref>Proceedings of the 41st International Colloquium on Automata,
  Languages and Programming, Springer Lecture Notes in Computer Science
  8573:208-219, 2014</journal-ref><doi>10.1007/978-3-662-43951-7_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterise piecewise Boolean domains, that is, those domains that arise
as Boolean subalgebras of a piecewise Boolean algebra. This leads to equivalent
descriptions of the category of piecewise Boolean algebras: either as piecewise
Boolean domains equipped with an orientation, or as full structure sheaves on
piecewise Boolean domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6281</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6281</id><created>2014-04-24</created><updated>2014-05-18</updated><authors><author><keyname>Mart&#xed;nez</keyname><forenames>F. E. Brochero</forenames></author><author><keyname>Vergara</keyname><forenames>C. R. Giraldo</forenames></author><author><keyname>de Oliveira</keyname><forenames>L. Batista</forenames></author></authors><title>Explicit factorization of $x^n-1\in \mathbb F_q[x]$</title><categories>cs.IT math.IT math.NT</categories><comments>Submitted to Designs, Codes and Cryptography. 9 pages</comments><msc-class>12E05(primary) and 94B05(secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb F_q$ be a finite field and $n$ a positive integer. In this
article, we prove that, under some conditions on $q$ and $n$, the polynomial
$x^n-1$ can be split into irreducible binomials $x^t-a$ and an explicit
factorization into irreducible factors is given.
  Finally, weakening one of our hypothesis, we also obtain factors of the form
$x^{2t}-ax^t+b$ and explicit splitting of $x^n-1$ into irreducible factors is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6287</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6287</id><created>2014-04-24</created><authors><author><keyname>Yousefi</keyname><forenames>Arman</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author></authors><title>Improved Approximation Algorithms for Earth-Mover Distance in Data
  Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For two multisets $S$ and $T$ of points in $[\Delta]^2$, such that $|S| =
|T|= n$, the earth-mover distance (EMD) between $S$ and $T$ is the minimum cost
of a perfect bipartite matching with edges between points in $S$ and $T$, i.e.,
$EMD(S,T) = \min_{\pi:S\rightarrow T}\sum_{a\in S}||a-\pi(a)||_1$, where $\pi$
ranges over all one-to-one mappings. The sketching complexity of approximating
earth-mover distance in the two-dimensional grid is mentioned as one of the
open problems in the literature. We give two algorithms for computing EMD
between two multi-sets when the number of distinct points in one set is a small
value $k=\log^{O(1)}(\Delta n)$. Our first algorithm gives a
$(1+\epsilon)$-approximation using $O(k\epsilon^{-2}\log^{4}n)$ space and works
only in the insertion-only model. The second algorithm gives a
$O(\min(k^3,\log\Delta))$-approximation using
$O(\log^{3}\Delta\cdot\log\log\Delta\cdot\log n)$-space in the turnstile model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6288</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6288</id><created>2014-04-24</created><authors><author><keyname>Quaddoura</keyname><forenames>Ruzayn</forenames></author></authors><title>An O(n) Time Algorithm For Maximum Induced Matching In Bipartite
  Star_123-free Graphs</title><categories>cs.DM</categories><comments>This paper is published in World of Computer Science and Information
  Technology Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A matching in a graph is a set of edges no two of which share a common
vertex. A matching M is an induced matching if no edge connects two edges of M.
The problem of finding a maximum induced matching is known to be NP-hard in
general and specifically for bipartite graphs. Lozin has been proposed an
O(n^3) time algorithm for this problem on the class of bipartite
Star_123,Sun_4-free graphs. In this paper we improve and generalize this result
in presenting a simple O(n) time algorithm for maximum induced matching problem
in bipartite Star_123-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6293</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6293</id><created>2014-04-24</created><updated>2015-01-29</updated><authors><author><keyname>Patney</keyname><forenames>Anjul</forenames></author><author><keyname>Tzeng</keyname><forenames>Stanley</forenames></author><author><keyname>Seitz</keyname><forenames>Kerry A.</forenames><suffix>Jr.</suffix></author><author><keyname>Owens</keyname><forenames>John D.</forenames></author></authors><title>Piko: A Design Framework for Programmable Graphics Pipelines</title><categories>cs.GR</categories><comments>13 pages, updated for 2015</comments><acm-class>I.3.1; I.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Piko, a framework for designing, optimizing, and retargeting
implementations of graphics pipelines on multiple architectures. Piko
programmers express a graphics pipeline by organizing the computation within
each stage into spatial bins and specifying a scheduling preference for these
bins. Our compiler, Pikoc, compiles this input into an optimized implementation
targeted to a massively-parallel GPU or a multicore CPU.
  Piko manages work granularity in a programmable and flexible manner, allowing
programmers to build load-balanced parallel pipeline implementations, to
exploit spatial and producer-consumer locality in a pipeline implementation,
and to explore tradeoffs between these considerations. We demonstrate that Piko
can implement a wide range of pipelines, including rasterization, Reyes, ray
tracing, rasterization/ray tracing hybrid, and deferred rendering. Piko allows
us to implement efficient graphics pipelines with relative ease and to quickly
explore design alternatives by modifying the spatial binning configurations and
scheduling preferences for individual stages, all while delivering real-time
performance that is within a factor six of state-of-the-art rendering systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6304</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6304</id><created>2014-04-24</created><authors><author><keyname>Neeman</keyname><forenames>Joe</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author></authors><title>Non-Reconstructability in the Stochastic Block Model</title><categories>math.PR cs.SI</categories><comments>26 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of clustering (or reconstruction) in the stochastic
block model, in the regime where the average degree is constant. For the case
of two clusters with equal sizes, recent results by Mossel, Neeman and Sly, and
by Massoulie, show that reconstructability undergoes a phase transition at the
Kesten-Stigum bound of $\lambda_2^2 d = 1$, where $\lambda_2$ is the second
largest eigenvalue of a related stochastic matrix and $d$ is the average
degree. In this paper, we address the general case of more than two clusters
and/or unbalanced cluster sizes. Our main result is a sufficient condition for
clustering to be impossible, which matches the existing result for two clusters
of equal sizes. A key ingredient in our result is a new connection between
non-reconstructability and non-distinguishability of the block model from an
Erd\H{o}s-R\'enyi model with the same average degree. We also show that it is
some times possible to reconstruct even when $\lambda_2^2 d &lt; 1$. Our results
provide evidence supporting a series of conjectures made by Decelle, Krzkala,
Moore and Zdeborov\'a regarding reconstructability and distinguishability of
stochastic block models (but do not settle them).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6312</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6312</id><created>2014-04-25</created><updated>2014-05-28</updated><authors><author><keyname>Berzak</keyname><forenames>Yevgeni</forenames></author><author><keyname>Reichart</keyname><forenames>Roi</forenames></author><author><keyname>Katz</keyname><forenames>Boris</forenames></author></authors><title>Reconstructing Native Language Typology from Foreign Language Usage</title><categories>cs.CL</categories><comments>CoNLL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linguists and psychologists have long been studying cross-linguistic
transfer, the influence of native language properties on linguistic performance
in a foreign language. In this work we provide empirical evidence for this
process in the form of a strong correlation between language similarities
derived from structural features in English as Second Language (ESL) texts and
equivalent similarities obtained from the typological features of the native
languages. We leverage this finding to recover native language typological
similarity structure directly from ESL text, and perform prediction of
typological features in an unsupervised fashion with respect to the target
languages. Our method achieves 72.2% accuracy on the typology prediction task,
a result that is highly competitive with equivalent methods that rely on
typological resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6320</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6320</id><created>2014-04-25</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Demystifying the Scaling Laws of Dense Wireless Networks: No Linear
  Scaling in Practice</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, ISIT 2014. arXiv admin note: substantial text
  overlap with arXiv:1402.1815</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We optimize the hierarchical cooperation protocol of Ozgur, Leveque and Tse,
which is supposed to yield almost linear scaling of the capacity of a dense
wireless network with the number of users $n$. Exploiting recent results on the
optimality of &quot;treating interference as noise&quot; in Gaussian interference
channels, we are able to optimize the achievable average per-link rate and not
just its scaling law. Our optimized hierarchical cooperation protocol
significantly outperforms the originally proposed scheme. On the negative side,
we show that even for very large $n$, the rate scaling is far from linear, and
the optimal number of stages $t$ is less than 4, instead of $t \rightarrow
\infty$ as required for almost linear scaling. Combining our results and the
fact that, beyond a certain user density, the network capacity is fundamentally
limited by Maxwell laws, as shown by Francheschetti, Migliore and Minero, we
argue that there is indeed no intermediate regime of linear scaling for dense
networks in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6325</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6325</id><created>2014-04-25</created><updated>2014-07-03</updated><authors><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Schramm</keyname><forenames>Tselil</forenames></author></authors><title>Global and Local Information in Clustering Labeled Block Models</title><categories>math.PR cs.SI</categories><comments>24 pages, 2 figures. A short abstract describing these results will
  appear in proceedings of RANDOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model is a classical cluster-exhibiting random graph
model that has been widely studied in statistics, physics and computer science.
In its simplest form, the model is a random graph with two equal-sized
clusters, with intra-cluster edge probability p, and inter-cluster edge
probability q. We focus on the sparse case, i.e., p, q = O(1/n), which is
practically more relevant and also mathematically more challenging. A
conjecture of Decelle, Krzakala, Moore and Zdeborova, based on ideas from
statistical physics, predicted a specific threshold for clustering. The
negative direction of the conjecture was proved by Mossel, Neeman and Sly
(2012), and more recently the positive direction was proven independently by
Massoulie and Mossel, Neeman, and Sly.
  In many real network clustering problems, nodes contain information as well.
We study the interplay between node and network information in clustering by
studying a labeled block model, where in addition to the edge information, the
true cluster labels of a small fraction of the nodes are revealed. In the case
of two clusters, we show that below the threshold, a small amount of node
information does not affect recovery. On the other hand, we show that for any
small amount of information efficient local clustering is achievable as long as
the number of clusters is sufficiently large (as a function of the amount of
revealed information).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6331</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6331</id><created>2014-04-25</created><authors><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panagiotis</forenames></author></authors><title>Active Adversaries from an Information-Theoretic Perspective: Data
  Modification Attacks</title><categories>cs.IT cs.CR math.IT</categories><comments>A shorter version of this paper was accepted to the IEEE
  International Symposium on Information Theory (ISIT 2014), Honolulu, HI, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of reliable communication in the presence of
active adversaries that can tamper with the transmitted data. We consider a
legitimate transmitter-receiver pair connected over multiple communication
paths (routes). We propose two new models of adversary, a &quot;memoryless&quot; and a
&quot;foreseer&quot; adversary. For both models, the adversaries are placing themselves
arbitrarily on the routes, keeping their placement fixed throughout the
transmission block. This placement may or may not be known to the transmitter.
The adversaries can choose their best modification strategy to increase the
error at the legitimate receiver, subject to a maximum distortion constraint.
We investigate the communication rates that can be achieved in the presence of
the two types of adversaries and the channel (benign) stochastic behavior. For
memoryless adversaries, the capacity is derived. Our method is to use the
typical set of the anticipated received signal for all possible adversarial
strategies (including their best one) in a compound channel that also captures
adversarial placement. For the foreseer adversaries, which have enhanced
observation capabilities compared to the memoryless ones, we propose a new
coding scheme to guarantee resilience, i.e., recovery of the codeword
independently of the adversarial (best) choice. We derive an achievable rate
and we propose an upper bound on the capacity. We evaluate our general results
for specific cases (e.g., binary symbol replacement or erasing attacks), to
gain insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6334</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6334</id><created>2014-04-25</created><updated>2015-08-08</updated><authors><author><keyname>Mayer</keyname><forenames>Norbert Michael</forenames></author></authors><title>Input anticipating critical reservoirs show power law forgetting of
  unexpected input events</title><categories>cs.NE</categories><journal-ref>Neural Computation May 2015, Vol. 27, No. 5: 1102-1119</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually, reservoir computing shows an exponential memory decay. This paper
investigates under which circumstances echo state networks can show a power law
forgetting. That means traces of earlier events can be found in the reservoir
for very long time spans. Such a setting requires critical connectivity exactly
at the limit of what is permissible according the echo state condition.
However, for general matrices the limit cannot be determined exactly from
theory. In addition, the behavior of the network is strongly influenced by the
input flow. Results are presented that use certain types of restricted
recurrent connectivity and anticipation learning with regard to the input,
where indeed power law forgetting can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6348</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6348</id><created>2014-04-25</created><authors><author><keyname>Wagdy</keyname><forenames>Ahmed</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author></authors><title>A DoF-Optimal Scheme for the two-user X-channel with Synergistic
  Alternating CSIT</title><categories>cs.IT math.IT</categories><comments>accepted at 2014 IEEE International Symposium on Information Theory
  for presentation at the symposium and for publication in the proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the degrees of freedom (DoF) of the two-user single input
single output (SISO) X-channel are investigated. Three cases are considered for
the availability of channel state information at the transmitters (CSIT);
perfect, delayed, and no-CSIT. A new achievable scheme is proposed to elucidate
the potency of interference creation-resurrection (IRC) when the available CSIT
alternates between these three cases. For some patterns of alternating CSIT,
the proposed scheme achieves $4/3$ DoF, and hence, coincides with the
information theoretic upper bound on the DoF of the two-user X-channel with
perfect and instantaneous CSIT. The CSIT alternation patterns are investigated
where the patterns that provide extraordinary synergistic gain and dissociative
ones are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6351</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6351</id><created>2014-04-25</created><authors><author><keyname>Ganster</keyname><forenames>Harald</forenames></author><author><keyname>Uray</keyname><forenames>Martina</forenames></author><author><keyname>Steginska</keyname><forenames>Sylwia</forenames></author><author><keyname>Croonen</keyname><forenames>Gerardus</forenames></author><author><keyname>Kaltenb&#xf6;ck</keyname><forenames>Rudolf</forenames></author><author><keyname>Hennermann</keyname><forenames>Karin</forenames></author></authors><title>Improving weather radar by fusion and classification</title><categories>cs.CV</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In air traffic management (ATM) all necessary operations (tactical planing,
sector configuration, required staffing, runway configuration, routing of
approaching aircrafts) rely on accurate measurements and predictions of the
current weather situation. An essential basis of information is delivered by
weather radar images (WXR), which, unfortunately, exhibit a vast amount of
disturbances. Thus, the improvement of these datasets is the key factor for
more accurate predictions of weather phenomena and weather conditions. Image
processing methods based on texture analysis and geometric operators allow to
identify regions including artefacts as well as zones of missing information.
Correction of these zones is implemented by exploiting multi-spectral satellite
data (Meteosat Second Generation). Results prove that the proposed system for
artefact detection and data correction significantly improves the quality of
WXR data and, thus, enables more reliable weather now- and forecast leading to
increased ATM safety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6360</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6360</id><created>2014-04-25</created><updated>2014-08-02</updated><authors><author><keyname>Iwamasa</keyname><forenames>Yuni</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Networks maximizing the consensus time of voter models</title><categories>cond-mat.dis-nn cs.SI physics.soc-ph</categories><comments>9 figures, 2 tables</comments><journal-ref>Physical Review E, 90, 012816 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the networks that yield the largest mean consensus time of voter
models under different update rules. By analytical and numerical means, we show
that the so-called lollipop graph, barbell graph, and double star graph
maximise the mean consensus time under the update rules called the link
dynamics, voter model, and invasion process, respectively. For each update
rule, the largest mean consensus time scales as O(N^3), where N is the number
of nodes in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6368</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6368</id><created>2014-04-25</created><authors><author><keyname>Bogaerts</keyname><forenames>Bart</forenames></author><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Inference in the FO(C) Modelling Language</title><categories>cs.LO</categories><comments>The paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, FO(C), the integration of C-Log with classical logic, was
introduced as a knowledge representation language. Up to this point, no systems
exist that perform inference on FO(C), and very little is known about
properties of inference in FO(C). In this paper, we study both of the above
problems. We define normal forms for FO(C), one of which corresponds to FO(ID).
We define transformations between these normal forms, and show that, using
these transformations, several inference tasks for FO(C) can be reduced to
inference tasks for FO(ID), for which solvers exist. We implemented a prototype
of this transformation, and thus present the first system to perform inference
in FO(C). We also provide results about the complexity of reasoning in FO(C).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6369</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6369</id><created>2014-04-25</created><authors><author><keyname>Huang</keyname><forenames>Zongyan</forenames></author><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>Paulson</keyname><forenames>Lawrence C.</forenames></author><author><keyname>Bridge</keyname><forenames>James</forenames></author></authors><title>Applying machine learning to the problem of choosing a heuristic to
  select the variable ordering for cylindrical algebraic decomposition</title><categories>cs.SC cs.LG</categories><comments>16 pages</comments><msc-class>68W30, 68T05, O3C10</msc-class><acm-class>I.2.6</acm-class><journal-ref>Intelligent Computer Mathematics, pp. 92-107. (Lecture Notes in
  Artificial Intelligence, 8543). Springer Berlin Heidelberg, 2014</journal-ref><doi>10.1007/978-3-319-08434-3_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decomposition(CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. When using CAD, there is often a choice for the ordering placed on the
variables. This can be important, with some problems infeasible with one
variable ordering but easy with another. Machine learning is the process of
fitting a computer model to a complex function based on properties learned from
measured data. In this paper we use machine learning (specifically a support
vector machine) to select between heuristics for choosing a variable ordering,
outperforming each of the separate heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6371</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6371</id><created>2014-04-25</created><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>Chen</keyname><forenames>Changbo</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>Maza</keyname><forenames>Marc Moreno</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author></authors><title>Problem formulation for truth-table invariant cylindrical algebraic
  decomposition by incremental triangular decomposition</title><categories>cs.SC</categories><msc-class>68W30, O3C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>Intelligent Computer Mathematics, pp. 45-60. (Lecture Notes in
  Artificial Intelligence, 8543). Springer Berlin Heidelberg, 2014</journal-ref><doi>10.1007/978-3-319-08434-3_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decompositions (CADs) are a key tool for solving
problems in real algebraic geometry and beyond. We recently presented a new CAD
algorithm combining two advances: truth-table invariance, making the CAD
invariant with respect to the truth of logical formulae rather than the signs
of polynomials; and CAD construction by regular chains technology, where first
a complex decomposition is constructed by refining a tree incrementally by
constraint. We here consider how best to formulate problems for input to this
algorithm. We focus on a choice (not relevant for other CAD algorithms) about
the order in which constraints are presented. We develop new heuristics to help
make this choice and thus allow the best use of the algorithm in practice. We
also consider other choices of problem formulation for CAD, as discussed in
CICM 2013, revisiting these in the context of the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6383</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6383</id><created>2014-04-25</created><updated>2014-04-29</updated><authors><author><keyname>Haenel</keyname><forenames>Valentin</forenames></author></authors><title>Bloscpack: a compressed lightweight serialization format for numerical
  data</title><categories>cs.MS cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-02</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper introduces the Bloscpack file format and the accompanying Python
reference implementation. Bloscpack is a lightweight, compressed binary
file-format based on the Blosc codec and is designed for lightweight, fast
serialization of numerical data. This article presents the features of the
file-format and some some API aspects of the reference implementation, in
particular the ability to handle Numpy ndarrays. Furthermore, in order to
demonstrate its utility, the format is compared both feature- and
performance-wise to a few alternative lightweight serialization solutions for
Numpy ndarrays. The performance comparisons take the form of some comprehensive
benchmarks over a range of different artificial datasets with varying size and
complexity, the results of which are presented as the last section of this
article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6384</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6384</id><created>2014-04-25</created><authors><author><keyname>Oh</keyname><forenames>Jinook</forenames></author></authors><title>CATOS: Computer Aided Training/Observing System</title><categories>cs.CE cs.HC</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-03</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In animal behavioral biology, there are several cases in which an autonomous
observing/training system would be useful. 1) Observation of certain species
continuously, or for documenting specific events, which happen irregularly; 2)
Longterm intensive training of animals in preparation for behavioral
experiments; and 3) Training and testing of animals without human interference,
to eliminate potential cues and biases induced by humans. The primary goal of
this study is to build a system named CATOS (Computer Aided Training/Observing
System) that could be used in the above situations. As a proof of concept, the
system was built and tested in a pilot experiment, in which cats were trained
to press three buttons differently in response to three different sounds (human
speech) to receive food rewards. The system was built in use for about 6
months, successfully training two cats. One cat learned to press a particular
button, out of three buttons, to obtain the food reward with over 70 percent
correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6385</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6385</id><created>2014-04-25</created><updated>2014-04-29</updated><authors><author><keyname>Salvaire</keyname><forenames>Fabrice</forenames></author></authors><title>High-Content Digital Microscopy with Python</title><categories>cs.CE cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-05</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  High-Content Digital Microscopy enhances user comfort, data storage and
analysis throughput, paving the way to new researches and medical diagnostics.
A digital microscopy platform aims at capturing an image of a cover slip, at
storing information on a file server and a database, at visualising the image
and analysing its content. We will discuss how the Python ecosystem can provide
such software framework efficiently. Moreover this paper will give an
illustration of the data chunking approach to manage the huge amount of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6387</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6387</id><created>2014-04-25</created><authors><author><keyname>D'Souza</keyname><forenames>Kelsey</forenames></author></authors><title>PySTEMM: Executable Concept Modeling for K-12 STEM Learning</title><categories>cs.CY cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-06</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Modeling should play a central role in K-12 STEM education, where it could
make classes much more engaging. A model underlies every scientific theory, and
models are central to all the STEM disciplines (Science, Technology,
Engineering, Math). This paper describes executable concept modeling of STEM
concepts using immutable objects and pure functions in Python. I present
examples in math, physics, chemistry, and engineering, built using a
proof-of-concept tool called PySTEMM . The approach applies to all STEM areas
and supports learning with pictures, narrative, animation, and graph plots.
Models can extend each other, simplifying getting started. The
functional-programming style reduces incidental complexity and code debugging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6388</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6388</id><created>2014-04-25</created><updated>2014-04-29</updated><authors><author><keyname>Murri</keyname><forenames>Riccardo</forenames></author></authors><title>Performance of Python runtimes on a non-numeric scientific code</title><categories>cs.MS cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-07</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Python library FatGHol FatGHoL used in Murri2012 to reckon the rational
homology of the moduli space of Riemann surfaces is an example of a non-numeric
scientific code: most of the processing it does is generating graphs
(represented by complex Python objects) and computing their isomorphisms (a
triple of Python lists; again a nested data structure). These operations are
repeated many times over: for example, the spaces and are triangulated by
4'583'322 and 747'664 graphs, respectively. This is an opportunity for every
Python runtime to prove its strength in optimization. The purpose of this
experiment was to assess the maturity of alternative Python runtimes, in terms
of: compatibility with the language as implemented in CPython 2.7, and
performance speedup. This paper compares the results and experiences from
running FatGHol with different Python runtimes: CPython 2.7.5, PyPy 2.1, Cython
0.19, Numba 0.11, Nuitka 0.4.4 and Falcon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6389</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6389</id><created>2014-04-25</created><authors><author><keyname>Haessig</keyname><forenames>Pierre</forenames></author><author><keyname>Kovaltchouk</keyname><forenames>Thibaut</forenames></author><author><keyname>Multon</keyname><forenames>Bernard</forenames></author><author><keyname>Ahmed</keyname><forenames>Hamid Ben</forenames></author><author><keyname>Lascaud</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Computing an Optimal Control Policy for an Energy Storage</title><categories>cs.SY</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-08</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce StoDynProg, a small library created to solve Optimal Control
problems arising in the management of Renewable Power Sources, in particular
when coupled with an Energy Storage System. The library implements generic
Stochastic Dynamic Programming (SDP) numerical methods which can solve a large
class of Dynamic Optimization problems. We demonstrate the library capabilities
with a prototype problem: smoothing the power of an Ocean Wave Energy
Converter. First we use time series analysis to derive a stochastic Markovian
model of this system since it is required by Dynamic Programming. Then, we
briefly describe the &quot;policy iteration&quot; algorithm we have implemented and the
numerical tools being used. We show how the API design of the library is
generic enough to address Dynamic Optimization problems outside the field of
Energy Management. Finally, we solve the power smoothing problem and compare
the optimal control with a simpler heuristic control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6390</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6390</id><created>2014-04-25</created><updated>2014-04-29</updated><authors><author><keyname>Richthofer</keyname><forenames>Stefan</forenames></author></authors><title>JyNI - Using native CPython-Extensions in Jython</title><categories>cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-09</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Jython is a Java based Python implementation and the most seamless way to
integrate Python and Java. However, it does not support native extensions
written for CPython like NumPy or SciPy. Since most scientific Python code
fundamentally depends on exactly such native extensions directly or indirectly,
it usually cannot be run with Jython. JyNI (Jython Native Interface) aims to
close this gap. It is a layer that enables Jython users to load native CPython
extensions and access them from Jython the same way as they would do in
CPython. In order to leverage the JyNI functionality, you just have to put it
on the Java classpath when Jython is launched. It neither requires you to
recompile the extension code, nor to build a customized Jython fork. That
means, it is binary compatible with existing extension builds. At the time of
writing, JyNI does not fully implement the Python C-API and it is only capable
of loading simple examples that only involve most basic built-in types. The
concept is rather complete though and our goal is to provide the C-API needed
to load NumPy as soon as possible. After that we will focus on SciPy and
others. We expect that our work will also enable Java developers to use CPython
extensions like NumPy in their Java code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6391</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6391</id><created>2014-04-25</created><updated>2014-04-29</updated><authors><author><keyname>Cimrman</keyname><forenames>Robert</forenames></author></authors><title>SfePy - Write Your Own FE Application</title><categories>cs.CE cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-10</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  SfePy (Simple Finite Elements in Python) is a framework for solving various
kinds of problems (mechanics, physics, biology, ...) described by partial
differential equations in two or three space dimensions by the finite element
method. The paper illustrates its use in an interactive environment or as a
framework for building custom finite-element based solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6394</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6394</id><created>2014-04-25</created><authors><author><keyname>Bogaerts</keyname><forenames>Bart</forenames></author><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>FO(C) and Related Modelling Paradigms</title><categories>cs.LO</categories><comments>This paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, C-Log was introduced as a language for modelling causal processes.
Its formal semantics has been defined together with introductory examples, but
the study of this language is far from finished. In this paper, we compare
C-Log to other declarative modelling languages. More specifically, we compare
to first-order logic (FO), and argue that C-Log and FO are orthogonal and that
their integration, FO(C), is a knowledge representation language that allows
for clear and succinct models. We compare FO(C) to E-disjunctive logic
programming with the stable semantics, and define a fragment on which both
semantics coincide. Furthermore, we discuss object-creation in FO(C), relating
it to mathematics, business rules systems, and data base systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6399</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6399</id><created>2014-04-25</created><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>Jahed</keyname><forenames>Karim A.</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author></authors><title>A Hybrid Graph Representation for Exact Graph Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many exact search algorithms for NP-hard graph problems adopt the old
Davis-Putman branch-and-reduce paradigm. The performance of these algorithms
often suffers from the increasing number of graph modifications, such as
vertex/edge deletions, that reduce the problem instance and have to be &quot;taken
back&quot; frequently during the search process. We investigate practical
implementation-based aspects of exact graph algorithms by providing a simple
hybrid graph representation that trades space for time to address the said
take-back challenge. Experiments on three well studied problems show consistent
significant improvements over classical methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6413</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6413</id><created>2014-04-25</created><authors><author><keyname>Waltner</keyname><forenames>Georg</forenames></author><author><keyname>Mauthner</keyname><forenames>Thomas</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author></authors><title>Indoor Activity Detection and Recognition for Sport Games Analysis</title><categories>cs.CV</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity recognition in sport is an attractive field for computer vision
research. Game, player and team analysis are of great interest and research
topics within this field emerge with the goal of automated analysis. The very
specific underlying rules of sports can be used as prior knowledge for the
recognition task and present a constrained environment for evaluation. This
paper describes recognition of single player activities in sport with special
emphasis on volleyball. Starting from a per-frame player-centered activity
recognition, we incorporate geometry and contextual information via an activity
context descriptor that collects information about all player's activities over
a certain timespan relative to the investigated player. The benefit of this
context information on single player activity recognition is evaluated on our
new real-life dataset presenting a total amount of almost 36k annotated frames
containing 7 activity classes within 6 videos of professional volleyball games.
Our incorporation of the contextual information improves the average
player-centered classification performance of 77.56% by up to 18.35% on
specific classes, proving that spatio-temporal context is an important clue for
activity recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6415</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6415</id><created>2014-04-25</created><authors><author><keyname>Rossetto</keyname><forenames>Anubis G. M.</forenames></author><author><keyname>Geyer</keyname><forenames>Cl&#xe1;udio F. R.</forenames></author><author><keyname>Arantes</keyname><forenames>Luciana</forenames></author><author><keyname>Sens</keyname><forenames>Pierre</forenames></author></authors><title>The Impact Failure Detector</title><categories>cs.DC</categories><comments>EDCC-2014, Fast-Abstracts, failure detector, impact factor, trust
  level</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a new and flexible unreliable failure detector whose
output is related to the trust level of a set of processes. By expressing the
relevance of each process of the set by an impact factor value, our approach
allows the tuning of the detector output, making possible a softer or stricter
monitoring. The idea behind our proposal is that, according to an acceptable
margin of failures and the impact factor assigned to processes, in some
scenarios, the failure of some low impact processes may not change the user
confidence in the set of processes, while the crash of a high impact factor
process may seriously affect it. We outline the application scenarios and the
proposed unreliable failure detector, giving a detailed account of the concept
on which it is based.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6419</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6419</id><created>2014-04-25</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>On some numerical characteristics of a bipartite graph</title><categories>math.CO cs.DM</categories><msc-class>05C30</msc-class><journal-ref>Mathematics and Education in Mathematics, Proceedings of the Forty
  Third Spring Conference of the Union of Bulgarian Mathematicians, Borovetz,
  April 2-6, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper consider an equivalence relation in the set of vertices of a
bipartite graph. Some numerical characteristics showing the cardinality of
equivalence classes are introduced. A combinatorial identity that is in
relationship to these characteristics of the set of all bipartite graphs of the
type $g=\langle R_g \cup C_g, E_g \rangle$ is formulated and proved, where
$V=R_g \cup C_g$ is the set of vertices, $E_g$ is the set of edges of the graph
$g$, $ |R_g |=m\ge 1$, $|C_g |= n\ge 1$, $|E_g |=k\ge 0$, $m,n$ and $k$ are
integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6441</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6441</id><created>2014-04-25</created><authors><author><keyname>Delfosse</keyname><forenames>Nicolas</forenames></author><author><keyname>Li</keyname><forenames>Zhentao</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>A note on the minimum distance of quantum LDPC codes</title><categories>quant-ph cs.IT math.CO math.IT</categories><journal-ref>Mathematical Foundations of Computer Science 2014, Vol. 8635 LNCS
  pp 239-250</journal-ref><doi>10.1007/978-3-662-44465-8_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new lower bound on the minimum distance of a family of quantum
LDPC codes based on Cayley graphs proposed by MacKay, Mitchison and
Shokrollahi. Our bound is exponential, improving on the quadratic bound of
Couvreur, Delfosse and Z\'emor. This result is obtained by examining a family
of subsets of the hypercube which locally satisfy some parity conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6445</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6445</id><created>2014-04-25</created><authors><author><keyname>Creignou</keyname><forenames>Nadia</forenames></author><author><keyname>Papini</keyname><forenames>Odile</forenames></author><author><keyname>R&#xfc;mmele</keyname><forenames>Stefan</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Belief merging within fragments of propositional logic</title><categories>cs.AI cs.LO</categories><comments>To appear in the Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><msc-class>03B42</msc-class><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, belief change within the framework of fragments of propositional
logic has gained increasing attention. Previous works focused on belief
contraction and belief revision on the Horn fragment. However, the problem of
belief merging within fragments of propositional logic has been neglected so
far. This paper presents a general approach to define new merging operators
derived from existing ones such that the result of merging remains in the
fragment under consideration. Our approach is not limited to the case of Horn
fragment but applicable to any fragment of propositional logic characterized by
a closure property on the sets of models of its formulae. We study the logical
properties of the proposed operators in terms of satisfaction of merging
postulates, considering in particular distance-based merging operators for Horn
and Krom fragments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6451</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6451</id><created>2014-04-25</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>On an Algorithm for Isomorphism-Free Generations of Combinatorial
  Objects</title><categories>cs.DS cs.DM math.CO</categories><journal-ref>International Journal of Emerging Trends &amp; Technology in Computer
  Science (IJETTCS), Vol. 2, No. 6 (2013) 215-220</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the work are defined the concepts semi-canonical and canonical binary
matrix. What is described is an algorithm solving the combinatorial problem for
finding the semi-canonical matrices in the set \Lambda_n^k consisting of all
n\times n binary matrices having exactly k 1's in every row and every column
without perambulating all elements. In the described algorithm bitwise
operations are substantially used. In this way it becomes easier to find the
solution to the problem for receiving one representative from every equivalence
class regarding the introduced in the article equivalence relation in the set
\Lambda_n^k . The last problem is equivalent to the problem for finding all
canonical matrices in \Lambda_n^k .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6452</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6452</id><created>2014-04-25</created><updated>2014-10-07</updated><authors><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Otop</keyname><forenames>Jan</forenames></author><author><keyname>Samanta</keyname><forenames>Roopsha</forenames></author></authors><title>Lipschitz Robustness of Finite-state Transducers</title><categories>cs.FL</categories><comments>In FSTTCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of checking if a finite-state transducer is robust
to uncertainty in its input. Our notion of robustness is based on the analytic
notion of Lipschitz continuity --- a transducer is K-(Lipschitz) robust if the
perturbation in its output is at most K times the perturbation in its input. We
quantify input and output perturbation using similarity functions. We show that
K-robustness is undecidable even for deterministic transducers. We identify a
class of functional transducers, which admits a polynomial time
automata-theoretic decision procedure for K-robustness. This class includes
Mealy machines and functional letter-to-letter transducers. We also study
K-robustness of nondeterministic transducers. Since a nondeterministic
transducer generates a set of output words for each input word, we quantify
output perturbation using set-similarity functions. We show that K-robustness
of nondeterministic transducers is undecidable, even for letter-to-letter
transducers. We identify a class of set-similarity functions which admit
decidable K-robustness of letter-to-letter transducers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6457</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6457</id><created>2014-04-25</created><authors><author><keyname>Neumann</keyname><forenames>Christoph</forenames></author><author><keyname>Heen</keyname><forenames>Olivier</forenames></author><author><keyname>Onno</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>An empirical study of passive 802.11 Device Fingerprinting</title><categories>cs.CR cs.NI</categories><comments>Proceedings of 32nd International Conference on Distributed Computing
  Systems Workshops (ICDCSW 2012), Workshop on Network Forensics, Security and
  Privacy (NFSP'12)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  802.11 device fingerprinting is the action of characterizing a target device
through its wireless traffic. This results in a signature that may be used for
identification, network monitoring or intrusion detection. The fingerprinting
method can be active by sending traffic to the target device, or passive by
just observing the traffic sent by the target device. Many passive
fingerprinting methods rely on the observation of one particular network
feature, such as the rate switching behavior or the transmission pattern of
probe requests. In this work, we evaluate a set of global wireless network
parameters with respect to their ability to identify 802.11 devices. We
restrict ourselves to parameters that can be observed passively using a
standard wireless card. We evaluate these parameters for two different tests:
i) the identification test that returns one single result being the closest
match for the target device, and ii) the similarity test that returns a set of
devices that are close to the target devices. We find that the network
parameters transmission time and frame inter-arrival time perform best in
comparison to the other network parameters considered. Finally, we focus on
inter-arrival times, the most promising parameter for device identification,
and show its dependency from several device characteristics such as the
wireless card and driver but also running applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6471</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6471</id><created>2014-04-25</created><authors><author><keyname>Zhang</keyname><forenames>Huishuai</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames></author><author><keyname>Wang</keyname><forenames>Hua</forenames></author></authors><title>The Capacity Region of the Source-Type Model for Secret Key and Private
  Key Generation</title><categories>cs.IT math.IT</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of simultaneously generating a secret key (SK) and private key
(PK) pair among three terminals via public discussion is investigated, in which
each terminal observes a component of correlated sources. All three terminals
are required to generate a common secret key concealed from an eavesdropper
that has access to public discussion, while two designated terminals are
required to generate an extra private key concealed from both the eavesdropper
and the remaining terminal. An outer bound on the SK-PK capacity region was
established in [1], and was shown to be achievable for one case. In this paper,
achievable schemes are designed to achieve the outer bound for the remaining
two cases, and hence the SK-PK capacity region is established in general. The
main technique lies in the novel design of a random binning-joint decoding
scheme that achieves the existing outer bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6472</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6472</id><created>2014-04-25</created><authors><author><keyname>Duan</keyname><forenames>Ruchen</forenames><affiliation>Shitz</affiliation></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames><affiliation>Shitz</affiliation></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Parallel Gaussian Networks with a Common State-Cognitive Helper</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of state-dependent parallel networks with a common state-cognitive
helper, in which $K$ transmitters wish to send $K$ messages to their
corresponding receivers over $K$ state-corrupted parallel channels, and a
helper who knows the state information noncausally wishes to assist these
receivers to cancel state interference. Furthermore, the helper also has its
own message to be sent simultaneously to its corresponding receiver. Since the
state information is known only to the helper, but not to the corresponding
transmitters $1,\dots,K$, transmitter-side state cognition and receiver-side
state interference are mismatched. Our focus is on the high state power regime,
i.e., the state power goes to infinity. Three (sub)models are studied. Model I
serves as a basic model, which consists of only one transmitter-receiver (with
state corruption) pair in addition to a helper that assists the receiver to
cancel state in addition to transmitting its own message. Model II consists of
two transmitter-receiver pairs in addition to a helper, and only one receiver
is interfered by a state sequence. Model III generalizes model I include
multiple transmitter-receiver pairs with each receiver corrupted by independent
state. For all models, inner and outer bounds on the capacity region are
derived, and comparison of the two bounds leads to characterization of either
full or partial boundary of the capacity region under various channel
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6474</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6474</id><created>2014-04-25</created><authors><author><keyname>Zou</keyname><forenames>Shaofeng</forenames><affiliation>Shitz</affiliation></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames><affiliation>Shitz</affiliation></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>An Information Theoretic Approach to Secret Sharing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel information theoretic approach is proposed to solve the secret
sharing problem, in which a dealer distributes one or multiple secrets among a
set of participants that for each secret only qualified sets of users can
recover it by pooling their shares together while non-qualified sets of users
obtain no information about the secret even if they pool their shares together.
While existing secret sharing systems (implicitly) assume that communications
between the dealer and participants are noiseless, this paper takes a more
practical assumption that the dealer delivers shares to the participants via a
noisy broadcast channel. An information theoretic approach is proposed, which
exploits the channel as additional resources to achieve secret sharing
requirements. In this way, secret sharing problems can be reformulated as
equivalent secure communication problems via wiretap channels, and can be
solved by employing powerful information theoretic security techniques. This
approach is first developed for the classic secret sharing problem, in which
only one secret is to be shared. This classic problem is shown to be equivalent
to a communication problem over a compound wiretap channel. The lower and upper
bounds on the secrecy capacity of the compound channel provide the
corresponding bounds on the secret sharing rate. The power of the approach is
further demonstrated by a more general layered multi-secret sharing problem,
which is shown to be equivalent to the degraded broadcast multiple-input
multiple-output (MIMO) channel with layered decoding and secrecy constraints.
The secrecy capacity region for the degraded MIMO broadcast channel is
characterized, which provides the secret sharing capacity region. Furthermore,
these secure encoding schemes that achieve the secrecy capacity region provide
an information theoretic scheme for sharing the secrets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6476</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6476</id><created>2014-04-25</created><updated>2014-05-19</updated><authors><author><keyname>L&#xed;&#x161;ka</keyname><forenames>Martin</forenames></author><author><keyname>Sojka</keyname><forenames>Petr</forenames></author><author><keyname>R\ru&#x17e;i&#x10d;ka</keyname><forenames>Michal</forenames></author></authors><title>Math Indexer and Searcher Web Interface: Towards Fulfillment of
  Mathematicians' Information Needs</title><categories>cs.DL</categories><comments>Preprint of CICM 2014 (http://cicm-conference.org/2014/) paper: S.M.
  Watt et al. (Eds.): CICM 2014, LNAI 8543, pp. 444-448, Springer International
  Publishing Switzerland 2014</comments><acm-class>H.3.7; H.3.6; H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are designing and developing a web user interface for digital mathematics
libraries called WebMIaS. It allows queries to be expressed by mathematicians
through a faceted search interface. Users can combine standard textual
autocompleted keywords with keywords in the form of mathematical formulae in
LaTeX or MathML formats. Formulae are shown rendered by the web browser
on-the-fly for users' feedback. We describe WebMIaS design principles and our
experiences deploying in the European Digital Mathematics Library (EuDML). We
further describe the issues addressed by formulae canonicalization and by
extending the MIaS indexing engine with Content MathML support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6487</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6487</id><created>2014-04-25</created><updated>2014-06-11</updated><authors><author><keyname>Burnik</keyname><forenames>Konrad</forenames><affiliation>University of Zagreb</affiliation></author><author><keyname>Iljazovic</keyname><forenames>Zvonko</forenames><affiliation>University of Zagreb</affiliation></author></authors><title>Computability of 1-manifolds</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 12,
  2014) lmcs:961</journal-ref><doi>10.2168/LMCS-10(2:8)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semi-computable set S in a computable metric space need not be computable.
However, in some cases, if S has certain topological properties, we can
conclude that S is computable. It is known that if a semi-computable set S is a
compact manifold with boundary, then the computability of \deltaS implies the
computability of S. In this paper we examine the case when S is a 1-manifold
with boundary, not necessarily compact. We show that a similar result holds in
this case under assumption that S has finitely many components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6491</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6491</id><created>2014-04-23</created><authors><author><keyname>Wiebe</keyname><forenames>Janyce</forenames></author><author><keyname>Deng</keyname><forenames>Lingjia</forenames></author></authors><title>An Account of Opinion Implicatures</title><categories>cs.CL cs.IR</categories><comments>50 Pages. Submitted to the journal, Language Resources and Evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While previous sentiment analysis research has concentrated on the
interpretation of explicitly stated opinions and attitudes, this work initiates
the computational study of a type of opinion implicature (i.e.,
opinion-oriented inference) in text. This paper described a rule-based
framework for representing and analyzing opinion implicatures which we hope
will contribute to deeper automatic interpretation of subjective language. In
the course of understanding implicatures, the system recognizes implicit
sentiments (and beliefs) toward various events and entities in the sentence,
often attributed to different sources (holders) and of mixed polarities; thus,
it produces a richer interpretation than is typical in opinion analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6502</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6502</id><created>2014-04-25</created><updated>2014-06-26</updated><authors><author><keyname>Srivastav</keyname><forenames>Abhinav</forenames></author><author><keyname>Trystram</keyname><forenames>Denis</forenames></author></authors><title>Total stretch minimization on single and identical parallel machines</title><categories>cs.DS</categories><comments>Submitted to ISAAC, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical problem of scheduling $n$ jobs with release dates
on both single and identical parallel machines. We measure the quality of
service provided to each job by its stretch, which is defined as the ratio of
its response time to processing time. Our objective is to schedule these jobs
non-preemptively so as to minimize sum stretch. So far, there have been very
few results for sum stretch minimization especially for the non-preemptive
case. For the preemptive version, the Shortest remaining processing time (SRPT)
algorithm is known to give $2$-competitive for sum stretch on single machine
while its is $13$-competitive on identical parallel machines. Leonardi and
Kellerer provided the strong lower bound for the more general problem of
\textit{sum (weighted) flow time} in single machine and identical parallel
machines, respectively . Therefore, we study this problem with some additional
assumptions and present two new competitive ratio for existing algorithms. We
show that the Shortest processing time (SPT) algorithm is $\Delta -
\frac{1}{\Delta}+1$-competitive for non-preemptive sum stretch minimization on
single machine and it is $\Delta - \frac{1}{\Delta}+ \frac{3}{2} -\frac{1}{2m}$
on $m$ identical parallel machines, where $\Delta$ is the upper bound on the
ratio between the maximum and the minimum processing time of the jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6503</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6503</id><created>2014-04-25</created><authors><author><keyname>Reiter</keyname><forenames>Fabian</forenames></author></authors><title>Distributed Graph Automata</title><categories>cs.FL cs.LO</categories><comments>Master's Thesis, 64 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by distributed algorithms, we introduce a new class of finite graph
automata that recognize precisely the graph languages definable in monadic
second-order logic. For the cases of words and trees, it has been long known
that the regular languages are precisely those definable in monadic
second-order logic. In this regard, the automata proposed in the present work
can be seen, to some extent, as a generalization of finite automata to graphs.
  Furthermore, we show that, unlike for finite automata on words and trees, the
deterministic, nondeterministic and alternating variants of our automata form a
strict hierarchy with respect to their expressive power. For the weaker
variants, the emptiness problem is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6508</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6508</id><created>2014-04-22</created><authors><author><keyname>Ding</keyname><forenames>Guoru</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Yao</keyname><forenames>Yu-Dong</forenames></author></authors><title>Big Spectrum Data: The New Resource for Cognitive Wireless Networking</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The era of Big Data is here now, which has brought both unprecedented
opportunities and critical challenges. In this article, from a perspective of
cognitive wireless networking, we start with a definition of Big Spectrum Data
by analyzing its characteristics in terms of six Vs, i.e., volume, variety,
velocity, veracity, viability, and value. We then present a high-level tutorial
on research frontiers in Big Spectrum Data analytics to guide the development
of practical algorithms. We also highlight Big Spectrum Data as the new
resource for cognitive wireless networking by presenting the emerging use
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6512</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6512</id><created>2014-04-25</created><updated>2014-07-31</updated><authors><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Cellular Interference Alignment: Omni-Directional Antennas and
  Asymmetric Configurations</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1402.3119</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although interference alignment (IA) can theoretically achieve the optimal
degrees of freedom (DoFs) in the $K$-user Gaussian interference channel, its
direct application comes at the prohibitive cost of precoding over
exponentially-many signaling dimensions. On the other hand, it is known that
practical &quot;one-shot&quot; IA precoding (i.e., linear schemes without symbol
expansion) provides a vanishing DoFs gain in large fully-connected networks
with generic channel coefficients. In our previous work, we introduced the
concept of &quot;Cellular IA&quot; for a network topology induced by hexagonal cells with
sectors and nearest-neighbor interference. Assuming that neighboring sectors
can exchange decoded messages (and not received signal samples) in the uplink,
we showed that linear one-shot IA precoding over $M$ transmit/receive antennas
can achieve the optimal $M/2$ DoFs per user. In this paper we extend this
framework to networks with omni-directional (non-sectorized) cells and consider
the practical scenario where users have $2$ antennas, and base-stations have
$2$, $3$ or $4$ antennas. In particular, we provide linear one-shot IA schemes
for the $2\times 2$, $2\times3$ and $2\times 4$ cases, and show the
achievability of $3/4$, $1$ and $7/6$ DoFs per user, respectively. DoFs
converses for one-shot schemes require the solution of a discrete optimization
problem over a number of variables that grows with the network size. We develop
a new approach to transform such challenging optimization problem into a
tractable linear program (LP) with significantly fewer variables. This approach
is used to show that the achievable $3/4$ DoFs per user are indeed optimal for
a large (extended) cellular network with $2\times 2$ links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6519</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6519</id><created>2014-04-25</created><updated>2014-05-14</updated><authors><author><keyname>Cohl</keyname><forenames>Howard S.</forenames></author><author><keyname>McClain</keyname><forenames>Marjorie A.</forenames></author><author><keyname>Saunders</keyname><forenames>Bonita V.</forenames></author><author><keyname>Schubotz</keyname><forenames>Moritz</forenames></author><author><keyname>Williams</keyname><forenames>Janelle C.</forenames></author></authors><title>Digital Repository of Mathematical Formulae</title><categories>cs.DL</categories><doi>10.1007/978-3-319-08434-3_30</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The purpose of the NIST Digital Repository of Mathematical Formulae (DRMF) is
to create a digital compendium of mathematical formulae for orthogonal
polynomials and special functions (OPSF) and of associated mathematical data.
The DRMF addresses needs of working mathematicians, physicists and engineers:
providing a platform for publication and interaction with OPSF formulae on the
web. Using MediaWiki extensions and other existing technology (such as software
and macro collections developed for the NIST Digital Library of Mathematical
Functions), the DRMF acts as an interactive web domain for OPSF formulae.
Whereas Wikipedia and other web authoring tools manifest notions or
descriptions as first class objects, the DRMF does that with mathematical
formulae. See http://gw32.iu.xsede.org/index.php/Main_Page.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6535</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6535</id><created>2014-04-25</created><authors><author><keyname>Anthony</keyname><forenames>Martin</forenames></author><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Crama</keyname><forenames>Yves</forenames></author><author><keyname>Gruber</keyname><forenames>Aritanan</forenames></author></authors><title>Quadratization of Symmetric Pseudo-Boolean Functions</title><categories>math.OC cs.CC cs.CV math.CO</categories><comments>17 pages</comments><msc-class>06E30, 90C09, 90C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudo-Boolean function is a real-valued function
$f(x)=f(x_1,x_2,\ldots,x_n)$ of $n$ binary variables; that is, a mapping from
$\{0,1\}^n$ to $\mathbb{R}$. For a pseudo-Boolean function $f(x)$ on
$\{0,1\}^n$, we say that $g(x,y)$ is a quadratization of $f$ if $g(x,y)$ is a
quadratic polynomial depending on $x$ and on $m$ auxiliary binary variables
$y_1,y_2,\ldots,y_m$ such that $f(x)= \min \{g(x,y) : y \in \{0,1\}^m \}$ for
all $x \in \{0,1\}^n$. By means of quadratizations, minimization of $f$ is
reduced to minimization (over its extended set of variables) of the quadratic
function $g(x,y)$. This is of some practical interest because minimization of
quadratic functions has been thoroughly studied for the last few decades, and
much progress has been made in solving such problems exactly or heuristically.
A related paper \cite{ABCG} initiated a systematic study of the minimum number
of auxiliary $y$-variables required in a quadratization of an arbitrary
function $f$ (a natural question, since the complexity of minimizing the
quadratic function $g(x,y)$ depends, among other factors, on the number of
binary variables). In this paper, we determine more precisely the number of
auxiliary variables required by quadratizations of symmetric pseudo-Boolean
functions $f(x)$, those functions whose value depends only on the Hamming
weight of the input $x$ (the number of variables equal to $1$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6538</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6538</id><created>2014-04-25</created><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Gruber</keyname><forenames>Aritanan</forenames></author></authors><title>On Quadratization of Pseudo-Boolean Functions</title><categories>math.OC cs.CV math.CO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey current term-wise techniques for quadratizing high-degree
pseudo-Boolean functions and introduce a new one, which allows multiple splits
of terms. We also introduce the first aggregative approach, which splits a
collection of terms based on their common parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6544</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6544</id><created>2014-04-25</created><authors><author><keyname>Abu-Shaban</keyname><forenames>Zohair</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>R.</keyname><forenames>Bhavani Shankar M.</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Interference Mitigating Satellite Broadcast Receiver using Reduced
  Complexity List-Based Detection in Correlated Noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent commercial trends towards using smaller dish antennas for
satellite receivers, and the growing density of broadcasting satellites,
necessitate the application of robust adjacent satellite interference (ASI)
cancellation schemes. This orbital density growth along with the wider
beamwidth of a smaller dish have imposed an overloaded scenario at the
satellite receiver, where the number of transmitting satellites exceeds the
number of receiving elements at the dish antenna. To ensure successful
operation in this practical scenario, we propose a satellite receiver that
enhances signal detection from the desired satellite by mitigating the
interference from neighboring satellites. Towards this objective, we propose a
reduced complexity list-based group-wise search detection (RC-LGSD) receiver
under the assumption of spatially correlated additive noise. To further enhance
detection performance, the proposed satellite receiver utilizes a newly
designed whitening filter to remove the spatial correlation amongst the noise
parameters, while also applying a preprocessor that maximizes the
signal-to-interference-plus-noise ratio (SINR). Extensive simulations under
practical scenarios show that the proposed receiver enhances the performance of
satellite broadcast systems in the presence of ASI compared to existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6545</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6545</id><created>2014-04-25</created><updated>2015-04-10</updated><authors><author><keyname>Wagner</keyname><forenames>Caroline</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Recent Developments in China-U.S. Cooperation in Science</title><categories>cs.DL</categories><comments>Conference on China's Science and Technology International Relations,
  April, 2014, Arizona State University; accepted for publication in Minerva,
  April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  China's remarkable gains in science over the past 25 years have been well
documented (e.g., Jin and Rousseau, 2005a; Zhou and Leydesdorff, 2006; Shelton
&amp; Foland, 2009) but it is less well known that China and the United States have
become each other's top collaborating country. Science and technology has been
a primary vehicle for growing the bilateral relationship between China and the
United States since the opening of relations between the two countries in the
late 1970s. During the 2000s, the scientific relationship between China and the
United States--as measured in coauthored papers--showed significant growth.
Chinese scientists claim first authorship much more frequently than U.S.
counterparts by the end of the decade. The sustained rate of increase of
collaboration with one other country is unprecedented on the U.S. side. Even
growth in relations with eastern European nations does not match the growth in
the relationship between China and the United States. Both countries can
benefit from the relationship, but for the U.S., greater benefit would come
from a more targeted strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6547</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6547</id><created>2014-04-25</created><authors><author><keyname>Ginev</keyname><forenames>Deyan</forenames></author><author><keyname>Miller</keyname><forenames>Bruce R.</forenames></author><author><keyname>Oprea</keyname><forenames>Silviu</forenames></author></authors><title>E-books and Graphics with LaTeXML</title><categories>cs.DL</categories><comments>4 pages, accepted at Conferences on Intelligent Computer Mathematics
  CICM2014</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Marked by the highlights of native generation of EPUB E-books and TikZ
support for creating SVG images, we present an annual report of LaTeXML
development in 2013. LaTeXML provides a reimplementation of the $\TeX$ parser,
geared towards preserving macro semantics; it supports an array of output
formats, notably HTML5, EPUB, XHTML and its own $\LaTeX$-near XML. Other
highlights include enhancing performance when used inside high-throughput
build-systems, via incorporating a native ZIP archive workflow, as well as a
simplified installation procedure that now allows to deploy LaTeXML as a cloud
service. To this end, we also introduce an official plugin-based scheme for
publishing new features that go beyond the core scope of LaTeXML, such as web
services or unconventional post-processors. The software suite has now migrated
to GitHub and we welcome forks and patches from the wider FLOSS community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6548</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6548</id><created>2014-04-25</created><authors><author><keyname>Ginev</keyname><forenames>Deyan</forenames></author><author><keyname>Corneli</keyname><forenames>Joseph</forenames></author></authors><title>NNexus Reloaded</title><categories>cs.DL</categories><comments>4 pages, accepted at Conferences on Intelligent Computer Mathematics
  CICM2014</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Interlinking knowledge is one of the cornerstones of online collaboration.
While wiki systems typically rely on links supplied by authors, in the early
2000s the mathematics encyclopedia at PlanetMath.org introduced a feature that
provides automatic linking for previously defined concepts. The NNexus software
suite was developed to support the necessary subtasks of concept indexing,
concept discovery and link-annotation. In this paper, we describe our recent
reimplementation and revisioning of the NNexus system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6549</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6549</id><created>2014-04-25</created><authors><author><keyname>Ginev</keyname><forenames>Deyan</forenames></author><author><keyname>Miller</keyname><forenames>Bruce R.</forenames></author></authors><title>LaTeXML 2012 - A Year of LaTeXML</title><categories>cs.DL</categories><comments>5 pages, in proceedings of Intelligent Computer Mathematics, pp.
  335-338. No. 7961 in Lecture Notes in Computer Science, Springer (2013)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  LaTeXML, a $\TeX$ to XML converter, is being used in a wide range of MKM
applications. In this paper, we present a progress report for the 2012 calendar
year. Noteworthy enhancements include: increased coverage such as Wikipedia
syntax; enhanced capabilities such as embeddable JavaScript and CSS resources
and RDFa support; a web service for remote processing via web-sockets; along
with general accuracy and reliability improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6556</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6556</id><created>2014-04-25</created><authors><author><keyname>Guo</keyname><forenames>Anjin</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Asymptotic Deployment Gain: A Simple Approach to Characterize the SINR
  Distribution in General Cellular Networks</title><categories>cs.IT cs.NI math.IT math.PR</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cellular network models, the base stations are usually assumed to form a
lattice or a Poisson point process (PPP). In reality, however, they are
deployed neither fully regularly nor completely randomly. Accordingly, in this
paper, we consider the very general class of motion-invariant models and
analyze the behavior of the outage probability (the probability that the
signal-to-interference-plus-noise-ratio (SINR) is smaller than a threshold) as
the threshold goes to zero. We show that, remarkably, the slope of the outage
probability (in dB) as a function of the threshold (also in dB) is the same for
essentially all motion-invariant point processes. The slope merely depends on
the fading statistics. Using this result, we introduce the notion of the
asymptotic deployment gain (ADG), which characterizes the horizontal gap
between the success probabilities of the PPP and another point process in the
high-reliability regime (where the success probability is near 1). To
demonstrate the usefulness of the ADG for the characterization of the SINR
distribution, we investigate the outage probabilities and the ADGs for
different point processes and fading statistics by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6560</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6560</id><created>2014-04-25</created><updated>2015-03-12</updated><authors><author><keyname>Hachem</keyname><forenames>Jad</forenames></author><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Content Caching and Delivery over Heterogeneous Wireless Networks</title><categories>cs.IT math.IT</categories><comments>A shorter version is to appear in IEEE INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging heterogeneous wireless architectures consist of a dense deployment
of local-coverage wireless access points (APs) with high data rates, along with
sparsely-distributed, large-coverage macro-cell base stations (BS). We design a
coded caching-and-delivery scheme for such architectures that equips APs with
storage, enabling content pre-fetching prior to knowing user demands. Users
requesting content are served by connecting to local APs with cached content,
as well as by listening to a BS broadcast transmission. For any given content
popularity profile, the goal is to design the caching-and-delivery scheme so as
to optimally trade off the transmission cost at the BS against the storage cost
at the APs and the user cost of connecting to multiple APs. We design a coded
caching scheme for non-uniform content popularity that dynamically allocates
user access to APs based on requested content. We demonstrate the approximate
optimality of our scheme with respect to information-theoretic bounds. We
numerically evaluate it on a YouTube dataset and quantify the trade-off between
transmission rate, storage, and access cost. Our numerical results also suggest
the intriguing possibility that, to gain most of the benefits of coded caching,
it suffices to divide the content into a small number of popularity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6561</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6561</id><created>2014-04-25</created><updated>2015-09-15</updated><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Distributed Computing on Core-Periphery Networks: Axiom-based Design</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by social networks and complex systems, we propose a core-periphery
network architecture that supports fast computation for many distributed
algorithms and is robust and efficient in number of links. Rather than
providing a concrete network model, we take an axiom-based design approach. We
provide three intuitive (and independent) algorithmic axioms and prove that any
network that satisfies all axioms enjoys an efficient algorithm for a range of
tasks (e.g., MST, sparse matrix multiplication, etc.). We also show the
minimality of our axiom set: for networks that satisfy any subset of the
axioms, the same efficiency cannot be guaranteed for any deterministic
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6563</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6563</id><created>2014-04-25</created><updated>2015-12-31</updated><authors><author><keyname>Hachem</keyname><forenames>Jad</forenames></author><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Coded Caching for Multi-level Popularity and Access</title><categories>cs.IT math.IT</categories><comments>Parts of the results in this paper have already been published and
  are available on arXiv: abs/1404.6560, abs/1504.05931, and the older version
  of this submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address the exponentially rising demand for wireless content, use of
caching is emerging as a potential solution. It has been recently established
that joint design of content delivery and storage (coded caching) can
significantly improve performance over conventional caching. Coded caching is
well suited to emerging heterogeneous wireless architectures which consist of a
dense deployment of local-coverage wireless access points (APs) with high data
rates, along with sparsely-distributed, large-coverage macro-cell base stations
(BS). This enables design of coded caching-and-delivery schemes that equip APs
with storage, and place content in them in a way that creates coded-multicast
opportunities for combining with macro-cell broadcast to satisfy users even
with different demands. Such coded-caching schemes have been shown to be
order-optimal with respect to the BS transmission rate, for a system with
single-level content, i.e., one where all content is uniformly popular. In this
work, we consider a system with non-uniform popularity content which is divided
into multiple levels, based on varying degrees of popularity. The main
contribution of this work is the derivation of an order-optimal scheme which
judiciously shares cache memory among files with different popularities. To
show order-optimality we derive new information-theoretic lower bounds, which
use a sliding-window entropy inequality, effectively creating a non-cutset
bound. We also extend the ideas to when users can access multiple caches along
with the broadcast. Finally we consider two extreme cases of user distribution
across caches for the multi-level popularity model: a single user per cache
(single-user setup) versus a large number of users per cache (multi-user
setup), and demonstrate a dichotomy in the order-optimal strategies for these
two extreme cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6566</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6566</id><created>2014-04-25</created><authors><author><keyname>Gil</keyname><forenames>Oliver Fern&#xe1;ndez</forenames></author></authors><title>On the Non-Monotonic Description Logic
  $\mathcal{ALC}$+T$_{\mathsf{min}}$</title><categories>cs.AI</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last 20 years many proposals have been made to incorporate
non-monotonic reasoning into description logics, ranging from approaches based
on default logic and circumscription to those based on preferential semantics.
In particular, the non-monotonic description logic
$\mathcal{ALC}$+T$_{\mathsf{min}}$ uses a combination of the preferential
semantics with minimization of a certain kind of concepts, which represent
atypical instances of a class of elements. One of its drawbacks is that it
suffers from the problem known as the \emph{property blocking inheritance},
which can be seen as a weakness from an inferential point of view. In this
paper we propose an extension of $\mathcal{ALC}$+T$_{\mathsf{min}}$, namely
$\mathcal{ALC}$+T$^+_{\mathsf{min}}$, with the purpose to solve the mentioned
problem. In addition, we show the close connection that exists between
$\mathcal{ALC}$+T$^+_{\mathsf{min}}$ and concept-circumscribed knowledge bases.
Finally, we study the complexity of deciding the classical reasoning tasks in
$\mathcal{ALC}$+T$^+_{\mathsf{min}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6567</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6567</id><created>2014-04-25</created><authors><author><keyname>Bekkouche</keyname><forenames>Mohammed</forenames></author><author><keyname>Collavizza</keyname><forenames>H&#xe9;l&#xe8;ne</forenames></author><author><keyname>Rueher</keyname><forenames>Michel</forenames></author></authors><title>Une approche CSP pour l'aide \`a la localisation d'erreurs</title><categories>cs.AI cs.SE</categories><comments>10 pages, in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce in this paper a new CP-based approach to support errors location
in a program for which a counter-example is available, i.e. an instantiation of
the input variables that violates the post-condition. To provide helpful
information for error location, we generate a constraint system for the paths
of the CFG (Control Flow Graph) for which at most k conditional statements may
be erroneous. Then, we calculate Minimal Correction Sets (MCS) of bounded size
for each of these paths. The removal of one of these sets of constraints yields
a maximal satisfiable subset, in other words, a maximal subset of constraints
satisfying the post condition. We extend the algorithm proposed by Liffiton and
Sakallah \cite{LiS08} to handle programs with numerical statements more
efficiently. We present preliminary experimental results that are quite
encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6570</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6570</id><created>2014-04-25</created><authors><author><keyname>Mondal</keyname><forenames>Jayanta</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author></authors><title>EAGr: Supporting Continuous Ego-centric Aggregate Queries over Large
  Dynamic Graphs</title><categories>cs.DB</categories><comments>18 pages, 1 table, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present EAGr, a system for supporting large numbers of
continuous neighborhood-based (&quot;ego-centric&quot;) aggregate queries over large,
highly dynamic, and rapidly evolving graphs. Examples of such queries include
computation of personalized, tailored trends in social networks, anomaly/event
detection in financial transaction networks, local search and alerts in
spatio-temporal networks, to name a few. Key challenges in supporting such
continuous queries include high update rates typically seen in these
situations, large numbers of queries that need to be executed simultaneously,
and stringent low latency requirements. We propose a flexible, general, and
extensible in-memory framework for executing different types of ego-centric
aggregate queries over large dynamic graphs with low latencies. Our framework
is built around the notion of an aggregation overlay graph, a pre-compiled data
structure that encodes the computations to be performed when an update/query is
received. The overlay graph enables sharing of partial aggregates across
multiple ego-centric queries (corresponding to the nodes in the graph), and
also allows partial pre-computation of the aggregates to minimize the query
latencies. We present several highly scalable techniques for constructing an
overlay graph given an aggregation function, and also design incremental
algorithms for handling structural changes to the underlying graph. We also
present an optimal, polynomial-time algorithm for making the pre-computation
decisions given an overlay graph, and evaluate an approach to incrementally
adapt those decisions as the workload changes. Although our approach is
naturally parallelizable, we focus on a single-machine deployment and show that
our techniques can easily handle graphs of size up to 320 million nodes and
edges, and achieve update/query throughputs of over 500K/s using a single,
powerful machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6573</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6573</id><created>2014-04-25</created><authors><author><keyname>Krontiris</keyname><forenames>Athanasios</forenames></author><author><keyname>Shome</keyname><forenames>Rahul</forenames></author><author><keyname>Dobson</keyname><forenames>Andrew</forenames></author><author><keyname>Kimmel</keyname><forenames>Andrew</forenames></author><author><keyname>Yochelson</keyname><forenames>Issac</forenames></author><author><keyname>Bekris</keyname><forenames>Kostas</forenames></author></authors><title>Similar Part Rearrangement With Pebble Graphs</title><categories>cs.RO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a method for effectively computing manipulation paths to
rearrange similar objects in a cluttered space. The solution can be used to
place similar products in a factory floor in a desirable arrangement or for
retrieving a particular object from a shelf blocked by similarly sized objects.
These are challenging problems as they involve combinatorially large,
continuous configuration spaces due to the presence of multiple moving bodies
and kinematically complex manipulators. This work leverages ideas from
algorithmic theory, multi-robot motion planning and manipulation planning to
propose appropriate graphical representations for this challenge. These
representations allow to quickly reason whether manipulation paths allow the
transition between entire sets of objects arrangements without having to
explicitly enumerate the path for each pair of arrangements. The proposed
method also allows to take advantage of precomputation given a manipulation
roadmap for transferring a single object in the same cluttered space. The
resulting approach is evaluated in simulation for a realistic model of a Baxter
robot and executed in open-loop on the real system, showing that the approach
solves complex instances and is promising in terms of scalability and success
ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6580</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6580</id><created>2014-04-25</created><authors><author><keyname>Agarwal</keyname><forenames>Arvind</forenames></author><author><keyname>Kataria</keyname><forenames>Saurabh</forenames></author></authors><title>Multitask Learning for Sequence Labeling Tasks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a learning method for sequence labeling tasks in
which each example sequence has multiple label sequences. Our method learns
multiple models, one model for each label sequence. Each model computes the
joint probability of all label sequences given the example sequence. Although
each model considers all label sequences, its primary focus is only one label
sequence, and therefore, each model becomes a task-specific model, for the task
belonging to that primary label. Such multiple models are learned {\it
simultaneously} by facilitating the learning transfer among models through {\it
explicit parameter sharing}. We experiment the proposed method on two
applications and show that our method significantly outperforms the
state-of-the-art method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6583</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6583</id><created>2014-04-25</created><authors><author><keyname>Beyer</keyname><forenames>Andreas</forenames></author><author><keyname>Mara</keyname><forenames>Hubert</forenames></author><author><keyname>Kr&#xf6;mker</keyname><forenames>Susanne</forenames></author></authors><title>ILATO Project: Fusion of Optical Surface Models and Volumetric CT Data</title><categories>cs.CG</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/17</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Project ILATO focuses on Improving Limited Angle computed Tomography by
Optical data integration in order to enhance image quality and shorten
acquisition times in X-ray based industrial quality inspection. Limited angle
computed tomography is indicated whenever specimen dimensions exceed cone beam
limits or the object is impenetrable from certain angles. Thus, acquiring only
a subset of a full circle CT scan poses problems for reconstruction algorithms
due to incomplete data which introduces blurred edges and other artifacts. To
support volumetric data reconstruction algorithm a surface mesh of the object
obtained via structured light optical scan acts as a mask defining boundaries
of the reconstructed image. The registration of optically acquired surfaces
with data acquired from computed tomography is our current challenge. This
article presents our setup, the methods applied and discusses the problems
arising from registration of data sets created with considerably different
imaging techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6585</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6585</id><created>2014-04-25</created><authors><author><keyname>Graves</keyname><forenames>Eric</forenames></author><author><keyname>Wong</keyname><forenames>Tan F.</forenames></author></authors><title>Equating the achievable exponent region to the achievable entropy region
  by partitioning the source</title><categories>cs.IT math.IT</categories><comments>5 Pages, ISIT 14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the image size characterization problem. We show
that any arbitrary source set may be decomposed into sets whose image size
characterization is the same as its entropy characterization. We also show that
the number of these sets required is small enough that one may consider that
from a coding perspective the achievable entropy region and achievable exponent
region are equal. This has an impact on many source networks and network
problems whose solution heretofore could not have the image size
characterization applied to them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6602</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6602</id><created>2014-04-26</created><authors><author><keyname>Leino</keyname><forenames>K. Rustan M.</forenames><affiliation>Microsoft Research, Redmond, WA, USA</affiliation></author><author><keyname>W&#xfc;stholz</keyname><forenames>Valentin</forenames><affiliation>ETH Zurich, Department of Computer Science, Switzerland</affiliation></author></authors><title>The Dafny Integrated Development Environment</title><categories>cs.PL cs.HC cs.SE</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.6; D.3.4</acm-class><journal-ref>EPTCS 149, 2014, pp. 3-15</journal-ref><doi>10.4204/EPTCS.149.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, program verifiers and interactive theorem provers have
become more powerful and more suitable for verifying large programs or proofs.
This has demonstrated the need for improving the user experience of these tools
to increase productivity and to make them more accessible to non-experts. This
paper presents an integrated development environment for Dafny-a programming
language, verifier, and proof assistant-that addresses issues present in most
state-of-the-art verifiers: low responsiveness and lack of support for
understanding non-obvious verification failures. The paper demonstrates several
new features that move the state-of-the-art closer towards a verification
environment that can provide verification feedback as the user types and can
present more helpful information about the program or failed verifications in a
demand-driven and unobtrusive way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6603</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6603</id><created>2014-04-26</created><authors><author><keyname>Bendisposto</keyname><forenames>Jens</forenames><affiliation>Heinrich Heine University, D&#xfc;sseldorf, Germany</affiliation></author><author><keyname>Krings</keyname><forenames>Sebastian</forenames><affiliation>Heinrich Heine University, D&#xfc;sseldorf, Germany</affiliation></author><author><keyname>Leuschel</keyname><forenames>Michael</forenames><affiliation>Heinrich Heine University, D&#xfc;sseldorf, Germany</affiliation></author></authors><title>Who watches the watchers: Validating the ProB Validation Tool</title><categories>cs.SE</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><acm-class>D.2.4;D.2.5</acm-class><journal-ref>EPTCS 149, 2014, pp. 16-29</journal-ref><doi>10.4204/EPTCS.149.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, ProB has moved from a tool that complemented proving, to a
development environment that is now sometimes used instead of proving for
applications, such as exhaustive model checking or data validation. This has
led to much more stringent requirements on the integrity of ProB. In this paper
we present a summary of our validation efforts for ProB, in particular within
the context of the norm EN 50128 and safety critical applications in the
railway domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6604</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6604</id><created>2014-04-26</created><authors><author><keyname>Jaume</keyname><forenames>Mathieu</forenames><affiliation>LIP6-UPMC</affiliation></author><author><keyname>Laurent</keyname><forenames>Th&#xe9;o</forenames><affiliation>UPMC</affiliation></author></authors><title>Teaching Formal Methods and Discrete Mathematics</title><categories>cs.CY cs.LO cs.SE</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 149, 2014, pp. 30-43</journal-ref><doi>10.4204/EPTCS.149.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite significant advancements in the conception of (formal) integrated
development environments, applying formal methods in software industry is still
perceived as a difficult task. To make the task easier, providing tools that
help during the development cycle is essential but we think that education of
computer scientists and software engineers is also an important challenge to
take up. Indeed, we believe that formal methods courses do not appear
sufficiently early in compter science curricula and thus are not widely used
and perceived as a valid professional skill. In this paper, we claim that
teaching formal methods could be done at the undergraduate level by mixing
formal methods and discrete mathematics courses and we illustrate such an
approach with a small develop- ment within FoCaLiZe. We also believe that this
could considerably benefit the learning of discrete mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6605</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6605</id><created>2014-04-26</created><authors><author><keyname>Cok</keyname><forenames>David R.</forenames></author><author><keyname>Johnson</keyname><forenames>Scott C.</forenames></author></authors><title>SPEEDY: An Eclipse-based IDE for invariant inference</title><categories>cs.LO cs.PL cs.SE</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 149, 2014, pp. 44-57</journal-ref><doi>10.4204/EPTCS.149.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SPEEDY is an Eclipse-based IDE for exploring techniques that assist users in
generating correct specifications, particularly including invariant inference
algorithms and tools. It integrates with several back-end tools that propose
invariants and will incorporate published algorithms for inferring object and
loop invariants. Though the architecture is language-neutral, current SPEEDY
targets C programs. Building and using SPEEDY has confirmed earlier experience
demonstrating the importance of showing and editing specifications in the IDEs
that developers customarily use, automating as much of the production and
checking of specifications as possible, and showing counterexample information
directly in the source code editing environment. As in previous work,
automation of specification checking is provided by back-end SMT solvers.
However, reducing the effort demanded of software developers using formal
methods also requires a GUI design that guides users in writing, reviewing, and
correcting specifications and automates specification inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6606</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6606</id><created>2014-04-26</created><authors><author><keyname>Doligez</keyname><forenames>Damien</forenames><affiliation>Inria</affiliation></author><author><keyname>Faure</keyname><forenames>Christ&#xe8;le</forenames><affiliation>SafeRiver</affiliation></author><author><keyname>Hardin</keyname><forenames>Th&#xe9;r&#xe8;se</forenames><affiliation>UPMC</affiliation></author><author><keyname>Maarek</keyname><forenames>Manuel</forenames><affiliation>SafeRiver</affiliation></author></authors><title>Experience in using a typed functional language for the development of a
  security application</title><categories>cs.SE cs.CR</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 149, 2014, pp. 58-63</journal-ref><doi>10.4204/EPTCS.149.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present our experience in developing a security application
using a typed functional language. We describe how the formal grounding of its
semantic and compiler have allowed for a trustworthy development and have
facilitated the fulfillment of the security specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6607</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6607</id><created>2014-04-26</created><authors><author><keyname>Pessaux</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>ENSTA ParisTech</affiliation></author></authors><title>FoCaLiZe: Inside an F-IDE</title><categories>cs.PL cs.LO</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 149, 2014, pp. 64-78</journal-ref><doi>10.4204/EPTCS.149.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For years, Integrated Development Environments have demonstrated their
usefulness in order to ease the development of software. High-level security or
safety systems require proofs of compliance to standards, based on analyses
such as code review and, increasingly nowadays, formal proofs of conformance to
specifications. This implies mixing computational and logical aspects all along
the development, which naturally raises the need for a notion of Formal IDE.
This paper examines the FoCaLiZe environment and explores the implementation
issues raised by the decision to provide a single language to express
specification properties, source code and machine-checked proofs while allowing
incremental development and code reusability. Such features create strong
dependencies between functions, properties and proofs, and impose an particular
compilation scheme, which is described here. The compilation results are
runnable OCaml code and a checkable Coq term. All these points are illustrated
through a running example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6608</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6608</id><created>2014-04-26</created><authors><author><keyname>Cok</keyname><forenames>David R.</forenames><affiliation>GrammaTech, Inc.</affiliation></author></authors><title>OpenJML: Software verification for Java 7 using JML, OpenJDK, and
  Eclipse</title><categories>cs.SE cs.LO cs.PL</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 149, 2014, pp. 79-92</journal-ref><doi>10.4204/EPTCS.149.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenJML is a tool for checking code and specifications of Java programs. We
describe our experience building the tool on the foundation of JML, OpenJDK and
Eclipse, as well as on many advances in specification-based software
verification. The implementation demonstrates the value of integrating
specification tools directly in the software development IDE and in automating
as many tasks as possible. The tool, though still in progress, has now been
used for several college-level courses on software specification and
verification and for small-scale studies on existing Java programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6609</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6609</id><created>2014-04-26</created><authors><author><keyname>Witulski</keyname><forenames>John</forenames><affiliation>Heinrich-Heine Universit&#xe4;t D&#xfc;sseldorf</affiliation></author><author><keyname>Leuschel</keyname><forenames>Michael</forenames><affiliation>Heinrich-Heine Universit&#xe4;t D&#xfc;sseldorf</affiliation></author></authors><title>Checking Computations of Formal Method Tools - A Secondary Toolchain for
  ProB</title><categories>cs.SE cs.LO cs.PL</categories><comments>In Proceedings F-IDE 2014, arXiv:1404.5785</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 149, 2014, pp. 93-105</journal-ref><doi>10.4204/EPTCS.149.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the implementation of pyB, a predicate - and expression - checker
for the B language. The tool is to be used for a secondary tool chain for data
validation and data generation, with ProB being used in the primary tool chain.
Indeed, pyB is an independent cleanroom-implementation which is used to
double-check solutions generated by ProB, an animator and model-checker for B
specifications. One of the major goals is to use ProB together with pyB to
generate reliable outputs for high-integrity safety critical applications.
Although pyB is still work in progress, the ProB/pyB toolchain has already been
successfully tested on various industrial B machines and data validation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6610</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6610</id><created>2014-04-26</created><authors><author><keyname>Peneva</keyname><forenames>Ivelina</forenames></author><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>The assertive profile of the Bulgarian students in computer science and
  computer engineering</title><categories>cs.CY</categories><journal-ref>I.J. Education and Management Engineering, 2014, 1, 1-8</journal-ref><doi>10.5815/ijeme.2014.01.01</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different points of view on the nature and content of the assertiveness are
followed in this paper. The main purpose is to study the assertive profile of
Bulgarian students in computer science and computer engineering by analyzing
the components of assertiveness. Research was performed using testing methods.
It was found that the level of expressivity of this personal quality among
subjects were above-average level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6612</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6612</id><created>2014-04-26</created><authors><author><keyname>Deodhe</keyname><forenames>Yeshwant</forenames></author><author><keyname>Jain</keyname><forenames>Swapnil</forenames></author><author><keyname>Gimonkar</keyname><forenames>Ravindra</forenames></author></authors><title>Implementation of Sensor Network using Efficient CAN Interface</title><categories>cs.OH cs.NI</categories><comments>5 PAGES PAPER</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensors monitored by centralized system, that may be used for controlling and
monitoring industrial parameters (Temp, Pressure, Speed, Torque) by using CAN
interface. In this paper we presents a comprehensive overview of controller
area networks, their architecture, protocol, and standards. Also, this paper
gives an overview of CAN applications, in both the industrial and nonindustrial
fields. Due to CAN reliability, efficiency and robustness, we also propose the
extension of CAN applications to sensor network. In this paper, a framework of
sensor network for monitoring industrial parameters is explained where sensors
are physically distributed and CAN is used to exchange system information. CAN
(Controller Area Network) is a high integrity serial bus protocol that is
designed to operate at high speeds ranging from 20kbit/s to 1Mbit/s which
provide an efficient, reliable and very economical link
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6613</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6613</id><created>2014-04-26</created><updated>2015-05-20</updated><authors><author><keyname>Guha</keyname><forenames>Shibashis</forenames></author><author><keyname>Narayan</keyname><forenames>Chinmay</forenames></author><author><keyname>Arun-Kumar</keyname><forenames>S.</forenames></author></authors><title>Reducing Clocks in Timed Automata while Preserving Bisimulation</title><categories>cs.FL cs.LO</categories><comments>28 pages including reference, 8 figures, full version of paper
  accepted in CONCUR 2014</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking timed automata becomes increasingly complex with the increase
in the number of clocks. Hence it is desirable that one constructs an automaton
with the minimum number of clocks possible. The problem of checking whether
there exists a timed automaton with a smaller number of clocks such that the
timed language accepted by the original automaton is preserved is known to be
undecidable. In this paper, we give a construction, which for any given timed
automaton produces a timed bisimilar automaton with the least number of clocks.
Further, we show that such an automaton with the minimum possible number of
clocks can be constructed in time that is doubly exponential in the number of
clocks of the original automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6614</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6614</id><created>2014-04-26</created><authors><author><keyname>Mishra</keyname><forenames>Manoj</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>The Oblivious Transfer Capacity of the Wiretapped Binary Erasure Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>This is an extended version of the paper &quot;The Oblivious Transfer
  Capacity of the Wiretapped Binary Erasure Channel&quot; to be presented at ISIT
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider oblivious transfer between Alice and Bob in the presence of an
eavesdropper Eve when there is a broadcast channel from Alice to Bob and Eve.
In addition to the secrecy constraints of Alice and Bob, Eve should not learn
the private data of Alice and Bob. When the broadcast channel consists of two
independent binary erasure channels, we derive the oblivious transfer capacity
for both 2-privacy (where the eavesdropper may collude with either party) and
1-privacy (where there are no collusions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6620</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6620</id><created>2014-04-26</created><updated>2015-11-18</updated><authors><author><keyname>Lucani</keyname><forenames>Daniel E.</forenames></author><author><keyname>Pedersen</keyname><forenames>Morten V.</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author><author><keyname>S&#xf8;rensen</keyname><forenames>Chres W.</forenames></author><author><keyname>Fitzek</keyname><forenames>Frank H. P.</forenames></author><author><keyname>Heide</keyname><forenames>Janus</forenames></author><author><keyname>Geil</keyname><forenames>Olav</forenames></author></authors><title>Fulcrum Network Codes: A Code for Fluid Allocation of Complexity</title><categories>cs.IT cs.NI math.IT</categories><comments>30 pages, 12 figures, Submitted to the IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes Fulcrum network codes, a network coding framework that
achieves three seemingly conflicting objectives: (i) to reduce the coding
coefficient overhead to almost n bits per packet in a generation of n packets;
(ii) to operate the network using only GF(2) operations at intermediate nodes
if necessary, dramatically reducing complexity in the network; (iii) to deliver
an end-to-end performance that is close to that of a high-field network coding
system for high-end receivers while simultaneously catering to low-end
receivers that decode in GF(2). As a consequence of (ii) and (iii), Fulcrum
codes have a unique trait missing so far in the network coding literature: they
provide the network with the flexibility to spread computational complexity
over different devices depending on their current load, network conditions, or
even energy targets in a decentralized way. At the core of our framework lies
the idea of precoding at the sources using an expansion field GF(2h) to
increase the number of dimensions seen by the network using a linear mapping.
Fulcrum codes can use any high-field linear code for precoding, e.g.,
Reed-Solomon, with the structure of the precode determining some of the key
features of the resulting code. For example, a systematic structure provides
the ability to manage heterogeneous receivers while using the same data stream.
Our analysis shows that the number of additional dimensions created during
precoding controls the trade-off between delay, overhead, and complexity. Our
implementation and measurements show that Fulcrum achieves similar decoding
probability as high field Random Linear Network Coding (RLNC) approaches but
with encoders/decoders that are an order of magnitude faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6626</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6626</id><created>2014-04-26</created><authors><author><keyname>Yamada</keyname><forenames>Akihisa</forenames></author><author><keyname>Kusakari</keyname><forenames>Keiichirou</forenames></author><author><keyname>Sakabe</keyname><forenames>Toshiki</forenames></author></authors><title>Nagoya Termination Tool</title><categories>cs.LO</categories><comments>12 pages, 1 figure, full version of the paper which is to appear in
  RTA/TLCA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the implementation and techniques of the Nagoya
Termination Tool, a termination prover for term rewrite systems. The main
features of the tool are: the first implementation of the weighted path order
which subsumes most of the existing reduction pairs, and the efficiency due to
the strong cooperation with external SMT solvers. We present some new ideas
that contribute to the efficiency and power of the tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6632</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6632</id><created>2014-04-26</created><updated>2015-06-02</updated><authors><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author></authors><title>Complexity of Atoms, Combinatorially</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Atoms of a (regular) language $L$ were introduced by Brzozowski and Tamm in
2011 as intersections of complemented and uncomplemented quotients of $L$. They
derived tight upper bounds on the complexity of atoms in 2013. In 2014,
Brzozowski and Davies characterized the regular languages meeting these bounds.
To achieve these results, they used the so-called &quot;atomaton&quot; of a language,
introduced by Brzozowski and Tamm in 2011. In this note we give an alternative
proof of their characterization, via a purely combinatorial approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6635</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6635</id><created>2014-04-26</created><updated>2014-07-12</updated><authors><author><keyname>Thoppe</keyname><forenames>Gugan</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek S.</forenames></author><author><keyname>Garg</keyname><forenames>Dinesh</forenames></author></authors><title>Greedy Block Coordinate Descent (GBCD) Method for High Dimensional
  Quadratic Programs</title><categories>math.OC cs.SY stat.CO</categories><comments>29 pages, 3 figures, New references added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High dimensional unconstrained quadratic programs (UQPs) involving massive
datasets are now common in application areas such as web, social networks, etc.
Unless computational resources that match up to these datasets are available,
solving such problems using classical UQP methods is very difficult. This paper
discusses alternatives. We first define high dimensional compliant (HDC)
methods for UQPs---methods that can solve high dimensional UQPs by adapting to
available computational resources. We then show that the class of block
Kaczmarz and block coordinate descent (BCD) are the only existing methods that
can be made HDC. As a possible answer to the question of the `best' amongst BCD
methods for UQP, we propose a novel greedy BCD (GBCD) method with serial,
parallel and distributed variants. Convergence rates and numerical tests
confirm that the GBCD is indeed an effective method to solve high dimensional
UQPs. In fact, it sometimes beats even the conjugate gradient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6645</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6645</id><created>2014-04-26</created><authors><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>Lu</keyname><forenames>Hsiao-feng Francis</forenames></author></authors><title>Space-Time Storage Codes for Wireless Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><msc-class>11R04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems (DSSs) have gained a lot of interest recently,
thanks to their robustness and scalability compared to single-device storage.
Majority of the related research has exclusively concerned the network layer.
At the same time, the number of users of, e.g., peer-to-peer (p2p) and
device-to-device (d2d) networks as well as proximity based services is growing
rapidly, and the mobility of users is considered more and more important. This
motivates, in contrast to the existing literature, the study of the physical
layer functionality of wireless distributed storage systems.
  In this paper, we take the first step towards protecting the storage repair
transmissions from physical layer errors when the transmission takes place over
a fading channel. To this end, we introduce the notion of a space-time storage
code, drawing together the aspects of network layer and physical layer
functionality and resulting in cross-layer robustness. It is also pointed out
that existing space-time codes are too complex to be utilized in storage
networks when the number of helpers involved is larger than the number of
receive antennas at the newcomer or data collector, hence creating a call for
less complex transmission protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6662</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6662</id><created>2014-04-26</created><authors><author><keyname>Kotagiri</keyname><forenames>Vamsi Sashank</forenames></author></authors><title>A Wireless System Using Random Residue Sequences</title><categories>cs.CR</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the architecture of wireless communication system using
random residue sequences. The basic scheme is that of spread spectrum but
instead of using PN sequences for coding, we use random residue sequences. Such
a system can provide cryptographic security whose strength would depend on the
number of code sequences being used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6664</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6664</id><created>2014-04-26</created><updated>2014-05-11</updated><authors><author><keyname>Burdon</keyname><forenames>Marc</forenames></author></authors><title>How to extract data from proprietary software database systems using
  TCP/IP?</title><categories>cs.DB</categories><comments>7 pages, 5 figures, corrected Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document is an white paper about how to connect reverse engineering and
programing skills to extract data from a proprietary implementation of a
database system to build EML-Tools for data format conversion into raw data.
This article shows how to access data of a source software system without any
interface for data conversion. We discuss how raw data can be transfered into
structural format by using XML or any other custom designed software solution.
For demonstration purposes only, we will use a CRM system called Harmony(r) by
Harmony(r) Software AG, the programing language Python and methods of computer
security, which are used to get quick access to the raw data.
  All trademarks are property of their owners, as Harmony(r) is of Harmony
Software AG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6667</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6667</id><created>2014-04-26</created><updated>2014-07-01</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Cooperative Cognitive Relaying Under Primary and Secondary Quality of
  Service Satisfaction</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper was accepted in PIMRC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new cooperative protocol which involves cooperation
between primary and secondary users. We consider a cognitive setting with one
primary user and multiple secondary users. The time resource is partitioned
into discrete time slots. Each time slot, a secondary user is scheduled for
transmission according to time division multiple access, and the remainder of
the secondary users, which we refer to as secondary relays, attempt to decode
the primary packet. Afterwards, the secondary relays employ cooperative
beamforming to forward the primary packet and to provide protection to the
secondary destination of the secondary source scheduled for transmission from
interference. We characterize the diversity-multiplexing tradeoff of the
primary source under the proposed protocol. We consider certain quality of
service for each user specified by its required throughput. The optimization
problem is stated under such condition. It is shown that the optimization
problem is linear and can be readily solved. We show that the sum of the
secondary required throughputs must be less than or equal to the probability of
correct packets reception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6673</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6673</id><created>2014-04-26</created><updated>2014-05-01</updated><authors><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Wachter</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Stability and Complexity of Minimising Probabilistic Automata</title><categories>cs.FL</categories><comments>This is the full version of an ICALP'14 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the state-minimisation problem for weighted and probabilistic
automata. We provide a numerically stable polynomial-time minimisation
algorithm for weighted automata, with guaranteed bounds on the numerical error
when run with floating-point arithmetic. Our algorithm can also be used for
&quot;lossy&quot; minimisation with bounded error. We show an application in image
compression. In the second part of the paper we study the complexity of the
minimisation problem for probabilistic automata. We prove that the problem is
NP-hard and in PSPACE, improving a recent EXPTIME-result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6674</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6674</id><created>2014-04-26</created><authors><author><keyname>Wei</keyname><forenames>Yu</forenames></author><author><keyname>Thomas</keyname><forenames>Pock</forenames></author></authors><title>A Comparison of First-order Algorithms for Machine Learning</title><categories>cs.LG</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using an optimization algorithm to solve a machine learning problem is one of
mainstreams in the field of science. In this work, we demonstrate a
comprehensive comparison of some state-of-the-art first-order optimization
algorithms for convex optimization problems in machine learning. We concentrate
on several smooth and non-smooth machine learning problems with a loss function
plus a regularizer. The overall experimental results show the superiority of
primal-dual algorithms in solving a machine learning problem from the
perspectives of the ease to construct, running time and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6681</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6681</id><created>2014-04-26</created><updated>2014-05-07</updated><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>Power Management Techniques for Data Centers: A Survey</title><categories>cs.DC</categories><comments>Keywords: Data Centers, Power Management, Low-power Design, Energy
  Efficiency, Green Computing, DVFS, Server Consolidation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With growing use of internet and exponential growth in amount of data to be
stored and processed (known as 'big data'), the size of data centers has
greatly increased. This, however, has resulted in significant increase in the
power consumption of the data centers. For this reason, managing power
consumption of data centers has become essential. In this paper, we highlight
the need of achieving energy efficiency in data centers and survey several
recent architectural techniques designed for power management of data centers.
We also present a classification of these techniques based on their
characteristics. This paper aims to provide insights into the techniques for
improving energy efficiency of data centers and encourage the designers to
invent novel solutions for managing the large power dissipation of data
centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6682</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6682</id><created>2014-04-26</created><authors><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author></authors><title>Sampling Techniques for Boolean Satisfiability</title><categories>cs.LO</categories><comments>MS Thesis submitted to Rice University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean satisfiability ({\SAT}) has played a key role in diverse areas
spanning testing, formal verification, planning, optimization, inferencing and
the like. Apart from the classical problem of checking boolean satisfiability,
the problems of generating satisfying uniformly at random, and of counting the
total number of satisfying assignments have also attracted significant
theoretical and practical interest over the years. Prior work offered heuristic
approaches with very weak or no guarantee of performance, and theoretical
approaches with proven guarantees, but poor performance in practice.
  We propose a novel approach based on limited-independence hashing that allows
us to design algorithms for both problems, with strong theoretical guarantees
and scalability extending to thousands of variables. Based on this approach, we
present two practical algorithms, {\UniformWitness}: a near uniform generator
and {\approxMC}: the first scalable approximate model counter, along with
reference implementations. Our algorithms work by issuing polynomial calls to
{\SAT} solver. We demonstrate scalability of our algorithms over a large set of
benchmarks arising from different application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6683</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6683</id><created>2014-04-26</created><authors><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Kim</keyname><forenames>Kyu-Han</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Scheduling of Multicast and Unicast Services under Limited Feedback by
  using Rateless Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many opportunistic scheduling techniques are impractical because they require
accurate channel state information (CSI) at the transmitter. In this paper, we
investigate the scheduling of unicast and multicast services in a downlink
network with a very limited amount of feedback information. Specifically,
unicast users send imperfect (or no) CSI and infrequent acknowledgements (ACKs)
to a base station, and multicast users only report infrequent ACKs to avoid
feedback implosion. We consider the use of physical-layer rateless codes, which
not only combats channel uncertainty, but also reduces the overhead of ACK
feedback. A joint scheduling and power allocation scheme is developed to
realize multiuser diversity gain for unicast service and multicast gain for
multicast service. We prove that our scheme achieves a near-optimal throughput
region. Our simulation results show that our scheme significantly improves the
network throughput over schemes employing fixed-rate codes or using only
unicast communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6687</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6687</id><created>2014-04-26</created><authors><author><keyname>Chen</keyname><forenames>Shengbo</forenames></author><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Kozat</keyname><forenames>Ulas C.</forenames></author><author><keyname>Huang</keyname><forenames>Longbo</forenames></author><author><keyname>Sinha</keyname><forenames>Prasun</forenames></author><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>When Queueing Meets Coding: Optimal-Latency Data Retrieving Scheme in
  Storage Clouds</title><categories>cs.IT cs.DC cs.NI cs.PF math.IT</categories><comments>Original accepted by IEEE Infocom 2014, 9 pages. Some statements in
  the Infocom paper are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of reducing the delay of downloading data
from cloud storage systems by leveraging multiple parallel threads, assuming
that the data has been encoded and stored in the clouds using fixed rate
forward error correction (FEC) codes with parameters (n, k). That is, each file
is divided into k equal-sized chunks, which are then expanded into n chunks
such that any k chunks out of the n are sufficient to successfully restore the
original file. The model can be depicted as a multiple-server queue with
arrivals of data retrieving requests and a server corresponding to a thread.
However, this is not a typical queueing model because a server can terminate
its operation, depending on when other servers complete their service (due to
the redundancy that is spread across the threads). Hence, to the best of our
knowledge, the analysis of this queueing model remains quite uncharted.
  Recent traces from Amazon S3 show that the time to retrieve a fixed size
chunk is random and can be approximated as a constant delay plus an i.i.d.
exponentially distributed random variable. For the tractability of the
theoretical analysis, we assume that the chunk downloading time is i.i.d.
exponentially distributed. Under this assumption, we show that any
work-conserving scheme is delay-optimal among all on-line scheduling schemes
when k = 1. When k &gt; 1, we find that a simple greedy scheme, which allocates
all available threads to the head of line request, is delay optimal among all
on-line scheduling schemes. We also provide some numerical results that point
to the limitations of the exponential assumption, and suggest further research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6688</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6688</id><created>2014-04-26</created><authors><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Lee</keyname><forenames>Sung-Ju</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Network Control without CSI using Rateless Codes for Downlink Cellular
  Systems</title><categories>cs.IT math.IT</categories><comments>technical report, 12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless network scheduling and control techniques (e.g., opportunistic
scheduling) rely heavily on access to Channel State Information (CSI). However,
obtaining this information is costly in terms of bandwidth, time, and power,
and could result in large overhead. Therefore, a critical question is how to
optimally manage network resources in the absence of such information. To that
end, we develop a cross-layer solution for downlink cellular systems with
imperfect (and possibly no) CSI at the transmitter. We use rateless codes to
resolve channel uncertainty. To keep the decoding complexity low, we explicitly
incorporate time-average block-size constraints, and aim to maximize the system
utility. The block-size of a rateless code is determined by both the network
control decisions and the unknown CSI of many time slots. Therefore, unlike
standard utility maximization problems, this problem can be viewed as a
constrained partial observed Markov decision problem (CPOMDP), which is known
to be hard due to the &quot;curse of dimensionality.&quot; However, by using a modified
Lyapunov drift method, we develop a dynamic network control scheme, which
yields a total network utility within O(1/Lav) of utility-optimal point
achieved by infinite block-size channel codes, where Lav is the enforced value
of the time-average block-size of rateless codes. This opens the door of being
able to trade complexity/delay for performance gains in the absence of accurate
CSI. Our simulation results show that the proposed scheme improves the network
throughput by up to 68% over schemes that use fixed-rate codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6690</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6690</id><created>2014-04-26</created><updated>2015-04-20</updated><authors><author><keyname>Bustin</keyname><forenames>Ronit</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>The Effect of Maximal Rate Codes on the Interfering Message Rate</title><categories>cs.IT math.IT</categories><comments>A preliminary version of this work appears in the proceedings of ISIT
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effect of &quot;good&quot;, point-to-point capacity achieving, code sequences on an
additional signal, of bounded variance, transmitted over the additive Gaussian
noise channel is examined. For such code sequences, it is shown that their
effect, in terms of mutual information, on the additional bounded variance
signal, is as if additional additive Gaussian noise has been transmitted.
Moreover, the analysis shows that for reliable communication the bounded
variance signal must be completely estimated by the receiver (i.e., the minimum
mean-square error tends to zero). This result resolves the &quot;Costa Conjecture&quot;
regarding the corner points of the two-user Gaussian interference channel for
code sequences of bounded variance, and shows that both messages must be
reliably decoded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6691</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6691</id><created>2014-04-26</created><authors><author><keyname>Schiffer</keyname><forenames>Clemens</forenames></author><author><keyname>Bredies</keyname><forenames>Kristian</forenames></author></authors><title>Sinogram constrained TV-minimization for metal artifact reduction in CT</title><categories>math.NA cs.CV physics.med-ph</categories><comments>Part of the OAGM 2014 proceedings (arXiv:1404.3538)</comments><report-no>OAGM/2014/13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for reducing metal artifacts in X-ray computed tomography (CT)
images is presented. It bases on the solution of a convex optimization problem
with inequality constraints on the sinogram, and total variation regularization
for the reconstructed image. The Chambolle-Pock algorithm is used to
numerically solve the discretized version of the optimization problem. As proof
of concept we present and discuss numerical results for synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6694</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6694</id><created>2014-04-26</created><authors><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author><author><keyname>Maculan</keyname><forenames>Nelson</forenames></author></authors><title>A Decomposition Algorithm for Nested Resource Allocation Problems</title><categories>cs.DS math.OC</categories><comments>Working Paper -- MIT, 23 pages</comments><msc-class>90C25, 52A41, 90B06, 90B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an exact polynomial algorithm for a resource allocation problem
with convex costs and constraints on partial sums of resource consumptions, in
the presence of either continuous or integer variables. No assumption of strict
convexity or differentiability is needed. The method solves a hierarchy of
resource allocation subproblems, whose solutions are used to convert
constraints on sums of resources into bounds for separate variables at higher
levels. The resulting time complexity for the integer problem is $O(n \log m
\log (B/n))$, and the complexity of obtaining an $\epsilon$-approximate
solution for the continuous case is $O(n \log m \log (B/\epsilon))$, $n$ being
the number of variables, $m$ the number of ascending constraints (such that $m
&lt; n$), $\epsilon$ a desired precision, and $B$ the total resource. This
algorithm attains the best-known complexity when $m = n$, and improves it when
$\log m = o(\log n)$. Extensive experimental analyses are conducted with four
recent algorithms on various continuous problems issued from theory and
practice. The proposed method achieves a higher performance than previous
algorithms, addressing all problems with up to one million variables in less
than one minute on a modern computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6696</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6696</id><created>2014-04-26</created><authors><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author><author><keyname>Battarra</keyname><forenames>Maria</forenames></author><author><keyname>Subramanian</keyname><forenames>Anand</forenames></author><author><keyname>Erdo&#x1e7;an</keyname><forenames>G&#xfc;ne&#x15f;</forenames></author></authors><title>Hybrid Metaheuristics for the Clustered Vehicle Routing Problem</title><categories>cs.AI</categories><comments>Working Paper, MIT -- 22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Clustered Vehicle Routing Problem (CluVRP) is a variant of the
Capacitated Vehicle Routing Problem in which customers are grouped into
clusters. Each cluster has to be visited once, and a vehicle entering a cluster
cannot leave it until all customers have been visited. This article presents
two alternative hybrid metaheuristic algorithms for the CluVRP. The first
algorithm is based on an Iterated Local Search algorithm, in which only
feasible solutions are explored and problem-specific local search moves are
utilized. The second algorithm is a Hybrid Genetic Search, for which the
shortest Hamiltonian path between each pair of vertices within each cluster
should be precomputed. Using this information, a sequence of clusters can be
used as a solution representation and large neighborhoods can be efficiently
explored by means of bi-directional dynamic programming, sequence
concatenations, by using appropriate data structures. Extensive computational
experiments are performed on benchmark instances from the literature, as well
as new large scale ones. Recommendations on promising algorithm choices are
provided relatively to average cluster size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6699</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6699</id><created>2014-04-26</created><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Simari</keyname><forenames>Gerardo I.</forenames></author><author><keyname>Moores</keyname><forenames>Geoffrey</forenames></author><author><keyname>Parsons</keyname><forenames>Simon</forenames></author><author><keyname>Falappa</keyname><forenames>Marcelo A.</forenames></author></authors><title>An Argumentation-Based Framework to Address the Attribution Problem in
  Cyber-Warfare</title><categories>cs.CR cs.AI cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.1475</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Attributing a cyber-operation through the use of multiple pieces of technical
evidence (i.e., malware reverse-engineering and source tracking) and
conventional intelligence sources (i.e., human or signals intelligence) is a
difficult problem not only due to the effort required to obtain evidence, but
the ease with which an adversary can plant false evidence. In this paper, we
introduce a formal reasoning system called the InCA (Intelligent Cyber
Attribution) framework that is designed to aid an analyst in the attribution of
a cyber-operation even when the available information is conflicting and/or
uncertain. Our approach combines argumentation-based reasoning, logic
programming, and probabilistic models to not only attribute an operation but
also explain to the analyst why the system reaches its conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6700</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6700</id><created>2014-04-26</created><authors><author><keyname>Wang</keyname><forenames>Tong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Schmeink</keyname><forenames>Anke</forenames></author></authors><title>Alternating Optimization Techniques for Power Allocation and Receiver
  Design in Multihop Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>10 figures, 13 pages. IEEE Transactions on Vehicular Technology,
  2014. arXiv admin note: text overlap with arXiv:1303.3849</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multihop wireless sensor network with multiple
relay nodes for each hop where the amplify-and-forward scheme is employed. We
present algorithmic strategies to jointly design linear receivers and the power
allocation parameters via an alternating optimization approach subject to
different power constraints which include global, local and individual ones.
Two design criteria are considered: the first one minimizes the mean-square
error and the second one maximizes the sum-rate of the wireless sensor network.
We derive constrained minimum mean-square error and constrained maximum
sum-rate expressions for the linear receivers and the power allocation
parameters that contain the optimal complex amplification coefficients for each
relay node. An analysis of the computational complexity and the convergence of
the algorithms is also presented. Computer simulations show good performance of
our proposed methods in terms of bit error rate and sum-rate compared to the
method with equal power allocation and an existing power allocation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6701</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6701</id><created>2014-04-26</created><updated>2015-01-22</updated><authors><author><keyname>Kosut</keyname><forenames>Oliver</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Equivalence for Networks with Adversarial State</title><categories>cs.IT math.IT</categories><comments>29 pages, 3 figures. Submitted to IEEE Transactions in Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of finding the capacity of noisy networks with either
independent point-to-point compound channels (CC) or arbitrarily varying
channels (AVC). These channels model the presence of a Byzantine adversary
which controls a subset of links or nodes in the network. We derive equivalence
results showing that these point-to-point channels with state can be replaced
by noiseless bit-pipes without changing the network capacity region. Exact
equivalence results are found for the CC model, and for some instances of the
AVC, including all nonsymmetrizable AVCs. These results show that a feedback
path between the output and input of a CC can increase the equivalent capacity,
and that if common randomness can be established between the terminals of an
AVC (either by feedback, a forward path, or via a third-party node), then again
the equivalent capacity can increase. This leads to an observation that
deleting an edge of arbitrarily small capacity can cause a significant change
in network capacity. We also analyze an example involving an AVC for which no
fixed-capacity bit-pipe is equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6702</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6702</id><created>2014-04-26</created><authors><author><keyname>Koyejo</keyname><forenames>Oluwasanmi</forenames></author><author><keyname>Lee</keyname><forenames>Cheng</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author></authors><title>A Constrained Matrix-Variate Gaussian Process for Transposable Data</title><categories>stat.ML cs.LG</categories><comments>23 pages, Preliminary version, Accepted for publication in Machine
  Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transposable data represents interactions among two sets of entities, and are
typically represented as a matrix containing the known interaction values.
Additional side information may consist of feature vectors specific to entities
corresponding to the rows and/or columns of such a matrix. Further information
may also be available in the form of interactions or hierarchies among entities
along the same mode (axis). We propose a novel approach for modeling
transposable data with missing interactions given additional side information.
The interactions are modeled as noisy observations from a latent noise free
matrix generated from a matrix-variate Gaussian process. The construction of
row and column covariances using side information provides a flexible mechanism
for specifying a-priori knowledge of the row and column correlations in the
data. Further, the use of such a prior combined with the side information
enables predictions for new rows and columns not observed in the training data.
In this work, we combine the matrix-variate Gaussian process model with low
rank constraints. The constrained Gaussian process approach is applied to the
prediction of hidden associations between genes and diseases using a small set
of observed associations as well as prior covariances induced by gene-gene
interaction networks and disease ontologies. The proposed approach is also
applied to recommender systems data which involves predicting the item ratings
of users using known associations as well as prior covariances induced by
social networks. We present experimental results that highlight the performance
of constrained matrix-variate Gaussian process as compared to state of the art
approaches in each domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6719</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6719</id><created>2014-04-27</created><authors><author><keyname>Marandi</keyname><forenames>Parisa Jalili</forenames></author><author><keyname>Benz</keyname><forenames>Samuel</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author><author><keyname>Birman</keyname><forenames>Ken</forenames></author></authors><title>Practical Experience Report: The Performance of Paxos in the Cloud</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This experience report presents the results of an extensive performance
evaluation conducted using four open-source implementations of Paxos deployed
in Amazon's EC2. Paxos is a fundamental algorithm for building fault-tolerant
services, at the core of state-machine replication. Implementations of Paxos
are currently used in many prototypes and production systems in both academia
and industry. Although all protocols surveyed in the paper implement Paxos,
they are optimized in a number of different ways, resulting in very different
behavior, as we show in the paper. We have considered a variety of
configurations and failure-free and faulty executions. In addition to reporting
our findings, we propose and assess additional optimizations to existing
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6721</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6721</id><created>2014-04-27</created><authors><author><keyname>Marandi</keyname><forenames>Parisa Jalili</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author></authors><title>Optimistic Parallel State-Machine Replication</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-machine replication, a fundamental approach to fault tolerance,
requires replicas to execute commands deterministically, which usually results
in sequential execution of commands. Sequential execution limits performance
and underuses servers, which are increasingly parallel (i.e., multicore). To
narrow the gap between state-machine replication requirements and the
characteristics of modern servers, researchers have recently come up with
alternative execution models. This paper surveys existing approaches to
parallel state-machine replication and proposes a novel optimistic protocol
that inherits the scalable features of previous techniques. Using a replicated
B+-tree service, we demonstrate in the paper that our protocol outperforms the
most efficient techniques by a factor of 2.4 times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6723</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6723</id><created>2014-04-27</created><updated>2015-05-07</updated><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Subspace Codes based on Graph Matchings, Ferrers Diagrams and Pending
  Blocks</title><categories>cs.IT math.IT</categories><comments>Parts of this work were presented at ISIT 2013 in Istanbul, Turkey.
  To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides new constructions and lower bounds for subspace codes,
using Ferrers diagram rank-metric codes from matchings of the complete graph
and pending blocks. We present different constructions for constant dimension
codes with minimum injection distance $2$ or $k-1$, where $k$ is the constant
dimension. Furthermore, we present a construction of new codes from old codes
for any minimum distance. Then we construct non-constant dimension codes from
these codes. The examples of codes obtained by these constructions are the
largest known codes for the given parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6724</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6724</id><created>2014-04-27</created><updated>2014-05-01</updated><authors><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Approximately Minwise Independence with Twisted Tabulation</title><categories>cs.DS</categories><comments>To appear in Proceedings of SWAT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random hash function $h$ is $\varepsilon$-minwise if for any set $S$,
$|S|=n$, and element $x\in S$, $\Pr[h(x)=\min h(S)]=(1\pm\varepsilon)/n$.
Minwise hash functions with low bias $\varepsilon$ have widespread applications
within similarity estimation.
  Hashing from a universe $[u]$, the twisted tabulation hashing of
P\v{a}tra\c{s}cu and Thorup [SODA'13] makes $c=O(1)$ lookups in tables of size
$u^{1/c}$. Twisted tabulation was invented to get good concentration for
hashing based sampling. Here we show that twisted tabulation yields $\tilde
O(1/u^{1/c})$-minwise hashing.
  In the classic independence paradigm of Wegman and Carter [FOCS'79] $\tilde
O(1/u^{1/c})$-minwise hashing requires $\Omega(\log u)$-independence [Indyk
SODA'99]. P\v{a}tra\c{s}cu and Thorup [STOC'11] had shown that simple
tabulation, using same space and lookups yields $\tilde O(1/n^{1/c})$-minwise
independence, which is good for large sets, but useless for small sets. Our
analysis uses some of the same methods, but is much cleaner bypassing a
complicated induction argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6727</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6727</id><created>2014-04-27</created><authors><author><keyname>Bateni</keyname><forenames>MohammadHossein</forenames></author><author><keyname>Feldman</keyname><forenames>Jon</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Wong</keyname><forenames>Sam Chiu-wai</forenames></author></authors><title>Multiplicative Bidding in Online Advertising</title><categories>cs.DS</categories><comments>25 pages; accepted to EC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we initiate the study of the multiplicative bidding language
adopted by major Internet search companies. In multiplicative bidding, the
effective bid on a particular search auction is the product of a base bid and
bid adjustments that are dependent on features of the search (for example, the
geographic location of the user, or the platform on which the search is
conducted). We consider the task faced by the advertiser when setting these bid
adjustments, and establish a foundational optimization problem that captures
the core difficulty of bidding under this language. We give matching
algorithmic and approximation hardness results for this problem; these results
are against an information-theoretic bound, and thus have implications on the
power of the multiplicative bidding language itself. Inspired by empirical
studies of search engine price data, we then codify the relevant restrictions
of the problem, and give further algorithmic and hardness results. Our main
technical contribution is an $O(\log n)$-approximation for the case of
multiplicative prices and monotone values. We also provide empirical
validations of our problem restrictions, and test our algorithms on real data
against natural benchmarks. Our experiments show that they perform favorably
compared with the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6731</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6731</id><created>2014-04-27</created><authors><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author></authors><title>Synchronizing automata with random inputs</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of synchronization of automata with random inputs. We
present a series of automata such that the expected number of steps until
synchronization is exponential in the number of states. At the same time, we
show that the expected number of letters to synchronize any pair of the famous
Cerny automata is at most cubic in the number of states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6736</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6736</id><created>2014-04-27</created><authors><author><keyname>Lu</keyname><forenames>Can-Yi</forenames></author><author><keyname>Min</keyname><forenames>Hai</forenames></author><author><keyname>Zhao</keyname><forenames>Zhong-Qiu</forenames></author><author><keyname>Zhu</keyname><forenames>Lin</forenames></author><author><keyname>Huang</keyname><forenames>De-Shuang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Robust and Efficient Subspace Segmentation via Least Squares Regression</title><categories>cs.CV</categories><comments>European Conference on Computer Vision, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the subspace segmentation problem which aims to segment
data drawn from a union of multiple linear subspaces. Recent works by using
sparse representation, low rank representation and their extensions attract
much attention. If the subspaces from which the data drawn are independent or
orthogonal, they are able to obtain a block diagonal affinity matrix, which
usually leads to a correct segmentation. The main differences among them are
their objective functions. We theoretically show that if the objective function
satisfies some conditions, and the data are sufficiently drawn from independent
subspaces, the obtained affinity matrix is always block diagonal. Furthermore,
the data sampling can be insufficient if the subspaces are orthogonal. Some
existing methods are all special cases. Then we present the Least Squares
Regression (LSR) method for subspace segmentation. It takes advantage of data
correlation, which is common in real data. LSR encourages a grouping effect
which tends to group highly correlated data together. Experimental results on
the Hopkins 155 database and Extended Yale Database B show that our method
significantly outperforms state-of-the-art methods. Beyond segmentation
accuracy, all experiments demonstrate that LSR is much more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6737</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6737</id><created>2014-04-27</created><authors><author><keyname>Kishk</keyname><forenames>Mustafa A.</forenames></author><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author></authors><title>On the Capacity of the Underwater Acoustic Channel with Dominant Noise
  Sources</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an upper-bound for the capacity of the underwater
acoustic (UWA) channel with dominant noise sources and generalized fading
environments. Previous works have shown that UWA channel noise statistics are
not necessary Gaussian, especially in a shallow water environment which is
dominated by impulsive noise sources. In this case, noise is best represented
by the Generalized Gaussian (GG) noise model with a shaping parameter $\beta$.
On the other hand, fading in the UWA channel is generally represented using an
$\alpha$-$\mu$ distribution, which is a generalization of a wide range of well
known fading distributions. We show that the Additive White Generalized
Gaussian Noise (AWGGN) channel capacity is upper bounded by the AWGN capacity
in addition to a constant gap of $\frac{1}{2} \log \left(\frac{\beta^{2} \pi
e^{1-\frac{2}{\beta}} \Gamma(\frac{3}{\beta})}{2(\Gamma(\frac{1}{\beta}))^{3}}
\right)$ bits. The same gap also exists when characterizing the ergodic
capacity of AWGGN channels with $\alpha$-$\mu$ fading compared to the faded
AWGN channel capacity. We justify our results by revisiting the sphere-packing
problem, which represents a geometric interpertation of the channel capacity.
Moreover, UWA channel secrecy rates are characterized and the dependency of UWA
channel secrecy on the shaping parameters of the legitimate and eavesdropper
channels is highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6743</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6743</id><created>2014-04-27</created><authors><author><keyname>Kandl</keyname><forenames>Susanne</forenames></author><author><keyname>Elshuber</keyname><forenames>Martin</forenames></author></authors><title>A Formal Approach to System Integration Testing</title><categories>cs.SE</categories><comments>EDCC-2014, Fast-Abstracts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System integration testing is the process of testing a system by the stepwise
integration of sub-components. Usually these sub-components are already
verified to guarantee their correct functional behavior. By integration of
these verified subcomponents into the overall system, emergent behavior may
occur, i.e. behavior that evolves by the assembling of the subcomponents. For
system integration testing, both, the correct functional behavior of the
overall system, and, the proper functioning of the sub-components in their
system environment, have to be verified. In this work we present the idea of an
approach for system integration testing based on formal verification. The
system components are modeled in SystemC. In a first step these components are
formally verified. Then a model of the overall system is built. In a second
step this system model is formally verified. The novelty of this approach is
given by two aspects: First, up to now the available verification frameworks
for SystemC-models are more a proof of concept than really applicable to real
industrial case studies. Secondly, although formal verification techniques are
a common technique for the verification of software and hardware, by now they
have only marginally considered for system integration testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6745</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6745</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Adaptable Menu: A TRIZ based analysis</title><categories>cs.HC</categories><comments>10 pages, 6 figures</comments><journal-ref>Mishra, Umakant, Inventions on Adaptable Menu: A TRIZ Based
  Analysis. December 6, 2006, Available at SSRN:
  http://ssrn.com/abstract=949236</journal-ref><doi>10.2139/ssrn.949236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The menu is one of the most widely used elements of a graphical user
interface. The objective of a menu system is to provide various commands and
functions to the user in an easy way so that the user can just select the
desired operation from a given list instead of typing a complex command in the
command prompt.
  In a conventional menu system the menu items or options are hard-coded in the
computer program. The programmer or developer composes menu items at the time
of development. The developer tries to include all options that he feels may be
required by the user in future. Although the items are decided from
&quot;requirement analysis&quot; and other studies, it is difficult to know the exact
need of a user at a future period of time. This leads to inclusion of a lot of
items in the menu, which leads to user confusion and frustration.
  Thus there is a need for adaptable menu that can be changed according to user
requirement. The items of the adaptable menu should change from user to user
and from time to time depending on the program context and likelihood of user
selection.
  This article defines the Ideal Final Result of a dropdown menu system,
defines the desirable functions of an adaptable menu, finds and solves the
contradictions faced in achieving the desirable functions, and illustrates six
selected cases on adaptable menu from US patent database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6747</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6747</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 inventions on Improving Toolbars: A TRIZ based analysis</title><categories>cs.HC</categories><comments>15 pages</comments><journal-ref>Mishra, Umakant, 10 Inventions on Improving Toolbars: A TRIZ Based
  Analysis (September 7, 2007). Available at SSRN:
  http://ssrn.com/abstract=1264683</journal-ref><doi>10.2139/ssrn.1264683</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Toolbar is one among the popular elements of a graphical user interface. The
other popular elements of graphical user interface are buttons, menu,
scrollbar, dialog box etc., all of which provide easy access to various
functions of a GUI System.
  A toolbar often does a similar function as the menu but with certain
differences. A menu has the advantage of holding a large number of items
without needing any additional screen space. In contrast, each button on the
toolbar permanently occupies some space on the screen. It is not possible to
implement large number of functions through a toolbar, as they will occupy more
and more valuable screen space. However, the toolbar has an advantage as it
gives a single click access to any function unlike a menu system where the user
has to navigate through sub-menus to ultimate discover the item he is looking
for.
  This article explores the desired features of a toolbar and the ideal
features of an advanced toolbar. The contradictions are described from a TRIZ
perspective and solutions are derived using Inventive principles. Besides the
article illustrates 10 inventions on improving Toolbars selected from US patent
database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6750</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6750</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on Command Buttons in a Graphical User Interface</title><categories>cs.HC</categories><comments>12 pages</comments><journal-ref>Mishra, Umakant, 10 Inventions on Command Buttons in a Graphical
  User Interface, (December 6, 2006) Available at SSRN:
  http://ssrn.com/abstract=949240</journal-ref><doi>10.2139/ssrn.949240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A command button may contain a textual label or a graphic image or both. It
may be static or animated. There can be many different features to make a
command button attractive and effective. As command button is a typical GUI
element, most improvement on GUI in general will also be applicable to command
buttons. Besides, there are also inventions to improve various aspects of
command buttons in specific. This article illustrates 10 selected inventions
from US patent database. Each invention is followed by a TRIZ based analysis in
brief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6752</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6752</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on scrolling and scrollbars in Graphical User Interface</title><categories>cs.HC</categories><comments>13 pages</comments><journal-ref>Mishra, Umakant, 10 Inventions on Scrolling and Scrollbars in
  Graphical User Interface, (December 6, 2006), Available at SSRN:
  http://ssrn.com/abstract=949243</journal-ref><doi>10.2139/ssrn.949243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scrolling mechanism is used to display and edit large documents in a limited
screen space or window. The scrolling mechanism may include a vertical scroll
bar or a horizontal scroll bar or both to move the contents of the documents up
and down or left and right. There may be navigation buttons on the screen
representing the navigation keys on the keyboard. The user can click these
buttons to scroll the screen.
  There may be very different methods of scrolling such as by using the
&quot;thumb&quot;, as popularly used with a PDF document. Scrolling may be achieved
through eyeball tracking in a hands-free environment where the user does not
have hands or wants to use hands for other activities. This article illustrates
ten interesting inventions on scrolling selected from US patent database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6754</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6754</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on dialog boxes used in GUI</title><categories>cs.HC</categories><comments>Available at SSRN: http://ssrn.com/abstract=949247. Mishra, Umakant,
  Inventions on Dialog Boxes Used in GUI, (December 6, 2006)</comments><doi>10.2139/ssrn.949247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dialog boxes are useful in case of displaying warnings, errors,
confirmations etc. in special situations. A typical dialog box is displayed in
a small window with some text message along with a few options for the user to
select. However, there are certain difficulties associated in programming and
implementing a conventional dialog box, such as, severe programming effort,
rigidity of the hard coded message, obscuring screen space and so on. There is
a need to overcome these difficulties of the dialog box to make them more
efficient and useful.
  The modality of the dialog boxes also creates some limitations. While modal
dialog boxes needs to be closed explicitly by the user, modeless dialog boxes
can grow in number and become difficult to control. Thus, an ideal dialog box
should be deprived of all the above-mentioned drawbacks. The dialog box should
not obscure the screen. The user should be able open multiple dialog boxes but
without obscuring the screen.
  This article analyses 5 interesting inventions on dialog boxes selected from
US Patent database. Each invention tries to overcome some limitations of a
conventional dialog box and provides some innovative features. Each solution is
also analyzed from a TRIZ perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6756</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6756</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Tree Navigators used in Graphical User Interface</title><categories>cs.HC</categories><comments>7 pages, 4 figures. Umakant Mishra, Inventions on Tree Navigators
  Used in Graphical User Interface. (December 6, 2006), Available at SSRN:
  http://ssrn.com/abstract=949244</comments><doi>10.2139/ssrn.949244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tree view or tree navigator is used to display hierarchical data organized
in the form of a tree. In a tree structure there are parent and child nodes.
The child nodes may further have descendants to n levels.
  There are many methods to make the navigation easy. Some of these are
expanding and collapsing branches, splitting the tree, displaying a parent node
in a separate tree, zooming branches, scrolling in various directions etc. It
is still a difficult exercise to handle large trees efficiently. The effort
still continues to manage large number of nodes with faster speed, greater
control, user friendliness and aesthetics.
  This article illustrates five inventions on tree navigators selected from US
patent database. Each of them tries to solve various problems relating to the
tree navigator in different ways. Each invention is also analyzed from a TRIZ
perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6757</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6757</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on expressing emotions In Graphical User Interface</title><categories>cs.HC</categories><comments>7 pages, 4 figures. Umakant Mishra, Inventions on Expressing Emotions
  in Graphical User Interface, (December 6, 2006), Available at SSRN:
  http://ssrn.com/abstract=949250</comments><doi>10.2139/ssrn.949250</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional GUI is more mechanical and does not recognize or communicate
emotions. The modern GUIs are trying to infer the likely emotional state and
personality of the user and communicate through a corresponding emotional
state.
  Emotions are expressed in graphical icons, sounds, pictures and other means.
The emotions are found to be useful in especially in communication software,
interactive learning systems, robotics and other adaptive environments. Various
mechanisms have been developed to express emotions through graphical user
interfaces. This article illustrates some interesting inventions selected from
US patent database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6761</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6761</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on GUI for Touch Sensitive Screens</title><categories>cs.HC</categories><comments>6 pages, 4 figures</comments><journal-ref>Umakant Mishra, Inventions on GUI for Touch Sensitive Screens
  (September 7, 2007). Available at SSRN: http://ssrn.com/abstract=1264684 or
  http://dx.doi.org/10.2139/ssrn.1264684</journal-ref><doi>10.2139/ssrn.1264684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A touch sensitive screen displays the information on the screen and also
receives the input by sensing a user's touch on the same screen. This mechanism
facilitates system interaction directly through the screen without needing a
mouse or keyboard. This method has the advantage to make the system compact by
removing keyboard, mouse and similar interactive device.
  However there are certain difficulties to implement a touch screen interface.
The display screens of portable devices are becoming smaller thereby leaving
lesser space for display of data, menu or touch screen interaction. Besides
some screens need to display so much of information that they hardly can afford
any space to display touch screen buttons. This article illustrates various
inventions which have successfully eliminated these difficulties by applying
appropriate Inventive principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6763</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6763</id><created>2014-04-27</created><updated>2014-05-08</updated><authors><author><keyname>Emek</keyname><forenames>Yuval</forenames></author><author><keyname>Rosen</keyname><forenames>Adi</forenames></author></authors><title>Semi-Streaming Set Cover</title><categories>cs.DS</categories><comments>Full version of the extended abstract that will appear in Proceedings
  of ICALP 2014 track A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the set cover problem under the semi-streaming model. The
underlying set system is formalized in terms of a hypergraph $G = (V, E)$ whose
edges arrive one-by-one and the goal is to construct an edge cover $F \subseteq
E$ with the objective of minimizing the cardinality (or cost in the weighted
case) of $F$. We consider a parameterized relaxation of this problem, where
given some $0 \leq \epsilon &lt; 1$, the goal is to construct an edge $(1 -
\epsilon)$-cover, namely, a subset of edges incident to all but an
$\epsilon$-fraction of the vertices (or their benefit in the weighted case).
The key limitation imposed on the algorithm is that its space is limited to
(poly)logarithmically many bits per vertex.
  Our main result is an asymptotically tight trade-off between $\epsilon$ and
the approximation ratio: We design a semi-streaming algorithm that on input
graph $G$, constructs a succinct data structure $\mathcal{D}$ such that for
every $0 \leq \epsilon &lt; 1$, an edge $(1 - \epsilon)$-cover that approximates
the optimal edge \mbox{($1$-)cover} within a factor of $f(\epsilon, n)$ can be
extracted from $\mathcal{D}$ (efficiently and with no additional space
requirements), where \[ f(\epsilon, n) = \left\{ \begin{array}{ll} O (1 /
\epsilon), &amp; \text{if } \epsilon &gt; 1 / \sqrt{n} \\ O (\sqrt{n}), &amp;
\text{otherwise} \end{array} \right. \, . \] In particular for the traditional
set cover problem we obtain an $O(\sqrt{n})$-approximation. This algorithm is
proved to be best possible by establishing a family (parameterized by
$\epsilon$) of matching lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6764</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6764</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on presenting textual items in Graphical User Interface</title><categories>cs.HC</categories><comments>9 pages, 6 figures</comments><journal-ref>Umakant Mishra, Inventions on Presenting Textual Items in
  Graphical User Interface (September 7, 2007). Available at SSRN:
  http://ssrn.com/abstract=1264685 or http://dx.doi.org/10.2139/ssrn.1264685</journal-ref><doi>10.2139/ssrn.1264685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although a GUI largely replaces textual descriptions by graphical icons, the
textual items are not completely removed. The textual items are inevitably used
in window titles, message boxes, help items, menu items and popup items.
Textual items are necessary for communicating messages that are beyond the
limitation of graphical messages.
  However, it is necessary to harness the textual items on the graphical
interface in such a way that they complement each other to produce the best
effect. One has to keep various considerations in mind while applying textual
items in Graphical User Interface. This article illustrates a few inventions on
presenting textual items in a Graphical user Interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6765</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6765</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on GUI for Eye Cursor Controls Systems</title><categories>cs.HC</categories><comments>6 pages, 4 figures</comments><journal-ref>Mishra, Umakant, Inventions on GUI for Eye Cursor Control Systems
  (September 7, 2007), Available at SSRN: http://ssrn.com/abstract=1264687 or
  http://dx.doi.org/10.2139/ssrn.1264687</journal-ref><doi>10.2139/ssrn.1264687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating a GUI through eyeball is a complex mechanism and not used as often
as mouse or trackball. But there are situations where eye-mouse devices can
play a tremendous role especially where the hands of the user are not available
or busy to perform other activities. The difficulties of implementing an
eye-cursor control system are many. The article illustrates some inventions on
eye-cursor control system, which attempt to eliminate the difficulties of the
prior art mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6766</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6766</id><created>2014-04-27</created><updated>2015-10-11</updated><authors><author><keyname>Ghosh</keyname><forenames>Arnob</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author></authors><title>Quality Sensitive Price Competition in Spectrum Oligopoly: Part II</title><categories>cs.GT cs.NI</categories><comments>Added several results from the other versions. Submitted to IEEE
  Transactions on Information Theory. 63 pages one column, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a spectrum oligopoly market where each primary seeks to sell
secondary access to its channel at multiple locations. Transmission qualities
of a channel evolve randomly. Each primary needs to select a price and a set of
non-interfering locations (which is an independent set in the conflict graph of
the region) at which to offer its channel without knowing the transmission
qualities of the channels of its competitors. We formulate the above problem as
a non-cooperative game. We consider two scenarios-i) when the region is small,
ii) when the region is large. In the first setting, we focus on a class of
conflict graphs, known as mean valid graphs which commonly arise when the
region is small. We explicitly compute a symmetric Nash equilibrium (NE); the
NE is threshold type in that primaries only choose independent set whose
cardinality is greater than a certain threshold. The threshold on the
cardinality increases with increase in quality of the channel on sale. We show
that the symmetric NE strategy profile is unique in a special class of conflict
graphs (linear graph). In the second setting, we consider node symmetric
conflict graphs which arises when the number of locations is large
(potentially, infinite). We explicitly compute a symmetric NE that randomizes
equally among the maximum independent sets at a given channel state vector. In
the NE a primary only selects the maximum independent set at a given channel
state vector. We show that the two symmetric NEs computed in two settings
exhibit important structural difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6771</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6771</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on using sound and speech in GUI</title><categories>cs.HC</categories><comments>9 pages, 5 figures. Mishra, Umakant, Inventions on Using Sound and
  Speech in GUI (September 7, 2007). Available at SSRN:
  http://ssrn.com/abstract=1264688 or http://dx.doi.org/10.2139/ssrn.1264688</comments><doi>10.2139/ssrn.1264688</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice Recognition (VR) facilitates a human interaction with the machine. VR
may be used to replace the manual task of pushing buttons on a wireless
telephone keypad. This is particularly useful when the hands of the user are
busy with other activities like driving a car.
  However, the VRS system has several limitations. The VRS requires lot of
training and customization in order to be effectively used by individual users
as each individual falls into different voice patterns. Besides the voice
interface is complex and is not as reliable as the keyboard or mouse. This
article illustrates some interesting inventions on using sound and voice in
Graphical User Interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6772</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6772</id><created>2014-04-27</created><authors><author><keyname>Kopetz</keyname><forenames>Hermann</forenames></author></authors><title>Why a Global Time is Needed in a Dependable SoS</title><categories>cs.DC</categories><comments>EDCC-2014, EDSoS-2014, System-of-Systems, global time, clock,
  synchronization, sparse time base, error detection, dependability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system-of-systems (SoS) is a large information processing system formed by
the integration of autonomous computer systems (called constituent systems,
CS), physical machines and humans for the purpose of providing new synergistic
services and/or more efficient economic processes. In a number of applications,
e.g robotics, the autonomous CSs must coordinate their actions in the temporal
domain to realize the desired objectives. In this paper we argue that the
introduction of a proper global physical time establishes a shared view about
the progress of physical time and helps to realize the temporal coordination of
the autonomous CSs. The available global time can also be used to simplify the
solution of many challenging problems within the SoS, such as distributed
resource allocation, and helps to improve the dependability and fault-tolerance
of the SoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6773</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6773</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Using Colors in Graphical User Interfaces</title><categories>cs.HC</categories><comments>7 pages, 3 figures. Umakant Mishra, Inventions on Color Selections in
  Graphical User Interfaces (September 7, 2007), Available at SSRN:
  http://ssrn.com/abstract=1264689 or http://dx.doi.org/10.2139/ssrn.1264689</comments><doi>10.2139/ssrn.1264689</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color is an important aspect of any graphical user interface (GUI). Color is
used to make a GUI attractive and meaningful. But there are difficulties in
using colors too. Improper use of color can result in adverse effects. Wrong
colors at wrong place can make the GUI look clumsy and confusing. Apart from
the aesthetics issues there are many other issues involved with colors too.
  One of the contradictions relating to usage of color is &quot;The color of the GUI
should be customizable to suit user preference. But at the same time it should
not be customizable, as that would cause annoyance and confusion to other
users.&quot;
  Another contraction relating to using color is &quot;The user should be displayed
all 16 million colors to select the desired color precisely, But from another
angle the user should not be displayed all 16 million colors as that would
create confusion and difficulty in selection.&quot;
  This article analyses some inventions selected from US Patent database and
illustrates how the inventors have been able to solve various contradictions
relating to usage of colors in Graphical User Interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6776</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6776</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on GUI Aesthetics</title><categories>cs.HC</categories><comments>9 pages, 7 figures</comments><journal-ref>Umakant Mishra, Inventions on GUI Aesthetics (September 7, 2007).
  Available at SSRN: http://ssrn.com/abstract=1264690 or
  http://dx.doi.org/10.2139/ssrn.1264690</journal-ref><doi>10.2139/ssrn.1264690</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aesthetics or &quot;look and feel&quot; is one of the most important features of any
graphical user interface. Better aesthetics makes the interface user-friendlier
and more popular. Better aesthetics helps the user to understand the meaning of
various components and memorize the navigation paths. A better look and feel
ultimately makes a GUI more efficient and effective. Various methods are
adopted to improve the aesthetics of a GUI, such as, by using colors, using 3D
graphics, using pictorial icons, using sound etc.
  It is important to provide links to all the important features on a desktop
or on a quick access panel. But too many icons or buttons sometimes creates
confusion. Hence it is important to restrict the temptation of putting
everything on the first screen or load the rarely used buttons on the toolbar.
One should ensure that the aesthetics of a GUI is not compromising with its
accessibility and other important features. This article illustrates some
inventions made on GUI aesthetics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6779</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6779</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on selecting GUI elements</title><categories>cs.HC</categories><comments>8 pages, 5 figures</comments><journal-ref>Mishra, Umakant, Inventions on Selecting GUI Elements (September
  7, 2007), Available at SSRN: http://ssrn.com/abstract=1264692 or
  http://dx.doi.org/10.2139/ssrn.1264692</journal-ref><doi>10.2139/ssrn.1264692</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selecting an object or element is a fundamental operation in any graphic user
interface. It is necessary to select an object before doing any operation (such
as, dragging, copying, opening, deleting etc.) on that object. The GUI may
provide features to select any single object or even multiple objects. The
feature of selecting multiple objects can provides tremendous power to the GUI
as the user can do complex operations on multiple objects in one go.
  However, the process of selection is not as simple as it appears to the user
of a GUI. The internal logic of a selection mechanism can be very complex in
some situations. The article describes some fundamental difficulties associated
with the selection mechanism, and illustrates the solutions provided by
different inventions selected from US patent database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6781</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6781</id><created>2014-04-27</created><authors><author><keyname>&#x160;imko</keyname><forenames>Alexander</forenames></author></authors><title>A Family of Descriptive Approaches To Preferred Answer Sets</title><categories>cs.LO</categories><comments>10 pages, 1 figure, The paper appears in the Proceedings of the 15th
  International Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In logic programming under the answer set semantics, preferences on rules are
used to choose which of the conflicting rules are applied. Many interesting
semantics have been proposed. Brewka and Eiter's Principle I expresses the
basic intuition behind the preferences. All the approaches that satisfy
Principle I introduce a rather imperative feature into otherwise declarative
language. They understand preferences as the order, in which the rules of a
program have to be applied. In this paper we present two purely declarative
approaches for preference handling that satisfy Principle I, and work for
general conflicts, including direct and indirect conflicts between rules. The
first approach is based on the idea that a rule cannot be defeated by a less
preferred conflicting rule. This approach is able to ignore preferences between
non-conflicting rules, and, for instance, is equivalent with the answer set
semantics for the subclass of stratified programs. It is suitable for the
scenarios, when developers do not have full control over preferences. The
second approach relaxes the requirement for ignoring conflicting rules, which
ensures that it stays in the NP complexity class. It is based on the idea that
a rule cannot be defeated by a rule that is less preferred or depends on a less
preferred rule. The second approach can be also characterized by a
transformation to logic programs without preferences. It turns out that the
approaches form a hierarchy, a branch in the hierarchy of the approaches by
Delgrande et. al., Wang et. al., and Brewka and Eiter. Finally, we show an
application for which the existing approaches are not usable, and the
approaches of this paper produce expected results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6782</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6782</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Displaying and Resizing Windows</title><categories>cs.HC</categories><comments>10 pages, 6 figures</comments><doi>10.2139/ssrn.1264693</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Windows are used quite frequently in a GUI environment. The greatest
advantage of using windows is that each window creates a virtual screen space.
Hence, although the physical screen space is limited to a few inches, use of
windows can create unlimited screen space to display innumerable items.
  The use of windows facilitates the user to open and interact with multiple
programs or documents simultaneously in different windows. Sometimes a single
program may also open multiple windows to display various items. The user can
resize the windows and move their location time to time as desired.
  However, there are several concerns of a window relating to its size,
appearance, positioning, color, visibility, resizability etc. For example, the
window should have a minimum and a maximum size, dragging and resizing the
window should be easy, one window should not be obscured by another window,
windows should adjust their size and location in order to match with the
changes in the resolution and display environment etc.
  This article illustrates six patents from US Patent database solving problems
relating to displaying and resizing of windows. The inventions include
automatic resizing and relocating of windows, alternative modes of displaying
windows to accommodate within the limited display area, combining both spatial
and temporal methods to resize the window, locking and minimizing windows and
making the windows invisible on specific situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6784</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6784</id><created>2014-04-27</created><updated>2014-07-09</updated><authors><author><keyname>Slota</keyname><forenames>Martin</forenames></author><author><keyname>Bal&#xe1;z</keyname><forenames>Martin</forenames></author><author><keyname>Leite</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>On Strong and Default Negation in Logic Program Updates (Extended
  Version)</title><categories>cs.AI</categories><comments>14 pages, extended version of the paper to appear in the online
  supplement of Theory and Practice of Logic Programming (TPLP), and presented
  at the 15th International Workshop on Non-Monotonic Reasoning (NMR 2014) and
  at the 30th International Conference on Logic Programming (ICLP 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing semantics for answer-set program updates fall into two categories:
either they consider only strong negation in heads of rules, or they primarily
rely on default negation in heads of rules and optionally provide support for
strong negation by means of a syntactic transformation. In this paper we
pinpoint the limitations of both these approaches and argue that both types of
negation should be first-class citizens in the context of updates. We identify
principles that plausibly constrain their interaction but are not
simultaneously satisfied by any existing rule update semantics. Then we extend
one of the most advanced semantics with direct support for strong negation and
show that it satisfies the outlined principles as well as a variety of other
desirable properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6785</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6785</id><created>2014-04-27</created><authors><author><keyname>Han</keyname><forenames>Yujuan</forenames></author><author><keyname>Lu</keyname><forenames>Wenlian</forenames></author><author><keyname>Xu</keyname><forenames>Shouhuai</forenames></author></authors><title>Characterizing the Power of Moving Target Defense via Cyber Epidemic
  Dynamics</title><categories>cs.SY</categories><comments>12 pages; 4 figures; Hotsos 14, 2014</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moving Target Defense (MTD) can enhance the resilience of cyber systems
against attacks. Although there have been many MTD techniques, there is no
systematic understanding and {\em quantitative} characterization of the power
of MTD. In this paper, we propose to use a cyber epidemic dynamics approach to
characterize the power of MTD. We define and investigate two complementary
measures that are applicable when the defender aims to deploy MTD to achieve a
certain security goal. One measure emphasizes the maximum portion of time
during which the system can afford to stay in an undesired configuration (or
posture), without considering the cost of deploying MTD. The other measure
emphasizes the minimum cost of deploying MTD, while accommodating that the
system has to stay in an undesired configuration (or posture) for a given
portion of time. Our analytic studies lead to algorithms for optimally
deploying MTD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6786</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6786</id><created>2014-04-27</created><authors><author><keyname>Blumrosen</keyname><forenames>Liad</forenames></author><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author></authors><title>Reallocation Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider reallocation problems in settings where the initial endowment of
each agent consists of a subset of the resources. The private information of
the players is their value for every possible subset of the resources. The goal
is to redistribute resources among agents to maximize efficiency. Monetary
transfers are allowed, but participation is voluntary.
  We develop incentive-compatible, individually-rational and budget balanced
mechanisms for several classic settings, including bilateral trade, partnership
dissolving, Arrow-Debreu markets, and combinatorial exchanges. All our
mechanisms (except one) provide a constant approximation to the optimal
efficiency in these settings, even in ones where the preferences of the agents
are complex multi-parameter functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6788</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6788</id><created>2014-04-27</created><updated>2016-02-15</updated><authors><author><keyname>Vinkler</keyname><forenames>Dror A.</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Analogy Between Gambling and Measurement-Based Work Extraction</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In information theory, one area of interest is gambling, where mutual
information characterizes the maximal gain in wealth growth rate due to
knowledge of side information; the betting strategy that achieves this maximum
is named the Kelly strategy. In the field of physics, it was recently shown
that mutual information can characterize the maximal amount of work that can be
extracted from a single heat bath using measurement-based control protocols,
i.e., using &quot;information engines&quot;. However, to the best of our knowledge, no
relation between gambling and information engines has been presented before. In
this paper, we briefly review the two concepts and then demonstrate an analogy
between gambling, where bits are converted into wealth, and information
engines, where bits representing measurements are converted into energy. From
this analogy follows an extension of gambling to the continuous-valued case,
which is shown to be useful for investments in currency exchange rates or in
the stock market using options. Moreover, the analogy enables us to use
well-known methods and results from one field to solve problems in the other.
We present three such cases: maximum work extraction when the probability
distributions governing the system and measurements are unknown, work
extraction when some energy is lost in each cycle, e.g., due to friction, and
an analysis of systems with memory. In all three cases, the analogy enables us
to use known results in order to obtain new ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6793</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6793</id><created>2014-04-27</created><authors><author><keyname>Han</keyname><forenames>Yujuan</forenames></author><author><keyname>Lu</keyname><forenames>Wenlian</forenames></author><author><keyname>Li</keyname><forenames>Zhe</forenames></author><author><keyname>Chen</keyname><forenames>Tianping</forenames></author></authors><title>Pinning dynamic systems of networks with Markovian switching couplings
  and controller-node set</title><categories>cs.SY math.OC</categories><comments>9 pages; 3 figures</comments><journal-ref>Systems &amp; Control Letters, Volume 65, March 2014, Pages 56-63</journal-ref><doi>10.1016/j.sysconle.2013.12.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study pinning control problem of coupled dynamical systems
with stochastically switching couplings and stochastically selected
controller-node set. Here, the coupling matrices and the controller-node sets
change with time, induced by a continuous-time Markovian chain. By constructing
Lyapunov functions, we establish tractable sufficient conditions for
exponentially stability of the coupled system. Two scenarios are considered
here. First, we prove that if each subsystem in the switching system, i.e. with
the fixed coupling, can be stabilized by the fixed pinning controller-node set,
and in addition, the Markovian switching is sufficiently slow, then the
time-varying dynamical system is stabilized. Second, in particular, for the
problem of spatial pinning control of network with mobile agents, we conclude
that if the system with the average coupling and pinning gains can be
stabilized and the switching is sufficiently fast, the time-varying system is
stabilized. Two numerical examples are provided to demonstrate the validity of
these theoretical results, including a switching dynamical system between
several stable sub-systems, and a dynamical system with mobile nodes and
spatial pinning control towards the nodes when these nodes are being in a
pre-designed region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6801</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6801</id><created>2014-04-27</created><authors><author><keyname>McDermid</keyname><forenames>John A.</forenames></author></authors><title>Nothing is Certain but Doubt and Tests</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014, software safety standards, uncertainty,
  experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective software safety standards will contribute to confidence, or
assurance, in the safety of the systems in which the software is used. It is
infeasible to demonstrate a correlation between standards and accidents, but
there is an alternative view that makes standards &quot;testable&quot;. Software projects
are subject to uncertainty; good standards reduce uncertainty more than poor
ones. Similarly assurance or integrity levels in standards should define an
uncertainty gradient. The paper proposes an argument -based method of reasoning
about uncertainty that can be used as a basis for conducting experiments
(tests) to evaluate standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6802</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6802</id><created>2014-04-27</created><authors><author><keyname>Habli</keyname><forenames>Ibrahim</forenames></author><author><keyname>Rae</keyname><forenames>Andrew</forenames></author></authors><title>Formalism of Requirements for Safety-Critical Software: Where Does the
  Benefit Come From?</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014, software, formal methods, safety,
  certification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safety and assurance standards often rely on the principle that requirements
errors can be minimised by expressing the requirements more formally. Although
numerous case studies have shown that the act of formalising previously
informal requirements finds requirements errors, this principle is really just
a hypothesis. An industrially persuasive causal relationship between
formalisation and better requirements has yet to be established. We describe
multiple competing explanations for this hypothesis, in terms of the levels of
precision, re-formulation, expertise, effort and automation that are typically
associated with formalising requirements. We then propose an experiment to
distinguish between these explanations, without necessarily excluding the
possibility that none of them are correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6803</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6803</id><created>2014-04-27</created><authors><author><keyname>Holloway</keyname><forenames>C. Michael</forenames></author><author><keyname>Johnson</keyname><forenames>Chris W.</forenames></author></authors><title>Towards Assessing Necessary Competence</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014, safety, argument, evidence, experiment,
  prescriptive, goal-based, fantasy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We sketch a series of studies and experiments designed to provide empirical
evidence about the truth or falsity of claims that non-prescriptive approaches
to standards demand greater competence from regulators than prescriptive
approaches require.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6804</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6804</id><created>2014-04-27</created><authors><author><keyname>Daniels</keyname><forenames>Dewi</forenames></author></authors><title>The Efficacy of DO-178B</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014, DO-178B</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DO-178B was based on the consensus of the avionic software community as it
existed in 1992. Twenty two years after publication, we have no publically
available experimental data as to its efficacy. It appears to work extremely
well, since there have been no hull loss accidents in passenger service
ascribed to software failure. This is a comforting and surprising result.
However, if we don't know why DO-178B works so well, there is a danger that we
could stop doing something that really matters, which could lead to an
accident.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6805</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6805</id><created>2014-04-27</created><authors><author><keyname>Fusani</keyname><forenames>Mario</forenames></author><author><keyname>Lami</keyname><forenames>Giuseppe</forenames></author></authors><title>On the efficacy of safety-related software standards</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014, standards, safety-critical-software</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Difficulty of safety-related software standards to help producing software
for safe systems is discussed. Some research activity and other actions are
proposed to focus on and possibly resolve long-lasting related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6810</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6810</id><created>2014-04-27</created><updated>2014-11-28</updated><authors><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>No</keyname><forenames>Albert</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Information Measures: the Curious Case of the Binary Alphabet</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four problems related to information divergence measures defined on finite
alphabets are considered. In three of the cases we consider, we illustrate a
contrast which arises between the binary-alphabet and larger-alphabet settings.
This is surprising in some instances, since characterizations for the
larger-alphabet settings do not generalize their binary-alphabet counterparts.
Specifically, we show that $f$-divergences are not the unique decomposable
divergences on binary alphabets that satisfy the data processing inequality,
thereby clarifying claims that have previously appeared in the literature. We
also show that KL divergence is the unique Bregman divergence which is also an
$f$-divergence for any alphabet size. We show that KL divergence is the unique
Bregman divergence which is invariant to statistically sufficient
transformations of the data, even when non-decomposable divergences are
considered. Like some of the problems we consider, this result holds only when
the alphabet size is at least three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6812</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6812</id><created>2014-04-27</created><authors><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Relations between Information and Estimation in Scalar L\'evy Channels</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental relations between information and estimation have been
established in the literature for the scalar Gaussian and Poisson channels. In
this work, we demonstrate that such relations hold for a much larger class of
observation models. We introduce the natural family of scalar L\'evy channels
where the distribution of the output conditioned on the input is infinitely
divisible. For L\'evy channels, we establish new representations relating the
mutual information between the channel input and output to an optimal expected
estimation loss, thereby unifying and considerably extending results from the
Gaussian and Poissonian settings. We demonstrate the richness of our results by
working out two examples of L\'evy channels, namely the Gamma channel and the
Negative Binomial channel, with corresponding relations between information and
estimation. Extensions to the setting of mismatched estimation are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6813</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6813</id><created>2014-04-27</created><updated>2015-02-28</updated><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Diffusion LMS over Multitask Networks</title><categories>cs.SY</categories><comments>30 pages, 14 figures. To appear in IEEE Trans. Signal Process., 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diffusion LMS algorithm has been extensively studied in recent years.
This efficient strategy allows to address distributed optimization problems
over networks in the case where nodes have to collaboratively estimate a single
parameter vector. Problems of this type are referred to as single-task
problems. Nevertheless, there are several problems in practice that are
multitask-oriented in the sense that the optimum parameter vector may not be
the same for every node. This brings up the issue of studying the performance
of the diffusion LMS algorithm when it is run, either intentionally or
unintentionally, in a multitask environment. In this paper, we conduct a
theoretical analysis on the stochastic behavior of diffusion LMS in the case
where the so-called single-task hypothesis is violated. We explain under what
conditions diffusion LMS continues to deliver performance superior to
non-cooperative strategies in the multitask environment. When the conditions
are violated, we explain how to endow the nodes with the ability to cluster
with other similar nodes to remove bias. We propose an unsupervised clustering
strategy that allows each node to select, via adaptive adjustments of
combination weights, the neighboring nodes with which it can collaborate to
estimate a common parameter vector. Simulations are presented to illustrate the
theoretical results, and to demonstrate the efficiency of the proposed
clustering strategy. The framework is applied to a useful problem involving a
multi-target tracking task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6817</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6817</id><created>2014-04-27</created><updated>2014-06-29</updated><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author></authors><title>Novel Approach to Real Polynomial Root-finding and Matrix Eigen-solving</title><categories>math.NA cs.NA</categories><comments>17 pages, added algorithm 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Univariate polynomial root-finding is both classical and important for modern
computing. Frequently one seeks just the real roots of a polynomial with real
coefficients. They can be approximated at a low computational cost if the
polynomial has no nonreal roots, but typically nonreal roots are much more
numerous than the real ones. We dramatically accelerate the known algorithms in
this case by exploiting the correlation between the computations with matrices
and polynomials, extending the techniques of the matrix sign iteration, and
exploiting the structure of the companion matrix of the input polynomial. We
extend some of the proposed techniques to the approximation of the real
eigenvalues of a real nonsymmetric matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6818</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6818</id><created>2014-04-27</created><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>Tschannen</keyname><forenames>Michael</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Subspace clustering of dimensionality-reduced data</title><categories>cs.IT math.IT stat.ML</categories><comments>ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering refers to the problem of clustering unlabeled
high-dimensional data points into a union of low-dimensional linear subspaces,
assumed unknown. In practice one may have access to dimensionality-reduced
observations of the data only, resulting, e.g., from &quot;undersampling&quot; due to
complexity and speed constraints on the acquisition device. More pertinently,
even if one has access to the high-dimensional data set it is often desirable
to first project the data points into a lower-dimensional space and to perform
the clustering task there; this reduces storage requirements and computational
cost. The purpose of this paper is to quantify the impact of
dimensionality-reduction through random projection on the performance of the
sparse subspace clustering (SSC) and the thresholding based subspace clustering
(TSC) algorithms. We find that for both algorithms dimensionality reduction
down to the order of the subspace dimensions is possible without incurring
significant performance degradation. The mathematical engine behind our
theorems is a result quantifying how the affinities between subspaces change
under random dimensionality reducing projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6822</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6822</id><created>2014-04-27</created><updated>2015-09-20</updated><authors><author><keyname>Culnane</keyname><forenames>Chris</forenames></author><author><keyname>Ryan</keyname><forenames>Peter Y. A.</forenames></author><author><keyname>Schneider</keyname><forenames>Steve</forenames></author><author><keyname>Teague</keyname><forenames>Vanessa</forenames></author></authors><title>vVote: a Verifiable Voting System</title><categories>cs.CR</categories><comments>Previously titled &quot;Draft Technical Report for VEC vVote System&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Pret a Voter cryptographic voting system was designed to be flexible and
to offer voters a familiar and easy voting experience. In this paper we present
a case study of our efforts to adapt Pret a Voter to the idiosyncrasies of
elections in the Australian state of Victoria. This technical report includes
general background, user experience and details of the cryptographic protocols
and human processes. We explain the problems, present solutions, then analyse
their security properties and explain how they tie in to other design
decisions. We hope this will be an interesting case study on the application of
end-to-end verifiable voting protocols to real elections.
  A preliminary version of this paper appeared as the 10th February 2014
version of &quot;Draft Technical Report for VEC vVote System&quot;.
  The team involved in developing the vVote design described in this report
were: Craig Burton, Chris Culnane, James Heather, Rui Joaquim, Peter Y. A.
Ryan, Steve Schneider and Vanessa Teague.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6823</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6823</id><created>2014-04-27</created><updated>2014-08-05</updated><authors><author><keyname>Garland</keyname><forenames>Joshua</forenames></author><author><keyname>James</keyname><forenames>Ryan</forenames></author><author><keyname>Bradley</keyname><forenames>Elizabeth</forenames></author></authors><title>Model-free quantification of time-series predictability</title><categories>cs.IT math.IT</categories><comments>23 pages, 8 figures, 1 table</comments><report-no>Santa Fe Institute Working Paper #14-05-014</report-no><journal-ref>Physical Review E 90 (5), 052910 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052910</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides insight into when, why, and how forecast strategies fail
when they are applied to complicated time series. We conjecture that the
inherent complexity of real-world time-series data---which results from the
dimension, nonlinearity, and non-stationarity of the generating process, as
well as from measurement issues like noise, aggregation, and finite data
length---is both empirically quantifiable and directly correlated with
predictability. In particular, we argue that redundancy is an effective way to
measure complexity and predictive structure in an experimental time series and
that weighted permutation entropy is an effective way to estimate that
redundancy. To validate these conjectures, we study 120 different time-series
data sets. For each time series, we construct predictions using a wide variety
of forecast models, then compare the accuracy of the predictions with the
permutation entropy of that time series. We use the results to develop a
model-free heuristic that can help practitioners recognize when a particular
prediction method is not well matched to the task at hand: that is, when the
time series has more predictive structure than that method can capture and
exploit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6830</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6830</id><created>2014-04-27</created><authors><author><keyname>Khan</keyname><forenames>Sobia K</forenames></author></authors><title>Orthogonal Fault Tolerance for Dynamically Adaptive Systems</title><categories>cs.SE</categories><comments>EDCC-2014, Student-Forum, dynamically adaptive systems, fault
  tolerance, orthogonal fault tolerance</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic systems that adapt to users' needs and changing environments,
dependability needs cannot be avoided. This paper proposes an orthogonal fault
tolerance model as a means to manage and reason about multiple fault tolerance
mechanisms that co-exist in dynamically adaptive systems. One of the key
challenges associated with dynamically evolving fault tolerance needs is the
feature interaction problem arising from the integration of fault tolerance
features. The proposed approach provides a separation of fault tolerance
concerns to study the effects of integrated fault tolerance on the system. This
approach uses state machine and operational semantics to reason about these
interactions and inconsistencies. The proposed approach is supported by the
tool NuSMV to simulate and verify the state machines against logic statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6832</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6832</id><created>2014-04-27</created><authors><author><keyname>Place</keyname><forenames>Thomas</forenames></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames></author></authors><title>Going higher in the First-order Quantifier Alternation Hierarchy on
  Words</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the quantifier alternation hierarchy in first-order logic on
finite words. Levels in this hierarchy are defined by counting the number of
quantifier alternations in formulas. We prove that one can decide membership of
a regular language to the levels $\mathcal{B}\Sigma_2$ (boolean combination of
formulas having only 1 alternation) and $\Sigma_3$ (formulas having only 2
alternations beginning with an existential block). Our proof works by
considering a deeper problem, called separation, which, once solved for lower
levels, allows us to solve membership for higher levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6833</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6833</id><created>2014-04-27</created><authors><author><keyname>Taliga</keyname><forenames>Miklos</forenames></author></authors><title>Unit verification procedure as a test of real time messaging-based
  processes</title><categories>cs.SE</categories><comments>EDCC-2014, Student-Forum</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents the first results of a PhD study connected to testing of
safety critical medical devices: a systematically executed case study at a
Hungarian manufacturer of medical devices. The article shortly describes the
process of testing currently being used. Elements of the testing approach less
commonly applied in software industry are emphasized . The ending point of the
actual testing process in the case study is the starting point for further
research: the automated analysis of the testing results. The author started to
develop a new approach, using a combination of tools, and modeling a
model-based test generating tool - something that is both novel and intensive
as an area of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6835</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6835</id><created>2014-04-27</created><authors><author><keyname>Parter</keyname><forenames>Merav</forenames></author></authors><title>Bypassing Erd\H{o}s' Girth Conjecture: Hybrid Stretch and Sourcewise
  Spanners</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(\alpha,\beta)$-spanner of an $n$-vertex graph $G=(V,E)$ is a subgraph
$H$ of $G$ satisfying that $dist(u, v, H) \leq \alpha \cdot dist(u, v,
G)+\beta$ for every pair $(u, v)\in V \times V$, where $dist(u,v,G')$ denotes
the distance between $u$ and $v$ in $G' \subseteq G$. It is known that for
every integer $k \geq 1$, every graph $G$ has a polynomially constructible
$(2k-1,0)$-spanner of size $O(n^{1+1/k})$. This size-stretch bound is
essentially optimal by the girth conjecture. It is therefore intriguing to ask
if one can &quot;bypass&quot; the conjecture by settling for a multiplicative stretch of
$2k-1$ only for \emph{neighboring} vertex pairs, while maintaining a strictly
\emph{better} multiplicative stretch for the rest of the pairs. We answer this
question in the affirmative and introduce the notion of \emph{$k$-hybrid
spanners}, in which non neighboring vertex pairs enjoy a \emph{multiplicative}
$k$-stretch and the neighboring vertex pairs enjoy a \emph{multiplicative}
$(2k-1)$ stretch (hence, tight by the conjecture). We show that for every
unweighted $n$-vertex graph $G$ with $m$ edges, there is a (polynomially
constructible) $k$-hybrid spanner with $O(k^2 \cdot n^{1+1/k})$ edges. \indent
An alternative natural approach to bypass the girth conjecture is to allow
ourself to take care only of a subset of pairs $S \times V$ for a given subset
of vertices $S \subseteq V$ referred to here as \emph{sources}. Spanners in
which the distances in $S \times V$ are bounded are referred to as
\emph{sourcewise spanners}. Several constructions for this variant are provided
(e.g., multiplicative sourcewise spanners, additive sourcewise spanners and
more).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6838</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6838</id><created>2014-04-27</created><authors><author><keyname>Acher</keyname><forenames>Mathieu</forenames></author><author><keyname>Combemale</keyname><forenames>Benoit</forenames></author><author><keyname>Collet</keyname><forenames>Philippe</forenames></author></authors><title>Metamorphic Domain-Specific Languages: A Journey Into the Shapes of a
  Language</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  External or internal domain-specific languages (DSLs) or (fluent) APIs?
Whoever you are -- a developer or a user of a DSL -- you usually have to choose
your side; you should not! What about metamorphic DSLs that change their shape
according to your needs? We report on our 4-years journey of providing the
&quot;right&quot; support (in the domain of feature modeling), leading us to develop an
external DSL, different shapes of an internal API, and maintain all these
languages. A key insight is that there is no one-size-fits-all solution or no
clear superiority of a solution compared to another. On the contrary, we found
that it does make sense to continue the maintenance of an external and internal
DSL. The vision that we foresee for the future of software languages is their
ability to be self-adaptable to the most appropriate shape (including the
corresponding integrated development environment) according to a particular
usage or task. We call metamorphic DSL such a language, able to change from one
shape to another shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6844</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6844</id><created>2014-04-27</created><authors><author><keyname>Rushby</keyname><forenames>John</forenames></author><author><keyname>Littlewood</keyname><forenames>Bev</forenames></author><author><keyname>Strigini</keyname><forenames>Lorenzo</forenames></author></authors><title>Evaluating the Assessment of Software Fault-Freeness</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to validate experimentally a theory of software certification that
proceeds from assessment of confidence in fault-freeness (due to standards) to
conservative prediction of failure-free operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6846</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6846</id><created>2014-04-27</created><authors><author><keyname>Jaradat</keyname><forenames>Omar</forenames></author><author><keyname>Graydon</keyname><forenames>Patrick</forenames></author><author><keyname>Bate</keyname><forenames>Iain</forenames></author></authors><title>An Approach to Maintaining Safety Case Evidence After A System Change</title><categories>cs.SE</categories><comments>EDCC-2014, Fast-Abstracts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developers of some safety critical systems construct a safety case.
Developers changing a system during development or after release must analyse
the change's impact on the safety case. Evidence might be invalidated by
changes to the system design, operation, or environmental context. Assumptions
valid in one context might be invalid elsewhere. The impact of change might not
be obvious. This paper proposes a method to facilitate safety case maintenance
by highlighting the impact of changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6849</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6849</id><created>2014-04-27</created><authors><author><keyname>Carlson</keyname><forenames>Frederick R.</forenames></author></authors><title>Security Analysis of Cloud Computing</title><categories>cs.CR</categories><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper produces a baseline security analysis of the Cloud Computing
Operational Environment in terms of threats, vulnerabilities and impacts. An
analysis is conducted and the top three threats are identified with
recommendations for practitioners. The conclusion of the analysis is that the
most serious threats are non-technical and can be solved via management
processes rather than technical countermeasures
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6850</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6850</id><created>2014-04-27</created><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>Precedence-constrained Scheduling of Malleable Jobs with Preemption</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling jobs with precedence constraints on a set of identical machines to
minimize the total processing time (makespan) is a fundamental problem in
combinatorial optimization. In practical settings such as cloud computing, jobs
are often malleable, i.e., can be processed on multiple machines
simultaneously. The instantaneous processing rate of a job is a non-decreasing
function of the number of machines assigned to it (we call it the processing
function). Previous research has focused on practically relevant concave
processing functions, which obey the law of diminishing utility and generalize
the classical (non-malleable) problem. Our main result is a
$(2+\epsilon)$-approximation algorithm for concave processing functions (for
any $\epsilon &gt; 0$), which is the best possible under complexity theoretic
assumptions. The approximation ratio improves to $(1 + \epsilon)$ for the
interesting and practically relevant special case of power functions, i.e.,
$p_j(z) = c_j \cdot z^{\gamma}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6851</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6851</id><created>2014-04-27</created><updated>2014-05-08</updated><authors><author><keyname>Mart&#xed;nez</keyname><forenames>F. E. Brochero</forenames></author><author><keyname>Vergara</keyname><forenames>C. R. Giraldo</forenames></author></authors><title>Weight enumerator of some irreducible cyclic codes</title><categories>cs.IT math.IT math.NT</categories><comments>Submitted to Designs, Codes and Cryptography, 8 pages</comments><msc-class>12E05(primary) and 94B05(secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we show explicitly all possible weight enumerators for every
irreducible cyclic code of length $n$ over a finite field $\mathbb F_q$, in the
case which each prime divisor of $n$ is also a divisor of $q-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6857</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6857</id><created>2014-04-27</created><updated>2014-06-28</updated><authors><author><keyname>Salimi</keyname><forenames>Babak</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author></authors><title>Causality in Databases: The Diagnosis and Repair Connections</title><categories>cs.DB</categories><comments>Proc. 15th International Workshop on Non-Monotonic Reasoning (NMR
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we establish and investigate the connections between causality
for query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. The vast body of research on database repairs can be applied
to the newer problem of determining actual causes for query answers. By
formulating a causality problem as a diagnosis problem, we manage to
characterize causes in terms of a system's diagnoses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6862</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6862</id><created>2014-04-27</created><authors><author><keyname>Fish</keyname><forenames>Alexander</forenames></author><author><keyname>Gurevich</keyname><forenames>Shamgar</forenames></author></authors><title>Performance Estimates of the Pseudo-Random Method for Radar Detection</title><categories>cs.IT math.IT</categories><comments>5 pages, two figures, to appear in Proceedings of ISIT 2014 - IEEE
  International Symposium on Information Theory, Honolulu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A performance of the pseudo-random method for the radar detection is
analyzed. The radar sends a pseudo-random sequence of length $N$, and receives
echo from $r$ targets. We assume the natural assumptions of uniformity on the
channel and of the square root cancellation on the noise. Then for $r \leq
N^{1-\delta}$, where $\delta &gt; 0$, the following holds: (i) the probability of
detection goes to one, and (ii) the expected number of false targets goes to
zero, as $N$ goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6866</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6866</id><created>2014-04-28</created><updated>2015-10-28</updated><authors><author><keyname>Liang</keyname><forenames>Wang</forenames></author><author><keyname>KaiYong</keyname><forenames>Zhao</forenames></author></authors><title>Detecting &quot;protein words&quot; through unsupervised word segmentation</title><categories>cs.CE q-bio.GN</categories><comments>11 pages,8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised word segmentation methods were applied to analyze the protein
sequence. Protein sequences, such as 'MTMDKSELVQKA...', were used as input to
these methods. Segmented 'protein word' sequences, such as 'MTM DKSE LVQKA',
were then obtained. We compare the 'protein words' produced by unsupervised
segmentation and the protein secondary structure segmentation. An interesting
finding is that the unsupervised word segmentation is more efficient than
secondary structure segmentation in expressing information. Our experiment also
suggests there may be some 'protein ruins' in current noncoding regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6871</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6871</id><created>2014-04-28</created><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Proximal Iteratively Reweighted Algorithm with Multiple Splitting for
  Nonconvex Sparsity Optimization</title><categories>cs.NA cs.CV</categories><journal-ref>Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for
solving a general problem, which involves a large body of nonconvex sparse and
structured sparse related problems. Comparing with previous iterative solvers
for nonconvex sparse problem, PIRE is much more general and efficient. The
computational cost of PIRE in each iteration is usually as low as the
state-of-the-art convex solvers. We further propose the PIRE algorithm with
Parallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating
(PIRE-AU) to handle the multi-variable problems. In theory, we prove that our
proposed methods converge and any limit solution is a stationary point.
Extensive experiments on both synthesis and real data sets demonstrate that our
methods achieve comparative learning performance, but are much more efficient,
by comparing with previous nonconvex solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6876</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6876</id><created>2014-04-28</created><authors><author><keyname>Tangkaratt</keyname><forenames>Voot</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Conditional Density Estimation with Dimensionality Reduction via
  Squared-Loss Conditional Entropy Minimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression aims at estimating the conditional mean of output given input.
However, regression is not informative enough if the conditional density is
multimodal, heteroscedastic, and asymmetric. In such a case, estimating the
conditional density itself is preferable, but conditional density estimation
(CDE) is challenging in high-dimensional space. A naive approach to coping with
high-dimensionality is to first perform dimensionality reduction (DR) and then
execute CDE. However, such a two-step process does not perform well in practice
because the error incurred in the first DR step can be magnified in the second
CDE step. In this paper, we propose a novel single-shot procedure that performs
CDE and DR simultaneously in an integrated way. Our key idea is to formulate DR
as the problem of minimizing a squared-loss variant of conditional entropy, and
this is solved via CDE. Thus, an additional CDE step is not needed after DR. We
demonstrate the usefulness of the proposed method through extensive experiments
on various datasets including humanoid robot transition and computer art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6878</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6878</id><created>2014-04-28</created><updated>2014-12-01</updated><authors><author><keyname>Hu</keyname><forenames>Songlin</forenames></author><author><keyname>Liu</keyname><forenames>Wantao</forenames></author><author><keyname>Rabl</keyname><forenames>Tilmann</forenames></author><author><keyname>Huang</keyname><forenames>Shuo</forenames></author><author><keyname>Liang</keyname><forenames>Ying</forenames></author><author><keyname>Xiao</keyname><forenames>Zheng</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author><author><keyname>Pei</keyname><forenames>Xubin</forenames></author><author><keyname>Wang</keyname><forenames>Jiye</forenames></author></authors><title>DualTable: A Hybrid Storage Model for Update Optimization in Hive</title><categories>cs.DB cs.DC</categories><comments>accepted by industry session of ICDE2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hive is the most mature and prevalent data warehouse tool providing SQL-like
interface in the Hadoop ecosystem. It is successfully used in many Internet
companies and shows its value for big data processing in traditional
industries. However, enterprise big data processing systems as in Smart Grid
applications usually require complicated business logics and involve many data
manipulation operations like updates and deletes. Hive cannot offer sufficient
support for these while preserving high query performance. Hive using the
Hadoop Distributed File System (HDFS) for storage cannot implement data
manipulation efficiently and Hive on HBase suffers from poor query performance
even though it can support faster data manipulation.There is a project based on
Hive issue Hive-5317 to support update operations, but it has not been finished
in Hive's latest version. Since this ACID compliant extension adopts same data
storage format on HDFS, the update performance problem is not solved.
  In this paper, we propose a hybrid storage model called DualTable, which
combines the efficient streaming reads of HDFS and the random write capability
of HBase. Hive on DualTable provides better data manipulation support and
preserves query performance at the same time. Experiments on a TPC-H data set
and on a real smart grid data set show that Hive on DualTable is up to 10 times
faster than Hive when executing update and delete operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6881</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6881</id><created>2014-04-28</created><authors><author><keyname>Barfuss</keyname><forenames>Hendrik</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Improving Blind Source Separation Performance By Adaptive Array
  Geometries For Humanoid Robots</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the concept of an adaptation algorithm is proposed, which can
be used to blindly adapt the microphone array geometry of a humanoid robot such
that the performance of the underlying signal separation algorithm is improved.
As a decisive feature, an online performance measure for blind source
separation is introduced which allows a robust and reliable estimation of the
instantaneous separation performance based on currently observable data.
Experimental results from a simulated environment confirm the efficacy of the
concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6883</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6883</id><created>2014-04-28</created><authors><author><keyname>Frt&#xfa;s</keyname><forenames>Jozef</forenames></author></authors><title>Credulous and Skeptical Argument Games for Complete Semantics in
  Conflict Resolution based Argumentation</title><categories>cs.AI</categories><comments>appears in the Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Argumentation is one of the most popular approaches of defining
a~non-monotonic formalism and several argumentation based semantics were
proposed for defeasible logic programs. Recently, a new approach based on
notions of conflict resolutions was proposed, however with declarative
semantics only. This paper gives a more procedural counterpart by developing
skeptical and credulous argument games for complete semantics and soundness and
completeness theorems for both games are provided. After that, distribution of
defeasible logic program into several contexts is investigated and both
argument games are adapted for multi-context system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6890</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6890</id><created>2014-04-28</created><authors><author><keyname>Banerjee</keyname><forenames>Joydeep</forenames></author><author><keyname>Das</keyname><forenames>Arun</forenames></author><author><keyname>Sen</keyname><forenames>Arunabha</forenames></author></authors><title>Analysis of d-Hop Dominating Set Problem for Directed Graph with
  Indegree Bounded by One</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  d-hop dominating set problem was introduced for cluster formation in wireless
ad-hoc networks and is proved to be NP-complete. Dominating set problem for
directed graph with indegree of at most 1 can be solved polynomially. In this
article we perform an analysis of d-hop dominating set problem for directed
graph with indegree 1. We show that the problem can be solved polynomially by
exploiting certain properties of the graph under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6891</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6891</id><created>2014-04-28</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Jan&#xdf;en</keyname><forenames>Gisbert</forenames></author></authors><title>Resource Cost Results for Entanglement Distillation and State Merging
  under Source Uncertainties</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>5 pages, 0 figures. Accepted for presentation at the IEEE ISIT 2014
  Honolulu. This is a conference version of arXiv:1401.6063</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce one-way LOCC protocols for quantum state merging for compound
sources, which have asymptotically optimal entanglement as well as classical
communication resource costs. For the arbitrarily varying quantum source (AVQS)
model, we determine the one-way entanglement distillation capacity, where we
utilize the robustification and elimination techniques, well-known from
classical as well as quantum channel coding under assumption of arbitrarily
varying noise. Investigating quantum state merging for AVQS, we demonstrate by
example, that the usual robustification procedure leads to suboptimal resource
costs in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6924</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6924</id><created>2014-04-28</created><authors><author><keyname>Vlasiou</keyname><forenames>Maria</forenames></author><author><keyname>Zhang</keyname><forenames>Jiheng</forenames></author><author><keyname>Zwart</keyname><forenames>Bert</forenames></author></authors><title>Separation of timescales in a two-layered network</title><categories>math.PR cs.PF</categories><comments>8 pages, 2 figures, 1 table, ITC 24 (2012)</comments><journal-ref>Proceedings of the 24th International Teletraffic Congress,
  Article No.: 32, 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We investigate a computer network consisting of two layers occurring in, for
example, application servers. The first layer incorporates the arrival of jobs
at a network of multi-server nodes, which we model as a many-server Jackson
network. At the second layer, active servers at these nodes act now as
customers who are served by a common CPU. Our main result shows a separation of
time scales in heavy traffic: the main source of randomness occurs at the
(aggregate) CPU layer; the interactions between different types of nodes at the
other layer is shown to converge to a fixed point at a faster time scale; this
also yields a state-space collapse property. Apart from these fundamental
insights, we also obtain an explicit approximation for the joint law of the
number of jobs in the system, which is provably accurate for heavily loaded
systems and performs numerically well for moderately loaded systems. The
obtained results for the model under consideration can be applied to
thread-pool dimensioning in application servers, while the technique seems
applicable to other layered systems too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6929</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6929</id><created>2014-04-28</created><updated>2014-10-22</updated><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Grosso</keyname><forenames>Paola</forenames></author><author><keyname>Hillegas</keyname><forenames>Curtis</forenames></author><author><keyname>Holzman</keyname><forenames>Burt</forenames></author><author><keyname>Janssen</keyname><forenames>Ruben L.</forenames></author><author><keyname>Klous</keyname><forenames>Sander</forenames></author><author><keyname>Knight</keyname><forenames>Robert</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author></authors><title>Power-aware applications for scientific cluster and distributed
  computing</title><categories>physics.comp-ph cs.DC hep-ex</categories><comments>Submitted to proceedings of International Symposium on Grids and
  Clouds (ISGC) 2014, 23-28 March 2014, Academia Sinica, Taipei, Taiwan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aggregate power use of computing hardware is an important cost factor in
scientific cluster and distributed computing systems. The Worldwide LHC
Computing Grid (WLCG) is a major example of such a distributed computing
system, used primarily for high throughput computing (HTC) applications. It has
a computing capacity and power consumption rivaling that of the largest
supercomputers. The computing capacity required from this system is also
expected to grow over the next decade. Optimizing the power utilization and
cost of such systems is thus of great interest.
  A number of trends currently underway will provide new opportunities for
power-aware optimizations. We discuss how power-aware software applications and
scheduling might be used to reduce power consumption, both as autonomous
entities and as part of a (globally) distributed system. As concrete examples
of computing centers we provide information on the large HEP-focused Tier-1 at
FNAL, and the Tigress High Performance Computing Center at Princeton
University, which provides HPC resources in a university context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6931</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6931</id><created>2014-04-28</created><authors><author><keyname>Kai</keyname><forenames>Caihong</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Lusheng</forenames></author></authors><title>To Achieve Maximal Throughputs in CSMA Wireless Networks Through
  Offered_load Control</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies how to achieve the maximal link throughputs in a CSMA
wireless network through offered-load control. First, we propose an analytical
model, contention-graph-combination (CGC), to describe the relationship between
the offered-load and the output link throughputs of an unsaturated CSMA
network. Based on CGC, we then formulate a linear optimization model to improve
the aggregate link throughput through properly setting the occurrence
probabilities of each sub-network, based on which we can obtain the optimal
offered-load of each link. Simulation results bore out the accuracy of our CGC
analysis and the maximal link throughputs can be closely achieved. Different
from prior work in which CSMA protocol parameters are adaptively adjusted to
achieve better performance, in this paper we propose to achieve maximal link
throughputs by adjusting the rates of the traffic pumped into the source nodes
of links, which runs in a software manner and is more practical to implement in
real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6935</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6935</id><created>2014-04-28</created><authors><author><keyname>Alstott</keyname><forenames>Jeff</forenames></author><author><keyname>Madnick</keyname><forenames>Stuart</forenames></author><author><keyname>Velu</keyname><forenames>Chander</forenames></author></authors><title>Homophily and the Speed of Social Mobilization: The Effect of Acquired
  and Ascribed Traits</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>9 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.3805</comments><journal-ref>PLoS ONE 9(4): e95140. 2014</journal-ref><doi>10.1371/journal.pone.0095140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale mobilization of individuals across social networks is becoming
increasingly prevalent in society. However, little is known about what affects
the speed of social mobilization. Here we use a framed field experiment to
identify and measure properties of individuals and their relationships that
predict mobilization speed. We ran a global social mobilization contest and
recorded personal traits of the participants and those they recruited. We
studied the effects of ascribed traits (gender, age) and acquired traits
(geography, and information source) on the speed of mobilization. We found that
homophily, a preference for interacting with other individuals with similar
traits, had a mixed role in social mobilization. Homophily was present for
acquired traits, in which mobilization speed was faster when the recuiter and
recruit had the same trait compared to different traits. In contrast, we did
not find support for homophily for the ascribed traits. Instead, those traits
had other, non-homophily effects: Females mobilized other females faster than
males mobilized other males. Younger recruiters mobilized others faster, and
older recruits mobilized slower. Recruits also mobilized faster when they first
heard about the contest directly from the contest organization, and decreased
in speed when hearing from less personal source types (e.g. family vs. media).
These findings show that social mobilization includes dynamics that are unlike
other, more passive forms of social activity propagation. These findings
suggest relevant factors for engineering social mobilization tasks for
increased speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6938</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6938</id><created>2014-04-28</created><authors><author><keyname>Skowron</keyname><forenames>Marcin</forenames></author><author><keyname>Rank</keyname><forenames>Stefan</forenames></author><author><keyname>&#x15a;widerska</keyname><forenames>Aleksandra</forenames></author><author><keyname>K&#xfc;ster</keyname><forenames>Dennis</forenames></author><author><keyname>Kappas</keyname><forenames>Arvid</forenames></author></authors><title>Applying a Text-Based Affective Dialogue System in Psychological
  Research: Case Studies on the Effects of System Behavior, Interaction Context
  and Social Exclusion</title><categories>cs.HC</categories><comments>31 pages, 11 figures</comments><doi>10.1007/s12559-014-9271-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents two studies conducted with an affective dialogue system
in which text-based system-user communication was used to model, generate, and
present different affective and social interaction scenarios. We specifically
investigated the influence of interaction context and roles assigned to the
system and the participants, as well as the impact of pre-structured social
interaction patterns that were modelled to mimic aspects of 'social exclusion'
scenarios. The results of the first study demonstrate that both the social
context of the interaction and the roles assigned to the system influence the
system evaluation, interaction patterns, textual expressions of affective
states, as well as emotional self-reports. The results observed for the second
study show the system's ability to partially exclude a participant from a
triadic conversation without triggering significantly different affective
reactions or a more negative system evaluation. The experimental evidence
provides insights on the perception, modelling and generation of affective and
social cues in artificial systems that can be realized in different modalities,
including the text modality, thus delivering valuable input for applying
affective dialogue systems as tools for studying affect and social aspects in
online communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6945</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6945</id><created>2014-04-28</created><updated>2014-12-30</updated><authors><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Zero-Outage Cellular Downlink with Fixed-Rate D2D Underlay</title><categories>cs.IT math.IT</categories><comments>Revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two of the emerging trends in wireless cellular systems are Device-to-Device
(D2D) and Machine-to-Machine (M2M) communications. D2D enables efficient reuse
of the licensed spectrum to support localized transmissions, while M2M
connections are often characterized by fixed and low transmission rates. D2D
connections can be instrumental in localized aggregation of uplink M2M traffic
to a more capable cellular device, before being finally delivered to the Base
Station (BS). In this paper we show that a fixed M2M rate is an enabler of
efficient Machine-Type D2D underlay operation taking place simultaneously with
another \emph{downlink} cellular transmission. In the considered scenario, a BS
$B$ transmits to a user $U$, while there are $N_M$ Machine-Type Devices (MTDs)
attached to $U$, all sending simultaneously to $U$ and each using the same rate
$R_M$. While assuming that $B$ knows the channel $B-U$, but not the interfering
channels from the MTDs to $U$, we prove that there is a positive downlink rate
that can always be decoded by $U$, leading to zero-outage of the downlink
signal. This is a rather surprising consequence of the features of the multiple
access channel and the fixed rate $R_M$. We also consider the case of a
simpler, single-user decoder at $U$ with successive interference cancellation.
However, with single-user decoder, a positive zero-outage rate exists only when
$N_M=1$ and is zero when $N_M&gt;1$. This implies that joint decoding is
instrumental in enabling fixed-rate underlay operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6949</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6949</id><created>2014-04-28</created><authors><author><keyname>Khan</keyname><forenames>Susanta Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Interval-Valued Intuitionistic Fuzzy Matrices</title><categories>cs.DM</categories><comments>12 pages</comments><journal-ref>Notes on Intuitionistic Fuzzy Sets, 11(1) (2005)16-27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the interval-valued intuitionistic fuzzy matrix (IVIFM) is
introduced. The interval-valued intuitionistic fuzzy determinant is also
defined. Some fundamental operations are also presented. The need of IVIFM is
explain by an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6950</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6950</id><created>2014-04-28</created><authors><author><keyname>Zhu</keyname><forenames>Zhen</forenames></author><author><keyname>Cerina</keyname><forenames>Federica</forenames></author><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author></authors><title>The Rise of China in the International Trade Network: A Community Core
  Detection Approach</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 4 figures</comments><journal-ref>PLoS ONE 9(8): e105496 (2014)</journal-ref><doi>10.1371/journal.pone.0105496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theory of complex networks proved successful in the description of a variety
of static networks ranging from biology to computer and social sciences and to
economics and finance. Here we use network models to describe the evolution of
a particular economic system, namely the International Trade Network (ITN).
Previous studies often assume that globalization and regionalization in
international trade are contradictory to each other. We re-examine the
relationship between globalization and regionalization by viewing the
international trade system as an interdependent complex network. We use the
modularity optimization method to detect communities and community cores in the
ITN during the years 1995-2011. We find rich dynamics over time both inter- and
intra-communities. Most importantly, we have a multilevel description of the
evolution where the global dynamics (i.e., communities disappear or reemerge)
tend to be correlated with the regional dynamics (i.e., community core changes
between community members). In particular, the Asia-Oceania community
disappeared and reemerged over time along with a switch in leadership from
Japan to China. Moreover, simulation results show that the global dynamics can
be generated by a preferential attachment mechanism both inter- and
intra-communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6955</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6955</id><created>2014-04-23</created><authors><author><keyname>Nelson</keyname><forenames>Kenric P.</forenames></author><author><keyname>Barbu</keyname><forenames>Madalina</forenames></author><author><keyname>Scannell</keyname><forenames>Brian J.</forenames></author></authors><title>Probabilistic graphs using coupled random variables</title><categories>cs.LG cs.IT cs.NE math.IT</categories><comments>Submitted for presentation at the Machine Intelligence and
  Bio-inspired Computation: Theory and Applications Conference, SPIE Sensing
  Technology and Applications, Baltimore, MD, May 8, 2014</comments><doi>10.1117/12.2050759</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network design has utilized flexible nonlinear processes which can
mimic biological systems, but has suffered from a lack of traceability in the
resulting network. Graphical probabilistic models ground network design in
probabilistic reasoning, but the restrictions reduce the expressive capability
of each node making network designs complex. The ability to model coupled
random variables using the calculus of nonextensive statistical mechanics
provides a neural node design incorporating nonlinear coupling between input
states while maintaining the rigor of probabilistic reasoning. A generalization
of Bayes rule using the coupled product enables a single node to model
correlation between hundreds of random variables. A coupled Markov random field
is designed for the inferencing and classification of UCI's MLR 'Multiple
Features Data Set' such that thousands of linear correlation parameters can be
replaced with a single coupling parameter with just a (3%, 4%) percent
reduction in (classification, inference) performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6960</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6960</id><created>2014-04-28</created><authors><author><keyname>Kozyrev</keyname><forenames>S. V.</forenames></author></authors><title>Cluster networks and Bruhat-Tits buildings</title><categories>math.MG cs.DM</categories><comments>11 pages</comments><doi>10.1007/s11232-014-0191-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering procedure for the case where instead of a fixed metric one applies
a family of metrics is considered. In this case instead of a classification
tree one obtains a classification network (a directed acyclic graph with non
directed cycles).
  Relation to Bruhat-Tits buildings is discussed. Dimension of a general
cluster system is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6962</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6962</id><created>2014-04-28</created><updated>2014-09-01</updated><authors><author><keyname>Nicaud</keyname><forenames>Cyril</forenames></author></authors><title>Fast Synchronization of Random Automata</title><categories>cs.FL cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A synchronizing word for an automaton is a word that brings that automaton
into one and the same state, regardless of the starting position. Cerny
conjectured in 1964 that if a n-state deterministic automaton has a
synchronizing word, then it has a synchronizing word of size at most (n-1)^2.
Berlinkov recently made a breakthrough in the probabilistic analysis of
synchronization by proving that with high probability, an automaton has a
synchronizing word. In this article, we prove that with high probability an
automaton admits a synchronizing word of length smaller than n^(1+\epsilon),
and therefore that the Cerny conjecture holds with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6965</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6965</id><created>2014-04-28</created><authors><author><keyname>Madnani</keyname><forenames>Khushraj</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Pandya</keyname><forenames>Paritosh</forenames></author></authors><title>Partially Punctual Metric Temporal Logic is Decidable</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric Temporal Logic $\mathsf{MTL}[\until_I,\since_I]$ is one of the most
studied real time logics. It exhibits considerable diversity in expressiveness
and decidability properties based on the permitted set of modalities and the
nature of time interval constraints $I$. Henzinger et al., in their seminal
paper showed that the non-punctual fragment of $\mathsf{MTL}$ called
$\mathsf{MITL}$ is decidable. In this paper, we sharpen this decidability
result by showing that the partially punctual fragment of $\mathsf{MTL}$
(denoted $\mathsf{PMTL}$) is decidable over strictly monotonic finite point
wise time. In this fragment, we allow either punctual future modalities, or
punctual past modalities, but never both together. We give two satisfiability
preserving reductions from $\mathsf{PMTL}$ to the decidable logic
$\mathsf{MTL}[\until_I]$. The first reduction uses simple projections, while
the second reduction uses a novel technique of temporal projections with
oversampling. We study the trade-off between the two reductions: while the
second reduction allows the introduction of extra action points in the
underlying model, the equisatisfiable $\mathsf{MTL}[\until_I]$ formula obtained
is exponentially succinct than the one obtained via the first reduction, where
no oversampling of the underlying model is needed. We also show that
$\mathsf{PMTL}$ is strictly more expressive than the fragments
$\mathsf{MTL}[\until_I,\since]$ and $\mathsf{MTL}[\until,\since_I]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6966</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6966</id><created>2014-04-25</created><authors><author><keyname>Tugores</keyname><forenames>Ant&#xf2;nia</forenames></author><author><keyname>Colet</keyname><forenames>Pere</forenames></author></authors><title>Mining online social networks with Python to study urban mobility</title><categories>cs.SI cs.PL</categories><comments>Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2013-04</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  On-line social networks have grown quickly over the last few years and
nowadays many people use them frequently. Furthermore the emergence of
smartphones allows to access these networks any time from any physical
location. Among the social networks, Twitter offers a particularly large set of
data publicly available. Here we discuss the procedure to mine this data and
store it in distributed databases using Python scripts. We also illustrate how
geolocated tweets can be used to study the mobility of people in urban areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6974</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6974</id><created>2014-04-28</created><updated>2014-09-18</updated><authors><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Schon</keyname><forenames>Claudia</forenames></author></authors><title>Deontic Logic for Human Reasoning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deontic logic is shown to be applicable for modelling human reasoning. For
this the Wason selection task and the suppression task are discussed in detail.
Different versions of modelling norms with deontic logic are introduced and in
the case of the Wason selection task it is demonstrated how differences in the
performance of humans in the abstract and in the social contract case can be
explained. Furthermore it is shown that an automated theorem prover can be used
as a reasoning tool for deontic logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6979</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6979</id><created>2014-04-28</created><authors><author><keyname>Stewart</keyname><forenames>I M</forenames></author></authors><title>An adjustable-width window with good dynamic range</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new variable-width window is presented and compared with several other
windows, both of variable and fixed widths. The comparison focuses on
sensitivity and dynamic range. The equivalent noise bandwidth or ENBW (or
rather, its reciprocal) is used as a proxy for the first; maximum sidelobe
level and high-frequency roll-off in the Fourier transform, for the second. The
new window can access any value of ENBW by appropriate choice of the width
parameter. At any given value of ENBW below about 3, a setting can be found at
which the sidelobes of the window are lower than those of any other in the
moderate frequency regime below about 100 cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6981</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6981</id><created>2014-04-28</created><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>Konrad</forenames></author><author><keyname>Grobler-D&#x119;bska</keyname><forenames>Katarzyna</forenames></author><author><keyname>W&#x105;s</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Heuristic rating estimation - geometric approach</title><categories>cs.DM</categories><comments>11 pages</comments><journal-ref>Journal of Global Optimization, July 2015, Volume 62, Issue 3, pp
  529-543</journal-ref><doi>10.1007/s10898-014-0253-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heuristic Rating Estimation (HRE) is a newly proposed method supporting
decisions analysis based on the use of pairwise comparisons. It allows that the
ranking values of some alternatives (herein referred to as concepts) are
initially known, whilst the ranks for the other concepts have yet to be
estimated. To calculate the missing ranks it is assumed that the priority of
every single concept can be determined as the weighted arithmetic mean of
priorities of all the other concepts. It has been shown that the problem has
admissible solution if the inconsistency of pairwise comparisons is not too
high. The proposed approach adopts the heuristics according to which to
determine the missing priorities a weighted geometric mean is used. In this
approach, despite an increased complexity, the solution always exists and their
existence does not depend on the inconsistency of the input matrix. Thus, the
presented approach might be appropriate for a larger number of problems than
the previous method. The formal definition of the proposed geometric heuristics
is accompanied by two numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6984</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6984</id><created>2014-04-28</created><authors><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author></authors><title>An Extremal Inequality for Long Markov Chains</title><categories>cs.IT math.IT</categories><comments>18 pages, 1 figure. Submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X,Y$ be jointly Gaussian vectors, and consider random variables $U,V$
that satisfy the Markov constraint $U-X-Y-V$. We prove an extremal inequality
relating the mutual informations between all ${4 \choose 2}$ pairs of random
variables from the set $(U,X,Y,V)$. As a first application, we show that the
rate region for the two-encoder quadratic Gaussian source coding problem
follows as an immediate corollary of the the extremal inequality. In a second
application, we establish the rate region for a vector-Gaussian source coding
problem where L\&quot;{o}wner-John ellipsoids are approximated based on
rate-constrained descriptions of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6993</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6993</id><created>2014-04-28</created><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>de Oliveira</keyname><forenames>Paulo Murilo Castro</forenames></author></authors><title>The first shall be last: selection-driven minority becomes majority</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><comments>8 pages, 4 figures, accepted for publication in Physica A</comments><journal-ref>Physica A 409, 48 (2014)</journal-ref><doi>10.1016/j.physa.2014.04.033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Street demonstrations occur across the world. In Rio de Janeiro, June/July
2013, they reach beyond one million people. A wrathful reader of \textit{O
Globo}, leading newspaper in the same city, published a letter \cite{OGlobo}
where many social questions are stated and answered Yes or No. These million
people of street demonstrations share opinion consensus about a similar set of
social issues. But they did not reach this consensus within such a huge
numbered meetings. Earlier, they have met in diverse small groups where some of
them could be convinced to change mind by other few fellows. Suddenly, a
macroscopic consensus emerges. Many other big manifestations are widespread all
over the world in recent times, and are supposed to remain in the future. The
interesting questions are: 1) How a binary-option opinion distributed among
some population evolves in time, through local changes occurred within
small-group meetings? and 2) Is there some natural selection rule acting upon?
Here, we address these questions through an agent-based model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.6999</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.6999</id><created>2014-04-28</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Dodaro</keyname><forenames>Carmine</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>Preliminary Report on WASP 2.0</title><categories>cs.AI</categories><comments>The paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a declarative programming paradigm. The
intrinsic complexity of the evaluation of ASP programs makes the development of
more effective and faster systems a challenging research topic. This paper
reports on the recent improvements of the ASP solver WASP. WASP is undergoing a
refactoring process which will end up in the release of a new and more
performant version of the software. In particular the paper focus on the
improvements to the core evaluation algorithms working on normal programs. A
preliminary experiment on benchmarks from the 3rd ASP competition belonging to
the NP class is reported. The previous version of WASP was often not
competitive with alternative solutions on this class. The new version of WASP
shows a substantial increase in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7006</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7006</id><created>2014-04-28</created><updated>2015-06-23</updated><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author></authors><title>Parameterized Complexity Dichotomy for Steiner Multicut</title><categories>cs.DS cs.DM</categories><comments>As submitted to journal. This version also adds a proof of
  fixed-parameter tractability for parameter k+t using the technique of
  randomized contractions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Steiner Multicut problem asks, given an undirected graph G, terminals
sets T1,...,Tt $\subseteq$ V(G) of size at most p, and an integer k, whether
there is a set S of at most k edges or nodes s.t. of each set Ti at least one
pair of terminals is in different connected components of G \ S. This problem
generalizes several graph cut problems, in particular the Multicut problem (the
case p = 2), which is fixed-parameter tractable for the parameter k [Marx and
Razgon, Bousquet et al., STOC 2011].
  We provide a dichotomy of the parameterized complexity of Steiner Multicut.
That is, for any combination of k, t, p, and the treewidth tw(G) as constant,
parameter, or unbounded, and for all versions of the problem (edge deletion and
node deletion with and without deletable terminals), we prove either that the
problem is fixed-parameter tractable or that the problem is hard (W[1]-hard or
even (para-)NP-complete). We highlight that:
  - The edge deletion version of Steiner Multicut is fixed-parameter tractable
for the parameter k+t on general graphs (but has no polynomial kernel, even on
trees). We present two proofs: one using the randomized contractions technique
of Chitnis et al, and one relying on new structural lemmas that decompose the
Steiner cut into important separators and minimal s-t cuts.
  - In contrast, both node deletion versions of Steiner Multicut are W[1]-hard
for the parameter k+t on general graphs.
  - All versions of Steiner Multicut are W[1]-hard for the parameter k, even
when p=3 and the graph is a tree plus one node. Hence, the results of Marx and
Razgon, and Bousquet et al. do not generalize to Steiner Multicut.
  Since we allow k, t, p, and tw(G) to be any constants, our characterization
includes a dichotomy for Steiner Multicut on trees (for tw(G) = 1), and a
polynomial time versus NP-hardness dichotomy (by restricting k,t,p,tw(G) to
constant or unbounded).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7015</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7015</id><created>2014-04-28</created><authors><author><keyname>Fu</keyname><forenames>Yuxi</forenames></author><author><keyname>Yin</keyname><forenames>Qiang</forenames></author></authors><title>Dividing Line between Decidable PDA's and Undecidable Ones</title><categories>cs.LO</categories><comments>26 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Senizergues has proved that language equivalence is decidable for disjoint
epsilon-deterministic PDA. Stirling has showed that strong bisimilarity is
decidable for PDA. On the negative side Srba demonstrated that the weak
bisimilarity is undecidable for normed PDA. Later Jancar and Srba established
the undecidability of the weak bisimilarity for disjoint epsilon-pushing PDA
and disjoint epsilon-popping PDA. These decidability and undecidability results
are extended in the present paper. The extension is accomplished by looking at
the equivalence checking issue for the branching bisimilarity of several
variants of PDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7017</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7017</id><created>2014-04-28</created><authors><author><keyname>Hung</keyname><forenames>N. M.</forenames></author><author><keyname>Nam</keyname><forenames>N. H.</forenames></author></authors><title>On the procedural structure of learning ecosystem toward competency
  learning model</title><categories>cs.CY</categories><comments>Published in Special Issue of the Journal of Science, Vol 53 (87),
  pp13-23. ISSN: 1859-3100</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Ecosystem is new model for learning, that addresses to holistic
learning model with attention to practical implementation. This paper is
conducting the further study on detailed structure of learning ecosystem in
component and procedural view. As case study, it connects Learning Ecosystem to
Competency Education as Competency Learning Ecosystem Model for reference for
practical use.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="59000" completeListSize="102538">1122234|60001</resumptionToken>
</ListRecords>
</OAI-PMH>
