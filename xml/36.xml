<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:03:39Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|35001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1208.1983</identifier>
 <datestamp>2012-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.1983</id><created>2012-08-09</created><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author></authors><title>An algorithm for improving the quality of compacted JPEG image by
  minimizes the blocking artifacts</title><categories>cs.GR</categories><comments>18 pages with 19 Figures, To appear in the Proceedings of
  International Journal of Computer Graphics &amp; Animation (IJCGA) Vol.2, No.2/3,
  July 2012, 17-35</comments><report-no>2212ijcga02</report-no><msc-class>68U10</msc-class><acm-class>I.4.2</acm-class><journal-ref>International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol.2, No.2/3, July 2012, 17-35</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The Block Transform Coded, JPEG- a lossy image compression format has been
used to keep storage and bandwidth requirements of digital image at practical
levels. However, JPEG compression schemes may exhibit unwanted image artifacts
to appear - such as the 'blocky' artifact found in smooth/monotone areas of an
image, caused by the coarse quantization of DCT coefficients. A number of image
filtering approaches have been analyzed in literature incorporating
value-averaging filters in order to smooth out the discontinuities that appear
across DCT block boundaries. Although some of these approaches are able to
decrease the severity of these unwanted artifacts to some extent, other
approaches have certain limitations that cause excessive blurring to
high-contrast edges in the image. The image deblocking algorithm presented in
this paper aims to filter the blocked boundaries. This is accomplished by
employing smoothening, detection of blocked edges and then filtering the
difference between the pixels containing the blocked edge. The deblocking
algorithm presented has been successful in reducing blocky artifacts in an
image and therefore increases the subjective as well as objective quality of
the reconstructed image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.1984</identifier>
 <datestamp>2012-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.1984</id><created>2012-08-09</created><authors><author><keyname>Kanchu</keyname><forenames>Krishnama Raju</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Goldbach Ellipse Sequences for Cryptographic Applications</title><categories>cs.CR</categories><comments>8 pages, 5 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies cryptographically useful properties of the sequence of the
sizes of Goldbach ellipses. We show that binary subsequences based on this
sequence have useful properties. They can be used to generate keys and to
provide an index-based mapping to numbers. The paper also presents a protocol
for secure session keys that is based on Goldbach partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2002</identifier>
 <datestamp>2012-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2002</id><created>2012-08-09</created><authors><author><keyname>Balan</keyname><forenames>Horia Vlad</forenames></author><author><keyname>Psounis</keyname><forenames>Konstantinos</forenames></author></authors><title>Tag Spotting at the Interference Range</title><categories>cs.NI</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless networks, the presence of interference among wireless links in-
troduces dependencies among flows that do not share a single link or node. As a
result, when designing a resource allocation scheme, be it a medium access
scheduler or a flow rate controller, one needs to consider the interdependence
among nodes within interference range of each other. Specifically, control
plane information needs to reach nearby nodes which often lie outside the
communi- cation range, but within the interference range of a node of interest.
But how can one communicate control plane information well beyond the existing
communication range? To address this fundamental need we introduce tag
spotting. Tag spotting refers to a communication system which allows re- liable
control data transmission at SNR values as low as 0 dB. It does this by
employing a number of signal encoding techniques including adding redundancy to
multitone modulation, shaping the spectrum to reduce inter-carrier interfer-
ence, and the use of algebraic coding. Making use of a detection theory-based
model we analyze the performance achievable by our modulation as well as the
trade-off between the rate of the information transmitted and the likelihood of
error. Using real-world experiments on an OFDM system built with software
radios, we show that we can transmit data at the target SNR value of 0 dB with
a 6% overhead; that is, 6% of our packet is used for our low-SNR decodable tags
(which carry up to a couple of bytes of data in our testbed), while the remain-
ing 94% is used for traditional header and payload data. We also demonstrate
via simulations how tag spotting can be used in implementing fair and efficient
rate control and scheduling schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2013</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2013</id><created>2012-08-09</created><authors><author><keyname>Cheung</keyname><forenames>Alvin</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author></authors><title>Inferring SQL Queries Using Program Synthesis</title><categories>cs.PL cs.DB</categories><acm-class>H.2.3; I.2.2; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing high-performance applications that interact with databases is a
difficult task, as developers need to understand both the details of the
language in which their applications are written in, and also the intricacies
of the relational model. One popular solution to this problem is the use of
object-relational mapping (ORM) libraries that provide transparent access to
the database using the same language that the application is written in.
Unfortunately, using such frameworks can easily lead to applications with poor
performance because developers often end up implementing relational operations
in application code, and doing so usually does not take advantage of the
optimized implementations of relational operations, efficient query plans, or
push down of predicates that database systems provide. In this paper we present
QBS, an algorithm that automatically identifies fragments of application logic
that can be pushed into SQL queries. The QBS algorithm works by automatically
synthesizing invariants and postconditions for the original code fragment. The
postconditions and invariants are expressed using a theory of ordered relations
that allows us to reason precisely about the contents and order of the records
produced even by complex code fragments that compute joins and aggregates. The
theory is close in expressiveness to SQL, so the synthesized postconditions can
be readily translated to SQL queries. Using 40 code fragments extracted from
over 120k lines of open-source code written using the Java Hibernate ORM, we
demonstrate that our approach can convert a variety of imperative constructs
into relational specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2015</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2015</id><created>2012-08-09</created><updated>2013-05-22</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>Sharp analysis of low-rank kernel matrix approximations</title><categories>cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>International Conference on Learning Theory (COLT), \'Etats-Unis
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider supervised learning problems within the positive-definite kernel
framework, such as kernel ridge regression, kernel logistic regression or the
support vector machine. With kernels leading to infinite-dimensional feature
spaces, a common practical limiting difficulty is the necessity of computing
the kernel matrix, which most frequently leads to algorithms with running time
at least quadratic in the number of observations n, i.e., O(n^2). Low-rank
approximations of the kernel matrix are often considered as they allow the
reduction of running time complexities to O(p^2 n), where p is the rank of the
approximation. The practicality of such methods thus depends on the required
rank p. In this paper, we show that in the context of kernel ridge regression,
for approximations based on a random subset of columns of the original kernel
matrix, the rank p may be chosen to be linear in the degrees of freedom
associated with the problem, a quantity which is classically used in the
statistical analysis of such methods, and is often seen as the implicit number
of parameters of non-parametric estimators. This result enables simple
algorithms that have sub-quadratic running time complexity, but provably
exhibit the same predictive performance than existing algorithms, for any given
problem instance, and not only for worst-case situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2043</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2043</id><created>2012-08-09</created><updated>2013-12-02</updated><authors><author><keyname>Vats</keyname><forenames>Divyanshu</forenames></author></authors><title>High-Dimensional Screening Using Multiple Grouping of Variables</title><categories>stat.ML cs.IT math.IT</categories><comments>This paper will appear in the IEEE Transactions on Signal Processing.
  See http://www.ima.umn.edu/~dvats/MuGScreening.html for more details</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Screening is the problem of finding a superset of the set of non-zero entries
in an unknown p-dimensional vector \beta* given n noisy observations.
Naturally, we want this superset to be as small as possible. We propose a novel
framework for screening, which we refer to as Multiple Grouping (MuG), that
groups variables, performs variable selection over the groups, and repeats this
process multiple number of times to estimate a sequence of sets that contains
the non-zero entries in \beta*. Screening is done by taking an intersection of
all these estimated sets. The MuG framework can be used in conjunction with any
group based variable selection algorithm. In the high-dimensional setting,
where p &gt;&gt; n, we show that when MuG is used with the group Lasso estimator,
screening can be consistently performed without using any tuning parameter. Our
numerical simulations clearly show the merits of using the MuG framework in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2060</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2060</id><created>2012-08-09</created><authors><author><keyname>Khlifi</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Comparison between Performances of Channel Estimation Techniques for
  CP-LTE and ZP-LTE Downlink Systems</title><categories>cs.NI</categories><comments>11 pages, 11 figures</comments><doi>10.5121/ijcnc.2012.4414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to evaluate the performance of channel estimation
techniques for Long Term Evolution (LTE) Downlink systems based on Zero Padding
technique (ZP) instead of Cyclic Prefixing (CP). LTE Downlink system is a
multiuser system based on a MIMO-OFDMA technology. Usually, in OFDM systems, a
guard interval is inserted in order to mitigate both inter-carrier interference
(ICI) and inter-symbol interference (ISI). LTE Downlink systems are based on
CP-OFDM technique which consists of a copy a last OFDM symbols inserted at the
beginning of each transmitted OFDM symbol. Although this technique shows good
performances, the CP-LTE system suffers from a power efficiency loss.With the
number of present OFDM symbols in LTE Downlink radio frame, the bandwidth loss
becomes more important. Instead of CP, we propose to evaluate the performance
of ZP-LTE systems in order to avoid the power efficiency .In this paper, we
interest to evaluate the performance of channel estimation techniques for the
two LTE Downlink systems. Simulations results show that although ZP-LTE systems
outperform CP-LTE Downlink systems in terms of power efficiency, the CP-LTE
systems show better performance than ZP-LTE systems and especially for high SNR
values. MATLAB Monte-Carlo simulations are used to evaluate the performance of
LS, LMMSE and Lr-LMMSE estimators in terms of Mean Square Error (MSE) and Bit
Error Rate (BER) for 2x2 LTE Downlink systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2073</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2073</id><created>2012-08-09</created><authors><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author></authors><title>Discovery of Malicious Attacks to Improve Mobile Collaborative Learning
  (MCL)</title><categories>cs.NI cs.CR</categories><comments>20 pages and 11 figures; International Journal of Computer Networks
  and Communications (IJCNC) July 2012, Volume 4. Number 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile collaborative learning (MCL) is highly acknowledged and focusing
paradigm in eductional institutions and several organizations across the world.
It exhibits intellectual synergy of various combined minds to handle the
problem and stimulate the social activity of mutual understanding. To improve
and foster the baseline of MCL, several supporting architectures, frameworks
including number of the mobile applications have been introduced. Limited
research was reported that particularly focuses to enhance the security of
those pardigms and provide secure MCL to users. The paper handles the issue of
rogue DHCP server that affects and disrupts the network resources during the
MCL. The rogue DHCP is unauthorized server that releases the incorrect IP
address to users and sniffs the traffic illegally. The contribution specially
provides the privacy to users and enhances the security aspects of mobile
supported collaborative framework (MSCF). The paper introduces multi-frame
signature-cum anomaly-based intrusion detection systems (MSAIDS) supported with
novel algorithms through addition of new rules in IDS and mathematcal model.
The major target of contribution is to detect the malicious attacks and blocks
the illegal activities of rogue DHCP server. This innovative security mechanism
reinforces the confidence of users, protects network from illicit intervention
and restore the privacy of users. Finally, the paper validates the idea through
simulation and compares the findings with other existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2076</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2076</id><created>2012-08-09</created><updated>2013-11-21</updated><authors><author><keyname>Kim</keyname><forenames>Ryul</forenames></author><author><keyname>Sin</keyname><forenames>Myong-Son</forenames></author><author><keyname>Song</keyname><forenames>Ok-Hyon</forenames></author></authors><title>Upper Bounds on the Number of Codewords of Some Separating Codes</title><categories>cs.IT cs.CR math.IT</categories><comments>7 pages, in version 2 corrected typos</comments><report-no>KISU-MATH-2012-E-R-001</report-no><journal-ref>Journal of Theoretical Physics and Cryptography, 2013, Vol.2,
  pp17-20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separating codes have their applications in collusion-secure fingerprinting
for generic digital data, while they are also related to the other structures
including hash family, intersection code and group testing. In this paper we
study upper bounds for separating codes. First, some new upper bound for
restricted separating codes is proposed. Then we illustrate that the Upper
Bound Conjecture for separating Reed-Solomon codes inherited from Silverberg's
question holds true for almost all Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2078</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2078</id><created>2012-08-09</created><authors><author><keyname>Van</keyname><forenames>Vo Tam</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author></authors><title>Non-homogeneous distributed storage systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a non-homogeneous distributed storage systems (DSS),
where there is one super node which has a larger storage size and higher
reliability and availability than the other storage nodes. We propose three
distributed storage schemes based on (k+2; k) maximum distance separable (MDS)
codes and non-MDS codes to show the efficiency of such non-homogeneous DSS in
terms of repair efficiency and data availability. Our schemes achieve optimal
bandwidth (k+1/2)(M/k) when repairing 1-node failure, but require only one
fourth of the minimum required file size and can operate with a smaller field
size leading to significant complexity reduction than traditional homogeneous
DSS. Moreover, with non-MDS codes, our scheme can achieve an even smaller
repair bandwidth of M/2k . Finally, we show that our schemes can increase the
data availability by 10% than the traditional homogeneous DSS scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2079</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2079</id><created>2012-08-09</created><authors><author><keyname>Forootaninia</keyname><forenames>A.</forenames></author><author><keyname>Ghaznavi-Ghoushchi</keyname><forenames>M. B.</forenames></author></authors><title>An Improved Watchdog Technique Based On Power-Aware Hierarchical Design
  For Ids In Wireless Sensor Networks</title><categories>cs.NI</categories><comments>http://airccse.org/journal/nsa/0712nsa11.pdf</comments><journal-ref>(2012) International Journal of Network Security &amp; Its
  Applications (IJNSA), 4(4), 161 - 178</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preserving security and confidentiality in wireless sensor networks (WSN) are
crucial. Wireless sensor networks in comparison with wired networks are more
substantially vulnerable to attacks and intrusions. In WSN, a third person can
eavesdrop to the information or link to the network. So, preventing these
intrusions by detecting them has become one of the most demanding challenges.
This paper, proposes an improved watchdog technique as an effective technique
for detecting malicious nodes based on a power aware hierarchical model. This
technique overcomes the common problems in the original Watchdog mechanism. The
main purpose to present this model is reducing the power consumption as a key
factor for increasing the network's lifetime. For this reason, we simulated our
model with Tiny-OS simulator and then, compared our results with non
hierarchical model to ensure the improvement. The results indicate that, our
proposed model is better in performance than the original models and it has
increased the lifetime of the wireless sensor nodes by around 2611.492 seconds
for a network with 100 sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2092</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2092</id><created>2012-08-10</created><authors><author><keyname>Rajalakshmi</keyname><forenames>M.</forenames></author><author><keyname>Subashini</keyname><forenames>P.</forenames></author></authors><title>A study on non-destructive method for detecting Toxin in pepper using
  Neural networks</title><categories>cs.NE cs.CV</categories><comments>11 pages,1 figure; International Journal of Artificial Intelligence &amp;
  Applications (IJAIA), Vol.3, No.4, July 2012</comments><doi>10.5121/ijaia.2012.3414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mycotoxin contamination in certain agricultural systems have been a serious
concern for human and animal health. Mycotoxins are toxic substances produced
mostly as secondary metabolites by fungi that grow on seeds and feed in the
field, or in storage. The food-borne Mycotoxins likely to be of greatest
significance for human health in tropical developing countries are Aflatoxins
and Fumonisins. Chili pepper is also prone to Aflatoxin contamination during
harvesting, production and storage periods.Various methods used for detection
of Mycotoxins give accurate results, but they are slow, expensive and
destructive. Destructive method is testing a material that degrades the sample
under investigation. Whereas, non-destructive testing will, after testing,
allow the part to be used for its intended purpose. Ultrasonic methods,
Multispectral image processing methods, Terahertz methods, X-ray and
Thermography have been very popular in nondestructive testing and
characterization of materials and health monitoring. Image processing methods
are used to improve the visual quality of the pictures and to extract useful
information from them. In this proposed work, the chili pepper samples will be
collected, and the X-ray, multispectral images of the samples will be processed
using image processing methods. The term &quot;Computational Intelligence&quot; referred
as simulation of human intelligence on computers. It is also called as
&quot;Artificial Intelligence&quot; (AI) approach. The techniques used in AI approach are
Neural network, Fuzzy logic and evolutionary computation. Finally, the
computational intelligence method will be used in addition to image processing
to provide best, high performance and accurate results for detecting the
Mycotoxin level in the samples collected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2102</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2102</id><created>2012-08-10</created><authors><author><keyname>Kareem</keyname><forenames>Abdul</forenames></author><author><keyname>Azeem</keyname><forenames>Mohammad Fazle</forenames></author></authors><title>A Novel Fuzzy Logic Based Adaptive Supertwisting Sliding Mode Control
  Algorithm for Dynamic Uncertain Systems</title><categories>cs.AI</categories><comments>14 pages</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.4, July 2012, 21-34</journal-ref><doi>10.5121/ijaia.2012.3402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel fuzzy logic based Adaptive Super-twisting Sliding
Mode Controller for the control of dynamic uncertain systems. The proposed
controller combines the advantages of Second order Sliding Mode Control, Fuzzy
Logic Control and Adaptive Control. The reaching conditions, stability and
robustness of the system with the proposed controller are guaranteed. In
addition, the proposed controller is well suited for simple design and
implementation. The effectiveness of the proposed controller over the first
order Sliding Mode Fuzzy Logic controller is illustrated by Matlab based
simulations performed on a DC-DC Buck converter. Based on this comparison, the
proposed controller is shown to obtain the desired transient response without
causing chattering and error under steady-state conditions. The proposed
controller is able to give robust performance in terms of rejection to input
voltage variations and load variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2112</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2112</id><created>2012-08-10</created><updated>2013-01-21</updated><authors><author><keyname>Qiao</keyname><forenames>Qifeng</forenames></author><author><keyname>Beling</keyname><forenames>Peter A.</forenames></author></authors><title>Inverse Reinforcement Learning with Gaussian Process</title><categories>cs.LG</categories><comments>conferencel American Control Conference 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new algorithms for inverse reinforcement learning (IRL, or inverse
optimal control) in convex optimization settings. We argue that finite-space
IRL can be posed as a convex quadratic program under a Bayesian inference
framework with the objective of maximum a posterior estimation. To deal with
problems in large or even infinite state space, we propose a Gaussian process
model and use preference graphs to represent observations of decision
trajectories. Our method is distinguished from other approaches to IRL in that
it makes no assumptions about the form of the reward function and yet it
retains the promise of computationally manageable implementations for potential
real-world applications. In comparison with an establish algorithm on
small-scale numerical problems, our method demonstrated better accuracy in
apprenticeship learning and a more robust dependence on the number of
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2113</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2113</id><created>2012-08-10</created><authors><author><keyname>Mosler</keyname><forenames>Karl</forenames></author><author><keyname>Bazovkin</keyname><forenames>Pavel</forenames></author></authors><title>Stochastic linear programming with a distortion risk constraint</title><categories>stat.ME cs.CG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Linear optimization problems are investigated whose parameters are uncertain.
We apply coherent distortion risk measures to capture the possible violation of
a restriction. Each risk constraint induces an uncertainty set of coefficients,
which is shown to be a weighted-mean trimmed region. Given an external sample
of the coefficients, an uncertainty set is a convex polytope that can be
exactly calculated. We construct an efficient geometrical algorithm to solve
stochastic linear programs that have a single distortion risk constraint. The
algorithm is available as an R-package. Also the algorithm's asymptotic
behavior is investigated, when the sample is i.i.d. from a general probability
distribution. Finally, we present some computational experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2115</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2115</id><created>2012-08-10</created><authors><author><keyname>Etxeberria-Agiriano</keyname><forenames>Ismael</forenames></author><author><keyname>Calvo</keyname><forenames>Isidro</forenames></author><author><keyname>Montero</keyname><forenames>Liliana</forenames></author><author><keyname>Alonso</keyname><forenames>Ivan</forenames></author></authors><title>A DDS-Based Scalable and Reconfigurable Framework for Cyber-Physical
  Systems</title><categories>cs.DC cs.NI</categories><comments>13 pages, 6 figures</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), vol. 3, no. 4 (2012) 25-37</journal-ref><doi>10.5121/ijsea.2012.3403</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cyber-Physical Systems (CPSs) involve the interconnection of heterogeneous
computing devices which are closely integrated with the physical processes
under control. Often, these systems are resource-constrained and require
specific features such as the ability to adapt in a timeliness and efficient
fashion to dynamic environments. Also, they must support fault tolerance and
avoid single points of failure. This paper describes a scalable framework for
CPSs based on the OMG DDS standard. The proposed solution allows reconfiguring
this kind of systems at run-time and managing efficiently their resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2116</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2116</id><created>2012-08-10</created><updated>2012-10-08</updated><authors><author><keyname>K.</keyname><forenames>Ishaque Ashar</forenames></author><author><keyname>V.</keyname><forenames>Prathyusha</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author></authors><title>Outer Bounds for the Capacity Region of a Gaussian Two-way Relay Channel</title><categories>cs.IT math.IT</categories><comments>Presented at Allerton Conference on Communication, Control and
  Computing 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a three-node half-duplex Gaussian relay network where two nodes
(say $a$, $b$) want to communicate with each other and the third node acts as a
relay for this twoway communication. Outer bounds and achievable rate regions
for the possible rate pairs $(R_a, R_b)$ for two-way communication are
investigated. The modes (transmit or receive) of the halfduplex nodes together
specify the state of the network. A relaying protocol uses a specific sequence
of states and a coding scheme for each state. In this paper, we first obtain an
outer bound for the rate region of all achievable $(R_a,R_b)$ based on the
half-duplex cut-set bound. This outer bound can be numerically computed by
solving a linear program. It is proved that at any point on the boundary of the
outer bound only four of the six states of the network are used. We then
compare it with achievable rate regions of various known protocols. We consider
two kinds of protocols: (1) protocols in which all messages transmitted in a
state are decoded with the received signal in the same state, and (2) protocols
where information received in one state can also be stored and used as side
information to decode messages in future states. Various conclusions are drawn
on the importance of using all states, use of side information, and the choice
of processing at the relay. Then, two analytical outer bounds (as opposed to an
optimization problem formulation) are derived. Using an analytical outer bound,
we obtain the symmetric capacity within 0.5 bits for some channel conditions
where the direct link between nodes a and b is weak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2121</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2121</id><created>2012-08-10</created><authors><author><keyname>Sridhar</keyname><forenames>Murali</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author></authors><title>On the Sum Rate of a 2 x 2 Interference Network</title><categories>cs.IT math.IT</categories><comments>Presented at IEEE ICC 2012, Ottawa, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an M x N interference network, there are M transmitters and N receivers
with each transmitter having independent messages for each of the 2^N -1
possible non-empty subsets of the receivers. We consider the 2 x 2 interference
network with 6 possible messages, of which the 2 x 2 interference channel and X
channel are special cases obtained by using only 2 and 4 messages respectively.
Starting from an achievable rate region similar to the Han-Kobayashi region, we
obtain an achievable sum rate. For the Gaussian interference network, we
determine which of the 6 messages are sufficient for maximizing the sum rate
within this rate region for the low, mixed, and strong interference conditions.
It is observed that 2 messages are sufficient in several cases. Finally, we
show that sum capacity is achieved using only 2 messages for a subset of the
mixed interference conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2125</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2125</id><created>2012-08-10</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Muscholl</keyname><forenames>Anca</forenames></author></authors><title>On distributed monitoring of asynchronous systems</title><categories>cs.FL cs.LO</categories><comments>Paper appears as an invited lecture at WoLLIC 2012, 19th Workshop on
  Logic, Language, Information and Computation. September 3rd to 6th, 2012
  University of Buenos Aires, Buenos Aires, Argentina</comments><msc-class>68Q45</msc-class><acm-class>F.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed systems are notoriously difficult to understand and analyze in
order to assert their correction w.r.t. given properties. They often exhibit a
huge number of different behaviors, as soon as the active entities (peers,
agents, processes, etc) behave in an asynchronous manner. Already the
modelization of such systems is a non-trivial task, let alone their formal
verification.
  The purpose of this paper is to discuss the problem of distributed monitoring
on a simple model of finite-state distributed automata based on shared actions,
called asynchronous automata. Monitoring is a question related to runtime
verification: assume that we have to check a property $L$ against an unknown or
very complex system $A$, so that classical static analysis is not possible.
Therefore instead of model-checking a monitor is used, that checks the property
on the underlying system at runtime.
  We are interested here in monitoring distributed systems modeled as
asynchronous automata. It is natural to require that monitors should be of the
same kind as the underlying system, so we consider here distributed monitoring.
A distributed monitor does not have a global view of the system, therefore we
propose the notion of locally monitorable trace language. Our main result shows
that if the distributed alphabet of actions is connected and if $L$ is a set of
infinite traces such that both $L$ and its complement $L^c$ are countable
unions of locally safety languages, then $L$ is locally monitorable. We also
show that over infinite traces, recognizable countable unions of locally safety
languages are precisely the complements of deterministic languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2128</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2128</id><created>2012-08-10</created><authors><author><keyname>Rathi</keyname><forenames>V. P. Gladis Pushpa</forenames></author><author><keyname>Palani</keyname><forenames>S.</forenames></author></authors><title>Brain tumor MRI image classification with feature selection and
  extraction using linear discriminant analysis</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature extraction is a method of capturing visual content of an image. The
feature extraction is the process to represent raw image in its reduced form to
facilitate decision making such as pattern classification. We have tried to
address the problem of classification MRI brain images by creating a robust and
more accurate classifier which can act as an expert assistant to medical
practitioners. The objective of this paper is to present a novel method of
feature selection and extraction. This approach combines the Intensity,
Texture, shape based features and classifies the tumor as white matter, Gray
matter, CSF, abnormal and normal area. The experiment is performed on 140 tumor
contained brain MR images from the Internet Brain Segmentation Repository. The
proposed technique has been carried out over a larger database as compare to
any previous work and is more robust and effective. PCA and Linear Discriminant
Analysis (LDA) were applied on the training sets. The Support Vector Machine
(SVM) classifier served as a comparison of nonlinear techniques Vs linear ones.
PCA and LDA methods are used to reduce the number of features used. The feature
selection using the proposed technique is more beneficial as it analyses the
data according to grouping class variable and gives reduced feature set with
high classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2154</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2154</id><created>2012-08-10</created><authors><author><keyname>Gonzalez</keyname><forenames>Javier G.</forenames></author><author><keyname>Collaboration</keyname><forenames>for the Pierre Auger</forenames></author></authors><title>The Offline Software of the Pierre Auger Observatory: Lessons Learned</title><categories>astro-ph.IM astro-ph.HE cs.SE</categories><comments>Presented at the International Workshop on AstroParticles Physics
  Advanced Computing, ISPA 2013, Madrid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Offline software framework for data analysis of the Pierre Auger
Observatory is a set of computational tools developed to cater to the needs of
a large and geographically dispersed collaboration established to measure the
spectrum, arrival directions, and composition of ultra-high energy cosmic rays
over a period of 20 years. One of its design goals was to facilitate the
collaborative effort by allowing collaborators to progressively contribute
small portions of code. The observatory has grown over time and it has
undergone improvements and additions that have tested the flexibility of the
framework. The framework was originally thought to accommodate a hybrid view of
cosmic ray detection, made of a surface and a fluorescence detector. Since
then, the framework has been extended to include a radio antenna array and both
under-ground and above-ground scintillator arrays. Different tools from the
framework have been used by other collaborations, notably NA61/Shine and HAWC.
All these experiences accumulated over the years allow us to draw conclusions
in terms of the successes and failures of the original design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2159</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2159</id><created>2012-08-10</created><updated>2012-09-28</updated><authors><author><keyname>Wolf</keyname><forenames>Karsten</forenames><affiliation>Universit&#xe4;t Rostock, Institut f&#xfc;r Informatik</affiliation></author><author><keyname>Wimmel</keyname><forenames>Harro</forenames><affiliation>Universit&#xe4;t Rostock, Institut f&#xfc;r Informatik</affiliation></author></authors><title>Applying CEGAR to the Petri Net State Equation</title><categories>cs.LO cs.DS</categories><proxy>LMCS</proxy><acm-class>F.2.2, I.6.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  29, 2012) lmcs:1036</journal-ref><doi>10.2168/LMCS-8(3:27)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a reachability verification technique that combines the Petri net
state equation (a linear algebraic overapproximation of the set of reachable
states) with the concept of counterexample guided abstraction refinement. In
essence, we replace the search through the set of reachable states by a search
through the space of solutions of the state equation. We demonstrate the
excellent performance of the technique on several real-world examples. The
technique is particularly useful in those cases where the reachability query
yields a negative result: While state space based techniques need to fully
expand the state space in this case, our technique often terminates promptly.
In addition, we can derive some diagnostic information in case of
unreachability while state space methods can only provide witness paths in the
case of reachability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2169</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2169</id><created>2012-08-10</created><authors><author><keyname>Merit</keyname><forenames>Khaled</forenames></author><author><keyname>Ouamri</keyname><forenames>Abdelazziz</forenames></author></authors><title>Securing Speech in GSM Networks using DES with Random Permutation and
  Inversion Algorithm</title><categories>cs.CR</categories><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.4, July 2012, 157-164</journal-ref><doi>10.5121/ijdps.2012.3416</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global System for Mobile Communications (GSM) is one of the most commonly
used cellular technologies in the world. One of the objectives in mobile
communication systems is the security of the exchanged data. GSM employs many
cryptographic algorithms for security like A5/1, A5/2 and A5/3. Even so, these
algorithms do not provide sufficient level of security for protecting the
confidentiality of GSM. Therefore, it is desirable to increase security by
additional encryption methods. This paper presents a voice encryption method
called: &quot;DES with Random permutation and Inversion&quot;, based on current voice
channel, which overcomes data channel's insufficiencies and solves the problem
of penetrating the RPE-LTP vocoder by the encrypted voice. The proposed method
fulfils an end-to-end secured communication in the GSM; insure a good
compatibility to all GSM networks, and easy implementation without any
modification in these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2171</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2171</id><created>2012-08-10</created><authors><author><keyname>Rao</keyname><forenames>Shravas</forenames></author></authors><title>Finding hitting times in various graphs</title><categories>math.PR cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hitting time, h_uv, of a random walk on a finite graph G, is the expected
time for the walk to reach vertex v given that it started at vertex u. We
present two methods of calculating the hitting time between vertices of finite
graphs, along with applications to specific classes of graphs, including grids,
trees, and the 'tadpole' graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2175</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2175</id><created>2012-08-10</created><authors><author><keyname>Bell</keyname><forenames>Michael J.</forenames></author><author><keyname>Gillespie</keyname><forenames>Colin S.</forenames></author><author><keyname>Swan</keyname><forenames>Daniel</forenames></author><author><keyname>Lord</keyname><forenames>Phillip</forenames></author></authors><title>An approach to describing and analysing bulk biological annotation
  quality: a case study using UniProtKB</title><categories>cs.CE cs.IR q-bio.GN</categories><comments>Paper accepted at The European Conference on Computational Biology
  2012 (ECCB'12). Subsequently will be published in a special issue of the
  journal Bioinformatics. Paper consists of 8 pages, made up of 5 figures</comments><doi>10.1093/bioinformatics/bts372</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Motivation: Annotations are a key feature of many biological databases, used
to convey our knowledge of a sequence to the reader. Ideally, annotations are
curated manually, however manual curation is costly, time consuming and
requires expert knowledge and training. Given these issues and the exponential
increase of data, many databases implement automated annotation pipelines in an
attempt to avoid un-annotated entries. Both manual and automated annotations
vary in quality between databases and annotators, making assessment of
annotation reliability problematic for users. The community lacks a generic
measure for determining annotation quality and correctness, which we look at
addressing within this article. Specifically we investigate word reuse within
bulk textual annotations and relate this to Zipf's Principle of Least Effort.
We use UniProt Knowledge Base (UniProtKB) as a case study to demonstrate this
approach since it allows us to compare annotation change, both over time and
between automated and manually curated annotations.
  Results: By applying power-law distributions to word reuse in annotation, we
show clear trends in UniProtKB over time, which are consistent with existing
studies of quality on free text English. Further, we show a clear distinction
between manual and automated analysis and investigate cohorts of protein
records as they mature. These results suggest that this approach holds distinct
promise as a mechanism for judging annotation quality.
  Availability: Source code is available at the authors website:
http://homepages.cs.ncl.ac.uk/m.j.bell1/annotation.
  Contact: phillip.lord@newcastle.ac.uk
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2199</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2199</id><created>2012-08-10</created><authors><author><keyname>Havaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Prasad</keyname><forenames>Nandivada Krishna</forenames></author><author><keyname>Sudheer</keyname><forenames>Velleshala</forenames></author></authors><title>Elimination of ISI Using Improved LMS Based Decision Feedback Equalizer</title><categories>cs.AI</categories><msc-class>93Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the implementation of Least Mean Square (LMS) algorithm
in Decision Feedback Equalizer (DFE) for removal of Inter Symbol Interference
(ISI) at the receiver. The channel disrupts the transmitted signal by spreading
it in time. Although, the LMS algorithm is robust and reliable, it is slow in
convergence. In order to increase the speed of convergence, modifications have
been made in the algorithm where the weights get updated depending on the
severity of disturbance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2205</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2205</id><created>2012-08-10</created><authors><author><keyname>Moshirian</keyname><forenames>Sanaz</forenames></author><author><keyname>Ghadami</keyname><forenames>Soheil</forenames></author><author><keyname>Havaei</keyname><forenames>Mohammad</forenames></author></authors><title>Blind Channel Equalization</title><categories>cs.IT math.IT</categories><msc-class>93Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future services demand high data rate and quality. Thus, it is necessary to
define new and robust algorithms to equalize channels and reduce noise in
communications. Nowadays, new equalization algorithms are being developed to
optimize the channel bandwidth and reduce noise, namely, Blind Channel
Equalization. Conventional equalizations minimizing mean-square error generally
require a training sequence accompanying the data sequence. In this study, the
result of Least Mean Square (LMS) algorithm applied on two given communication
channels is analyzed. Considering the fact that blind equalizers do not require
pilot signals to recover the transmitted data, implementation of four types of
Constant Modulus Algorithm (CMA) for blind equalization of the channels are
shown. Finally, a comparison of the simulation results of LMS and CMA for the
test channels is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2214</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2214</id><created>2012-08-10</created><authors><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Curved Space Optimization: A Random Search based on General Relativity
  Theory</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a fast and efficient optimization method with local optima
avoidance capability on a variety of optimization problems is still an open
problem for many researchers. In this work, the concept of a new global
optimization method with an open implementation area is introduced as a Curved
Space Optimization (CSO) method, which is a simple probabilistic optimization
method enhanced by concepts of general relativity theory. To address global
optimization challenges such as performance and convergence, this new method is
designed based on transformation of a random search space into a new search
space based on concepts of space-time curvature in general relativity theory.
In order to evaluate the performance of our proposed method, an implementation
of CSO is deployed and its results are compared on benchmark functions with
state-of-the art optimization methods. The results show that the performance of
CSO is promising on unimodal and multimodal benchmark functions with different
search space dimension sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2217</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2217</id><created>2012-08-10</created><authors><author><keyname>Taranovsky</keyname><forenames>Dmytro</forenames></author></authors><title>Space-Efficient Circuit Evaluation</title><categories>cs.CC</categories><comments>HTML, UTF-8 encoding, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that uniform circuits of size n can be evaluated in space O(n/log
n). Thus, Space(O(n)) is not in uniform Size(o(n*log n)). For uniformity, we
only require that the circuit is O(n/log n)-Space uniform. We also generalize
the construction to prove that a machine with O(n^delta) (delta&lt;1) internal
storage and O(2^n^delta) length single-bit-access read-write RAM that does only
O(n) RAM reads (1 bit per read) can be simulated in space O(n * log log n / log
n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2223</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2223</id><created>2012-08-10</created><updated>2013-05-17</updated><authors><author><keyname>Klein</keyname><forenames>Philip N.</forenames></author><author><keyname>Mozes</keyname><forenames>Shay</forenames></author><author><keyname>Sommer</keyname><forenames>Christian</forenames></author></authors><title>Structured Recursive Separator Decompositions for Planar Graphs in
  Linear Time</title><categories>cs.DM cs.DS math.CO</categories><comments>30 pages, 5 figures</comments><journal-ref>STOC 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a planar graph G on n vertices and an integer parameter r&lt;n, an
r-division of G with few holes is a decomposition of G into O(n/r) regions of
size at most r such that each region contains at most a constant number of
faces that are not faces of G (also called holes), and such that, for each
region, the total number of vertices on these faces is O(sqrt r).
  We provide a linear-time algorithm for computing r-divisions with few holes.
In fact, our algorithm computes a structure, called decomposition tree, which
represents a recursive decomposition of G that includes r-divisions for
essentially all values of r. In particular, given an exponentially increasing
sequence r = (r_1,r_2,...), our algorithm can produce a recursive r-division
with few holes in linear time.
  r-divisions with few holes have been used in efficient algorithms to compute
shortest paths, minimum cuts, and maximum flows. Our linear-time algorithm
improves upon the decomposition algorithm used in the state-of-the-art
algorithm for minimum st-cut (Italiano, Nussbaum, Sankowski, and Wulff-Nilsen,
STOC 2011), removing one of the bottlenecks in the overall running time of
their algorithm (analogously for minimum cut in planar and bounded-genus
graphs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2239</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2239</id><created>2012-08-10</created><authors><author><keyname>Ryu</keyname><forenames>Ernest</forenames></author><author><keyname>Choi</keyname><forenames>Sean</forenames></author></authors><title>Stochastic Kronecker Graph on Vertex-Centric BSP</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently Stochastic Kronecker Graph (SKG), a network generation model, and
vertex-centric BSP, a graph processing framework like Pregel, have attracted
much attention in the network analysis community. Unfortunately the two are not
very well-suited for each other and thus an implementation of SKG on
vertex-centric BSP must either be done serially or in an unnatural manner.
  In this paper, we present a new network generation model, which we call
Poisson Stochastic Kronecker Graph (PSKG), that generate edges according to the
Poisson distribution. The advantage of PSKG is that it is easily parallelizable
on vertex-centric BSP, requires no communication between computational nodes,
and yet retains all the desired properties of SKG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2245</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2245</id><created>2012-08-08</created><updated>2012-09-12</updated><authors><author><keyname>Zheng</keyname><forenames>Xizhong</forenames><affiliation>Jiangsu University and Arcadia University</affiliation></author><author><keyname>Rettinger</keyname><forenames>Robert</forenames><affiliation>Hagen University</affiliation></author></authors><title>Point-Separable Classes of Simple Computable Planar Curves</title><categories>cs.CG</categories><proxy>LMCS</proxy><acm-class>F.1.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  13, 2012) lmcs:941</journal-ref><doi>10.2168/LMCS-8(3:15)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mathematics curves are typically defined as the images of continuous real
functions (parametrizations) defined on a closed interval. They can also be
defined as connected one-dimensional compact subsets of points. For simple
curves of finite lengths, parametrizations can be further required to be
injective or even length-normalized. All of these four approaches to curves are
classically equivalent. In this paper we investigate four different versions of
computable curves based on these four approaches. It turns out that they are
all different, and hence, we get four different classes of computable curves.
More interestingly, these four classes are even point-separable in the sense
that the sets of points covered by computable curves of different versions are
also different. However, if we consider only computable curves of computable
lengths, then all four versions of computable curves become equivalent. This
shows that the definition of computable curves is robust, at least for those of
computable lengths. In addition, we show that the class of computable curves of
computable lengths is point-separable from the other four classes of computable
curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2261</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2261</id><created>2012-08-10</created><updated>2012-08-13</updated><authors><author><keyname>Nandy</keyname><forenames>Sudarshan</forenames></author><author><keyname>Sarkar</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author></authors><title>Analysis of Statistical Hypothesis based Learning Mechanism for Faster
  Crawling</title><categories>cs.IR cs.AI</categories><comments>14 Pages, 7 Figures This paper has been withdrawn by the author due
  to a crucial sign error in page no. 3,4,7 and 11. The error is also observed
  with equation no in page 10</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.4, July 2012, 117-130</journal-ref><doi>10.5121/ijaia.2012.3409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of world-wide-web (WWW) spreads its wings from an intangible
quantities of web-pages to a gigantic hub of web information which gradually
increases the complexity of crawling process in a search engine. A search
engine handles a lot of queries from various parts of this world, and the
answers of it solely depend on the knowledge that it gathers by means of
crawling. The information sharing becomes a most common habit of the society,
and it is done by means of publishing structured, semi-structured and
unstructured resources on the web. This social practice leads to an exponential
growth of web-resource, and hence it became essential to crawl for continuous
updating of web-knowledge and modification of several existing resources in any
situation. In this paper one statistical hypothesis based learning mechanism is
incorporated for learning the behavior of crawling speed in different
environment of network, and for intelligently control of the speed of crawler.
The scaling technique is used to compare the performance proposed method with
the standard crawler. The high speed performance is observed after scaling, and
the retrieval of relevant web-resource in such a high speed is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2265</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2265</id><created>2012-08-10</created><authors><author><keyname>Swain</keyname><forenames>Ranjita Kumari</forenames><affiliation>RIMS, Rourkela</affiliation></author><author><keyname>Behera</keyname><forenames>Prafulla Kumar</forenames><affiliation>Utkal University, Vani Vihar</affiliation></author><author><keyname>Mohapatra</keyname><forenames>Durga Prasad</forenames><affiliation>NIT, Rourkela</affiliation></author></authors><title>Minimal TestCase Generation for Object-Oriented Software with State
  Charts</title><categories>cs.SE</categories><comments>21 pages, 7 figures, 3-4 tables; International Journal of Software
  Engineering &amp; Applications (IJSEA), Vol.3, No.4, July 2012. arXiv admin note:
  substantial text overlap with arXiv:1206.0373</comments><doi>10.5121/ijsea.2012.3404</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Today statecharts are a de facto standard in industry for modeling system
behavior. Test data generation is one of the key issues in software testing.
This paper proposes an reduction approach to test data generation for the
state-based software testing. In this paper, first state transition graph is
derived from state chart diagram. Then, all the required information are
extracted from the state chart diagram. Then, test cases are generated. Lastly,
a set of test cases are minimized by calculating the node coverage for each
test case. It is also determined that which test cases are covered by other
test cases. The advantage of our test generation technique is that it optimizes
test coverage by minimizing time and cost. The present test data generation
scheme generates test cases which satisfy transition path coverage criteria,
path coverage criteria and action coverage criteria. A case study on Railway
Ticket Vending Machine (RTVM) has been presented to illustrate our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2278</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2278</id><created>2012-08-10</created><authors><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author><author><keyname>van de Ven</keyname><forenames>Peter M.</forenames></author></authors><title>Balancing Lifetime and Classification Accuracy of Wireless Sensor
  Networks</title><categories>cs.NI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are composed of distributed sensors that can be used
for signal detection or classification. The likelihood functions of the
hypotheses are often not known in advance, and decision rules have to be
learned via supervised learning. A specific such algorithm is Fisher
discriminant analysis (FDA), the classification accuracy of which has been
previously studied in the context of wireless sensor networks. Previous work,
however, does not take into account the communication protocol or battery
lifetime of the sensor networks; in this paper we extend the existing studies
by proposing a model that captures the relationship between battery lifetime
and classification accuracy. In order to do so we combine the FDA with a model
that captures the dynamics of the Carrier-Sense Multiple-Access (CSMA)
algorithm, the random-access algorithm used to regulate communications in
sensor networks. This allows us to study the interaction between the
classification accuracy, battery lifetime and effort put towards learning, as
well as the impact of the back-off rates of CSMA on the accuracy. We
characterize the tradeoff between the length of the training stage and
accuracy, and show that accuracy is non-monotone in the back-off rate due to
changes in the training sample size and overfitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2292</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2292</id><created>2012-08-10</created><updated>2012-08-14</updated><authors><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>L1 Splines for Robust, Simple, and Fast Smoothing of Grid Data</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Splines are a popular and attractive way of smoothing noisy data. Computing
splines involves minimizing a functional which is a linear combination of a
fitting term and a regularization term. The former is classically computed
using a (weighted) L2 norm while the latter ensures smoothness. Thus, when
dealing with grid data, the optimization can be solved very efficiently using
the DCT. In this work we propose to replace the L2 norm in the fitting term
with an L1 norm, leading to automatic robustness to outliers. To solve the
resulting minimization problem we propose an extremely simple and efficient
numerical scheme based on split-Bregman iteration combined with DCT.
Experimental validation shows the high-quality results obtained in short
processing times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2294</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2294</id><created>2012-08-10</created><authors><author><keyname>Raskhodnikova</keyname><forenames>Sofya</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>Learning pseudo-Boolean k-DNF and Submodular Functions</title><categories>cs.LG cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that any submodular function f: {0,1}^n -&gt; {0,1,...,k} can be
represented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are a
natural generalization of DNF representation for functions with integer range.
Each term in such a formula has an associated integral constant. We show that
an analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if all
constants associated with the terms of the formula are bounded.
  This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs to
pseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membership
queries under the uniform distribution for submodular functions of the form
f:{0,1}^n -&gt; {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k
\log k / \epsilon)}, 1/\epsilon and log(1/\delta) and works even in the
agnostic setting. The line of previous work on learning submodular functions
[Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi,
Klivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity for
learning submodular functions in this setting, for fixed epsilon and delta.
  Our learning algorithm implies a property tester for submodularity of
functions f:{0,1}^n -&gt; {0, ..., k} with query complexity polynomial in n for
k=O((\log n/ \loglog n)^{1/2}) and constant proximity parameter \epsilon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2305</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2305</id><created>2012-08-10</created><authors><author><keyname>Doomun</keyname><forenames>M. Razvi</forenames></author><author><keyname>Soyjaudah</keyname><forenames>K. M. Sunjiv</forenames></author></authors><title>LOTKIP: Low Overhead TKIP Optimization for Ad Hoc Wireless Network</title><categories>cs.CR</categories><comments>31 pages, journal paper IJNS</comments><journal-ref>International Journal of Network Security, Vol.10, No.3,
  PP.225-237, May 2010</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Temporal Key Integrity Protocol (TKIP) is a provisional solution for Wired
Equivalent Privacy (WEP) security loopholes present in already widely deployed
legacy 802.11 wireless devices. In this work, we model and analyse the
computational complexity of TKIP security mechanism and propose an optimised
implementation, called LOTKIP, to decrease processing overhead for better
energy efficient security performance. The LOTKIP improvements are based on
minimising key mixing redundancy and a novel frame encapsulation with low
overhead. We simulate and compare LOTKIP with baseline TKIP in terms of
complexity and energy consumption for ad hoc wireless network security. From
simulation results, we demonstrate that LOTKIP executes with lower
computational complexity, hence, with faster encryption time and more
energy-efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2311</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2311</id><created>2012-08-11</created><updated>2013-05-25</updated><authors><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>Compressed Hypothesis Testing: to Mix or Not to Mix?</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the hypothesis testing problem of, among $n$ random
variables, determining $k$ random variables which have different probability
distributions from the rest $(n-k)$ random variables. Instead of using separate
measurements of each individual random variable, we propose to use mixed
measurements which are functions of multiple random variables. It is
demonstrated that $O({\displaystyle \frac{k \log(n)}{\min_{P_i, P_j} C(P_i,
P_j)}})$ observations are sufficient for correctly identifying the $k$
anomalous random variables with high probability, where $C(P_i, P_j)$ is the
Chernoff information between two possible distributions $P_i$ and $P_j$ for the
proposed mixed observations. We characterized the Chernoff information
respectively under fixed time-invariant mixed observations, random time-varying
mixed observations, and deterministic time-varying mixed observations; in our
derivations, we introduced the \emph{inner and outer conditional Chernoff
information} for time-varying measurements. It is demonstrated that mixed
observations can strictly improve the error exponent of hypothesis testing,
over separate observations of individual random variables. We also
characterized the optimal mixed observations maximizing the error exponent, and
derived an explicit construction of the optimal mixed observations for the case
of Gaussian random variables. These results imply that mixed observations of
random variables can reduce the number of required samples in hypothesis
testing applications. Compared with compressed sensing problems, this paper
considers random variables which are allowed to dramatically change values in
different measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2314</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2314</id><created>2012-08-11</created><authors><author><keyname>Almasri</keyname><forenames>Marwah</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author></authors><title>Analytical Study of Pre-congestion notification (PCN) techniques</title><categories>cs.NI</categories><comments>16 Pages, 16 figures; International Journal of Computer Networks &amp;
  Communications (IJCNC) July 2012, Volume 4, Number 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maintaining the quality of service (QOS) and controlling the network
congestion are quite complicated tasks. They cause degrading the performance of
the network, and disturbing the continuous communication process. To overcome
these issues, one step towards this dilemma has been taken in form of
Pre-congestion notification (PCN) technique. PCN uses a packet marking
technique within a PCN domain over IP networks. It is notified by egress node
that works as guard at entry point of network. Egress node gives feedback to
communicating servers whether rate on the link is exceeded than configured
admissible threshold or within the limit. Based on this feedback, admission
decisions are taken to determine whether to allow/block new coming flows or
terminate already accepted. The actual question is about selection of right
algorithm for PCN domain. In this paper, we investigate the analytical behavior
of some known PCN algorithms. We make slide modifications in originality of PCN
algorithms without disquieting working process in order to employ those within
similar types of scenarios. Our goal is to simulate them either in highly
congested or less congested realistic scenarios. On the basis of simulation
done in ns2, we are able to recommend each PCN algorithm for specific
conditions. Finally, we develop a benchmark that helps researchers and
scientific communities to pick the right algorithm. Furthermore, the benchmark
is designed to achieve specific objectives according to the users' requirements
without congesting the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2318</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2318</id><created>2012-08-11</created><authors><author><keyname>Mersmann</keyname><forenames>Olaf</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Trautmann</keyname><forenames>Heike</forenames></author><author><keyname>Wagner</keyname><forenames>Markus</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>A Novel Feature-Based Approach to Characterize Algorithm Performance for
  the Traveling Salesman Problem</title><categories>cs.DS</categories><comments>33 pages, 17 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meta-heuristics are frequently used to tackle NP-hard combinatorial
optimization problems. With this paper we contribute to the understanding of
the success of 2-opt based local search algorithms for solving the traveling
salesman problem (TSP). Although 2-opt is widely used in practice, it is hard
to understand its success from a theoretical perspective. We take a statistical
approach and examine the features of TSP instances that make the problem either
hard or easy to solve. As a measure of problem difficulty for 2-opt we use the
approximation ratio that it achieves on a given instance. Our investigations
point out important features that make TSP instances hard or easy to be
approximated by 2-opt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2321</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2321</id><created>2012-08-11</created><authors><author><keyname>Anas</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Mahmood</keyname><forenames>A.</forenames></author><author><keyname>Raza</keyname><forenames>S. M.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Minimizing Electricity Theft using Smart Meters in AMI</title><categories>cs.NI</categories><comments>7th International Conference on P2P, Parallel, Grid, Cloud and
  Internet Computing (3PGCIC-2012), Victoria, Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global energy crises are increasing every moment. Every one has the attention
towards more and more energy production and also trying to save it. Electricity
can be produced through many ways which is then synchronized on a main grid for
usage. The main issue for which we have written this survey paper is losses in
electrical system. Weather these losses are technical or non-technical.
Technical losses can be calculated easily, as we discussed in section of
mathematical modeling that how to calculate technical losses. Where as
nontechnical losses can be evaluated if technical losses are known. Theft in
electricity produce non-technical losses. To reduce or control theft one can
save his economic resources. Smart meter can be the best option to minimize
electricity theft, because of its high security, best efficiency, and excellent
resistance towards many of theft ideas in electromechanical meters. So in this
paper we have mostly concentrated on theft issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2322</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2322</id><created>2012-08-11</created><updated>2014-07-22</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Adaptive Control Design under Structured Model Information Limitation: A
  Cost-Biased Maximum-Likelihood Approach</title><categories>math.OC cs.SY</categories><comments>Improved presentation, Fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked control strategies based on limited information about the plant
model usually results in worse closed-loop performance than optimal centralized
control with full plant model information. Recently, this fact has been
established by utilizing the concept of competitive ratio, which is defined as
the worst case ratio of the cost of a control design with limited model
information to the cost of the optimal control design with full model
information. We show that an adaptive controller, inspired by a controller
proposed by Campi and Kumar, with limited plant model information,
asymptotically achieves the closed-loop performance of the optimal centralized
controller with full model information for almost any plant. Therefore, there
exists, at least, one adaptive control design strategy with limited plant model
information that can achieve a competitive ratio equal to one. The plant model
considered in the paper belongs to a compact set of stochastic linear
time-invariant systems and the closed loop performance measure is the ergodic
mean of a quadratic function of the state and control input. We illustrate the
applicability of the results numerically on a vehicle platooning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2329</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2329</id><created>2012-08-11</created><authors><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>A New Direction for Counting Perfect Matchings</title><categories>cs.DS cs.DM</categories><comments>The 53rd Annual Symposium on Foundations of Computer Science (FOCS
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new exact algorithm for counting perfect
matchings, which relies on neither inclusion-exclusion principle nor
tree-decompositions. For any bipartite graph of $2n$ nodes and $\Delta n$ edges
such that $\Delta \geq 3$, our algorithm runs with $O^{\ast}(2^{(1 - 1/O(\Delta
\log \Delta))n})$ time and exponential space. Compared to the previous
algorithms, it achieves a better time bound in the sense that the performance
degradation to the increase of $\Delta$ is quite slower. The main idea of our
algorithm is a new reduction to the problem of computing the cut-weight
distribution of the input graph. The primary ingredient of this reduction is
MacWilliams Identity derived from elementary coding theory. The whole of our
algorithm is designed by combining that reduction with a non-trivial fast
algorithm computing the cut-weight distribution. To the best of our knowledge,
the approach posed in this paper is new and may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2330</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2330</id><created>2012-08-11</created><updated>2013-04-16</updated><authors><author><keyname>Carrillo</keyname><forenames>Rafael E.</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>Van De Ville</keyname><forenames>Dimitri</forenames></author><author><keyname>Thiran</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Sparsity Averaging for Compressive Imaging</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>4 pages, 3 figures, accepted in IEEE signal processing letters</comments><journal-ref>IEEE Signal Processing Letters. Vol. 20, No. 6, 2013, pp 591-594</journal-ref><doi>10.1109/LSP.2013.2259813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a novel sparsity prior for compressive imaging in the context of
the theory of compressed sensing with coherent redundant dictionaries, based on
the observation that natural images exhibit strong average sparsity over
multiple coherent frames. We test our prior and the associated algorithm, based
on an analysis reweighted $\ell_1$ formulation, through extensive numerical
simulations on natural images for spread spectrum and random Gaussian
acquisition schemes. Our results show that average sparsity outperforms
state-of-the-art priors that promote sparsity in a single orthonormal basis or
redundant frame, or that promote gradient sparsity. Code and test data are
available at https://github.com/basp-group/sopt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2331</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2331</id><created>2012-08-11</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Sharif</keyname><forenames>A.</forenames></author><author><keyname>Mahmood</keyname><forenames>A.</forenames></author><author><keyname>Ahmed</keyname><forenames>S.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Monitoring and Controlling Power using Zigbee Communications</title><categories>cs.NI</categories><comments>5th International Workshop on NGWMN with 7th IEEE International
  Conference on BWCCA 2012, Victoria, Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart grid is a modified form of electrical grid where generation,
transmission, distribution and customers are not only connected electrically
but also through strong communication network with each other as well as with
market, operation and service provider. For achieving good communication link
among them, it is very necessary to find suitable protocol. In this paper, we
discuss different hardware techniques for power monitoring, power management
and remote power controlling at home and transmission side and also discuss the
suitability of Zigbee for required communication link. Zigbee has major role in
monitoring and direct load controlling for efficient power utilization. It
covers enough area needed for communication and it works on low data rate of
20Kbps to 250Kbps with minimum power consumption. This paper describes the user
friendly control home appliances, power on/off through the internet, PDA using
Graphical User Interface (GUI) and through GSM cellular mobile phone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2332</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2332</id><created>2012-08-11</created><authors><author><keyname>Ain</keyname><forenames>Q.</forenames></author><author><keyname>Ikram</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Modeling Propagation Characteristics for Arm-Motion in Wireless Body
  Area Sensor Networks</title><categories>cs.SY cs.NI</categories><comments>7th International Conference on Broadband and Wireless Computing,
  Communication and Applications (BWCCA-2012), Victoria, Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To monitor health information using wireless sensors on body is a promising
new application. Human body acts as a transmission channel in wearable wireless
devices, so electromagnetic propagation modeling is well thought-out for
transmission channel in Wireless Body Area Sensor Network (WBASN). In this
paper we have presented the wave propagation in WBASN which is modeled as point
source (Antenna), close to the arm of the human body. Four possible cases are
presented, where transmitter and receiver are inside or outside of the body.
Dyadic Green's function is specifically used to propose a channel model for arm
motion of human body model. This function is expanded in terms of vector wave
function and scattering superposition principle. This paper describes the
analytical derivation of the spherical electric field distribution model and
the simulation of those derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2333</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2333</id><created>2012-08-11</created><authors><author><keyname>Sarkar</keyname><forenames>Arindam</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Energy Efficient Wireless Communication using Genetic Algorithm Guided
  Faster Light Weight Digital Signature Algorithm (GADSA)</title><categories>cs.CR cs.NE</categories><comments>17 pages</comments><journal-ref>International Journal Of Advanced Smart Sensor Network Systems (
  IJASSN ), Vol 2, No.3, July 2012, 9-25</journal-ref><doi>10.5121/ijassn.2012.2302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper GA based light weight faster version of Digital Signature
Algorithm (GADSA) in wireless communication has been proposed. Various genetic
operators like crossover and mutation are used to optimizing amount of modular
multiplication. Roulette Wheel selection mechanism helps to select best
chromosome which in turn helps in faster computation and minimizes the time
requirements for DSA. Minimization of number of modular multiplication itself a
NP-hard problem that means there is no polynomial time deterministic algorithm
for this purpose. This paper deals with this problem using GA based
optimization algorithm for minimization of the modular multiplication. Proposed
GADSA initiates with an initial population comprises of set of valid and
complete set of individuals. Some operators are used to generate feasible valid
offspring from the existing one. Among several exponents the best solution
reached by GADSA is compared with some of the existing techniques. Extensive
simulations shows competitive results for the proposed GADSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2334</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2334</id><created>2012-08-11</created><authors><author><keyname>Sarkar</keyname><forenames>Arindam</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Multilayer Perceptron Guided Key Generation Through Mutation with
  Recursive Replacement in Wireless Communication (MLPKG)</title><categories>cs.CR</categories><comments>18 pages</comments><journal-ref>International Journal on AdHoc Networking Systems (IJANS) Vol. 2,
  No. 3, July 2012, 11-28</journal-ref><doi>10.5121/ijans.2012.2302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multilayer perceptron guided key generation for
encryption/decryption (MLPKG) has been proposed through recursive replacement
using mutated character code generation for wireless communication of
data/information. Multilayer perceptron transmitting systems at both ends
accept an identical input vector, generate an output bit and the network are
trained based on the output bit which is used to form a protected variable
length secret-key. For each session, different hidden layer of multilayer
neural network is selected randomly and weights or hidden units of this
selected hidden layer help to form a secret session key. The plain text is
encrypted using mutated character code table. Intermediate cipher text is yet
again encrypted through recursive replacement technique to from next
intermediate encrypted text which is again encrypted to form the final cipher
text through chaining, cascaded xoring of multilayer perceptron generated
session key. If size of the final block of intermediate cipher text is less
than the size of the key then this block is kept unaltered. Receiver will use
identical multilayer perceptron generated session key for performing
deciphering process for getting the recursive replacement encrypted cipher text
and then mutated character code table is used for decoding. Parametric tests
have been done and results are compared in terms of Chi-Square test, response
time in transmission with some existing classical techniques, which shows
comparable results for the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2335</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2335</id><created>2012-08-11</created><authors><author><keyname>Khan</keyname><forenames>A. A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Lu</keyname><forenames>Z.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>HSEP: Heterogeneity-aware Hierarchical Stable Election Protocol for WSNs</title><categories>cs.NI</categories><comments>ASTSA with 7th IEEE International Conference on Broadband and ireless
  Computing, Communication and Applications (BWCCA 2012), Victoria, Canada,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are increasing to handle complex situations
and functions. In these networks some of the nodes become Cluster Heads (CHs)
which are responsible to aggregate data of from cluster members and transmit it
to Base Stations (BS). Those clustering techniques which are designed for
homogenous network are not enough efficient for consuming energy. Stable
Election Protocol (SEP) introduces heterogeneity in WSNs, consisting of two
type of nodes. SEP is based on weighted election probabilities of each node to
become CH according to remaining energy of nodes. We propose
Heterogeneity-aware Hierarchal Stable Election Protocol (HSEP) having two level
of energies. Simulation results show that HSEP prolongs stability period and
network lifetime, as compared to conventional routing protocols and having
higher average throughput than selected clustering protocols in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2345</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2345</id><created>2012-08-11</created><authors><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Tang</keyname><forenames>Ke</forenames></author><author><keyname>Chen</keyname><forenames>Guoliang</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>A Large Population Size Can Be Unhelpful in Evolutionary Algorithms</title><categories>cs.NE</categories><comments>25 pages, 1 figure</comments><journal-ref>Theoretical Computer Science, vol. 436, 2012, pp. 54-70</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The utilization of populations is one of the most important features of
evolutionary algorithms (EAs). There have been many studies analyzing the
impact of different population sizes on the performance of EAs. However, most
of such studies are based computational experiments, except for a few cases.
The common wisdom so far appears to be that a large population would increase
the population diversity and thus help an EA. Indeed, increasing the population
size has been a commonly used strategy in tuning an EA when it did not perform
as well as expected for a given problem. He and Yao (2002) showed theoretically
that for some problem instance classes, a population can help to reduce the
runtime of an EA from exponential to polynomial time. This paper analyzes the
role of population further in EAs and shows rigorously that large populations
may not always be useful. Conditions, under which large populations can be
harmful, are discussed in this paper. Although the theoretical analysis was
carried out on one multi-modal problem using a specific type of EAs, it has
much wider implications. The analysis has revealed certain problem
characteristics, which can be either the problem considered here or other
problems, that lead to the disadvantages of large population sizes. The
analytical approach developed in this paper can also be applied to analyzing
EAs on other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2346</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2346</id><created>2012-08-11</created><updated>2012-08-16</updated><authors><author><keyname>Bluher</keyname><forenames>Antonia W.</forenames></author></authors><title>On existence of Budaghyan-Carlet APN hexanomials</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>7 pages. Revised version on Aug 16, 2012 contains a stronger result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Budaghyan and Carlet constructed a family of almost perfect nonlinear (APN)
hexanomials over a field with r^2 elements, and with terms of degrees r+1, s+1,
rs+1, rs+r, rs+s, and r+s, where r = 2^m and s = 2^n with GCD(m,n)=1. The
construction requires a technical condition, which was verified empirically in
a finite number of examples. Bracken, Tan, and Tan (arXiv:1110.3177 [cs.it])
proved the condition holds when m = 2 or 4 (mod 6). In this article, we prove
that the construction of Budaghyan and Carlet produces APN polynomials for all
m and n.
  In the case where GCD(m,n) = k &gt;= 1, Budaghyan and Carlet showed that the
nonzero derivatives of the hexanomials are 2^k-to-one maps from F_{r^2} to
F_{r^2}, provided the same technical condition holds. We prove their
construction produces hexanomials with this differential property for all m and
n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2351</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2351</id><created>2012-08-11</created><authors><author><keyname>Rahim</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Aslam</keyname><forenames>M.</forenames></author><author><keyname>Rahman</keyname><forenames>Z.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>A Comprehensive Survey of MAC Protocols for Wireless Body Area Networks</title><categories>cs.NI</categories><comments>BioSPAN-2012 with 7th IEEE International Conference on Broadband and
  Wireless Computing, Communication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a comprehensive study of Medium Access Control
(MAC) protocols developed for Wireless Body Area Networks (WBANs). In WBANs,
small batteryoperated on-body or implanted biomedical sensor nodes are used to
monitor physiological signs such as temperature, blood pressure,
ElectroCardioGram (ECG), ElectroEncephaloGraphy (EEG) etc. We discuss design
requirements for WBANs with major sources of energy dissipation. Then, we
further investigate the existing designed protocols for WBANs with focus on
their strengths and weaknesses. Paper ends up with concluding remarks and open
research issues for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2355</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2355</id><created>2012-08-11</created><authors><author><keyname>Zhukovskiy</keyname><forenames>Maxim</forenames></author><author><keyname>Vinogradov</keyname><forenames>Dmitry</forenames></author><author><keyname>Pritykin</keyname><forenames>Yuri</forenames></author><author><keyname>Ostroumova</keyname><forenames>Liudmila</forenames></author><author><keyname>Grechnikov</keyname><forenames>Evgeniy</forenames></author><author><keyname>Gusev</keyname><forenames>Gleb</forenames></author><author><keyname>Serdyukov</keyname><forenames>Pavel</forenames></author><author><keyname>Raigorodskii</keyname><forenames>Andrei</forenames></author></authors><title>Empirical Validation of the Buckley--Osthus Model for the Web Host
  Graph: Degree and Edge Distributions</title><categories>cs.SI cs.DM cs.IR physics.soc-ph</categories><comments>21 pages, 8 figures; short version to appear in CIKM 2012 (see
  http://www.cikm2012.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a lot of research on random graph models for large real-world
networks such as those formed by hyperlinks between web pages in the world wide
web. Though largely successful qualitatively in capturing their key properties,
such models may lack important quantitative characteristics of Internet graphs.
While preferential attachment random graph models were shown to be capable of
reflecting the degree distribution of the webgraph, their ability to reflect
certain aspects of the edge distribution was not yet well studied.
  In this paper, we consider the Buckley--Osthus implementation of preferential
attachment and its ability to model the web host graph in two aspects. One is
the degree distribution that we observe to follow the power law, as often being
the case for real-world graphs. Another one is the two-dimensional edge
distribution, the number of edges between vertices of given degrees. We fit a
single &quot;initial attractiveness&quot; parameter $a$ of the model, first with respect
to the degree distribution of the web host graph, and then, absolutely
independently, with respect to the edge distribution. Surprisingly, the values
of $a$ we obtain turn out to be nearly the same. Therefore the same model with
the same value of the parameter $a$ fits very well the two independent and
basic aspects of the web host graph. In addition, we demonstrate that other
models completely lack the asymptotic behavior of the edge distribution of the
web host graph, even when accurately capturing the degree distribution.
  To the best of our knowledge, this is the first attempt for a real graph of
Internet to describe the distribution of edges between vertices with respect to
their degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2357</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2357</id><created>2012-08-11</created><authors><author><keyname>Gilad</keyname><forenames>Yossi</forenames></author><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author></authors><title>TCP Injections for Fun and Clogging</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new type of clogging DoS attacks, with the highest amplification
factors achieved by off-path attackers, using only puppets, i.e., sandboxed
malware on victim machines. Specifically, we present off-path variants of the
Opt-ack, Ack-storm and Coremelt DoS attacks, achieving results comparable to
these achieved previously achieved by eavesdropping/MitM attackers and
(unrestricted) malware. In contrast to previous off-path attacks, which
attacked the client (machine) running the malware, our attacks address a very
different goal: large-scale clogging DoS of a third party, or even of backbone
connections.
  Our clogging attacks are based on off-path TCP injections. Indeed, as an
additional contribution, we present improved off-path TCP injection attacks.
Our new attacks significantly relax the requirements cf. to the known attacks;
specifically, our injection attack requires only a Java script in browser
sandbox (not 'restricted malware'), does not depend on specific operating
system properties, and is efficient even when client's port is determined using
recommended algorithm. Our attacks are constructed modularly, allowing reuse of
modules for other scenarios and replacing modules as necessary. We present
specific defenses, however, this work is further proof to the need to base
security on sound foundations, using cryptography to provide security even
against MitM attackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2361</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2361</id><created>2012-08-11</created><updated>2012-12-17</updated><authors><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author><author><keyname>Sheikholeslam</keyname><forenames>S. Arash</forenames></author></authors><title>Lexicodes over Rings</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we consider the construction of linear lexicodes over finite
chain rings by using a $B$-ordering over these rings and a selection criterion.
% and a greedy Algorithm. As examples we give lexicodes over $\mathbb{Z}_4$ and
$\mathbb{F}_2+u\mathbb{F}_2$. %First, greedy algorithms are presented to
construct %lexicodes using a multiplicative property. Then, greedy algorithms
%are given for the case when the selection criteria is not %multiplicative such
as the minimum distance constraint. It is shown that this construction produces
many optimal codes over rings and also good binary codes. Some of these codes
meet the Gilbert bound. We also obtain optimal self-dual codes, in particular
the octacode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2362</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2362</id><created>2012-08-11</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Broekaert</keyname><forenames>Jan</forenames></author><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author></authors><title>The Guppy Effect as Interference</title><categories>cs.AI quant-ph</categories><comments>10 pages</comments><journal-ref>Quantum Interaction. Lecture Notes in Computer Science, 7620, pp
  36-47, 2012</journal-ref><doi>10.1007/978-3-642-35659-9_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People use conjunctions and disjunctions of concepts in ways that violate the
rules of classical logic, such as the law of compositionality. Specifically,
they overextend conjunctions of concepts, a phenomenon referred to as the Guppy
Effect. We build on previous efforts to develop a quantum model that explains
the Guppy Effect in terms of interference. Using a well-studied data set with
16 exemplars that exhibit the Guppy Effect, we developed a 17-dimensional
complex Hilbert space H that models the data and demonstrates the relationship
between overextension and interference. We view the interference effect as, not
a logical fallacy on the conjunction, but a signal that out of the two
constituent concepts, a new concept has emerged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2374</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2374</id><created>2012-08-11</created><updated>2012-11-03</updated><authors><author><keyname>Lashgar</keyname><forenames>Ahmad</forenames></author><author><keyname>Baniasadi</keyname><forenames>Amirali</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author></authors><title>Dynamic Warp Resizing in High-Performance SIMT</title><categories>cs.AR</categories><comments>9 pages, 5 Figures, 3 Lists, 1 Table, The extended version of ICCD
  2012 poster paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern GPUs synchronize threads grouped in a warp at every instruction. These
results in improving SIMD efficiency and makes sharing fetch and decode
resources possible. The number of threads included in each warp (or warp size)
affects divergence, synchronization overhead and the efficiency of memory
access coalescing. Small warps reduce the performance penalty associated with
branch and memory divergence at the expense of a reduction in memory
coalescing. Large warps enhance memory coalescing significantly but also
increase branch and memory divergence. Dynamic workload behavior, including
branch/memory divergence and coalescing, is an important factor in determining
the warp size returning best performance. Optimal warp size can vary from one
workload to another or from one program phase to the next. Based on this
observation, we propose Dynamic Warp Resizing (DWR). DWR takes innovative
microarchitectural steps to adjust warp size during runtime and according to
program characteristics. DWR outperforms static warp size decisions, up to 1.7X
to 2.28X, while imposing less than 1% area overhead. We investigate various
alternative configurations and show that DWR performs better for narrower SIMD
and larger caches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2376</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2376</id><created>2012-08-11</created><authors><author><keyname>Rehman</keyname><forenames>A.</forenames></author><author><keyname>Mustafa</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Analytical Survey of Wearable Sensors</title><categories>cs.SY cs.NI</categories><comments>BioSPAN with 7th IEEE International Conference on Broadband and
  Wireless Computing, Communication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wearable sensors inWireless Body Area Networks (WBANs) provide health and
physical activity monitoring. Modern communication systems have extended this
monitoring remotely. In this survey, various types of wearable sensors
discussed, their medical applications like ECG, EEG, blood pressure, detection
of blood glucose level, pulse rate, respiration rate and non medical
applications like daily exercise monitoring and motion detection of different
body parts. Different types of noise removing filters also discussed at the end
that are helpful in to remove noise from ECG signals. Main purpose of this
survey is to provide a platform for researchers in wearable sensors for WBANs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2378</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2378</id><created>2012-08-11</created><authors><author><keyname>Mahmood</keyname><forenames>D.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Routing Load of Route Calculation and Route Maintenance in Wireless
  Proactive Routing Protocols</title><categories>cs.NI</categories><comments>7th IEEE International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA 2012), Victoria, Canada,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents mathematical framework and study of proactive routing
Protocols. The performance analysis of three major proactive routing protocols:
Destination-Sequenced Distance Vector (DSDV), Fish-eye State Routing (FSR) and
Optimized Link State Routing (OLSR) are under consideration in this work.
Taking these routing protocols into account, we enhance existing framework. In
the next step we further discuss and produce analytical framework by
considering variations in different network and protocol parameters. Finally,
experiments are performed regarding above mentioned routing protocols followed
with detailed comparison and analysis of different environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2383</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2383</id><created>2012-08-11</created><updated>2013-05-02</updated><authors><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames></author><author><keyname>Rochel</keyname><forenames>Jan</forenames></author></authors><title>Expressibility in the Lambda Calculus with Letrec</title><categories>cs.PL cs.LO</categories><comments>79 pages, 25 figures</comments><report-no>1304.6284</report-no><msc-class>68N18</msc-class><acm-class>F.4.1; F.3.3</acm-class><doi>10.4230/LIPIcs.RTA.2013.206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the relationship between finite terms in lambda-letrec, the
lambda calculus with letrec, and the infinite lambda terms they express. As
there are easy examples of lambda-terms that, intuitively, are not unfoldings
of terms in lambda-letrec, we consider the question: How can those infinite
lambda terms be characterised that are lamda-letrec-expressible in the sense
that they can be obtained as infinite unfoldings of terms in lambda-letrec?
  For 'observing' infinite lambda-terms through repeated 'experiments' carried
out at the head of the term we introduce two rewrite systems (with rewrite
relations) -reg-&gt; and -reg+-&gt; that decompose the term, and produce 'generated
subterms' in two notions. Thereby the sort of the step can be observed as well
as its target, a generated subterm. In both systems there are four sorts of
decomposition steps: -lambda-&gt; steps (decomposing a lambda-abstraction), -@0&gt;
and -@1&gt; steps (decomposing an application into its function and argument), and
respectively, -del-&gt; steps (delimiting the scope of an abstraction, for
-reg-&gt;), and -S-&gt; (delimiting of scopes, for -reg+-&gt;). These steps take place
on infinite lambda-terms furnished with a leading prefix of abstractions for
gathering previously encountered lambda-abstractions and keeping the generated
subterms closed. We call an infinite lambda-term 'regular'/'strongly regular'
if its set of -reg-&gt; -reachable / -reg-&gt; -reachable generated subterms is
finite. Furthermore, we analyse the binding structure of lambda-terms with the
concept of 'binding-capturing chain'.
  Using these concepts, we answer the question above by providing two
characterisations of lambda-letrec-expressibility. For all infinite
lambda-terms M, the following statements are equivalent: (i) M is
lambda-letrec-expressible; (ii) M is strongly regular; (iii) M is regular, and
it only has finite binding-capturing chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2387</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2387</id><created>2012-08-11</created><authors><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Yu</keyname><forenames>Mingchao</forenames></author></authors><title>Instantly Decodable versus Random Linear Network Coding: A Comparative
  Framework for Throughput and Decoding Delay Performance</title><categories>cs.IT math.IT</categories><comments>39 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the tension between throughput and decoding delay
performance of two widely-used network coding schemes: random linear network
coding (RLNC) and instantly decodable network coding (IDNC). A single-hop
broadcasting system model is considered that aims to deliver a block of packets
to all receivers in the presence of packet erasures. For a fair and
analytically tractable comparison between the two coding schemes, the
transmission comprises two phases: a systematic transmission phase and a
network coded transmission phase which is further divided into rounds. After
the systematic transmission phase and given the same packet reception state,
three quantitative metrics are proposed and derived in each scheme: 1) the
absolute minimum number of transmissions in the first coded transmission round
(assuming no erasures), 2) probability distribution of extra coded
transmissions in a subsequent round (due to erasures), and 3) average packet
decoding delay. This comparative study enables application-aware adaptive
selection between IDNC and RLNC after systematic transmission phase.
  One contribution of this paper is to provide a deep and systematic
understanding of the IDNC scheme, to propose the notion of packet diversity and
an optimal IDNC encoding scheme for minimizing metric 1. This is generally
NP-hard, but nevertheless required for characterizing and deriving all the
three metrics. Analytical and numerical results show that there is no clear
winner between RLNC and IDNC if one is concerned with both throughput and
decoding delay performance. IDNC is more preferable than RLNC when the number
of receivers is smaller than packet block size, and the case reverses when the
number of receivers is much greater than the packet block size. In the middle
regime, the choice can depend on the application and a specific instance of the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2394</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2394</id><created>2012-08-12</created><authors><author><keyname>Fang</keyname><forenames>Yi</forenames></author><author><keyname>Chen</keyname><forenames>Pingping</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author></authors><title>Performance Analysis of Protograph-based LDPC Codes with Spatial
  Diversity</title><categories>cs.IT math.IT</categories><comments>17 pages, 5 figures, IET Communications, under second review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless communications, spatial diversity techniques, such as space-time
block code (STBC) and single-input multiple-output (SIMO), are employed to
strengthen the robustness of the transmitted signal against channel fading.
This paper studies the performance of protograph-based low-density parity-check
(LDPC) codes with receive antenna diversity. We first propose a modified
version of the protograph extrinsic information transfer (PEXIT) algorithm and
use it for deriving the threshold of the protograph codes in a single-input
multiple-output (SIMO) system. We then calculate the decoding threshold and
simulate the bit error rate (BER) of two protograph codes
(accumulate-repeat-by-3-accumulate (AR3A) code and
accumulate-repeat-by-4-jagged-accumulate (AR4JA) code), a regular (3, 6) LDPC
code and two optimized irregular LDPC codes. The results reveal that the
irregular codes achieve the best error performance in the low
signal-to-noise-ratio (SNR) region and the AR3A code outperforms all other
codes in the high-SNR region. Utilizing the theoretical analyses and the
simulated results, we further discuss the effect of the diversity order on the
performance of the protograph codes. Accordingly, the AR3A code stands out as a
good candidate for wireless communication systems with multiple receive
antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2397</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2397</id><created>2012-08-12</created><authors><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Jaffar</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Saqib</keyname><forenames>M. N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Performance Analysis of Hierarchical Routing Protocols in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>NGWMN with 7th IEEE International Conference on Broadband and
  Wireless Computing, Communication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focusses on analyzing the optimization strategies of routing
protocols with respect to energy utilization of sensor nodes in Wireless Sensor
Network (WSNs). Different routing mechanisms have been proposed to address
energy optimization problem in sensor nodes. Clustering mechanism is one of the
popular WSNs routing mechanisms. In this paper, we first address energy
limitation constraints with respect to maximizing network life time using
linear programming formulation technique. To check the efficiency of different
clustering scheme against modeled constraints, we select four cluster based
routing protocols; Low Energy Adaptive Clustering Hierarchy (LEACH), Threshold
Sensitive Energy Efficient sensor Network (TEEN), Stable Election Protocol
(SEP), and Distributed Energy Efficient Clustering (DEEC). To validate our
mathematical framework, we perform analytical simulations in MATLAB by choosing
number of alive nodes, number of dead nodes, number of packets and number of
CHs, as performance metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2399</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2399</id><created>2012-08-12</created><authors><author><keyname>Fareed</keyname><forenames>M. S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Akbar</keyname><forenames>M.</forenames></author><author><keyname>Rehman</keyname><forenames>S.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Optimal Number of Cluster Head Selection for Efficient Distribution of
  Sources in WSNs</title><categories>cs.NI</categories><comments>NGWMN with 7th IEEE International Conference on Broadband and
  Wireless Computing, Communication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compare problems of cluster formation and cluster-head
selection between different protocols for data aggregation and transmission. We
focus on two aspects of the problem: (i) how to guess number of clusters
required to proficiently consume available sources for a sensor network, and
(ii) how to select number of cluster-heads to cover up sensor networks more
proficiently. A sensor in Wireless Sensor Networks (WSNs) can communicate
directly only with other sensors that are within a radio range in a cluster.
However, in order to enable communication between sensors not within
communication range, they must form new clusters in distributed sensors.
Several clustering algorithms such as LEACH, DEEC, and SEP have been proposed
with the objectives of energy minimization, route-path selection, increased
connectivity and network longevity. LEACH protocol and the similar ones assume
an energy homogeneous system where a node is not likely to fail due to failure
in connectivity and packet dropping. More recent protocols like SEP and TEEN
considered the reverse that is energy heterogeneity which is more applicable to
case of WSNs. We developed a bi-dimensional chain model to select average
number of for DEEC. Simulation results are used to compare performance of
different protocols to found optimal solutions of above mentioned problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2400</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2400</id><created>2012-08-12</created><authors><author><keyname>Fareed</keyname><forenames>M. S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Ahmed</keyname><forenames>S.</forenames></author><author><keyname>Rehman</keyname><forenames>S.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Analyzing Energy-efficiency and Route-selection of Multi-level
  Hierarchal Routing Protocols in WSNs</title><categories>cs.NI</categories><comments>NGWMN with 7th IEEE Inter- national Conference on Broadband and
  Wireless Computing, Communication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent and development in the field of Wireless Sensor Networks (WSNs) in
recent years has seen the growth of extremely small and low-cost sensors that
possess sensing, signal processing and wireless communication capabilities.
These sensors can be expended at a much lower cost and are capable of detecting
conditions such as temperature, sound, security or any other system. A good
protocol design should be able to scale well both in energy heterogeneous and
homogeneous environment, meet the demands of different application scenarios
and guarantee reliability. On this basis, we have compared six different
protocols of different scenarios which are presenting their own schemes of
energy minimizing, clustering and route selection in order to have more
effective communication. This research is motivated to have an insight that
which of the under consideration protocols suit well in which application and
can be a guide-line for the design of a more robust and efficient protocol.
MATLAB simulations are performed to analyze and compare the performance of
LEACH, multi-level hierarchal LEACH and multihop LEACH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2401</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2401</id><created>2012-08-12</created><authors><author><keyname>Qureshi</keyname><forenames>T. N.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Malik</keyname><forenames>M.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>On Performance Evaluation of Variants of DEEC in WSNs</title><categories>cs.NI</categories><comments>7th International Conference on Broadband and Wireless Computing,
  Communication and Applications (BWCCA-2012), Victoria, Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) contain numerous sensor nodes having limited
power resource, which report sensed data to the Base Station (BS) that requires
high energy usage. Many routing protocols have been proposed in this regard
achieving energy efficiency in heterogeneous scenarios. However, every protocol
is not suitable for heterogeneous WSNs. Efficiency of protocol degrades while
changing the heterogeneity parameters. In this paper, we first test Distributed
Energy- Efficient Clustering (DEEC), Developed DEEC (DDEEC), Enhanced DEEC
(EDEEC) and Threshold DEEC (TDEEC) under several different scenarios containing
high level heterogeneity to low level heterogeneity. We observe thoroughly
regarding the performance based on stability period, network life time and
throughput. EDEEC and TDEEC perform better in all heterogeneous scenarios
containing variable heterogeneity in terms of life time, however TDEEC is best
of all for the stability period of the network. However, the performance of
DEEC and DDEEC is highly effected by changing the heterogeneity parameters of
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2403</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2403</id><created>2012-08-12</created><authors><author><keyname>Abbas</keyname><forenames>Z.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Ahmed</keyname><forenames>S.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Simulation Analysis of IEEE 802.15.4 Non-beacon Mode at Varying Data
  Rates</title><categories>cs.NI</categories><comments>7th International Conference on Broadband and Wireless Computing,
  Communication and Applications (BWCCA-2012), Victoria, Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEEE 802.15.4 standard is designed for low power and low data rate
applications with high reliability. It operates in beacon enable and non-beacon
enable modes. In this work, we analyze delay, throughput, load, and end-to-end
delay of nonbeacon enable mode. Analysis of these parameters are performed at
varying data rates. Evaluation of non beacon enabled mode is done in a 10 node
network. We limit our analysis to non beacon or unslotted version because, it
performs better than other. Protocol performance is examined by changing
different Medium Access Control (MAC) parameters. We consider a full size MAC
packet with payload size of 114 bytes. In this paper we show that maximum
throughput and lowest delay is achieved at highest data rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2405</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2405</id><created>2012-08-12</created><authors><author><keyname>Mahmood</keyname><forenames>D.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Routing Load of Route Discovery and Route Maintenance in Wireless
  Reactive Routing Protocols</title><categories>cs.NI</categories><comments>ASTSA with 7th IEEE International Conference on Broadband and ireless
  Computing, Communication and Applications (BWCCA 2012), Victoria, Canada,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present analytical study of routing overhead of reactive
routing protocols for Wireless Multihop Networks (WMhNs). To accomplish the
framework of generalized routing overhead, we choose Ad-Hoc on Demand Distance
Vector (AODV), Dynamic Source Routing (DSR) and Dynamic MANET on Demand (DYMO).
Considering basic themes of these protocols, we enhance the generalized network
models by adding route monitoring overhead. Later, we take different network
parameters and produce framework discussing the impact of variations of these
parameters in network and routing performance. In the second part of our work,
we simulate above mentioned routing protocols and give a brief discussion and
comparison about the environments where these routing protocols perform better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2406</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2406</id><created>2012-08-12</created><authors><author><keyname>Israr</keyname><forenames>I.</forenames></author><author><keyname>Yaqoob</keyname><forenames>M. M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Simulation Analysis of Medium Access Techniques</title><categories>cs.NI</categories><comments>NGWMN with 7th IEEE International Conference on Broadband and
  Wireless Computing, Com- munication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents comparison of Access Techniques used in Medium Access
Control (MAC) protocol for Wireless Body Area Networks (WBANs). Comparison is
performed between Time Division Multiple Access (TDMA), Frequency Division
Multiple Access (FDMA), Carrier Sense Multiple Access with Collision Avoidance
(CSMA/CA), Pure ALOHA and Slotted ALOHA (S-ALOHA). Performance metrics used for
comparison are throughput (T), delay (D) and offered load (G). The main goal
for comparison is to show which technique gives highest Throughput and lowest
Delay with increase in Load. Energy efficiency is major issue in WBAN that is
why there is need to know which technique performs best for energy conservation
and also gives minimum delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2407</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2407</id><created>2012-08-12</created><authors><author><keyname>Wittek</keyname><forenames>Peter</forenames></author><author><keyname>Cucchietti</keyname><forenames>Fernando M.</forenames></author></authors><title>A Second-Order Distributed Trotter-Suzuki Solver with a Hybrid Kernel</title><categories>physics.comp-ph cs.DC quant-ph</categories><comments>11 pages, 10 figures</comments><acm-class>J.2</acm-class><journal-ref>Computer Physics Communications 184, 1165-1171 (2013)</journal-ref><doi>10.1016/j.cpc.2012.12.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Trotter-Suzuki approximation leads to an efficient algorithm for solving
the time-dependent Schr\&quot;odinger equation. Using existing highly optimized CPU
and GPU kernels, we developed a distributed version of the algorithm that runs
efficiently on a cluster. Our implementation also improves single node
performance, and is able to use multiple GPUs within a node. The scaling is
close to linear using the CPU kernels, whereas the efficiency of GPU kernels
improve with larger matrices. We also introduce a hybrid kernel that
simultaneously uses multicore CPUs and GPUs in a distributed system. This
kernel is shown to be efficient when the matrix size would not fit in the GPU
memory. Larger quantum systems scale especially well with a high number nodes.
The code is available under an open source license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2409</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2409</id><created>2012-08-12</created><authors><author><keyname>Yaqoob</keyname><forenames>M. M.</forenames></author><author><keyname>Israr</keyname><forenames>I.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Transmission Delay of Multi-hop Heterogeneous Networks for Medical
  Applications</title><categories>cs.NI</categories><comments>BioSPAN with 7th IEEE International Conference on Broadband and
  Wireless Computing, Communication and Applications (BWCCA 2012), Victoria,
  Canada, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, with increase in ageing population, Health care market keeps
growing. There is a need for monitoring of Health issues. Body Area Network
consists of wireless sensors attached on or inside human body for monitoring
vital Health related problems e.g, Electro Cardiogram (ECG),
ElectroEncephalogram (EEG), ElectronyStagmography(ENG) etc. Data is recorded by
sensors and is sent towards Health care center. Due to life threatening
situations, timely sending of data is essential. For data to reach Health care
center, there must be a proper way of sending data through reliable connection
and with minimum delay. In this paper transmission delay of different paths,
through which data is sent from sensor to Health care center over heterogeneous
multi-hop wireless channel is analyzed. Data of medical related diseases is
sent through three different paths. In all three paths, data from sensors first
reaches ZigBee, which is the common link in all three paths. After ZigBee there
are three available networks, through which data is sent. Wireless Local Area
Network (WLAN), Worldwide Interoperability for Microwave Access (WiMAX),
Universal Mobile Telecommunication System (UMTS) are connected with ZigBee.
Each network (WLAN, WiMAX, UMTS) is setup according to environmental
conditions, suitability of device and availability of structure for that
device. Data from these networks is sent to IP-Cloud, which is further
connected to Health care center. Main aim of this paper is to calculate delay
of each link in each path over multihop wireless channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2417</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2417</id><created>2012-08-12</created><authors><author><keyname>Hallak</keyname><forenames>Assaf</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>How to sample if you must: on optimal functional sampling</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a fundamental problem that models various active sampling setups,
such as network tomography. We analyze sampling of a multivariate normal
distribution with an unknown expectation that needs to be estimated: in our
setup it is possible to sample the distribution from a given set of linear
functionals, and the difficulty addressed is how to optimally select the
combinations to achieve low estimation error. Although this problem is in the
heart of the field of optimal design, no efficient solutions for the case with
many functionals exist. We present some bounds and an efficient sub-optimal
solution for this problem for more structured sets such as binary functionals
that are induced by graph walks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2428</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2428</id><created>2012-08-12</created><authors><author><keyname>Szkoda</keyname><forenames>Sebastian</forenames></author><author><keyname>Koza</keyname><forenames>Zbigniew</forenames></author><author><keyname>Tykierko</keyname><forenames>Mateusz</forenames></author></authors><title>Accelerating cellular automata simulations using AVX and CUDA</title><categories>cs.DC physics.comp-ph physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigated various methods of parallelization of the
Frish-Hasslacher-Pomeau (FHP) cellular automata algorithm for modeling fluid
flow. These methods include SSE, AVX, and POSIX Threads for central processing
units (CPUs) and CUDA for graphics processing units (GPUs). We present
implementation details of the FHP algorithm based on AVX/SSE and CUDA
technologies. We found that (a) using AVX or SSE is necessary to fully utilize
the potential of modern CPUs; (b) CPUs and GPUs are comparable in terms of
computational and economic efficiency only if the CPU code uses AVX or SSE
instructions; (c) AVX does not offer any substantial improvement relative to
SSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2429</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2429</id><created>2012-08-12</created><authors><author><keyname>Grammatico</keyname><forenames>Sergio</forenames></author><author><keyname>Pannocchia</keyname><forenames>Gabriele</forenames></author></authors><title>Linear model predictive control based on polyhedral control Lyapunov
  functions: theory and applications</title><categories>cs.SY math.OC</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polyhedral control Lyapunov functions (PCLFs) are exploited in finite-horizon
linear model predictive control formulations in order to guarantee the maximal
domain of attraction (DoA), in contrast to traditional formulations based on
quadratic control Lyapunov functions. In particular, the terminal region is
chosen as the largest DoA, namely the entire controllable set, which is
parametrized by a level set of a suitable PCLF. Closed-loop stability of the
origin is guaranteed either by using an &quot;inflated&quot; PCLF as terminal cost or by
adding a contraction constraint for the PCLF evaluated at the current state.
Two variants of the formulation based on the inflated PCLF terminal cost are
also presented. In all proposed formulations, the guaranteed DoA is always the
entire controllable set, independently of the chosen finite horizon.
Closed-loop inherent robustness with respect to arbitrary, sufficiently small
perturbations is also established. Moreover, all proposed schemes can be
formulated as Quadratic Programming problems. Numerical examples show the main
benefits and achievements of the proposed formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2432</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2432</id><created>2012-08-12</created><updated>2012-09-23</updated><authors><author><keyname>Stewart</keyname><forenames>Fraser</forenames></author></authors><title>Pirates and Treasure</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new game; in this game there are two players who
play as rival pirate gangs. The goal is to gather more treasure than your
rival. The game is played on a graph and a player gathers treasure by moving to
an unvisited vertex. At the end of the game, the player with the most treasure
wins. We will show that this game is NP-Hard, and we will also look at the
structure of this game under the disjunctive sum. We will show that there are
cases where this game behaves like a normal play game, and cases where it
behaves like a mis`ere play game. We then leave an open problem about scoring
play games in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2434</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2434</id><created>2012-08-12</created><updated>2012-09-16</updated><authors><author><keyname>Noori</keyname><forenames>Amir</forenames></author></authors><title>Distributed Multi-objective Multidisciplinary Design Optimization
  Algorithms</title><categories>math.OC cs.SY</categories><comments>Some of lemmas and assumptions, originate from arXiv:0802.3922,
  demonstrating the existing strong theoretical foundation and ensuring a
  complete statement of the problem at hand</comments><report-no>KIAU-90-12-3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes multi-agent systems setting for concurrent engineering
system design optimization and gradually paves the way towards examining graph
theoretic constructs in the context of multidisciplinary design optimization
problem. The flow of the algorithm can be described as follow; generated
estimates of the optimal (shared design) variables are exchanged locally with
neighbor subspaces and then updated by computing a weighted sum of the local
and received estimates. To comply with the consistency requirement, the
resultant values are projected to local constraint sets. By employing the
existing rules and results of the field, it has shown that the dual task of
reaching consensus and asymptotic convergence of the algorithms to locally and
globally optimal and consistent designs can be achieved. Finally, simulations
are provided to illustrate the effectiveness and capability of the presented
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2437</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2437</id><created>2012-08-12</created><authors><author><keyname>Castelli</keyname><forenames>Mauro</forenames></author><author><keyname>Manzoni</keyname><forenames>Luca</forenames></author><author><keyname>Vanneschi</keyname><forenames>Leonardo</forenames></author></authors><title>An Efficient Genetic Programming System with Geometric Semantic
  Operators and its Application to Human Oral Bioavailability Prediction</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very recently new genetic operators, called geometric semantic operators,
have been defined for genetic programming. Contrarily to standard genetic
operators, which are uniquely based on the syntax of the individuals, these new
operators are based on their semantics, meaning with it the set of input-output
pairs on training data. Furthermore, these operators present the interesting
property of inducing a unimodal fitness landscape for every problem that
consists in finding a match between given input and output data (for instance
regression and classification). Nevertheless, the current definition of these
operators has a serious limitation: they impose an exponential growth in the
size of the individuals in the population, so their use is impossible in
practice. This paper is intended to overcome this limitation, presenting a new
genetic programming system that implements geometric semantic operators in an
extremely efficient way. To demonstrate the power of the proposed system, we
use it to solve a complex real-life application in the field of
pharmacokinetic: the prediction of the human oral bioavailability of potential
new drugs. Besides the excellent performances on training data, which were
expected because the fitness landscape is unimodal, we also report an excellent
generalization ability of the proposed system, at least for the studied
application. In fact, it outperforms standard genetic programming and a wide
set of other well-known machine learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2440</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2440</id><created>2012-08-12</created><authors><author><keyname>Luttik</keyname><forenames>Bas</forenames></author><author><keyname>Reniers</keyname><forenames>Michel A.</forenames></author></authors><title>Proceedings Combined 19th International Workshop on Expressiveness in
  Concurrency and 9th Workshop on Structured Operational Semantics</title><categories>cs.LO cs.FL cs.PL</categories><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.3.0; F.4.0</acm-class><journal-ref>EPTCS 89, 2012</journal-ref><doi>10.4204/EPTCS.89</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Combined 19th International
Workshop on Expressiveness in Concurrency and the 9th Workshop on Structural
Operational Semantics (EXPRESS/SOS 2012), which took place on 3rd September
2012 in Newcastle upon Tyne, as a satellite workshop of CONCUR 2012. The
EXPRESS workshop series aims at bringing together researchers interested in the
expressiveness of various formal systems and semantic notions, particularly in
the field of concurrency. The SOS workshop series aims at being a forum for
researchers, students and practitioners interested in new developments, and
directions for future investigation, in the field of structural operational
semantics. In 2012, the EXPRESS and SOS communities organized a joint
EXPRESS/SOS 2012 workshop on the formal semantics of systems and programming
concepts, and on the expressiveness of mathematical models of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2447</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2447</id><created>2012-08-12</created><authors><author><keyname>Gupta</keyname><forenames>Rishi</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Rachlin</keyname><forenames>Yaron</forenames></author></authors><title>Compressive Sensing with Local Geometric Features</title><categories>cs.CG cs.DS</categories><acm-class>F.2.0</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a framework for compressive sensing of images with local
distinguishable objects, such as stars, and apply it to solve a problem in
celestial navigation. Specifically, let x be an N-pixel real-valued image,
consisting of a small number of local distinguishable objects plus noise. Our
goal is to design an m-by-N measurement matrix A with m &lt;&lt; N, such that we can
recover an approximation to x from the measurements Ax.
  We construct a matrix A and recovery algorithm with the following properties:
(i) if there are k objects, the number of measurements m is O((k log N)/(log
k)), undercutting the best known bound of O(k log(N/k)) (ii) the matrix A is
very sparse, which is important for hardware implementations of compressive
sensing algorithms, and (iii) the recovery algorithm is empirically fast and
runs in time polynomial in k and log(N).
  We also present a comprehensive study of the application of our algorithm to
attitude determination, or finding one's orientation in space. Spacecraft
typically use cameras to acquire an image of the sky, and then identify stars
in the image to compute their orientation. Taking pictures is very expensive
for small spacecraft, since camera sensors use a lot of power. Our algorithm
optically compresses the image before it reaches the camera's array of pixels,
reducing the number of sensors that are required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2448</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2448</id><created>2012-08-12</created><updated>2012-11-07</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Bao</keyname><forenames>Zhifeng</forenames></author><author><keyname>Li</keyname><forenames>Guoliang</forenames></author><author><keyname>Ling</keyname><forenames>Tok Wang</forenames></author><author><keyname>Lu</keyname><forenames>Jiaheng</forenames></author></authors><title>Breaking Out The XML MisMatch Trap</title><categories>cs.DB</categories><comments>The article is already withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In keyword search, when user cannot get what she wants, query refinement is
needed and reason can be various. We first give a thorough categorization of
the reason, then focus on solving one category of query refinement problem in
the context of XML keyword search, where what user searches for does not exist
in the data. We refer to it as the MisMatch problem in this paper. Then we
propose a practical way to detect the MisMatch problem and generate helpful
suggestions to users. Our approach can be viewed as a post-processing job of
query evaluation, and has three main features: (1) it adopts both the suggested
queries and their sample results as the output to user, helping user judge
whether the MisMatch problem is solved without consuming all query results; (2)
it is portable in the sense that it can work with any LCA-based matching
semantics and orthogonal to the choice of result retrieval method adopted; (3)
it is lightweight in the way that it occupies a very small proportion of the
whole query evaluation time. Extensive experiments on three real datasets
verify the effectiveness, efficiency and scalability of our approach. An online
XML keyword search engine called XClear that embeds the MisMatch problem
detector and suggester has been built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2451</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2451</id><created>2012-08-12</created><authors><author><keyname>Khabou</keyname><forenames>Amal</forenames><affiliation>LRI</affiliation></author><author><keyname>Demmel</keyname><forenames>James W.</forenames><affiliation>UC Berkeley</affiliation></author><author><keyname>Grigori</keyname><forenames>Laura</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Gu</keyname><forenames>Ming</forenames><affiliation>UC Berkeley</affiliation></author></authors><title>LU factorization with panel rank revealing pivoting and its
  communication avoiding version</title><categories>cs.NA</categories><comments>No. RR-7867 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the LU decomposition with panel rank revealing pivoting (LU_PRRP),
an LU factorization algorithm based on strong rank revealing QR panel
factorization. LU_PRRP is more stable than Gaussian elimination with partial
pivoting (GEPP). Our extensive numerical experiments show that the new
factorization scheme is as numerically stable as GEPP in practice, but it is
more resistant to pathological cases and easily solves the Wilkinson matrix and
the Foster matrix. We also present CALU_PRRP, a communication avoiding version
of LU_PRRP that minimizes communication. CALU_PRRP is based on tournament
pivoting, with the selection of the pivots at each step of the tournament being
performed via strong rank revealing QR factorization. CALU_PRRP is more stable
than CALU, the communication avoiding version of GEPP. CALU_PRRP is also more
stable in practice and is resistant to pathological cases on which GEPP and
CALU fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2456</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2456</id><created>2012-08-12</created><updated>2012-08-29</updated><authors><author><keyname>Martinez</keyname><forenames>Genaro J.</forenames></author><author><keyname>Seck-Tuoh-Mora</keyname><forenames>J. C.</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Wolfram's Classification and Computation in Cellular Automata Classes
  III and IV</title><categories>nlin.CG cs.CC cs.IT math.DS math.IT</categories><comments>27 pages, 13 figures, forthcoming in Irreducibility and Computational
  Equivalence to be published by Springer Verlag
  (http://www.mathrix.org/ANKSAnniversaryVolume.html). Extended paper version
  to appear in the Journal of Cellular Automata (JCA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct a brief survey on Wolfram's classification, in particular related
to the computing capabilities of Cellular Automata (CA) in Wolfram's classes
III and IV. We formulate and shed light on the question of whether Class III
systems are capable of Turing universality or may turn out to be &quot;too hot&quot; in
practice to be controlled and programmed. We show that systems in Class III are
indeed capable of computation and that there is no reason to believe that they
are unable, in principle, to reach Turing-completness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2457</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2457</id><created>2012-08-12</created><authors><author><keyname>Syropoulos</keyname><forenames>Apostolos</forenames></author></authors><title>On Generalized Fuzzy Multisets and their Use in Computation</title><categories>cs.LO</categories><comments>13 pages, 1 figure</comments><journal-ref>Iranian Journal of Fuzzy Systems, vol. 9, number 2 (2012), pp.
  115-127</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An orthogonal approach to the fuzzification of both multisets and hybrid sets
is presented. In particular, we introduce L-multi-fuzzy and L-fuzzy hybrid
sets, which are general enough and in spirit with the basic concepts of fuzzy
set theory. In addition, we study the properties of these structures. Also, the
usefulness of these structures is examined in the framework of mechanical
multiset processing. More specifically, we introduce a variant of fuzzy P
systems and, since simple fuzzy membrane systems have been introduced
elsewhere, we simply extend previously stated results and ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2460</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2460</id><created>2012-08-12</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Decision Taking for Selling Thread Startup</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision Taking is discussed in the context of the role it may play for a
selling agent in a search market, in particular for agents involved in the sale
of valuable and relatively unique items, such as a dwelling, a second hand car,
or a second hand recreational vessel.
  Detailed connections are made between the architecture of decision making
processes and a sample of software technology based concepts including
instruction sequences, multi-threading, and thread algebra.
  Ample attention is paid to the initialization or startup of a thread
dedicated to achieving a given objective, and to corresponding decision taking.
As an application, the selling of an item is taken as an objective to be
achieved by running a thread that was designed for that purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2469</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2469</id><created>2012-08-12</created><authors><author><keyname>Bonet</keyname><forenames>Maria Luisa</forenames></author><author><keyname>Buss</keyname><forenames>Sam</forenames></author><author><keyname>Johannsen</keyname><forenames>Jan</forenames></author></authors><title>Improved Separations of Regular Resolution from Clause Learning Proof
  Systems</title><categories>cs.LO math.LO</categories><comments>40 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1202.2296</comments><msc-class>03F20, 03B35, 68T15, 68Q99</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the graph tautology formulas of Alekhnovich, Johannsen,
Pitassi, and Urquhart have polynomial size pool resolution refutations that use
only input lemmas as learned clauses and without degenerate resolution
inferences. We also prove that these graph tautology formulas can be refuted by
polynomial size DPLL proofs with clause learning, even when restricted to
greedy, unit-propagating DPLL search. We prove similar results for the guarded,
xor-fied pebbling tautologies which Urquhart proved are hard for regular
resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2478</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2478</id><created>2012-08-12</created><authors><author><keyname>Gollapudi</keyname><forenames>Sreenivas</forenames></author><author><keyname>Ieong</keyname><forenames>Samuel</forenames></author><author><keyname>Kannan</keyname><forenames>Anitha</forenames></author></authors><title>Structured Query Reformulations in Commerce Search</title><categories>cs.IR cs.DB</categories><comments>A shorter version appeared in CIKM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in commerce search has shown that understanding the semantics in
user queries enables more effective query analysis and retrieval of relevant
products. However, due to lack of sufficient domain knowledge, user queries
often include terms that cannot be mapped directly to any product attribute.
For example, a user looking for {\tt designer handbags} might start with such a
query because she is not familiar with the manufacturers, the price ranges,
and/or the material that gives a handbag designer appeal. Current commerce
search engines treat terms such as {\tt designer} as keywords and attempt to
match them to contents such as product reviews and product descriptions, often
resulting in poor user experience.
  In this study, we propose to address this problem by reformulating queries
involving terms such as {\tt designer}, which we call \emph{modifiers}, to
queries that specify precise product attributes. We learn to rewrite the
modifiers to attribute values by analyzing user behavior and leveraging
structured data sources such as the product catalog that serves the queries. We
first produce a probabilistic mapping between the modifiers and attribute
values based on user behavioral data. These initial associations are then used
to retrieve products from the catalog, over which we infer sets of attribute
values that best describe the semantics of the modifiers. We evaluate the
effectiveness of our approach based on a comprehensive Mechanical Turk study.
We find that users agree with the attribute values selected by our approach in
about 95% of the cases and they prefer the results surfaced for our
reformulated queries to ones for the original queries in 87% of the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2486</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2486</id><created>2012-08-12</created><authors><author><keyname>Upreti</keyname><forenames>Nitish</forenames></author></authors><title>`CodeAliker' - Plagiarism Detection on the Cloud</title><categories>cs.OH</categories><journal-ref>Advanced Computing: An International Journal ( ACIJ ), Vol.3,
  No.4, July 2012, 21-26</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plagiarism is a burning problem that academics have been facing in all of the
varied levels of the educational system. With the advent of digital content,
the challenge to ensure the integrity of academic work has been amplified. This
paper discusses on defining a precise definition of plagiarized computer code,
various solutions available for detecting plagiarism and building a cloud
platform for plagiarism disclosure. 'CodeAliker', our application thus
developed automates the submission of assignments and the review process
associated for essay text as well as computer code. It has been made available
under the GNU's General Public License as a Free and Open Source Software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2488</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2488</id><created>2012-08-12</created><authors><author><keyname>Zhou</keyname><forenames>Bo</forenames></author><author><keyname>Song</keyname><forenames>Qiankun</forenames></author></authors><title>Period Distribution of Inversive Pseudorandom Number Generators Over
  Galois Rings</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inf. Theory, 9 pages with 3 figures and 5
  tables</comments><msc-class>11B50, 11K45, 11R33, 94A55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2009, Sol\'{e} and Zinoviev (\emph{Eur. J. Combin.}, vol. 30, no. 2, pp.
458-467, 2009) proposed an open problem of arithmetic interest to study the
period of the inversive pseudorandom number generators (IPRNGs) and to give
conditions bearing on $a, b$ to achieve maximal period, we focus on resolving
this open problem. In this paper, the period distribution of the IPRNGs over
the Galois ring $({\rm Z}_{p^{e}},+,\times)$ is considered, where $p&gt;3$ is a
prime and $e\geq 2$ is an integer. The IPRNGs are transformed to 2-dimensional
linear feedback shift registers (LFSRs) so that the analysis of the period
distribution of the IPRNGs is transformed to the analysis of the period
distribution of the LFSRs. Then, by employing some analytical approaches, the
full information on the period distribution of the IPRNGs is obtained, which is
to make exact statistics about the period of the IPRNGs then count the number
of IPRNGs of a specific period when $a$, $b$ and $x_{0}$ traverse all elements
in ${\rm Z}_{p^{e}}$. The analysis process also indicates how to choose the
parameters and the initial values such that the IPRNGs fit specific periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2498</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2498</id><created>2012-08-13</created><authors><author><keyname>Formenti</keyname><forenames>Enrico</forenames><affiliation>Nice Sophia Antipolis University</affiliation></author></authors><title>Proceedings 18th international workshop on Cellular Automata and
  Discrete Complex Systems and 3rd international symposium Journ\'ees Automates
  Cellulaires</title><categories>cs.FL cs.DM</categories><comments>EPTCS 90, 2012</comments><proxy>EPTCS</proxy><acm-class>F.1.1, F.1.2, F.1.3</acm-class><doi>10.4204/EPTCS.90</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 18th International workshop
AUTOMATA and the 3rd international symposium JAC.
  AUTOMATA workshop series aims at gathering researchers from all over the
world working in fundamental aspects of cellular automata and related discrete
complex systems. Topics cover (although they are not limited to): dynamics,
topological, ergodic and algebraic aspects, algorithmic and complexity issues,
emergent properties, formal language processing, symbolic dynamics, models of
parallelism and distributed systems, phenomenological descriptions, scientific
modeling and practical applications. JAC (Journ\'ees Automates Cellulaires) is
a bi-annual symposium covering the same topics and was created to have a very
high standard international conference in the domain. This one will be the
third event of the series after Uz\`es (2008, France), Turku (2010, Finland).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2503</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2503</id><created>2012-08-13</created><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Distributed Pareto Optimization via Diffusion Strategies</title><categories>cs.MA math.OC</categories><comments>35 pages, 9 figures, submitted for publication</comments><doi>10.1109/JSTSP.2013.2246763</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider solving multi-objective optimization problems in a distributed
manner by a network of cooperating and learning agents. The problem is
equivalent to optimizing a global cost that is the sum of individual
components. The optimizers of the individual components do not necessarily
coincide and the network therefore needs to seek Pareto optimal solutions. We
develop a distributed solution that relies on a general class of adaptive
diffusion strategies. We show how the diffusion process can be represented as
the cascade composition of three operators: two combination operators and a
gradient descent operator. Using the Banach fixed-point theorem, we establish
the existence of a unique fixed point for the composite cascade. We then study
how close each agent converges towards this fixed point, and also examine how
close the Pareto solution is to the fixed point. We perform a detailed
mean-square error analysis and establish that all agents are able to converge
to the same Pareto optimal solution within a sufficiently small
mean-square-error (MSE) bound even for constant step-sizes. We illustrate one
application of the theory to collaborative decision making in finance by a
network of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2504</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2504</id><created>2012-08-13</created><updated>2013-02-22</updated><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author></authors><title>Computational topology with Regina: Algorithms, heuristics and
  implementations</title><categories>math.GT cs.CG cs.MS</categories><comments>29 pages, 10 figures; v2: minor revisions. To appear in &quot;Geometry &amp;
  Topology Down Under&quot;, Contemporary Mathematics, AMS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regina is a software package for studying 3-manifold triangulations and
normal surfaces. It includes a graphical user interface and Python bindings,
and also supports angle structures, census enumeration, combinatorial
recognition of triangulations, and high-level functions such as 3-sphere
recognition, unknot recognition and connected sum decomposition.
  This paper brings 3-manifold topologists up-to-date with Regina as it appears
today, and documents for the first time in the literature some of the key
algorithms, heuristics and implementations that are central to Regina's
performance. These include the all-important simplification heuristics, key
choices of data structures and algorithms to alleviate bottlenecks in normal
surface enumeration, modern implementations of 3-sphere recognition and
connected sum decomposition, and more. We also give some historical background
for the project, including the key role played by Rubinstein in its genesis 15
years ago, and discuss current directions for future development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2505</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2505</id><created>2012-08-13</created><updated>2012-12-23</updated><authors><author><keyname>Balkova</keyname><forenames>Lubomira</forenames></author><author><keyname>Pelantova</keyname><forenames>Edita</forenames></author><author><keyname>Starosta</keyname><forenames>Stepan</forenames></author></authors><title>Proof of Brlek-Reutenauer conjecture</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><msc-class>68R15</msc-class><journal-ref>Theoretical Computer Science 475 (2013), 120-125</journal-ref><doi>10.1016/j.tcs.2012.12.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brlek and Reutenauer conjectured that any infinite word u with language
closed under reversal satisfies the equality 2D(u) = \sum_{n=0}^{\infty}T_u(n)
in which D(u) denotes the defect of u and T_u(n) denotes C_u(n+1)-C_u(n) +2 -
P_U(n+1) - P_u(n), where C_u and P_u are the factor and palindromic complexity
of u, respectively. This conjecture was verified for periodic words by Brlek
and Reutenauer themselves. Using their results for periodic words, we have
recently proved the conjecture for uniformly recurrent words. In the present
article we prove the conjecture in its general version by a new method without
exploiting the result for periodic words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2507</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2507</id><created>2012-08-13</created><authors><author><keyname>Bogana</keyname><forenames>S.</forenames></author><author><keyname>Helali</keyname><forenames>M.</forenames></author><author><keyname>Paykan</keyname><forenames>K.</forenames></author></authors><title>Error Probability of OSTB Codes and Capacity Analysis with Antenna
  Selection over Single-Antenna AF Relay Channels</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the symbol error rate (SER) and the bit error rate (BER) of
orthogonal space-time block codes (OSTBCs) and their achievable capacity over
an amplify-and-forward (AF) relay channel with multiple antennas at source and
destination and single antenna at relay node are investigated. Considered are
receive antenna selection, transmit antenna selection, and joint antenna
selection at both the transmitter and the receiver. The exact SERs of OSTBCs
for M-PSK and square M-QAM constellations are obtained using the moment
generating functions (MGFs). Also, we analyze the achievable capacity over such
channels assuming antenna selection is done at the source and relay nodes. We
show that a small number of selected antennas can achieve the capacity of the
system in which no channel state information (CSI) is available at the source
and relay nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2515</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2515</id><created>2012-08-13</created><updated>2013-07-06</updated><authors><author><keyname>Baransky</keyname><forenames>Eliahu</forenames></author><author><keyname>Itzhak</keyname><forenames>Gal</forenames></author><author><keyname>Shmuel</keyname><forenames>Idan</forenames></author><author><keyname>Wagner</keyname><forenames>Noam</forenames></author><author><keyname>Shoshan</keyname><forenames>Eli</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>A Sub-Nyquist Radar Prototype: Hardware and Algorithms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional radar sensing typically involves matched filtering between the
received signal and the shape of the transmitted pulse. Under the confinement
of classic sampling theorem this requires that the received signals must first
be sampled at twice the baseband bandwidth, in order to avoid aliasing. The
growing demands for target distinction capability and spatial resolution imply
significant growth in the bandwidth of the transmitted pulse. Thus, correlation
based radar systems require high sampling rates, and with the large amounts of
data sampled also necessitate vast memory capacity. In addition, real-time
processing of the data typically results in high power consumption. Recently,
new approaches for radar sensing and detection were introduced, based on the
Finite Rate of Innovation and Xampling frameworks. These techniques allow
significant reduction in sampling rate, implying potential power savings, while
maintaining the system's detection capabilities at high enough SNR. Here we
present for the first time a design and implementation of a Xampling-based
hardware prototype that allows sampling of radar signals at rates much lower
than Nyquist. We demostrate by real-time analog experiments that our system is
able to maintain reasonable detection capabilities, while sampling radar
signals that require sampling at a rate of about 30MHz at a total rate of 1Mhz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2518</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2518</id><created>2012-08-13</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Software systems through complex networks science: Review, analysis and
  applications</title><categories>cs.SI cs.SE physics.soc-ph</categories><journal-ref>Proceedings of the KDD Workshop on Software Mining 2012
  (SoftwareMining '12), pp. 8</journal-ref><doi>10.1145/2384416.2384418</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex software systems are among most sophisticated human-made systems, yet
only little is known about the actual structure of 'good' software. We here
study different software systems developed in Java from the perspective of
network science. The study reveals that network theory can provide a prominent
set of techniques for the exploratory analysis of large complex software
system. We further identify several applications in software engineering, and
propose different network-based quality indicators that address software
design, efficiency, reusability, vulnerability, controllability and other. We
also highlight various interesting findings, e.g., software systems are highly
vulnerable to processes like bug propagation, however, they are not easily
controllable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2523</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2523</id><created>2012-08-13</created><authors><author><keyname>Rawlik</keyname><forenames>Konrad</forenames></author><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author><author><keyname>Vijayakumar</keyname><forenames>Sethu</forenames></author></authors><title>Path Integral Control by Reproducing Kernel Hilbert Space Embedding</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an embedding of stochastic optimal control problems, of the so
called path integral form, into reproducing kernel Hilbert spaces. Using
consistent, sample based estimates of the embedding leads to a model free,
non-parametric approach for calculation of an approximate solution to the
control problem. This formulation admits a decomposition of the problem into an
invariant and task dependent component. Consequently, we make much more
efficient use of the sample data compared to previous sample based approaches
in this domain, e.g., by allowing sample re-use across tasks. Numerical
examples on test problems, which illustrate the sample efficiency, are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2534</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2534</id><created>2012-08-13</created><authors><author><keyname>Pinto</keyname><forenames>Pedro C.</forenames></author><author><keyname>Thiran</keyname><forenames>Patrick</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Locating the Source of Diffusion in Large-Scale Networks</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>To appear in Physical Review Letters. Includes pre-print of main
  paper, and supplementary material</comments><doi>10.1103/PhysRevLett.109.068702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we localize the source of diffusion in a complex network? Due to the
tremendous size of many real networks--such as the Internet or the human social
graph--it is usually infeasible to observe the state of all nodes in a network.
We show that it is fundamentally possible to estimate the location of the
source from measurements collected by sparsely-placed observers. We present a
strategy that is optimal for arbitrary trees, achieving maximum probability of
correct localization. We describe efficient implementations with complexity
O(N^{\alpha}), where \alpha=1 for arbitrary trees, and \alpha=3 for arbitrary
graphs. In the context of several case studies, we determine how localization
accuracy is affected by various system parameters, including the structure of
the network, the density of observers, and the number of observed cascades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2543</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2543</id><created>2012-08-13</created><authors><author><keyname>Luxen</keyname><forenames>Dennis</forenames></author><author><keyname>Schieferdecker</keyname><forenames>Dennis</forenames></author></authors><title>Doing More for Less -- Cache-Aware Parallel Contraction Hierarchies
  Preprocessing</title><categories>cs.DS cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contraction Hierarchies is a successful speedup-technique to Dijkstra's
seminal shortest path algorithm that has a convenient trade-off between
preprocessing and query times. We investigate a shared-memory parallel
implementation that uses $O(n+m)$ space for storing the graph and O(1) space
for each core during preprocessing. The presented data structures and
algorithms consequently exploits cache locality and thus exhibit competitive
preprocessing times. The presented implementation is especially suitable for
preprocessing graphs of planet-wide scale in practice. Also, our experiments
show that optimal data structures in the PRAM model can be beaten in practice
by exploiting memory cache hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2547</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2547</id><created>2012-08-13</created><authors><author><keyname>Wang</keyname><forenames>Yanxiang</forenames></author><author><keyname>Sundaram</keyname><forenames>Hari</forenames></author><author><keyname>Xie</keyname><forenames>Lexing</forenames></author></authors><title>Social Event Detection with Interaction Graph Modeling</title><categories>cs.SI cs.IR cs.MM physics.soc-ph</categories><comments>ACM Multimedia 2012</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on detecting social, physical-world events from photos
posted on social media sites. The problem is important: cheap media capture
devices have significantly increased the number of photos shared on these
sites. The main contribution of this paper is to incorporate online social
interaction features in the detection of physical events. We believe that
online social interaction reflect important signals among the participants on
the &quot;social affinity&quot; of two photos, thereby helping event detection. We
compute social affinity via a random-walk on a social interaction graph to
determine similarity between two photos on the graph. We train a support vector
machine classifier to combine the social affinity between photos and
photo-centric metadata including time, location, tags and description.
Incremental clustering is then used to group photos to event clusters. We have
very good results on two large scale real-world datasets: Upcoming and
MediaEval. We show an improvement between 0.06-0.10 in F1 on these datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2559</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2559</id><created>2012-08-13</created><updated>2016-02-19</updated><authors><author><keyname>Wild</keyname><forenames>Marcel</forenames></author></authors><title>Revisiting the enumeration of all models of a Boolean 2-CNF</title><categories>cs.CC</categories><comments>Among other things, find a more detailed analysis of how our
  algorithm compares to Feder's 1992 method</comments><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An O(Nn^2 + n^2) time algorithm to enumerate all N models of a Boolean 2-CNF
with n variables is presented. Using don't care symbols the models are output
in clusters rather than one by one. Computer experiments confirm the high
efficiency of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2561</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2561</id><created>2012-08-13</created><authors><author><keyname>Traxler</keyname><forenames>Patrick</forenames></author></authors><title>The Relative Exponential Time Complexity of Approximate Counting
  Satisfying Assignments</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the exponential time complexity of approximate counting satisfying
assignments of CNFs. We reduce the problem to deciding satisfiability of a CNF.
Our reduction preserves the number of variables of the input formula and thus
also preserves the exponential complexity of approximate counting.
  Our algorithm is also similar to an algorithm which works particular well in
practice for which however no approximation guarantee was known. Towards an
analysis of our reduction we provide a new inequality similar to the
Bonami-Beckner hypercontractive inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2563</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2563</id><created>2012-08-13</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author></authors><title>Towards a Formalization of the OSGi Component Framework</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formalization of the OSGi component framework. Our formalization
is intended to be used as a basis for describing behavior of OSGi based
systems. Furthermore, we describe specification formalisms for describing
properties of OSGi based systems. One application is its use for behavioral
types. Potential uses comprise the derivation of runtime monitors, checking
compatibility of component composition, discovering components using brokerage
services and checking the compatibility of implementation artifacts towards a
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2566</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2566</id><created>2012-08-13</created><authors><author><keyname>Baeckstroem</keyname><forenames>Christer</forenames></author><author><keyname>Chen</keyname><forenames>Yue</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>The Complexity of Planning Revisited - A Parameterized Analysis</title><categories>cs.AI</categories><comments>(author's self-archived copy)</comments><journal-ref>Proc. AAAI'12 (AAAI Press 2012) pp. 1735-1741</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The early classifications of the computational complexity of planning under
various restrictions in STRIPS (Bylander) and SAS+ (Baeckstroem and Nebel) have
influenced following research in planning in many ways. We go back and
reanalyse their subclasses, but this time using the more modern tool of
parameterized complexity analysis. This provides new results that together with
the old results give a more detailed picture of the complexity landscape. We
demonstrate separation results not possible with standard complexity theory,
which contributes to explaining why certain cases of planning have seemed
simpler in practice than theory has predicted. In particular, we show that
certain restrictions of practical interest are tractable in the parameterized
sense of the term, and that a simple heuristic is sufficient to make a
well-known partial-order planner exploit this fact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2572</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2572</id><created>2012-08-13</created><authors><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Villa</keyname><forenames>Silvia</forenames></author><author><keyname>Mosci</keyname><forenames>Sofia</forenames></author><author><keyname>Santoro</keyname><forenames>Matteo</forenames></author><author><keyname>verri</keyname><forenames>Alessandro</forenames></author></authors><title>Nonparametric sparsity and regularization</title><categories>stat.ML cs.LG math.OC</categories><comments>45 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we are interested in the problems of supervised learning and
variable selection when the input-output dependence is described by a nonlinear
function depending on a few variables. Our goal is to consider a sparse
nonparametric model, hence avoiding linear or additive models. The key idea is
to measure the importance of each variable in the model by making use of
partial derivatives. Based on this intuition we propose a new notion of
nonparametric sparsity and a corresponding least squares regularization scheme.
Using concepts and results from the theory of reproducing kernel Hilbert spaces
and proximal methods, we show that the proposed learning algorithm corresponds
to a minimization problem which can be provably solved by an iterative
procedure. The consistency properties of the obtained estimator are studied
both in terms of prediction and selection performance. An extensive empirical
analysis shows that the proposed method performs favorably with respect to the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2583</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2583</id><created>2012-08-13</created><updated>2012-12-11</updated><authors><author><keyname>Veller</keyname><forenames>Carl</forenames></author><author><keyname>Rajpaul</keyname><forenames>Vinesh</forenames></author></authors><title>Purely competitive evolutionary dynamics for games</title><categories>q-bio.PE cs.GT math.DS</categories><comments>13 pages, 6 figures</comments><journal-ref>Phys. Rev. E 86, 041907 (2012)</journal-ref><doi>10.1103/PhysRevE.86.041907</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze a purely competitive dynamics for the evolution of
an infinite population subject to a 3-strategy game. We argue that this
dynamics represents a characterization of how certain systems, both natural and
artificial, are governed. In each period, the population is randomly sorted
into pairs, which engage in a once-off play of the game; the probability that a
member propagates its type to its offspring is proportional only to its payoff
within the pair. We show that if a type is dominant (obtains higher payoffs in
games with both other types), its 'pure' population state, comprising only
members of that type, is globally attracting. If there is no dominant type,
there is an unstable 'mixed' fixed point; the population state eventually
oscillates between the three near-pure states. We then allow for mutations,
where offspring have a non-zero probability of randomly changing their type. In
this case, the existence of a dominant type renders a point near its pure state
globally attracting. If no dominant type exists, a supercritical Hopf
bifurcation occurs at the unique mixed fixed point, and above a critical
(typically low) mutation rate, this fixed point becomes globally attracting:
the implication is that even very low mutation rates can stabilize a system
that would, in the absence of mutations, be unstable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2585</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2585</id><created>2012-07-30</created><authors><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames><affiliation>Tel Aviv University</affiliation></author></authors><title>The Generic Model of Computation</title><categories>cs.LO cs.PL</categories><comments>In Proceedings DCM 2011, arXiv:1207.6821</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.3.1</acm-class><journal-ref>EPTCS 88, 2012, pp. 59-71</journal-ref><doi>10.4204/EPTCS.88.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades, Yuri Gurevich and his colleagues have formulated
axiomatic foundations for the notion of algorithm, be it classical,
interactive, or parallel, and formalized them in the new generic framework of
abstract state machines. This approach has recently been extended to suggest a
formalization of the notion of effective computation over arbitrary countable
domains. The central notions are summarized herein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2596</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2596</id><created>2012-08-13</created><authors><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author><author><keyname>Lu</keyname><forenames>Xin</forenames></author></authors><title>Near-Optimal Online Algorithms for Dynamic Resource Allocation Problems</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a general online linear programming problem whose
formulation encompasses many practical dynamic resource allocation problems,
including internet advertising display applications, revenue management,
various routing, packing, and auction problems. We propose a model, which under
mild assumptions, allows us to design near-optimal learning-based online
algorithms that do not require the a priori knowledge about the total number of
online requests to come, a first of its kind. We then consider two variants of
the problem that relax the initial assumptions imposed on the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2609</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2609</id><created>2012-08-09</created><authors><author><keyname>Carvalho</keyname><forenames>Alexsandro M.</forenames></author><author><keyname>Goncalves</keyname><forenames>Sebastian</forenames></author></authors><title>Epidemics scenarios in the &quot;Romantic network&quot;</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>9 pages text, plus references, and 10 figures (with subfigures)
  Epidemic simulations on a small real network</comments><msc-class>81T80, 91D30, 92D25</msc-class><doi>10.1371/journal.pone.0049009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of sexual contacts, its contacts network and its temporal
interactions, play an important role in the spread of sexually transmitted
infections. Unfortunately, that kind of data is very hard to obtain. One of the
few exceptions is the &quot;Romantic network&quot; which is a complete structure of a
real sexual network of a high school. In terms of topology, unlike other sexual
networks classified as scale-free network. Regarding the temporal structure,
several studies indicate that relationship timing can have effects on diffusion
through networks, as relationship order determines transmission routes.With the
aim to check if the particular structure, static and dynamic, of the Romantic
network is determinant for the propagation of an STI in it, we perform
simulations in two scenarios: the static network where all contacts are
available and the dynamic case where contacts evolve in time. In the static
case, we compare the epidemic results in the Romantic network with some
paradigmatic topologies. We further study the behavior of the epidemic on the
Romantic network in response to the effect of any individual, belonging to the
network, having a contact with an external infected subject, the influence of
the degree of the initial infected, and the effect of the variability of
contacts per unit time. We also consider the dynamics of formation of pairs in
and we study the propagation of the diseases in this dynamic scenario. Our
results suggest that while the Romantic network can not be labeled as a
Watts-Strogatz network, it is, regarding the propagation of an STI, very close
to one with high disorder. Our simulations confirm that relationship timing
affects, but strongly lowering, the final outbreak size. Besides, shows a clear
correlation between the average degree and the outbreak size over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2618</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2618</id><created>2012-08-07</created><authors><author><keyname>Carro</keyname><forenames>Adrian</forenames></author><author><keyname>Toral</keyname><forenames>Raul</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author></authors><title>The role of noise and initial conditions in the asymptotic solution of a
  bounded confidence, continuous-opinion model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>J. Stat. Phys. 151 (2013) pp 131-149</journal-ref><doi>10.1007/s10955-012-0635-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model for continuous-opinion dynamics under bounded confidence. In
particular, we analyze the importance of the initial distribution of opinions
in determining the asymptotic configuration. Thus, we sketch the structure of
attractors of the dynamical system, by means of the numerical computation of
the time evolution of the agents density. We show that, for a given bound of
confidence, a consensus can be encouraged or prevented by certain initial
conditions. Furthermore, a noisy perturbation is added to the system with the
purpose of modeling the free will of the agents. As a consequence, the
importance of the initial condition is partially replaced by that of the
statistical distribution of the noise. Nevertheless, we still find evidence of
the influence of the initial state upon the final configuration for a short
range of the bound of confidence parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2620</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2620</id><created>2012-08-07</created><authors><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author></authors><title>Empowered Customers in E-Health Business Process</title><categories>cs.OH</categories><comments>ASEAN University Network (AUN), FBEPS-AGBEP PhD Colloquium (5-6 June,
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-health innovations support empowered customers. It offers the ability for
customers to have greater control and ready access applications of health
information, clinical information, and social interaction between interested
groups. However, providing empowerment in any state of interaction levels to
customers (patients) in a healthcare organization is challenging tasks.
Customers are empowered in the sense of controlling the process of interaction
between a firm with its customers, and among customers themselves. This paper
discusses dimension of customers' empowerment in e-health business process. We
propose reference model of Personal Health Cycle (PHC) as a holistic view of
healthcare business process. The PHC is used to define and distinct electronic
health record (EHR) from electronic medical record (EMR) and customers
empowerment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2624</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2624</id><created>2012-08-13</created><authors><author><keyname>Klimosova</keyname><forenames>Tereza</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author></authors><title>Hereditary properties of permutations are strongly testable</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for every hereditary permutation property P and every eps&gt;0,
there exists an integer M such that if a permutation p is eps-far from P in the
Kendall's tau distance, then a random subpermutation of p of order M has the
property P with probability at most eps. This settles an open problem whether
hereditary permutation properties are strongly testable, i.e., testable with
respect to the Kendall's tau distance. In addition, our method also yields a
proof of a conjecture of Hoppen, Kohayakawa, Moreira and Sampaio on the
relation of the rectangular distance and the Kendall's tau distance of a
permutation from a hereditary property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2631</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2631</id><created>2012-08-13</created><authors><author><keyname>Citkin</keyname><forenames>Alex</forenames></author></authors><title>Characteristic formulas over intermediate logics</title><categories>cs.LO math.LO</categories><msc-class>03B55, 03C05, 03B45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We expand the notion of characteristic formula to infinite finitely
presentable subdirectly irreducible algebras. We prove that there is a
continuum of varieties of Heyting algebras containing infinite finitely
presentable subdirectly irreducible algebras. Moreover, we prove that there is
a continuum of intermediate logics that can be axiomatized by characteristic
formulas of infinite algebras while they are not axiomatizable by standard
Jankov formulas. We give the examples of intermediate logics that are not
axiomatizable by characteristic formulas of infinite algebras. Also, using the
Goedel-McKinsey-Tarski translation we extend these results to the varieties of
interior algebras and normal extensions of S4
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2649</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2649</id><created>2012-08-13</created><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author><author><keyname>Parashar</keyname><forenames>Manish</forenames></author><author><keyname>Rana</keyname><forenames>Omer</forenames></author><author><keyname>Weissman</keyname><forenames>Jon</forenames></author></authors><title>Survey and Analysis of Production Distributed Computing Infrastructures</title><categories>cs.DC</categories><report-no>Computation Institute, University of Chicago, Technical Report
  CI-TR-7-0811</report-no><acm-class>C.2.4; C.5.0; K.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report has two objectives. First, we describe a set of the production
distributed infrastructures currently available, so that the reader has a basic
understanding of them. This includes explaining why each infrastructure was
created and made available and how it has succeeded and failed. The set is not
complete, but we believe it is representative.
  Second, we describe the infrastructures in terms of their use, which is a
combination of how they were designed to be used and how users have found ways
to use them. Applications are often designed and created with specific
infrastructures in mind, with both an appreciation of the existing capabilities
provided by those infrastructures and an anticipation of their future
capabilities. Here, the infrastructures we discuss were often designed and
created with specific applications in mind, or at least specific types of
applications. The reader should understand how the interplay between the
infrastructure providers and the users leads to such usages, which we call
usage modalities. These usage modalities are really abstractions that exist
between the infrastructures and the applications; they influence the
infrastructures by representing the applications, and they influence the ap-
plications by representing the infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2651</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2651</id><created>2012-08-13</created><authors><author><keyname>Boulesteix</keyname><forenames>Anne-Laure</forenames></author><author><keyname>Eugster</keyname><forenames>Manuel J. A.</forenames></author></authors><title>A Plea for Neutral Comparison Studies in Computational Sciences</title><categories>stat.CO cs.CV stat.ME stat.ML</categories><doi>10.1371/journal.pone.0061562</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a context where most published articles are devoted to the development of
&quot;new methods&quot;, comparison studies are generally appreciated by readers but
surprisingly given poor consideration by many scientific journals. In
connection with recent articles on over-optimism and epistemology published in
Bioinformatics, this letter stresses the importance of neutral comparison
studies for the objective evaluation of existing methods and the establishment
of standards by drawing parallels with clinical research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2654</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2654</id><created>2012-08-13</created><updated>2013-10-01</updated><authors><author><keyname>Sroka</keyname><forenames>Jacek</forenames></author><author><keyname>Hidders</keyname><forenames>Jan</forenames></author></authors><title>On Generating *-Sound Nets with Substitution</title><categories>cs.FL cs.DC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for hierarchically generating sound workflow nets by
substitution of nets with multiple inputs and outputs. We show that this method
is correct and generalizes the class of nets generated by other hierarchical
approaches. The method involves a new notion of soundness which is preserved by
the generalized type of substitution that is presented in this paper. We show
that this notion is better suited than *-soundness for use with the presented
type of generalized substitution, since {*}-soundness is not preserved by it.
It is moreover shown that it is in some sense the optimal notion of soundness
for the purpose of generating sound nets by the presented type of substitution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2655</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2655</id><created>2012-08-13</created><authors><author><keyname>Kharinov</keyname><forenames>M.</forenames></author></authors><title>Stable Segmentation of Digital Image</title><categories>cs.CV</categories><comments>6 pages, 10 formulas, 2 figures, in russian</comments><journal-ref>Proc. of the 22th Int. Conf. on Comp. Graphics and Vision
  (Graphicon 2012), Moscow: MSU, 01-05 Oct. 2012, pp. 208-213. (in Russian)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper the optimal image segmentation by means of piecewise constant
approximations is considered. The optimality is defined by a minimum value of
the total squared error or by equivalent value of standard deviation of the
approximation from the image. The optimal approximations are defined
independently on the method of their obtaining and might be generated in
different algorithms. We investigate the computation of the optimal
approximation on the grounds of stability with respect to a given set of
modifications. To obtain the optimal approximation the Mumford-Shuh model is
generalized and developed, which in the computational part is combined with the
Otsu method in multi-thresholding version. The proposed solution is proved
analytically and experimentally on the example of the standard image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2675</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2675</id><created>2012-08-13</created><authors><author><keyname>Paul</keyname><forenames>Gerald</forenames></author></authors><title>A GPU implementation of the Simulated Annealing Heuristic for the
  Quadratic Assignment Problem</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quadratic assignment problem (QAP) is one of the most difficult
combinatorial optimization problems. An effective heuristic for obtaining
approximate solutions to the QAP is simulated annealing (SA). Here we describe
an SA implementation for the QAP which runs on a graphics processing unit
(GPU). GPUs are composed of low cost commodity graphics chips which in
combination provide a powerful platform for general purpose parallel computing.
For SA runs with large numbers of iterations, we find performance 50-100 times
better than that of a recent non-parallel but very efficient implementation of
SA for the QAP
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2686</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2686</id><created>2012-08-13</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Xu</keyname><forenames>Shenmeng</forenames></author><author><keyname>Peng</keyname><forenames>Lian</forenames></author><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Wang</keyname><forenames>Chuanli</forenames></author><author><keyname>Zhang</keyname><forenames>Chunbo</forenames></author><author><keyname>Wang</keyname><forenames>Xianbing</forenames></author></authors><title>Exploring scientists' working timetable: Do scientists often work
  overtime?</title><categories>cs.DL physics.soc-ph</categories><comments>9 pages, 5 figures</comments><journal-ref>Journal of Informetrics. 2012, 6(4): 655-660</journal-ref><doi>10.1016/j.joi.2012.07.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method is proposed to monitor and record scientists' working
timetable. We record the downloads information of scientific papers real-timely
from Springer round the clock, and try to explore scientists' working habits.
As our observation demonstrates, many scientists are still engaged in their
research after working hours every day. Many of them work far into the night,
even till next morning. In addition, research work also intrudes into their
weekends. Different working time patterns are revealed. In the US, overnight
work is more prevalent among scientists, while Chinese scientists mostly have
busy weekends with their scientific research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2698</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2698</id><created>2012-08-13</created><authors><author><keyname>Canzar</keyname><forenames>Stefan</forenames></author><author><keyname>Andreotti</keyname><forenames>Sandro</forenames></author></authors><title>A Branch-and-Cut Algorithm for the 2-Species Duplication-Loss Phylogeny
  Problem</title><categories>math.CO cs.DM q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reconstruction of the history of evolutionary genome-wide events among a
set of related organisms is of great biological interest. A simplified model
that captures only content modifying operations was introduced recently. It
allows the small phylogeny problem to be formulated as an alignment problem. In
this work we present a branch-and-cut algorithm for this so-called
duplication-loss alignment problem. Our method clearly outperforms the existing
ILP based method by several orders of magnitude. We define classes of valid
inequalities and provide algorithms to separate them efficiently and prove the
NP-hardness of the duplication-loss alignment problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2712</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2712</id><created>2012-08-13</created><authors><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>PERA</affiliation></author><author><keyname>Atay</keyname><forenames>Ozgovde</forenames><affiliation>PERA</affiliation></author></authors><title>Topological measures for the analysis of wireless sensor networks</title><categories>cs.NI cs.SI</categories><comments>3rd International Conference on Ambient Systems (ANT), Networks and
  Technologies, Niagara Falls : Canada (2012)</comments><proxy>ccsd</proxy><doi>10.1016/j.procs.2012.06.052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concepts such as energy dependence, random deployment, dynamic topological
update, self-organization, varying large number of nodes are among many factors
that make WSNs a type of complex system. However, when analyzing WSNs
properties using complex network tools, classical topological measures must be
considered with care as they might not be applicable in their original form. In
this work, we focus on the topological measures frequently used in the related
field of Internet topological analysis. We illustrate their applicability to
the WSNs domain through simulation experiments. In the cases when the classic
metrics turn out to be incompatible, we propose some alternative measures and
discuss them based on the WSNs characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2719</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2719</id><created>2012-08-13</created><authors><author><keyname>Coskun</keyname><forenames>Ahmet Faruk</forenames></author><author><keyname>Kucur</keyname><forenames>Oguz</forenames></author></authors><title>Unified Analysis of Transmit Antenna Selection/Space-Time Block Coding
  with Receive Selection and Combining over Nakagami-m Fading Channels in the
  Presence of Feedback Errors</title><categories>cs.IT math.IT stat.OT</categories><comments>29 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Examining the effect of imperfect transmit antenna selection (TAS) caused by
the feedback link errors on the performance of hybrid TAS/space-time block
coding (STBC) with selection combining (SC) (i.e., joint transmit and receive
antenna selection (TRAS)/STBC) and TAS/STBC (with receive maximal-ratio
combining (MRC)-like combining structure) over Nakagami-m fading channels is
the main objective of this paper. Under ideal channel estimation and delay-free
feedback assumptions, statistical expressions and several performance metrics
related to the post-processing signal-to-noise ratio (SNR) are derived for a
unified system model concerning both joint TRAS/STBC and TAS/STBC schemes.
Exact analytical expressions for outage probability and bit/symbol error rates
(BER/SER) of binary and M-ary modulations are presented in order to provide an
extensive examination on the capacity and error performance of the unified
system that experiences feedback errors. Also, the asymptotic diversity order
analysis, which shows that the diversity order of the investigated schemes is
lower bounded by the diversity order provided by STBC transmission itself, is
included in the paper. Moreover, all theoretical results are validated by
performing Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2721</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2721</id><created>2012-08-13</created><updated>2013-07-25</updated><authors><author><keyname>Cook</keyname><forenames>Stephen A.</forenames></author><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author><author><keyname>Le</keyname><forenames>Dai Tri Man</forenames></author></authors><title>The Complexity of the Comparator Circuit Value Problem</title><categories>cs.CC</categories><comments>continues the previous work of Cook, Le and Ye [arXiv:1106.4142]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1990 Subramanian defined the complexity class CC as the set of problems
log-space reducible to the comparator circuit value problem (CCV). He and Mayr
showed that NL \subseteq CC \subseteq P, and proved that in addition to CCV
several other problems are complete for CC, including the stable marriage
problem, and finding the lexicographically first maximal matching in a
bipartite graph. We are interested in CC because we conjecture that it is
incomparable with the parallel class NC which also satisfies NL \subseteq NC
\subseteq P, and note that this conjecture implies that none of the CC-complete
problems has an efficient polylog time parallel algorithm. We provide evidence
for our conjecture by giving oracle settings in which relativized CC and
relativized NC are incomparable.
  We give several alternative definitions of CC, including (among others) the
class of problems computed by uniform polynomial-size families of comparator
circuits supplied with copies of the input and its negation, the class of
problems AC^0-reducible to CCV, and the class of problems computed by uniform
AC^0 circuits with CCV gates. We also give a machine model for CC, which
corresponds to its characterization as log-space uniform polynomial-size
families of comparator circuits. These various characterizations show that CC
is a robust class. The main technical tool we employ is universal comparator
circuits.
  Other results include a simpler proof of NL \subseteq CC, and an explanation
of the relation between the Gale-Shapley algorithm and Subramanian's algorithm
for stable marriage.
  This paper continues the previous work of Cook, L\^e and Ye which focused on
Cook-Nguyen style uniform proof complexity, answering several open questions
raised in that paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2724</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2724</id><created>2012-08-13</created><updated>2012-10-18</updated><authors><author><keyname>Khare</keyname><forenames>Monik</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Caching with rental cost and zapping</title><categories>cs.DS</categories><comments>Caching with rental cost, caching with zapping</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{file caching} problem is defined as follows. Given a cache of size
$k$ (a positive integer), the goal is to minimize the total retrieval cost for
the given sequence of requests to files. A file $f$ has size $size(f)$ (a
positive integer) and retrieval cost $cost(f)$ (a non-negative number) for
bringing the file into the cache. A \emph{miss} or \emph{fault} occurs when the
requested file is not in the cache and the file has to be retrieved into the
cache by paying the retrieval cost, and some other file may have to be removed
(\emph{evicted}) from the cache so that the total size of the files in the
cache does not exceed $k$.
  We study the following variants of the online file caching problem.
\textbf{\emph{Caching with Rental Cost} (or \emph{Rental Caching})}: There is a
rental cost $\lambda$ (a positive number) for each file in the cache at each
time unit. The goal is to minimize the sum of the retrieval costs and the
rental costs. \textbf{\emph{Caching with Zapping}}: A file can be \emph{zapped}
by paying a zapping cost $N \ge 1$. Once a file is zapped, all future requests
of the file don't incur any cost. The goal is to minimize the sum of the
retrieval costs and the zapping costs.
  We study these two variants and also the variant which combines these two
(rental caching with zapping). We present deterministic lower and upper bounds
in the competitive-analysis framework. We study and extend the online covering
algorithm from \citep{young02online} to give deterministic online algorithms.
We also present randomized lower and upper bounds for some of these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2737</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2737</id><created>2012-08-13</created><authors><author><keyname>Tucci</keyname><forenames>Robert R.</forenames></author></authors><title>Shannon Information Theory Without Shedding Tears Over Delta \&amp; Epsilon
  Proofs or Typical Sequences</title><categories>cs.IT math.IT quant-ph</categories><comments>37 pages (files: 1 .tex, 1 .sty)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper begins with a discussion of integration over probability types
(p-types). After doing that, the paper re-visits 3 mainstay problems of
classical (non-quantum) Shannon Information Theory (SIT): source coding without
distortion, channel coding, and source coding with distortion. The paper proves
well-known, conventional results for each of these 3 problems. However, the
proofs given for these results are not conventional. They are based on complex
integration techniques (approximations obtained by applying the method of
steepest descent to p-type integrals) instead of the usual delta &amp; epsilon and
typical sequences arguments. Another unconventional feature of this paper is
that we make ample use of classical Bayesian networks (CB nets). This paper
showcases some of the benefits of using CB nets to do classical SIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2746</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2746</id><created>2012-08-13</created><authors><author><keyname>Bonsangue</keyname><forenames>Marcello M.</forenames><affiliation>LIACS, Leiden University</affiliation></author><author><keyname>Milius</keyname><forenames>Stefan</forenames><affiliation>Technische Universit&#xe4;t Braunschweig</affiliation></author><author><keyname>Rot</keyname><forenames>Jurriaan</forenames><affiliation>LIACS, Leiden University</affiliation></author></authors><title>On the specification of operations on the rational behaviour of systems</title><categories>cs.LO cs.FL</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 89, 2012, pp. 3-18</journal-ref><doi>10.4204/EPTCS.89.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural operational semantics can be studied at the general level of
distributive laws of syntax over behaviour. This yields specification formats
for well-behaved algebraic operations on final coalgebras, which are a domain
for the behaviour of all systems of a given type functor. We introduce a format
for specification of algebraic operations that restrict to the rational
fixpoint of a functor, which captures the behaviour of finite systems. In other
words, we show that rational behaviour is closed under operations specified in
our format. As applications we consider operations on regular languages,
regular processes and finite weighted transition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2747</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2747</id><created>2012-08-13</created><authors><author><keyname>Czerwi&#x144;ski</keyname><forenames>Wojciech</forenames><affiliation>Institute of Informatics, University of Warsaw</affiliation></author><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames><affiliation>Institute of Informatics, University of Warsaw</affiliation></author></authors><title>Partially-commutative context-free languages</title><categories>cs.FL</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 89, 2012, pp. 35-48</journal-ref><doi>10.4204/EPTCS.89.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is about a class of languages that extends context-free languages
(CFL) and is stable under shuffle. Specifically, we investigate the class of
partially-commutative context-free languages (PCCFL), where non-terminal
symbols are commutative according to a binary independence relation, very much
like in trace theory. The class has been recently proposed as a robust class
subsuming CFL and commutative CFL. This paper surveys properties of PCCFL. We
identify a natural corresponding automaton model: stateless multi-pushdown
automata. We show stability of the class under natural operations, including
homomorphic images and shuffle. Finally, we relate expressiveness of PCCFL to
two other relevant classes: CFL extended with shuffle and trace-closures of
CFL. Among technical contributions of the paper are pumping lemmas, as an
elegant completion of known pumping properties of regular languages, CFL and
commutative CFL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2748</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2748</id><created>2012-08-13</created><authors><author><keyname>Gazda</keyname><forenames>Maciej</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Willemse</keyname><forenames>Tim A. C.</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Expressiveness and Completeness in Abstraction</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 89, 2012, pp. 49-64</journal-ref><doi>10.4204/EPTCS.89.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two notions of expressiveness, which have appeared in abstraction
theory for model checking, and find them incomparable in general. In
particular, we show that according to the most widely used notion, the class of
Kripke Modal Transition Systems is strictly less expressive than the class of
Generalised Kripke Modal Transition Systems (a generalised variant of Kripke
Modal Transition Systems equipped with hypertransitions). Furthermore, we
investigate the ability of an abstraction framework to prove a formula with a
finite abstract model, a property known as completeness. We address the issue
of completeness from a general perspective: the way it depends on certain
abstraction parameters, as well as its relationship with expressiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2749</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2749</id><created>2012-08-13</created><authors><author><keyname>Giunti</keyname><forenames>Marco</forenames><affiliation>CITI and DI-FCT, Universidade Nova de Lisboa, Portugal</affiliation></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames><affiliation>INRIA Saclay and LIX, Ecole Polytechnique, France</affiliation></author><author><keyname>Valencia</keyname><forenames>Frank D.</forenames><affiliation>INRIA Saclay and LIX, Ecole Polytechnique, France</affiliation></author></authors><title>Hide and New in the Pi-Calculus</title><categories>cs.PL cs.CR</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><acm-class>D.3.2; D.3.3</acm-class><journal-ref>EPTCS 89, 2012, pp. 65-79</journal-ref><doi>10.4204/EPTCS.89.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we enrich the pi-calculus with an operator for confidentiality
(hide), whose main effect is to restrict the access to the object of the
communication, thus representing confidentiality in a natural way. The hide
operator is meant for local communication, and it differs from new in that it
forbids the extrusion of the name and hence has a static scope. Consequently, a
communication channel in the scope of a hide can be implemented as a dedicated
channel, and it is more secure than one in the scope of a new. To emphasize the
difference, we introduce a spy context that represents a side-channel attack
and breaks some of the standard security equations for new. To formally reason
on the security guarantees provided by the hide construct, we introduce an
observational theory and establish stronger equivalences by relying on a proof
technique based on bisimulation semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2750</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2750</id><created>2012-08-13</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames><affiliation>NICTA</affiliation></author></authors><title>Musings on Encodings and Expressiveness</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><acm-class>F.1.2</acm-class><journal-ref>EPTCS 89, 2012, pp. 81-98</journal-ref><doi>10.4204/EPTCS.89.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a definition of what it means for one system description
language to encode another one, thereby enabling an ordering of system
description languages with respect to expressive power. I compare the proposed
definition with other definitions of encoding and expressiveness found in the
literature, and illustrate it on a case study: comparing the expressive power
of CCS and CSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2751</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2751</id><created>2012-08-13</created><authors><author><keyname>Hofman</keyname><forenames>Piotr</forenames><affiliation>University of Warsaw</affiliation></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Approximating Weak Bisimilarity of Basic Parallel Processes</title><categories>cs.FL cs.CC cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><acm-class>F.0</acm-class><journal-ref>EPTCS 89, 2012, pp. 99-113</journal-ref><doi>10.4204/EPTCS.89.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the well known approximation approach to decide weak
bisimilarity of Basic Parallel Processes. We look into how different refinement
functions can be used to prove weak bisimilarity decidable for certain
subclasses. We also show their limitations for the general case. In particular,
we show a lower bound of {\omega} \ast {\omega} for the approximants which
allow weak steps and a lower bound of {\omega} + {\omega} for the approximants
that allow sequences of actions. The former lower bound negatively answers the
open question of Jan\v{c}ar and Hirshfeld.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2752</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2752</id><created>2012-08-13</created><authors><author><keyname>Lee</keyname><forenames>Matias David</forenames><affiliation>Famaf, UNC - Conicet</affiliation></author><author><keyname>Gebler</keyname><forenames>Daniel</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>D'Argenio</keyname><forenames>Pedro R.</forenames><affiliation>Famaf, UNC - Conicet</affiliation></author></authors><title>Tree rules in probabilistic transition system specifications with
  negative and quantitative premises</title><categories>cs.PL</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 89, 2012, pp. 115-130</journal-ref><doi>10.4204/EPTCS.89.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic transition system specifications (PTSSs) in the ntmufnu/ntmuxnu
format provide structural operational semantics for Segala-type systems that
exhibit both probabilistic and nondeterministic behavior and guarantee that
isimilarity is a congruence.Similar to the nondeterministic case of rule format
tyft/tyxt, we show that the well-foundedness requirement is unnecessary in the
probabilistic setting. To achieve this, we first define an extended version of
the ntmufnu/ntmuxnu format in which quantitative premises and conclusions
include nested convex combinations of distributions. This format also
guarantees that bisimilarity is a congruence. Then, for a given (possibly
non-well-founded) PTSS in the new format, we construct an equivalent
well-founded transition system consisting of only rules of the simpler
(well-founded) probabilistic ntree format. Furthermore, we develop a
proof-theoretic notion for these PTSSs that coincides with the existing
stratification-based meaning in case the PTSS is stratifiable. This continues
the line of research lifting structural operational semantic results from the
nondeterministic setting to systems with both probabilistic and
nondeterministic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2753</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2753</id><created>2012-08-13</created><authors><author><keyname>Mennicke</keyname><forenames>Stephan</forenames><affiliation>TU Braunschweig</affiliation></author></authors><title>An Operational Petri Net Semantics for the Join-Calculus</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 89, 2012, pp. 131-147</journal-ref><doi>10.4204/EPTCS.89.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a concurrent operational Petri net semantics for the
join-calculus, a process calculus for specifying concurrent and distributed
systems. There often is a gap between system specifications and the actual
implementations caused by synchrony assumptions on the specification side and
asynchronously interacting components in implementations. The join-calculus is
promising to reduce this gap by providing an abstract specification language
which is asynchronously distributable. Classical process semantics establish an
implicit order of actually independent actions, by means of an interleaving. So
does the semantics of the join-calculus. To capture such independent actions,
step-based semantics, e.g., as defined on Petri nets, are employed. Our Petri
net semantics for the join-calculus induces step-behavior in a natural way. We
prove our semantics behaviorally equivalent to the original join-calculus
semantics by means of a bisimulation. We discuss how join specific assumptions
influence an existing notion of distributability based on Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2754</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2754</id><created>2012-08-13</created><authors><author><keyname>Strygin</keyname><forenames>Maxim</forenames><affiliation>School of Computer Science University of Birmingham</affiliation></author><author><keyname>Thielecke</keyname><forenames>Hayo</forenames><affiliation>School of Computer Science University of Birmingham</affiliation></author></authors><title>Operational semantics for signal handling</title><categories>cs.PL cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><acm-class>D.3.1; D.3.3</acm-class><journal-ref>EPTCS 89, 2012, pp. 149-163</journal-ref><doi>10.4204/EPTCS.89.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signals are a lightweight form of interprocess communication in Unix. When a
process receives a signal, the control flow is interrupted and a previously
installed signal handler is run. Signal handling is reminiscent both of
exception handling and concurrent interleaving of processes. In this paper, we
investigate different approaches to formalizing signal handling in operational
semantics, and compare them in a series of examples. We find the big-step style
of operational semantics to be well suited to modelling signal handling. We
integrate exception handling with our big-step semantics of signal handling, by
adopting the exception convention as defined in the Definition of Standard ML.
The semantics needs to capture the complex interactions between signal handling
and exception handling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2755</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2755</id><created>2012-08-13</created><authors><author><keyname>Pighizzini</keyname><forenames>Giovanni</forenames><affiliation>Universit&#xe0; degli Studi di Milano, Italy</affiliation></author></authors><title>Two-Way Finite Automata: Old and Recent Results</title><categories>cs.FL cs.CC</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 3-20</journal-ref><doi>10.4204/EPTCS.90.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of two-way automata was introduced at the very beginning of
automata theory. In 1959, Rabin and Scott and, independently, Shepherdson,
proved that these models, both in the deterministic and in the nondeterministic
versions, have the same power of one-way automata, namely, they characterize
the class of regular languages.
  In 1978, Sakoda and Sipser posed the question of the cost, in the number of
the states, of the simulation of one-way and two-way nondeterministic automata
by two-way deterministic automata. They conjectured that these costs are
exponential. In spite of all attempts to solve it, this question is still open.
  In the last ten years the problem of Sakoda and Sipser was widely
reconsidered and many new results related to it have been obtained. In this
work we discuss some of them. In particular, we focus on the restriction to the
unary case and on the connections with open questions in space complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2756</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2756</id><created>2012-08-13</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames><affiliation>University of Turku, Finland</affiliation></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames><affiliation>University of Turku, Finland</affiliation></author></authors><title>On Derivatives and Subpattern Orders of Countable Subshifts</title><categories>cs.CC</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 23-36</journal-ref><doi>10.4204/EPTCS.90.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational and structural aspects of countable
two-dimensional SFTs and other subshifts. Our main focus is on the topological
derivatives and subpattern posets of these objects, and our main results are
constructions of two-dimensional countable subshifts with interesting
properties. We present an SFT whose iterated derivatives are maximally complex
from the computational point of view, a sofic shift whose subpattern poset
contains an infinite descending chain, a family of SFTs whose finite subpattern
posets contain arbitrary finite posets, and a natural example of an SFT with
infinite Cantor-Bendixon rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2757</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2757</id><created>2012-08-13</created><authors><author><keyname>De Menibus</keyname><forenames>Benjamin Hellouin</forenames><affiliation>Universit&#xe9; d'Aix-Marseille, France</affiliation></author><author><keyname>Sablik</keyname><forenames>Mathieu</forenames><affiliation>Universit&#xe9; d'Aix-Marseille, France</affiliation></author></authors><title>Entry times in automata with simple defect dynamics</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 97-109</journal-ref><doi>10.4204/EPTCS.90.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a simple cellular automaton with two particles of
different speeds that annihilate on contact. Following a previous work by K\r
urka et al., we study the asymptotic distribution, starting from a random
configuration, of the waiting time before a particle crosses the central column
after time n. Drawing a parallel between the behaviour of this automata on a
random initial configuration and a certain random walk, we approximate this
walk using a Brownian motion, and we obtain explicit results for a wide class
of initial measures and other automata with similar dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2758</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2758</id><created>2012-08-13</created><authors><author><keyname>Betel</keyname><forenames>Heater</forenames><affiliation>University of Ottawa, Canada</affiliation></author><author><keyname>de Oliveira</keyname><forenames>Pedro P. B.</forenames><affiliation>Universidade Presbiteriana Mackenzie, Brazil</affiliation></author><author><keyname>Flocchini</keyname><forenames>Paola</forenames><affiliation>University of Ottawa, Canada</affiliation></author></authors><title>On the Parity Problem in One-Dimensional Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 110-126</journal-ref><doi>10.4204/EPTCS.90.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the parity problem in one-dimensional, binary, circular cellular
automata: if the initial configuration contains an odd number of 1s, the
lattice should converge to all 1s; otherwise, it should converge to all 0s. It
is easy to see that the problem is ill-defined for even-sized lattices (which,
by definition, would never be able to converge to 1). We then consider only odd
lattices.
  We are interested in determining the minimal neighbourhood that allows the
problem to be solvable for any initial configuration. On the one hand, we show
that radius 2 is not sufficient, proving that there exists no radius 2 rule
that can possibly solve the parity problem from arbitrary initial
configurations. On the other hand, we design a radius 4 rule that converges
correctly for any initial configuration and we formally prove its correctness.
Whether or not there exists a radius 3 rule that solves the parity problem
remains an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2759</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2759</id><created>2012-08-13</created><authors><author><keyname>Fernique</keyname><forenames>Thomas</forenames><affiliation>CNRS and Univ. Paris 13, France</affiliation></author><author><keyname>Sablik</keyname><forenames>Mathieu</forenames><affiliation>LATP - Univ. Aix-Marseille, France</affiliation></author></authors><title>Local Rules for Computable Planar Tilings</title><categories>cs.FL cs.CC cs.DM math.CO</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 133-141</journal-ref><doi>10.4204/EPTCS.90.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aperiodic tilings are non-periodic tilings characterized by local
constraints. They play a key role in the proof of the undecidability of the
domino problem (1964) and naturally model quasicrystals (discovered in 1982). A
central question is to characterize, among a class of non-periodic tilings, the
aperiodic ones. In this paper, we answer this question for the well-studied
class of non-periodic tilings obtained by digitizing irrational vector spaces.
Namely, we prove that such tilings are aperiodic if and only if the digitized
vector spaces are computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2760</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2760</id><created>2012-08-13</created><authors><author><keyname>Morita</keyname><forenames>Kenichi</forenames><affiliation>Hiroshima University, Japan</affiliation></author></authors><title>Universality of One-Dimensional Reversible and Number-Conserving
  Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 142-150</journal-ref><doi>10.4204/EPTCS.90.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study one-dimensional reversible and number-conserving cellular automata
(RNCCA) that have both properties of reversibility and number-conservation. In
the case of 2-neighbor RNCCA, Garc\'ia-Ramos proved that every RNCCA shows
trivial behavior in the sense that all the signals in the RNCCA do not interact
each other. However, if we increase the neighborhood size, we can find many
complex RNCCAs. Here, we show that for any one-dimensional 2-neighbor
reversible partitioned CA (RPCA) with s states, we can construct a 4-neighbor
RNCCA with 4s states that simulates the former. Since it is known that there is
a computationally universal 24-state 2-neighbor RPCA, we obtain a universal
96-state 4-neighbor RNCCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2761</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2761</id><created>2012-08-13</created><authors><author><keyname>Umeo</keyname><forenames>Hiroshi</forenames><affiliation>Univ. of Osaka Electro-Communication, Japan</affiliation></author><author><keyname>Nishide</keyname><forenames>Kinuo</forenames><affiliation>Univ. of Osaka Electro-Communication, Japan</affiliation></author><author><keyname>Kubo</keyname><forenames>Keisuke</forenames><affiliation>Univ. of Osaka Electro-Communication, Japan</affiliation></author></authors><title>A Simple Optimum-Time FSSP Algorithm for Multi-Dimensional Cellular
  Automata</title><categories>cs.FL cs.DM cs.DS nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 151-165</journal-ref><doi>10.4204/EPTCS.90.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The firing squad synchronization problem (FSSP) on cellular automata has been
studied extensively for more than forty years, and a rich variety of
synchronization algorithms have been proposed for not only one-dimensional
arrays but two-dimensional arrays. In the present paper, we propose a simple
recursive-halving based optimum-time synchronization algorithm that can
synchronize any rectangle arrays of size m*n with a general at one corner in
m+n+max(m, n)-3 steps. The algorithm is a natural expansion of the well-known
FSSP algorithm proposed by Balzer [1967], Gerken [1987], and Waksman [1966] and
it can be easily expanded to three-dimensional arrays, even to
multi-dimensional arrays with a general at any position of the array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2762</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2762</id><created>2012-08-13</created><authors><author><keyname>Vielhaber</keyname><forenames>Michael</forenames><affiliation>Hochschule Bremerhaven, Germany</affiliation></author></authors><title>Computing by Temporal Order: Asynchronous Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 166-176</journal-ref><doi>10.4204/EPTCS.90.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our concern is the behaviour of the elementary cellular automata with state
set 0,1 over the cell set Z/nZ (one-dimensional finite wrap-around case), under
all possible update rules (asynchronicity).
  Over the torus Z/nZ (n&lt;= 11),we will see that the ECA with Wolfram rule 57
maps any v in F_2^n to any w in F_2^n, varying the update rule.
  We furthermore show that all even (element of the alternating group)
bijective functions on the set F_2^n = 0,...,2^n-1, can be computed by ECA57,
by iterating it a sufficient number of times with varying update rules, at
least for n &lt;= 10. We characterize the non-bijective functions computable by
asynchronous rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2763</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2763</id><created>2012-08-13</created><authors><author><keyname>Arrighi</keyname><forenames>Pablo</forenames><affiliation>Univ. de Grenoble</affiliation></author><author><keyname>Schabanel</keyname><forenames>Nicolas</forenames><affiliation>CNRS, Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Theyssier</keyname><forenames>Guillaume</forenames><affiliation>CNRS, Universit&#xe9; de Savoie</affiliation></author></authors><title>Intrinsic Simulations between Stochastic Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 208-224</journal-ref><doi>10.4204/EPTCS.90.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a simple formalism for dealing with deterministic,
non-deterministic and stochastic cellular automata in a unifying and composable
manner. Armed with this formalism, we extend the notion of intrinsic simulation
between deterministic cellular automata, to the non-deterministic and
stochastic settings. We then provide explicit tools to prove or disprove the
existence of such a simulation between two stochastic cellular automata, even
though the intrinsic simulation relation is shown to be undecidable in
dimension two and higher. The key result behind this is the caracterization of
equality of stochastic global maps by the existence of a coupling between the
random sources. We then prove that there is a universal non-deterministic
cellular automaton, but no universal stochastic cellular automaton. Yet we
provide stochastic cellular automata achieving optimal partial universality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2764</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2764</id><created>2012-08-13</created><authors><author><keyname>Grandjean</keyname><forenames>Ana&#xeb;l</forenames><affiliation>GREYC and ENS Lyon, France</affiliation></author><author><keyname>Richard</keyname><forenames>Ga&#xe9;tan</forenames><affiliation>GREYC, France</affiliation></author><author><keyname>Terrier</keyname><forenames>V&#xe9;ronique</forenames><affiliation>GREYC, France</affiliation></author></authors><title>Linear functional classes over cellular automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 177-193</journal-ref><doi>10.4204/EPTCS.90.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular automata are a discrete dynamical system which models massively
parallel computation. Much attention is devoted to computations with small time
complexity for which the parallelism may provide further possibilities. In this
paper, we investigate the ability of cellular automata related to functional
computation. We introduce several functional classes of low time complexity
which contain &quot;natural&quot; problems. We examine their inclusion relationships and
emphasize that several questions arising from this functional framework are
related to current ones coming from the recognition context. We also provide a
negative result which explicits limits on the information transmission whose
consequences go beyond the functional point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2765</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2765</id><created>2012-08-13</created><authors><author><keyname>Wacker</keyname><forenames>Simon</forenames><affiliation>Karlsruhe Institute of Technology, Germany</affiliation></author><author><keyname>Worsch</keyname><forenames>Thomas</forenames><affiliation>Karlsruhe Institute of Technology, Germany</affiliation></author></authors><title>Phase Space Invertible Asynchronous Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1;F.1.2;F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 236-254</journal-ref><doi>10.4204/EPTCS.90.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While for synchronous deterministic cellular automata there is an accepted
definition of reversibility, the situation is less clear for asynchronous
cellular automata. We first discuss a few possibilities and then investigate
what we call phase space invertible asynchronous cellular automata in more
detail. We will show that for each Turing machine there is such a cellular
automaton simulating it, and that it is decidable whether an asynchronous
cellular automaton has this property or not, even in higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2766</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2766</id><created>2012-08-13</created><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames><affiliation>Universit&#xe9; Nice Sophia Antipolis, France</affiliation></author><author><keyname>Fiorenzi</keyname><forenames>Francesca</forenames><affiliation>Universit&#xe9; Paris-Sud 11, France</affiliation></author></authors><title>Topological properties of cellular automata on trees</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1;F.1.2;F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 255-266</journal-ref><doi>10.4204/EPTCS.90.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there do not exist positively expansive cellular automata
defined on the full k-ary tree shift (for k&gt;=2). Moreover, we investigate some
topological properties of these automata and their relationships, namely
permutivity, surjectivity, preinjectivity, right-closingness and openness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2767</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2767</id><created>2012-08-13</created><authors><author><keyname>Noual</keyname><forenames>Mathilde</forenames><affiliation>LIP, ENS de Lyon, IXXI, Institut rh&#xf4;ne-alpin des syst&#xe8;mes complexes</affiliation></author><author><keyname>Regnault</keyname><forenames>Damien</forenames><affiliation>IBISC, Universit&#xe9; d'Evry - Val d'Essonne</affiliation></author><author><keyname>Sen&#xe9;</keyname><forenames>Sylvain</forenames><affiliation>IBISC, Universit&#xe9; d'Evry - Val d'Essonne, IXXI, Institut rh&#xf4;ne-alpin des syst&#xe8;mes complexes</affiliation></author></authors><title>Boolean networks synchronism sensitivity and XOR circulant networks
  convergence time</title><categories>cs.FL cs.CC cs.DM</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 37-52</journal-ref><doi>10.4204/EPTCS.90.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper are presented first results of a theoretical study on the role
of non-monotone interactions in Boolean automata networks. We propose to
analyse the contribution of non-monotony to the diversity and complexity in
their dynamical behaviours according to two axes. The first one consists in
supporting the idea that non-monotony has a peculiar influence on the
sensitivity to synchronism of such networks. It leads us to the second axis
that presents preliminary results and builds an understanding of the dynamical
behaviours, in particular concerning convergence times, of specific
non-monotone Boolean automata networks called XOR circulant networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2768</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2768</id><created>2012-08-13</created><authors><author><keyname>Kutrib</keyname><forenames>Martin</forenames><affiliation>Institut f&#xfc;r Informatik, Universit&#xe4;t Giessen, Germany</affiliation></author><author><keyname>Malcher</keyname><forenames>Andreas</forenames><affiliation>University of Giessen, Germany</affiliation></author></authors><title>Transductions Computed by One-Dimensional Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 194-207</journal-ref><doi>10.4204/EPTCS.90.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular automata are investigated towards their ability to compute
transductions, that is, to transform inputs into outputs. The families of
transductions computed are classified with regard to the time allowed to
process the input and to compute the output. Since there is a particular
interest in fast transductions, we mainly focus on the time complexities real
time and linear time. We first investigate the computational capabilities of
cellular automaton transducers by comparing them to iterative array
transducers, that is, we compare parallel input/output mode to sequential
input/output mode of massively parallel machines. By direct simulations, it
turns out that the parallel mode is not weaker than the sequential one.
Moreover, with regard to certain time complexities cellular automaton
transducers are even more powerful than iterative arrays. In the second part of
the paper, the model in question is compared with the sequential devices
single-valued finite state transducers and deterministic pushdown transducers.
It turns out that both models can be simulated by cellular automaton
transducers faster than by iterative array transducers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2769</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2769</id><created>2012-08-13</created><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>LIRMM, France</affiliation></author><author><keyname>Rolin</keyname><forenames>Nicolas</forenames><affiliation>LIP6 - ENS Cachan, France</affiliation></author></authors><title>Fixed Parameter Undecidability for Wang Tilesets</title><categories>cs.FL cs.CC</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1, F.1.2, F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 69-85</journal-ref><doi>10.4204/EPTCS.90.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding if a given set of Wang tiles admits a tiling of the plane is
decidable if the number of Wang tiles (or the number of colors) is bounded, for
a trivial reason, as there are only finitely many such tilesets. We prove
however that the tiling problem remains undecidable if the difference between
the number of tiles and the number of colors is bounded by 43.
  One of the main new tool is the concept of Wang bars, which are equivalently
inflated Wang tiles or thin polyominoes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2770</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2770</id><created>2012-08-13</created><authors><author><keyname>Dennunzio</keyname><forenames>Alberto</forenames><affiliation>Universit&#xe1; degli Studi di Milano-Bicocca, Dipartimento di Informatica Sistemistica e Comunicazione, Italy</affiliation></author><author><keyname>Di Lena</keyname><forenames>Pietro</forenames><affiliation>Universit&#xe1; degli Studi di Bologna, Dipartimento di Scienze dell'Informazione, Italy</affiliation></author><author><keyname>Margara</keyname><forenames>Luciano</forenames><affiliation>Universit&#xe1; degli Studi di Bologna, Dipartimento di Scienze dell'Informazione, Italy</affiliation></author></authors><title>Strictly Temporally Periodic Points in Cellular Automata</title><categories>cs.FL cs.CC cs.DM nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1;F.1.2;F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 225-235</journal-ref><doi>10.4204/EPTCS.90.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the set of strictly periodic points in surjective cellular automata,
i.e., the set of those configurations which are temporally periodic for a given
automaton but they not spatially periodic. This set turns out to be dense for
almost equicontinuous surjective cellular automata while it is empty for the
positively expansive ones. In the class of additive cellular automata, the set
of strictly periodic points can be either dense or empty. The latter happens if
and only if the cellular automaton is topologically transitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2771</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2771</id><created>2012-08-13</created><authors><author><keyname>Imai</keyname><forenames>Katsunobu</forenames><affiliation>Graduate School of Engineering, Hiroshima University, Japan</affiliation></author><author><keyname>Hatsuda</keyname><forenames>Takahiro</forenames><affiliation>Graduate School of Engineering, Hiroshima University, Japan</affiliation></author><author><keyname>Poupet</keyname><forenames>Victor</forenames><affiliation>Laboratoire d'Informatique Fondamentale de Marseille, Aix-Marseille University, France</affiliation></author><author><keyname>Sato</keyname><forenames>Kota</forenames><affiliation>Graduate School of Engineering, Hiroshima University, Japan</affiliation></author></authors><title>A Universal Semi-totalistic Cellular Automaton on Kite and Dart Penrose
  Tilings</title><categories>cs.FL cs.CC nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 267-278</journal-ref><doi>10.4204/EPTCS.90.21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate certain properties of semi-totalistic cellular
automata (CA) on the well known quasi-periodic kite and dart two dimensional
tiling of the plane presented by Roger Penrose. We show that, despite the
irregularity of the underlying grid, it is possible to devise a semi-totalistic
CA capable of simulating any boolean circuit on this aperiodic tiling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2773</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2773</id><created>2012-08-13</created><authors><author><keyname>Bonomi</keyname><forenames>Luca</forenames></author><author><keyname>Xiong</keyname><forenames>Li</forenames></author><author><keyname>Chen</keyname><forenames>Rui</forenames></author><author><keyname>Fung</keyname><forenames>Benjamin C. M.</forenames></author></authors><title>Privacy Preserving Record Linkage via grams Projections</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Record linkage has been extensively used in various data mining applications
involving sharing data. While the amount of available data is growing, the
concern of disclosing sensitive information poses the problem of utility vs
privacy. In this paper, we study the problem of private record linkage via
secure data transformations. In contrast to the existing techniques in this
area, we propose a novel approach that provides strong privacy guarantees under
the formal framework of differential privacy. We develop an embedding strategy
based on frequent variable length grams mined in a private way from the
original data. We also introduce personalized threshold for matching individual
records in the embedded space which achieves better linkage accuracy than the
existing global threshold approach. Compared with the state-of-the-art secure
matching schema, our approach provides formal, provable privacy guarantees and
achieves better scalability while providing comparable utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2777</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2777</id><created>2012-08-13</created><authors><author><keyname>Kim</keyname><forenames>Hyonil</forenames></author><author><keyname>Choe</keyname><forenames>Changil</forenames></author></authors><title>A Method for Selecting Noun Sense using Co-occurrence Relation in
  English-Korean Translation</title><categories>cs.CL</categories><report-no>KISU-MATH-2012-E-R-004</report-no><journal-ref>Serdica Journal of Computing, Vol.'6 No.4, 2012, pp. 401-408</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sense analysis is still critical problem in machine translation system,
especially such as English-Korean translation which the syntactical different
between source and target languages is very great. We suggest a method for
selecting the noun sense using contextual feature in English-Korean
Translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2782</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2782</id><created>2012-08-14</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>Multidimensional Web Page Evaluation Model Using Segmentation And
  Annotations</title><categories>cs.IR</categories><comments>11 Pages, 4 Figures; International Journal on Cybernetics &amp;
  Informatics (IJCI), Vol.1, No.4, August 2012. arXiv admin note: substantial
  text overlap with arXiv:1203.3613</comments><report-no>2277 - 548X</report-no><msc-class>68P20</msc-class><journal-ref>International Journal on Cybernetics &amp; Informatics (IJCI), Vol.1,
  No.4, August 2012</journal-ref><doi>10.5121/ijci.2012.1401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of web pages against a query is the pivot around which the
Information Retrieval domain revolves around. The context sensitive, semantic
evaluation of web pages is a non-trivial problem which needs to be addressed
immediately. This research work proposes a model to evaluate the web pages by
cumulating the segment scores which are computed by multidimensional evaluation
methodology. The model proposed is hybrid since it utilizes both the structural
semantics and content semantics in the evaluation process. The score of the web
page is computed in a bottom-up process by evaluating individual segment's
score through a multi-dimensional approach. The model incorporates an approach
for segment level annotation. The proposed model is prototyped for evaluation;
experiments conducted on the prototype confirm the model's efficiency in
semantic evaluation of pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2783</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2783</id><created>2012-08-14</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames><affiliation>University of Turku, Finland</affiliation></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames><affiliation>University of Turku, Finland</affiliation></author></authors><title>Topology Inspired Problems for Cellular Automata, and a Counterexample
  in Topology</title><categories>nlin.CG cs.FL math.GN</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1</acm-class><journal-ref>EPTCS 90, 2012, pp. 53-68</journal-ref><doi>10.4204/EPTCS.90.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two relatively natural topologizations of the set of all cellular
automata on a fixed alphabet. The first turns out to be rather pathological, in
that the countable space becomes neither first-countable nor sequential. Also,
reversible automata form a closed set, while surjective ones are dense. The
second topology, which is induced by a metric, is studied in more detail.
Continuity of composition (under certain restrictions) and inversion, as well
as closedness of the set of surjective automata, are proved, and some
counterexamples are given. We then generalize this space, in the sense that
every shift-invariant measure on the configuration space induces a pseudometric
on cellular automata, and study the properties of these spaces. We also
characterize the pseudometric spaces using the Besicovitch distance, and show a
connection to the first (pathological) space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2785</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2785</id><created>2012-08-14</created><authors><author><keyname>Ashok</keyname><forenames>Pradeesha</forenames></author><author><keyname>Azmi</keyname><forenames>Umair</forenames></author><author><keyname>Govindarajan</keyname><forenames>Sathish</forenames></author></authors><title>Small Strong Epsilon Nets</title><categories>cs.CG cs.DM</categories><comments>19 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P be a set of n points in $\mathbb{R}^d$. A point x is said to be a
centerpoint of P if x is contained in every convex object that contains more
than $dn\over d+1$ points of P. We call a point x a strong centerpoint for a
family of objects $\mathcal{C}$ if $x \in P$ is contained in every object $C
\in \mathcal{C}$ that contains more than a constant fraction of points of P. A
strong centerpoint does not exist even for halfspaces in $\mathbb{R}^2$. We
prove that a strong centerpoint exists for axis-parallel boxes in
$\mathbb{R}^d$ and give exact bounds. We then extend this to small strong
$\epsilon$-nets in the plane and prove upper and lower bounds for
$\epsilon_i^\mathcal{S}$ where $\mathcal{S}$ is the family of axis-parallel
rectangles, halfspaces and disks. Here $\epsilon_i^\mathcal{S}$ represents the
smallest real number in $[0,1]$ such that there exists an
$\epsilon_i^\mathcal{S}$-net of size i with respect to $\mathcal{S}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2786</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2786</id><created>2012-08-14</created><authors><author><keyname>Burnashev</keyname><forenames>M. V.</forenames></author><author><keyname>Yamamoto</keyname><forenames>H.</forenames></author></authors><title>On Reliability Function of Gaussian Channel with Noisy Feedback: Zero
  Transmission Rate</title><categories>cs.IT math.IT</categories><comments>Published in Problems of Information Transmission, vol. 48, no. 3,
  pp. 3--23, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For information transmission a discrete time channel with independent
additive Gaussian noise is used. There is also feedback channel with
independent additive Gaussian noise, and the transmitter observes without delay
all outputs of the forward channel via that feedback channel. Transmission of
nonexponential number of messages is considered and the achievable decoding
error exponent for such a combination of channels is investigated. It is shown
that for any finite noise in the feedback channel the achievable error exponent
is better than similar error exponent of the no-feedback channel. Method of
transmission/decoding used in the paper strengthens the earlier method used by
authors for BSC. In particular, for small feedback noise, it allows to get the
gain of 23.6% (instead of 14.3% earlier for BSC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2787</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2787</id><created>2012-08-14</created><updated>2013-01-21</updated><authors><author><keyname>Hu</keyname><forenames>Yuchong</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author><author><keyname>Shum</keyname><forenames>Kenneth W.</forenames></author></authors><title>Analysis and Construction of Functional Regenerating Codes with Uncoded
  Repair for Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>9 pages; IEEE INFOCOM (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern distributed storage systems apply redundancy coding techniques to
stored data. One form of redundancy is based on regenerating codes, which can
minimize the repair bandwidth, i.e., the amount of data transferred when
repairing a failed storage node. Existing regenerating codes mainly require
surviving storage nodes encode data during repair. In this paper, we study
functional minimum storage regenerating (FMSR) codes, which enable uncoded
repair without the encoding requirement in surviving nodes, while preserving
the minimum repair bandwidth guarantees and also minimizing disk reads. Under
double-fault tolerance settings, we formally prove the existence of FMSR codes,
and provide a deterministic FMSR code construction that can significantly speed
up the repair process. We further implement and evaluate our deterministic FMSR
codes to show the benefits. Our work is built atop a practical cloud storage
system that implements FMSR codes, and we provide theoretical validation to
justify the practicality of FMSR codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2808</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2808</id><created>2012-08-14</created><authors><author><keyname>Nandy</keyname><forenames>Sudarshan</forenames></author><author><keyname>Sarkar</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author></authors><title>Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster
  crawling</title><categories>cs.LG cs.IR</categories><comments>14 Pages, 7 Figure</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.4, July 2012, 117-130</journal-ref><doi>10.5121/ijaia.2012.3409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of world-wide-web (WWW) spreads its wings from an intangible
quantities of web-pages to a gigantic hub of web information which gradually
increases the complexity of crawling process in a search engine. A search
engine handles a lot of queries from various parts of this world, and the
answers of it solely depend on the knowledge that it gathers by means of
crawling. The information sharing becomes a most common habit of the society,
and it is done by means of publishing structured, semi-structured and
unstructured resources on the web. This social practice leads to an exponential
growth of web-resource, and hence it became essential to crawl for continuous
updating of web-knowledge and modification of several existing resources in any
situation. In this paper one statistical hypothesis based learning mechanism is
incorporated for learning the behavior of crawling speed in different
environment of network, and for intelligently control of the speed of crawler.
The scaling technique is used to compare the performance proposed method with
the standard crawler. The high speed performance is observed after scaling, and
the retrieval of relevant web-resource in such a high speed is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2825</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2825</id><created>2012-08-14</created><updated>2013-01-02</updated><authors><author><keyname>Reynaud</keyname><forenames>Laurent</forenames></author><author><keyname>Kandeepan</keyname><forenames>Sithamparanathan</forenames></author><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author></authors><title>Adaptive Energy Efficient Communications for Hybrid Aerial-Terrestrial
  Systems</title><categories>cs.NI</categories><comments>This paper has been withdrawn</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aerial telecommunications networks based on Low Altitude Platforms (LAPs) are
expected to optimally meet the urgent needs of emergency relief and recovery
operations for tackling large scale natural disasters. The energy efficient
operation of such networks is important given the fact that the entire network
infrastructure including the battery operated ground terminals, exhibit
requirements to operate under power constrained situations. In this paper, we
propose and evaluate a real-time adaptive transmission strategy for dynamic
selection of direct and cooperative links based on the channel conditions for
improved energy efficiency. We show that the cooperation between mobile
terrestrial terminals on the ground could improve the energy efficiency in the
uplink depending on the temporal behavior of the terrestrial and the aerial
uplink channels. The simulation analysis corroborates that the adaptive
transmission technique improves the overall energy efficiency of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2832</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2832</id><created>2012-08-14</created><authors><author><keyname>Yakhontov</keyname><forenames>Sergey V.</forenames></author></authors><title>Time- and space-efficient evaluation of the complex exponential function
  using series expansion</title><categories>cs.DS</categories><journal-ref>Vestnik St.Peterburg University. Ser. 10. 2011. Issue 4. P.
  105-118</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for the evaluation of the complex exponential function is
proposed which is quasi-linear in time and linear in space. This algorithm is
based on a modified binary splitting method for the hypergeometric series and a
modified Karatsuba method for the fast evaluation of the exponential function.
The time complexity of this algorithm is equal to that of the ordinary
algorithm for the evaluation of the exponential function based on the series
expansion: O(M(n)log(n)^2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2846</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2846</id><created>2012-08-14</created><updated>2012-08-15</updated><authors><author><keyname>Brody</keyname><forenames>Joshua</forenames></author><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author></authors><title>Adapt or Die: Polynomial Lower Bounds for Non-Adaptive Dynamic Data
  Structures</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the role non-adaptivity plays in maintaining dynamic
data structures. Roughly speaking, a data structure is non-adaptive if the
memory locations it reads and/or writes when processing a query or update
depend only on the query or update and not on the contents of previously read
cells. We study such non-adaptive data structures in the cell probe model. This
model is one of the least restrictive lower bound models and in particular,
cell probe lower bounds apply to data structures developed in the popular
word-RAM model. Unfortunately, this generality comes at a high cost: the
highest lower bound proved for any data structure problem is only
polylogarithmic. Our main result is to demonstrate that one can in fact obtain
polynomial cell probe lower bounds for non-adaptive data structures.
  To shed more light on the seemingly inherent polylogarithmic lower bound
barrier, we study several different notions of non-adaptivity and identify key
properties that must be dealt with if we are to prove polynomial lower bounds
without restrictions on the data structures.
  Finally, our results also unveil an interesting connection between data
structures and depth-2 circuits. This allows us to translate conjectured hard
data structure problems into good candidates for high circuit lower bounds; in
particular, in the area of linear circuits for linear operators. Building on
lower bound proofs for data structures in slightly more restrictive models, we
also present a number of properties of linear operators which we believe are
worth investigating in the realm of circuit lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2847</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2847</id><created>2012-08-14</created><updated>2013-01-24</updated><authors><author><keyname>Cibulka</keyname><forenames>Josef</forenames></author></authors><title>Maximum size of reverse-free sets of permutations</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>05D05</msc-class><journal-ref>SIAM J. Discrete Math., 27(1), 232-239, 2013</journal-ref><doi>10.1137/120888168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two words have a reverse if they have the same pair of distinct letters on
the same pair of positions, but in reversed order. A set of words no two of
which have a reverse is said to be reverse-free. Let F(n,k) be the maximum size
of a reverse-free set of words from [n]^k where no letter repeats within a
word. We show the following lower and upper bounds in the case n &gt;= k: F(n,k)
\in n^k k^{-k/2 + O(k/log k)}. As a consequence of the lower bound, a set of
n-permutations each two having a reverse has size at most n^{n/2 + O(n/log n)}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2849</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2849</id><created>2012-08-14</created><authors><author><keyname>Chakaravarthy</keyname><forenames>Venkatesan T.</forenames></author><author><keyname>Katta</keyname><forenames>Naga Praveen Kumar</forenames></author><author><keyname>Kedia</keyname><forenames>Monu</forenames></author><author><keyname>Rajamony</keyname><forenames>Ramakrishnan</forenames></author><author><keyname>Ramanan</keyname><forenames>Aruna</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author></authors><title>Mapping Strategies for the PERCS Architecture</title><categories>cs.DC</categories><comments>Full version of paper at HiPC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PERCS system was designed by IBM in response to a DARPA challenge that
called for a high-productivity high-performance computing system. The IBM PERCS
architecture is a two level direct network having low diameter and high
bisection bandwidth. Mapping and routing strategies play an important role in
the performance of applications on such a topology. In this paper, we study
mapping strategies for PERCS architecture, that examine how to map tasks of a
given job on to the physical processing nodes. We develop and present
fundamental principles for designing good mapping strategies that minimize
congestion. This is achieved via a theoretical study of some common
communication patterns under both direct and indirect routing mechanisms
supported by the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2852</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2852</id><created>2012-08-14</created><updated>2014-10-27</updated><authors><author><keyname>Lai</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author></authors><title>Ordered {AND, OR}-Decomposition and Binary-Decision Diagram</title><categories>cs.AI cs.LO</categories><comments>The authors have resubmitted a new version with a different tittle,
  arXiv:1410.6671</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the context of knowledge compilation (KC), we study the effect of
augmenting Ordered Binary Decision Diagrams (OBDD) with two kinds of
decomposition nodes, i.e., AND-vertices and OR-vertices which denote
conjunctive and disjunctive decomposition of propositional knowledge bases,
respectively. The resulting knowledge compilation language is called Ordered
{AND, OR}-decomposition and binary-Decision Diagram (OAODD). Roughly speaking,
several previous languages can be seen as special types of OAODD, including
OBDD, AND/OR Binary Decision Diagram (AOBDD), OBDD with implied Literals
(OBDD-L), Multi-Level Decomposition Diagrams (MLDD). On the one hand, we
propose some families of algorithms which can convert some fragments of OAODD
into others; on the other hand, we present a rich set of polynomial-time
algorithms that perform logical operations. According to these algorithms, as
well as theoretical analysis, we characterize the space efficiency and
tractability of OAODD and its some fragments with respect to the evaluating
criteria in the KC map. Finally, we present a compilation algorithm which can
convert formulas in negative normal form into OAODD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2853</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2853</id><created>2012-08-14</created><updated>2012-12-20</updated><authors><author><keyname>Fedrizzi</keyname><forenames>Riccardo</forenames></author><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Kandeepan</keyname><forenames>Sithamparanathan</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author><author><keyname>Saradhi</keyname><forenames>Chava Vijaya</forenames></author></authors><title>Energy Aware Routing in Heterogeneous Multi-Hop Wireless Networks</title><categories>cs.NI</categories><comments>to be re-submited later after extending the article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous wireless networks with wireless devices supporting multitude of
radio access technologies are witnessing increasing interest from network
providers and consumers alike. Energy efficiency in such networks has become an
important design consideration due to the limited battery life of mobile
terminals on one side, and the ever increasing operational expenses pertaining
to energy expenditure on the other. In this paper, we present a routing
protocol for multi-radio multi-hop wireless networks, which aims to achieve a
trade-off between energy consumption in the network and routing delay,
considering both the energy consumption at the devices and the link energy
costs. We also present optimum route-path selection strategies by defining a
utility function to minimize the energy consumption in the network while
maximizing the network lifetime. Using simulations, we verify the utility of
the route-path selection strategies and the efficiency of the energy aware
routing algorithm. It turns out that the proposed protocol is energy efficient
in terms of path selection, with a slight compromise in the end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2856</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2856</id><created>2012-08-14</created><authors><author><keyname>Madill</keyname><forenames>Blake</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>The abelian complexity of the paperfolding word</title><categories>math.CO cs.FL</categories><comments>14 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the abelian complexity function of the ordinary paperfolding
word is a 2-regular sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2861</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2861</id><created>2012-08-14</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Jankowski</keyname><forenames>Bartosz</forenames></author></authors><title>Towards Steganography Detection Through Network Traffic Visualisation</title><categories>cs.CR</categories><comments>8 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents initial step toward new network anomaly detection method
that is based on traffic visualisation. The key design principle of the
proposed approach is the lack of direct, linear time dependencies for the
created network traffic visualisations. The method's feasibility is
demonstrated in network steganography environment by presenting steg-tomography
methodology and developing the dedicated visualisation tool. To authors' best
knowledge this is the first utilization of network traffic visualisations for
steganalysis purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2870</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2870</id><created>2012-08-14</created><authors><author><keyname>Rizk</keyname><forenames>Amr</forenames></author><author><keyname>Bozakov</keyname><forenames>Zdravko</forenames></author><author><keyname>Fidler</keyname><forenames>Markus</forenames></author></authors><title>H-Probe: Estimating Traffic Correlations from Sampling and Active
  Network Probing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extensive body of research deals with estimating the correlation and the
Hurst parameter of Internet traffic traces. The significance of these
statistics is due to their fundamental impact on network performance. The
coverage of Internet traffic traces is, however, limited since acquiring such
traces is challenging with respect to, e.g., confidentiality, logging speed,
and storage capacity. In this work, we investigate how the correlation of
Internet traffic can be reliably estimated from random traffic samples. These
samples are observed either by passive monitoring within the network, or
otherwise by active packet probes at end systems. We analyze random sampling
processes with different inter-sample distributions and show how to obtain
asymptotically unbiased estimates from these samples. We quantify the inherent
limitations that are due to limited observations and explore the influence of
various parameters, such as sampling intensity, network utilization, or Hurst
parameter on the estimation accuracy. We design an active probing method which
enables simple and lightweight traffic sampling without support from the
network. We verify our approach in a controlled network environment and present
comprehensive Internet measurements. We find that the correlation exhibits
properties such as long range dependence as well as periodicities and that it
differs significantly across Internet paths and observation times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2873</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2873</id><created>2012-08-13</created><authors><author><keyname>Lampos</keyname><forenames>Vasileios</forenames></author></authors><title>Detecting Events and Patterns in Large-Scale User Generated Textual
  Streams with Statistical Learning Methods</title><categories>cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML</categories><comments>PhD thesis, 238 pages, 9 chapters, 2 appendices, 58 figures, 49
  tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A vast amount of textual web streams is influenced by events or phenomena
emerging in the real world. The social web forms an excellent modern paradigm,
where unstructured user generated content is published on a regular basis and
in most occasions is freely distributed. The present Ph.D. Thesis deals with
the problem of inferring information - or patterns in general - about events
emerging in real life based on the contents of this textual stream. We show
that it is possible to extract valuable information about social phenomena,
such as an epidemic or even rainfall rates, by automatic analysis of the
content published in Social Media, and in particular Twitter, using Statistical
Machine Learning methods. An important intermediate task regards the formation
and identification of features which characterise a target event; we select and
use those textual features in several linear, non-linear and hybrid inference
approaches achieving a significantly good performance in terms of the applied
loss function. By examining further this rich data set, we also propose methods
for extracting various types of mood signals revealing how affective norms - at
least within the social web's population - evolve during the day and how
significant events emerging in the real world are influencing them. Lastly, we
present some preliminary findings showing several spatiotemporal
characteristics of this textual information as well as the potential of using
it to tackle tasks such as the prediction of voting intentions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2877</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2877</id><created>2012-08-14</created><authors><author><keyname>Wagener</keyname><forenames>Gerard</forenames></author><author><keyname>Dulaunoy</keyname><forenames>Alexandre</forenames></author><author><keyname>State</keyname><forenames>Radu</forenames></author></authors><title>Torinj : Automated Exploitation Malware Targeting Tor Users</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose in this paper a new propagation vector for malicious software by
abusing the Tor network. Tor is particularly relevant, since operating a Tor
exit node is easy and involves low costs compared to attack institutional or
ISP networks. After presenting the Tor network from an attacker perspective, we
describe an automated exploitation malware which is operated on a Tor exit node
targeting to infect web browsers. Our experiments show that the current
deployed Tor network, provides a large amount of potential victims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2896</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2896</id><created>2012-08-14</created><authors><author><keyname>Battey</keyname><forenames>Matthew</forenames></author><author><keyname>Parakh</keyname><forenames>Abhishek</forenames></author></authors><title>Efficient Quasigroup Block Cipher for Sensor Networks</title><categories>cs.CR</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new quasigroup based block encryption system with and without
cipher-block-chaining. We compare its performance against Advanced Encryption
Standard-256 (AES256) bit algorithm using the NIST statistical test suite
(NIST-STS) that tests for randomness of a sequence. Since it is well known that
a good encryption algorithm must destroy any statistical properties of the
input sequence and produce an output close to a true random sequence, the
NIST-STS suite results provide a good test bench. In almost all tests from the
suite the proposed algorithm performs better than AES256.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2900</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2900</id><created>2012-08-14</created><updated>2012-08-23</updated><authors><author><keyname>Yang</keyname><forenames>Lu</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>On Achievable Degrees of Freedom for MIMO X Channels</title><categories>cs.IT math.IT</categories><comments>18 pages 6 figures, minor revisions in Abstract and Introduction of
  version 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the achievable DoF of MIMO X channels for constant channel
coefficients with $M_t$ antennas at transmitter $t$ and $N_r$ antennas at
receiver $r$ ($t,r=1,2$) is studied. A spatial interference alignment and
cancelation scheme is proposed to achieve the maximum DoF of the MIMO X
channels. The scenario of $M_1\geq M_2\geq N_1\geq N_2$ is first considered and
divided into 3 cases, $3N_2&lt;M_1+M_2&lt;2N_1+N_2$ (Case $A$), $M_1+M_2\geq2N_1+N_2$
(Case $B$), and $M_1+M_2\leq3N_2$ (Case $C$). With the proposed scheme, it is
shown that in Case $A$, the outer-bound $\frac{M_1+M_2+N_2}{2}$ is achievable;
in Case $B$, the achievable DoF equals the outer-bound $N_1+N_2$ if $M_2&gt;N_1$,
otherwise it is 1/2 or 1 less than the outer-bound; in Case $C$, the achievable
DoF is equal to the outer-bound $2/3(M_1+M_2)$ if $(3N_2-M_1-M_2)\mod 3=0$, and
it is 1/3 or 1/6 less than the outer-bound if $(3N_2-M_1-M_2)\mod 3=1
\mathrm{or} 2$. In the scenario of $M_t\leq N_r$, the exact symmetrical results
of DoF can be obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2908</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2908</id><created>2012-08-14</created><updated>2014-03-19</updated><authors><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Habich</keyname><forenames>Johannes</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Exploring performance and power properties of modern multicore chips via
  simple machine models</title><categories>cs.PF cs.DC</categories><comments>23 pages, 10 figures. Typos corrected, DOI added</comments><doi>10.1002/cpe.3180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern multicore chips show complex behavior with respect to performance and
power. Starting with the Intel Sandy Bridge processor, it has become possible
to directly measure the power dissipation of a CPU chip and correlate this data
with the performance properties of the running code. Going beyond a simple
bottleneck analysis, we employ the recently published Execution-Cache-Memory
(ECM) model to describe the single- and multi-core performance of streaming
kernels. The model refines the well-known roofline model, since it can predict
the scaling and the saturation behavior of bandwidth-limited loop kernels on a
multicore chip. The saturation point is especially relevant for considerations
of energy consumption. From power dissipation measurements of benchmark
programs with vastly different requirements to the hardware, we derive a
simple, phenomenological power model for the Sandy Bridge processor. Together
with the ECM model, we are able to explain many peculiarities in the
performance and power behavior of multicore processors, and derive guidelines
for energy-efficient execution of parallel programs. Finally, we show that the
ECM and power models can be successfully used to describe the scaling and power
behavior of a lattice-Boltzmann flow solver code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2920</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2920</id><created>2012-08-14</created><updated>2014-01-16</updated><authors><author><keyname>Friesen</keyname><forenames>Mirjam</forenames></author><author><keyname>Hamed</keyname><forenames>Aya</forenames></author><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Theis</keyname><forenames>Dirk Oliver</forenames></author></authors><title>Fooling sets and rank</title><categories>math.CO cs.CC</categories><comments>10 pages. Now resolves the open problem also in characteristic 0</comments><msc-class>15B35, 05C70, 15B34, 94A05, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $n\times n$ matrix $M$ is called a \textit{fooling-set matrix of size $n$}
if its diagonal entries are nonzero and $M_{k,\ell} M_{\ell,k} = 0$ for every
$k\ne \ell$. Dietzfelbinger, Hromkovi{\v{c}}, and Schnitger (1996) showed that
$n \le (\mbox{rk} M)^2$, regardless of over which field the rank is computed,
and asked whether the exponent on $\mbox{rk} M$ can be improved.
  We settle this question. In characteristic zero, we construct an infinite
family of rational fooling-set matrices with size $n = \binom{\mbox{rk}
M+1}{2}$. In nonzero characteristic, we construct an infinite family of
matrices with $n= (1+o(1))(\mbox{rk} M)^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2921</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2921</id><created>2012-08-14</created><updated>2012-10-03</updated><authors><author><keyname>Osherson</keyname><forenames>Daniel</forenames></author><author><keyname>Weinstein</keyname><forenames>Scott</forenames></author></authors><title>Quantified preference logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logic of reason-based preference advanced in Osherson and Weinstein
(2012) is extended to quantifiers. Basic properties of the new system are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2925</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2925</id><created>2012-08-14</created><authors><author><keyname>Cheung</keyname><forenames>Alvin</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author></authors><title>Using Program Synthesis for Social Recommendations</title><categories>cs.LG cs.DB cs.PL cs.SI physics.soc-ph</categories><report-no>MIT-CSAIL-TR-2012-025</report-no><acm-class>H.2; I.2.2; H.2.8; D.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach to select events of interest to a user in
a social media setting where events are generated by the activities of the
user's friends through their mobile devices. We argue that given the unique
requirements of the social media setting, the problem is best viewed as an
inductive learning problem, where the goal is to first generalize from the
users' expressed &quot;likes&quot; and &quot;dislikes&quot; of specific events, then to produce a
program that can be manipulated by the system and distributed to the collection
devices to collect only data of interest. The key contribution of this paper is
a new algorithm that combines existing machine learning techniques with new
program synthesis technology to learn users' preferences. We show that when
compared with the more standard approaches, our new algorithm provides up to
order-of-magnitude reductions in model training time, and significantly higher
prediction accuracies for our target application. The approach also improves on
standard machine learning techniques in that it produces clear programs that
can be manipulated to optimize data collection and filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2936</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2936</id><created>2012-08-14</created><authors><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Forwarding Without Repeating: Efficient Rumor Spreading in
  Bounded-Degree Graphs</title><categories>cs.DC cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a gossip protocol called forwarding without repeating (FWR). The
objective is to spread multiple rumors over a graph as efficiently as possible.
FWR accomplishes this by having nodes record which messages they have forwarded
to each neighbor, so that each message is forwarded at most once to each
neighbor. We prove that FWR spreads a rumor over a strongly connected digraph,
with high probability, in time which is within a constant factor of optimal for
digraphs with bounded out-degree. Moreover, on digraphs with bounded out-degree
and bounded number of rumors, the number of transmissions required by FWR is
arbitrarily better than that of existing approaches. Specifically, FWR requires
O(n) messages on bounded-degree graphs with n nodes, whereas classical
forwarding and an approach based on network coding both require {\omega}(n)
messages. Our results are obtained using combinatorial and probabilistic
arguments. Notably, they do not depend on expansion properties of the
underlying graph, and consequently the message complexity of FWR is arbitrarily
better than classical forwarding even on constant-degree expander graphs, as n
\rightarrow \infty. In resource-constrained applications, where each
transmission consumes battery power and bandwidth, our results suggest that
using a small amount of memory at each node leads to a significant savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2943</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2943</id><created>2012-08-14</created><updated>2013-10-01</updated><authors><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>A differential Lyapunov framework for contraction analysis</title><categories>cs.SY math.DG math.DS</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lyapunov's second theorem is an essential tool for stability analysis of
differential equations. The paper provides an analog theorem for incremental
stability analysis by lifting the Lyapunov function to the tangent bundle. The
Lyapunov function endows the state-space with a Finsler structure. Incremental
stability is inferred from infinitesimal contraction of the Finsler metrics
through integration along solutions curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2955</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2955</id><created>2012-08-14</created><updated>2013-12-31</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Enumerable Distributions, Randomness, Dependence</title><categories>cs.CC</categories><comments>7 pages, some extensions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kolmogorov-Martin-Lof Randomness concept is extended from computable to
enumerable distributions. This allows definitions of various other properties,
such as mutual information in infinite sequences. Enumerable distributions (as
well as distributions faced in some finite multi-party settings) are
semimeasures; handling those requires some amount of care.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2956</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2956</id><created>2012-08-14</created><updated>2013-06-21</updated><authors><author><keyname>Campagna</keyname><forenames>Andrea</forenames></author><author><keyname>Guo</keyname><forenames>Alan</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>Local reconstructors and tolerant testers for connectivity and diameter</title><categories>cs.DS</categories><comments>21 pages, updated abstract, improved exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A local property reconstructor for a graph property is an algorithm which,
given oracle access to the adjacency list of a graph that is &quot;close&quot; to having
the property, provides oracle access to the adjacency matrix of a &quot;correction&quot;
of the graph, i.e. a graph which has the property and is close to the given
graph. For this model, we achieve local property reconstructors for the
properties of connectivity and $k$-connectivity in undirected graphs, and the
property of strong connectivity in directed graphs. Along the way, we present a
method of transforming a local reconstructor (which acts as a &quot;adjacency matrix
oracle&quot; for the corrected graph) into an &quot;adjacency list oracle&quot;. This allows
us to recursively use our local reconstructor for $(k-1)$-connectivity to
obtain a local reconstructor for $k$-connectivity.
  We also extend this notion of local property reconstruction to parametrized
graph properties (for instance, having diameter at most $D$ for some parameter
$D$) and require that the corrected graph has the property with parameter close
to the original. We obtain a local reconstructor for the low diameter property,
where if the original graph is close to having diameter $D$, then the corrected
graph has diameter roughly 2D.
  We also exploit a connection between local property reconstruction and
property testing, observed by Brakerski, to obtain new tolerant property
testers for all of the aforementioned properties. Except for the one for
connectivity, these are the first tolerant property testers for these
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2972</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2972</id><created>2012-08-14</created><authors><author><keyname>Kasparick</keyname><forenames>Martin</forenames></author><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author></authors><title>Wireless Network Design Under Service Constraints</title><categories>cs.SY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the design of wireless queueing network control
policies with special focus on application-dependent service constraints. In
particular we consider streaming traffic induced requirements such as avoiding
buffer underflows, which significantly complicate the control problem compared
to guaranteeing throughput optimality only. Since state-of-the-art approaches
for enforcing minimum buffer constraints in broadcast networks are not suitable
for application in general networks we argue for a cost function based
approach, which combines throughput optimality with flexibility regarding
service constraints. New theoretical stability results are presented and
various candidate cost functions are investigated concerning their suitability
for use in wireless networks with streaming media traffic. Furthermore we show
how the cost function based approach can be used to aid wireless network design
with respect to important system parameters. The performance is demonstrated
using numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.2976</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.2976</id><created>2012-08-14</created><authors><author><keyname>Takahashi</keyname><forenames>Daniel Yasumasa</forenames></author><author><keyname>Sato</keyname><forenames>Jo&#xe3;o Ricardo</forenames></author><author><keyname>Ferreira</keyname><forenames>Carlos Eduardo</forenames></author><author><keyname>Fujita</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Discriminating different classes of biological networks by analyzing the
  graphs spectra distribution</title><categories>stat.ME cs.SI physics.soc-ph q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The brain's structural and functional systems, protein-protein interaction,
and gene networks are examples of biological systems that share some features
of complex networks, such as highly connected nodes, modularity, and
small-world topology. Recent studies indicate that some pathologies present
topological network alterations relative to norms seen in the general
population. Therefore, methods to discriminate the processes that generate the
different classes of networks (e.g., normal and disease) might be crucial for
the diagnosis, prognosis, and treatment of the disease. It is known that
several topological properties of a network (graph) can be described by the
distribution of the spectrum of its adjacency matrix. Moreover, large networks
generated by the same random process have the same spectrum distribution,
allowing us to use it as a &quot;fingerprint&quot;. Based on this relationship, we
introduce and propose the entropy of a graph spectrum to measure the
&quot;uncertainty&quot; of a random graph and the Kullback-Leibler and Jensen-Shannon
divergences between graph spectra to compare networks. We also introduce
general methods for model selection and network model parameter estimation, as
well as a statistical procedure to test the nullity of divergence between two
classes of complex networks. Finally, we demonstrate the usefulness of the
proposed methods by applying them on (1) protein-protein interaction networks
of different species and (2) on networks derived from children diagnosed with
Attention Deficit Hyperactivity Disorder (ADHD) and typically developing
children. We conclude that scale-free networks best describe all the
protein-protein interactions. Also, we show that our proposed measures
succeeded in the identification of topological changes in the network while
other commonly used measures (number of edges, clustering coefficient, average
path length) failed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3001</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3001</id><created>2012-08-14</created><authors><author><keyname>Chen</keyname><forenames>Zhili</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Meng</keyname><forenames>Peng</forenames></author><author><keyname>Miao</keyname><forenames>Haibo</forenames></author></authors><title>More than Word Frequencies: Authorship Attribution via Natural Frequency
  Zoned Word Distribution Analysis</title><categories>cs.CL</categories><comments>27pages, 7figures, submited to Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With such increasing popularity and availability of digital text data,
authorships of digital texts can not be taken for granted due to the ease of
copying and parsing. This paper presents a new text style analysis called
natural frequency zoned word distribution analysis (NFZ-WDA), and then a basic
authorship attribution scheme and an open authorship attribution scheme for
digital texts based on the analysis. NFZ-WDA is based on the observation that
all authors leave distinct intrinsic word usage traces on texts written by them
and these intrinsic styles can be identified and employed to analyze the
authorship. The intrinsic word usage styles can be estimated through the
analysis of word distribution within a text, which is more than normal word
frequency analysis and can be expressed as: which groups of words are used in
the text; how frequently does each group of words occur; how are the
occurrences of each group of words distributed in the text. Next, the basic
authorship attribution scheme and the open authorship attribution scheme
provide solutions for both closed and open authorship attribution problems.
Through analysis and extensive experimental studies, this paper demonstrates
the efficiency of the proposed method for authorship attribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3015</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3015</id><created>2012-08-14</created><updated>2012-09-10</updated><authors><author><keyname>Schutt</keyname><forenames>Andreas</forenames></author><author><keyname>Feydy</keyname><forenames>Thibaut</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Explaining Time-Table-Edge-Finding Propagation for the Cumulative
  Resource Constraint</title><categories>cs.AI</categories><comments>22 pages, 3 figures, 11 tables, 2 algorithms</comments><acm-class>F.4.1; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cumulative resource constraints can model scarce resources in scheduling
problems or a dimension in packing and cutting problems. In order to
efficiently solve such problems with a constraint programming solver, it is
important to have strong and fast propagators for cumulative resource
constraints. One such propagator is the recently developed
time-table-edge-finding propagator, which considers the current resource
profile during the edge-finding propagation. Recently, lazy clause generation
solvers, i.e. constraint programming solvers incorporating nogood learning,
have proved to be excellent at solving scheduling and cutting problems. For
such solvers, concise and accurate explanations of the reasons for propagation
are essential for strong nogood learning. In this paper, we develop the first
explaining version of time-table-edge-finding propagation and show preliminary
results on resource-constrained project scheduling problems from various
standard benchmark suites. On the standard benchmark suite PSPLib, we were able
to close one open instance and to improve the lower bound of about 60% of the
remaining open instances. Moreover, 6 of those instances were closed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3017</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3017</id><created>2012-08-14</created><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Expurgation Exponent of Leaked Information in Privacy Amplification for
  Binary Sources</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, to be presented at IEEE Information Theory
  Workshop (ITW) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the privacy amplification problem in which Eve can observe the
uniform binary source through a binary erasure channel (BEC) or a binary
symmetric channel (BSC). For this problem, we derive the so-called expurgation
exponent of the information leaked to Eve. The exponent is derived by relating
the leaked information to the error probability of the linear code that is
generated by the linear hash function used in the privacy amplification, which
is also interesting in its own right. The derived exponent is larger than
state-of-the-art exponent recently derived by Hayashi at low rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3022</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3022</id><created>2012-08-15</created><authors><author><keyname>Saboori</keyname><forenames>Ehsan</forenames></author><author><keyname>Rafigh</keyname><forenames>Majid</forenames></author><author><keyname>Nooriyan</keyname><forenames>Alireza</forenames></author></authors><title>Analyzing the Dual-Path Peer-to-Peer Anonymous Approach</title><categories>cs.NI</categories><comments>7 Pages. Published by Foundation of Computer Science, New York, USA</comments><journal-ref>International Journal of Computer Applications 37(11):16-22,
  January 2012</journal-ref><doi>10.5120/4729-6919</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual-Path is an anonymous peer-to-peer approach which provides requester
anonymity. This approach provides anonymity between a requester and a provider
in peer-to-peer networks with trusted servers called suppernode so the provider
will not be able to identify the requester and no other peers can identify the
two communicating parties with certainty. Dual-Path establishes two paths for
transmitting data. These paths called Request path and Response path. The first
one is used for requesting data and the second one is used for sending the
requested data to the requester. As Dual-Path approach is similar to Crowds
approach, this article compares reliability and performance of Dual-Path and
Crowds. For this purpose a simulator is developed and several scenarios are
defined to compare Dual-Path and Crowds in different situations. In chapter 2
and 3 Dual-Path and Crowds approaches are briefly described. Chapter 4 is
talking about simulator. Chapter 5 explains the scenarios for comparison of
performance. Chapter 6 is about comparison of reliability and chapter 7 is
conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3024</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3024</id><created>2012-08-15</created><updated>2013-05-25</updated><authors><author><keyname>Zhou</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Uplink Multicell Processing with Limited Backhaul via Per-Base-Station
  Successive Interference Cancellation</title><categories>cs.IT math.IT</categories><comments>JSAC Oct 2013, special issue on VMIMO</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper studies an uplink multicell joint processing model in which the
base-stations are connected to a centralized processing server via rate-limited
digital backhaul links. Unlike previous studies where the centralized processor
jointly decodes all the source messages from all base-stations, this paper
proposes a suboptimal achievability scheme in which the Wyner-Ziv
compress-and-forward relaying technique is employed on a per-base-station
basis, but successive interference cancellation (SIC) is used at the central
processor to mitigate multicell interference. This results in an achievable
rate region that is easily computable, in contrast to the joint processing
schemes in which the rate regions can only be characterized by exponential
number of rate constraints. Under the per-base-station SIC framework, this
paper further studies the impact of the limited-capacity backhaul links on the
achievable rates and establishes that in order to achieve to within constant
number of bits to the maximal SIC rate with infinite-capacity backhaul, the
backhaul capacity must scale logarithmically with the
signal-to-interference-and-noise ratio (SINR) at each base-station. Finally,
this paper studies the optimal backhaul rate allocation problem for an uplink
multicell joint processing model with a total backhaul capacity constraint. The
analysis reveals that the optimal strategy that maximizes the overall sum rate
should also scale with the log of the SINR at each base-station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3029</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3029</id><created>2012-08-15</created><authors><author><keyname>Wu</keyname><forenames>Huasen</forenames></author><author><keyname>Zhu</keyname><forenames>Chenxi</forenames></author><author><keyname>La</keyname><forenames>Richard J.</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Zhang</keyname><forenames>Youguang</forenames></author></authors><title>Fast Adaptive S-ALOHA Scheme for Event-driven M2M Communications
  (Journal version)</title><categories>cs.IT math.IT</categories><comments>15 pages, 6 figures, submitted to IEEE/ACM Trans. on Networking</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Supporting massive device transmission is challenging in Machine-to-Machine
(M2M) communications. Particularly, in event-driven M2M communications, a large
number of devices activate within a short period of time, which in turn causes
high radio congestions and severe access delay. To address this issue, we
propose a Fast Adaptive S-ALOHA (FASA) scheme for random access control of M2M
communication systems with bursty traffic. Instead of the observation in a
single slot, the statistics of consecutive idle and collision slots are used in
FASA to accelerate the tracking process of network status which is critical for
optimizing S-ALOHA systems. Using drift analysis, we design the FASA scheme
such that the estimate of the backlogged devices converges fast to the true
value. Furthermore, by examining the $T$-slot drifts, we prove that the
proposed FASA scheme is stable as long as the average arrival rate is smaller
than $e^{-1}$, in the sense that the Markov Chain derived from the scheme is
geometrically ergodic. Simulation results demonstrate that the proposed FASA
scheme outperforms traditional additive schemes such as PB-ALOHA and achieves
near-optimal performance in reducing access delay. Moreover, compared to
multiplicative schemes, FASA shows its robustness under heavy traffic load in
addition to better delay performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3030</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3030</id><created>2012-08-15</created><updated>2013-04-22</updated><authors><author><keyname>Bian</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fisher's linear discriminant analysis (FLDA) is an important dimension
reduction method in statistical pattern recognition. It has been shown that
FLDA is asymptotically Bayes optimal under the homoscedastic Gaussian
assumption. However, this classical result has the following two major
limitations: 1) it holds only for a fixed dimensionality $D$, and thus does not
apply when $D$ and the training sample size $N$ are proportionally large; 2) it
does not provide a quantitative description on how the generalization ability
of FLDA is affected by $D$ and $N$. In this paper, we present an asymptotic
generalization analysis of FLDA based on random matrix theory, in a setting
where both $D$ and $N$ increase and $D/N\longrightarrow\gamma\in[0,1)$. The
obtained lower bound of the generalization discrimination power overcomes both
limitations of the classical result, i.e., it is applicable when $D$ and $N$
are proportionally large and provides a quantitative description of the
generalization ability of FLDA in terms of the ratio $\gamma=D/N$ and the
population discrimination power. Besides, the discrimination power bound also
leads to an upper bound on the generalization error of binary-classification
with FLDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3047</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3047</id><created>2012-08-15</created><authors><author><keyname>Nurwidyantoro</keyname><forenames>Arif</forenames></author><author><keyname>Winarko</keyname><forenames>Edi</forenames></author></authors><title>Parallelization of Maximum Entropy POS Tagging for Bahasa Indonesia with
  MapReduce</title><categories>cs.DC cs.CL</categories><comments>International Journal of Computer Science Issues (IJCSI), Vol. 9,
  Issue 4, No 2, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, MapReduce programming model is used to parallelize training
and tagging proceess in Maximum Entropy part of speech tagging for Bahasa
Indonesia. In training process, MapReduce model is implemented dictionary,
tagtoken, and feature creation. In tagging process, MapReduce is implemented to
tag lines of document in parallel. The training experiments showed that total
training time using MapReduce is faster, but its result reading time inside the
process slow down the total training time. The tagging experiments using
different number of map and reduce process showed that MapReduce implementation
could speedup the tagging process. The fastest tagging result is showed by
tagging process using 1,000,000 word corpus and 30 map process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3054</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3054</id><created>2012-08-15</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Khuller</keyname><forenames>Samir</forenames></author></authors><title>LP Rounding for k-Centers with Non-uniform Hard Capacities</title><categories>cs.DS</categories><comments>To appear in FOCS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a generalization of the classical k-center problem
with capacities. Our goal is to select k centers in a graph, and assign each
node to a nearby center, so that we respect the capacity constraints on
centers. The objective is to minimize the maximum distance a node has to travel
to get to its assigned center. This problem is NP-hard, even when centers have
no capacity restrictions and optimal factor 2 approximation algorithms are
known. With capacities, when all centers have identical capacities, a 6
approximation is known with no better lower bounds than for the infinite
capacity version.
  While many generalizations and variations of this problem have been studied
extensively, no progress was made on the capacitated version for a general
capacity function. We develop the first constant factor approximation algorithm
for this problem. Our algorithm uses an LP rounding approach to solve this
problem, and works for the case of non-uniform hard capacities, when multiple
copies of a node may not be chosen and can be extended to the case when there
is a hard bound on the number of copies of a node that may be selected. In
addition we establish a lower bound on the integrality gap of 7(5) for
non-uniform (uniform) hard capacities. In addition we prove that if there is a
(3-eps)-factor approximation for this problem then P=NP.
  Finally, for non-uniform soft capacities we present a much simpler
11-approximation algorithm, which we find as one more evidence that hard
capacities are much harder to deal with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3056</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3056</id><created>2012-08-15</created><authors><author><keyname>Onay</keyname><forenames>Saygun</forenames></author></authors><title>Polar Codes for Nonasymmetric Slepian-Wolf Coding</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method to construct nonasymmetric distributed source coding (DSC) scheme
using polar codes which can achieve any point on the dominant face of the
Slepian-Wolf (SW) rate region for sources with uniform marginals is considered.
In addition to nonasymmetric case, we also discuss and show explicitly how
asymmetric and single source compression is done using successive cancellation
(SC) polar decoder. We then present simulation results that exhibit the
performance of the considered methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3071</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3071</id><created>2012-08-15</created><updated>2015-11-25</updated><authors><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Molla</keyname><forenames>Anisur Rahaman</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Fast Distributed PageRank Computation</title><categories>cs.DC cs.DS</categories><comments>14 pages</comments><journal-ref>Theoretical Computer Science, Volume 561, Pages 113-121, 2015</journal-ref><doi>10.1016/j.tcs.2014.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, PageRank has gained importance in a wide range of
applications and domains, ever since it first proved to be effective in
determining node importance in large graphs (and was a pioneering idea behind
Google's search engine). In distributed computing alone, PageRank vector, or
more generally random walk based quantities have been used for several
different applications ranging from determining important nodes, load
balancing, search, and identifying connectivity structures. Surprisingly,
however, there has been little work towards designing provably efficient
fully-distributed algorithms for computing PageRank. The difficulty is that
traditional matrix-vector multiplication style iterative methods may not always
adapt well to the distributed setting owing to communication bandwidth
restrictions and convergence rates.
  In this paper, we present fast random walk-based distributed algorithms for
computing PageRanks in general graphs and prove strong bounds on the round
complexity. We first present a distributed algorithm that takes $O\big(\log
n/\eps \big)$ rounds with high probability on any graph (directed or
undirected), where $n$ is the network size and $\eps$ is the reset probability
used in the PageRank computation (typically $\eps$ is a fixed constant). We
then present a faster algorithm that takes $O\big(\sqrt{\log n}/\eps \big)$
rounds in undirected graphs. Both of the above algorithms are scalable, as each
node sends only small ($\polylog n$) number of bits over each edge per round.
To the best of our knowledge, these are the first fully distributed algorithms
for computing PageRank vector with provably efficient running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3085</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3085</id><created>2012-08-15</created><authors><author><keyname>Bhat</keyname><forenames>Trshant</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Variance based Scheduling to Improve the QoS Performance at the Cell
  Edge</title><categories>cs.NI</categories><comments>10 pages, 4 figures, submitted to IEEE ANTS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now a days mobile phones are most often used for data communication rather
than voice calls. Due to this change in user behavior, there is a need to
improve the QoS received by the user. One of the ways of improving the QoS is
an efficient scheduling algorithm which incorporates the needs of the users and
variation in channel condition. The parameters used to measure the efficiency
of the scheduling algorithms are the Jain Fairness Index and the overall system
throughput. In this paper we have proposed a variance based scheduling
algorithm which selects the user who has the highest variance of data
transmitted in a given time frame as a parameter for scheduling. This ensures
that eventually, the users transmit almost equal amounts of data regardless of
channel condition. The simulation results shows that the proposed algorithm
achieves high Jain Fairness Index of 0.92 with a lesser drop in the system
throughput 18% as compared to Dynamically altering Proportionally Fair
Algorithm's 20% using the Proportionally Fair Algorithm as reference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3091</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3091</id><created>2012-08-15</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>An Adaptive Successive Cancellation List Decoder for Polar Codes with
  Cyclic Redundancy Check</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this letter, we propose an adaptive SC (Successive Cancellation)-List
decoder for polar codes with CRC. This adaptive SC-List decoder iteratively
increases the list size until the decoder outputs contain at least one survival
path which can pass CRC. Simulation shows that the adaptive SC-List decoder
provides significant complexity reduction. We also demonstrate that polar code
(2048, 1024) with 24-bit CRC decoded by our proposed adaptive SC-List decoder
with very large list size can achieve a frame error rate FER=0.001 at
Eb/No=1.1dB, which is about 0.2dB from the information theoretic limit at this
block length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3101</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3101</id><created>2012-08-15</created><updated>2013-03-08</updated><authors><author><keyname>Serpa</keyname><forenames>F. G.</forenames></author><author><keyname>Graves</keyname><forenames>Adam M.</forenames></author><author><keyname>Javier</keyname><forenames>Artjay</forenames></author></authors><title>Statistical Common Author Networks (SCAN)</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>Accepted to JASIST (February 2013). Copyright 2013 American Society
  of Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for visualizing the relatedness of scientific areas is developed
that is based on measuring the overlap of researchers between areas. It is
found that closely related areas have a high propensity to share a larger
number of common authors. A methodology for comparing areas of vastly different
sizes and to handle name homonymy is constructed, allowing for the robust
deployment of this method on real data sets. A statistical analysis of the
probability distributions of the common author overlap that accounts for noise
is carried out along with the production of network maps with weighted links
proportional to the overlap strength. This is demonstrated on two case studies,
complexity science and neutrino physics, where the level of relatedness of
areas within each area is expected to vary greatly. It is found that the
results returned by this method closely match the intuitive expectation that
the broad, multidisciplinary area of complexity science possesses areas that
are weakly related to each other while the much narrower area of neutrino
physics shows very strongly related areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3122</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3122</id><created>2012-08-15</created><authors><author><keyname>Al-Khazali</keyname><forenames>Hisham A. H.</forenames></author><author><keyname>Askari</keyname><forenames>Mohamad R.</forenames></author></authors><title>Defect Diagnosis in Rotors Systems by Vibrations Data Collectors Using
  Trending Software</title><categories>cs.CE</categories><comments>11 pages,6 figures,1 pictuer,1 scheme</comments><journal-ref>IJACSA 3 (2012) 33-43</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vibration measurements have been used to reliably diagnose performance
problems in machinery and related mechanical products. A vibration data
collector can be used effectively to measure and analyze the machinery
vibration content in gearboxes, engines, turbines, fans, compressors, pumps and
bearings. Ideally, a machine will have little or no vibration, indicating that
the rotating components are appropriately balanced, aligned, and well
maintained. Quick analysis and assessment of the vibration content can lead to
fault diagnosis and prognosis of a machine's ability to continue running. The
aim of this research used vibration measurements to pinpoint mechanical defects
such as (unbalance, misalignment, resonance, and part loosening), consequently
diagnosis all necessary process for engineers and technicians who desire to
understand the vibration that exists in structures and machines.
  Keywords- vibration data collectors; analysis software; rotating components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3124</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3124</id><created>2012-08-14</created><updated>2013-04-29</updated><authors><author><keyname>Reem</keyname><forenames>Daniel</forenames></author></authors><title>On the computation of zone and double zone diagrams</title><categories>cs.CG math.FA</categories><comments>30 pages, 15 figures; the discussion in certain introductory parts
  was improved; the main theorem was divided into two parts; added a section
  regarding topological properties of Voronoi cells; added references</comments><msc-class>47H10, 68U05, 46B20, 65D18</msc-class><acm-class>F.2.2; G.0; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical objects in computational geometry are defined by explicit
relations. A few years ago an interesting family of geometric objects defined
by implicit relations was introduced in the pioneering works of T. Asano, J.
Matousek and T. Tokuyama. An important member in this family is a zone diagram,
defined formally as a solution to a fixed point equation involving sets. As a
result, computation of zone diagrams is a challenging task and in a continuous
setting it has been addressed (briefly) only by these authors in the Euclidean
plane with point sites. This paper discusses the possibility to compute zone
diagrams in a wide class of spaces. This class, which is introduced here,
includes, in particular, Euclidean spheres and finite dimensional strictly
convex normed spaces. Sites of a general form are allowed and it is shown that
a generalization of the iterative algorithm suggested by the above mentioned
authors converges to a double zone diagram, another implicit geometric object
whose existence is known in general. Occasionally a zone diagram can be
obtained from the resulting double zone diagram. The actual (approximate)
computation of the corresponding iterations and the resulting (double) zone
diagram is done, in the normed case, using a new algorithm which enables the
computation of Voronoi diagrams in a general setting. Along the way certain
topological properties of Voronoi cells are discussed. Unexplained interesting
phenomena are discussed too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3133</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3133</id><created>2012-08-15</created><authors><author><keyname>Abd-Elhafiez</keyname><forenames>Walaa M.</forenames></author><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author></authors><title>Color Image Compression Algorithm Based on the DCT Blocks</title><categories>cs.CV</categories><comments>5 pages. arXiv admin note: text overlap with standard references on
  JPEG without attribution</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 3, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the performance of different blockbased discrete cosine
transform (DCT) algorithms for compressing color image. In this RGB component
of color image are converted to YCbCr before DCT transform is applied. Y is
luminance component;Cb and Cr are chrominance components of the image. The
modification of the image data is done based on the classification of image
blocks to edge blocks and non-edge blocks, then the edge block of the image is
compressed with low compression and the nonedge blocks is compressed with high
compression. The analysis results have indicated that the performance of the
suggested method is much better, where the constructed images are less
distorted and compressed with higher factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3138</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3138</id><created>2012-08-10</created><authors><author><keyname>Ramalingam</keyname><forenames>Ashokkumar</forenames></author><author><keyname>Dorairaj</keyname><forenames>Prabhu</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Saranya</forenames></author></authors><title>Personal Safety Triggering System on Android Mobile Platform</title><categories>cs.OH</categories><comments>19 pages, International Journal of Network Security &amp; Its
  Applications (IJNSA), July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduction of Smart phones redefined the usage of mobile phones in the
communication world. Smart phones are equipped with various sophisticated
features such as Wi-Fi, GPS navigation, high resolution camera, touch screen
with broadband access which helps the mobile phone users to keep in touch with
the modern world. Many of these features are primarily integrated with the
mobile operating system which is out of reach to public, by which the users
can't manipulate those features. Google came up with an innovative operation
system termed as ANDROID, which is open system architecture with customizable
third party development and debugging environment which helps the user's to
manipulate the features and to create their own customizable applications. In
this paper, 'Emergency Based Remote Collateral Tracking System' application
using Google's Android Mobile Platform is addressed. Emergency is divided into
three categories: heart beat based emergency, security threats like personal
safety and road accidents. This application is targeted to a person who is
driving a vehicle. Heart rate monitoring device is integrated with our
application to sense the heart beat of a person driving the vehicle and if
there is any abnormalities in the heart beat, then our application performs a
dual role. One in which, application uses a GPS to track the location
information of the user and send those location information as a message via
SMS, email and post it on Facebook wall Simultaneously, an emergency signal is
sent to Arduino Microcontroller.This application is written in JAVA programming
language which runs on Eclipse Integrated Development Kit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3145</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3145</id><created>2012-08-14</created><authors><author><keyname>van Dongen</keyname><forenames>Stijn</forenames></author><author><keyname>Enright</keyname><forenames>Anton J.</forenames></author></authors><title>Metric distances derived from cosine similarity and Pearson and Spearman
  correlations</title><categories>stat.ME cs.LG</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two classes of transformations of cosine similarity and
Pearson and Spearman correlations into metric distances, utilising the simple
tool of metric-preserving functions. The first class puts anti-correlated
objects maximally far apart. Previously known transforms fall within this
class. The second class collates correlated and anti-correlated objects. An
example of such a transformation that yields a metric distance is the sine
function when applied to centered data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3148</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3148</id><created>2012-08-15</created><authors><author><keyname>Meilicke</keyname><forenames>Christian</forenames><affiliation>University of Mannheim</affiliation></author><author><keyname>Sv&#xe1;b-Zamazal</keyname><forenames>Ondrej</forenames><affiliation>University of Economics Prague</affiliation></author><author><keyname>Trojahn</keyname><forenames>C&#xe1;ssia</forenames><affiliation>INRIA and LIG Grenoble</affiliation></author><author><keyname>Jim&#xe9;nez-Ruiz</keyname><forenames>Ernesto</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Aguirre</keyname><forenames>Jos&#xe9;-Luis</forenames><affiliation>INRIA and LIG Grenoble</affiliation></author><author><keyname>Stuckenschmidt</keyname><forenames>Heiner</forenames><affiliation>University of Mannheim</affiliation></author><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Evaluating Ontology Matching Systems on Large, Multilingual and
  Real-world Test Cases</title><categories>cs.AI</categories><comments>Technical Report of the OAEI 2011.5 Evaluation Campaign</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of ontology matching, the most systematic evaluation of matching
systems is established by the Ontology Alignment Evaluation Initiative (OAEI),
which is an annual campaign for evaluating ontology matching systems organized
by different groups of researchers. In this paper, we report on the results of
an intermediary OAEI campaign called OAEI 2011.5. The evaluations of this
campaign are divided in five tracks. Three of these tracks are new or have been
improved compared to previous OAEI campaigns. Overall, we evaluated 18 matching
systems. We discuss lessons learned, in terms of scalability, multilingual
issues and the ability do deal with real world cases from different domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3150</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3150</id><created>2012-08-15</created><authors><author><keyname>Kalbat</keyname><forenames>F.</forenames></author><author><keyname>Al-Dweik</keyname><forenames>A.</forenames></author></authors><title>Low Complexity Space-Frequency MIMO OFDM System for Double-Selective
  Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Vehicular Technology 18 June
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a highly robust space-frequency block coded (SFBC)
multiple-input multiple-output (MIMO) orthogonal frequency division
multiplexing (OFDM) system. The proposed system is based on applying a short
block length Walsh Hadamard transform (WHT) after the SFBC encoder. The main
advantage of the proposed system is that the channel frequency responses over
every two adjacent subcarriers become equal. Such interesting result provides
an exceptional operating conditions for SFBC-OFDM systems transmitting over
time and frequencyselective fading channels. Monte Carlo simulation is used to
evaluate the bit error rate (BER) performance of the proposed system using
various wireless channels with different degrees of frequency selectivity and
Doppler spreads. The simulation results demonstrated that the proposed scheme
substantially outperforms the standard SFBC-OFDM and the space-time block coded
(STBC) OFDM systems in severe time-varying frequency-selective fading channels.
Moreover, the proposed system has very low complexity because it is based on
short block length WHT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3151</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3151</id><created>2012-08-15</created><authors><author><keyname>Bartocci</keyname><forenames>Ezio</forenames><affiliation>Vienna University of Technology</affiliation></author><author><keyname>Bortolussi</keyname><forenames>Luca</forenames><affiliation>University of Trieste</affiliation></author></authors><title>Proceedings First International Workshop on Hybrid Systems and Biology</title><categories>cs.CE cs.LO cs.SY</categories><comments>EPTCS 92, 2012</comments><proxy>EPTCS</proxy><acm-class>I.6.0; I.6.1; I.6.3; I.6.4; I.6.7; I.6.8</acm-class><doi>10.4204/EPTCS.92</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the First International Workshop on
Hybrid Systems and Biology (HSB 2012), that will be held in Newcastle upon
Tyne, UK, on the 3rd September, 2012. HSB 2012 is a satellite event of the 23rd
International Conference on Concurrency Theory (CONCUR 2012).
  This workshop aims at collecting scientists working in the area of hybrid
modeling applied to systems biology, in order to discuss about current achieved
goals, current challenges and future possible developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3153</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3153</id><created>2012-08-15</created><updated>2012-08-16</updated><authors><author><keyname>Andersen</keyname><forenames>Jakob L.</forenames></author><author><keyname>Flamm</keyname><forenames>Christoph</forenames></author><author><keyname>Merkle</keyname><forenames>Daniel</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Inferring Chemical Reaction Patterns Using Rule Composition in Graph
  Grammars</title><categories>cs.DM cs.CE q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling molecules as undirected graphs and chemical reactions as graph
rewriting operations is a natural and convenient approach tom odeling
chemistry. Graph grammar rules are most naturally employed to model elementary
reactions like merging, splitting, and isomerisation of molecules. It is often
convenient, in particular in the analysis of larger systems, to summarize
several subsequent reactions into a single composite chemical reaction. We use
a generic approach for composing graph grammar rules to define a chemically
useful rule compositions. We iteratively apply these rule compositions to
elementary transformations in order to automatically infer complex
transformation patterns. This is useful for instance to understand the net
effect of complex catalytic cycles such as the Formose reaction. The
automatically inferred graph grammar rule is a generic representative that also
covers the overall reaction pattern of the Formose cycle, namely two carbonyl
groups that can react with a bound glycolaldehyde to a second glycolaldehyde.
Rule composition also can be used to study polymerization reactions as well as
more complicated iterative reaction schemes. Terpenes and the polyketides, for
instance, form two naturally occurring classes of compounds of utmost
pharmaceutical interest that can be understood as &quot;generalized polymers&quot;
consisting of five-carbon (isoprene) and two-carbon units, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3160</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3160</id><created>2012-08-15</created><authors><author><keyname>Chastain</keyname><forenames>Erick</forenames></author><author><keyname>Livnat</keyname><forenames>Adi</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Vazirani</keyname><forenames>Umesh</forenames></author></authors><title>Multiplicative Updates in Coordination Games and the Theory of Evolution</title><categories>cs.GT q-bio.PE</categories><msc-class>91A99</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the population genetics of Evolution in the important special case
of weak selection, in which all fitness values are assumed to be close to one
another. We show that in this regime natural selection is tantamount to the
multiplicative updates game dynamics in a coordination game between genes.
Importantly, the utility maximized in this game, as well as the amount by which
each allele is boosted, is precisely the allele's mixability, or average
fitness, a quantity recently proposed in [1] as a novel concept that is crucial
in understanding natural selection under sex, thus providing a rigorous
demonstration of that insight. We also prove that the equilibria in two-person
coordination games can have large supports, and thus genetic diversity does not
suffer much at equilibrium. Establishing large supports involves answering
through a novel technique the following question: what is the probability that
for a random square matrix A both systems Ax = 1 and A^T y = 1 have positive
solutions? Both the question and the technique may be of broader interest.
  [1] A. Livnat, C. Papadimitriou, J. Dushoff, and M.W. Feldman. A mixability
theory for the role of sex in evolution. Proceedings of the National Academy of
Sciences, 105(50):19803-19808, 2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3192</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3192</id><created>2012-08-15</created><authors><author><keyname>Saboori</keyname><forenames>Ehsan</forenames></author><author><keyname>Mohammadi</keyname><forenames>Shahriar</forenames></author></authors><title>Anonymous Communication in Peer-to-Peer Networks for providing more
  Privacy and Security</title><categories>cs.NI cs.CR</categories><comments>5 Pages. arXiv admin note: substantial text overlap with
  arXiv:1208.3022</comments><journal-ref>International Journal of Modeling and Optimization vol. 2, no. 3,
  pp. 217-221, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important issues in peer-to-peer networks is anonymity. The
major anonymity for peer-to-peer users concerned with the users' identities and
actions which can be revealed by any other members. There are many approaches
proposed to provide anonymous peer-to-peer communications. An intruder can get
information about the content of the data, the sender's and receiver's
identities. Anonymous approaches are designed with the following three goals:
to protect the identity of provider, to protect the identity of requester and
to protect the contents of transferred data between them. This article presents
a new peer-to-peer approach to achieve anonymity between a requester and a
provider in peer-to-peer networks with trusted servers called suppernode so
that the provider will not be able to identify the requester and no other peers
can identify the two communicating parties with certainty. This article shows
that the proposed algorithm improved reliability and has more security. This
algorithm, based on onion routing and randomization, protects transferring data
against traffic analysis attack. The ultimate goal of this anonymous
communications algorithm is to allow a requester to communicate with a provider
in such a manner that nobody can determine the requester's identity and the
content of transferred data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3205</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3205</id><created>2012-08-15</created><authors><author><keyname>Gaur</keyname><forenames>Manas</forenames></author></authors><title>Software Security analysis, static and dynamic testing in java and C
  environment, a comparative study</title><categories>cs.CR cs.SE</categories><comments>the research paper consists of 11 figures and 7 tabular comparison</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main stretch in the paper is buffer overflow anomaly occurring in major
source codes, designed in various programming language. It describes the
various as to how to improve your code and increase its strength to withstand
security theft occurring at vulnerable areas in the code. The main language
used is JAVA, regarded as one of the most object oriented language still create
lot of error like stack overflow, illegal/inappropriate method overriding. I
used tools confined to JAVA to test as how weak points in the code can be
rectified before compiled. The byte code theft is difficult to be conquered, so
it's a better to get rid of it in the plain java code itself. The tools used in
the research are PMD(Programming mistake detector), it helps to detect line of
code that make pop out error in near future like defect in hashcode(memory
maps) overriding due to which the java code will not function correctly.
Another tool is FindBUGS which provide the tester of the code to analyze the
weak points in the code like infinite loop, unsynchronized wait, deadlock
situation, null referring and dereferencing. Another tool which provides the
base to above tools is JaCoCo code coverage analysis used to detect unreachable
part and unused conditions of the code which improves the space complexity and
helps in easy clarification of errors.
  Through this paper, we design an algorithm to prevent the loss of data. The
main audience is the white box tester who might leave out essential line of
code like, index variables, infinite loop, and inappropriate hashcode in the
major source program. This algorithm serves to reduce the damage in case of
buffer overflow
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3212</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3212</id><created>2012-08-15</created><authors><author><keyname>Kim</keyname><forenames>MinJi</forenames></author><author><keyname>Klein</keyname><forenames>Thierry</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Barros</keyname><forenames>Joao</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Modeling Network Coded TCP: Analysis of Throughput and Energy Cost</title><categories>cs.NI</categories><comments>14 pages, 21 figures, manuscript/report. arXiv admin note:
  substantial text overlap with arXiv:1008.0420, arXiv:1203.2841</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the performance of TCP and TCP with network coding (TCP/NC) in
lossy networks. We build upon the framework introduced by Padhye et al. and
characterize the throughput behavior of classical TCP and TCP/NC as a function
of erasure probability, round-trip time, maximum window size, and duration of
the connection. Our analytical results show that network coding masks random
erasures from TCP, thus preventing TCP's performance degradation in lossy
networks. It is further seen that TCP/NC has significant throughput gains over
TCP.
  In addition, we show that TCP/NC may lead to cost reduction for wireless
network providers while maintaining a certain quality of service to their
users. We measure the cost in terms of number of base stations, which is highly
correlated to the energy, capital, and operational costs of a network provider.
We show that increasing the available bandwidth may not necessarily lead to
increase in throughput, particularly in lossy networks in which TCP does not
perform well. We show that using protocols such as TCP/NC, which are more
resilient to erasures, may lead to a throughput commensurate the bandwidth
dedicated to each user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3213</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3213</id><created>2012-08-15</created><authors><author><keyname>van Handel</keyname><forenames>Ramon</forenames></author></authors><title>Ergodicity, Decisions, and Partial Information</title><categories>math.PR cs.IT math.IT math.OC</categories><comments>45 pages</comments><journal-ref>S\'eminaire de Probabilit\'es XLVI, Lecture Notes in Mathematics
  2123, Springer (2014)</journal-ref><doi>10.1007/978-3-319-11970-0_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the simplest sequential decision problem for an ergodic stochastic process
X, at each time n a decision u_n is made as a function of past observations
X_0,...,X_{n-1}, and a loss l(u_n,X_n) is incurred. In this setting, it is
known that one may choose (under a mild integrability assumption) a decision
strategy whose pathwise time-average loss is asymptotically smaller than that
of any other strategy. The corresponding problem in the case of partial
information proves to be much more delicate, however: if the process X is not
observable, but decisions must be based on the observation of a different
process Y, the existence of pathwise optimal strategies is not guaranteed.
  The aim of this paper is to exhibit connections between pathwise optimal
strategies and notions from ergodic theory. The sequential decision problem is
developed in the general setting of an ergodic dynamical system (\Omega,B,P,T)
with partial information Y\subseteq B. The existence of pathwise optimal
strategies grounded in two basic properties: the conditional ergodic theory of
the dynamical system, and the complexity of the loss function. When the loss
function is not too complex, a general sufficient condition for the existence
of pathwise optimal strategies is that the dynamical system is a conditional
K-automorphism relative to the past observations \bigvee_n T^n Y. If the
conditional ergodicity assumption is strengthened, the complexity assumption
can be weakened. Several examples demonstrate the interplay between complexity
and ergodicity, which does not arise in the case of full information. Our
results also yield a decision-theoretic characterization of weak mixing in
ergodic theory, and establish pathwise optimality of ergodic nonlinear filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3227</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3227</id><created>2012-08-15</created><authors><author><keyname>Doomun</keyname><forenames>Razvi</forenames></author><author><keyname>Doma</keyname><forenames>Jayramsingh</forenames></author><author><keyname>Tengur</keyname><forenames>Sundeep</forenames></author></authors><title>AES-CBC Software Execution Optimization</title><categories>cs.CR</categories><comments>8 pages, IEEE 2008</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the proliferation of high-speed wireless networking, the necessity for
efficient, robust and secure encryption modes is ever increasing. But,
cryptography is primarily a computationally intensive process. This paper
investigates the performance and efficiency of IEEE 802.11i approved Advanced
Encryption Standard (AES)-Rijndael ciphering/deciphering software in Cipher
Block Chaining (CBC) mode. Simulations are used to analyse the speed, resource
consumption and robustness of AES-CBC to investigate its viability for image
encryption usage on common low power devices. The detailed results presented in
this paper provide a basis for performance estimation of AES cryptosystems
implemented on wireless devices. The use of optimized AES-CBC software
implementation gives a superior encryption speed performance by 12 - 30%, but
at the cost of twice more memory for code size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3235</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3235</id><created>2012-08-15</created><updated>2013-04-25</updated><authors><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Chamberland</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>First-Passage Time and Large-Deviation Analysis for Erasure Channels
  with Memory</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, ISSN 0018-9448, Vol. 59,
  No. 9, pp. 5547-5565, Sep. 2013</journal-ref><doi>10.1109/TIT.2013.2260593</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the performance of digital communication systems
transmitting messages over finite-state erasure channels with memory.
Information bits are protected from channel erasures using error-correcting
codes; successful receptions of codewords are acknowledged at the source
through instantaneous feedback. The primary focus of this research is on
delay-sensitive applications, codes with finite block lengths and, necessarily,
non-vanishing probabilities of decoding failure. The contribution of this
article is twofold. A methodology to compute the distribution of the time
required to empty a buffer is introduced. Based on this distribution, the mean
hitting time to an empty queue and delay-violation probabilities for specific
thresholds can be computed explicitly. The proposed techniques apply to
situations where the transmit buffer contains a predetermined number of
information bits at the onset of the data transfer. Furthermore, as additional
performance criteria, large deviation principles are obtained for the empirical
mean service time and the average packet-transmission time associated with the
communication process. This rigorous framework yields a pragmatic methodology
to select code rate and block length for the communication unit as functions of
the service requirements. Examples motivated by practical systems are provided
to further illustrate the applicability of these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3241</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3241</id><created>2012-08-15</created><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>Hidden information and regularities of information dynamics III</title><categories>nlin.AO cs.IT math.IT</categories><comments>27 pages,7 figures. arXiv admin note: text overlap with
  arXiv:1109.3125</comments><msc-class>58J65, 60J65, 93B52, 93E02, 93E15, 93E30</msc-class><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This presentation's Part 3 studies the evolutionary information processes and
regularities of evolution dynamics, evaluated by an entropy functional (EF) of
a random field (modeled by a diffusion information process) and an
informational path functional (IPF) on trajectories of the related dynamic
process (Lerner 2012). The integral information measure on the process'
trajectories accumulates and encodes inner connections and dependencies between
the information states, and contains more information than a sum of Shannon's
entropies, which measures and encodes each process's states separately. Cutting
off the process' measured information under action of impulse controls (Lerner
2012a), extracts and reveals hidden information, covering the states'
correlations in a multi-dimensional random process, and implements the EF-IPF
minimax variation principle (VP). The approach models an information observer
(Lerner 2012b)-as an extractor of such information, which is able to convert
the collected information of the random process in the information dynamic
process and organize it in the hierarchical information network (IN), Part2
(Lerner, 2012c). The IN's highest level of the structural hierarchy, measured
by a maximal quantity and quality of the accumulated cooperative information,
evaluates the observer's intelligence level, associated with its ability to
recognize and build such structure of a meaningful hidden information. The
considered evolution of optimal extraction, assembling, cooperation, and
organization of this information in the IN, satisfying the VP, creates the
phenomena of an evolving observer's intelligence. The requirements of
preserving the evolutionary hierarchy impose the restrictions that limit the
observer's intelligence level in the IN. The cooperative information geometry,
evolving under observations, limits the size and volumes of a particular
observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3251</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3251</id><created>2012-08-15</created><updated>2013-02-11</updated><authors><author><keyname>Nokleby</keyname><forenames>Matthew</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Toward Resource-Optimal Consensus over the Wireless Medium</title><categories>cs.IT math.IT</categories><comments>12 pages, 3 figures, to appear in IEEE Journal Selected Topics in
  Signal Processing, April 2013</comments><doi>10.1109/JSTSP.2013.2246765</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We carry out a comprehensive study of the resource cost of averaging
consensus in wireless networks. Most previous approaches suppose a graphical
network, which abstracts away crucial features of the wireless medium, and
measure resource consumption only in terms of the total number of transmissions
required to achieve consensus. Under a path-loss dominated model, we study the
resource requirements of consensus with respect to three wireless-appropriate
metrics: total transmit energy, elapsed time, and time-bandwidth product. First
we characterize the performance of several popular gossip algorithms, showing
that they may be order-optimal with respect to transmit energy but are strictly
suboptimal with respect to elapsed time and time-bandwidth product. Further, we
propose a new consensus scheme, termed hierarchical averaging, and show that it
is nearly order-optimal with respect to all three metrics. Finally, we examine
the effects of quantization, showing that hierarchical averaging provides a
nearly order-optimal tradeoff between resource consumption and quantization
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3252</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3252</id><created>2012-08-15</created><updated>2012-11-30</updated><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>The Effect of Exogenous Inputs and Defiant Agents on Opinion Dynamics
  with Local and Global Interactions</title><categories>physics.soc-ph cs.SI nlin.AO</categories><journal-ref>IEEE Journal of Selected Topics in Signal Processing, Volume: 7 ,
  Issue 2, Page(s): 347 - 357, April 2013</journal-ref><doi>10.1109/JSTSP.2013.2245629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the conventional models for opinion dynamics mainly account for a
fully local influence, where myopic agents decide their actions after they
interact with other agents that are adjacent to them. For example, in the case
of social interactions, this includes family, friends, and other strong social
ties. The model proposed in this contribution, embodies a global influence as
well where, by global, we mean that each node also observes a sample of the
average behavior of the entire population (in the social example, people
observe other people on the streets, subway, and other social venues). We
consider a case where nodes have dichotomous states (examples include elections
with two major parties, whether or not to adopt a new technology or product,
and any yes/no opinion such as in voting on a referendum). The dynamics of
states on a network with arbitrary degree distribution are studied. For a given
initial condition, we find the probability to reach consensus on each state and
the expected time reach to consensus. The effect of an exogenous bias on the
average orientation of the system is investigated, to model mass media. To do
so, we add an external field to the model that favors one of the states over
the other. This field interferes with the regular decision process of each node
and creates a constant probability to lean towards one of the states. We solve
for the average state of the system as a function of time for given initial
conditions. Then anti-conformists (stubborn nodes who never revise their
states) are added to the network, in an effort to circumvent the external bias.
We find necessary conditions on the number of these defiant nodes required to
cancel the effect of the external bias. Our analysis is based on a mean field
approximation of the agent opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3254</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3254</id><created>2012-08-15</created><authors><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Fung</keyname><forenames>Patrick Ho Wang</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Carrier Frequency Offset Estimation for Two-Way Relaying: Optimal
  Preamble and Estimator Design</title><categories>cs.IT math.IT</categories><comments>submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of carrier frequency offset (CFO) estimation for a
two-way relaying system based on the amplify-and-forward (AF) protocol. Our
contributions are in designing an optimal preamble, and the corresponding
estimator, to closely achieve the minimum Cramer-Rao bound (CRB) for the CFO.
This optimality is asserted with respect to the novel class of preambles,
referred to as the block-rotated preambles (BRPs). This class includes the
periodic preamble that is used widely in practice, yet it provides an
additional degree of design freedom via a block rotation angle. We first
identify the catastrophic scenario of an arbitrarily large CRB when a
conventional periodic preamble is used. We next resolve this problem by using a
BRP with a non-zero block rotation angle. This angle creates, in effect, an
artificial frequency offset that separates the desired relayed signal from the
self-interference that is introduced in the AF protocol. With appropriate
optimization, the CRB incurs only marginal loss from one-way relaying under
practical channel conditions. To facilitate implementation, a specific
low-complexity class of estimators is examined, and conditions for the
estimators to achieve the optimized CRB is established. Numerical results are
given which corroborate with theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3261</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3261</id><created>2012-08-15</created><authors><author><keyname>Han</keyname><forenames>Guangyue</forenames></author><author><keyname>Marcus</keyname><forenames>Brian</forenames></author></authors><title>Analyticity of Entropy Rate of Continuous-State Hidden Markov Chains</title><categories>math.PR cs.IT math.IT</categories><comments>27 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that under certain mild assumptions, the entropy rate of a hidden
Markov chain, observed when passing a finite-state stationary Markov chain
through a discrete-time continuous-output channel, is jointly analytic as a
function of the input Markov chain parameters and the channel parameters. In
particular, as consequences of the main theorems, we obtain analyticity for the
entropy rate associated with representative channels: Cauchy and Gaussian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3279</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3279</id><created>2012-08-06</created><authors><author><keyname>Weiss</keyname><forenames>David</forenames></author><author><keyname>Sapp</keyname><forenames>Benjamin</forenames></author><author><keyname>Taskar</keyname><forenames>Ben</forenames></author></authors><title>Structured Prediction Cascades</title><categories>stat.ML cs.LG</categories><comments>32 pages, in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured prediction tasks pose a fundamental trade-off between the need for
model complexity to increase predictive power and the limited computational
resources for inference in the exponentially-sized output spaces such models
require. We formulate and develop the Structured Prediction Cascade
architecture: a sequence of increasingly complex models that progressively
filter the space of possible outputs. The key principle of our approach is that
each model in the cascade is optimized to accurately filter and refine the
structured output state space of the next model, speeding up both learning and
inference in the next layer of the cascade. We learn cascades by optimizing a
novel convex loss function that controls the trade-off between the filtering
efficiency and the accuracy of the cascade, and provide generalization bounds
for both accuracy and efficiency. We also extend our approach to intractable
models using tree-decomposition ensembles, and provide algorithms and theory
for this setting. We evaluate our approach on several large-scale problems,
achieving state-of-the-art performance in handwriting recognition and human
pose recognition. We find that structured prediction cascades allow tremendous
speedups and the use of previously intractable features and models in both
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3290</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3290</id><created>2012-08-16</created><authors><author><keyname>Martins</keyname><forenames>Andr&#xe9; C. R.</forenames></author><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>The building up of individual inflexibility in opinion dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 7 figures</comments><journal-ref>Physical Review E 87 (4), 042807, 2013</journal-ref><doi>10.1103/PhysRevE.87.042807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two models of opinion dynamics are entangled in order to build a more
realistic model of inflexibility. The first one is the Galam Unifying Frame
(GUF), which incorporates rational and inflexible agents, and the other one
considers the combination of Continuous Opinions and Discrete Actions (CODA).
While initially in GUF, inflexibility is a fixed given feature of an agent, it
is now the result of an accumulation for a given agent who makes the same
choice through repeated updates. Inflexibility thus emerges as an internal
property of agents becoming a continuous function of the strength of its
opinion. Therefore an agent can be more or less inflexible and can shift from
inflexibility along one choice to inflexibility along the opposite choice.
These individual dynamics of the building up and falling off of an agent
inflexibility are driven by the successive local updates of the associated
individual opinions. New results are obtained and discussed in terms of
predicting outcomes of public debates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3291</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3291</id><created>2012-08-16</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author></authors><title>When to look at a noisy Markov chain in sequential decision making if
  measurements are costly?</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A decision maker records measurements of a finite-state Markov chain
corrupted by noise. The goal is to decide when the Markov chain hits a specific
target state. The decision maker can choose from a finite set of sampling
intervals to pick the next time to look at the Markov chain. The aim is to
optimize an objective comprising of false alarm, delay cost and cumulative
measurement sampling cost. Taking more frequent measurements yields accurate
estimates but incurs a higher measurement cost. Making an erroneous decision
too soon incurs a false alarm penalty. Waiting too long to declare the target
state incurs a delay penalty. What is the optimal sequential strategy for the
decision maker? The paper shows that under reasonable conditions, the optimal
strategy has the following intuitive structure: when the Bayesian estimate
(posterior distribution) of the Markov chain is away from the target state,
look less frequently; while if the posterior is close to the target state, look
more frequently. Bounds are derived for the optimal strategy. Also the
achievable optimal cost of the sequential detector as a function of transition
dynamics and observation distribution is analyzed. The sensitivity of the
optimal achievable cost to parameter variations is bounded in terms of the
Kullback divergence. To prove the results in this paper, novel stochastic
dominance results on the Bayesian filtering recursion are derived. The
formulation in this paper generalizes quickest time change detection to
consider optimal sampling and also yields useful results in sensor scheduling
(active sensing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3295</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3295</id><created>2012-08-16</created><authors><author><keyname>Joy</keyname><forenames>Preetha Theresa</forenames></author><author><keyname>Jacob</keyname><forenames>K. Poulose</forenames></author></authors><title>Cache Replacement Policies for Cooperative Caching in Mobile Ad hoc
  Networks</title><categories>cs.NI</categories><comments>6 pages, 1 Table; IJCSI International Journal of Computer Science
  Issues, Vol. 9, Issue 3, No 2, May 2012 ISSN (Online): 1694-0814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative caching is a technique used in mobile ad hoc networks to improve
the efficiency of information access by reducing the access latency and
bandwidth usage. Cache replacement policy plays a significant role in response
time reduction by selecting suitable subset of items for eviction from the
cache. In this paper we have made a review of the existing cache replacement
algorithms proposed for cooperative caching in ad hoc networks. We made an
attempt to classify existing replacement policies for ad hoc networks based on
the replacement decision taken. In addition, this paper suggests some
alternative techniques for cache replacement. Finally, the paper concludes with
a discussion on future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3307</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3307</id><created>2012-08-16</created><updated>2013-04-21</updated><authors><author><keyname>Evgeny</keyname><forenames>Grigoriev</forenames></author></authors><title>Impedance mismatch is not an &quot;Objects vs. Relations&quot; problem</title><categories>cs.DB</categories><comments>11 pages</comments><acm-class>H.2.1; H.2.2; H.2.4; C.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of impedance mismatch between applications written in OO languages
and relational DB is not a problem of discrepancy between object-oriented and
relational approaches themselves. Its real causes can be found in usual
implementation of the OO approach. Direct comparison of the two approaches
cannot be used as a base for the conclusion that they are discrepant or
mismatched. Experimental proof of absence of contradiction between
object-oriented paradigm and relational data model is also presented
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3313</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3313</id><created>2012-08-16</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Kubica</keyname><forenames>Marcin</forenames></author><author><keyname>Pachocki</keyname><forenames>Jakub</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Tyczy&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>A Note on Efficient Computation of All Abelian Periods in a String</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a simple efficient algorithm for Abelian periods knowing all
Abelian squares in a string. An efficient algorithm for the latter problem was
given by Cummings and Smyth in 1997. By the way we show an alternative
algorithm for Abelian squares. We also obtain a linear time algorithm finding
all `long' Abelian periods. The aim of the paper is a (new) reduction of the
problem of all Abelian periods to that of (already solved) all Abelian squares
which provides new insight into both connected problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3323</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3323</id><created>2012-08-16</created><authors><author><keyname>Murano</keyname><forenames>Pietro</forenames></author><author><keyname>Sethi</keyname><forenames>Tanvi</forenames></author></authors><title>Anthropomorphic User Interface Feedback in a Sewing Context and
  Affordances</title><categories>cs.HC cs.MM</categories><comments>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 2, No. 4, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the authors' research is to gain better insights into the
effectiveness and user satisfaction of anthropomorphism at the user interface.
Therefore, this paper presents a between users experiment and the results in
the context of anthropomorphism at the user interface and the giving of
instruction for learning sewing stitches. Two experimental conditions were
used, where the information for learning sewing stitches was the same. However
the manner of presentation was varied. Therefore one condition was
anthropomorphic and the other was non-anthropomorphic. Also the work is closely
linked with Hartson's theory of affordances applied to user interfaces. The
results suggest that facilitation of the affordances in an anthropomorphic user
interface lead to statistically significant results in terms of effectiveness
and user satisfaction in the sewing context. Further some violation of the
affordances leads to an interface being less usable in terms of effectiveness
and user satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3324</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3324</id><created>2012-08-16</created><authors><author><keyname>Uteshev</keyname><forenames>Alexei Yu.</forenames></author></authors><title>Analytical Solution for the Generalized Fermat-Torricelli Problem</title><categories>cs.CG</categories><comments>15 pages, 3 figures</comments><msc-class>68W30, 14Q20</msc-class><acm-class>G.1.6; I.1.2; I.3.5</acm-class><journal-ref>Amer.Math.Monthly. 121(4), 318-331, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present explicit analytical solution for the problem of minimization of
the function $ F(x,y)= \sum_{j=1}^3 m_j \sqrt{(x-x_j)^2+(y-y_j)^2} $, i.e. we
find the coordinates of stationary point and the corresponding critical value
of $ F(x,y) $ as functions of $ {m_j,x_j,y_j}_{j=1}^3 $. In addition, we also
discuss inverse problem of finding such values of $ m_1,m_2,m_3 $ with the aim
for the corresponding function $ F $ to posses a prescribed position of
stationary point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3334</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3334</id><created>2012-08-16</created><authors><author><keyname>Whitfield</keyname><forenames>James D.</forenames></author><author><keyname>Love</keyname><forenames>Peter J.</forenames></author><author><keyname>Aspuru-Guzik</keyname><forenames>Alan</forenames></author></authors><title>Computational Complexity in Electronic Structure</title><categories>physics.chem-ph cs.CC quant-ph</categories><comments>14 pages, 2 figures, 1 table. Comments welcome</comments><doi>10.1039/c2cp42695a</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In quantum chemistry, the price paid by all known efficient model chemistries
is either the truncation of the Hilbert space or uncontrolled approximations.
Theoretical computer science suggests that these restrictions are not mere
shortcomings of the algorithm designers and programmers but could stem from the
inherent difficulty of simulating quantum systems. Extensions of computer
science and information processing exploiting quantum mechanics has led to new
ways of understanding the ultimate limitations of computational power.
Interestingly, this perspective helps us understand widely used model
chemistries in a new light. In this article, the fundamentals of computational
complexity will be reviewed and motivated from the vantage point of chemistry.
Then recent results from the computational complexity literature regarding
common model chemistries including Hartree-Fock and density functional theory
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3337</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3337</id><created>2012-08-16</created><updated>2013-02-22</updated><authors><author><keyname>Polikarpova</keyname><forenames>Nadia</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Pei</keyname><forenames>Yu</forenames></author><author><keyname>Wei</keyname><forenames>Yi</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>What Good Are Strong Specifications?</title><categories>cs.SE</categories><comments>This is the extended version of the camera-ready published as ICSE
  2013</comments><journal-ref>Proceedings of the 35th International Conference on Software
  Engineering (ICSE). Pgg. 257--266, ACM, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experience with lightweight formal methods suggests that programmers are
willing to write specification if it brings tangible benefits to their usual
development activities. This paper considers stronger specifications and
studies whether they can be deployed as an incremental practice that brings
additional benefits without being unacceptably expensive. We introduce a
methodology that extends Design by Contract to write strong specifications of
functional properties in the form of preconditions, postconditions, and
invariants. The methodology aims at being palatable to developers who are not
fluent in formal techniques but are comfortable with writing simple
specifications. We evaluate the cost and the benefits of using strong
specifications by applying the methodology to testing data structure
implementations written in Eiffel and C#. In our extensive experiments, testing
against strong specifications detects twice as many bugs as standard contracts,
with a reasonable overhead in terms of annotation burden and run-time
performance while testing. In the wide spectrum of formal techniques for
software quality, testing against strong specifications lies in a &quot;sweet spot&quot;
with a favorable benefit to effort ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3340</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3340</id><created>2012-08-16</created><updated>2012-08-17</updated><authors><author><keyname>Diertens</keyname><forenames>Bob</forenames></author></authors><title>Concurrent Models for Object Execution</title><categories>cs.SE</categories><report-no>tcs1201</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work we developed a framework of computational models for the
concurrent execution of functions on different levels of abstraction. It shows
that the traditional sequential execution of function is just a possible
implementation of an abstract computational model that allows for the
concurrent execution of function. We use this framework as base for the
development of abstract computational models that allow for the concurrent
execution of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3384</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3384</id><created>2012-08-16</created><updated>2013-05-30</updated><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>On Range Searching with Semialgebraic Sets II</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in $\R^d$. We present a linear-size data
structure for answering range queries on $P$ with constant-complexity
semialgebraic sets as ranges, in time close to $O(n^{1-1/d})$. It essentially
matches the performance of similar structures for simplex range searching, and,
for $d\ge 5$, significantly improves earlier solutions by the first two authors
obtained in~1994. This almost settles a long-standing open problem in range
searching.
  The data structure is based on the polynomial-partitioning technique of Guth
and Katz [arXiv:1011.4105], which shows that for a parameter $r$, $1 &lt; r \le
n$, there exists a $d$-variate polynomial $f$ of degree $O(r^{1/d})$ such that
each connected component of $\R^d\setminus Z(f)$ contains at most $n/r$ points
of $P$, where $Z(f)$ is the zero set of $f$. We present an efficient randomized
algorithm for computing such a polynomial partition, which is of independent
interest and is likely to have additional applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3390</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3390</id><created>2012-08-16</created><updated>2013-01-09</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author></authors><title>A Unified Linear MSE Minimization MIMO Beamforming Design Based on
  Quadratic Matrix Programming</title><categories>cs.IT math.IT</categories><comments>6 Pages WCSP 2012 Final Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a unified linear transceiver design with
mean-square-error (MSE) as the objective function for a wide range of wireless
systems. The unified design is based on an elegant mathematical programming
technology namely quadratic matrix programming (QMP). It is revealed that for
different wireless systems such as multi-cell coordination systems, multi-user
MIMO systems, MIMO cognitive radio systems, amplify-and-forward MIMO relaying
systems, the MSE minimization beamforming design problems can always be solved
by solving a number of QMP problems. A comprehensive framework on how to solve
QMP problems is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3391</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3391</id><created>2012-08-16</created><authors><author><keyname>Ackerman</keyname><forenames>Margareta</forenames></author><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author></authors><title>Research Quality, Fairness, and Authorship Order</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The order in which authors are listed on an academic paper determines the
credit that each receives on a co-authored publication, influencing hiring,
tenure and promotions. Two of the prevalent author ordering schemes are
alphabetical, which involves listing authors in lexicographical order of their
last names, implying that all contributed equally, and by contribution, where
authors are listed in decreasing order of their contribution to the paper. We
perform a game theoretic analysis of the impact of author ordering schemes,
uncovering two considerable advantages of alphabetical ordering: it leads to
improved research quality, and it is the more fair of the two approaches in the
worst case. On the other hand, contribution-based ordering results in a denser
collaboration network and a greater number of publications than is achieved
using alphabetical author ordering. Furthermore, authors can overcome some of
the limitations of contribution-based ordering by performing rotations,
alternating who is the first author on joint papers. This often allows authors
to achieve optimal research quality and perfect fairness under any given
contribution scheme; however, this is obtained at the expense of truthfulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3394</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3394</id><created>2012-08-16</created><authors><author><keyname>Nah</keyname><forenames>Seungahn</forenames></author><author><keyname>Saxton</keyname><forenames>Gregory D.</forenames></author></authors><title>Modeling the adoption and use of social media by nonprofit organizations</title><categories>cs.CY cs.HC</categories><comments>Seungahn Nah and Gregory D. Saxton. (in press). Modeling the adoption
  and use of social media by nonprofit organizations. New Media &amp; Society,
  forthcoming</comments><doi>10.1177/1461444812452411</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines what drives organizational adoption and use of social
media through a model built around four key factors - strategy, capacity,
governance, and environment. Using Twitter, Facebook, and other data on 100
large US nonprofit organizations, the model is employed to examine the
determinants of three key facets of social media utilization: 1) adoption, 2)
frequency of use, and 3) dialogue. We find that organizational strategies,
capacities, governance features, and external pressures all play a part in
these social media adoption and utilization outcomes. Through its integrated,
multi-disciplinary theoretical perspective, this study thus helps foster
understanding of which types of organizations are able and willing to adopt and
juggle multiple social media accounts, to use those accounts to communicate
more frequently with their external publics, and to build relationships with
those publics through the sending of dialogic messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3398</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3398</id><created>2012-08-16</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>How Agreement and Disagreement Evolve over Random Dynamic Networks</title><categories>cs.SI cs.MA cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of an agreement protocol interacting with a disagreement process
over a common random network is considered. The model can represent the
spreading of true and false information over a communication network, the
propagation of faults in a large-scale control system, or the development of
trust and mistrust in a society. At each time instance and with a given
probability, a pair of network nodes are selected to interact. At random each
of the nodes then updates its state towards the state of the other node
(attraction), away from the other node (repulsion), or sticks to its current
state (neglect). Agreement convergence and disagreement divergence results are
obtained for various strengths of the updates for both symmetric and asymmetric
update rules. Impossibility theorems show that a specific level of attraction
is required for almost sure asymptotic agreement and a specific level of
repulsion is required for almost sure asymptotic disagreement. A series of
sufficient and/or necessary conditions are then established for agreement
convergence or disagreement divergence. In particular, under symmetric updates,
a critical convergence measure in the attraction and repulsion update strength
is found, in the sense that the asymptotic property of the network state
evolution transits from agreement convergence to disagreement divergence when
this measure goes from negative to positive. The result can be interpreted as a
tight bound on how much bad action needs to be injected in a dynamic network in
order to consistently steer its overall behavior away from consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3422</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3422</id><created>2012-08-16</created><updated>2013-01-08</updated><authors><author><keyname>Xu</keyname><forenames>Zhixiang</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Chapelle</keyname><forenames>Olivier</forenames></author></authors><title>Distance Metric Learning for Kernel Machines</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in metric learning has significantly improved the
state-of-the-art in k-nearest neighbor classification. Support vector machines
(SVM), particularly with RBF kernels, are amongst the most popular
classification algorithms that uses distance metrics to compare examples. This
paper provides an empirical analysis of the efficacy of three of the most
popular Mahalanobis metric learning algorithms as pre-processing for SVM
training. We show that none of these algorithms generate metrics that lead to
particularly satisfying improvements for SVM-RBF classification. As a remedy we
introduce support vector metric learning (SVML), a novel algorithm that
seamlessly combines the learning of a Mahalanobis metric with the training of
the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine
benchmark data sets of varying sizes and difficulties. In our study, SVML
outperforms all alternative state-of-the-art metric learning algorithms in
terms of accuracy and establishes itself as a serious alternative to the
standard Euclidean metric with model selection by cross validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3428</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3428</id><created>2012-08-16</created><updated>2012-09-14</updated><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>Comparative Bi-stochastizations and Associated
  Clusterings/Regionalizations of the 1995-2000 U. S. Intercounty Migration
  Network</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>33 pages, 12 figures, restructured and expanded text</comments><msc-class>91C20, 62H30, 05C82</msc-class><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wang, Li and Konig have recently compared the cluster-theoretic properties of
bi-stochasticized symmetric data similarity (e. g. kernel) matrices, produced
by minimizing two different forms of Bregman divergences. We extend their
investigation to non-symmetric matrices, specifically studying the 1995-2000 U.
S. 3,107 x 3,107 intercounty migration matrix. A particular bi-stochastized
form of it had been obtained (arXiv:1207.0437), using the well-established
Sinkhorn-Knopp (SK) (biproportional) algorithm--which minimizes the
Kullback-Leibler form of the divergence. This matrix has but a single entry
equal to (the maximal possible value of) 1. Highly contrastingly, the
bi-stochastic matrix obtained here, implementing the Wang-Li-Konig-algorithm
for the minimum of the alternative, squared-norm form of the divergence, has
2,707 such unit entries. The corresponding 3,107-vertex, 2,707-link directed
graph has 2,352 strong components. These consist of 1,659 single/isolated
counties, 654 doublets (thirty-one interstate in nature), 22 triplets (one
being interstate), 13 quartets (one being interstate), three quintets and one
septet. Not manifest in these graph-theoretic results, however, are the
five-county states of Hawaii and Rhode Island and the eight-county state of
Connecticut. These--among other regional configurations--appealingly emerged as
well-defined entities in the SK-based strong-component hierarchical clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3432</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3432</id><created>2012-08-15</created><authors><author><keyname>Badami</keyname><forenames>Mahsa</forenames></author><author><keyname>Hamzeh</keyname><forenames>Ali</forenames></author><author><keyname>Hashemi</keyname><forenames>Sattar</forenames></author></authors><title>A Novel Strategy Selection Method for Multi-Objective Clustering
  Algorithms Using Game Theory</title><categories>cs.GT cs.AI</categories><comments>9 Pages, 9 Figures; International Journal of Computer Science Issues
  2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The most important factors which contribute to the efficiency of
game-theoretical algorithms are time and game complexity. In this study, we
have offered an elegant method to deal with high complexity of game theoretic
multi-objective clustering methods in large-sized data sets. Here, we have
developed a method which selects a subset of strategies from strategies profile
for each player. In this case, the size of payoff matrices reduces
significantly which has a remarkable impact on time complexity. Therefore,
practical problems with more data are tractable with less computational
complexity. Although strategies set may grow with increasing the number of data
points, the presented model of strategy selection reduces the strategy space,
considerably, where clusters are subdivided into several sub-clusters in each
local game. The remarkable results demonstrate the efficiency of the presented
approach in reducing computational complexity of the problem of concern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3461</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3461</id><created>2012-08-16</created><authors><author><keyname>Vishal</keyname><forenames>Vivek</forenames></author><author><keyname>Gugwad</keyname><forenames>Sagar</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Modeling and Verification of Agent based Adaptive Traffic Signal using
  Symbolic Model Verifier</title><categories>cs.SE cs.LO</categories><comments>13 pages, 6 figures, Submitted to International Journal of Computer
  Application (IJCA)</comments><doi>10.5120/8402-2321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the issue of modeling and verification of a Multi Agent
System (MAS) scenario. We have considered an agent based adaptive traffic
signal system. The system monitors the smooth flow of traffic at intersection
of two road segment. After describing how the adaptive traffic signal system
can efficiently be used and showing its advantages over traffic signals with
predetermined periods, we have shown how we can transform this scenario into
Finite State Machine (FSM). Once the system is transformed into a FSM, we have
verified the specifications specified in Computational Tree Logic(CTL) using
NuSMV as a model checking tool. Simulation results obtained from NuSMV showed
us whether the system satisfied the specifications or not. It has also showed
us the state where the system specification does not hold. Using which we
traced back our system to find the source, leading to the specification
violation. Finally, we again verified the modified system with NuSMV for its
specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3486</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3486</id><created>2012-08-14</created><authors><author><keyname>Ramana</keyname><forenames>V. Venkata</forenames></author><author><keyname>Reddy</keyname><forenames>A. Rama Mohan</forenames></author><author><keyname>Sekaran</keyname><forenames>K. Chandra</forenames></author></authors><title>Bio Inspired Approach to Secure Routing in MANETs</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, the author explore the challenges with respect to the security
aspect in MANETs and propose a new approach which makes use of a bio-inspired
methodology. This paper elaborates various attacks which can be perpetrated on
MANETs and current solutions to the aforementioned problems, and then it
describes a Bio-Inspired Method which could be a possible solution to security
issues in MANETs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3512</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3512</id><created>2012-08-16</created><authors><author><keyname>Kubota</keyname><forenames>Toshiro</forenames></author></authors><title>Contour Completion Around a Fixation Point</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents two edge grouping algorithms for finding a closed contour
starting from a particular edge point and enclosing a fixation point. Both
algorithms search a shortest simple cycle in \textit{an angularly ordered
graph} derived from an edge image where a vertex is an end point of a contour
fragment and an undirected arc is drawn between a pair of end-points whose
visual angle from the fixation point is less than a threshold value, which is
set to $\pi/2$ in our experiments. The first algorithm restricts the search
space by disregarding arcs that cross the line extending from the fixation
point to the starting point. The second algorithm improves the solution of the
first algorithm in a greedy manner. The algorithms were tested with a large
number of natural images with manually placed fixation and starting points. The
results are promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3530</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3530</id><created>2012-08-17</created><authors><author><keyname>Dutta</keyname><forenames>Haimonti</forenames></author><author><keyname>Chan</keyname><forenames>William</forenames></author><author><keyname>Shankargouda</keyname><forenames>Deepak</forenames></author><author><keyname>Pooleery</keyname><forenames>Manoj</forenames></author><author><keyname>Radeva</keyname><forenames>Axinia</forenames></author><author><keyname>Rego</keyname><forenames>Kyle</forenames></author><author><keyname>Xie</keyname><forenames>Boyi</forenames></author><author><keyname>Passonneau</keyname><forenames>Rebecca</forenames></author><author><keyname>Lee</keyname><forenames>Austin</forenames></author><author><keyname>Taranto</keyname><forenames>Barbara</forenames></author></authors><title>Leveraging Subjective Human Annotation for Clustering Historic Newspaper
  Articles</title><categories>cs.IR cs.CL cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The New York Public Library is participating in the Chronicling America
initiative to develop an online searchable database of historically significant
newspaper articles. Microfilm copies of the newspapers are scanned and high
resolution Optical Character Recognition (OCR) software is run on them. The
text from the OCR provides a wealth of data and opinion for researchers and
historians. However, categorization of articles provided by the OCR engine is
rudimentary and a large number of the articles are labeled editorial without
further grouping. Manually sorting articles into fine-grained categories is
time consuming if not impossible given the size of the corpus. This paper
studies techniques for automatic categorization of newspaper articles so as to
enhance search and retrieval on the archive. We explore unsupervised (e.g.
KMeans) and semi-supervised (e.g. constrained clustering) learning algorithms
to develop article categorization schemes geared towards the needs of
end-users. A pilot study was designed to understand whether there was unanimous
agreement amongst patrons regarding how articles can be categorized. It was
found that the task was very subjective and consequently automated algorithms
that could deal with subjective labels were used. While the small scale pilot
study was extremely helpful in designing machine learning algorithms, a much
larger system needs to be developed to collect annotations from users of the
archive. The &quot;BODHI&quot; system currently being developed is a step in that
direction, allowing users to correct wrongly scanned OCR and providing keywords
and tags for newspaper articles used frequently. On successful implementation
of the beta version of this system, we hope that it can be integrated with
existing software being developed for the Chronicling America project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3533</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3533</id><created>2012-08-17</created><updated>2013-06-26</updated><authors><author><keyname>Drosou</keyname><forenames>Marina</forenames></author><author><keyname>Pitoura</keyname><forenames>Evaggelia</forenames></author></authors><title>DisC Diversity: Result Diversification based on Dissimilarity and
  Coverage</title><categories>cs.DB</categories><comments>To appear at the 39th International Conference on Very Large Data
  Bases (VLDB), August 26-31, 2013, Riva del Garda, Trento, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, result diversification has attracted a lot of attention as a means
to improve the quality of results retrieved by user queries. In this paper, we
propose a new, intuitive definition of diversity called DisC diversity. A DisC
diverse subset of a query result contains objects such that each object in the
result is represented by a similar object in the diverse subset and the objects
in the diverse subset are dissimilar to each other. We show that locating a
minimum DisC diverse subset is an NP-hard problem and provide heuristics for
its approximation. We also propose adapting DisC diverse subsets to a different
degree of diversification. We call this operation zooming. We present efficient
implementations of our algorithms based on the M-tree, a spatial index
structure, and experimentally evaluate their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3545</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3545</id><created>2012-08-17</created><authors><author><keyname>B&#xe9;daride</keyname><forenames>Nicolas</forenames></author><author><keyname>Fernique</keyname><forenames>Thomas</forenames></author></authors><title>The Ammann-Beenker tilings revisited</title><categories>math.CO cs.DM</categories><comments>7 pages, 4 figures</comments><msc-class>52C23, 05B45, 37B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces two tiles whose tilings form a one-parameter family of
tilings which can all be seen as digitization of two-dimensional planes in the
four-dimensional Euclidean space. This family contains the Ammann-Beenker
tilings as the solution of a simple optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3546</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3546</id><created>2012-08-17</created><updated>2014-07-01</updated><authors><author><keyname>Shi</keyname><forenames>ZiQiang</forenames></author><author><keyname>Zheng</keyname><forenames>TieRan</forenames></author><author><keyname>Han</keyname><forenames>JiQing</forenames></author></authors><title>Identifiability of multivariate logistic mixture models</title><categories>math.PR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture models have been widely used in modeling of continuous observations.
For the possibility to estimate the parameters of a mixture model consistently
on the basis of observations from the mixture, identifiability is a necessary
condition. In this study, we give some results on the identifiability of
multivariate logistic mixture models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3549</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3549</id><created>2012-08-17</created><updated>2013-06-23</updated><authors><author><keyname>Seslija</keyname><forenames>Marko</forenames></author><author><keyname>Scherpen</keyname><forenames>Jacquelien M. A.</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan</forenames></author></authors><title>Explicit Simplicial Discretization of Distributed-Parameter
  Port-Hamiltonian Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simplicial Dirac structures as finite analogues of the canonical Stokes-Dirac
structure, capturing the topological laws of the system, are defined on
simplicial manifolds in terms of primal and dual cochains related by the
coboundary operators. These finite-dimensional Dirac structures offer a
framework for the formulation of standard input-output finite-dimensional
port-Hamiltonian systems that emulate the behavior of distributed-parameter
port-Hamiltonian systems. This paper elaborates on the matrix representations
of simplicial Dirac structures and the resulting port-Hamiltonian systems on
simplicial manifolds. Employing these representations, we consider the
existence of structural invariants and demonstrate how they pertain to the
energy shaping of port-Hamiltonian systems on simplicial manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3557</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3557</id><created>2012-08-17</created><authors><author><keyname>Gupta</keyname><forenames>B. B.</forenames></author><author><keyname>Joshi</keyname><forenames>R. C.</forenames></author><author><keyname>Misra</keyname><forenames>Manoj</forenames></author></authors><title>Distributed Denial of Service Prevention Techniques</title><categories>cs.CR</categories><comments>ISSN: 1793-8198</comments><journal-ref>International Journal of Computer and Electrical Engineering
  (IJCEE), vol. 2, number 2, pp. 268-276, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The significance of the DDoS problem and the increased occurrence,
sophistication and strength of attacks has led to the dawn of numerous
prevention mechanisms. Each proposed prevention mechanism has some unique
advantages and disadvantages over the others. In this paper, we present a
classification of available mechanisms that are proposed in literature on
preventing Internet services from possible DDoS attacks and discuss the
strengths and weaknesses of each mechanism. This provides better understanding
of the problem and enables a security administrator to effectively equip his
arsenal with proper prevention mechanisms for fighting against DDoS threat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3561</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3561</id><created>2012-08-17</created><updated>2013-05-25</updated><authors><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Efficient Active Learning of Halfspaces: an Aggressive Approach</title><categories>cs.LG</categories><comments>Full version of: Gonen, Sabato and Shalev-Shwartz, &quot;Efficient Active
  Learning of Halfspaces: an Aggressive Approach&quot;, ICML 2013</comments><journal-ref>Journal of Machine Learning Research, 14(Sep):2487-2519, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study pool-based active learning of half-spaces. We revisit the aggressive
approach for active learning in the realizable case, and show that it can be
made efficient and practical, while also having theoretical guarantees under
reasonable assumptions. We further show, both theoretically and experimentally,
that it can be preferable to mellow approaches. Our efficient aggressive active
learner of half-spaces has formal approximation guarantees that hold when the
pool is separable with a margin. While our analysis is focused on the
realizable setting, we show that a simple heuristic allows using the same
algorithm successfully for pools with low error as well. We further compare the
aggressive approach to the mellow approach, and prove that there are cases in
which the aggressive approach results in significantly better label complexity
compared to the mellow approach. We demonstrate experimentally that substantial
improvements in label complexity can be achieved using the aggressive approach,
for both realizable and low-error settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3568</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3568</id><created>2012-08-17</created><updated>2013-05-19</updated><authors><author><keyname>Shapira</keyname><forenames>Asaf</forenames></author><author><keyname>Sudakov</keyname><forenames>Benny</forenames></author></authors><title>Small Complete Minors Above the Extremal Edge Density</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental result of Mader from 1972 asserts that a graph of high average
degree contains a highly connected subgraph with roughly the same average
degree. We prove a lemma showing that one can strengthen Mader's result by
replacing the notion of high connectivity by the notion of vertex expansion.
  Another well known result in graph theory states that for every integer t
there is a smallest real c(t) so that every n-vertex graph with c(t)n edges
contains a K_t-minor. Fiorini, Joret, Theis and Wood conjectured that if an
n-vertex graph G has (c(t)+\epsilon)n edges then G contains a K_t-minor of
order at most C(\epsilon)log n. We use our extension of Mader's theorem to
prove that such a graph G must contain a K_t-minor of order at most
C(\epsilon)log n loglog n. Known constructions of graphs with high girth show
that this result is tight up to the loglog n factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3576</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3576</id><created>2012-08-17</created><authors><author><keyname>Codabux-Rossan</keyname><forenames>Zadia</forenames></author><author><keyname>Doomun</keyname><forenames>M. Razvi</forenames></author></authors><title>AES CCMP Algorithm with N-Way Interleaved Cipher Block Chaining</title><categories>cs.CR</categories><comments>18 pages, University of Mauritius Research Journal - Volume 15 - 2009</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nowadays, the increased use of battery-powered mobile appliances and the urge
to access time-sensitive data anytime anywhere has fuelled a high demand for
wireless networks. However, wireless networks are susceptible to intrusion and
security problems. There is an inherent need to secure the wireless data
communication to ensure the confidentiality, authenticity, integrity and non
repudiation of the data being exchanged. On the other hand, the computation and
the resultant energy consumption to achieve sufficient security can be high.
Encryption algorithms are generally computationally intensive, and consume a
significant amount of computing resources (such as CPU time, memory, and
battery power). Considering the limited resources on wireless devices, it is
crucial that security protocols be implemented efficiently. This manuscript
focuses on how energy consumption is impacted by the use of unoptimised
AES-CCMP algorithms and proposes an optimized AES CCMP algorithm using 2-way
interleaving that does not compromise the security of wireless communication
sessions. There is also analysis of the performance of AES (a.k.a. Rijndael) in
its AES-CCMP implementation. The 2-way interleaving technique is an
optimization of the CBC-MAC that is investigated using two performance metrics
(namely encryption time and throughput).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3596</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3596</id><created>2012-08-17</created><updated>2012-10-01</updated><authors><author><keyname>Birkedal</keyname><forenames>Lars</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>M&#xf8;gelberg</keyname><forenames>Rasmus Ejlers</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Schwinghammer</keyname><forenames>Jan</forenames><affiliation>Saarland University</affiliation></author><author><keyname>St&#xf8;vring</keyname><forenames>Kristian</forenames><affiliation>DIKU, University of Copenhagen</affiliation></author></authors><title>First steps in synthetic guarded domain theory: step-indexing in the
  topos of trees</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.3.1; F.3.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October 3,
  2012) lmcs:1041</journal-ref><doi>10.2168/LMCS-8(4:1)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the topos S of trees as a model of guarded recursion. We study the
internal dependently-typed higher-order logic of S and show that S models two
modal operators, on predicates and types, which serve as guards in recursive
definitions of terms, predicates, and types. In particular, we show how to
solve recursive type equations involving dependent types. We propose that the
internal logic of S provides the right setting for the synthetic construction
of abstract versions of step-indexed models of programming languages and
program logics. As an example, we show how to construct a model of a
programming language with higher-order store and recursive types entirely
inside the internal logic of S. Moreover, we give an axiomatic categorical
treatment of models of synthetic guarded domain theory and prove that, for any
complete Heyting algebra A with a well-founded basis, the topos of sheaves over
A forms a model of synthetic guarded domain theory, generalizing the results
for S.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3598</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3598</id><created>2012-08-17</created><updated>2013-01-16</updated><authors><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Niu</keyname><forenames>Kai</forenames></author><author><keyname>Lin</keyname><forenames>Jia-Ru</forenames></author></authors><title>Improved Successive Cancellation Decoding of Polar Codes</title><categories>cs.IT math.IT</categories><comments>This paper is modified and submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As improved versions of successive cancellation (SC) decoding algorithm,
successive cancellation list (SCL) decoding and successive cancellation stack
(SCS) decoding are used to improve the finite-length performance of polar
codes. Unified descriptions of SC, SCL and SCS decoding algorithms are given as
path searching procedures on the code tree of polar codes. Combining the ideas
of SCL and SCS, a new decoding algorithm named successive cancellation hybrid
(SCH) is proposed, which can achieve a better trade-off between computational
complexity and space complexity. Further, to reduce the complexity, a pruning
technique is proposed to avoid unnecessary path searching operations.
Performance and complexity analysis based on simulations show that, with proper
configurations, all the three improved successive cancellation (ISC) decoding
algorithms can have a performance very close to that of maximum-likelihood (ML)
decoding with acceptable complexity. Moreover, with the help of the proposed
pruning technique, the complexities of ISC decoders can be very close to that
of SC decoder in the moderate and high signal-to-noise ratio (SNR) regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3600</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3600</id><created>2012-08-17</created><authors><author><keyname>Shrivastava</keyname><forenames>Piyush</forenames></author></authors><title>Modeling and Control of CSTR using Model based Neural Network Predictive
  Control</title><categories>cs.AI cs.NE nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a predictive control strategy based on neural network
model of the plant is applied to Continuous Stirred Tank Reactor (CSTR). This
system is a highly nonlinear process; therefore, a nonlinear predictive method,
e.g., neural network predictive control, can be a better match to govern the
system dynamics. In the paper, the NN model and the way in which it can be used
to predict the behavior of the CSTR process over a certain prediction horizon
are described, and some comments about the optimization procedure are made.
Predictive control algorithm is applied to control the concentration in a
continuous stirred tank reactor (CSTR), whose parameters are optimally
determined by solving quadratic performance index using the optimization
algorithm. An efficient control of the product concentration in cstr can be
achieved only through accurate model. Here an attempt is made to alleviate the
modeling difficulties using Artificial Intelligent technique such as Neural
Network. Simulation results demonstrate the feasibility and effectiveness of
the NNMPC technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3619</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3619</id><created>2012-08-17</created><updated>2013-02-22</updated><authors><author><keyname>Nguyen</keyname><forenames>Tin Chi</forenames></author><author><keyname>Deng</keyname><forenames>Nan</forenames></author><author><keyname>Zhu</keyname><forenames>Dongxiao</forenames></author></authors><title>SASeq: A Selective and Adaptive Shrinkage Approach to Detect and
  Quantify Active Transcripts using RNA-Seq</title><categories>q-bio.QM cs.CE q-bio.GN</categories><comments>The GUI software suite is freely available from
  http://sammate.sourceforge.net; Contact: tin.nguyenchi@wayne.edu,
  dzhu@wayne.edu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identification and quantification of condition-specific transcripts using
RNA-Seq is vital in transcriptomics research. While initial efforts using
mathematical or statistical modeling of read counts or per-base exonic signal
have been successful, they may suffer from model overfitting since not all the
reference transcripts in a database are expressed under a specific biological
condition. Standard shrinkage approaches, such as Lasso, shrink all the
transcript abundances to zero in a non-discriminative manner. Thus it does not
necessarily yield the set of condition-specific transcripts. Informed shrinkage
approaches, using the observed exonic coverage signal, are thus desirable.
Motivated by ubiquitous uncovered exonic regions in RNA-Seq data, termed as
&quot;naked exons&quot;, we propose a new computational approach that first filters out
the reference transcripts not supported by splicing and paired-end reads, then
followed by fitting a new mathematical model of per-base exonic coverage signal
and the underlying transcripts structure. We introduce a tuning parameter to
penalize the specific regions of the selected transcripts that were not
supported by the naked exons. Our approach compares favorably with the selected
competing methods in terms of both time complexity and accuracy using simulated
and real-world data. Our method is implemented in SAMMate, a GUI software suite
freely available from http://sammate.sourceforge.net
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3620</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3620</id><created>2012-08-16</created><updated>2013-02-18</updated><authors><author><keyname>Pabst</keyname><forenames>Oliver</forenames></author><author><keyname>Schmidt</keyname><forenames>Torsten</forenames></author></authors><title>Sinusoidal analysis of memristor bridge circuit-rectifier for low
  frequencies</title><categories>cond-mat.mtrl-sci cs.ET</categories><comments>6 pages, 14 figures; A new paper reference is included. It is cited
  and described shortly at top of section V</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoned by its dynamical behavior, the memristor enables a lot of new
applications in analog circuit design. Since some realizations are shown (e.g.
2007 by Hewlett Packard), the development of applications with memristors
becomes more and more interesting. Whereas most of the research was done in the
direction of memristor applications in neural networks and storage devices,
less publications deal with practical applications of analog memristive
circuits. But this topic is also promising further applications. Therefore,
this article proposes a frequency dependent rectifier memristor bridge for
different purposes (e.g. using as a programmable synaptic membrane voltage
generator for Spike-Time-Dependent-Plasticity) and describes the circuit
theory. In this context it is shown that the Picard Iteration is one
possibility to solve the system of nonlinear state equations of memristor
circuits analytically. An intuitive picture of how a memristor works in a
network in general is given as well. In this context some research on the
dynamical behavior of a HP memristor should be done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3623</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3623</id><created>2012-08-17</created><authors><author><keyname>Rafi</keyname><forenames>Muhammad</forenames></author><author><keyname>Hassan</keyname><forenames>Sundus</forenames></author><author><keyname>Shaikh</keyname><forenames>Mohammad Shahid</forenames></author></authors><title>Content-based Text Categorization using Wikitology</title><categories>cs.IR cs.AI</categories><comments>9 pages; IJCSI August 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major computational burden, while performing document clustering, is the
calculation of similarity measure between a pair of documents. Similarity
measure is a function that assign a real number between 0 and 1 to a pair of
documents, depending upon the degree of similarity between them. A value of
zero means that the documents are completely dissimilar whereas a value of one
indicates that the documents are practically identical. Traditionally,
vector-based models have been used for computing the document similarity. The
vector-based models represent several features present in documents. These
approaches to similarity measures, in general, cannot account for the semantics
of the document. Documents written in human languages contain contexts and the
words used to describe these contexts are generally semantically related.
Motivated by this fact, many researchers have proposed semantic-based
similarity measures by utilizing text annotation through external thesauruses
like WordNet (a lexical database). In this paper, we define a semantic
similarity measure based on documents represented in topic maps. Topic maps are
rapidly becoming an industrial standard for knowledge representation with a
focus for later search and extraction. The documents are transformed into a
topic map based coded knowledge and the similarity between a pair of documents
is represented as a correlation between the common patterns. The experimental
studies on the text mining datasets reveal that this new similarity measure is
more effective as compared to commonly used similarity measures in text
clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3629</identifier>
 <datestamp>2013-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3629</id><created>2012-08-17</created><updated>2013-09-04</updated><authors><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Zhou</keyname><forenames>Hang</forenames></author></authors><title>Sublinear-Time Algorithms for Monomer-Dimer Systems on Bounded Degree
  Graphs</title><categories>cs.DS cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $G$, let $Z(G,\lambda)$ be the partition function of the
monomer-dimer system defined by $\sum_k m_k(G)\lambda^k$, where $m_k(G)$ is the
number of matchings of size $k$ in $G$. We consider graphs of bounded degree
and develop a sublinear-time algorithm for estimating $\log Z(G,\lambda)$ at an
arbitrary value $\lambda&gt;0$ within additive error $\epsilon n$ with high
probability. The query complexity of our algorithm does not depend on the size
of $G$ and is polynomial in $1/\epsilon$, and we also provide a lower bound
quadratic in $1/\epsilon$ for this problem. This is the first analysis of a
sublinear-time approximation algorithm for a $# P$-complete problem. Our
approach is based on the correlation decay of the Gibbs distribution associated
with $Z(G,\lambda)$. We show that our algorithm approximates the probability
for a vertex to be covered by a matching, sampled according to this Gibbs
distribution, in a near-optimal sublinear time. We extend our results to
approximate the average size and the entropy of such a matching within an
additive error with high probability, where again the query complexity is
polynomial in $1/\epsilon$ and the lower bound is quadratic in $1/\epsilon$.
Our algorithms are simple to implement and of practical use when dealing with
massive datasets. Our results extend to other systems where the correlation
decay is known to hold as for the independent set problem up to the critical
activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3639</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3639</id><created>2012-08-17</created><authors><author><keyname>Benoit</keyname><forenames>Alexandre</forenames></author><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>van der Hoeven</keyname><forenames>Joris</forenames></author></authors><title>Quasi-optimal multiplication of linear differential operators</title><categories>cs.CC cs.SC</categories><comments>To appear in the Proceedings of the 53rd Annual IEEE Symposium on
  Foundations of Computer Science (FOCS'12)</comments><msc-class>68Q25, 34A30, 68W30</msc-class><acm-class>F.2.1; G.1.7; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that linear differential operators with polynomial coefficients over
a field of characteristic zero can be multiplied in quasi-optimal time. This
answers an open question raised by van der Hoeven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3653</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3653</id><created>2012-08-17</created><updated>2013-08-10</updated><authors><author><keyname>Nguyen</keyname><forenames>Tommy</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Using Location-Based Social Networks to Validate Human Mobility and
  Relationships Models</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 8 figures, SNAA</comments><journal-ref>Proc. 2012 IEEE/ACM Int. Conf. Advances in Social Networks
  Analysis and Mining, Istanbul, Turkey, August 26, 2012, pp. 1247-1253</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to use social networking data to validate mobility models for
pervasive mobile ad-hoc networks (MANETs) and delay tolerant networks (DTNs).
The Random Waypoint (RWP) and Erdos-Renyi (ER) models have been a popular
choice among researchers for generating mobility traces of nodes and
relationships between them. Not only RWP and ER are useful in evaluating
networking protocols in a simulation environment, but they are also used for
theoretical analysis of such dynamic networks. However, it has been observed
that neither relationships among people nor their movements are random.
Instead, human movements frequently contain repeated patterns and friendship is
bounded by distance. We used social networking site Gowalla to collect, create
and validate models of human mobility and relationships for analysis and
evaluations of applications in opportunistic networks such as sensor networks
and transportation models in civil engineering. In doing so, we hope to provide
more human-like movements and social relationship models to researchers to
study problems in complex and mobile networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3663</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3663</id><created>2012-08-17</created><updated>2014-06-25</updated><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Sadakane</keyname><forenames>Kunikiko</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo</forenames></author></authors><title>Space-Time Trade-offs for Stack-Based Algorithms</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In memory-constrained algorithms we have read-only access to the input, and
the number of additional variables is limited. In this paper we introduce the
compressed stack technique, a method that allows to transform algorithms whose
space bottleneck is a stack into memory-constrained algorithms. Given an
algorithm \alg\ that runs in O(n) time using $\Theta(n)$ variables, we can
modify it so that it runs in $O(n^2/s)$ time using a workspace of O(s)
variables (for any $s\in o(\log n)$) or $O(n\log n/\log p)$ time using $O(p\log
n/\log p)$ variables (for any $2\leq p\leq n$). We also show how the technique
can be applied to solve various geometric problems, namely computing the convex
hull of a simple polygon, a triangulation of a monotone polygon, the shortest
path between two points inside a monotone polygon, 1-dimensional pyramid
approximation of a 1-dimensional vector, and the visibility profile of a point
inside a simple polygon. Our approach exceeds or matches the best-known results
for these problems in constant-workspace models (when they exist), and gives
the first trade-off between the size of the workspace and running time. To the
best of our knowledge, this is the first general framework for obtaining
memory-constrained algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3665</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3665</id><created>2012-08-17</created><updated>2012-11-26</updated><authors><author><keyname>Christlein</keyname><forenames>Vincent</forenames></author><author><keyname>Riess</keyname><forenames>Christian</forenames></author><author><keyname>Jordan</keyname><forenames>Johannes</forenames></author><author><keyname>Riess</keyname><forenames>Corinna</forenames></author><author><keyname>Angelopoulou</keyname><forenames>Elli</forenames></author></authors><title>An Evaluation of Popular Copy-Move Forgery Detection Approaches</title><categories>cs.CV</categories><comments>Main paper: 14 pages, supplemental material: 12 pages, main paper
  appeared in IEEE Transaction on Information Forensics and Security</comments><acm-class>I.4.9</acm-class><journal-ref>IEEE Transactions on Information Forensics and Security, volume 7,
  number 6, 2012, pp. 1841-1854</journal-ref><doi>10.1109/TIFS.2012.2218597</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A copy-move forgery is created by copying and pasting content within the same
image, and potentially post-processing it. In recent years, the detection of
copy-move forgeries has become one of the most actively researched topics in
blind image forensics. A considerable number of different algorithms have been
proposed focusing on different types of postprocessed copies. In this paper, we
aim to answer which copy-move forgery detection algorithms and processing steps
(e.g., matching, filtering, outlier detection, affine transformation
estimation) perform best in various postprocessing scenarios. The focus of our
analysis is to evaluate the performance of previously proposed feature sets. We
achieve this by casting existing algorithms in a common pipeline. In this
paper, we examined the 15 most prominent feature sets. We analyzed the
detection performance on a per-image basis and on a per-pixel basis. We created
a challenging real-world copy-move dataset, and a software framework for
systematic image manipulation. Experiments show, that the keypoint-based
features SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA and
Zernike features perform very well. These feature sets exhibit the best
robustness against various noise sources and downsampling, while reliably
identifying the copied regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3667</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3667</id><created>2012-08-17</created><authors><author><keyname>Gjoka</keyname><forenames>Minas</forenames></author><author><keyname>Kurant</keyname><forenames>Maciej</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>2.5K-Graphs: from Sampling to Generation</title><categories>cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding network structure and having access to realistic graphs plays a
central role in computer and social networks research. In this paper, we
propose a complete, and practical methodology for generating graphs that
resemble a real graph of interest. The metrics of the original topology we
target to match are the joint degree distribution (JDD) and the
degree-dependent average clustering coefficient ($\bar{c}(k)$). We start by
developing efficient estimators for these two metrics based on a node sample
collected via either independence sampling or random walks. Then, we process
the output of the estimators to ensure that the target properties are
realizable. Finally, we propose an efficient algorithm for generating
topologies that have the exact target JDD and a $\bar{c}(k)$ close to the
target. Extensive simulations using real-life graphs show that the graphs
generated by our methodology are similar to the original graph with respect to,
not only the two target metrics, but also a wide range of other topological
metrics; furthermore, our generator is order of magnitudes faster than
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3670</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3670</id><created>2012-08-08</created><authors><author><keyname>Liu</keyname><forenames>Qiong</forenames></author></authors><title>A Survey of Recent View-based 3D Model Retrieval Methods</title><categories>cs.CV</categories><comments>15 pages. arXiv admin note: text overlap with arXiv:1207.7244 by
  other author without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensive research efforts have been dedicated to 3D model retrieval in
recent decades. Recently, view-based methods have attracted much research
attention due to the high discriminative property of multi-views for 3D object
representation. In this report, we summarize the view-based 3D model methods
and provide the further research trends. This paper focuses on the scheme for
matching between multiple views of 3D models and the application of
bag-of-visual-words method in 3D model retrieval. For matching between multiple
views, the many-to-many matching, probabilistic matching and semisupervised
learning methods are introduced. For bag-of-visual-words application in 3D
model retrieval, we first briefly review the bag-of-visual-words works on
multimedia and computer vision tasks, where the visual dictionary has been
detailed introduced. Then a series of 3D model retrieval methods by using
bag-of-visual-words description are surveyed in this paper. At last, we
summarize the further research content in view-based 3D model retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3681</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3681</id><created>2012-08-16</created><authors><author><keyname>Al-Khazali</keyname><forenames>Hisham A. H.</forenames></author><author><keyname>Askari</keyname><forenames>Mohamad R.</forenames></author></authors><title>Calculations of Frequency Response Functions (FRF) Using Computer Smart
  Office Software and Nyquist Plot under Gyroscopic Effect Rotation</title><categories>cs.CE</categories><comments>8 pages, 11 figures, 1 picture</comments><journal-ref>IRACST - International Journal of Computer Science and Information
  Technology &amp; Security (IJCSITS), Vol. 1, No. 2 (2011) pp. 90-97</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerated (FRF curves), synthesis of (FRF) curves there are two main
requirement in the form of response model, The first being that of regenerating
&quot;Theoretical&quot; curve for the frequency response function actually measured and
analysis and the second being that of synthesising the other functions which
were not measured,(FRF) that isolates the inherent dynamic properties of a
mechanical structure. Experimental modal parameters (frequency, damping, and
mode shape) are also obtained from a set of (FRF) measurements. The (FRF)
describes the input-output relationship between two points on a structure as a
function of frequency. Therefore, an (FRF) is actually defined between a single
input DOF (point &amp; direction), and a single output (DOF), although the FRF was
previously defined as a ratio of the Fourier transforms of an output and input
signal. In this paper we detection FRF curve using Nyquist plot under
gyroscopic effect in revolving structure using computer smart office software.
  Keywords - FRF curve; modal test; Nyquist plot; software engineering;
gyroscopic effect; smart office.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3687</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3687</id><created>2012-08-17</created><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Information-theoretic Dictionary Learning for Image Classification</title><categories>cs.CV cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a two-stage approach for learning dictionaries for object
classification tasks based on the principle of information maximization. The
proposed method seeks a dictionary that is compact, discriminative, and
generative. In the first stage, dictionary atoms are selected from an initial
dictionary by maximizing the mutual information measure on dictionary
compactness, discrimination and reconstruction. In the second stage, the
selected dictionary atoms are updated for improved reconstructive and
discriminative power using a simple gradient ascent algorithm on mutual
information. Experiments using real datasets demonstrate the effectiveness of
our approach for image classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3689</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3689</id><created>2012-08-17</created><authors><author><keyname>Bouaguel</keyname><forenames>Waad</forenames></author><author><keyname>Mufti</keyname><forenames>Ghazi Bel</forenames></author></authors><title>An improvement direction for filter selection techniques using
  information theory measures and quadratic optimization</title><categories>cs.LG cs.IT math.IT</categories><comments>4 pages, 2 tables, (IJARAI) International Journal of Advanced
  Research in Artificial Intelligence</comments><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence, 1:7-11, 8 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filter selection techniques are known for their simplicity and efficiency.
However this kind of methods doesn't take into consideration the features
inter-redundancy. Consequently the un-removed redundant features remain in the
final classification model, giving lower generalization performance. In this
paper we propose to use a mathematical optimization method that reduces
inter-features redundancy and maximize relevance between each feature and the
target variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3691</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3691</id><created>2012-08-17</created><authors><author><keyname>Doostmohammadian</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author></authors><title>On the genericity properties in networked estimation: Topology design
  and sensor placement</title><categories>cs.MA cs.IT math.IT</categories><comments>submitted for IEEE journal publication</comments><journal-ref>M. Doostmohammadian and U. A. Khan, &quot;On the genericity properties
  in distributed estimation: Topology design and sensor placement,&quot; in IEEE
  JSTSP--Adaptation and Learning over Complex Networks, vol. 7, no. 2, pp.
  195-204, Apr. 2013</journal-ref><doi>10.1109/JSTSP.2013.2246135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider networked estimation of linear, discrete-time
dynamical systems monitored by a network of agents. In order to minimize the
power requirement at the (possibly, battery-operated) agents, we require that
the agents can exchange information with their neighbors only \emph{once per
dynamical system time-step}; in contrast to consensus-based estimation where
the agents exchange information until they reach a consensus. It can be
verified that with this restriction on information exchange, measurement fusion
alone results in an unbounded estimation error at every such agent that does
not have an observable set of measurements in its neighborhood. To over come
this challenge, state-estimate fusion has been proposed to recover the system
observability. However, we show that adding state-estimate fusion may not
recover observability when the system matrix is structured-rank ($S$-rank)
deficient.
  In this context, we characterize the state-estimate fusion and measurement
fusion under both full $S$-rank and $S$-rank deficient system matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3700</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3700</id><created>2012-08-17</created><updated>2012-08-22</updated><authors><author><keyname>Borcea</keyname><forenames>Liliana</forenames></author><author><keyname>Callaghan</keyname><forenames>Thomas</forenames></author><author><keyname>Papanicolaou</keyname><forenames>George</forenames></author></authors><title>Synthetic Aperture Radar Imaging and Motion Estimation via Robust
  Principle Component Analysis</title><categories>cs.IT math.IT math.NA</categories><comments>33 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of synthetic aperture radar (SAR) imaging and motion
estimation of complex scenes. By complex we mean scenes with multiple targets,
stationary and in motion. We use the usual setup with one moving antenna
emitting and receiving signals. We address two challenges: (1) the detection of
moving targets in the complex scene and (2) the separation of the echoes from
the stationary targets and those from the moving targets. Such separation
allows high resolution imaging of the stationary scene and motion estimation
with the echoes from the moving targets alone. We show that the robust
principal component analysis (PCA) method which decomposes a matrix in two
parts, one low rank and one sparse, can be used for motion detection and data
separation. The matrix that is decomposed is the pulse and range compressed SAR
data indexed by two discrete time variables: the slow time, which parametrizes
the location of the antenna, and the fast time, which parametrizes the echoes
received between successive emissions from the antenna. We present an analysis
of the rank of the data matrix to motivate the use of the robust PCA method. We
also show with numerical simulations that successful data separation with
robust PCA requires proper data windowing. Results of motion estimation and
imaging with the separated data are presented, as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3716</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3716</id><created>2012-08-17</created><updated>2012-12-25</updated><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Liu</keyname><forenames>Shaohui</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author><author><keyname>Xiong</keyname><forenames>Ruiqin</forenames></author><author><keyname>Ma</keyname><forenames>Siwei</forenames></author></authors><title>Improved Total Variation based Image Compressive Sensing Recovery by
  Nonlocal Regularization</title><categories>cs.CV</categories><comments>4 Pages, 1 figures, 3 tables, to be published at IEEE Int. Symposium
  of Circuits and Systems (ISCAS) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, total variation (TV) based minimization algorithms have achieved
great success in compressive sensing (CS) recovery for natural images due to
its virtue of preserving edges. However, the use of TV is not able to recover
the fine details and textures, and often suffers from undesirable staircase
artifact. To reduce these effects, this letter presents an improved TV based
image CS recovery algorithm by introducing a new nonlocal regularization
constraint into CS optimization problem. The nonlocal regularization is built
on the well known nonlocal means (NLM) filtering and takes advantage of
self-similarity in images, which helps to suppress the staircase effect and
restore the fine details. Furthermore, an efficient augmented Lagrangian based
algorithm is developed to solve the above combined TV and nonlocal
regularization constrained problem. Experimental results demonstrate that the
proposed algorithm achieves significant performance improvements over the
state-of-the-art TV based algorithm in both PSNR and visual perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3718</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3718</id><created>2012-08-17</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Xiong</keyname><forenames>Ruiqin</forenames></author><author><keyname>Zhao</keyname><forenames>Chen</forenames></author><author><keyname>Ma</keyname><forenames>Siwei</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author></authors><title>Exploiting Image Local And Nonlocal Consistency For Mixed
  Gaussian-Impulse Noise Removal</title><categories>cs.MM</categories><comments>6 pages, 4 figures, 3 tables, to be published at IEEE Int. Conf. on
  Multimedia &amp; Expo (ICME) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing image denoising algorithms can only deal with a single type of
noise, which violates the fact that the noisy observed images in practice are
often suffered from more than one type of noise during the process of
acquisition and transmission. In this paper, we propose a new variational
algorithm for mixed Gaussian-impulse noise removal by exploiting image local
consistency and nonlocal consistency simultaneously. Specifically, the local
consistency is measured by a hyper-Laplace prior, enforcing the local
smoothness of images, while the nonlocal consistency is measured by
three-dimensional sparsity of similar blocks, enforcing the nonlocal
self-similarity of natural images. Moreover, a Split-Bregman based technique is
developed to solve the above optimization problem efficiently. Extensive
experiments for mixed Gaussian plus impulse noise show that significant
performance improvements over the current state-of-the-art schemes have been
achieved, which substantiates the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3719</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3719</id><created>2012-08-17</created><updated>2013-03-06</updated><authors><author><keyname>Thornton</keyname><forenames>Chris</forenames></author><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>Auto-WEKA: Combined Selection and Hyperparameter Optimization of
  Classification Algorithms</title><categories>cs.LG</categories><comments>9 pages, 3 figures</comments><report-no>Technical Report TR-2012-05</report-no><acm-class>I.2.6; D.2.10; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many different machine learning algorithms exist; taking into account each
algorithm's hyperparameters, there is a staggeringly large number of possible
alternatives overall. We consider the problem of simultaneously selecting a
learning algorithm and setting its hyperparameters, going beyond previous work
that addresses these issues in isolation. We show that this problem can be
addressed by a fully automated approach, leveraging recent innovations in
Bayesian optimization. Specifically, we consider a wide range of feature
selection techniques (combining 3 search and 8 evaluator methods) and all
classification approaches implemented in WEKA, spanning 2 ensemble methods, 10
meta-methods, 27 base classifiers, and hyperparameter settings for each
classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup
09, variants of the MNIST dataset and CIFAR-10, we show classification
performance often much better than using standard selection/hyperparameter
optimization methods. We hope that our approach will help non-expert users to
more effectively identify machine learning algorithms and hyperparameter
settings appropriate to their applications, and hence to achieve improved
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3723</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3723</id><created>2012-08-17</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Chen</forenames></author><author><keyname>Xiong</keyname><forenames>Ruiqin</forenames></author><author><keyname>Ma</keyname><forenames>Siwei</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author></authors><title>Image Super-Resolution via Dual-Dictionary Learning And Sparse
  Representation</title><categories>cs.CV</categories><comments>4 pages, 4 figures, 1 table, to be published at IEEE Int. Symposium
  of Circuits and Systems (ISCAS) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning-based image super-resolution aims to reconstruct high-frequency (HF)
details from the prior model trained by a set of high- and low-resolution image
patches. In this paper, HF to be estimated is considered as a combination of
two components: main high-frequency (MHF) and residual high-frequency (RHF),
and we propose a novel image super-resolution method via dual-dictionary
learning and sparse representation, which consists of the main dictionary
learning and the residual dictionary learning, to recover MHF and RHF
respectively. Extensive experimental results on test images validate that by
employing the proposed two-layer progressive scheme, more image details can be
recovered and much better results can be achieved than the state-of-the-art
algorithms in terms of both PSNR and visual perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3728</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3728</id><created>2012-08-18</created><updated>2014-05-24</updated><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Online Learning with Predictable Sequences</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present methods for online linear optimization that take advantage of
benign (as opposed to worst-case) sequences. Specifically if the sequence
encountered by the learner is described well by a known &quot;predictable process&quot;,
the algorithms presented enjoy tighter bounds as compared to the typical worst
case bounds. Additionally, the methods achieve the usual worst-case regret
bounds if the sequence is not benign. Our approach can be seen as a way of
adding prior knowledge about the sequence within the paradigm of online
learning. The setting is shown to encompass partial and side information.
Variance and path-length bounds can be seen as particular examples of online
learning with simple predictable sequences.
  We further extend our methods and results to include competing with a set of
possible predictable processes (models), that is &quot;learning&quot; the predictable
process itself concurrently with using it to obtain better regret guarantees.
We show that such model selection is possible under various assumptions on the
available feedback. Our results suggest a promising direction of further
research with potential applications to stock market and time series
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3730</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3730</id><created>2012-08-18</created><authors><author><keyname>Castillo-Perez</keyname><forenames>Sergio</forenames></author><author><keyname>Garcia-Alfaro</keyname><forenames>Joaquin</forenames></author></authors><title>On the Use of Latency Graphs for the Construction of Tor Circuits</title><categories>cs.CR</categories><comments>34</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of anonymity-based infrastructures and anonymisers is a plausible
solution to mitigate privacy problems on the Internet. Tor (short for The onion
router) is a popular low-latency anonymity system that can be installed as an
end-user application on a wide range of operating systems to redirect the
traffic through a series of anonymising proxy circuits. The construction of
these circuits determines both the latency and the anonymity degree of the Tor
anonymity system. While some circuit construction strategies lead to delays
which are tolerated for activities like Web browsing, they can make the system
vulnerable to linking attacks. We evaluate in this paper three classical
strategies for the construction of Tor circuits, with respect to their
de-anonymisation risk and latency performance. We then develop a new circuit
selection algorithm that considerably reduces the success probability of
linking attacks while keeping a good degree of performance. We finally conduct
experiments on a real-world Tor deployment over PlanetLab. Our experimental
results confirm the validity of our strategy and its performance increase for
Web browsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3739</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3739</id><created>2012-08-18</created><authors><author><keyname>Meyer</keyname><forenames>S. J.</forenames></author></authors><title>Adding Methodological Testing to Naur's Anti-formalism</title><categories>cs.OH</categories><comments>8 pages, no figures, and 28 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peter Naur is the leading critic of formalist computing because of his
extensive writings that disprove the now dominate characterization of human
thought as cognitive information processing. Naur criticizes the ideological
position that only discourse that adopts computer inspired forms are
acceptable. Lakatosian philosophy of the methodology of scientific research
programmes (MSRP) is added to Naur's studies to allow testing of computing
theories. After discussing Naur's criticism of mechanical cognitive information
processing, I show how to add MSRP competition to Naur's descriptive
philosophy. Next, Naur's claim that computing can not become scientific until
organizational issues involving ideological suppression of discussions of
computing and human thinking are solved is corroborated by institutional
suppression of my 1970s attempts to criticize structured programming (SP).
  Various problems in computing related philosophy are discussed. First, I
argue that my MSRP based degenerating research programme disproof of SP is
better than Naur's programming as a human activity, Demillo's social processes
and Fetzer's unprovable causal nature. Three areas for post ideologically based
computing study are discussed: computing as a path to rediscovering 19th
century conceptions of infinity, axiom of choice testing facilitated by
computing and relation to physical theory, and testing concrete complexity
methods based on efficiency proof analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3747</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3747</id><created>2012-08-18</created><authors><author><keyname>Efraimidis</keyname><forenames>Pavlos S.</forenames></author><author><keyname>Koutsiamanis</keyname><forenames>Remous-Aris</forenames></author></authors><title>On Money as a Means of Coordination between Network Packets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we apply a common economic tool, namely money, to coordinate
network packets. In particular, we present a network economy, called
PacketEconomy, where each flow is modeled as a population of rational network
packets, and these packets can self-regulate their access to network resources
by mutually trading their positions in router queues. Every packet of the
economy has its price, and this price determines if and when the packet will
agree to buy or sell a better position. We consider a corresponding Markov
model of trade and show that there are Nash equilibria (NE) where queue
positions and money are exchanged directly between the network packets. This
simple approach, interestingly, delivers improvements even when fiat money is
used. We present theoretical arguments and experimental results to support our
claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3751</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3751</id><created>2012-08-18</created><updated>2014-03-07</updated><authors><author><keyname>Molinero</keyname><forenames>Xavier</forenames></author><author><keyname>Riquelme</keyname><forenames>Fabi&#xe1;n</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author></authors><title>Social Influence as a Voting System: a Complexity Analysis of Parameters
  and Properties</title><categories>cs.GT cs.CC</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple and altruistic multiagent system in which the agents are
eager to perform a collective task but where their real engagement depends on
the willingness to perform the task of other influential agents. We model this
scenario by an influence game, a cooperative simple game in which a team (or
coalition) of players succeeds if it is able to convince enough agents to
participate in the task (to vote in favor of a decision). We take the linear
threshold model as the influence model. We show first the expressiveness of
influence games showing that they capture the class of simple games. Then we
characterize the computational complexity of various problems on influence
games, including measures (length and width), values (Shapley-Shubik and
Banzhaf) and properties (of teams and players). Finally, we analyze those
problems for some particular extremal cases, with respect to the propagation of
influence, showing tighter complexity characterizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3769</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3769</id><created>2012-08-18</created><authors><author><keyname>Kabir</keyname><forenames>A. F. M Sultanul</forenames></author><author><keyname>Khan</keyname><forenames>Md. Razib Hayat</forenames></author><author><keyname>Haque</keyname><forenames>Abul Ahsan Md. Mahmudul</forenames></author><author><keyname>Mamun</keyname><forenames>Mohammad Saiful Islam</forenames></author></authors><title>WiMAX or Wi-Fi: The Best Suited Candidate Technology for Building
  Wireless Access Infrastructure</title><categories>cs.NI</categories><comments>6 pages,2 figures, Conference: ICLAN'2007, IEEE France</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a description of the existing wireless technology Wi-Fi
and WiMAX, and try to compare Wi-Fi (IEEE 802.11) and WiMAX (IEEE 802.16), with
respect to which technology provides a better solution to build a wireless
access infrastructure. Each technology is evaluated based on some key
characteristics. This paper concludes with a statement of, which technology
will be the best and most cost effective solution to end user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3771</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3771</id><created>2012-08-18</created><authors><author><keyname>Saha</keyname><forenames>Sumanta</forenames></author><author><keyname>Islam</keyname><forenames>Md. Safiqul</forenames></author><author><keyname>Hossen</keyname><forenames>Md. Sakhawat</forenames></author><author><keyname>Mamun</keyname><forenames>Mohammad Saiful Islam</forenames></author></authors><title>A Novel Overlay IDS For Wireless Sensor Networks</title><categories>cs.CR</categories><comments>5 pages, International Conference Wireless Applications and Computing
  2008(IADIS), ISBN: 978-972-8924-62-1 \c{opyright} 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Network (WSN) consists of low cost sensor nodes which cannot
afford to implement sophisticated security system in it. That is why intrusion
detection architecture for WSN is considerably different and difficult to
implement. Most of the current implementations are based on exchanging anomaly
signals among the leaf level sensors resulting in too much power consumption.
We propose a novel architecture for Intrusion Detection System (IDS) in WSN
based on Hierarchical Overlay Design (HOD) that will distribute the overall
responsibility of intrusion detection into entities and thus conserve memory
and power of the nodes. The architecture uses layered design with GSM cell like
structure based on special monitor nodes. The HOD structure enables the sensors
to communicate using far less messages and thus conserve precious power and
also saves memory by not implementing IDS module on each sensor. The proposal
also uses rippling of alarm through layers and thus ensures proper delivery to
the uppermost layer with redundancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3772</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3772</id><created>2012-08-18</created><authors><author><keyname>Mamun</keyname><forenames>Mohammad Saiful Islam</forenames></author><author><keyname>Kabir</keyname><forenames>A. F. M. Sultanul</forenames></author></authors><title>Hierarchical Design Based Intrusion Detection System For Wireless Ad hoc
  Network</title><categories>cs.CR</categories><comments>16 pages, International Journal of Network Security &amp; Its
  Applications (IJNSA), Vol.2, No.3, July 2010. arXiv admin note: text overlap
  with arXiv:1111.1933 by other authors</comments><doi>10.5121/ijnsa.2010.2307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, wireless ad hoc sensor network becomes popular both in civil
and military jobs. However, security is one of the significant challenges for
sensor network because of their deployment in open and unprotected environment.
As cryptographic mechanism is not enough to protect sensor network from
external attacks, intrusion detection system needs to be introduced. Though
intrusion prevention mechanism is one of the major and efficient methods
against attacks, but there might be some attacks for which prevention method is
not known. Besides preventing the system from some known attacks, intrusion
detection system gather necessary information related to attack technique and
help in the development of intrusion prevention system. In addition to
reviewing the present attacks available in wireless sensor network this paper
examines the current efforts to intrusion detection system against wireless
sensor network. In this paper we propose a hierarchical architectural design
based intrusion detection system that fits the current demands and restrictions
of wireless ad hoc sensor network. In this proposed intrusion detection system
architecture we followed clustering mechanism to build a four level
hierarchical network which enhances network scalability to large geographical
area and use both anomaly and misuse detection techniques for intrusion
detection. We introduce policy based detection mechanism as well as intrusion
response together with GSM cell concept for intrusion detection architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3773</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3773</id><created>2012-08-18</created><authors><author><keyname>Junior</keyname><forenames>Francisco Heron de Carvalho</forenames></author><author><keyname>Lins</keyname><forenames>Rafael Dueire</forenames></author></authors><title>Haskell_#: Coordinating Functional Processes</title><categories>cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Haskell#, a coordination language targeted at the
efficient implementation of parallel scientific applications on loosely coupled
parallel architectures, using the functional language Haskell. Examples of
applications, their implementation details and performance figures are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3774</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3774</id><created>2012-08-18</created><authors><author><keyname>Kabir</keyname><forenames>A. F. M. Sultanul</forenames></author><author><keyname>Mamun</keyname><forenames>Mohammad Saiful Islam</forenames></author></authors><title>Graphical Query Builder in Opportunistic Sensor Networks to discover
  Sensor Information</title><categories>cs.IR</categories><comments>15 pages, International Journal of Computer Applications (IJCA) (0975
  - 8887) Volume 15- No.1, February 2011</comments><journal-ref>International Journal of Computer Applications 15(1):11-25,
  February 2011. Published by Foundation of Computer Science</journal-ref><doi>10.5120/1913-2551 10.5120/1913-2551 10.5120/1913-2551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of sensor network applications are data-driven. We believe that query
is the most preferred way to discover sensor services. Normally users are
unaware of available sensors. Thus users need to pose different types of query
over the sensor network to get the desired information. Even users may need to
input more complicated queries with higher levels of aggregations, and requires
more complex interactions with the system. As the users have no prior knowledge
of the sensor data or services our aim is to develop a visual query interface
where users can feed more user friendly queries and machine can understand
those. In this paper work, we have developed an Interactive visual query
interface for the users. To accomplish this we have considered several use
cases and we have derived graphical representation of query from their text
based format for those use case scenario. We have facilitated the user by
extracting class, subclass and properties from Ontology. To do so we have
parsed OWL file in the user interface and based upon the parsed information
users build visual query. Later on we have translated the visual query
languages into SPARQL query, a machine understandable format which helps the
machine to communicate with the underlying technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3779</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3779</id><created>2012-08-18</created><updated>2013-04-21</updated><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Bensmail</keyname><forenames>Halima</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>Multiple graph regularized protein domain ranking</title><categories>cs.LG cs.CE cs.IR q-bio.QM</categories><comments>21 pages</comments><journal-ref>Jim Jing-Yan Wang, Halima Bensmail and Xin Gao: Multiple graph
  regularized protein domain ranking, BMC Bioinformatics (2012), 13:307</journal-ref><doi>10.1186/1471-2105-13-307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background Protein domain ranking is a fundamental task in structural
biology. Most protein domain ranking methods rely on the pairwise comparison of
protein domains while neglecting the global manifold structure of the protein
domain database. Recently, graph regularized ranking that exploits the global
structure of the graph defined by the pairwise similarities has been proposed.
However, the existing graph regularized ranking methods are very sensitive to
the choice of the graph model and parameters, and this remains a difficult
problem for most of the protein domain ranking methods.
  Results To tackle this problem, we have developed the Multiple Graph
regularized Ranking algorithm, MultiG- Rank. Instead of using a single graph to
regularize the ranking scores, MultiG-Rank approximates the intrinsic manifold
of protein domain distribution by combining multiple initial graphs for the
regularization. Graph weights are learned with ranking scores jointly and
automatically, by alternately minimizing an ob- jective function in an
iterative algorithm. Experimental results on a subset of the ASTRAL SCOP
protein domain database demonstrate that MultiG-Rank achieves a better ranking
performance than single graph regularized ranking methods and pairwise
similarity based ranking methods.
  Conclusion The problem of graph model and parameter selection in graph
regularized protein domain ranking can be solved effectively by combining
multiple graphs. This aspect of generalization introduces a new frontier in
applying multiple graphs to solving protein domain ranking applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3789</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3789</id><created>2012-08-18</created><updated>2014-08-25</updated><authors><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Kaligounder</keyname><forenames>Lakshmi</forenames></author></authors><title>On Global Stability of Financial Networks</title><categories>q-fin.GN cs.CE</categories><comments>arXiv admin note: text overlap with arXiv:1112.5687 by other authors.
  Prior title for this article was &quot;Contagion in Financial Networks: Measure,
  Evaluation and Implications&quot;</comments><msc-class>91B55, 91G99, 05C82</msc-class><acm-class>J.4</acm-class><journal-ref>Journal of Complex Networks, 2(3), 313-354, 2014</journal-ref><doi>10.1093/comnet/cnu004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent financial crisis have generated renewed interests in fragilities
of global financial networks among economists and regulatory authorities. In
particular, a potential vulnerability of the financial networks is the
&quot;financial contagion&quot; process in which insolvencies of individual entities
propagate through the &quot;web of dependencies&quot; to affect the entire system. In
this paper, we formalize an extension of a financial network model originally
proposed by Nier et al. for scenarios such as the OTC derivatives market,
define a suitable global stability measure for this model, and perform a
comprehensive empirical evaluation of this stability measure over more than
700,000 combinations of networks types and parameter combinations. Based on our
evaluations, we discover many interesting implications of our evaluations of
this stability measure, and derive topological properties and parameters
combinations that may be used to flag the network as a possible fragile
network. An interactive software FIN-STAB for computing the stability is
available from the website www2.cs.uic.edu/~dasgupta/financial-simulator-files
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3790</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3790</id><created>2012-08-18</created><authors><author><keyname>Chou</keyname><forenames>Tzu-Han</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Sayeed</keyname><forenames>Akbar M.</forenames></author></authors><title>Secret Key Generation from Sparse Wireless Channels: Ergodic Capacity
  and Secrecy Outage</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates generation of a secret key from a reciprocal wireless
channel. In particular we consider wireless channels that exhibit sparse
structure in the wideband regime and the impact of the sparsity on the secret
key capacity. We explore this problem in two steps. First, we study key
generation from a state-dependent discrete memoryless multiple source. The
state of source captures the effect of channel sparsity. Secondly, we consider
a wireless channel model that captures channel sparsity and correlation between
the legitimate users' channel and the eavesdropper's channel. Such dependency
can significantly reduce the secret key capacity.
  According to system delay requirements, two performance measures are
considered: (i) ergodic secret key capacity and (ii) outage probability. We
show that in the wideband regime when a white sounding sequence is adopted, a
sparser channel can achieve a higher ergodic secret key rate than a richer
channel can. For outage performance, we show that if the users generate secret
keys at a fraction of the ergodic capacity, the outage probability will decay
exponentially in signal bandwidth. Moreover, a larger exponent is achieved by a
richer channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3794</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3794</id><created>2012-08-18</created><authors><author><keyname>Chen</keyname><forenames>Qi</forenames></author><author><keyname>Prautzsch</keyname><forenames>Hartmut</forenames></author></authors><title>General Midpoint Subdivision</title><categories>cs.GR math.NA</categories><comments>19 pages, 13 figures</comments><msc-class>65D18, 65D17, 68U07, 68U05</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce two generalizations of midpoint subdivision and
analyze the smoothness of the resulting subdivision surfaces at regular and
extraordinary points.
  The smoothing operators used in midpoint and mid-edge subdivision connect the
midpoints of adjacent faces or of adjacent edges, respectively. An arbitrary
combination of these two operators and the refinement operator that splits each
face with m vertices into m quadrilateral subfaces forms a general midpoint
subdivision operator. We analyze the smoothness of the resulting subdivision
surfaces by estimating the norm of a special second order difference scheme and
by using established methods for analyzing midpoint subdivision. The surfaces
are smooth at their regular points and they are also smooth at extraordinary
points for a certain subclass of general midpoint subdivision schemes.
  Generalizing the smoothing rules of non general midpoint subdivision schemes
around extraordinary and regular vertices or faces results in a class of
subdivision schemes, which includes the Catmull-Clark algorithm with restricted
parameters. We call these subdivision schemes generalized Catmull-Clark schemes
and we analyze their smoothness properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3798</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3798</id><created>2012-08-18</created><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author></authors><title>On-line Indexing for General Alphabets via Predecessor Queries on
  Subsets of an Ordered List</title><categories>cs.DS</categories><comments>Accepted to FOCS 2012, 17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Text Indexing is a fundamental algorithmic problem in which
one wishes to preprocess a text in order to quickly locate pattern queries
within the text. In the ever evolving world of dynamic and on-line data, there
is also a need for developing solutions to index texts which arrive on-line,
i.e. a character at a time, and still be able to quickly locate said patterns.
In this paper, a new solution for on-line indexing is presented by providing an
on-line suffix tree construction in $O(\log \log n + \log\log |\Sigma|)$
worst-case expected time per character, where $n$ is the size of the string,
and $\Sigma$ is the alphabet. This improves upon all previously known on-line
suffix tree constructions for general alphabets, at the cost of having the run
time in expectation.
  The main idea is to reduce the problem of constructing a suffix tree on-line
to an interesting variant of the order maintenance problem, which may be of
independent interest. In the famous order maintenance problem, one wishes to
maintain a dynamic list $L$ of size $n$ under insertions, deletions, and order
queries. In an order query, one is given two nodes from $L$ and must determine
which node precedes the other in $L$. In the Predecessor search on Dynamic
Subsets of an Ordered Dynamic List problem (POLP) it is also necessary to
maintain dynamic subsets of $L$ such that given some $u\in L$ it will be
possible to quickly locate the predecessor of $u$ in any subset. This paper
provides an efficient data structure capable of solving the POLP with
worst-case expected bounds that match the currently best known bounds for
predecessor search in the RAM model, improving over a solution which may be
implicitly obtained from Dietz [Die89].
  Furthermore, this paper improves or simplifies bounds for several additional
applications, including fully-persistent arrays and the Order-Maintenance
Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3802</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3802</id><created>2012-08-18</created><authors><author><keyname>Vashisth</keyname><forenames>Archana</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author></authors><title>OntoAna: Domain Ontology for Human Anatomy</title><categories>cs.AI</categories><comments>Proceedings of 5th CSI National Conference on Education and Research.
  Organized by Lingayay University, Faridabad. Sponsored by Computer Society of
  India and IEEE Delhi Chapter. Proceedings published by Lingayay University
  Press</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Today, we can find many search engines which provide us with information
which is more operational in nature. None of the search engines provide domain
specific information. This becomes very troublesome to a novice user who wishes
to have information in a particular domain. In this paper, we have developed an
ontology which can be used by a domain specific search engine. We have
developed an ontology on human anatomy, which captures information regarding
cardiovascular system, digestive system, skeleton and nervous system. This
information can be used by people working in medical and health care domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3805</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3805</id><created>2012-08-18</created><updated>2012-12-17</updated><authors><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>Paved with Good Intentions: Analysis of a Randomized Block Kaczmarz
  Method</title><categories>math.NA cs.NA</categories><msc-class>65F10, 65F20, 68W20, 41A65</msc-class><journal-ref>Linear Alg. Appl., Vol. 441, pp. 199-221, Jan. 2014</journal-ref><doi>10.1016/j.laa.2012.12.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The block Kaczmarz method is an iterative scheme for solving overdetermined
least-squares problems. At each step, the algorithm projects the current
iterate onto the solution space of a subset of the constraints. This paper
describes a block Kaczmarz algorithm that uses a randomized control scheme to
choose the subset at each step. This algorithm is the first block Kaczmarz
method with an (expected) linear rate of convergence that can be expressed in
terms of the geometric properties of the matrix and its submatrices. The
analysis reveals that the algorithm is most effective when it is given a good
row paving of the matrix, a partition of the rows into well-conditioned blocks.
The operator theory literature provides detailed information about the
existence and construction of good row pavings. Together, these results yield
an efficient block Kaczmarz scheme that applies to many overdetermined
least-squares problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3806</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3806</id><created>2012-08-19</created><updated>2013-09-28</updated><authors><author><keyname>Fu</keyname><forenames>Amy</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Dynamic Rate Adaptation for Improved Throughput and Delay in Wireless
  Network Coded Broadcast</title><categories>cs.IT math.IT</categories><comments>14 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide theoretical and simulation-based study of the
delivery delay performance of a number of existing throughput optimal coding
schemes and use the results to design a new dynamic rate adaptation scheme that
achieves improved overall throughput-delay performance.
  Under a baseline rate control scheme, the receivers' delay performance is
examined. Based on their Markov states, the knowledge difference between the
sender and receiver, three distinct methods for packet delivery are identified:
zero state, leader state and coefficient-based delivery. We provide analyses of
each of these and show that, in many cases, zero state delivery alone presents
a tractable approximation of the expected packet delivery behaviour.
Interestingly, while coefficient-based delivery has so far been treated as a
secondary effect in the literature, we find that the choice of coefficients is
extremely important in determining the delay, and a well chosen encoding scheme
can, in fact, contribute a significant improvement to the delivery delay.
  Based on our delivery delay model, we develop a dynamic rate adaptation
scheme which uses performance prediction models to determine the sender
transmission rate. Surprisingly, taking this approach leads us to the simple
conclusion that the sender should regulate its addition rate based on the total
number of undelivered packets stored at the receivers. We show that despite its
simplicity, our proposed dynamic rate adaptation scheme results in noticeably
improved throughput-delay performance over existing schemes in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3809</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3809</id><created>2012-08-19</created><updated>2012-08-24</updated><authors><author><keyname>Taghipour</keyname><forenames>Nima</forenames></author><author><keyname>Fierens</keyname><forenames>Daan</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Davis</keyname><forenames>Jesse</forenames></author><author><keyname>Blockeel</keyname><forenames>Hendrik</forenames></author></authors><title>Lifted Variable Elimination: A Novel Operator and Completeness Results</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various methods for lifted probabilistic inference have been proposed, but
our understanding of these methods and the relationships between them is still
limited, compared to their propositional counterparts. The only existing
theoretical characterization of lifting is for weighted first-order model
counting (WFOMC), which was shown to be complete domain-lifted for the class of
2-logvar models. This paper makes two contributions to lifted variable
elimination (LVE). First, we introduce a novel inference operator called group
inversion. Second, we prove that LVE augmented with this operator is complete
in the same sense as WFOMC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3811</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3811</id><created>2012-08-19</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>State distributions and minimum relative entropy noise sequences in
  uncertain stochastic systems: the discrete time case</title><categories>cs.SY cs.IT math.IT math.OC math.PR</categories><comments>30 pages, 1 figure, submitted to a SIAM journal</comments><msc-class>93C55, 94A17, 93B05, 93E15, 93E20, 60J05, 49L20, 90C40, 60G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with a dissipativity theory and robust performance
analysis of discrete-time stochastic systems driven by a statistically
uncertain random noise. The uncertainty is quantified by the conditional
relative entropy of the actual probability law of the noise with respect to a
nominal product measure corresponding to a white noise sequence. We discuss a
balance equation, dissipation inequality and superadditivity property for the
corresponding conditional relative entropy supply as a function of time. The
problem of minimizing the supply required to drive the system between given
state distributions over a specified time horizon is considered. Such
variational problems, involving entropy and probabilistic boundary conditions,
are known in the literature as Schroedinger bridge problems. In application to
control systems, this minimum required conditional relative entropy supply
characterizes the robustness of the system with respect to an uncertain noise.
We obtain a dynamic programming Bellman equation for the minimum required
conditional relative entropy supply and establish a Markov property of the
worst-case noise with respect to the state of the system. For multivariable
linear systems with a Gaussian white noise sequence as the nominal noise model
and Gaussian initial and terminal state distributions, the minimum required
supply is obtained using an algebraic Riccati equation which admits a
closed-form solution. We propose a computable robustness index for such systems
in the framework of an entropy theoretic formulation of uncertainty and provide
an example to illustrate this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3812</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3812</id><created>2012-08-19</created><authors><author><keyname>Chanda</keyname><forenames>Pritam</forenames></author><author><keyname>Zhang</keyname><forenames>Aidong</forenames></author><author><keyname>Ramanathan</keyname><forenames>Murali</forenames></author></authors><title>Algorithms for Efficient Mining of Statistically Significant Attribute
  Association Information</title><categories>cs.DB</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge of the association information between the attributes in a data set
provides insight into the underlying structure of the data and explains the
relationships (independence, synergy, redundancy) between the attributes and
class (if present). Complex models learnt computationally from the data are
more interpretable to a human analyst when such interdependencies are known. In
this paper, we focus on mining two types of association information among the
attributes - correlation information and interaction information for both
supervised (class attribute present) and unsupervised analysis (class attribute
absent). Identifying the statistically significant attribute associations is a
computationally challenging task - the number of possible associations
increases exponentially and many associations contain redundant information
when a number of correlated attributes are present. In this paper, we explore
efficient data mining methods to discover non-redundant attribute sets that
contain significant association information indicating the presence of
informative patterns in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3815</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3815</id><created>2012-08-19</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Hardy-Schatten Norms of Systems, Output Energy Cumulants and Linear
  Quadro-Quartic Gaussian Control</title><categories>cs.SY math.OC math.PR</categories><comments>14 pages, 3 figures, published in the Proceedings of the 19th
  International Symposium on Mathematical Theory of Networks and Systems, 5-9
  July 2010, Budapest, Hungary, pp. 2383-2390</comments><msc-class>93C05, 93C35, 93D25, 93E20, 93B52, 60H10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with linear stochastic control systems in state
space. The integral of the squared norm of the system output over a bounded
time interval is interpreted as energy. The cumulants of the output energy in
the infinite-horizon limit are related to Schatten norms of the system in the
Hardy space of transfer functions and the risk-sensitive performance index. We
employ a novel performance criterion which seeks to minimize a combination of
the average value and the variance of the output energy of the system per unit
time. The resulting linear quadro-quartic Gaussian control problem involves the
H2 and H4-norms of the closed-loop system. We obtain equations for the optimal
controller and outline a homotopy method which reduces the solution of the
problem to the numerical integration of a differential equation initialized by
the standard linear quadratic Gaussian controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3822</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3822</id><created>2012-08-19</created><updated>2013-04-05</updated><authors><author><keyname>Wang</keyname><forenames>Jingyan</forenames></author></authors><title>Joint-ViVo: Selecting and Weighting Visual Words Jointly for
  Bag-of-Features based Tissue Classification in Medical Images</title><categories>cs.CV stat.ML</categories><comments>This paper has been withdrawn by the author due to the terrible
  writing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically classifying the tissues types of Region of Interest (ROI) in
medical imaging has been an important application in Computer-Aided Diagnosis
(CAD), such as classification of breast parenchymal tissue in the mammogram,
classify lung disease patterns in High-Resolution Computed Tomography (HRCT)
etc. Recently, bag-of-features method has shown its power in this field,
treating each ROI as a set of local features. In this paper, we investigate
using the bag-of-features strategy to classify the tissue types in medical
imaging applications. Two important issues are considered here: the visual
vocabulary learning and weighting. Although there are already plenty of
algorithms to deal with them, all of them treat them independently, namely, the
vocabulary learned first and then the histogram weighted. Inspired by
Auto-Context who learns the features and classifier jointly, we try to develop
a novel algorithm that learns the vocabulary and weights jointly. The new
algorithm, called Joint-ViVo, works in an iterative way. In each iteration, we
first learn the weights for each visual word by maximizing the margin of ROI
triplets, and then select the most discriminate visual words based on the
learned weights for the next iteration. We test our algorithm on three tissue
classification tasks: identifying brain tissue type in magnetic resonance
imaging (MRI), classifying lung tissue in HRCT images, and classifying breast
tissue density in mammograms. The results show that Joint-ViVo can perform
effectively for classifying tissues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3830</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3830</id><created>2012-08-19</created><authors><author><keyname>Wei</keyname><forenames>Fajin</forenames></author><author><keyname>Lecchini-Visintini</keyname><forenames>Andrea</forenames></author></authors><title>On the Stability of Receding Horizon Control for Continuous-Time
  Stochastic Systems</title><categories>math.OC cs.SY</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stability of receding horizon control for continuous-time
non-linear stochastic differential equations. We illustrate the results with a
simulation example in which we employ receding horizon control to design an
investment strategy to repay a debt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3832</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3832</id><created>2012-08-19</created><updated>2012-08-30</updated><authors><author><keyname>He</keyname><forenames>Guanglei</forenames></author><author><keyname>Qin</keyname><forenames>Zhihui</forenames></author></authors><title>A New Algorithm for the Subtraction Games</title><categories>cs.GT</categories><comments>5 pages, 3 figures</comments><msc-class>91A05, 91A46</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subtraction games is a class of combinatorial games. It was solved since the
Sprague-Grundy Theory was put forward. This paper described a new algorithm for
subtraction games. The new algorithm can find win or lost positions in
subtraction games. In addition, it is much simpler than Sprague-Grundy Theory
in one pile of the games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3835</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3835</id><created>2012-08-19</created><updated>2013-05-02</updated><authors><author><keyname>Liao</keyname><forenames>Kewen</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author><author><keyname>Guo</keyname><forenames>Longkun</forenames></author></authors><title>Constrained Fault-Tolerant Resource Allocation</title><categories>cs.DS</categories><comments>33 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Constrained Fault-Tolerant Resource Allocation (FTRA) problem, we are
given a set of sites containing facilities as resources, and a set of clients
accessing these resources. Specifically, each site i is allowed to open at most
R_i facilities with cost f_i for each opened facility. Each client j requires
an allocation of r_j open facilities and connecting j to any facility at site i
incurs a connection cost c_ij. The goal is to minimize the total cost of this
resource allocation scenario.
  FTRA generalizes the Unconstrained Fault-Tolerant Resource Allocation
(FTRA_{\infty}) [18] and the classical Fault-Tolerant Facility Location (FTFL)
[13] problems: for every site i, FTRA_{\infty} does not have the constraint
R_i, whereas FTFL sets R_i=1. These problems are said to be uniform if all
r_j's are the same, and general otherwise.
  For the general metric FTRA, we first give an LP-rounding algorithm achieving
the approximation ratio of 4. Then we show the problem reduces to FTFL,
implying the ratio of 1.7245 from [3]. For the uniform FTRA, we provide a
1.52-approximation primal-dual algorithm in O(n^4) time, where n is the total
number of sites and clients. We also consider the Constrained Fault-Tolerant
k-Resource Allocation (k-FTRA) problem where additionally the total number of
facilities can be opened across all sites is bounded by k. For the uniform
k-FTRA, we give the first constant-factor approximation algorithm with a factor
of 4. Note that the above results carry over to FTRA_{\infty} and
k-FTRA_{\infty}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3836</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3836</id><created>2012-08-19</created><authors><author><keyname>Mannava</keyname><forenames>Vishnuvardhan</forenames></author><author><keyname>Ramesh</keyname><forenames>T.</forenames></author></authors><title>Composite Design Pattern for Feature Oriented Service Injection and
  Composition of Web Services for Distributed Computing Systems with Service
  Oriented Architecture</title><categories>cs.SE cs.DC cs.PL</categories><comments>12 pages, 7 figures, International Journal of Web &amp; Semantic
  Technology (IJWesT)</comments><acm-class>D.2.11; D.2.10; D.3.3</acm-class><journal-ref>Volume 3, Number 3, Page Number 73--84, July 2012</journal-ref><doi>10.5121/ijwest</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the advent of newly introduced programming models like Feature-Oriented
Programming (FOP), we feel that it will be more flexible to include the new
service invocation function into the service providing server as a Feature
Module for the self-adaptive distributed systems. A composite design patterns
shows a synergy that makes the composition more than just the sum of its parts
which leads to ready-made software architectures. In this paper we describe the
amalgamation of Visitor and Case-Based Reasoning Design Patterns to the
development of the Service Invocation and Web Services Composition through SOA
with the help of JWS technologies and FOP. As far as we know, there are no
studies on composition of design patterns for self adaptive distributed
computing domain. We have provided with the sample code developed for the
application and simple UML class diagram is used to describe the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3839</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3839</id><created>2012-08-19</created><updated>2013-04-03</updated><authors><author><keyname>Wang</keyname><forenames>Jing-Yan</forenames></author></authors><title>Discriminative Sparse Coding on Multi-Manifold for Data Representation
  and Classification</title><categories>cs.CV cs.LG stat.ML</categories><comments>This paper has been withdrawn by the author due to the terrible
  writing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding has been popularly used as an effective data representation
method in various applications, such as computer vision, medical imaging and
bioinformatics, etc. However, the conventional sparse coding algorithms and its
manifold regularized variants (graph sparse coding and Laplacian sparse
coding), learn the codebook and codes in a unsupervised manner and neglect the
class information available in the training set. To address this problem, in
this paper we propose a novel discriminative sparse coding method based on
multi-manifold, by learning discriminative class-conditional codebooks and
sparse codes from both data feature space and class labels. First, the entire
training set is partitioned into multiple manifolds according to the class
labels. Then, we formulate the sparse coding as a manifold-manifold matching
problem and learn class-conditional codebooks and codes to maximize the
manifold margins of different classes. Lastly, we present a data point-manifold
matching error based strategy to classify the unlabeled data point.
Experimental results on somatic mutations identification and breast tumors
classification in ultrasonic images tasks demonstrate the efficacy of the
proposed data representation-classification approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3840</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3840</id><created>2012-08-19</created><authors><author><keyname>Huque</keyname><forenames>MD Sirajul</forenames></author><author><keyname>Surekha</keyname><forenames>C</forenames></author><author><keyname>Kumar</keyname><forenames>A Kishore</forenames></author><author><keyname>Latha</keyname><forenames>Sri</forenames></author></authors><title>Modelling Dead Rocking In Online Multiplayer Games</title><categories>cs.NI</categories><comments>Published</comments><journal-ref>CSC V01 1002, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the web based application, here we analysis the network traffic which
occurs when the player plays an online game. Here we are going to trace the
current position of the player to rectify the traffic while playing the game.
There are different types of measures for different applications, those can be
normalized and compared with one another but my application can resolve the
inconsistency by knowing the positions quickly and focus on quality of network
(QON) which affects a player to leave the game in middle because of poor
quality of service (QOS). The existing model leads to leave the game because of
network loss from both sides. The proposed model can resolve this problem by
applying the replacement of TCP along with the dejitter buffer (DB) it can
reduce the network loss and by applying the DEAD RECKONING (DR) vector we can
recover the network access because it can view the current position of the
player through this we can rectify why the player leaving the game and checks
the network conditions and try to reactive the game by using mobile devices
automatically or else they receive the message according to the network
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3845</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3845</id><created>2012-08-19</created><updated>2013-04-03</updated><authors><author><keyname>Wang</keyname><forenames>Jing-Yan</forenames></author><author><keyname>AbdulJabbar</keyname><forenames>Mustafa</forenames></author></authors><title>Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix
  Factorization</title><categories>cs.LG cs.CV stat.ML</categories><comments>This paper has been withdrawn by the author due to the terrible
  writing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative Matrix Factorization (NMF) has been continuously evolving in
several areas like pattern recognition and information retrieval methods. It
factorizes a matrix into a product of 2 low-rank non-negative matrices that
will define parts-based, and linear representation of nonnegative data.
Recently, Graph regularized NMF (GrNMF) is proposed to find a compact
representation,which uncovers the hidden semantics and simultaneously respects
the intrinsic geometric structure. In GNMF, an affinity graph is constructed
from the original data space to encode the geometrical information. In this
paper, we propose a novel idea which engages a Multiple Kernel Learning
approach into refining the graph structure that reflects the factorization of
the matrix and the new data space. The GrNMF is improved by utilizing the graph
refined by the kernel learning, and then a novel kernel learning method is
introduced under the GrNMF framework. Our approach shows encouraging results of
the proposed algorithm in comparison to the state-of-the-art clustering
algorithms like NMF, GrNMF, SVD etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3848</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3848</id><created>2012-08-19</created><authors><author><keyname>Bruce</keyname><forenames>Doug</forenames><affiliation>Computational Biology Group, Department of Computer Science, University of Oxford</affiliation></author><author><keyname>Pathmanathan</keyname><forenames>Pras</forenames><affiliation>Computational Biology Group, Department of Computer Science, University of Oxford</affiliation></author><author><keyname>Whiteley</keyname><forenames>Jonathan P.</forenames><affiliation>Computational Biology Group, Department of Computer Science, University of Oxford</affiliation></author></authors><title>Modelling the effect of gap junctions on tissue-level cardiac
  electrophysiology</title><categories>cs.CE physics.bio-ph q-bio.CB q-bio.TO</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 1-15</journal-ref><doi>10.4204/EPTCS.92.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When modelling tissue-level cardiac electrophysiology, continuum
approximations to the discrete cell-level equations are used to maintain
computational tractability. One of the most commonly used models is represented
by the bidomain equations, the derivation of which relies on a homogenisation
technique to construct a suitable approximation to the discrete model. This
derivation does not explicitly account for the presence of gap junctions
connecting one cell to another. It has been seen experimentally [Rohr,
Cardiovasc. Res. 2004] that these gap junctions have a marked effect on the
propagation of the action potential, specifically as the upstroke of the wave
passes through the gap junction.
  In this paper we explicitly include gap junctions in a both a 2D discrete
model of cardiac electrophysiology, and the corresponding continuum model, on a
simplified cell geometry. Using these models we compare the results of
simulations using both continuum and discrete systems. We see that the form of
the action potential as it passes through gap junctions cannot be replicated
using a continuum model, and that the underlying propagation speed of the
action potential ceases to match up between models when gap junctions are
introduced. In addition, the results of the discrete simulations match the
characteristics of those shown in Rohr 2004. From this, we suggest that a
hybrid model -- a discrete system following the upstroke of the action
potential, and a continuum system elsewhere -- may give a more accurate
description of cardiac electrophysiology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3849</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3849</id><created>2012-08-19</created><authors><author><keyname>Testylier</keyname><forenames>Romain</forenames><affiliation>VERIMAG, Joseph Fourier University</affiliation></author><author><keyname>Dang</keyname><forenames>Thao</forenames><affiliation>CNRS/VERIMAG</affiliation></author></authors><title>Analysis of parametric biological models with non-linear dynamics</title><categories>cs.CE</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 16-29</journal-ref><doi>10.4204/EPTCS.92.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present recent results on parametric analysis of biological
models. The underlying method is based on the algorithms for computing
trajectory sets of hybrid systems with polynomial dynamics. The method is then
applied to two case studies of biological systems: one is a cardiac cell model
for studying the conditions for cardiac abnormalities, and the second is a
model of insect nest-site choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3850</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3850</id><created>2012-08-19</created><authors><author><keyname>Georgoulas</keyname><forenames>Anastasis</forenames></author><author><keyname>Clark</keyname><forenames>Allan</forenames></author><author><keyname>Ocone</keyname><forenames>Andrea</forenames></author><author><keyname>Gilmore</keyname><forenames>Stephen</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Guido</forenames></author></authors><title>A subsystems approach for parameter estimation of ODE models of hybrid
  systems</title><categories>cs.CE q-bio.QM</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 30-41</journal-ref><doi>10.4204/EPTCS.92.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for parameter identification of ODE system
descriptions based on data measurements. Our method works by splitting the
system into a number of subsystems and working on each of them separately,
thereby being easily parallelisable, and can also deal with noise in the
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3851</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3851</id><created>2012-08-19</created><authors><author><keyname>Mobilia</keyname><forenames>Nicolas</forenames><affiliation>UJF-Grenoble 1 / CNRS TIMC-IMAG UMR 5525</affiliation></author><author><keyname>Donz&#xe9;</keyname><forenames>Alexandre</forenames><affiliation>University of California Berkeley, EECS Department</affiliation></author><author><keyname>Moulis</keyname><forenames>Jean Marc</forenames><affiliation>UJF-Grenoble 1 / CNRS UMR 4952, Institut de Recherches en Technologies et Sciences pour le Vivant</affiliation></author><author><keyname>Fanchon</keyname><forenames>&#xc9;ric</forenames><affiliation>UJF-Grenoble 1 / CNRS TIMC-IMAG UMR 5525</affiliation></author></authors><title>A Model of the Cellular Iron Homeostasis Network Using Semi-Formal
  Methods for Parameter Space Exploration</title><categories>cs.CE q-bio.MN q-bio.QM</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 42-57</journal-ref><doi>10.4204/EPTCS.92.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel framework for the modeling of biological
networks. It makes use of recent tools analyzing the robust satisfaction of
properties of (hybrid) dynamical systems. The main challenge of this approach
as applied to biological systems is to get access to the relevant parameter
sets despite gaps in the available knowledge. An initial estimate of useful
parameters was sought by formalizing the known behavior of the biological
network in the STL logic using the tool Breach. Then, once a set of parameter
values consistent with known biological properties was found, we tried to
locally expand it into the largest possible valid region. We applied this
methodology in an effort to model and better understand the complex network
regulating iron homeostasis in mammalian cells. This system plays an important
role in many biological functions, including erythropoiesis, resistance against
infections, and proliferation of cancer cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3852</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3852</id><created>2012-08-19</created><authors><author><keyname>Casagrande</keyname><forenames>Alberto</forenames><affiliation>University of Trieste</affiliation></author><author><keyname>Dreossi</keyname><forenames>Tommaso</forenames><affiliation>University of Udine</affiliation></author><author><keyname>Piazza</keyname><forenames>Carla</forenames><affiliation>University of Udine</affiliation></author></authors><title>Hybrid Automata and \epsilon-Analysis on a Neural Oscillator</title><categories>cs.CE cs.LO cs.SC</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 58-72</journal-ref><doi>10.4204/EPTCS.92.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a hybrid model of a neural oscillator, obtained by
partially discretizing a well-known continuous model. Our construction points
out that in this case the standard techniques, based on replacing sigmoids with
step functions, is not satisfactory. Then, we study the hybrid model through
both symbolic methods and approximation techniques. This last analysis, in
particular, allows us to show the differences between the considered
approximation approaches. Finally, we focus on approximations via
epsilon-semantics, proving how these can be computed in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3853</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3853</id><created>2012-08-19</created><authors><author><keyname>Dluho&#x161;</keyname><forenames>Petr</forenames><affiliation>Masaryk University</affiliation></author><author><keyname>Brim</keyname><forenames>Lubo&#x161;</forenames><affiliation>Masaryk University</affiliation></author><author><keyname>&#x160;afr&#xe1;nek</keyname><forenames>David</forenames><affiliation>Masaryk University</affiliation></author></authors><title>On Expressing and Monitoring Oscillatory Dynamics</title><categories>cs.CE cs.LO cs.NA cs.SY</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 73-87</journal-ref><doi>10.4204/EPTCS.92.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To express temporal properties of dense-time real-valued signals, the Signal
Temporal Logic (STL) has been defined by Maler et al. The work presented a
monitoring algorithm deciding the satisfiability of STL formulae on finite
discrete samples of continuous signals. The logic has been used to express and
analyse biological systems, but it is not expressive enough to sufficiently
distinguish oscillatory properties important in biology. In this paper we
define the extended logic STL* in which STL is augmented with a signal-value
freezing operator allowing us to express (and distinguish) detailed properties
of biological oscillations. The logic is supported by a monitoring algorithm
prototyped in Matlab. The monitoring procedure of STL* is evaluated on a
biologically-relevant case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3854</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3854</id><created>2012-08-19</created><authors><author><keyname>Noel</keyname><forenames>Vincent</forenames><affiliation>University of Rennes 1</affiliation></author><author><keyname>Grigoriev</keyname><forenames>Dima</forenames><affiliation>CNRS, University of Lille</affiliation></author><author><keyname>Vakulenko</keyname><forenames>Sergei</forenames><affiliation>Russian Academy of Sciences, St. Petersburg</affiliation></author><author><keyname>Radulescu</keyname><forenames>Ovidiu</forenames><affiliation>University of Montpellier 2</affiliation></author></authors><title>Hybrid models of the cell cycle molecular machinery</title><categories>cs.CE cs.SY q-bio.QM</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 88-105</journal-ref><doi>10.4204/EPTCS.92.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Piecewise smooth hybrid systems, involving continuous and discrete variables,
are suitable models for describing the multiscale regulatory machinery of the
biological cells. In hybrid models, the discrete variables can switch on and
off some molecular interactions, simulating cell progression through a series
of functioning modes. The advancement through the cell cycle is the archetype
of such an organized sequence of events. We present an approach, inspired from
tropical geometry ideas, allowing to reduce, hybridize and analyse cell cycle
models consisting of polynomial or rational ordinary differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3855</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3855</id><created>2012-08-19</created><authors><author><keyname>Caravagna</keyname><forenames>Giulio</forenames><affiliation>Dipartimento di Informatica Sistemistica e Comunicazione, Universit&#xe0; degli Studi Milano-Bicocca, Italy</affiliation></author><author><keyname>Graudenzi</keyname><forenames>Alex</forenames><affiliation>Dipartimento di Informatica Sistemistica e Comunicazione, Universit&#xe0; degli Studi Milano-Bicocca, Italy</affiliation></author><author><keyname>Antoniotti</keyname><forenames>Marco</forenames><affiliation>Dipartimento di Informatica Sistemistica e Comunicazione, Universit&#xe0; degli Studi Milano-Bicocca, Italy</affiliation></author><author><keyname>Mauri</keyname><forenames>Giancarlo</forenames><affiliation>Dipartimento di Informatica Sistemistica e Comunicazione, Universit&#xe0; degli Studi Milano-Bicocca, Italy</affiliation></author><author><keyname>d'Onofrio</keyname><forenames>Alberto</forenames><affiliation>Department of Experimental Oncology, European Institute of Oncology, Italy</affiliation></author></authors><title>Effects of delayed immune-response in tumor immune-system interplay</title><categories>cs.CE q-bio.CB</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 106-121</journal-ref><doi>10.4204/EPTCS.92.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tumors constitute a wide family of diseases kinetically characterized by the
co-presence of multiple spatio-temporal scales. So, tumor cells ecologically
interplay with other kind of cells, e.g. endothelial cells or immune system
effectors, producing and exchanging various chemical signals. As such, tumor
growth is an ideal object of hybrid modeling where discrete stochastic
processes model agents at low concentrations, and mean-field equations model
chemical signals. In previous works we proposed a hybrid version of the
well-known Panetta-Kirschner mean-field model of tumor cells, effector cells
and Interleukin-2. Our hybrid model suggested -at variance of the inferences
from its original formulation- that immune surveillance, i.e. tumor elimination
by the immune system, may occur through a sort of side-effect of large
stochastic oscillations. However, that model did not account that, due to both
chemical transportation and cellular differentiation/division, the
tumor-induced recruitment of immune effectors is not instantaneous but,
instead, it exhibits a lag period. To capture this, we here integrate a
mean-field equation for Interleukins-2 with a bi-dimensional delayed stochastic
process describing such delayed interplay. An algorithm to realize trajectories
of the underlying stochastic process is obtained by coupling the Piecewise
Deterministic Markov process (for the hybrid part) with a Generalized
Semi-Markovian clock structure (to account for delays). We (i) relate tumor
mass growth with delays via simulations and via parametric sensitivity analysis
techniques, (ii) we quantitatively determine probabilistic eradication times,
and (iii) we prove, in the oscillatory regime, the existence of a heuristic
stochastic bifurcation resulting in delay-induced tumor eradication, which is
neither predicted by the mean-field nor by the hybrid non-delayed models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3856</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3856</id><created>2012-08-19</created><authors><author><keyname>David</keyname><forenames>Alexandre</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Du</keyname><forenames>Dehui</forenames><affiliation>East China Normal University</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>INRIA Rennes</affiliation></author><author><keyname>Miku&#x10d;ionis</keyname><forenames>Marius</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Poulsen</keyname><forenames>Danny B&#xf8;gsted</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Sedwards</keyname><forenames>Sean</forenames><affiliation>INRIA Rennes</affiliation></author></authors><title>Statistical Model Checking for Stochastic Hybrid Systems</title><categories>cs.CE cs.SE</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5; G.3</acm-class><journal-ref>EPTCS 92, 2012, pp. 122-136</journal-ref><doi>10.4204/EPTCS.92.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents novel extensions and applications of the UPPAAL-SMC model
checker. The extensions allow for statistical model checking of stochastic
hybrid systems. We show how our race-based stochastic semantics extends to
networks of hybrid systems, and indicate the integration technique applied for
implementing this semantics in the UPPAAL-SMC simulation engine. We report on
two applications of the resulting tool-set coming from systems biology and
energy aware buildings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3857</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3857</id><created>2012-08-19</created><authors><author><keyname>Loohuis</keyname><forenames>Loes Olde</forenames><affiliation>CUNY Computer Science</affiliation></author><author><keyname>Witzel</keyname><forenames>Andreas</forenames><affiliation>NYU Courant Institute</affiliation></author><author><keyname>Mishra</keyname><forenames>Bud</forenames><affiliation>NYU Courant Institute</affiliation></author></authors><title>Towards Cancer Hybrid Automata</title><categories>cs.SY cs.FL</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 92, 2012, pp. 137-151</journal-ref><doi>10.4204/EPTCS.92.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Cancer Hybrid Automata (CHAs), a formalism to model the
progression of cancers through discrete phenotypes. The classification of
cancer progression using discrete states like stages and hallmarks has become
common in the biology literature, but primarily as an organizing principle, and
not as an executable formalism. The precise computational model developed here
aims to exploit this untapped potential, namely, through automatic verification
of progression models (e.g., consistency, causal connections, etc.),
classification of unreachable or unstable states and computer-generated
(individualized or universal) therapy plans. The paper builds on a
phenomenological approach, and as such does not need to assume a model for the
biochemistry of the underlying natural progression. Rather, it abstractly
models transition timings between states as well as the effects of drugs and
clinical tests, and thus allows formalization of temporal statements about the
progression as well as notions of timed therapies. The model proposed here is
ultimately based on hybrid automata, and we show how existing controller
synthesis algorithms can be generalized to CHA models, so that therapies can be
generated automatically. Throughout this paper we use cancer hallmarks to
represent the discrete states through which cancer progresses, but other
notions of discretely or continuously varying state formalisms could also be
used to derive similar therapies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3858</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3858</id><created>2012-08-19</created><authors><author><keyname>Li&#xf2;</keyname><forenames>Pietro</forenames><affiliation>Computer Laboratory - University of Cambridge</affiliation></author><author><keyname>Merelli</keyname><forenames>Emanuela</forenames><affiliation>School of Science and Technology - University of Camerino</affiliation></author><author><keyname>Paoletti</keyname><forenames>Nicola</forenames><affiliation>School of Science and Technology - University of Camerino</affiliation></author></authors><title>Disease processes as hybrid dynamical systems</title><categories>cs.LO cs.CE cs.SY q-bio.QM</categories><comments>In Proceedings HSB 2012, arXiv:1208.3151</comments><proxy>EPTCS</proxy><acm-class>D.3.1; J.3; I.2.8</acm-class><journal-ref>EPTCS 92, 2012, pp. 152-166</journal-ref><doi>10.4204/EPTCS.92.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of hybrid techniques in complex processes of
infectious diseases. Since predictive disease models in biomedicine require a
multiscale approach for understanding the molecule-cell-tissue-organ-body
interactions, heterogeneous methodologies are often employed for describing the
different biological scales. Hybrid models provide effective means for complex
disease modelling where the action and dosage of a drug or a therapy could be
meaningfully investigated: the infection dynamics can be classically described
in a continuous fashion, while the scheduling of multiple treatment discretely.
We define an algebraic language for specifying general disease processes and
multiple treatments, from which a semantics in terms of hybrid dynamical system
can be derived. Then, the application of control-theoretic tools is proposed in
order to compute the optimal scheduling of multiple therapies. The
potentialities of our approach are shown in the case study of the SIR epidemic
model and we discuss its applicability on osteomyelitis, a bacterial infection
affecting the bone remodelling system in a specific and multiscale manner. We
report that formal languages are helpful in giving a general homogeneous
formulation for the different scales involved in a multiscale disease process;
and that the combination of hybrid modelling and control theory provides solid
grounds for computational medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3859</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3859</id><created>2012-08-19</created><authors><author><keyname>Vadgama</keyname><forenames>Vishal</forenames></author><author><keyname>Tanti</keyname><forenames>Bhavin</forenames></author><author><keyname>Modi</keyname><forenames>Chirag</forenames></author><author><keyname>Doshi</keyname><forenames>Nishant</forenames></author></authors><title>A novel approach for e-payment using virtual password system</title><categories>cs.CR</categories><comments>http://airccse.org/journal/ijcis/papers/1111ijcis01.pdf, 2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In today's world of E-Commerce everything comes online like Music, E-Books,
Shopping all most everything is online. If you are using some service or buying
things online then you have to pay for that. For that you have to do Net
Banking or you have to use Credit card which will do online payment for you. In
today's environment when everything is online, the service you are using for
E-Payment must be secure and you must protect your banking information like
debit card or credit card information from possible threat of hacking. There
were lots way to threat like Key logger, Forgery Detection, Phishing, Shoulder
surfing. Therefore, we reveal our actual information of Bank and Credit Card
then there will be a chance to lose data and same credit card and hackers can
use banking information for malicious purpose. In this paper we discuss
available E-Payment protocols, examine its advantages and delimitation's and
shows that there are steel needs to design a more secure E-Payment protocol.
The suggested protocol is based on using hash function and using dynamic or
virtual password, which protects your banking or credit card information from
possible threat of hacking when doing online transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3866</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3866</id><created>2012-08-19</created><updated>2012-08-20</updated><authors><author><keyname>Bardhan</keyname><forenames>Jaydeep P.</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Brune</keyname><forenames>Peter R.</forenames></author></authors><title>Analytical Nonlocal Electrostatics Using Eigenfunction Expansions of
  Boundary-Integral Operators</title><categories>physics.chem-ph cs.MS cs.NA</categories><comments>19 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an analytical solution to nonlocal continuum
electrostatics for an arbitrary charge distribution in a spherical solute. Our
approach relies on two key steps: (1) re-formulating the PDE problem using
boundary-integral equations, and (2) diagonalizing the boundary-integral
operators using the fact their eigenfunctions are the surface spherical
harmonics. To introduce this uncommon approach for analytical calculations in
separable geometries, we rederive Kirkwood's classic results for a protein
surrounded concentrically by a pure-water ion-exclusion layer and then a dilute
electrolyte (modeled with the linearized Poisson--Boltzmann equation). Our main
result, however, is an analytical method for calculating the reaction potential
in a protein embedded in a nonlocal-dielectric solvent, the Lorentz model
studied by Dogonadze and Kornyshev. The analytical method enables biophysicists
to study the new nonlocal theory in a simple, computationally fast way; an
open-source MATLAB implementation is included as supplemental information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3871</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3871</id><created>2012-08-19</created><authors><author><keyname>Marandi</keyname><forenames>Ali</forenames></author><author><keyname>Imani</keyname><forenames>Mahdi Faghi</forenames></author><author><keyname>Salamatian</keyname><forenames>Kave</forenames></author></authors><title>Optimization of Bloom Filter Parameters for Practical Bloom Filter Based
  Epidemic Forwarding in DTNs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic forwarding has been proposed as a forwarding technique to achieve
opportunistic communication in Delay Tolerant Networks. Even if this technique
is well known and widely referred, one has to first deal with several practical
problems before using it. In particular, in order to manage the redundancy and
to avoid useless transmissions, it has been proposed to ask nodes to exchange
information about the buffer content prior to sending information. While Bloom
filter has been proposed to transport the buffer content information, up to our
knowledge no real evaluation has been provided to study the tradeoff that
exists in practice. In this paper we describe an implementation of an epidemic
forwarding scheme using Bloom filters. Then we propose some strategies for
Bloom filter management based on windowing and describe implementation
tradeoffs. By simulating our proposed strategies in ns-3 both with random
waypoint mobility and realistic mobility traces coming from San Francisco
taxicabs, we show that our proposed strategies alleviate the challenge of using
epidemic forwarding in DTNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3874</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3874</id><created>2012-08-19</created><authors><author><keyname>Sergeev</keyname><forenames>Igor S.</forenames></author></authors><title>Upper bounds for the formula size of the majority function</title><categories>cs.DS</categories><comments>12 pages, in English; 13 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the counting function of n Boolean variables can be
implemented with the formulae of size O(n^3.06) over the basis of all 2-input
Boolean functions and of size O(n^4.54) over the standard basis. The same
bounds follow for the complexity of any threshold symmetric function of n
variables and particularly for the majority function. Any bit of the product of
binary numbers of length n can be computed by formulae of size O(n^4.06) or
O(n^5.54) depending on basis. Incidentally the bounds O(n^3.23) and O(n^4.82)
on the formula size of any symmetric function of n variables with respect to
the basis are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3876</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3876</id><created>2012-08-19</created><authors><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Digging Deeper into Deep Web Databases by Breaking Through the Top-k
  Barrier</title><categories>cs.DB</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of web databases are only accessible through proprietary
form-like interfaces which require users to query the system by entering
desired values for a few attributes. A key restriction enforced by such an
interface is the top-k output constraint - i.e., when there are a large number
of matching tuples, only a few (top-k) of them are preferentially selected and
returned by the website, often according to a proprietary ranking function.
Since most web database owners set k to be a small value, the top-k output
constraint prevents many interesting third-party (e.g., mashup) services from
being developed over real-world web databases. In this paper we consider the
novel problem of &quot;digging deeper&quot; into such web databases. Our main
contribution is the meta-algorithm GetNext that can retrieve the next ranked
tuple from the hidden web database using only the restrictive interface of a
web database without any prior knowledge of its ranking function. This
algorithm can then be called iteratively to retrieve as many top ranked tuples
as necessary. We develop principled and efficient algorithms that are based on
generating and executing multiple reformulated queries and inferring the next
ranked tuple from their returned results. We provide theoretical analysis of
our algorithms, as well as extensive experimental results over synthetic and
real-world databases that illustrate the effectiveness of our techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3882</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3882</id><created>2012-08-19</created><authors><author><keyname>de Carvalho-Junior</keyname><forenames>Francisco Heron</forenames></author><author><keyname>Lins</keyname><forenames>Rafael Dueire</forenames></author></authors><title>Coordination Level Modeling and Analysis of Parallel Programs using
  Petri Nets</title><categories>cs.DC cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last fifteen years, the high performance computing (HPC) community has
claimed for parallel programming environments that reconciles generality,
higher level of abstraction, portability, and efficiency for distributed-memory
parallel computing platforms. The Hash component model appears as an
alternative for addressing HPC community claims for fitting these requirements.
This paper presents foundations that will enable a parallel programming
environment based on the Hash model to address the problems of &quot;debugging&quot;,
performance evaluation and verification of formal properties of parallel
program by means of a powerful, simple, and widely adopted formalism: Petri
nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3887</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3887</id><created>2012-08-19</created><authors><author><keyname>Kruba</keyname><forenames>Steve</forenames></author><author><keyname>Baynes</keyname><forenames>Steven</forenames></author><author><keyname>Hyer</keyname><forenames>Robert</forenames></author></authors><title>BPM, Agile, and Virtualization Combine to Create Effective Solutions</title><categories>cs.SE</categories><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 7, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate of change in business and government is accelerating. A number of
techniques for addressing that change have emerged independently to provide for
automated solutions in this environment. This paper will examine three of the
most popular of these technologies-business process management, the agile
software development movement, and infrastructure virtualization-to expose the
commonalities in these approaches and how, when used together, their combined
effect results in rapidly deployed, more successful solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3890</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3890</id><created>2012-08-19</created><authors><author><keyname>Kruba</keyname><forenames>Steve</forenames></author></authors><title>Significance of Rapid Solutions Development to Business Process
  Management</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Science and Information Security
  USA Vol. 8 No. 4 July 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business process management (BPM) is moving from a niche market into the
mainstream. One of the factors leading to this transformation is the emergence
of very powerful rapid solutions development tools for creating BPM solutions
(BPM RSD). It has been widely recognized that this facility is important for
achieving benefits quickly. Similar benefits are attributed to the agile
software movement, but BPM RSD differs in that the objective is to reduce the
need for custom software development. As the BPM RSD features of some of the
current business process management suites (BPMS) products have matured,
additional benefits have emerged that fundamentally change the way we approach
solutions in this space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3901</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3901</id><created>2012-08-19</created><updated>2012-11-26</updated><authors><author><keyname>Olaizola</keyname><forenames>Igor G.</forenames></author><author><keyname>Quartulli</keyname><forenames>Marco</forenames></author><author><keyname>Florez</keyname><forenames>Julian</forenames></author><author><keyname>Sierra</keyname><forenames>Basilio</forenames></author></authors><title>Trace transform based method for color image domain identification</title><categories>cs.CV</categories><comments>This paper has been momentaneously withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context categorization is a fundamental pre-requisite for multi-domain
multimedia content analysis applications in order to manage contextual
information in an efficient manner. In this paper, we introduce a new color
image context categorization method (DITEC) based on the trace transform. The
problem of dimensionality reduction of the obtained trace transform signal is
addressed through statistical descriptors that keep the underlying information.
These extracted features offer a highly discriminant behavior for content
categorization. The theoretical properties of the method are analyzed and
validated experimentally through two different datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3933</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3933</id><created>2012-08-20</created><authors><author><keyname>Nouredine</keyname><forenames>Melab</forenames><affiliation>LIFL</affiliation></author><author><keyname>Chakroun</keyname><forenames>Imen</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Mohand</keyname><forenames>Mezmaz</forenames></author><author><keyname>Tuyttens</keyname><forenames>Daniel</forenames></author></authors><title>A GPU-accelerated Branch-and-Bound Algorithm for the Flow-Shop
  Scheduling Problem</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>14th IEEE International Conference on Cluster Computing,
  Cluster'12 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Branch-and-Bound (B&amp;B) algorithms are time intensive tree-based exploration
methods for solving to optimality combinatorial optimization problems. In this
paper, we investigate the use of GPU computing as a major complementary way to
speed up those methods. The focus is put on the bounding mechanism of B&amp;B
algorithms, which is the most time consuming part of their exploration process.
We propose a parallel B&amp;B algorithm based on a GPU-accelerated bounding model.
The proposed approach concentrate on optimizing data access management to
further improve the performance of the bounding mechanism which uses large and
intermediate data sets that do not completely fit in GPU memory. Extensive
experiments of the contribution have been carried out on well known FSP
benchmarks using an Nvidia Tesla C2050 GPU card. We compared the obtained
performances to a single and a multithreaded CPU-based execution. Accelerations
up to x100 are achieved for large problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3939</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3939</id><created>2012-08-20</created><authors><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Karlin</keyname><forenames>Anna R.</forenames></author><author><keyname>Koutsoupias</keyname><forenames>Elias</forenames></author><author><keyname>Vidali</keyname><forenames>Angelina</forenames></author></authors><title>Approaching Utopia: Strong Truthfulness and Externality-Resistant
  Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study strongly truthful mechanisms and their applications.
We use strongly truthful mechanisms as a tool for implementation in undominated
strategies for several problems,including the design of externality resistant
auctions and a variant of multi-dimensional scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3942</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3942</id><created>2012-08-20</created><updated>2016-03-03</updated><authors><author><keyname>Droste</keyname><forenames>Manfred</forenames></author><author><keyname>Vogler</keyname><forenames>Heiko</forenames></author></authors><title>The Chomsky-Sch\&quot;utzenberger Theorem for Quantitative Context-Free
  Languages</title><categories>cs.FL</categories><comments>This new version combines a conference and a journal paper of the
  authors on the same topic, see references [15,16], and supplements them by a
  few additional examples and more detailed proofs. It also corrects a mistake
  in Theorem 7.7 of the first arxiv version (the property sequential was
  missing)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted automata model quantitative aspects of systems like the consumption
of resources during executions. Traditionally, the weights are assumed to form
the algebraic structure of a semiring, but recently also other weight
computations like average have been considered. Here, we investigate
quantitative context-free languages over very general weight structures
incorporating all semirings, average computations, lattices, and more. In our
main result, we derive the fundamental Chomsky-Sch\&quot;utzenberger theorem for
such quantitative context-free languages, showing that each arises as the image
of a Dyck language and a regular language under a suitable morphism. Moreover,
we show that quantitative context-free language are expressively equivalent to
a model of weighted pushdown automata. This generalizes results previously
known only for semirings. We also investigate when quantitative context-free
languages assume only finitely many values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3943</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3943</id><created>2012-08-20</created><authors><author><keyname>Gholap</keyname><forenames>Jay</forenames></author></authors><title>Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility</title><categories>cs.LG cs.DB cs.PF stat.ML</categories><comments>5 Pages</comments><journal-ref>Published in Asian Journal of Computer Science and Information
  Technology,Vol 2,No. 8 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining involves the systematic analysis of large data sets, and data
mining in agricultural soil datasets is exciting and modern research area. The
productive capacity of a soil depends on soil fertility. Achieving and
maintaining appropriate levels of soil fertility, is of utmost importance if
agricultural land is to remain capable of nourishing crop production. In this
research, Steps for building a predictive model of soil fertility have been
explained.
  This paper aims at predicting soil fertility class using decision tree
algorithms in data mining . Further, it focuses on performance tuning of J48
decision tree algorithm with the help of meta-techniques such as attribute
selection and boosting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3952</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3952</id><created>2012-08-20</created><authors><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Sawitzki</keyname><forenames>Frank</forenames></author><author><keyname>Wira-Alam</keyname><forenames>Andias</forenames></author><author><keyname>L&#xfc;ke</keyname><forenames>Thomas</forenames></author></authors><title>Dealing with Sparse Document and Topic Representations: Lab Report for
  CHiC 2012</title><categories>cs.IR</categories><comments>12 pages, to appear in CLEF 2012 Labs and Workshop, Notebook Papers.
  Report for the CHiC Lab: Ad-hoc Retrieval Task and Semantic Enrichment Task</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will report on the participation of GESIS at the first CHiC workshop
(Cultural Heritage in CLEF). Being held for the first time, no prior experience
with the new data set, a document dump of Europeana with ca. 23 million
documents, exists. The most prominent issues that arose from pretests with this
test collection were the very unspecific topics and sparse document
representations. Only half of the topics (26/50) contained a description and
the titles were usually short with just around two words. Therefore we focused
on three different term suggestion and query expansion mechanisms to surpass
the sparse topical description. We used two methods that build on concept
extraction from Wikipedia and on a method that applied co-occurrence statistics
on the available Europeana corpus. In the following paper we will present the
approaches and preliminary results from their assessments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3966</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3966</id><created>2012-08-20</created><authors><author><keyname>Zhang</keyname><forenames>Zhifang</forenames></author></authors><title>Network Coding Based on Chinese Remainder Theorem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random linear network code has to sacrifice part of bandwidth to transfer the
coding vectors, thus a head of size k log|T| is appended to each packet. We
present a distributed random network coding approach based on the Chinese
remainder theorem for general multicast networks. It uses a couple of modulus
as the head, thus reduces the size of head to O(log k). This makes it more
suitable for scenarios where the number of source nodes is large and the
bandwidth is limited. We estimate the multicast rate and show it is
satisfactory in performance for randomly designed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3970</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3970</id><created>2012-08-20</created><authors><author><keyname>Miszczak</keyname><forenames>J. A.</forenames></author></authors><title>Employing online quantum random number generators for generating truly
  random quantum states in Mathematica</title><categories>physics.comp-ph cs.MS quant-ph</categories><comments>New version of the package described in arXiv:1102.4598. Software
  available at http://www.iitis.pl/~miszczak/trqs</comments><acm-class>G.3; G.4</acm-class><journal-ref>Comput. Phys. Commun., Vol. 184 (2013), pp. 257-258</journal-ref><doi>10.1016/j.cpc.2012.08.012</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a new version of TRQS package for Mathematica computing system.
The package allows harnessing quantum random number generators (QRNG) for
investigating the statistical properties of quantum states. It implements a
number of functions for generating random states. The new version of the
package adds the ability to use the on-line quantum random number generator
service and implements new functions for retrieving lists of random numbers.
Thanks to the introduced improvements, the new version provides faster access
to high-quality sources of random numbers and can be used in simulations
requiring large amount of random data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3976</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3976</id><created>2012-08-20</created><authors><author><keyname>Gagen</keyname><forenames>Michael J.</forenames></author></authors><title>Using strong isomorphisms to construct game strategy spaces</title><categories>cs.GT math.OC physics.data-an</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When applied to the same game, probability theory and game theory can
disagree on calculated values of the Fisher information, the log likelihood
function, entropy gradients, the rank and Jacobian of variable transforms, and
even the dimensionality and volume of the underlying probability parameter
spaces. These differences arise as probability theory employs structure
preserving isomorphic mappings when constructing strategy spaces to analyze
games. In contrast, game theory uses weaker mappings which change some of the
properties of the underlying probability distributions within the mixed
strategy space. In this paper, we explore how using strong isomorphic mappings
to define game strategy spaces can alter rational outcomes in simple games, and
might resolve some of the paradoxes of game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3981</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3981</id><created>2012-08-20</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Minimum Relative Entropy State Transitions in Linear Stochastic Systems:
  the Continuous Time Case</title><categories>math.OC cs.IT cs.SY math.DS math.IT math.PR</categories><comments>15 pages, 1 figure, published in the Proceedings of the 19th
  International Symposium on Mathematical Theory of Networks and Systems, 5-9
  July 2010, Budapest, Hungary, pp. 51-58</comments><msc-class>60H10, 93E20, 94A17, 82C31, 35F21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a dissipativity theory for dynamical systems
governed by linear Ito stochastic differential equations driven by random noise
with an uncertain drift. The deviation of the noise from a standard Wiener
process in the nominal model is quantified by relative entropy. We discuss a
dissipation inequality for the noise relative entropy supply. The problem of
minimizing the supply required to drive the system between given Gaussian state
distributions over a specified time horizon is considered. This problem, known
in the literature as the Schroedinger bridge, was treated previously in the
context of reciprocal processes. A closed-form smooth solution is obtained for
a Hamilton-Jacobi equation for the minimum required relative entropy supply by
using nonlinear algebraic techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3984</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3984</id><created>2012-08-20</created><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Huppert</keyname><forenames>Carolin</forenames></author></authors><title>On the Capacity of the Cognitive Interference Channel with a Common
  Cognitive Message</title><categories>cs.IT math.IT</categories><comments>submitted to Transactions on Emerging Telecommunications Technologies
  (ETT). arXiv admin note: substantial text overlap with arXiv:1202.0977</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the cognitive interference channel with a common message, a
variation of the classical cognitive interference channel in which the
cognitive message is decoded at both receivers, is studied. For this channel
model new outer and inner bounds are developed as well as new capacity results
for both the discrete memoryless and the Gaussian case. The outer bounds are
derived using bounding techniques originally developed by Sato for the
classical interference channel and Nair and El Gamal for the broadcast channel.
A general inner bound is obtained combining rate-splitting, superposition
coding and binning. Inner and outer bounds are shown to coincide in the &quot;very
strong interference&quot; and the &quot;primary decodes cognitive&quot; regimes. The first
regime consists of channels in which there is no loss of optimality in having
both receivers decode both messages while in the latter regime interference
pre-cancellation at the cognitive receiver achieves capacity. Capacity for the
Gaussian channel is shown to within a constant additive gap and a constant
multiplicative factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.3994</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.3994</id><created>2012-08-20</created><authors><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author></authors><title>Coordination in Network Security Games: a Monotone Comparative Statics
  Approach</title><categories>cs.GT cs.NI cs.SI</categories><comments>10 pages, to appear in IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malicious softwares or malwares for short have become a major security
threat. While originating in criminal behavior, their impact are also
influenced by the decisions of legitimate end users. Getting agents in the
Internet, and in networks in general, to invest in and deploy security features
and protocols is a challenge, in particular because of economic reasons arising
from the presence of network externalities.
  In this paper, we focus on the question of incentive alignment for agents of
a large network towards a better security. We start with an economic model for
a single agent, that determines the optimal amount to invest in protection. The
model takes into account the vulnerability of the agent to a security breach
and the potential loss if a security breach occurs. We derive conditions on the
quality of the protection to ensure that the optimal amount spent on security
is an increasing function of the agent's vulnerability and potential loss. We
also show that for a large class of risks, only a small fraction of the
expected loss should be invested.
  Building on these results, we study a network of interconnected agents
subject to epidemic risks. We derive conditions to ensure that the incentives
of all agents are aligned towards a better security. When agents are strategic,
we show that security investments are always socially inefficient due to the
network externalities. Moreover alignment of incentives typically implies a
coordination problem, leading to an equilibrium with a very high price of
anarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4009</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4009</id><created>2012-08-20</created><authors><author><keyname>Aliabadi</keyname><forenames>Behrooz Kamary</forenames></author><author><keyname>Berrou</keyname><forenames>Claude</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaoran</forenames></author></authors><title>Learning sparse messages in networks of neural cliques</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extension to a recently introduced binary neural network is proposed in
order to allow the learning of sparse messages, in large numbers and with high
memory efficiency. This new network is justified both in biological and
informational terms. The learning and retrieval rules are detailed and
illustrated by various simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4016</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4016</id><created>2012-08-20</created><authors><author><keyname>Ramaswamy</keyname><forenames>Gowri Shankar</forenames></author><author><keyname>Francis</keyname><forenames>F Sagayaraj</forenames></author></authors><title>Concept driven framework for Latent Table Discovery</title><categories>cs.DB</categories><journal-ref>JOURNAL OF COMPUTING, VOLUME 4, ISSUE 7, JULY 2012, ISSN (Online)
  2151-9617</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database systems have to cater to the growing demands of the information age.
The growth of the new age information retrieval powerhouses like search engines
has thrown a challenge to the data management community to come up with novel
mechanisms for feeding information to end users. The burgeoning use of natural
language query interfaces compels system designers to present meaningful and
customised information. Conventional query languages like SQL do not cater to
these requirements due to syntax oriented design. Providing a semantic cover
over these systems was the aim of latent table discovery focusing on
semantically connecting unrelated tables that were not syntactically related by
design and document the discovered knowledge. This paper throws a new direction
towards improving the semantic capabilities of database systems by introducing
a concept driven framework over the latent table discovery method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4035</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4035</id><created>2012-08-16</created><authors><author><keyname>Barthou</keyname><forenames>Denis</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Grosdidier</keyname><forenames>Gilbert</forenames><affiliation>LAL</affiliation></author><author><keyname>Kruse</keyname><forenames>Michael</forenames><affiliation>LRI</affiliation></author><author><keyname>P&#xe8;ne</keyname><forenames>Olivier</forenames><affiliation>LPT</affiliation></author><author><keyname>Tadonki</keyname><forenames>Claude</forenames></author></authors><title>QIRAL: A High Level Language for Lattice QCD Code Generation</title><categories>cs.PL</categories><comments>ETAPS 2012, Tallin : Estonia (2012)</comments><proxy>ccsd</proxy><report-no>LPT-ORSAY 12-08</report-no><doi>10.4204/EPTCS</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum chromodynamics (QCD) is the theory of subnuclear physics, aiming at
mod- eling the strong nuclear force, which is responsible for the interactions
of nuclear particles. Lattice QCD (LQCD) is the corresponding discrete
formulation, widely used for simula- tions. The computational demand for the
LQCD is tremendous. It has played a role in the history of supercomputers, and
has also helped defining their future. Designing efficient LQCD codes that
scale well on large (probably hybrid) supercomputers requires to express many
levels of parallelism, and then to explore different algorithmic solutions.
While al- gorithmic exploration is the key for efficient parallel codes, the
process is hampered by the necessary coding effort. We present in this paper a
domain-specific language, QIRAL, for a high level expression of parallel
algorithms in LQCD. Parallelism is expressed through the mathematical struc-
ture of the sparse matrices defining the problem. We show that from these
expressions and from algorithmic and preconditioning formulations, a parallel
code can be automatically generated. This separates algorithms and mathematical
formulations for LQCD (that be- long to the field of physics) from the
effective orchestration of parallelism, mainly related to compilation and
optimization for parallel architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4037</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4037</id><created>2012-08-20</created><authors><author><keyname>Brigatti</keyname><forenames>Edgardo</forenames></author></authors><title>Viability of an elementary syntactic structure in a population playing
  Naming Games</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>8 pages, 6 figures</comments><msc-class>91F20, 82C44</msc-class><journal-ref>Physical Review E 86, 026107 (2012)</journal-ref><doi>10.1103/PhysRevE.86.026107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore how the social dynamics of communication and learning can bring
about the rise of a syntactic communication in a population of speakers. Our
study is developed starting from a version of the Naming Game model where an
elementary syntactic structure is introduced. This analysis shows how the
transition from non-syntactic to syntactic communication is socially favored in
communities which need to exchange a large number of concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4041</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4041</id><created>2012-08-20</created><updated>2013-07-09</updated><authors><author><keyname>Ben-Amram</keyname><forenames>Amir M.</forenames></author><author><keyname>Genaim</keyname><forenames>Samir</forenames></author></authors><title>Ranking Functions for Linear-Constraint Loops</title><categories>cs.PL cs.LO</categories><comments>51 pages, extended and revised version of the POPL'13 paper</comments><acm-class>F.2.0; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the complexity of the problems: given a loop,
described by linear constraints over a finite set of variables, is there a
linear or lexicographical-linear ranking function for this loop? While
existence of such functions implies termination, these problems are not
equivalent to termination. When the variables range over the rationals (or
reals), it is known that both problems are PTIME decidable. However, when they
range over the integers, whether for single-path or multipath loops, the
complexity has not yet been determined. We show that both problems are
coNP-complete. However, we point out some special cases of importance of PTIME
complexity. We also present complete algorithms for synthesizing linear and
lexicographical-linear ranking functions, both for the general case and the
special PTIME cases. Moreover, in the rational setting, our algorithm for
synthesizing lexicographical-linear ranking functions extends existing ones,
because our class of ranking functions is more general, yet it has polynomial
time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4042</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4042</id><created>2012-08-20</created><updated>2012-08-22</updated><authors><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author></authors><title>Measuring quality, reputation and trust in online communities</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Lecture Notes in Computer Science 7661, 405-414 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Internet era the information overload and the challenge to detect
quality content has raised the issue of how to rank both resources and users in
online communities. In this paper we develop a general ranking method that can
simultaneously evaluate users' reputation and objects' quality in an iterative
procedure, and that exploits the trust relationships and social acquaintances
of users as an additional source of information. We test our method on two real
online communities, the EconoPhysics forum and the Last.fm music catalogue, and
determine how different variants of the algorithm influence the resultant
ranking. We show the benefits of considering trust relationships, and define
the form of the algorithm better apt to common situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4043</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4043</id><created>2012-08-20</created><authors><author><keyname>Mardani</keyname><forenames>Morteza</forenames></author><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Dynamic Anomalography: Tracking Network Anomalies via Sparsity and Low
  Rank</title><categories>cs.NI cs.IT math.IT</categories><comments>33 pages, 7 figures, submitted to the IEEE Journal of Selected Topics
  in Signal Processing - Special issue on `Anomalous pattern discovery for
  spatial, temporal, networked, and high-dimensional signals'</comments><doi>10.1109/JSTSP.2012.2233193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the backbone of large-scale networks, origin-to-destination (OD) traffic
flows experience abrupt unusual changes known as traffic volume anomalies,
which can result in congestion and limit the extent to which end-user quality
of service requirements are met. As a means of maintaining seamless end-user
experience in dynamic environments, as well as for ensuring network security,
this paper deals with a crucial network monitoring task termed dynamic
anomalography. Given link traffic measurements (noisy superpositions of
unobserved OD flows) periodically acquired by backbone routers, the goal is to
construct an estimated map of anomalies in real time, and thus summarize the
network `health state' along both the flow and time dimensions. Leveraging the
low intrinsic-dimensionality of OD flows and the sparse nature of anomalies, a
novel online estimator is proposed based on an exponentially-weighted
least-squares criterion regularized with the sparsity-promoting $\ell_1$-norm
of the anomalies, and the nuclear norm of the nominal traffic matrix. After
recasting the non-separable nuclear norm into a form amenable to online
optimization, a real-time algorithm for dynamic anomalography is developed and
its convergence established under simplifying technical assumptions. For
operational conditions where computational complexity reductions are at a
premium, a lightweight stochastic gradient algorithm based on Nesterov's
acceleration technique is developed as well. Comprehensive numerical tests with
both synthetic and real network data corroborate the effectiveness of the
proposed online algorithms and their tracking capabilities, and demonstrate
that they outperform state-of-the-art approaches developed to diagnose traffic
anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4048</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4048</id><created>2012-08-20</created><updated>2012-12-17</updated><authors><author><keyname>Xiang</keyname><forenames>Zhengzheng</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Degrees of Freedom for MIMO Two-Way X Relay Channel</title><categories>cs.IT math.IT</categories><comments>28 pages, 6 figures</comments><doi>10.1109/TSP.2013.2238535</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the degrees of freedom (DOF) of a multiple-input multiple-output
(MIMO) two-way X relay channel, where there are two groups of source nodes and
one relay node, each equipped with multiple antennas, and each of the two
source nodes in one group exchanges independent messages with the two source
nodes in the other group via the relay node. It is assumed that every source
node is equipped with M antennas while the relay is equipped with N antennas.
We first show that the upper bound on the total DOF for this network is
2min{2M,N} and then focus on the case of N \leq 2M so that the DOF is upper
bounded by the number of antennas at the relay. By applying signal alignment
for network coding and joint transceiver design for interference cancellation,
we show that this upper bound can be achieved when N \leq8M/5. We also show
that with signal alignment only but no joint transceiver design, the upper
bound is achievable when N\leq4M/3. Simulation results are provided to
corroborate the theoretical results and to demonstrate the performance of the
proposed scheme in the finite signal-to-noise ratio regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4079</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4079</id><created>2012-08-20</created><authors><author><keyname>Shah</keyname><forenames>Nishal Pradeepkumar</forenames></author></authors><title>Recent Technological Advances in Natural Language Processing and
  Artificial Intelligence</title><categories>cs.CL</categories><comments>6 pages,0 figures</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent advance in computer technology has permitted scientists to implement
and test algorithms that were known from quite some time (or not) but which
were computationally expensive. Two such projects are IBM's Jeopardy as a part
of its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods
implement natural language processing (another goal of AI scientists) and try
to answer questions as asked by the user. Though the goal of the two projects
is similar, both of them have a different procedure at it's core. In the
following sections, the mechanism and history of IBM's Jeopardy and Wolfram
alpha has been explained followed by the implications of these projects in
realizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe
of taking the above projects to a new level is also explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4080</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4080</id><created>2012-08-20</created><updated>2013-01-24</updated><authors><author><keyname>Yedla</keyname><forenames>Arvind</forenames></author><author><keyname>Jian</keyname><forenames>Yung-Yih</forenames></author><author><keyname>Nguyen</keyname><forenames>Phong S.</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>A Simple Proof of Threshold Saturation for Coupled Vector Recursions</title><categories>cs.IT math.IT</categories><comments>7 pages, a slightly extended version of the paper with that appears
  in the proceedings of ITW 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional low-density parity-check (LDPC) codes (or spatially-coupled
codes) have now been shown to achieve capacity on binary-input memoryless
symmetric channels. The principle behind this surprising result is the
threshold-saturation phenomenon, which is defined by the belief-propagation
threshold of the spatially-coupled ensemble saturating to a fundamental
threshold defined by the uncoupled system.
  Previously, the authors demonstrated that potential functions can be used to
provide a simple proof of threshold saturation for coupled scalar recursions.
In this paper, we present a simple proof of threshold saturation that applies
to a wide class of coupled vector recursions. The conditions of the theorem are
verified for the density-evolution equations of: (i) joint decoding of
irregular LDPC codes for a Slepian-Wolf problem with erasures, (ii) joint
decoding of irregular LDPC codes on an erasure multiple-access channel, and
(iii) general protograph codes on the BEC. This proves threshold saturation for
these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4081</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4081</id><created>2012-08-20</created><authors><author><keyname>Maximov</keyname><forenames>Eugene A.</forenames></author><author><keyname>Kurdyukov</keyname><forenames>Alexander P.</forenames></author><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>Anisotropic Norm Bounded Real Lemma for Linear Discrete Time Varying
  Systems</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>7 pages, 1 figure, published in the Proceedings of the 18th IFAC
  World Congress, 28 August - 2 September 2011, Milano, Italy, pp. 4701-4706</comments><msc-class>93C55, 94A17, 93B05, 93E15, 93E20, 60G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a finite horizon linear discrete time varying system whose input
is a random noise with an imprecisely known probability law. The statistical
uncertainty is described by a nonnegative parameter a which constrains the
anisotropy of the noise as an entropy theoretic measure of deviation of the
actual noise distribution from Gaussian white noise laws with scalar covariance
matrices. The worst-case disturbance attenuation capabilities of the system
with respect to the statistically uncertain random inputs are quantified by the
a-anisotropic norm which is an appropriately constrained operator norm of the
system. We establish an anisotropic norm bounded real lemma which provides a
state-space criterion for the a-anisotropic norm of the system not to exceed a
given threshold. The criterion is organized as an inequality on the
determinants of matrices associated with a difference Riccati equation and
extends the Bounded Real Lemma of the H-infinity-control theory. We also
provide a necessary background on the anisotropy-based robust performance
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4093</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4093</id><created>2012-08-18</created><authors><author><keyname>Xu</keyname><forenames>Ran</forenames></author><author><keyname>Liu</keyname><forenames>Bin</forenames></author><author><keyname>Dong</keyname><forenames>Yuan</forenames></author></authors><title>Scalable hierarchical parallel algorithm for the solution of super
  large-scale sparse linear equations</title><categories>physics.comp-ph cs.NA math.NA</categories><comments>23 page, 9 figures 1 table</comments><doi>10.1115/1.4023481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel linear equations solver capable of effectively using 1000+
processors becomes the bottleneck of large-scale implicit engineering
simulations. In this paper, we present a new hierarchical parallel
master-slave-structural iterative algorithm for the solution of super
large-scale sparse linear equations in distributed memory computer cluster.
Through alternatively performing global equilibrium computation and local
relaxation, our proposed algorithm will reach the specific accuracy requirement
in a few of iterative steps. Moreover, each set/slave-processor majorly
communicate with its nearest neighbors, and the transferring data between
sets/slave-processors and master is always far below the set-neighbor
communication. The corresponding algorithm for implicit finite element analysis
has been implemented based on MPI library, and a super large 2-dimension square
system of triangle-lattice truss structure under random static loads is
simulated with over one billion degrees of freedom and up to 2001 processors on
&quot;Exploration 100&quot; cluster in Tsinghua University. The numerical experiments
demonstrate that this algorithm has excellent parallel efficiency and high
scalability, and it may have broad application in other implicit simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4125</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4125</id><created>2012-08-20</created><updated>2013-01-08</updated><authors><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Fishkind</keyname><forenames>Donniell E.</forenames></author></authors><title>Counting Spanning Trees of Threshold Graphs</title><categories>math.CO cs.DM</categories><comments>14 pages, 5 figures</comments><msc-class>05A19</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cayley's formula states that there are $n^{n-2}$ spanning trees in the
complete graph on $n$ vertices; it has been proved in more than a dozen
different ways over its 150 year history. The complete graphs are a special
case of threshold graphs, and using Merris' Theorem and the Matrix Tree
Theorem, there is a strikingly simple formula for counting the number of
spanning trees in a threshold graph on $n$ vertices; it is simply the product,
over $i=2,3, ...,n-1$, of the number of vertices of degree at least $i$. In
this manuscript, we provide a direct combinatorial proof for this formula which
does not use the Matrix Tree Theorem; the proof is an extension of Joyal's
proof for Cayley's formula. Then we apply this methodology to give a formula
for the number of spanning trees in any difference graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4126</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4126</id><created>2012-08-20</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Kosar</keyname><forenames>Toma&#x17e;</forenames></author><author><keyname>Mernik</keyname><forenames>Marjan</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author></authors><title>Upgrading EasyTime: from a textual to a visual language</title><categories>cs.PL</categories><journal-ref>I. Fister Jr., T. Kosar, M. Mernik, I. Fister, Upgrading EasyTime:
  from a textual to a visual language, In Proceedings of the 21st International
  Electrotechnical and Computer Science Conference, Portoro\v{z}, Slovenia,
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring time in mass sports competitions is usually performed using
expensive measuring devices. Unfortunately, these solutions are not acceptable
by many organizers of sporting competitions. In order to make the measuring
time as cheap as possible, the domain-specific language (DSL) EasyTime was
proposed. In practice, it has been proven to be universal, flexible, and
efficient. It can even reduce the number of required measuring devices. On the
other hand, programming in EasyTime is not easy, because it requires a
domain-expert to program in a textual manner. In this paper, the
domain-specific modeling language (DSML) EasyTime II is proposed, which
simplifies the programming of the measuring system. First, the DSL EasyTime
domain analysis is presented. Then, the development of DSML is described in
detail. Finally, the DSML was tested by regular organizers of a sporting
competition. This test showed that DSML can be used by end-users without any
previous programming knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4138</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4138</id><created>2012-08-20</created><authors><author><keyname>Iqbal</keyname><forenames>Ashraf Mohammed</forenames></author><author><keyname>Moh'd</keyname><forenames>Abidalrahman</forenames></author><author><keyname>Khan</keyname><forenames>Zahoor</forenames></author></authors><title>Semi-supervised Clustering Ensemble by Voting</title><categories>cs.LG stat.ML</categories><comments>The International Conference on Information and Communication Systems
  (ICICS 2009), Amman, Jordan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering ensemble is one of the most recent advances in unsupervised
learning. It aims to combine the clustering results obtained using different
algorithms or from different runs of the same clustering algorithm for the same
data set, this is accomplished using on a consensus function, the efficiency
and accuracy of this method has been proven in many works in literature. In the
first part of this paper we make a comparison among current approaches to
clustering ensemble in literature. All of these approaches consist of two main
steps: the ensemble generation and consensus function. In the second part of
the paper, we suggest engaging supervision in the clustering ensemble procedure
to get more enhancements on the clustering results. Supervision can be applied
in two places: either by using semi-supervised algorithms in the clustering
ensemble generation step or in the form of a feedback used by the consensus
function stage. Also, we introduce a flexible two parameter weighting
mechanism, the first parameter describes the compatibility between the datasets
under study and the semi-supervised clustering algorithms used to generate the
base partitions, the second parameter is used to provide the user feedback on
the these partitions. The two parameters are engaged in a &quot;relabeling and
voting&quot; based consensus function to produce the final clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4145</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4145</id><created>2012-08-20</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Gionis</keyname><forenames>Aris</forenames></author><author><keyname>Tassa</keyname><forenames>Tamir</forenames></author></authors><title>Injecting Uncertainty in Graphs for Identity Obfuscation</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.
  1376-1387 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data collected nowadays by social-networking applications create fascinating
opportunities for building novel services, as well as expanding our
understanding about social structures and their dynamics. Unfortunately,
publishing social-network graphs is considered an ill-advised practice due to
privacy concerns. To alleviate this problem, several anonymization methods have
been proposed, aiming at reducing the risk of a privacy breach on the published
data, while still allowing to analyze them and draw relevant conclusions. In
this paper we introduce a new anonymization approach that is based on injecting
uncertainty in social graphs and publishing the resulting uncertain graphs.
While existing approaches obfuscate graph data by adding or removing edges
entirely, we propose using a ?finer-grained perturbation that adds or removes
edges partially: this way we can achieve the same desired level of obfuscation
with smaller changes in the data, thus maintaining higher utility. Our
experiments on real-world networks con?firm that at the same level of identity
obfuscation our method provides higher usefulness than existing randomized
methods that publish standard graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4147</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4147</id><created>2012-08-20</created><updated>2015-11-27</updated><authors><author><keyname>Li</keyname><forenames>Yingzhen</forenames></author><author><keyname>Zhang</keyname><forenames>Ye</forenames></author></authors><title>Generating ordered list of Recommended Items: a Hybrid Recommender
  System of Microblog</title><categories>cs.IR cs.LG cs.SI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise recommendation of followers helps in improving the user experience
and maintaining the prosperity of twitter and microblog platforms. In this
paper, we design a hybrid recommender system of microblog as a solution of KDD
Cup 2012, track 1 task, which requires predicting users a user might follow in
Tencent Microblog. We describe the background of the problem and present the
algorithm consisting of keyword analysis, user taxonomy, (potential)interests
extraction and item recommendation. Experimental result shows the high
performance of our algorithm. Some possible improvements are discussed, which
leads to further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4161</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4161</id><created>2012-08-20</created><updated>2013-09-14</updated><authors><author><keyname>Shen</keyname><forenames>Xiaojing</forenames><affiliation>IEEE</affiliation></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames><affiliation>IEEE</affiliation></author><author><keyname>Zhu</keyname><forenames>Yunmin</forenames></author></authors><title>Robust Distributed Maximum Likelihood Estimation with Dependent
  Quantized Data</title><categories>cs.IT math.IT</categories><comments>submitted to journal</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we consider distributed maximum likelihood estimation (MLE)
with dependent quantized data under the assumption that the structure of the
joint probability density function (pdf) is known, but it contains unknown
deterministic parameters. The parameters may include different vector
parameters corresponding to marginal pdfs and parameters that describe
dependence of observations across sensors. Since MLE with a single quantizer is
sensitive to the choice of thresholds due to the uncertainty of pdf, we
concentrate on MLE with multiple groups of quantizers (which can be determined
by the use of prior information or some heuristic approaches) to fend off
against the risk of a poor/outlier quantizer. The asymptotic efficiency of the
MLE scheme with multiple quantizers is proved under some regularity conditions
and the asymptotic variance is derived to be the inverse of a weighted linear
combination of Fisher information matrices based on multiple different
quantizers which can be used to show the robustness of our approach. As an
illustrative example, we consider an estimation problem with a bivariate
non-Gaussian pdf that has applications in distributed constant false alarm rate
(CFAR) detection systems. Simulations show the robustness of the proposed MLE
scheme especially when the number of quantized measurements is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4165</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4165</id><created>2012-08-20</created><authors><author><keyname>Hellerstein</keyname><forenames>Joe</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Schoppmann</keyname><forenames>Florian</forenames></author><author><keyname>Wang</keyname><forenames>Daisy Zhe</forenames></author><author><keyname>Fratkin</keyname><forenames>Eugene</forenames></author><author><keyname>Gorajek</keyname><forenames>Aleksander</forenames></author><author><keyname>Ng</keyname><forenames>Kee Siong</forenames></author><author><keyname>Welton</keyname><forenames>Caleb</forenames></author><author><keyname>Feng</keyname><forenames>Xixuan</forenames></author><author><keyname>Li</keyname><forenames>Kun</forenames></author><author><keyname>Kumar</keyname><forenames>Arun</forenames></author></authors><title>The MADlib Analytics Library or MAD Skills, the SQL</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1700-1711 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MADlib is a free, open source library of in-database analytic methods. It
provides an evolving suite of SQL-based algorithms for machine learning, data
mining and statistics that run at scale within a database engine, with no need
for data import/export to other tools. The goal is for MADlib to eventually
serve a role for scalable database systems that is similar to the CRAN library
for R: a community repository of statistical methods, this time written with
scale and parallelism in mind. In this paper we introduce the MADlib project,
including the background that led to its beginnings, and the motivation for its
open source nature. We provide an overview of the library's architecture and
design patterns, and provide a description of various statistical methods in
that context. We include performance and speedup results of a core design
pattern from one of those methods over the Greenplum parallel DBMS on a
modest-sized test cluster. We then report on two initial efforts at
incorporating academic research into MADlib, which is one of the project's
goals. MADlib is freely available at http://madlib.net, and the project is open
for contributions of both new methods, and ports to additional database
platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4166</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4166</id><created>2012-08-20</created><authors><author><keyname>Floratou</keyname><forenames>Avrilia</forenames></author><author><keyname>Teletia</keyname><forenames>Nikhil</forenames></author><author><keyname>Dewitt</keyname><forenames>David J.</forenames></author><author><keyname>Patel</keyname><forenames>Jignesh M.</forenames></author><author><keyname>Zhang</keyname><forenames>Donghui</forenames></author></authors><title>Can the Elephants Handle the NoSQL Onslaught?</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1712-1723 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this new era of &quot;big data&quot;, traditional DBMSs are under attack from two
sides. At one end of the spectrum, the use of document store NoSQL systems
(e.g. MongoDB) threatens to move modern Web 2.0 applications away from
traditional RDBMSs. At the other end of the spectrum, big data DSS analytics
that used to be the domain of parallel RDBMSs is now under attack by another
class of NoSQL data analytics systems, such as Hive on Hadoop. So, are the
traditional RDBMSs, aka &quot;big elephants&quot;, doomed as they are challenged from
both ends of this &quot;big data&quot; spectrum? In this paper, we compare one
representative NoSQL system from each end of this spectrum with SQL Server, and
analyze the performance and scalability aspects of each of these approaches
(NoSQL vs. SQL) on two workloads (decision support analysis and interactive
data-serving) that represent the two ends of the application spectrum. We
present insights from this evaluation and speculate on potential trends for the
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4167</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4167</id><created>2012-08-20</created><authors><author><keyname>Rabl</keyname><forenames>Tilmann</forenames></author><author><keyname>Sadoghi</keyname><forenames>Mohammad</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author><author><keyname>G&#xf3;mez-Villamor</keyname><forenames>Sergio</forenames></author><author><keyname>Munt&#xe9;s-Mulero</keyname><forenames>Victor</forenames></author><author><keyname>Mankowskii</keyname><forenames>Serge</forenames></author></authors><title>Solving Big Data Challenges for Enterprise Application Performance
  Management</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1724-1735 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the complexity of enterprise systems increases, the need for monitoring
and analyzing such systems also grows. A number of companies have built
sophisticated monitoring tools that go far beyond simple resource utilization
reports. For example, based on instrumentation and specialized APIs, it is now
possible to monitor single method invocations and trace individual transactions
across geographically distributed systems. This high-level of detail enables
more precise forms of analysis and prediction but comes at the price of high
data rates (i.e., big data). To maximize the benefit of data monitoring, the
data has to be stored for an extended period of time for ulterior analysis.
This new wave of big data analytics imposes new challenges especially for the
application performance monitoring systems. The monitoring data has to be
stored in a system that can sustain the high data rates and at the same time
enable an up-to-date view of the underlying infrastructure. With the advent of
modern key-value stores, a variety of data storage systems have emerged that
are built with a focus on scalability and high data rates as predominant in
this monitoring use case. In this work, we present our experience and a
comprehensive performance evaluation of six modern (open-source) data stores in
the context of application performance monitoring as part of CA Technologies
initiative. We evaluated these systems with data and workloads that can be
found in application performance monitoring, as well as, on-line advertisement,
power monitoring, and many other use cases. We present our insights not only as
performance results but also as lessons learned and our experience relating to
the setup and configuration complexity of these data stores in an industry
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4168</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4168</id><created>2012-08-20</created><authors><author><keyname>Shinnar</keyname><forenames>Avraham</forenames></author><author><keyname>Cunningham</keyname><forenames>David</forenames></author><author><keyname>Herta</keyname><forenames>Benjamin</forenames></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author></authors><title>M3R: Increased performance for in-memory Hadoop jobs</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1736-1747 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Main Memory Map Reduce (M3R) is a new implementation of the Hadoop Map Reduce
(HMR) API targeted at online analytics on high mean-time-to-failure clusters.
It does not support resilience, and supports only those workloads which can fit
into cluster memory. In return, it can run HMR jobs unchanged -- including jobs
produced by compilers for higher-level languages such as Pig, Jaql, and
SystemML and interactive front-ends like IBM BigSheets -- while providing
significantly better performance than the Hadoop engine on several workloads
(e.g. 45x on some input sizes for sparse matrix vector multiply). M3R also
supports extensions to the HMR API which can enable Map Reduce jobs to run
faster on the M3R engine, while not affecting their performance under the
Hadoop engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4169</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4169</id><created>2012-08-20</created><authors><author><keyname>R&#xf6;sch</keyname><forenames>Philipp</forenames></author><author><keyname>Dannecker</keyname><forenames>Lars</forenames></author><author><keyname>Hackenbroich</keyname><forenames>Gregor</forenames></author><author><keyname>Faerber</keyname><forenames>Franz</forenames></author></authors><title>A Storage Advisor for Hybrid-Store Databases</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1748-1758 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the SAP HANA database, SAP offers a high-performance in-memory
hybrid-store database. Hybrid-store databases---that is, databases supporting
row- and column-oriented data management---are getting more and more prominent.
While the columnar management offers high-performance capabilities for
analyzing large quantities of data, the row-oriented store can handle
transactional point queries as well as inserts and updates more efficiently. To
effectively take advantage of both stores at the same time the novel question
whether to store the given data row- or column-oriented arises. We tackle this
problem with a storage advisor tool that supports database administrators at
this decision. Our proposed storage advisor recommends the optimal store based
on data and query characteristics; its core is a cost model to estimate and
compare query execution times for the different stores. Besides a per-table
decision, our tool also considers to horizontally and vertically partition the
data and manage the partitions on different stores. We evaluated the storage
advisor for the use in the SAP HANA database; we show the recommendation
quality as well as the benefit of having the data in the optimal store with
respect to increased query performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4170</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4170</id><created>2012-08-20</created><authors><author><keyname>&#x15a;witakowski</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Boncz</keyname><forenames>Peter</forenames></author><author><keyname>&#x17b;ukowski</keyname><forenames>Marcin</forenames></author></authors><title>From Cooperative Scans to Predictive Buffer Management</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1759-1770 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analytical applications, database systems often need to sustain workloads
with multiple concurrent scans hitting the same table. The Cooperative Scans
(CScans) framework, which introduces an Active Buffer Manager (ABM) component
into the database architecture, has been the most effective and elaborate
response to this problem, and was initially developed in the X100 research
prototype. We now report on the the experiences of integrating Cooperative
Scans into its industrial-strength successor, the Vectorwise database product.
During this implementation we invented a simpler optimization of concurrent
scan buffer management, called Predictive Buffer Management (PBM). PBM is based
on the observation that in a workload with long-running scans, the buffer
manager has quite a bit of information on the workload in the immediate future,
such that an approximation of the ideal OPT algorithm becomes feasible. In the
evaluation on both synthetic benchmarks as well as a TPC-H throughput run we
compare the benefits of naive buffer management (LRU) versus CScans, PBM and
OPT; showing that PBM achieves benefits close to Cooperative Scans, while
incurring much lower architectural impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4171</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4171</id><created>2012-08-20</created><authors><author><keyname>Lee</keyname><forenames>George</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Lorek</keyname><forenames>Andrew</forenames></author><author><keyname>Ryaboy</keyname><forenames>Dmitriy</forenames></author></authors><title>The Unified Logging Infrastructure for Data Analytics at Twitter</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1771-1780 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been a substantial amount of work on large-scale
data analytics using Hadoop-based platforms running on large clusters of
commodity machines. A less-explored topic is how those data, dominated by
application logs, are collected and structured to begin with. In this paper, we
present Twitter's production logging infrastructure and its evolution from
application-specific logging to a unified &quot;client events&quot; log format, where
messages are captured in common, well-formatted, flexible Thrift messages.
Since most analytics tasks consider the user session as the basic unit of
analysis, we pre-materialize &quot;session sequences&quot;, which are compact summaries
that can answer a large class of common queries quickly. The development of
this infrastructure has streamlined log collection and data analysis, thereby
improving our ability to rapidly experiment and iterate on various aspects of
the service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4172</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4172</id><created>2012-08-20</created><authors><author><keyname>Talius</keyname><forenames>Tomas</forenames></author><author><keyname>Dhamankar</keyname><forenames>Robin</forenames></author><author><keyname>Dumitrache</keyname><forenames>Andrei</forenames></author><author><keyname>Kodavalla</keyname><forenames>Hanuma</forenames></author></authors><title>Transaction Log Based Application Error Recovery and Point In-Time Query</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1781-1789 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database backups have traditionally been used as the primary mechanism to
recover from hardware and user errors. High availability solutions maintain
redundant copies of data that can be used to recover from most failures except
user or application errors. Database backups are neither space nor time
efficient for recovering from user errors which typically occur in the recent
past and affect a small portion of the database. Moreover periodic full backups
impact user workload and increase storage costs. In this paper we present a
scheme that can be used for both user and application error recovery starting
from the current state and rewinding the database back in time using the
transaction log. While we provide a consistent view of the entire database as
of a point in time in the past, the actual prior versions are produced only for
data that is accessed. We make the as of data accessible to arbitrary point in
time queries by integrating with the database snapshot feature in Microsoft SQL
Server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4173</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4173</id><created>2012-08-20</created><authors><author><keyname>Lamb</keyname><forenames>Andrew</forenames></author><author><keyname>Fuller</keyname><forenames>Matt</forenames></author><author><keyname>Varadarajan</keyname><forenames>Ramakrishna</forenames></author><author><keyname>Tran</keyname><forenames>Nga</forenames></author><author><keyname>Vandier</keyname><forenames>Ben</forenames></author><author><keyname>Doshi</keyname><forenames>Lyric</forenames></author><author><keyname>Bear</keyname><forenames>Chuck</forenames></author></authors><title>The Vertica Analytic Database: C-Store 7 Years Later</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1790-1801 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the system architecture of the Vertica Analytic Database
(Vertica), a commercialization of the design of the C-Store research prototype.
Vertica demonstrates a modern commercial RDBMS system that presents a classical
relational interface while at the same time achieving the high performance
expected from modern &quot;web scale&quot; analytic systems by making appropriate
architectural choices. Vertica is also an instructive lesson in how academic
systems research can be directly commercialized into a successful product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4174</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4174</id><created>2012-08-20</created><authors><author><keyname>Chen</keyname><forenames>Yanpei</forenames></author><author><keyname>Alspaugh</keyname><forenames>Sara</forenames></author><author><keyname>Katz</keyname><forenames>Randy</forenames></author></authors><title>Interactive Analytical Processing in Big Data Systems: A Cross-Industry
  Study of MapReduce Workloads</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1802-1813 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the past few years, organizations in diverse industries have adopted
MapReduce-based systems for large-scale data processing. Along with these new
users, important new workloads have emerged which feature many small, short,
and increasingly interactive jobs in addition to the large, long-running batch
jobs for which MapReduce was originally designed. As interactive, large-scale
query processing is a strength of the RDBMS community, it is important that
lessons from that field be carried over and applied where possible in this new
domain. However, these new workloads have not yet been described in the
literature. We fill this gap with an empirical analysis of MapReduce traces
from six separate business-critical deployments inside Facebook and at Cloudera
customers in e-commerce, telecommunications, media, and retail. Our key
contribution is a characterization of new MapReduce workloads which are driven
in part by interactive analysis, and which make heavy use of query-like
programming frameworks on top of MapReduce. These workloads display diverse
behaviors which invalidate prior assumptions about MapReduce such as uniform
data access, regular diurnal patterns, and prevalence of large jobs. A
secondary contribution is a first step towards creating a TPC-like data
processing benchmark for MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4175</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4175</id><created>2012-08-20</created><authors><author><keyname>Lam</keyname><forenames>Wang</forenames></author><author><keyname>Liu</keyname><forenames>Lu</forenames></author><author><keyname>Prasad</keyname><forenames>STS</forenames></author><author><keyname>Rajaraman</keyname><forenames>Anand</forenames></author><author><keyname>Vacheri</keyname><forenames>Zoheb</forenames></author><author><keyname>Doan</keyname><forenames>AnHai</forenames></author></authors><title>Muppet: MapReduce-Style Processing of Fast Data</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1814-1825 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MapReduce has emerged as a popular method to process big data. In the past
few years, however, not just big data, but fast data has also exploded in
volume and availability. Examples of such data include sensor data streams, the
Twitter Firehose, and Facebook updates. Numerous applications must process fast
data. Can we provide a MapReduce-style framework so that developers can quickly
write such applications and execute them over a cluster of machines, to achieve
low latency and high scalability? In this paper we report on our investigation
of this question, as carried out at Kosmix and WalmartLabs. We describe
MapUpdate, a framework like MapReduce, but specifically developed for fast
data. We describe Muppet, our implementation of MapUpdate. Throughout the
description we highlight the key challenges, argue why MapReduce is not well
suited to address them, and briefly describe our current solutions. Finally, we
describe our experience and lessons learned with Muppet, which has been used
extensively at Kosmix and WalmartLabs to power a broad range of applications in
social media and e-commerce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4176</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4176</id><created>2012-08-20</created><authors><author><keyname>Jacques-Silva</keyname><forenames>Gabriela</forenames></author><author><keyname>Gedik</keyname><forenames>Bu&#x11f;ra</forenames></author><author><keyname>Wagle</keyname><forenames>Rohit</forenames></author><author><keyname>Wu</keyname><forenames>Kun-Lung</forenames></author><author><keyname>Kumar</keyname><forenames>Vibhore</forenames></author></authors><title>Building User-defined Runtime Adaptation Routines for Stream Processing
  Applications</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1826-1837 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stream processing applications are deployed as continuous queries that run
from the time of their submission until their cancellation. This deployment
mode limits developers who need their applications to perform runtime
adaptation, such as algorithmic adjustments, incremental job deployment, and
application-specific failure recovery. Currently, developers do runtime
adaptation by using external scripts and/or by inserting operators into the
stream processing graph that are unrelated to the data processing logic. In
this paper, we describe a component called orchestrator that allows users to
write routines for automatically adapting the application to runtime
conditions. Developers build an orchestrator by registering and handling events
as well as specifying actuations. Events can be generated due to changes in the
system state (e.g., application component failures), built-in system metrics
(e.g., throughput of a connection), or custom application metrics (e.g.,
quality score). Once the orchestrator receives an event, users can take
adaptation actions by using the orchestrator actuation APIs. We demonstrate the
use of the orchestrator in IBM's System S in the context of three different
applications, illustrating application adaptation to changes on the incoming
data distribution, to application failures, and on-demand dynamic composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4178</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4178</id><created>2012-08-20</created><authors><author><keyname>Jiang</keyname><forenames>Junchen</forenames></author><author><keyname>Bao</keyname><forenames>Hongji</forenames></author><author><keyname>Chang</keyname><forenames>Edward Y.</forenames></author><author><keyname>Li</keyname><forenames>Yuqian</forenames></author></authors><title>MOIST: A Scalable and Parallel Moving Object Indexer with School
  Tracking</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1838-1849 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-Based Service (LBS) is rapidly becoming the next ubiquitous
technology for a wide range of mobile applications. To support applications
that demand nearest-neighbor and history queries, an LBS spatial indexer must
be able to efficiently update, query, archive and mine location records, which
can be in contention with each other. In this work, we propose MOIST, whose
baseline is a recursive spatial partitioning indexer built upon BigTable. To
reduce update and query contention, MOIST groups nearby objects of similar
trajectory into the same school, and keeps track of only the history of school
leaders. This dynamic clustering scheme can eliminate redundant updates and
hence reduce update latency. To improve history query processing, MOIST keeps
some history data in memory, while it flushes aged data onto parallel disks in
a locality-preserving way. Through experimental studies, we show that MOIST can
support highly efficient nearest-neighbor and history queries and can scale
well with an increasing number of users and update frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4179</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4179</id><created>2012-08-20</created><authors><author><keyname>Ports</keyname><forenames>Dan R. K.</forenames></author><author><keyname>Grittner</keyname><forenames>Kevin</forenames></author></authors><title>Serializable Snapshot Isolation in PostgreSQL</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1850-1861 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our experience implementing PostgreSQL's new
serializable isolation level. It is based on the recently-developed
Serializable Snapshot Isolation (SSI) technique. This is the first
implementation of SSI in a production database release as well as the first in
a database that did not previously have a lock-based serializable isolation
level. We reflect on our experience and describe how we overcame some of the
resulting challenges, including the implementation of a new lock manager, a
technique for ensuring memory usage is bounded, and integration with other
PostgreSQL features. We also introduce an extension to SSI that improves
performance for read-only transactions. We evaluate PostgreSQL's serializable
isolation level using several benchmarks and show that it achieves performance
only slightly below that of snapshot isolation, and significantly outperforms
the traditional two-phase locking approach on read-intensive workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4188</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4188</id><created>2012-08-21</created><authors><author><keyname>Savov</keyname><forenames>Ivan</forenames></author></authors><title>Network information theory for classical-quantum channels</title><categories>quant-ph cs.IT math.IT</categories><comments>Ph.D. Thesis, McGill University, School of Computer Science, July
  2012, 223 pages, 18 figures, 36 TikZ diagrams</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network information theory is the study of communication problems involving
multiple senders, multiple receivers and intermediate relay stations. The
purpose of this thesis is to extend the main ideas of classical network
information theory to the study of classical-quantum channels. We prove coding
theorems for quantum multiple access channels, quantum interference channels,
quantum broadcast channels and quantum relay channels.
  A quantum model for a communication channel describes more accurately the
channel's ability to transmit information. By using physically faithful models
for the channel outputs and the detection procedure, we obtain better
communication rates than would be possible using a classical strategy. In this
thesis, we are interested in the transmission of classical information, so we
restrict our attention to the study of classical-quantum channels. These are
channels with classical inputs and quantum outputs, and so the coding theorems
we present will use classical encoding and quantum decoding. We study the
asymptotic regime where many copies of the channel are used in parallel, and
the uses are assumed to be independent. In this context, we can exploit
information-theoretic techniques to calculate the maximum rates for error-free
communication for any channel, given the statistics of the noise on that
channel. These theoretical bounds can be used as a benchmark to evaluate the
rates achieved by practical communication protocols.
  Most of the results in this thesis consider classical-quantum channels with
finite dimensional output systems, which are analogous to classical discrete
memoryless channels. In the last chapter, we will show some applications of our
results to a practical optical communication scenario, in which the information
is encoded in continuous quantum degrees of freedom, which are analogous to
classical channels with Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4192</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4192</id><created>2012-08-21</created><authors><author><keyname>Preetha</keyname><forenames>K G</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>A</forenames></author><author><keyname>Jacob</keyname><forenames>K Poulose</forenames></author></authors><title>Performance improvement of multiple Connections in AODV with the concern
  of Node bandwidth</title><categories>cs.NI</categories><comments>9 Pages, 4 figures</comments><journal-ref>International Journal on AdHoc Networking Systems (IJANS) Vol. 2,
  No. 3, July 2012</journal-ref><doi>10.5121/ijans.2012.23034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad-hoc Networks (MANETS) consists of a collection of mobile nodes
without having a central coordination. In MANET, node mobility and dynamic
topology play an important role in the performance. MANET provide a solution
for network connection at anywhere and at any time. The major features of MANET
are quick set up, self organization and self maintenance. Routing is a major
challenge in MANET due to it's dynamic topology and high mobility. Several
routing algorithms have been developed for routing. This paper studies the AODV
protocol and how AODV is performed under multiple connections in the network.
Several issues have been identified. The bandwidth is recognized as the
prominent factor reducing the performance of the network. This paper gives an
improvement of normal AODV for simultaneous multiple connections under the
consideration of bandwidth of node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4208</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4208</id><created>2012-08-21</created><updated>2013-07-23</updated><authors><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Picciolo</keyname><forenames>Francesco</forenames></author><author><keyname>Ruzzenenti</keyname><forenames>Franco</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Reciprocity of weighted networks</title><categories>physics.data-an cs.SI physics.soc-ph</categories><comments>21 pages, 7 figures, 1 table</comments><journal-ref>Sci. Rep. 3 (2729) (2013)</journal-ref><doi>10.1038/srep02729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All types of networks arise as intricate combinations of dyadic building
blocks formed by pairs of vertices. In directed networks, the dyadic patterns
are entirely determined by reciprocity, i.e. the tendency to form, or to avoid,
mutual links. Reciprocity has dramatic effects on every networks dynamical
processes and the emergence of structures like motifs and communities. The
binary reciprocity has been extensively studied: that of weighted networks is
still poorly understood. We introduce a general approach to it, by defining
quantities capturing the observed patterns (from dyad-specific to
vertex-specific and network-wide) and introducing analytically solved models
(Exponential Random Graphs-type). Counter-intuitively, the previous reciprocity
measures based on the similarity of the mutual links-weights are uninformative.
By contrast, our measures can classify different weighted networks, track the
temporal evolution of a networks reciprocity, identify patterns. We show that
in some networks the local reciprocity structure can be inferred from the
global one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4225</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4225</id><created>2012-08-21</created><authors><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author><author><keyname>van der Zwaan</keyname><forenames>Ruben</forenames></author></authors><title>Reducing a Target Interval to a Few Exact Queries</title><categories>cs.DS</categories><comments>10 pages, to appear at MFCS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many combinatorial problems involving weights can be formulated as a
so-called ranged problem. That is, their input consists of a universe $U$, a
(succinctly-represented) set family $\mathcal{F} \subseteq 2^{U}$, a weight
function $\omega:U \rightarrow \{1,...,N\}$, and integers $0 \leq l \leq u \leq
\infty$. Then the problem is to decide whether there is an $X \in \mathcal{F}$
such that $l \leq \sum_{e \in X}\omega(e) \leq u$. Well-known examples of such
problems include Knapsack, Subset Sum, Maximum Matching, and Traveling
Salesman. In this paper, we develop a generic method to transform a ranged
problem into an exact problem (i.e. a ranged problem for which $l=u$). We show
that our method has several intriguing applications in exact exponential
algorithms and parameterized complexity, namely:
  - In exact exponential algorithms, we present new insight into whether Subset
Sum and Knapsack have efficient algorithms in both time and space. In
particular, we show that the time and space complexity of Subset Sum and
Knapsack are equivalent up to a small polynomial factor in the input size. We
also give an algorithm that solves sparse instances of Knapsack efficiently in
terms of space and time. - In parameterized complexity, we present the first
kernelization results on weighted variants of several well-known problems. In
particular, we show that weighted variants of Vertex Cover, Dominating Set,
Traveling Salesman and Knapsack all admit polynomial randomized Turing kernels
when parameterized by $|U|$.
  Curiously, our method relies on a technique more commonly found in
approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4238</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4238</id><created>2012-08-21</created><authors><author><keyname>Siragusa</keyname><forenames>Enrico</forenames></author><author><keyname>Weese</keyname><forenames>David</forenames></author><author><keyname>Reinert</keyname><forenames>Knut</forenames></author></authors><title>Fast and sensitive read mapping with approximate seeds and multiple
  backtracking</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Masai, a read mapper representing the state of the art in terms of
speed and sensitivity. Our tool is an order of magnitude faster than RazerS 3
and mrFAST, 2--3 times faster and more accurate than Bowtie 2 and BWA. The
novelties of our read mapper are filtration with approximate seeds and a method
for multiple backtracking. Approximate seeds, compared to exact seeds, increase
filtration specificity while preserving sensitivity. Multiple backtracking
amortizes the cost of searching a large set of seeds by taking advantage of the
repetitiveness of next-generation sequencing data. Combined together, these two
methods significantly speed up approximate search on genomic datasets. Masai is
implemented in C++ using the SeqAn library. The source code is distributed
under the BSD license and binaries for Linux, Mac OS X and Windows can be
freely downloaded from http://www.seqan.de/projects/masai.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4247</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4247</id><created>2012-08-21</created><updated>2013-01-11</updated><authors><author><keyname>Feng</keyname><forenames>Chunsheng</forenames></author><author><keyname>Shu</keyname><forenames>Shi</forenames></author><author><keyname>Xu</keyname><forenames>Jinchao</forenames></author><author><keyname>Zhang</keyname><forenames>Chen-Song</forenames></author></authors><title>Numerical Study of Geometric Multigrid Methods on CPU--GPU Heterogeneous
  Computers</title><categories>math.NA cs.NA physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The geometric multigrid method (GMG) is one of the most efficient solving
techniques for discrete algebraic systems arising from elliptic partial
differential equations. GMG utilizes a hierarchy of grids or discretizations
and reduces the error at a number of frequencies simultaneously. Graphics
processing units (GPUs) have recently burst onto the scientific computing scene
as a technology that has yielded substantial performance and energy-efficiency
improvements. A central challenge in implementing GMG on GPUs, though, is that
computational work on coarse levels cannot fully utilize the capacity of a GPU.
In this work, we perform numerical studies of GMG on CPU--GPU heterogeneous
computers. Furthermore, we compare our implementation with an efficient CPU
implementation of GMG and with the most popular fast Poisson solver, Fast
Fourier Transform, in the cuFFT library developed by NVIDIA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4269</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4269</id><created>2012-08-21</created><updated>2012-08-21</updated><authors><author><keyname>Macdonald</keyname><forenames>Brian</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Howard</keyname><forenames>Nicholas</forenames></author><author><keyname>Moores</keyname><forenames>Geoffrey</forenames></author></authors><title>Spreaders in the Network SIR Model: An Empirical Study</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We use the susceptible-infected-recovered (SIR) model for disease spread over
a network, and empirically study how well various centrality measures perform
at identifying which nodes in a network will be the best spreaders of disease
on 10 real-world networks. We find that the relative performance of degree,
shell number and other centrality measures can be sensitive to B, the
probability that an infected node will transmit the disease to a susceptible
node. We also find that eigenvector centrality performs very well in general
for values of B above the epidemic threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4270</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4270</id><created>2012-08-21</created><authors><author><keyname>Whang</keyname><forenames>Kyu-Young</forenames></author><author><keyname>Yun</keyname><forenames>Tae-Seob</forenames></author><author><keyname>Yeo</keyname><forenames>Yeon-Mi</forenames></author><author><keyname>Song</keyname><forenames>Il-Yeol</forenames></author><author><keyname>Kwon</keyname><forenames>Hyuk-Yoon</forenames></author><author><keyname>Kim</keyname><forenames>In-Joong</forenames></author></authors><title>ODYS: A Massively-Parallel Search Engine Using a DB-IR
  Tightly-Integrated Parallel DBMS</title><categories>cs.DB</categories><comments>34 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, parallel search engines have been implemented based on scalable
distributed file systems such as Google File System. However, we claim that
building a massively-parallel search engine using a parallel DBMS can be an
attractive alternative since it supports a higher-level (i.e., SQL-level)
interface than that of a distributed file system for easy and less error-prone
application development while providing scalability. In this paper, we propose
a new approach of building a massively-parallel search engine using a DB-IR
tightly-integrated parallel DBMS and demonstrate its commercial-level
scalability and performance. In addition, we present a hybrid (i.e., analytic
and experimental) performance model for the parallel search engine. We have
built a five-node parallel search engine according to the proposed architecture
using a DB-IR tightly-integrated DBMS. Through extensive experiments, we show
the correctness of the model by comparing the projected output with the
experimental results of the five-node engine. Our model demonstrates that ODYS
is capable of handling 1 billion queries per day (81 queries/sec) for 30
billion web pages by using only 43,472 nodes with an average query response
time of 211 ms, which is equivalent to or better than those of commercial
search engines. We also show that, by using twice as many (86,944) nodes, ODYS
can provide an average query response time of 162 ms, which is significantly
lower than those of commercial search engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4289</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4289</id><created>2012-08-21</created><updated>2013-03-04</updated><authors><author><keyname>Zanetti</keyname><forenames>Marcelo Serrano</forenames></author><author><keyname>Sarigol</keyname><forenames>Emre</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>A Quantitative Study of Social Organisation in Open Source Software
  Communities</title><categories>cs.SE cs.SI nlin.AO physics.soc-ph</categories><acm-class>D.2.8; K.4.3</acm-class><journal-ref>ICCSW 2012, pp. 116--122</journal-ref><doi>10.4230/OASIcs.ICCSW.2012.116</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The success of open source projects crucially depends on the voluntary
contributions of a sufficiently large community of users. Apart from the mere
size of the community, interesting questions arise when looking at the
evolution of structural features of collaborations between community members.
In this article, we discuss several network analytic proxies that can be used
to quantify different aspects of the social organisation in social
collaboration networks. We particularly focus on measures that can be related
to the cohesiveness of the communities, the distribution of responsibilities
and the resilience against turnover of community members. We present a
comparative analysis on a large-scale dataset that covers the full history of
collaborations between users of 14 major open source software communities. Our
analysis covers both aggregate and time-evolving measures and highlights
differences in the social organisation across communities. We argue that our
results are a promising step towards the definition of suitable, potentially
multi-dimensional, resilience and risk indicators for open source software
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4290</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4290</id><created>2012-08-21</created><updated>2012-12-06</updated><authors><author><keyname>Blasco</keyname><forenames>Pol</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author></authors><title>A Learning Theoretic Approach to Energy Harvesting Communication System
  Optimization</title><categories>cs.LG cs.NI</categories><journal-ref>IEEE TRANSACTIONS ON WIRELESS COMM UNICATIONS, VOL. 12, NO. 4,
  APRIL 2013</journal-ref><doi>10.1109/TWC.2013.030413.121120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A point-to-point wireless communication system in which the transmitter is
equipped with an energy harvesting device and a rechargeable battery, is
studied. Both the energy and the data arrivals at the transmitter are modeled
as Markov processes. Delay-limited communication is considered assuming that
the underlying channel is block fading with memory, and the instantaneous
channel state information is available at both the transmitter and the
receiver. The expected total transmitted data during the transmitter's
activation time is maximized under three different sets of assumptions
regarding the information available at the transmitter about the underlying
stochastic processes. A learning theoretic approach is introduced, which does
not assume any a priori information on the Markov processes governing the
communication system. In addition, online and offline optimization problems are
studied for the same setting. Full statistical knowledge and causal information
on the realizations of the underlying stochastic processes are assumed in the
online optimization problem, while the offline optimization problem assumes
non-causal knowledge of the realizations in advance. Comparing the optimal
solutions in all three frameworks, the performance loss due to the lack of the
transmitter's information regarding the behaviors of the underlying Markov
processes is quantified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4294</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4294</id><created>2012-08-21</created><authors><author><keyname>Shafi</keyname><forenames>S. Yusef</forenames></author></authors><title>Guaranteeing Spatial Uniformity in Diffusively-Coupled Systems</title><categories>math.DS cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a condition that guarantees spatially uniformity in the solution
trajectories of a diffusively-coupled compartmental ODE model, where each
compartment represents a spatial domain of components interconnected through
diffusion terms with like components in different compartments. Each set of
like components has its own weighted undirected graph describing the topology
of the interconnection between compartments. The condition makes use of the
Jacobian matrix to describe the dynamics of each compartment as well as the
Laplacian eigenvalues of each of the graphs. We discuss linear matrix
inequalities that can be used to verify the condition guaranteeing spatial
uniformity, and apply the result to a coupled oscillator network. Next we turn
to reaction-diffusion PDEs with Neumann boundary conditions, and derive an
analogous condition guaranteeing spatial uniformity of solutions. The paper
contributes a relaxed condition to check spatial uniformity that allows
individual components to have their own specific diffusion terms and
interconnection structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4301</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4301</id><created>2012-08-21</created><authors><author><keyname>Golas</keyname><forenames>Ulrike</forenames></author><author><keyname>Soboll</keyname><forenames>Thomas</forenames></author></authors><title>Proceedings Seventh ACCAT Workshop on Applied and Computational Category
  Theory</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 93, 2012</journal-ref><doi>10.4204/EPTCS.93</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Category Theory is a well-known powerful mathematical modeling language with
a wide area of applications in mathematics and computer science, including
especially the semantical foundations of topics in software science and
development. Categorical methods are already well established for the
semantical foundation of type theory (cartesian closed categories), data type
specification frameworks (institutions) and graph transformation (adhesive high
level replacement categories).
  It is the intention of the ACCAT Workshops on Applied and Computational
Category Theory to bring together leading researchers in these areas with those
in software science and development in order to transfer categorical concepts
and theories in both directions. The workshops aims to represent a forum for
researchers and practitioners who are interested in an exchange of ideas,
notions, and techniques for different applications of category theory.
  The seventh ACCAT workshop on Applied and Computational Category Theory 2012
was held in Tallinn, Estonia on the 1st of April 2012 as a satellite event of
ETAPS 2012. This issue contains the full version of one of the invited talks as
well as the submitted papers, which cover a wide range of applications of
category theory, from model-driven engineering over transition systems in
stochastic processes to transformations in M-adhesive categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4316</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4316</id><created>2012-08-21</created><authors><author><keyname>M</keyname><forenames>Sreeraj.</forenames></author><author><keyname>Idicula</keyname><forenames>Sumam Mary</forenames></author></authors><title>An Online Character Recognition System to Convert Grantha Script to
  Malayalam</title><categories>cs.CV</categories><comments>6 pages, 6 figures</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA, Volume 3 Issue 7, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to recognize Grantha, an ancient script
in South India and converting it to Malayalam, a prevalent language in South
India using online character recognition mechanism. The motivation behind this
work owes its credit to (i) developing a mechanism to recognize Grantha script
in this modern world and (ii) affirming the strong connection among Grantha and
Malayalam. A framework for the recognition of Grantha script using online
character recognition is designed and implemented. The features extracted from
the Grantha script comprises mainly of time-domain features based on writing
direction and curvature. The recognized characters are mapped to corresponding
Malayalam characters. The framework was tested on a bed of medium length
manuscripts containing 9-12 sample lines and printed pages of a book titled
Soundarya Lahari writtenin Grantha by Sri Adi Shankara to recognize the words
and sentences. The manuscript recognition rates with the system are for Grantha
as 92.11%, Old Malayalam 90.82% and for new Malayalam script 89.56%. The
recognition rates of pages of the printed book are for Grantha as 96.16%, Old
Malayalam script 95.22% and new Malayalam script as 92.32% respectively. These
results show the efficiency of the developed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4321</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4321</id><created>2012-08-21</created><authors><author><keyname>Bhat</keyname><forenames>Swaraj</forenames></author><author><keyname>H</keyname><forenames>Pradeep B.</forenames></author><author><keyname>Shetty</keyname><forenames>Keerthi S.</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Formal Verification of Safety Properties for Ownership Authentication
  Transfer Protocol</title><categories>cs.LO</categories><comments>16 pages, 7 figures,Submitted to ADCOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In ubiquitous computing devices, users tend to store some valuable
information in their device. Even though the device can be borrowed by the
other user temporarily, it is not safe for any user to borrow or lend the
device as it may cause private data of the user to be public. To safeguard the
user data and also to preserve user privacy we propose and model the technique
of ownership authentication transfer. The user who is willing to sell the
device has to transfer the ownership of the device under sale. Once the device
is sold and the ownership has been transferred, the old owner will not be able
to use that device at any cost. Either of the users will not be able to use the
device if the process of ownership has not been carried out properly. This also
takes care of the scenario when the device has been stolen or lost, avoiding
the impersonation attack. The aim of this paper is to model basic process of
proposed ownership authentication transfer protocol and check its safety
properties by representing it using CSP and model checking approach. For model
checking we have used a symbolic model checker tool called NuSMV. The safety
properties of ownership transfer protocol has been modeled in terms of CTL
specification and it is observed that the system satisfies all the protocol
constraint and is safe to be deployed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4324</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4324</id><created>2012-08-17</created><authors><author><keyname>Gupta</keyname><forenames>Aditi</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author></authors><title>Characterizing Pedophile Conversations on the Internet using Online
  Grooming</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-crime targeting children such as online pedophile activity are a major
and a growing concern to society. A deep understanding of predatory chat
conversations on the Internet has implications in designing effective solutions
to automatically identify malicious conversations from regular conversations.
We believe that a deeper understanding of the pedophile conversation can result
in more sophisticated and robust surveillance systems than majority of the
current systems relying only on shallow processing such as simple word-counting
or key-word spotting.
  In this paper, we study pedophile conversations from the perspective of
online grooming theory and perform a series of linguistic-based empirical
analysis on several pedophile chat conversations to gain useful insights and
patterns. We manually annotated 75 pedophile chat conversations with six stages
of online grooming and test several hypothesis on it. The results of our
experiments reveal that relationship forming is the most dominant online
grooming stage in contrast to the sexual stage. We use a widely used
word-counting program (LIWC) to create psycho-linguistic profiles for each of
the six online grooming stages to discover interesting textual patterns useful
to improve our understanding of the online pedophile phenomenon. Furthermore,
we present empirical results that throw light on various aspects of a pedophile
conversation such as probability of state transitions from one stage to
another, distribution of a pedophile chat conversation across various online
grooming stages and correlations between pre-defined word categories and online
grooming stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4327</identifier>
 <datestamp>2012-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4327</id><created>2012-08-15</created><authors><author><keyname>Kokash</keyname><forenames>Natallia</forenames></author><author><keyname>Ravara</keyname><forenames>Ant&#xf3;nio</forenames></author></authors><title>Proceedings 11th International Workshop on Foundations of Coordination
  Languages and Self Adaptation</title><categories>cs.DC cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2; D.3.; F.1; F.3; F.4</acm-class><journal-ref>EPTCS 91, 2012</journal-ref><doi>10.4204/EPTCS.91</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Welcome to the proceedings of FOCLASA 2012, the 11th International Workshop
on the Foundations of Coordination Languages and Self-Adaptation. FOCLASA 2012
was held in Newcastle upon Tyne, UK, on September 8, 2012 as a satellite event
of CONCUR 2012, the 23rd International Conference on Concurrency Theory. The
workshop provides a venue where researchers and practitioners could meet,
exchange ideas, identify common problems, determine some of the key and
fundamental issues related to coordination languages and self adaptation, and
explore together and disseminate solutions. Indeed, a number of hot research
topics are currently sharing the common problem of combining concurrent,
distributed, mobile and heterogeneous components, trying to harness the
intrinsic complexity of the resulting systems. Computation nowadays is becoming
inherently concurrent, either because of characteristics of the hardware (with
multicore processors becoming omnipresent) or due to the ubiquitous presence of
distributed systems (incarnated in the Internet). Computational systems are
therefore typically distributed, concurrent, mobile, and often involve
composition of heterogeneous components. To specify and reason about such
systems and go beyond the functional correctness proofs, e.g., by supporting
reusability and improving maintainability, approaches such as coordination
languages and self adaptation are recognised as fundamental.
  This year, we received 13 submissions involving 35 authors from 10 different
countries. Papers underwent a rigorous review process, and all accepted papers
received 3 review reports. After the review process, the international Program
Committee of FOCLASA 2012 decided to select 8 papers for presentation during
the workshop and inclusion in these proceedings. These papers tackle different
issues that are currently central to our community, self-adaptation and
coordination, processes and coordination, and type systems. The workshop
features an invited talk by Sebastian Uchitel from Imperial College London
(UK).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4368</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4368</id><created>2012-08-21</created><authors><author><keyname>Namuduri</keyname><forenames>Kamesh</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Gomathisankaran</keyname><forenames>Mahadevan</forenames></author><author><keyname>Varanasi</keyname><forenames>Murali</forenames></author></authors><title>The Chief Security Officer Problem</title><categories>cs.CR</categories><comments>20 pages and 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the chief security officer (CSO) problem, defines its
scope, and investigates several important research questions related within the
scope. The CSO problem is defined based on the concept of secrecy capacity of
wireless communication channels. It is also related to the chief
Estimation/Executive Officer (CEO) problem that has been well studied in
information theory. The CSO problem consists of a CSO, several agents capable
of having two-way communication with the CSO, and a group of eavesdroppers.
There are two scenarios in the CSO problem; one in which agents are not allowed
to cooperate with one another and the other in which agents are allowed to
cooperate with one another. While there are several research questions relevant
to the CSO problem, this paper focusses on the following and provides answers:
(1) How much information can be exchanged back and forth between the CSO and
the agents without leaking any information to the eavesdroppers? (2) What is
the power allocation strategy that the CSO needs to follow so as to maximize
the secrecy capacity? (3) How can agents cooperate with one another in order to
increase the overall secrecy capacity?.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4374</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4374</id><created>2012-08-21</created><authors><author><keyname>Friesz</keyname><forenames>Terry L.</forenames></author><author><keyname>Kwon</keyname><forenames>Changhyun</forenames></author><author><keyname>Kim</keyname><forenames>Tae Il</forenames></author><author><keyname>Fan</keyname><forenames>Lifan</forenames></author><author><keyname>Yao</keyname><forenames>Tao</forenames></author></authors><title>Competitive Robust Dynamic Pricing in Continuous Time with Fixed
  Inventories</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of robust dynamic pricing of an abstract commodity, whose
inventory is specified at an initial time but never subsequently replenished,
originally studied by Perakis and Sood (2006) in discrete time, is considered
from the perspective of continuous time. We use a multiplicative demand
function to model the uncertain demand, and develop a robust counterpart to
replace the uncertain demand constraint. The sellers' robust best response
problem yields a generalized Nash equilibrium problem, which can be formulated
as an equivalent, continuous-time quasi-variational inequality. We demonstrate
that, for appropriate regularity conditions, a generalized robust Nash
equilibrium exists. We show that the quasi-variational inequality may be
replaced by an equivalent variational inequality, and use a fixed-point
algorithm to solve the variational inequality. We also demonstrate how explicit
time lags associated with price updating in real-world decision environments,
as well as specific pricing decision rules, may be introduced to create a dual
time scale formulation and the associated solutions computed. We illustrate,
via numerical examples, how robust pricing based on our DPFI formulation offers
generally superior and never inferior worst case performance compared to
nominal pricing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4381</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4381</id><created>2012-08-21</created><updated>2013-08-19</updated><authors><author><keyname>Gandica</keyname><forenames>Y.</forenames></author><author><keyname>Medina</keyname><forenames>E.</forenames></author><author><keyname>Bonalde</keyname><forenames>I.</forenames></author></authors><title>A thermodynamic counterpart of the Axelrod model of social influence:
  The one-dimensional case</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>19 pages, 5 figures</comments><doi>10.1016/j.physa.2013.08.033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a thermodynamic version of the Axelrod model of social influence.
In one-dimensional (1D) lattices, the thermodynamic model becomes a coupled
Potts model with a bonding interaction that increases with the site matching
traits. We analytically calculate thermodynamic and critical properties for a
1D system and show that an order-disorder phase transition only occurs at T = 0
independent of the number of cultural traits q and features F. The 1D
thermodynamic Axelrod model belongs to the same universality class of the Ising
and Potts models, notwithstanding the increase of the internal dimension of the
local degree of freedom and the state-dependent bonding interaction. We suggest
a unifying proposal to compare exponents across different discrete 1D models.
The comparison with our Hamiltonian description reveals that in the
thermodynamic limit the original out-of-equilibrium 1D Axelrod model with noise
behaves like an ordinary thermodynamic 1D interacting particle system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4384</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4384</id><created>2012-08-21</created><updated>2013-02-22</updated><authors><author><keyname>Chang</keyname><forenames>Joshua C.</forenames></author><author><keyname>Chou</keyname><forenames>Tom</forenames></author></authors><title>Iterative graph cuts for image segmentation with a nonlinear statistical
  shape prior</title><categories>cs.CV math.OC physics.data-an q-bio.QM stat.AP</categories><comments>Revision submitted to JMIV (02/24/13)</comments><doi>10.1007/s10851-013-0440-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape-based regularization has proven to be a useful method for delineating
objects within noisy images where one has prior knowledge of the shape of the
targeted object. When a collection of possible shapes is available, the
specification of a shape prior using kernel density estimation is a natural
technique. Unfortunately, energy functionals arising from kernel density
estimation are of a form that makes them impossible to directly minimize using
efficient optimization algorithms such as graph cuts. Our main contribution is
to show how one may recast the energy functional into a form that is
minimizable iteratively and efficiently using graph cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4386</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4386</id><created>2012-08-21</created><authors><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Namuduri</keyname><forenames>Kamesh</forenames></author><author><keyname>Fu</keyname><forenames>Shengli</forenames></author></authors><title>Cooperative Communication Based on Random Beamforming Strategy in
  Wireless Sensor Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a two-phase cooperative communication strategy and an
optimal power allocation strategy to transmit sensor observations to a fusion
center in a large-scale sensor network. Outage probability is used to evaluate
the performance of the proposed system. Simulation results demonstrate that: 1)
when signal-to-noise ratio is low, the performance of the proposed system is
better than that of the multiple-input and multiple-output system over
uncorrelated slow fading Rayleigh channels; 2) given the transmission rate and
the total transmission SNR, there exists an optimal power allocation that
minimizes the outage probability; 3) on correlated slow fading Rayleigh
channels, channel correlation will degrade the system performance in linear
proportion to the correlation level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4390</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4390</id><created>2012-08-21</created><authors><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>On secure network coding with uniform wiretap sets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows determining the secrecy capacity of a unicast network with
uniform wiretap sets is at least as difficult as the k-unicast problem. In
particular, we show that a general k-unicast problem can be reduced to the
problem of finding the secrecy capacity of a corresponding single unicast
network with uniform link capacities and one arbitrary wiretap link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4391</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4391</id><created>2012-08-21</created><updated>2013-12-18</updated><authors><author><keyname>Yang</keyname><forenames>Yanchao</forenames></author><author><keyname>Sundaramoorthi</keyname><forenames>Ganesh</forenames></author></authors><title>Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev
  Descent</title><categories>cs.CV cs.SY</categories><comments>Extension of ICCV paper, added coarse-to-fine optimization based on
  new Riemannian manifold of parameterized regions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to track the precise shape of an object in video based on
new modeling and optimization on a new Riemannian manifold of parameterized
regions.
  Joint dynamic shape and appearance models, in which a template of the object
is propagated to match the object shape and radiance in the next frame, are
advantageous over methods employing global image statistics in cases of complex
object radiance and cluttered background. In cases of 3D object motion and
viewpoint change, self-occlusions and dis-occlusions of the object are
prominent, and current methods employing joint shape and appearance models are
unable to adapt to new shape and appearance information, leading to inaccurate
shape detection. In this work, we model self-occlusions and dis-occlusions in a
joint shape and appearance tracking framework.
  Self-occlusions and the warp to propagate the template are coupled, thus a
joint problem is formulated. We derive a coarse-to-fine optimization scheme,
advantageous in object tracking, that initially perturbs the template by coarse
perturbations before transitioning to finer-scale perturbations, traversing all
scales, seamlessly and automatically. The scheme is a gradient descent on a
novel infinite-dimensional Riemannian manifold that we introduce. The manifold
consists of planar parameterized regions, and the metric that we introduce is a
novel Sobolev-type metric defined on infinitesimal vector fields on regions.
The metric has the property of resulting in a gradient descent that
automatically favors coarse-scale deformations (when they reduce the energy)
before moving to finer-scale deformations.
  Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and
background show that occlusion/dis-occlusion modeling leads to superior shape
accuracy compared to recent methods employing joint shape/appearance models or
employing global statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4392</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4392</id><created>2012-08-21</created><authors><author><keyname>Saghaei</keyname><forenames>Hamed</forenames></author></authors><title>A Novel Architecture for Antenna Arrangement in Wireless Cellular CDMA
  Systems</title><categories>cs.NI</categories><comments>4 Pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wise arrangement of antennas is critical in wireless cellular systems for
both reduction of co-channel interference (CCI) and increase the quality of
service (QoS). In this paper, a novel architecture for antenna arrangement in
CDMA wireless cellular systems is presented. In this architecture that we
called microzone, every cell is divided into three (or more) zones and
information transmission in downlink channel is done by an antenna which is
placed at the outer region of the related zone. Also, the transmitting signal
by the mobile station (MS) in uplink channel is received by all antennas of the
related cell. Analytical calculations of the received signal to noise ratio
(SIR) and outage probability for both microzone and used architectures show
that proposed architecture has better performance in compared with the used
architecture. Also, simulation results confirm lower outage probability in
uplink channel for microzone architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4398</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4398</id><created>2012-08-21</created><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>A Unified Approach for Modeling and Recognition of Individual Actions
  and Group Activities</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing group activities is challenging due to the difficulties in
isolating individual entities, finding the respective roles played by the
individuals and representing the complex interactions among the participants.
Individual actions and group activities in videos can be represented in a
common framework as they share the following common feature: both are composed
of a set of low-level features describing motions, e.g., optical flow for each
pixel or a trajectory for each feature point, according to a set of composition
constraints in both temporal and spatial dimensions. In this paper, we present
a unified model to assess the similarity between two given individual or group
activities. Our approach avoids explicit extraction of individual actors,
identifying and representing the inter-person interactions. With the proposed
approach, retrieval from a video database can be performed through
Query-by-Example; and activities can be recognized by querying videos
containing known activities. The suggested video matching process can be
performed in an unsupervised manner. We demonstrate the performance of our
approach by recognizing a set of human actions and football plays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4405</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4405</id><created>2012-08-21</created><updated>2012-08-23</updated><authors><author><keyname>Fish</keyname><forenames>Alexander</forenames></author><author><keyname>Gurevich</keyname><forenames>Shamgar</forenames></author><author><keyname>Hadani</keyname><forenames>Ronny</forenames></author><author><keyname>Sayeed</keyname><forenames>Akbar</forenames></author><author><keyname>Schwartz</keyname><forenames>Oded</forenames></author></authors><title>Delay-Doppler Channel Estimation with Almost Linear Complexity</title><categories>cs.IT math.IT math.NT math.RT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental task in wireless communication is Channel Estimation: Compute
the channel parameters a signal undergoes while traveling from a transmitter to
a receiver. In the case of delay-Doppler channel, a widely used method is the
Matched Filter algorithm. It uses a pseudo-random sequence of length N, and, in
case of non-trivial relative velocity between transmitter and receiver, its
computational complexity is O(N^{2}log(N)). In this paper we introduce a novel
approach of designing sequences that allow faster channel estimation. Using
group representation techniques we construct sequences, which enable us to
introduce a new algorithm, called the flag method, that significantly improves
the matched filter algorithm. The flag method finds the channel parameters in
O(mNlog(N)) operations, for channel of sparsity m. We discuss applications of
the flag method to GPS, radar system, and mobile communication as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4414</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4414</id><created>2012-08-21</created><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Cover</keyname><forenames>Thomas</forenames></author><author><keyname>Kumar</keyname><forenames>Gowtham</forenames></author><author><keyname>Zhao</keyname><forenames>Lei</forenames></author></authors><title>A Lattice of Gambles</title><categories>math.PR cs.IT math.IT</categories><comments>ISIT 2011, 5 pages, 2 eps figures, uses IEEEtran.cls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A gambler walks into a hypothetical fair casino with a very real dollar bill,
but by the time he leaves he's exchanged the dollar for a random amount of
money. What is lost in the process? It may be that the gambler walks out at the
end of the day, after a roller-coaster ride of winning and losing, with his
dollar still intact, or maybe even with two dollars. But what the gambler loses
the moment he places his first bet is position. He exchanges one distribution
of money for a distribution of lesser quality, from which he cannot return. Our
first discussion in this work connects known results of economic inequality and
majorization to the probability theory of gambling and Martingales. We provide
a simple proof that fair gambles cannot increase the Lorenz curve, and we also
constructively demonstrate that any sequence of non-increasing Lorenz curves
corresponds to at least one Martingale.
  We next consider the efficiency of gambles. If all fair gambles are available
then one can move down the lattice of distributions defined by the Lorenz
ordering. However, the step from one distribution to the next is not unique. Is
there a sense of efficiency with which one can move down the Lorenz stream? One
approach would be to minimize the average total volume of money placed on the
table. In this case, it turns out that implementing part of the strategy using
private randomness can help reduce the need for the casino's randomness,
resulting in less money on the table that the casino cannot get its hands on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4415</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4415</id><created>2012-08-21</created><updated>2013-08-20</updated><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Distributed Channel Synthesis</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. on Information Theory (submitted Aug., 2012,
  accepted July, 2013), 26 pages, using IEEEtran.cls</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><journal-ref>IEEE Trans. on Inf. Theory, 59(11):7071-96, November, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two familiar notions of correlation are rediscovered as the extreme operating
points for distributed synthesis of a discrete memoryless channel, in which a
stochastic channel output is generated based on a compressed description of the
channel input. Wyner's common information is the minimum description rate
needed. However, when common randomness independent of the input is available,
the necessary description rate reduces to Shannon's mutual information. This
work characterizes the optimal trade-off between the amount of common
randomness used and the required rate of description. We also include a number
of related derivations, including the effect of limited local randomness, rate
requirements for secrecy, applications to game theory, and new insights into
common information duality.
  Our proof makes use of a soft covering lemma, known in the literature for its
role in quantifying the resolvability of a channel. The direct proof
(achievability) constructs a feasible joint distribution over all parts of the
system using a soft covering, from which the behavior of the encoder and
decoder is inferred, with no explicit reference to joint typicality or binning.
Of auxiliary interest, this work also generalizes and strengthens this soft
covering tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4423</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4423</id><created>2012-08-22</created><updated>2013-04-19</updated><authors><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Estimation in Phase-Shift and Forward Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>28 pages, 5 figures, accepted by IEEE Transactions on Signal
  Processing, Apr. 2013</comments><doi>10.1109/TSP.2013.2260333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network of single-antenna sensors that observe an unknown
deterministic parameter. Each sensor applies a phase shift to the observation
and the sensors simultaneously transmit the result to a multi-antenna fusion
center (FC). Based on its knowledge of the wireless channel to the sensors, the
FC calculates values for the phase factors that minimize the variance of the
parameter estimate, and feeds this information back to the sensors. The use of
a phase-shift-only transmission scheme provides a simplified analog
implementation at the sensor, and also leads to a simpler algorithm design and
performance analysis. We propose two algorithms for this problem, a numerical
solution based on a relaxed semidefinite programming problem, and a closed-form
solution based on the analytic constant modulus algorithm. Both approaches are
shown to provide performance close to the theoretical bound. We derive
asymptotic performance analyses for cases involving large numbers of sensors or
large numbers of FC antennas, and we also study the impact of phase errors at
the sensor transmitters. Finally, we consider the sensor selection problem, in
which only a subset of the sensors is chosen to send their observations to the
FC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4429</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4429</id><created>2012-08-22</created><authors><author><keyname>Hurwitz</keyname><forenames>E.</forenames></author><author><keyname>Marwala</keyname><forenames>T.</forenames></author></authors><title>Common Mistakes when Applying Computational Intelligence and Machine
  Learning to Stock Market modelling</title><categories>stat.AP cs.CY q-fin.GN</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a number of reasons, computational intelligence and machine learning
methods have been largely dismissed by the professional community. The reasons
for this are numerous and varied, but inevitably amongst the reasons given is
that the systems designed often do not perform as expected by their designers.
The reasons for this lack of performance is a direct result of mistakes that
are commonly seen in market-prediction systems. This paper examines some of the
more common mistakes, namely dataset insufficiency; inappropriate scaling;
time-series tracking; inappropriate target quantification and inappropriate
measures of performance. The rationale that leads to each of these mistakes is
examined, as well as the nature of the errors they introduce to the analysis /
design. Alternative ways of performing each task are also recommended in order
to avoid perpetuating these mistakes, and hopefully to aid in clearing the way
for the use of these powerful techniques in industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4434</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4434</id><created>2012-08-22</created><updated>2013-03-27</updated><authors><author><keyname>Vetter</keyname><forenames>Roman</forenames></author><author><keyname>Stoop</keyname><forenames>Norbert</forenames></author><author><keyname>Jenni</keyname><forenames>Thomas</forenames></author><author><keyname>Wittel</keyname><forenames>Falk K.</forenames></author><author><keyname>Herrmann</keyname><forenames>Hans J.</forenames></author></authors><title>Subdivision Shell Elements with Anisotropic Growth</title><categories>cs.NA cs.CE physics.comp-ph</categories><comments>20 pages, 12 figures, 1 table</comments><journal-ref>Int. J. Numer. Meth. Eng. 95, 791-810 (2013)</journal-ref><doi>10.1002/nme.4536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A thin shell finite element approach based on Loop's subdivision surfaces is
proposed, capable of dealing with large deformations and anisotropic growth. To
this end, the Kirchhoff-Love theory of thin shells is derived and extended to
allow for arbitrary in-plane growth. The simplicity and computational
efficiency of the subdivision thin shell elements is outstanding, which is
demonstrated on a few standard loading benchmarks. With this powerful tool at
hand, we demonstrate the broad range of possible applications by numerical
solution of several growth scenarios, ranging from the uniform growth of a
sphere, to boundary instabilities induced by large anisotropic growth. Finally,
it is shown that the problem of a slowly and uniformly growing sheet confined
in a fixed hollow sphere is equivalent to the inverse process where a sheet of
fixed size is slowly crumpled in a shrinking hollow sphere in the frictionless,
quasi-static, elastic limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4436</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4436</id><created>2012-08-22</created><authors><author><keyname>Keeble-Gagn&#xe8;re</keyname><forenames>Gabriel</forenames></author><author><keyname>Nystr&#xf6;m-Persson</keyname><forenames>Johan</forenames></author><author><keyname>Bellgard</keyname><forenames>Matthew</forenames></author><author><keyname>Mizuguchi</keyname><forenames>Kenji</forenames></author></authors><title>An Open Framework for Extensible Multi-Stage Bioinformatics Software</title><categories>cs.SE q-bio.GN</categories><comments>12 pages, 1 figure, to appear in proceedings of PRIB 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In research labs, there is often a need to customise software at every step
in a given bioinformatics workflow, but traditionally it has been difficult to
obtain both a high degree of customisability and good performance.
Performance-sensitive tools are often highly monolithic, which can make
research difficult. We present a novel set of software development principles
and a bioinformatics framework, Friedrich, which is currently in early
development. Friedrich applications support both early stage experimentation
and late stage batch processing, since they simultaneously allow for good
performance and a high degree of flexibility and customisability. These
benefits are obtained in large part by basing Friedrich on the multiparadigm
programming language Scala. We present a case study in the form of a basic
genome assembler and its extension with new functionality. Our architecture has
the potential to greatly increase the overall productivity of software
developers and researchers in bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4439</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4439</id><created>2012-08-22</created><updated>2012-09-04</updated><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author><author><keyname>Ang</keyname><forenames>Li-Minn</forenames></author><author><keyname>Prabaharan</keyname><forenames>S. R. S.</forenames></author><author><keyname>Seng</keyname><forenames>Kah Phooi</forenames></author></authors><title>Radio Frequency Energy Harvesting and Management for Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>40 pages, 9 figures, 5 tables, Book chapter</comments><journal-ref>Green Mobile Devices and Networks: Energy Optimization and
  Scavenging Techniques (CRC Press, Taylor and Francis Group, 2011) pp. 341-368</journal-ref><doi>10.1201/b10081-16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio Frequency (RF) Energy Harvesting holds a promising future for
generating a small amount of electrical power to drive partial circuits in
wirelessly communicating electronics devices. Reducing power consumption has
become a major challenge in wireless sensor networks. As a vital factor
affecting system cost and lifetime, energy consumption in wireless sensor
networks is an emerging and active research area. This chapter presents a
practical approach for RF Energy harvesting and management of the harvested and
available energy for wireless sensor networks using the Improved Energy
Efficient Ant Based Routing Algorithm (IEEABR) as our proposed algorithm. The
chapter looks at measurement of the RF power density, calculation of the
received power, storage of the harvested power, and management of the power in
wireless sensor networks. The routing uses IEEABR technique for energy
management. Practical and real-time implementations of the RF Energy using
Powercast harvesters and simulations using the energy model of our Libelium
Waspmote to verify the approach were performed. The chapter concludes with
performance analysis of the harvested energy, comparison of IEEABR and other
traditional energy management techniques, while also looking at open research
areas of energy harvesting and management for wireless sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4449</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4449</id><created>2012-08-22</created><authors><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Finding a maximum induced degenerate subgraph faster than 2^n</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of finding a maximum induced d-degenerate
subgraph in a given n-vertex graph from the point of view of exact algorithms.
We show that for any fixed d one can find a maximum induced d-degenerate
subgraph in randomized (2-eps_d)^n n^O(1) time, for some constant eps_d&gt;0
depending only on d. Moreover, our algorithm can be used to sample
inclusion-wise maximal induced d-degenerate subgraphs in such a manner that
every such subgraph is output with probability at least (2-eps_d)^-n; hence, we
prove that their number is bounded by (2-eps_d)^n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4455</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4455</id><created>2012-08-22</created><updated>2014-05-08</updated><authors><author><keyname>Hawtin</keyname><forenames>Daniel R.</forenames></author><author><keyname>Gillespie</keyname><forenames>Neil I.</forenames></author><author><keyname>Praeger</keyname><forenames>Cheryl E.</forenames></author></authors><title>Elusive Codes in Hamming Graphs</title><categories>math.CO cs.IT math.IT</categories><msc-class>94B60, 05E18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a code to be a subset of the vertex set of a Hamming graph. We
examine elusive pairs, code-group pairs where the code is not determined by
knowledge of its set of neighbours. We construct a new infinite family of
elusive pairs, where the group in question acts transitively on the set of
neighbours of the code. In our examples, we find that the alphabet size always
divides the length of the code, and prove that there is no elusive pair for the
smallest set of parameters for which this is not the case. We also pose several
questions regarding elusive pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4469</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4469</id><created>2012-08-22</created><authors><author><keyname>Bafghi</keyname><forenames>Hamid G.</forenames></author><author><keyname>Seyfe</keyname><forenames>Babak</forenames></author></authors><title>On The Secrecy of the Cognitive Interference Channel with Channel State</title><categories>cs.IT math.IT</categories><comments>9 pages; Rep. no. 1, Agu. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the secrecy problem in the cognitive statedependent
interference channel is considered. In this scenario we have a primary and a
cognitive transmitter-receiver pairs. The cognitive transmitter has the message
of the primary sender as side information. In addition, the state of the
channel is known at the cognitive encoder. So, the cognitive encoder uses this
side information to cooperate with the primary transmitter and sends its
individual message confidentially. An achievable rate region and an outer bound
for the rate region in this channel are derived. The results are extended to
the previous works as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4475</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4475</id><created>2012-08-22</created><updated>2013-02-15</updated><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Information-Theoretic Measures of Influence Based on Content Dynamics</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>10 pages, 10 figures. In Proceedings of the 6th International
  Conference on Web Search and Data Mining. (v2: title change, v3: minor edits)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental building block of social influence is for one person to
elicit a response in another. Researchers measuring a &quot;response&quot; in social
media typically depend either on detailed models of human behavior or on
platform-specific cues such as re-tweets, hash tags, URLs, or mentions. Most
content on social networks is difficult to model because the modes and
motivation of human expression are diverse and incompletely understood. We
introduce content transfer, an information-theoretic measure with a predictive
interpretation that directly quantifies the strength of the effect of one
user's content on another's in a model-free way. Estimating this measure is
made possible by combining recent advances in non-parametric entropy estimation
with increasingly sophisticated tools for content representation. We
demonstrate on Twitter data collected for thousands of users that content
transfer is able to capture non-trivial, predictive relationships even for
pairs of users not linked in the follower or mention graph. We suggest that
this measure makes large quantities of previously under-utilized social media
content accessible to rigorous statistical causal analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4484</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4484</id><created>2012-08-22</created><authors><author><keyname>Chaudhuri</keyname><forenames>Sruti Gan</forenames></author><author><keyname>Mukhopadhyaya</keyname><forenames>Krishnendu</forenames></author></authors><title>Leader Election and Gathering for Asynchronous Transparent Fat Robots
  without Chirality</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a distributed algorithm which deterministically gathers n
(n &gt; 4) asynchronous, fat robots. The robots are assumed to be transparent and
they have full visibility. The robots are initially considered to be
stationary. A robot is visible in its motion. The robots do not store past
actions. They are anonymous and can not be distinguished by their appearances
and do not have common coordinate system or chirality. The robots do not
communicate through message passing. In the proposed gathering algorithm one
robot moves at a time towards its destination. The robot which moves, is
selected in such a way that, it will be the only robot eligible to move, until
it reaches its destination. In case of a tie, this paper proposes a leader
election algorithm which produces an ordering of the robots and the first robot
in the ordering becomes the leader. The ordering is unique in the sense that,
each robot, characterized by its location, agrees on the same ordering. We show
that if a set of robots can be ordered then they can gather deterministically.
The paper also characterizes the cases, where ordering is not possible. This
paper also presents an important fact that, if leader election is possible then
gathering pattern formation is possible even with no chirality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4490</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4490</id><created>2012-08-22</created><updated>2012-09-30</updated><authors><author><keyname>Zabolotny</keyname><forenames>Wojciech M.</forenames></author></authors><title>Efficient transmission of measurement data from FPGA to embedded system
  via Ethernet link</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a system consisting of the FPGA IP core, the simple
network protocol and the Linux device driver, capable of efficient and reliable
data transmission from a low resources FPGA chip to the Linux-based embedded
computer system, via a private Ethernet network (consisting of a single segment
or a few segments connected via an Ethernet switch). The embedded system may
optionally process the acquired data, and distribute them further, using
standard network protocols. Proposed design targets cost-efficient multichannel
data acquisition systems, in which multiple FPGA based front end boards (FEB)
should transmit the stream of acquired data to the computer network,
responsible for their final processing and archiving. The presented solution
allows to minimize the cost of data concentration due to use of inexpensive
Ethernet network infrastructure. The work is mainly focused on minimization of
resources consumption in the FPGA, and minimization of acknowledge latency in
the Linux based system - which allows to achieve high throughput in spite of
use of inexpensive FPGA chips with small internal memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4501</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4501</id><created>2012-08-22</created><authors><author><keyname>Krishnaswamy</keyname><forenames>Srinivasan</forenames></author><author><keyname>Pillai</keyname><forenames>Harish K.</forenames></author></authors><title>On Multisequences and their extensions</title><categories>cs.CR</categories><comments>27 pages 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with the dimension of multisequences and related
properties. For a given multisequence W and an m tuple of positive integers R,
we define the R extension of W. Further we count the number of multisequences W
whose R extensions have maximum dimension and give an algorithm to derive such
multisequences. We then go on to use this theory to count the number of Linear
Feedback Shift Register(LFSR) configurations with multi input multi output
delay blocks for any given primitive characteristic polynomial and also to
design such LFSRs. Further, we use the result on multisequences to count the
number of Hankel matrices of any given dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4503</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4503</id><created>2012-08-22</created><authors><author><keyname>Hicham</keyname><forenames>Gueddah</forenames></author></authors><title>Introduction of the weight edition errors in the Levenshtein distance</title><categories>cs.CL</categories><comments>3 pages, 5 figures; International Journal of Advanced Research in
  Artificial Intelligence (IJARAI)2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new approach dedicated to correcting the spelling
errors of the Arabic language. This approach corrects typographical errors like
inserting, deleting, and permutation. Our method is inspired from the
Levenshtein algorithm, and allows a finer and better scheduling than
Levenshtein. The results obtained are very satisfactory and encouraging, which
shows the interest of our new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4505</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4505</id><created>2012-08-22</created><authors><author><keyname>Golbabaee</keyname><forenames>Mohammad</forenames></author><author><keyname>Arberet</keyname><forenames>Simon</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Compressive Source Separation: Theory and Methods for Hyperspectral
  Imaging</title><categories>cs.IT math.IT</categories><comments>32 pages</comments><doi>10.1109/TIP.2013.2281405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of numbers of high resolution data acquisition systems
and the global requirement to lower the energy consumption, the development of
efficient sensing techniques becomes critical. Recently, Compressed Sampling
(CS) techniques, which exploit the sparsity of signals, have allowed to
reconstruct signal and images with less measurements than the traditional
Nyquist sensing approach. However, multichannel signals like Hyperspectral
images (HSI) have additional structures, like inter-channel correlations, that
are not taken into account in the classical CS scheme. In this paper we exploit
the linear mixture of sources model, that is the assumption that the
multichannel signal is composed of a linear combination of sources, each of
them having its own spectral signature, and propose new sampling schemes
exploiting this model to considerably decrease the number of measurements
needed for the acquisition and source separation. Moreover, we give theoretical
lower bounds on the number of measurements required to perform reconstruction
of both the multichannel signal and its sources. We also proposed optimization
algorithms and extensive experimentation on our target application which is
HSI, and show that our approach recovers HSI with far less measurements and
computational effort than traditional CS approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4508</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4508</id><created>2012-08-22</created><updated>2013-03-24</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author></authors><title>Optimal Spectrum Access for Cognitive Radios</title><categories>cs.IT cs.NI math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1206.6153</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a time-slotted cognitive setting with buffered
primary and secondary users. In order to alleviate the negative effects of
misdetection and false alarm probabilities, a novel design of spectrum access
mechanism is proposed. We propose two schemes. First, the SU senses primary
channel to exploit the periods of silence, if the PU is declared to be idle,
the SU randomly accesses the channel with some access probability $a_s$.
Second, in addition to accessing the channel if the PU is idle, the SU possibly
accesses the channel if it is declared to be busy with some access probability
$b_s$. The access probabilities as function of the misdetection, false alarm
and average primary arrival rate are obtained via solving an optimization
problem designed to maximize the secondary service rate given a constraint on
primary queue stability. In addition, we propose a variable sensing duration
schemes where the SU optimizes over the optimal sensing time to achieve the
maximum stable throughput of the network. The results reveal the performance
gains of the proposed schemes over the conventional sensing scheme. We propose
a method to estimate the mean arrival rate and the outage probability of the PU
based on the primary feedback channel, i.e., acknowledgments (ACKs) and
negative-acknowledgments (NACKs) messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4511</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4511</id><created>2012-08-22</created><updated>2013-07-15</updated><authors><author><keyname>Tao</keyname><forenames>Yufei</forenames></author><author><keyname>Yoon</keyname><forenames>Jeonghun</forenames></author></authors><title>Optimal Planar Range Skyline Reporting with Linear Space in External
  Memory</title><categories>cs.DS</categories><comments>This article, after a merge with an article by Casper
  Kejlberg-Rasmussen, Konstantinos Tsakalidis, and Kostas Tsichlas, has appeard
  in PODS'13. The merged article can be found at http://arxiv.org/abs/1306.2815</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P be a set of n points in R^2. Given a rectangle Q = [\alpha_1, \alpha_2]
x [\beta_1, \beta_2], a range skyline query returns the maxima of the points in
P \cap Q. An important variant is the so-called top-open queries, where Q is a
3-sided rectangle whose upper edge is grounded at y = \infty (that is, \beta_2
= \infty). These queries are crucial in numerous database applications. In
internal memory, extensive research has been devoted to designing data
structures that can answer such queries efficiently. In contrast, currently
there is no clear understanding about their exact complexities in external
memory.
  This paper presents several structures of linear size for answering the above
queries with the optimal I/O cost. We show that a top-open query can be solved
in O(log_B(n) + k/B) I/Os, where B is the block size and k is the number of
points in the query result. The query cost can be made O(log log_B(U) + k/B)
when the data points lie in a U x U grid for some integer U &gt;= n, and further
lowered to O(1 + k/B) if U = O(n). The same efficiency also applies to 3-sided
queries where Q is a right-open rectangle. However, the hardness of the problem
increases if Q is a left- or bottom-open 3-sided rectangle. We prove that any
linear-size structure must perform \Omega((n/B)^\eps + k/B) I/Os to solve such
a query in the worst case, where \eps &gt; 0 can be an arbitrarily small constant.
In fact, left- and right-open queries are just as difficult as general
(4-sided) queries, for which we give a linear-size structure with query time
O((n/B)^\eps + k/B). Interestingly, this indicates that 4-sided range skyline
queries have exactly the same hardness as 4-sided range reporting (where the
goal is to report simply the whole P \cap Q). That is, the skyline requirement
does not alter the problem difficulty at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4516</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4516</id><created>2012-08-22</created><updated>2014-03-26</updated><authors><author><keyname>Tao</keyname><forenames>Yufei</forenames></author></authors><title>A Dynamic I/O-Efficient Structure for One-Dimensional Top-k Range
  Reporting</title><categories>cs.DS</categories><comments>In PODS'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a structure in external memory for &quot;top-k range reporting&quot;, which
uses linear space, answers a query in O(lg_B n + k/B) I/Os, and supports an
update in O(lg_B n) amortized I/Os, where n is the input size, and B is the
block size. This improves the state of the art which incurs O(lg^2_B n)
amortized I/Os per update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4528</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4528</id><created>2012-07-13</created><authors><author><keyname>Ahmed</keyname><forenames>E.</forenames></author><author><keyname>Shehata</keyname><forenames>M. I.</forenames></author><author><keyname>El-Saka</keyname><forenames>H. A. A.</forenames></author></authors><title>On Dynamical Cournot Game on a Graph</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cournot dynamical game is studied on a graph. The stability of the system is
studied. Prisoner's dilemma game is used to model natural gas transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4536</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4536</id><created>2012-06-05</created><updated>2013-10-08</updated><authors><author><keyname>Bartel</keyname><forenames>Alexandre</forenames><affiliation>SnT</affiliation></author><author><keyname>Klein</keyname><forenames>Jacques</forenames><affiliation>SnT</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Allix</keyname><forenames>Kevin</forenames><affiliation>SnT</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>SnT</affiliation></author></authors><title>In-Vivo Bytecode Instrumentation for Improving Privacy on Android
  Smartphones in Uncertain Environments</title><categories>cs.CR cs.SE</categories><comments>ISBN: 978-2-87971-111-9</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we claim that an efficient and readily applicable means to
improve privacy of Android applications is: 1) to perform runtime monitoring by
instrumenting the application bytecode and 2) in-vivo, i.e. directly on the
smartphone. We present a tool chain to do this and present experimental results
showing that this tool chain can run on smartphones in a reasonable amount of
time and with a realistic effort. Our findings also identify challenges to be
addressed before running powerful runtime monitoring and instrumentations
directly on smartphones. We implemented two use-cases leveraging the tool
chain: BetterPermissions, a fine-grained user centric permission policy system
and AdRemover an advertisement remover. Both prototypes improve the privacy of
Android systems thanks to in-vivo bytecode instrumentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4549</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4549</id><created>2012-08-22</created><updated>2012-09-28</updated><authors><author><keyname>Finkel</keyname><forenames>Alain</forenames><affiliation>LSV, ENS Cachan, CNRS</affiliation></author><author><keyname>Goubault-Larrecq</keyname><forenames>Jean</forenames><affiliation>LSV, ENS Cachan, CNRS, INRIA</affiliation></author></authors><title>Forward Analysis for WSTS, Part II: Complete WSTS</title><categories>cs.LO</categories><comments>35 pages, 6 figures. An extended abstract already appeared in Proc.
  36th Intl. Coll. Automata, Languages and Programming (ICALP'09)</comments><proxy>LMCS</proxy><acm-class>D.2.4, F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  29, 2012) lmcs:1217</journal-ref><doi>10.2168/LMCS-8(3:28)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a simple, conceptual forward analysis procedure for
infinity-complete WSTS S. This computes the so-called clover of a state. When S
is the completion of a WSTS X, the clover in S is a finite description of the
downward closure of the reachability set. We show that such completions are
infinity-complete exactly when X is an omega-2-WSTS, a new robust class of
WSTS. We show that our procedure terminates in more cases than the generalized
Karp-Miller procedure on extensions of Petri nets and on lossy channel systems.
We characterize the WSTS where our procedure terminates as those that are
clover-flattable. Finally, we apply this to well-structured counter systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4552</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4552</id><created>2012-08-22</created><authors><author><keyname>Medo</keyname><forenames>Matus</forenames></author></authors><title>Network-based information filtering algorithms: ranking and
  recommendation</title><categories>cs.SI cs.IR physics.data-an physics.soc-ph</categories><comments>book chapter; 21 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After the Internet and the World Wide Web have become popular and
widely-available, the electronically stored online interactions of individuals
have fast emerged as a challenge for researchers and, perhaps even faster, as a
source of valuable information for entrepreneurs. We now have detailed records
of informal friendship relations in social networks, purchases on e-commerce
sites, various sorts of information being sent from one user to another, online
collections of web bookmarks, and many other data sets that allow us to pose
questions that are of interest from both academical and commercial point of
view. For example, which other users of a social network you might want to be
friend with? Which other items you might be interested to purchase? Who are the
most influential users in a network? Which web page you might want to visit
next? All these questions are not only interesting per se but the answers to
them may help entrepreneurs provide better service to their customers and,
ultimately, increase their profits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4566</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4566</id><created>2012-08-22</created><updated>2013-09-23</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Scientometrics</title><categories>cs.DL</categories><comments>The International Encyclopedia of Social and Behavioral Sciences,
  Section 8.5: Science and Technology Studies, Subsection 85030, 2nd Edition.
  James D. Wright, Michael Lynch et al. (Eds.). Oxford, UK, etc.: Elsevier,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provides an overview of the field of scientometrics, that is: the
study of science, technology, and innovation from a quantitative perspective.
We cover major historical milestones in the development of this specialism from
the 1960s to today and discuss its relationship with the sociology of
scientific knowledge, the library and information sciences, and science policy
issues such as indicator development. The disciplinary organization of
scientometrics is analyzed both conceptually and empirically, using a map of
journals cited in the core journal of the field, entitled Scientometrics. A
state-of-the-art review of five major research threads is provided: (1) the
measurement of impact; (2) the delineation of reference sets; (3) theories of
citation; (4) mapping science; and (5) the policy and management contexts of
indicator developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4568</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4568</id><created>2012-07-22</created><authors><author><keyname>Slobbe</keyname><forenames>J.</forenames></author><author><keyname>Verberkt</keyname><forenames>S. L. C.</forenames></author></authors><title>Hacktivists: Cyberterrorists or Online Activists?</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last decade, online activism has vastly grown. In the current digital
society, from time to time citizens decide to express their opinion by
attacking large corporations digitally in some way. Where the activists claim
this to be a digital assembly, others see it as criminal offences.
  In this paper, we will explore the legal and technical borders of the digital
right to assembly. By doing so, we can gain insight into digital manifestations
and make up the balance on the digital right to assembly. As an additional
contribution, we will discuss how the digital right to assembly could be
granted and which legal and technical requirements should be set for a digital
assembly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4571</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4571</id><created>2012-07-24</created><authors><author><keyname>Giannakos</keyname><forenames>Michail N.</forenames></author><author><keyname>Mikalef</keyname><forenames>Patrick</forenames></author></authors><title>In the Face (book) of Social Learning</title><categories>cs.CY cs.SI</categories><comments>12 pages, 4 figures, 1 table</comments><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have risen to prominence over the last years as the
predominant form of electronic interaction between individuals. In an attempt
to harness the power of the large user base which they have managed to attract,
this study proposes an e-learning prototype which integrates concepts of the
social and semantic web. A selected set of services are deployed which have
been scientifically proven to positively impact the learning process of users
via electronic means. The integrability of these services into a social network
platform application is visualized through an exploratory prototype. The
Graphical User Interface (GUI) which is developed to implement these key
features is in alignment with User-Centered principles. The designed prototype
proves that a number of services can be integrated in a user-friendly
application and can potentially serve to gain feedback regarding additional
aspects that should be included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4572</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4572</id><created>2012-08-22</created><authors><author><keyname>Poss</keyname><forenames>Raphael</forenames></author></authors><title>SL: a &quot;quick and dirty&quot; but working intermediate language for SVP
  systems</title><categories>cs.PL cs.DC</categories><comments>22 pages, 3 figures, 18 listings, 1 table</comments><acm-class>C.1.3; D.1.3; D.3.3; D.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CSA group at the University of Amsterdam has developed SVP, a framework
to manage and program many-core and hardware multithreaded processors. In this
article, we introduce the intermediate language SL, a common vehicle to program
SVP platforms. SL is designed as an extension to the standard C language (ISO
C99/C11). It includes primitive constructs to bulk create threads, bulk
synchronize on termination of threads, and communicate using word-sized
dataflow channels between threads. It is intended for use as target language
for higher-level parallelizing compilers. SL is a research vehicle; as of this
writing, it is the only interface language to program a main SVP platform, the
new Microgrid chip architecture. This article provides an overview of the
language, to complement a detailed specification available separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4578</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4578</id><created>2012-08-22</created><updated>2014-12-10</updated><authors><author><keyname>Storath</keyname><forenames>Martin</forenames></author><author><keyname>Demaret</keyname><forenames>Laurent</forenames></author><author><keyname>Massopust</keyname><forenames>Peter</forenames></author></authors><title>Signal Analysis based on Complex Wavelet Signs</title><categories>math.NA cs.NA</categories><msc-class>42C40, 94A12, 44A15</msc-class><doi>10.1016/j.acha.2015.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a signal analysis tool based on the sign (or the phase) of complex
wavelet coefficients, which we call a signature. The signature is defined as
the fine-scale limit of the signs of a signal's complex wavelet coefficients.
We show that the signature equals zero at sufficiently regular points of a
signal whereas at salient features, such as jumps or cusps, it is non-zero. At
such feature points, the orientation of the signature in the complex plane can
be interpreted as an indicator of local symmetry and antisymmetry. We establish
that the signature rotates in the complex plane under fractional Hilbert
transforms. We show that certain random signals, such as white Gaussian noise
and Brownian motions, have a vanishing signature. We derive an appropriate
discretization and show the applicability to signal analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4583</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4583</id><created>2012-07-26</created><authors><author><keyname>Fogarasi</keyname><forenames>N.</forenames></author><author><keyname>Tornai</keyname><forenames>K.</forenames></author><author><keyname>Levendovszky</keyname><forenames>J.</forenames></author></authors><title>A novel Hopfield neural network approach for minimizing total weighted
  tardiness of jobs scheduled on identical machines</title><categories>cs.NE</categories><msc-class>90C27</msc-class><acm-class>G.1.6</acm-class><journal-ref>Acta Universitatis Sapientiae, Informatica, 4, 1 (2012) 48-66</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores fast, polynomial time heuristic approximate solutions to
the NP-hard problem of scheduling jobs on N identical machines. The jobs are
independent and are allowed to be stopped and restarted on another machine at a
later time. They have well-de?ned deadlines, and relative priorities quantified
by non-negative real weights. The objective is to find schedules which minimize
the total weighted tardiness (TWT) of all jobs. We show how this problem can be
mapped into quadratic form and present a polynomial time heuristic solution
based on the Hop?eld Neural Network (HNN) approach. It is demonstrated, through
the results of extensive numerical simulations, that this solution outperforms
other popular heuristic methods. The proposed heuristic is both theoretically
and empirically shown to be scalable to large problem sizes (over 100 jobs to
be scheduled), which makes it applicable to grid computing scheduling, arising
in fields such as computational biology, chemistry and finance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4586</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4586</id><created>2012-08-22</created><updated>2013-02-01</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>Differentially Private Data Analysis of Social Networks via Restricted
  Sensitivity</title><categories>cs.CR cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of restricted sensitivity as an alternative to global
and smooth sensitivity to improve accuracy in differentially private data
analysis. The definition of restricted sensitivity is similar to that of global
sensitivity except that instead of quantifying over all possible datasets, we
take advantage of any beliefs about the dataset that a querier may have, to
quantify over a restricted class of datasets. Specifically, given a query f and
a hypothesis H about the structure of a dataset D, we show generically how to
transform f into a new query f_H whose global sensitivity (over all datasets
including those that do not satisfy H) matches the restricted sensitivity of
the query f. Moreover, if the belief of the querier is correct (i.e., D is in
H) then f_H(D) = f(D). If the belief is incorrect, then f_H(D) may be
inaccurate.
  We demonstrate the usefulness of this notion by considering the task of
answering queries regarding social-networks, which we model as a combination of
a graph and a labeling of its vertices. In particular, while our generic
procedure is computationally inefficient, for the specific definition of H as
graphs of bounded degree, we exhibit efficient ways of constructing f_H using
different projection-based techniques. We then analyze two important query
classes: subgraph counting queries (e.g., number of triangles) and local
profile queries (e.g., number of people who know a spy and a computer-scientist
who know each other). We demonstrate that the restricted sensitivity of such
queries can be significantly lower than their smooth sensitivity. Thus, using
restricted sensitivity we can maintain privacy whether or not D is in H, while
providing more accurate results in the event that H holds true.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4589</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4589</id><created>2012-07-16</created><authors><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Xiao</keyname><forenames>Nan</forenames></author><author><keyname>You</keyname><forenames>Keyou</forenames></author><author><keyname>Sim</keyname><forenames>Kai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Road Pricing for Spreading Peak Travel: Modeling and Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A case study of the Singapore road network provides empirical evidence that
road pricing can significantly affect commuter trip timing behaviors. In this
paper, we propose a model of trip timing decisions that reasonably matches the
observed commuters' behaviors. Our model explicitly captures the difference in
individuals' sensitivity to price, travel time and early or late arrival at
destination. New pricing schemes are suggested to better spread peak travel and
reduce traffic congestion. Simulation results based on the proposed model are
provided in comparison with the real data for the Singapore case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4618</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4618</id><created>2012-08-22</created><updated>2012-10-07</updated><authors><author><keyname>Betts</keyname><forenames>Robert J.</forenames></author></authors><title>A nonconstructive Proof to show the Convergence of the $n^{th}$ root of
  diagonal Ramsey Number $r(n, n)$</title><categories>math.NT cs.DM math.CO</categories><comments>27 pages. Abstract revised. Further details added to the proof of
  Theorem 2.2. Two lemmas added, Section 5. Only helpful comments or
  suggestions from those with specializations in analysis and in graph theory</comments><msc-class>26A03 (Primary) 26A12 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Does the $n^{th}$ root of the diagonal Ramsey number converge to a finite
limit? The answer is yes. A sequence can be shown to converge if it satifies
convergence conditions other than or besides monotonicity. We show such a
property holds for which the sequence of $n^{th}$ roots does converge, even if
one has no a priori knowledge as to whether the sequence is monotone or not. We
show also the $n^{th}$ root of the diagonal Ramsey number can be expressed as a
product of two factors, the first being a known convergent sequence and the
second being an absolutely convergent infinite series. One also can express it
where one product is convergent and the other has all its values from a
uniformly convergent complex function holomorphic within the unit disc on the
complex plane. Our motivation solely is to prove the conjecture as a problem in
search of a solution, not to establish some deep theory about graphs. A second
question is: If the limit exists what is it? At the time of this writing the
understanding is the proofs sought need not be constructive. Here we show by
nonconstructive proofs that the $n^{th}$ root of the diagonal Ramsey number
converges to a finite limit. We also show that the limit of the $j^{th}$ root
of the diagonal Ramsey number is two, where positive integer $j$ depends upon
the Ramsey number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4630</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4630</id><created>2012-08-22</created><authors><author><keyname>Bj&#xf8;rk</keyname><forenames>Joakim</forenames><affiliation>University of Oslo</affiliation></author><author><keyname>Clarke</keyname><forenames>Dave</forenames><affiliation>Katholieke Universiteit Leuven</affiliation></author><author><keyname>Johnsen</keyname><forenames>Einar Broch</forenames><affiliation>University of Oslo</affiliation></author><author><keyname>Owe</keyname><forenames>Olaf</forenames><affiliation>University of Oslo</affiliation></author></authors><title>A Type-Safe Model of Adaptive Object Groups</title><categories>cs.PL cs.LO</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 91, 2012, pp. 1-15</journal-ref><doi>10.4204/EPTCS.91.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Services are autonomous, self-describing, technology-neutral software units
that can be described, published, discovered, and composed into software
applications at runtime. Designing software services and composing services in
order to form applications or composite services requires abstractions beyond
those found in typical object-oriented programming languages. This paper
explores service-oriented abstractions such as service adaptation, discovery,
and querying in an object-oriented setting. We develop a formal model of
adaptive object-oriented groups which offer services to their environment.
These groups fit directly into the object-oriented paradigm in the sense that
they can be dynamically created, they have an identity, and they can receive
method calls. In contrast to objects, groups are not used for structuring code.
A group exports its services through interfaces and relies on objects to
implement these services. Objects may join or leave different groups. Groups
may dynamically export new interfaces, they support service discovery, and they
can be queried at runtime for the interfaces they support. We define an
operational semantics and a static type system for this model of adaptive
object groups, and show that well-typed programs do not cause
method-not-understood errors at runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4632</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4632</id><created>2012-08-22</created><authors><author><keyname>Charalambides</keyname><forenames>Minas</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Dinges</keyname><forenames>Peter</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Agha</keyname><forenames>Gul</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author></authors><title>Parameterized Concurrent Multi-Party Session Types</title><categories>cs.PL cs.DC</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 91, 2012, pp. 16-30</journal-ref><doi>10.4204/EPTCS.91.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session types have been proposed as a means of statically verifying
implementations of communication protocols. Although prior work has been
successful in verifying some classes of protocols, it does not cope well with
parameterized, multi-actor scenarios with inherent asynchrony. For example, the
sliding window protocol is inexpressible in previously proposed session type
systems. This paper describes System-A, a new typing language which overcomes
many of the expressiveness limitations of prior work. System-A explicitly
supports asynchrony and parallelism, as well as multiple forms of
parameterization. We define System-A and show how it can be used for the static
verification of a large class of asynchronous communication protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4634</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4634</id><created>2012-08-22</created><authors><author><keyname>Ciobanu</keyname><forenames>Gabriel</forenames><affiliation>Romanian Academy, Institute of Computer Science</affiliation></author><author><keyname>Horne</keyname><forenames>Ross</forenames><affiliation>Romanian Academy, Institute of Computer Science</affiliation></author></authors><title>A Provenance Tracking Model for Data Updates</title><categories>cs.DC cs.DB</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><acm-class>F.1.2</acm-class><journal-ref>EPTCS 91, 2012, pp. 31-44</journal-ref><doi>10.4204/EPTCS.91.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For data-centric systems, provenance tracking is particularly important when
the system is open and decentralised, such as the Web of Linked Data. In this
paper, a concise but expressive calculus which models data updates is
presented. The calculus is used to provide an operational semantics for a
system where data and updates interact concurrently. The operational semantics
of the calculus also tracks the provenance of data with respect to updates.
This provides a new formal semantics extending provenance diagrams which takes
into account the execution of processes in a concurrent setting. Moreover, a
sound and complete model for the calculus based on ideals of series-parallel
DAGs is provided. The notion of provenance introduced can be used as a
subjective indicator of the quality of data in concurrent interacting systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4635</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4635</id><created>2012-08-22</created><authors><author><keyname>Iftikhar</keyname><forenames>M. Usman</forenames><affiliation>Linnaeus University</affiliation></author><author><keyname>Weyns</keyname><forenames>Danny</forenames><affiliation>Linnaeus University</affiliation></author></authors><title>A Case Study on Formal Verification of Self-Adaptive Behaviors in a
  Decentralized System</title><categories>cs.SE</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 91, 2012, pp. 45-62</journal-ref><doi>10.4204/EPTCS.91.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-adaptation is a promising approach to manage the complexity of modern
software systems. A self-adaptive system is able to adapt autonomously to
internal dynamics and changing conditions in the environment to achieve
particular quality goals. Our particular interest is in decentralized
self-adaptive systems, in which central control of adaptation is not an option.
One important challenge in self-adaptive systems, in particular those with
decentralized control of adaptation, is to provide guarantees about the
intended runtime qualities. In this paper, we present a case study in which we
use model checking to verify behavioral properties of a decentralized
self-adaptive system. Concretely, we contribute with a formalized architecture
model of a decentralized traffic monitoring system and prove a number of
self-adaptation properties for flexibility and robustness. To model the main
processes in the system we use timed automata, and for the specification of the
required properties we use timed computation tree logic. We use the Uppaal tool
to specify the system and verify the flexibility and robustness properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4650</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4650</id><created>2012-08-22</created><updated>2013-02-03</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Li</keyname><forenames>Baiyu</forenames></author></authors><title>Syntactic Complexity of R- and J-Trivial Regular Languages</title><categories>cs.FL</categories><comments>17 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic complexity of a regular language is the cardinality of its
syntactic semigroup. The syntactic complexity of a subclass of the class of
regular languages is the maximal syntactic complexity of languages in that
class, taken as a function of the state complexity n of these languages. We
study the syntactic complexity of R- and J-trivial regular languages, and prove
that n! and floor of [e(n-1)!] are tight upper bounds for these languages,
respectively. We also prove that 2^{n-1} is the tight upper bound on the state
complexity of reversal of J-trivial regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4651</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4651</id><created>2012-08-22</created><authors><author><keyname>Orhan</keyname><forenames>Oner</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Throughput Maximization for an Energy Harvesting Communication System
  with Processing Cost</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, To appear in ITW 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless networks, energy consumed for communication includes both the
transmission and the processing energy. In this paper, point-to-point
communication over a fading channel with an energy harvesting transmitter is
studied considering jointly the energy costs of transmission and processing.
Under the assumption of known energy arrival and fading profiles, optimal
transmission policy for throughput maximization is investigated. Assuming that
the transmitter has sufficient amount of data in its buffer at the beginning of
the transmission period, the average throughput by a given deadline is
maximized. Furthermore, a &quot;directional glue pouring algorithm&quot; that computes
the optimal transmission policy is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4656</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4656</id><created>2012-08-22</created><updated>2013-06-04</updated><authors><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Capacity of Compound MIMO Gaussian Channels with Additive Uncertainty</title><categories>cs.IT math.IT</categories><comments>8 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers reliable communications over a multiple-input
multiple-output (MIMO) Gaussian channel, where the channel matrix is within a
bounded channel uncertainty region around a nominal channel matrix, i.e., an
instance of the compound MIMO Gaussian channel. We study the optimal transmit
covariance matrix design to achieve the capacity of compound MIMO Gaussian
channels, where the channel uncertainty region is characterized by the spectral
norm. This design problem is a challenging non-convex optimization problem.
However, in this paper, we reveal that this problem has a hidden convexity
property, which can be exploited to map the problem into a convex optimization
problem. We first prove that the optimal transmit design is to diagonalize the
nominal channel, and then show that the duality gap between the capacity of the
compound MIMO Gaussian channel and the min-max channel capacity is zero, which
proves the conjecture of Loyka and Charalambous (IEEE Trans. Inf. Theory, vol.
58, no. 4, pp. 2048-2063, 2012). The key tools for showing these results are a
new matrix determinant inequality and some unitarily invariant properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4662</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4662</id><created>2012-08-22</created><updated>2013-05-07</updated><authors><author><keyname>Hu</keyname><forenames>Dandan</forenames></author><author><keyname>Sarder</keyname><forenames>Pinaki</forenames></author><author><keyname>Ronhovde</keyname><forenames>Peter</forenames></author><author><keyname>Orthaus</keyname><forenames>Sandra</forenames></author><author><keyname>Achilefu</keyname><forenames>Samuel</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>Automatic Segmentation of Fluorescence Lifetime Microscopy Images of
  Cells Using Multi-Resolution Community Detection</title><categories>physics.med-ph cond-mat.stat-mech cs.CV physics.data-an</categories><comments>21 pages, 6 figures</comments><journal-ref>Journal of Microscopy Volume 253, Issue 1, pages 54 - 64, 2014</journal-ref><doi>10.1111/jmi.12097</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed an automatic method for segmenting fluorescence lifetime
(FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolution
community detection (MCD) based network segmentation method. The image
processing problem is framed as identifying segments with respective average
FLTs against a background in FLIM images. The proposed method segments a FLIM
image for a given resolution of the network composed using image pixels as the
nodes and similarity between the pixels as the edges. In the resulting
segmentation, low network resolution leads to larger segments and high network
resolution leads to smaller segments. Further, the mean-square error (MSE) in
estimating the FLT segments in a FLIM image using the proposed method was found
to be consistently decreasing with increasing resolution of the corresponding
network. The proposed MCD method outperformed a popular spectral clustering
based method in performing FLIM image segmentation. The spectral segmentation
method introduced noisy segments in its output at high resolution. It was
unable to offer a consistent decrease in MSE with increasing resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4682</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4682</id><created>2012-08-23</created><updated>2013-02-10</updated><authors><author><keyname>Gupta</keyname><forenames>Avdhesh</forenames></author></authors><title>SOA Framework for Integrated Business</title><categories>cs.CY</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper SOA model for business is presented. This is a survey paper. As
the new technology evolve the new businesses use SOA framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4692</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4692</id><created>2012-08-23</created><updated>2012-12-18</updated><authors><author><keyname>Maes</keyname><forenames>Francis</forenames></author><author><keyname>St-Pierre</keyname><forenames>David Lupien</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author></authors><title>Monte Carlo Search Algorithm Discovery for One Player Games</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much current research in AI and games is being devoted to Monte Carlo search
(MCS) algorithms. While the quest for a single unified MCS algorithm that would
perform well on all problems is of major interest for AI, practitioners often
know in advance the problem they want to solve, and spend plenty of time
exploiting this knowledge to customize their MCS algorithm in a problem-driven
way. We propose an MCS algorithm discovery scheme to perform this in an
automatic and reproducible way. We first introduce a grammar over MCS
algorithms that enables inducing a rich space of candidate algorithms.
Afterwards, we search in this space for the algorithm that performs best on
average for a given distribution of training problems. We rely on multi-armed
bandits to approximately solve this optimization problem. The experiments,
generated on three different domains, show that our approach enables
discovering algorithms that outperform several well-known MCS algorithms such
as Upper Confidence bounds applied to Trees and Nested Monte Carlo search. We
also show that the discovered algorithms are generally quite robust with
respect to changes in the distribution over the training problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4696</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4696</id><created>2012-08-23</created><updated>2012-12-06</updated><authors><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Vehkapera</keyname><forenames>Mikko</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author></authors><title>Typical $l_1$-recovery limit of sparse vectors represented by
  concatenations of random orthogonal matrices</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>18 pages, 2 figures, 2 tables</comments><journal-ref>J. Stat. Mech. (2012) P12003</journal-ref><doi>10.1088/1742-5468/2012/12/P12003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering an $N$-dimensional sparse vector
$\vm{x}$ from its linear transformation $\vm{y}=\vm{D} \vm{x}$ of $M(&lt; N)$
dimension. Minimizing the $l_{1}$-norm of $\vm{x}$ under the constraint $\vm{y}
= \vm{D} \vm{x}$ is a standard approach for the recovery problem, and earlier
studies report that the critical condition for typically successful
$l_1$-recovery is universal over a variety of randomly constructed matrices
$\vm{D}$. For examining the extent of the universality, we focus on the case in
which $\vm{D}$ is provided by concatenating $\nb=N/M$ matrices $\vm{O}_{1},
\vm{O}_{2},..., \vm{O}_\nb$ drawn uniformly according to the Haar measure on
the $M \times M$ orthogonal matrices. By using the replica method in
conjunction with the development of an integral formula for handling the random
orthogonal matrices, we show that the concatenated matrices can result in
better recovery performance than what the universality predicts when the
density of non-zero signals is not uniform among the $\nb$ matrix modules. The
universal condition is reproduced for the special case of uniform non-zero
signal densities. Extensive numerical experiments support the theoretical
predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4721</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4721</id><created>2012-08-23</created><authors><author><keyname>Steeb</keyname><forenames>Willi-Hans</forenames></author><author><keyname>Hardy</keyname><forenames>Yorick</forenames></author></authors><title>Hamilton Operators, Discrete Symmetries, Brute Force and SymbolicC++</title><categories>cs.MS math-ph math.MP</categories><journal-ref>Int. J. Mod. Phys. C 24, 1250095 (2013)</journal-ref><doi>10.1142/S0129183112500957</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To find the discrete symmetries of a Hamilton operator $\hat H$ is of central
importance in quantum theory. Here we describe and implement a brute force
method to determine the discrete symmetries given by permutation matrices for
Hamilton operators acting in a finite-dimensional Hilbert space. Spin and Fermi
systems are considered as examples. A computer algebra implementation in
SymbolicC++ is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4722</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4722</id><created>2012-08-23</created><updated>2012-08-24</updated><authors><author><keyname>Morisset</keyname><forenames>Charles</forenames></author></authors><title>Implementing Access Control Markov Decision Processes with GLPK/GMPL</title><categories>cs.CR</categories><comments>15 pages, 6 figures, to be presented at the first International
  Workshop on Quantitative Aspects in Security Assurance, in colocation with
  ESORICS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent approach, we proposed to model an access control mechanism as a
Markov Decision Process, thus claiming that in order to make an access control
decision, one can use well-defined mechanisms from decision theory. We present
in this paper an implementation of such mechanism, using the open-source solver
GLPK, and we model the problem in the GMPL language. We illustrate our approach
with a simple, yet expressive example, and we show how the variation of some
parameters can change the final outcome. In particular, we show that in
addition to returning a decision, we can also calculate the value of each
decision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4743</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4743</id><created>2012-08-23</created><updated>2013-04-07</updated><authors><author><keyname>Chen</keyname><forenames>Niangjun</forenames></author><author><keyname>Tan</keyname><forenames>Chee Wei</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Optimal Charging of Electric Vehicles in Smart Grid: Characterization
  and Valley-Filling Algorithms</title><categories>cs.NI</categories><comments>This paper is temporarily withdrawn in preparation for journal
  submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric vehicles (EVs) offer an attractive long-term solution to reduce the
dependence on fossil fuel and greenhouse gas emission. However, a fleet of EVs
with different EV battery charging rate constraints, that is distributed across
a smart power grid network requires a coordinated charging schedule to minimize
the power generation and EV charging costs. In this paper, we study a joint
optimal power flow (OPF) and EV charging problem that augments the OPF problem
with charging EVs over time. While the OPF problem is generally nonconvex and
nonsmooth, it is shown recently that the OPF problem can be solved optimally
for most practical power networks using its convex dual problem. Building on
this zero duality gap result, we study a nested optimization approach to
decompose the joint OPF and EV charging problem. We characterize the optimal
offline EV charging schedule to be a valley-filling profile, which allows us to
develop an optimal offline algorithm with computational complexity that is
significantly lower than centralized interior point solvers. Furthermore, we
propose a decentralized online algorithm that dynamically tracks the
valley-filling profile. Our algorithms are evaluated on the IEEE 14 bus system,
and the simulations show that the online algorithm performs almost near
optimality ($&lt;1%$ relative difference from the offline optimal solution) under
different settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4766</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4766</id><created>2012-08-23</created><updated>2013-08-01</updated><authors><author><keyname>Teerapittayanon</keyname><forenames>S.</forenames></author><author><keyname>Fouli</keyname><forenames>K.</forenames></author><author><keyname>Medard</keyname><forenames>M.</forenames></author><author><keyname>Montpetit</keyname><forenames>M. -J.</forenames></author><author><keyname>Shi</keyname><forenames>X.</forenames></author><author><keyname>Seskar</keyname><forenames>I.</forenames></author><author><keyname>Gosain</keyname><forenames>A.</forenames></author></authors><title>Network Coding as a WiMAX Link Reliability Mechanism</title><categories>cs.NI</categories><comments>14 pages, 13 figures, extended version</comments><acm-class>H.4.3; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design and implement a network-coding-enabled reliability architecture for
next generation wireless networks. Our network coding (NC) architecture uses a
flexible thread-based design, with each encoder-decoder instance applying
systematic intra-session random linear network coding as a packet erasure code
at the IP layer, to ensure the fast and reliable transfer of information
between wireless nodes.
  Using Global Environment for Network Innovations (GENI) WiMAX platforms, a
series of point-to-point transmission experiments were conducted to compare the
performance of the NC architecture to that of the Automatic Repeated reQuest
(ARQ) and Hybrid ARQ (HARQ) mechanisms. At the application layer, Iperf and
UDP-based File Transfer Protocol (UFTP) are used to measure throughput, packet
loss and file transfer delay. In our selected scenarios, the proposed
architecture is able to decrease packet loss from around 11-32% to nearly 0%;
compared to HARQ and joint HARQ/ARQ mechanisms, the NC architecture offers up
to 5.9 times gain in throughput and 5.5 times reduction in end-to-end file
transfer delay. Our experiments show that network coding as a packet erasure
code in the upper layers of the protocol stack has the potential to reduce the
need for joint HARQ/ARQ schemes in the PHY/MAC layers, thus offering insights
into cross-layer designs of efficient next generation wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4773</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4773</id><created>2012-08-23</created><authors><author><keyname>Jung</keyname><forenames>Tobias</forenames></author><author><keyname>Wehenkel</keyname><forenames>Louis</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Maes</keyname><forenames>Francis</forenames></author></authors><title>Optimized Look-Ahead Tree Policies: A Bridge Between Look-Ahead Tree
  Policies and Direct Policy Search</title><categories>cs.SY cs.AI cs.LG</categories><comments>In Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct policy search (DPS) and look-ahead tree (LT) policies are two widely
used classes of techniques to produce high performance policies for sequential
decision-making problems. To make DPS approaches work well, one crucial issue
is to select an appropriate space of parameterized policies with respect to the
targeted problem. A fundamental issue in LT approaches is that, to take good
decisions, such policies must develop very large look-ahead trees which may
require excessive online computational resources. In this paper, we propose a
new hybrid policy learning scheme that lies at the intersection of DPS and LT,
in which the policy is an algorithm that develops a small look-ahead tree in a
directed way, guided by a node scoring function that is learned through DPS.
The LT-based representation is shown to be a versatile way of representing
policies in a DPS scheme, while at the same time, DPS enables to significantly
reduce the size of the look-ahead trees that are required to take high-quality
decisions.
  We experimentally compare our method with two other state-of-the-art DPS
techniques and four common LT policies on four benchmark domains and show that
it combines the advantages of the two techniques from which it originates. In
particular, we show that our method: (1) produces overall better performing
policies than both pure DPS and pure LT policies, (2) requires a substantially
smaller number of policy evaluations than other DPS techniques, (3) is easy to
tune and (4) results in policies that are quite robust with respect to
perturbations of the initial conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4777</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4777</id><created>2012-08-23</created><authors><author><keyname>Pillai</keyname><forenames>Sibi Raj B.</forenames></author><author><keyname>Dey</keyname><forenames>Bikash K.</forenames></author><author><keyname>Deshpande</keyname><forenames>Yash</forenames></author><author><keyname>Iyer</keyname><forenames>Krishnamoorthy</forenames></author></authors><title>Power Controlled Adaptive Sum-Capacity of Fading MACs with Distributed
  CSI</title><categories>cs.IT math.IT</categories><comments>15 pages, 5 figures, combined and extended version of ITW 2011 and
  ISITA 2012 papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding optimal, fair and distributed power-rate
strategies to achieve the sum capacity of the Gaussian multiple-access
block-fading channel. In here, the transmitters have access to only their own
fading coefficients, while the receiver has global access to all the fading
coefficients. Outage is not permitted in any communication block. The resulting
average sum-throughput is also known as `power-controlled adaptive
sum-capacity', which appears as an open problem in literature.
  This paper presents the power-controlled adaptive sum-capacity of a
wide-class of popular MAC models. In particular, we propose a power-rate
strategy in the presence of distributed channel state information (CSI), which
is throughput optimal when all the users have identical channel statistics. The
proposed scheme also has an efficient implementation using successive
cancellation and rate-splitting. We propose an upperbound when the channel laws
are not identical. Furthermore, the optimal schemes are extended to situations
in which each transmitter has additional finite-rate partial CSI on the link
quality of others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4790</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4790</id><created>2012-08-23</created><updated>2013-01-23</updated><authors><author><keyname>Yoo</keyname><forenames>Jae Won</forenames><affiliation>Shitz</affiliation></author><author><keyname>Liu</keyname><forenames>Tie</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>Worst-Case Expected-Capacity Loss of Slow-Fading Channels</title><categories>cs.IT math.IT</categories><comments>Simplified some notations. Added Section 2.2 and several references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For delay-limited communication over block-fading channels, the difference
between the ergodic capacity and the maximum achievable expected rate for
coding over a finite number of coherent blocks represents a fundamental measure
of the penalty incurred by the delay constraint. This paper introduces a notion
of worst-case expected-capacity loss. Focusing on the slow-fading scenario
(one-block delay), the worst-case additive and multiplicative expected-capacity
losses are precisely characterized for the point-to-point fading channel.
Extension to the problem of writing on fading paper is also considered, where
both the ergodic capacity and the additive expected-capacity loss over
one-block delay are characterized to within one bit per channel use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4803</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4803</id><created>2012-08-23</created><authors><author><keyname>Hella</keyname><forenames>Lauri</forenames></author><author><keyname>V&#xe4;&#xe4;n&#xe4;nen</keyname><forenames>Jouko</forenames></author></authors><title>The size of a formula as a measure of complexity</title><categories>math.LO cs.LO</categories><comments>25 pages</comments><msc-class>03C07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a refinement of the usual Ehrenfeucht-Fra\&quot;{\i}ss\'e game. The
new game will help us make finer distinctions than the traditional one. In
particular, it can be used to measure the size formulas needed for expressing a
given property. We will give two versions of the game: the first version
characterizes the size of formulas in propositional logic, and the second
version works for first-order predicate logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4809</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4809</id><created>2012-08-23</created><authors><author><keyname>Reddy</keyname><forenames>H. Venkateswara</forenames></author><author><keyname>Raju</keyname><forenames>Dr. S. Viswanadha</forenames></author><author><keyname>Reddy</keyname><forenames>B. Ramasubba</forenames></author></authors><title>Comparing N-Node Set Importance Representative results with Node
  Importance Representative results for Categorical Clustering: An exploratory
  study</title><categories>cs.DB</categories><comments>16 pages, 4 figures, 3 equations</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The proportionate increase in the size of the data with increase in space
implies that clustering a very large data set becomes difficult and is a time
consuming process.Sampling is one important technique to scale down the size of
dataset and to improve the efficiency of clustering. After sampling allocating
unlabeled objects into proper clusters is impossible in the categorical
domain.To address the problem, Chen employed a method called MAximal
Representative Data Labeling to allocate each unlabeled data point to the
appropriate cluster based on Node Importance Representative and N-Node
Importance Representative algorithms. This paper took off from Chen s
investigation and analyzed and compared the results of NIR and NNIR leading to
the conclusion that the two processes contradict each other when it comes to
finding the resemblance between an unlabeled data point and a cluster.A new and
better way of solving the problem was arrived at that finds resemblance between
unlabeled data point within all clusters, while also providing maximal
resemblance for allocation of data in the required cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4842</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4842</id><created>2012-08-23</created><authors><author><keyname>Al-Wassai</keyname><forenames>Firouz Abdullah</forenames></author><author><keyname>Kalyankar</keyname><forenames>N. V.</forenames></author></authors><title>The Segmentation Fusion Method On10 Multi-Sensors</title><categories>cs.CV</categories><comments>http://www.ijltemas.in/new-icae-2012-?df=1&amp;t=1345561079771</comments><journal-ref>International Journal of Latest Technology in
  Engineering,Management &amp; Applied Science (IJLTEMAS),Vol. I, Issue V, 2012,
  124-138</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most significant problem may be undesirable effects for the spectral
signatures of fused images as well as the benefits of using fused images mostly
compared to their source images were acquired at the same time by one sensor.
They may or may not be suitable for the fusion of other images. It becomes
therefore increasingly important to investigate techniques that allow
multi-sensor, multi-date image fusion to make final conclusions can be drawn on
the most suitable method of fusion. So, In this study we present a new method
Segmentation Fusion method (SF) for remotely sensed images is presented by
considering the physical characteristics of sensors, which uses a feature level
processing paradigm. In a particularly, attempts to test the proposed method
performance on 10 multi-sensor images and comparing it with different fusion
techniques for estimating the quality and degree of information improvement
quantitatively by using various spatial and spectral metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4867</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4867</id><created>2012-08-23</created><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author></authors><title>Parameter-independent Iterative Approximate Byzantine Consensus</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we explore iterative approximate Byzantine consensus algorithms
that do not make explicit use of the global parameter of the graph, i.e., the
upper-bound on the number of faults, f.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4869</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4869</id><created>2012-08-23</created><authors><author><keyname>Flatau</keyname><forenames>Piotr J.</forenames></author></authors><title>User Manual for the Complex Conjugate Gradient Methods Library CCGPAK
  2.0</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manual describes the library of conjugate gradients codes CCGPAK, which
solves system of complex linear system of equations. The library is written in
FORTRAN90 and is highly portable. The codes are general and provide mechanism
for matrix times vector multiplication which is separated from the conjugate
gradient iterations itself. It is simple to switch between single and double
precisions. All codes follow the same naming conventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4877</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4877</id><created>2012-08-23</created><authors><author><keyname>Jahid</keyname><forenames>Sonia</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>PIRATTE: Proxy-based Immediate Revocation of ATTribute-based Encryption</title><categories>cs.CR cs.SI</categories><comments>14 pages, Under review in TDSC</comments><acm-class>E.3; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access control to data in traditional enterprises is typically enforced
through reference monitors. However, as more and more enterprise data is
outsourced, trusting third party storage servers is getting challenging. As a
result, cryptography, specifically Attribute-based encryption (ABE) is getting
popular for its expressiveness. The challenge of ABE is revocation.
  To address this challenge, we propose PIRATTE, an architecture that supports
fine-grained access control policies and dynamic group membership. PIRATTE is
built using attribute-based encryption; a key and novel feature of our
architecture, however, is that it is possible to remove access from a user
without issuing new keys to other users or re-encrypting existing ciphertexts.
We achieve this by introducing a proxy that participates in the decryption
process and enforces revocation constraints. The proxy is minimally trusted and
cannot decrypt ciphertexts or provide access to previously revoked users. We
describe the PIRATTE construction and provide a security analysis along with
performance evaluation.We also describe an architecture for online social
network that can use PIRATTE, and prototype application of PIRATTE on Facebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4895</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4895</id><created>2012-08-23</created><updated>2012-08-31</updated><authors><author><keyname>Shaochuan</keyname><forenames>Wu</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>Broadcast Gossip Algorithms for Consensus on Strongly Connected Digraphs</title><categories>cs.SY cs.DC</categories><comments>30 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general framework for broadcast gossip algorithms which use
companion variables to solve the average consensus problem. Each node maintains
an initial state and a companion variable. Iterative updates are performed
asynchronously whereby one random node broadcasts its current state and
companion variable and all other nodes receiving the broadcast update their
state and companion variable. We provide conditions under which this scheme is
guaranteed to converge to a consensus solution, where all nodes have the same
limiting values, on any strongly connected directed graph. Under stronger
conditions, which are reasonable when the underlying communication graph is
undirected, we guarantee that the consensus value is equal to the average, both
in expectation and in the mean-squared sense. Our analysis uses tools from
non-negative matrix theory and perturbation theory. The perturbation results
rely on a parameter being sufficiently small. We characterize the allowable
upper bound as well as the optimal setting for the perturbation parameter as a
function of the network topology, and this allows us to characterize the
worst-case rate of convergence. Simulations illustrate that, in comparison to
existing broadcast gossip algorithms, the approaches proposed in this paper
have the advantage that they simultaneously can be guaranteed to converge to
the average consensus and they converge in a small number of broadcasts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4899</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4899</id><created>2012-08-24</created><updated>2013-01-24</updated><authors><author><keyname>Basnayaka</keyname><forenames>Dushyantha A.</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Martin</keyname><forenames>Philippa A.</forenames></author></authors><title>The Effect of Macrodiversity on the Performance of Maximal Ratio
  Combining in Flat Rayleigh Fading</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures; IEEE Transaction of Communication, 2012
  Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of maximal ratio combining (MRC) in Rayleigh channels with
co-channel interference (CCI) is well-known for receive arrays which are
co-located. Recent work in network MIMO, edge-excited cells and base station
collaboration is increasing interest in macrodiversity systems. Hence, in this
paper we consider the effect of macrodiversity on MRC performance in Rayleigh
fading channels with CCI. We consider the uncoded symbol error rate (SER) as
our performance measure of interest and investigate how different
macrodiversity power profiles affect SER performance. This is the first
analytical work in this area. We derive approximate and exact symbol error rate
results for M-QAM/BPSK modulations and use the analysis to provide a simple
power metric. Numerical results, verified by simulations, are used in
conjunction with the analysis to gain insight into the effects of the link
powers on performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4901</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4901</id><created>2012-08-24</created><updated>2012-09-20</updated><authors><author><keyname>Basnayaka</keyname><forenames>Dushyantha A.</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Martin</keyname><forenames>Philippa A.</forenames></author></authors><title>Performance Analysis of Dual-User Macrodiversity MIMO Systems with
  Linear Receivers in Flat Rayleigh Fading</title><categories>cs.IT math.IT</categories><comments>24 pages, 7 figures; IEEE Transaction of Wireless Communication 2012
  Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of linear receivers in the presence of co-channel
interference in Rayleigh channels is a fundamental problem in wireless
communications. Performance evaluation for these systems is well-known for
receive arrays where the antennas are close enough to experience equal average
SNRs from a source. In contrast, almost no analytical results are available for
macrodiversity systems where both the sources and receive antennas are widely
separated. Here, receive antennas experience unequal average SNRs from a source
and a single receive antenna receives a different average SNR from each source.
Although this is an extremely difficult problem, progress is possible for the
two-user scenario. In this paper, we derive closed form results for the
probability density function (pdf) and cumulative distribution function (cdf)
of the output signal to interference plus noise ratio (SINR) and signal to
noise ratio (SNR) of minimum mean squared error (MMSE) and zero forcing (ZF)
receivers in independent Rayleigh channels with arbitrary numbers of receive
antennas. The results are verified by Monte Carlo simulations and high SNR
approximations are also derived. The results enable further system analysis
such as the evaluation of outage probability, bit error rate (BER) and
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4909</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4909</id><created>2012-08-24</created><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Garay</keyname><forenames>Juan</forenames></author><author><keyname>Gilboa</keyname><forenames>Niv</forenames></author><author><keyname>Kolesnikov</keyname><forenames>Vladimir</forenames></author><author><keyname>Yuditsky</keyname><forenames>Yelena</forenames></author></authors><title>Efficient Private Distributed Computation on Unbounded Input Streams</title><categories>cs.DC cs.CR</categories><comments>18 pages, 2 figures. A brief announcement will be presented in DISC
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the problem of swarm computing, $n$ agents wish to securely and
distributively perform a computation on common inputs, in such a way that even
if the entire memory contents of some of them are exposed, no information is
revealed about the state of the computation. Recently, Dolev, Garay, Gilboa and
Kolesnikov [ICS 2011] considered this problem in the setting of
information-theoretic security, showing how to perform such computations on
input streams of unbounded length. The cost of their solution, however, is
exponential in the size of the Finite State Automaton (FSA) computing the
function.
  In this work we are interested in efficient computation in the above model,
at the expense of minimal additional assumptions. Relying on the existence of
one-way functions, we show how to process a priori unbounded inputs (but of
course, polynomial in the security parameter) at a cost linear in $m$, the
number of FSA states. In particular, our algorithms achieve the following:
  * In the case of $(n,n)$-reconstruction (i.e. in which all $n$ agents
participate in reconstruction of the distributed computation) and at most $n-1$
agents are corrupted, the agent storage, the time required to process each
input symbol and the time complexity for reconstruction are all $O(mn)$.
  * In the case of $(t+1,n)$-reconstruction (where only $t+1$ agents take part
in the reconstruction) and at most $t$ agents are corrupted, the agents'
storage and time required to process each input symbol are $O(m{n-1 \choose
t-1})$. The complexity of reconstruction is $O(m(t+1))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4930</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4930</id><created>2012-08-24</created><authors><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author><author><keyname>Meyers</keyname><forenames>Jeremy</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>Undecidable First-Order Theories of Affine Geometries</title><categories>math.LO cs.CC cs.LO</categories><comments>21 pages, 3 figures</comments><acm-class>F.4.1; F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tarski initiated a logic-based approach to formal geometry that studies
first-order structures with a ternary betweenness relation (\beta) and a
quaternary equidistance relation (\equiv). Tarski established, inter alia, that
the first-order (FO) theory of (R^2,\beta,\equiv) is decidable. Aiello and van
Benthem (2002) conjectured that the FO-theory of expansions of (R^2,\beta) with
unary predicates is decidable. We refute this conjecture by showing that for
all n&gt;1, the FO-theory of monadic expansions of (R^2,\beta) is \Pi^1_1-hard and
therefore not even arithmetical. We also define a natural and comprehensive
class C of geometric structures (T,\beta), where T is a subset of R^2, and show
that for each structure (T,\beta) in C, the FO-theory of the class of monadic
expansions of (T,\beta) is undecidable. We then consider classes of expansions
of structures (T,\beta) with restricted unary predicates, for example finite
predicates, and establish a variety of related undecidability results. In
addition to decidability questions, we briefly study the expressivity of
universal MSO and weak universal MSO over expansions of (R^n,\beta). While the
logics are incomparable in general, over expansions of (R^n,\beta), formulae of
weak universal MSO translate into equivalent formulae of universal MSO.
  This is an extended version of a publication in the proceedings of the 21st
EACSL Annual Conferences on Computer Science Logic (CSL 2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4942</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4942</id><created>2012-08-24</created><updated>2014-02-13</updated><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author></authors><title>A Unifying Survey of Reinforced, Sensitive and Stigmergic Agent-Based
  Approaches for E-GTSP</title><categories>cs.AI</categories><comments>9 pages, 2 figures</comments><msc-class>90C27, 68R05, 91B69</msc-class><acm-class>I.2.8; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard
combinatorial optimization problems. A variant of GTSP is E-GTSP where E,
meaning equality, has the constraint: exactly one node from a cluster of a
graph partition is visited. The main objective of the E-GTSP is to find a
minimum cost tour passing through exactly one node from each cluster of an
undirected graph. Agent-based approaches involving are successfully used
nowadays for solving real life complex problems. The aim of the current paper
is to illustrate some variants of agent-based algorithms including ant-based
models with specific properties for solving E-GTSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4945</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4945</id><created>2012-08-24</created><updated>2012-10-10</updated><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Crisan</keyname><forenames>Gloria Cerasela</forenames></author><author><keyname>Manea</keyname><forenames>Mihai</forenames></author></authors><title>Parallel ACO with a Ring Neighborhood for Dynamic TSP</title><categories>cs.AI</categories><comments>8 pages, 1 figure; accepted J. Information Technology Research</comments><msc-class>90C27, 68T10, 68T20</msc-class><acm-class>I.2.11; G.2.1; G.2.2</acm-class><journal-ref>J Information Technology Research 5(4) (2012) 1-13</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current paper introduces a new parallel computing technique based on ant
colony optimization for a dynamic routing problem. In the dynamic traveling
salesman problem the distances between cities as travel times are no longer
fixed. The new technique uses a parallel model for a problem variant that
allows a slight movement of nodes within their Neighborhoods. The algorithm is
tested with success on several large data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4987</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4987</id><created>2012-08-24</created><updated>2014-06-03</updated><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>McQuillan</keyname><forenames>Colin</forenames></author></authors><title>Approximating the partition function of planar two-state spin systems</title><categories>cs.CC</categories><msc-class>68Q17, 60C05, 68W25, 82B20</msc-class><doi>10.1016/j.jcss.2014.06.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating the partition function of the
hard-core model on planar graphs of degree at most 4. We show that when the
activity lambda is sufficiently large, there is no fully polynomial randomised
approximation scheme for evaluating the partition function unless NP=RP. The
result extends to a nearby region of the parameter space in a more general
two-state spin system with three parameters. We also give a polynomial-time
randomised approximation scheme for the logarithm of the partition function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.4993</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.4993</id><created>2012-08-24</created><updated>2013-02-15</updated><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Expressive Completeness of Metric Temporal Logic</title><categories>cs.LO</categories><comments>Submitted to LICS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric Temporal Logic (MTL) is a generalisation of Linear Temporal Logic in
which the Until and Since modalities are annotated with intervals that express
metric constraints. A seminal result of Hirshfeld and Rabinovich shows that
over the reals, first-order logic with binary order relation &lt; and unary
function +1 is strictly more expressive than MTL with integer constants. Indeed
they prove that no temporal logic whose modalities are definable by formulas of
bounded quantifier depth can be expressively complete for FO(&lt;,+1). In this
paper we show the surprising result that if we allow unary functions +q, (q
rational), in first-order logic and correspondingly allow rational constants in
MTL, then the two logics have the same expressive power. This gives the first
generalisation of Kamp's theorem on the expressive completeness of LTL for
FO(&lt;) to the quantitative setting. The proof of this result involves a
generalisation of Gabbay's notion of separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5002</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5002</id><created>2012-08-24</created><authors><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>A Note on Limited Pushdown Alphabets in Stateless Deterministic Pushdown
  Automata</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, an infinite hierarchy of languages accepted by stateless
deterministic pushdown automata has been established based on the number of
pushdown symbols. However, the witness language for the n-th level of the
hierarchy is over an input alphabet with 2(n-1) elements. In this paper, we
improve this result by showing that a binary alphabet is sufficient to
establish this hierarchy. As a consequence of our construction, we solve the
open problem formulated by Meduna et al. Then we extend these results to
m-state realtime deterministic pushdown automata, for all m at least 1. The
existence of such a hierarchy for m-state deterministic pushdown automata is
left open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5003</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5003</id><created>2012-08-24</created><updated>2014-07-15</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author><author><keyname>Chater</keyname><forenames>Nick</forenames><affiliation>Behavioural Science Group, Warwick Business School, University of Warwick</affiliation></author></authors><title>Identification of Probabilities of Languages</title><categories>cs.LG math.PR</categories><comments>23 pages LaTeX, no pictures 1311.7385 This paper has been withdrawn
  by the auther due to crucial errors. The same subject is attacked more
  succesfully with reduced claims in ArXiV 1311.7385</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of inferring the probability distribution associated
with a language, given data consisting of an infinite sequence of elements of
the languge. We do this under two assumptions on the algorithms concerned: (i)
like a real-life algorothm it has round-off errors, and (ii) it has no
round-off errors. Assuming (i) we (a) consider a probability mass function of
the elements of the language if the data are drawn independent identically
distributed (i.i.d.), provided the probability mass function is computable and
has a finite expectation. We give an effective procedure to almost surely
identify in the limit the target probability mass function using the Strong Law
of Large Numbers. Second (b) we treat the case of possibly incomputable
probabilistic mass functions in the above setting. In this case we can only
pointswize converge to the target probability mass function almost surely.
Third (c) we consider the case where the data are dependent assuming they are
typical for at least one computable measure and the language is finite. There
is an effective procedure to identify by infinite recurrence a nonempty subset
of the computable measures according to which the data is typical. Here we use
the theory of Kolmogorov complexity. Assuming (ii) we obtain the weaker result
for (a) that the target distribution is identified by infinite recurrence
almost surely; (b) stays the same as under assumption (i). We consider the
associated predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5012</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5012</id><created>2012-08-24</created><updated>2012-08-27</updated><authors><author><keyname>Marzouki</keyname><forenames>Abdelwaheb</forenames></author><author><keyname>Jin</keyname><forenames>Xin</forenames></author></authors><title>Precoder Design for Orthogonal Space-Time Block Coding based Cognitive
  Radio with Polarized Antennas</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The spectrum sharing has recently passed into a mainstream Cognitive Radio
(CR) strategy. We investigate the core issue in this strategy: interference
mitigation at Primary Receiver (PR).We propose a linear precoder design which
aims at alleviating the interference caused by Secondary User (SU) from the
source for Orthogonal Space-Time Block Coding (OSTBC) based CR. We resort to
Minimum Variance (MV) approach to contrive the precoding matrix at Secondary
Transmitter (ST) in order to maximize the Signal to Noise Ratio (SNR) at
Secondary Receiver (SR) on the premise that the orthogonality of OSTBC is kept,
the interference introduced to Primary Link (PL) by Secondary Link (SL) is
maintained under a tolerable level and the total transmitted power constraint
at ST is satisfied. Moreover, the selection of polarization mode for SL is
incorporated in the precoder design. In order to provide an analytic solution
with low computational cost, we put forward an original precoder design
algorithm which exploits an auxiliary variable to treat the optimization
problem with a mixture of linear and quadratic constraints. Numerical results
demonstrate that our proposed precoder design enable SR to have an agreeable
SNR on the prerequisite that the interference at PR is maintained below the
threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5016</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5016</id><created>2012-08-24</created><authors><author><keyname>Konukoglu</keyname><forenames>Ender</forenames></author><author><keyname>Glocker</keyname><forenames>Ben</forenames></author><author><keyname>Criminisi</keyname><forenames>Antonio</forenames></author><author><keyname>Pohl</keyname><forenames>Kilian M.</forenames></author></authors><title>WESD - Weighted Spectral Distance for Measuring Shape Dissimilarity</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a new distance for measuring shape dissimilarity
between objects. Recent publications introduced the use of eigenvalues of the
Laplace operator as compact shape descriptors. Here, we revisit the eigenvalues
to define a proper distance, called Weighted Spectral Distance (WESD), for
quantifying shape dissimilarity. The definition of WESD is derived through
analysing the heat-trace. This analysis provides the proposed distance an
intuitive meaning and mathematically links it to the intrinsic geometry of
objects. We analyse the resulting distance definition, present and prove its
important theoretical properties. Some of these properties include: i) WESD is
defined over the entire sequence of eigenvalues yet it is guaranteed to
converge, ii) it is a pseudometric, iii) it is accurately approximated with a
finite number of eigenvalues, and iv) it can be mapped to the [0,1) interval.
Lastly, experiments conducted on synthetic and real objects are presented.
These experiments highlight the practical benefits of WESD for applications in
vision and medical image analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5018</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5018</id><created>2012-08-24</created><updated>2014-03-25</updated><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Fan</keyname><forenames>Fengtao</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Computing Topological Persistence for Simplicial Maps</title><categories>cs.CG math.AT</categories><comments>This is the revised and full version of the paper that is going to
  appear in the Proceedings of 30th Annual Symposium on Computational Geometry</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for persistent homology and zigzag persistent homology are
well-studied for persistence modules where homomorphisms are induced by
inclusion maps. In this paper, we propose a practical algorithm for computing
persistence under $\mathbb{Z}_2$ coefficients for a sequence of general
simplicial maps and show how these maps arise naturally in some applications of
topological data analysis.
  First, we observe that it is not hard to simulate simplicial maps by
inclusion maps but not necessarily in a monotone direction. This, combined with
the known algorithms for zigzag persistence, provides an algorithm for
computing the persistence induced by simplicial maps.
  Our main result is that the above simple minded approach can be improved for
a sequence of simplicial maps given in a monotone direction. A simplicial map
can be decomposed into a set of elementary inclusions and vertex collapses--two
atomic operations that can be supported efficiently with the notion of simplex
annotations for computing persistent homology. A consistent annotation through
these atomic operations implies the maintenance of a consistent cohomology
basis, hence a homology basis by duality. While the idea of maintaining a
cohomology basis through an inclusion is not new, maintaining them through a
vertex collapse is new, which constitutes an important atomic operation for
simulating simplicial maps. Annotations support the vertex collapse in addition
to the usual inclusion quite naturally.
  Finally, we exhibit an application of this new tool in which we approximate
the persistence diagram of a filtration of Rips complexes where vertex
collapses are used to tame the blow-up in size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5024</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5024</id><created>2012-08-24</created><updated>2013-08-26</updated><authors><author><keyname>Do</keyname><forenames>An H.</forenames></author><author><keyname>Wang</keyname><forenames>Po T.</forenames></author><author><keyname>King</keyname><forenames>Christine E.</forenames></author><author><keyname>Chun</keyname><forenames>Sophia N.</forenames></author><author><keyname>Nenadic</keyname><forenames>Zoran</forenames></author></authors><title>Brain-Computer Interface Controlled Robotic Gait Orthosis</title><categories>cs.HC cs.RO</categories><comments>Supplementary video (http://www.youtube.com/watch?v=W97Z8fEAQ7g and
  http://www.youtube.com/watch?v=HXNCwonhjG8)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliance on wheelchairs after spinal cord injury (SCI) leads to many medical
co-morbidities. Treatment of these conditions contributes to the majority of
SCI health care costs. Restoring able-body-like ambulation after SCI may reduce
the incidence of these conditions, and increase independence and quality of
life. However, no biomedical solution exists that can reverse this lost
neurological function, and hence novel methods are needed. Brain-computer
interface (BCI) controlled lower extremity prosthesis may constitute one such
novel approach.
  One subject with able-body and one with paraplegia due to SCI underwent
electroencephalogram (EEG) recording while engaged in alternating epochs of
idling and walking kinesthetic motor imagery (KMI). These data were analyzed to
generate an EEG prediction model for online BCI operation. A commercial robotic
gait orthosis (RoGO) system (treadmill suspended), was interfaced with the BCI
computer. In an online test, the subjects were tasked to ambulate using the
BCI-RoGO system when prompted by computerized cues. The performance of this
system was assessed with cross-correlation analysis, and omission and false
alarm rates.
  The offline accuracy of the EEG prediction model averaged 86.3%. The
cross-correlation between instructional cues and BCI-RoGO walking epochs
averaged 0.812 +/- 0.048 (p-value&lt;10^-4). There were on average 0.8 false
alarms per session and no omissions.
  This is the first time a person with parapegia due to SCI regained basic
brain-controlled ambulation, thereby indicating that restoring brain-controlled
ambulation is feasible. Future work will test this system in a population of
individuals with SCI. If successful, this may justify future development of
invasive BCI-controlled lower extremity prostheses. This system may also be
applied to incomplete SCI to improve neurological outcomes beyond those of
standard physiotherapy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5037</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5037</id><created>2012-08-24</created><authors><author><keyname>Bekravi</keyname><forenames>Masoud</forenames></author><author><keyname>Jamali</keyname><forenames>Shahram</forenames></author><author><keyname>Shaker</keyname><forenames>Gholam</forenames></author></authors><title>Defense against SYN-Flood Denial of Service Attacks Based on Learning
  Automata</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  SYN-flooding attack uses the weakness available in TCP's three-way handshake
process to keep it from handling legitimate requests. This attack causes the
victim host to populate its backlog queue with forged TCP connections. In other
words it increases Ploss (probability of loss) and Pa (buffer occupancy
percentage of attack requests) and decreases Pr (buffer occupancy percentage of
regular requests) in the victim host and results to decreased performance of
the host. This paper proposes a self-managing approach, in which the host
defends against SYN-flooding attack by dynamically tuning of its own two
parameters, that is, m (maximum number of half-open connections) and h (hold
time for each half-open connection). In this way, it formulates the defense
problem to an optimization problem and then employs the learning automata (LA)
algorithm to solve it. The simulation results show that the proposed defense
strategy improves performance of the under attack system in terms of Ploss, Pa
and Pr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5045</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5045</id><created>2012-08-17</created><authors><author><keyname>Reem</keyname><forenames>Daniel</forenames></author></authors><title>On the existence of a neutral region</title><categories>cs.CG math.FA</categories><comments>22 pages, 9 figures</comments><msc-class>52C99, 68U05, 47H10</msc-class><acm-class>F.2.2; G.0; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a given space, e.g., the Euclidean plane, and its decomposition into
Voronoi regions induced by given sites. It seems intuitively clear that each
point in the space belongs to at least one of the regions, i.e., no neutral
region can exist. As simple counterexamples show this is not true in general,
but we present a simple necessary and sufficient condition ensuring the
non-existence of a neutral region. We discuss a similar phenomenon concerning
recent variations of Voronoi diagrams called zone diagrams, double zone
diagrams, and (double) territory diagrams. These objects are defined in a
somewhat implicit way and they also induce a decomposition of the space into
regions. In several works it was claimed without providing a proof that some of
these objects induce a decomposition in which a neutral region must exist. We
show that this assertion is true in a wide class of cases but not in general.
We also discuss other properties related to the neutral region, among them a
one related to the concentration of measure phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5052</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5052</id><created>2012-08-24</created><updated>2014-11-18</updated><authors><author><keyname>Ronhovde</keyname><forenames>Peter</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>Local multiresolution order in community detection</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>19 pages, 11 figures</comments><doi>10.1088/1742-5468/2015/01/P01001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection algorithms attempt to find the best clusters of nodes in
an arbitrary complex network. Multi-scale (&quot;multiresolution&quot;) community
detection extends the problem to identify the best network scale(s) for these
clusters. The latter task is generally accomplished by analyzing community
stability simultaneously for all clusters in the network. In the current work,
we extend this general approach to define local multiresolution methods, which
enable the extraction of well-defined local communities even if the global
community structure is vaguely defined in an average sense. Toward this end, we
propose measures analogous to variation of information and normalized mutual
information that are used to quantitatively identify the best resolution(s) at
the community level based on correlations between clusters in
independently-solved systems. We demonstrate our method on two constructed
networks as well as a real network and draw inferences about local community
strength. Our approach is independent of the applied community detection
algorithm save for the inherent requirement that the method be able to identify
communities across different network scales, with appropriate changes to
account for how different resolutions are evaluated or defined in a particular
community detection method. It should, in principle, easily adapt to
alternative community comparison measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5062</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5062</id><created>2012-08-24</created><updated>2012-12-07</updated><authors><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Huang</keyname><forenames>Jiaji</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Changepoint detection for high-dimensional time series with missing data</title><categories>stat.ML cs.LG</categories><doi>10.1109/JSTSP.2012.2234082</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel approach to change-point detection when the
observed high-dimensional data may have missing elements. The performance of
classical methods for change-point detection typically scales poorly with the
dimensionality of the data, so that a large number of observations are
collected after the true change-point before it can be reliably detected.
Furthermore, missing components in the observed data handicap conventional
approaches. The proposed method addresses these challenges by modeling the
dynamic distribution underlying the data as lying close to a time-varying
low-dimensional submanifold embedded within the ambient observation space.
Specifically, streaming data is used to track a submanifold approximation,
measure deviations from this approximation, and calculate a series of
statistics of the deviations for detecting when the underlying manifold has
changed in a sharp or unexpected manner. The approach described in this paper
leverages several recent results in the field of high-dimensional data
analysis, including subspace tracking with missing data, multiscale analysis
techniques for point clouds, online optimization, and change-point detection
performance analysis. Simulations and experiments highlight the robustness and
efficacy of the proposed approach in detecting an abrupt change in an otherwise
slowly varying low-dimensional manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5071</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5071</id><created>2012-08-24</created><authors><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On the Synergistic Benefits of Alternating CSIT for the MISO BC</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) of the two-user multiple-input single-output
(MISO) broadcast channel (BC) are studied under the assumption that the form,
I_i, i=1,2, of the channel state information at the transmitter (CSIT) for each
user's channel can be either perfect (P), delayed (D) or not available (N),
i.e., I_1 and I_2 can take values of either P, D or N, and therefore the
overall CSIT can alternate between the 9 resulting states, each state denoted
as I_1I_2. The fraction of time associated with CSIT state I_1I_2 is denoted by
the parameter \lambda_{I_1I_2} and it is assumed throughout that
\lambda_{I_1I_2}=\lambda_{I_2I_1}, i.e., \lambda_{PN}=\lambda_{NP},
\lambda_{PD}=\lambda_{DP}, \lambda_{DN}=\lambda_{ND}. Under this assumption of
symmetry, the main contribution of this paper is a complete characterization of
the DoF region of the two user MISO BC with alternating CSIT. Surprisingly, the
DoF region is found to depend only on the marginal probabilities (\lambda_P,
\lambda_D,\lambda_N)=(\sum_{I_2}\lambda_{PI_2},\sum_{I_2}\lambda_{DI_2},
\sum_{I_2}\lambda_{NI_2}), I_2\in {P,D,N}, which represent the fraction of time
that any given user (e.g., user 1) is associated with perfect, delayed, or no
CSIT, respectively. As a consequence, the DoF region with all 9 CSIT states,
\mathcal{D}(\lambda_{I_1I_2}:I_1,I_2\in{P,D,N}), is the same as the DoF region
with only 3 CSIT states \mathcal{D}(\lambda_{PP}, \lambda_{DD}, \lambda_{NN}),
under the same marginal distribution of CSIT states, i.e., (\lambda_{PP},
\lambda_{DD},\lambda_{NN})=(\lambda_P,\lambda_D,\lambda_N). The results
highlight the synergistic benefits of alternating CSIT and the tradeoffs
between various forms of CSIT for any given DoF value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5073</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5073</id><created>2012-08-24</created><updated>2013-08-27</updated><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author></authors><title>Incidence Theorems and Their Applications</title><categories>math.CO cs.CC</categories><comments>Survey. 104p</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey recent (and not so recent) results concerning arrangements of
lines, points and other geometric objects and the applications these results
have in theoretical computer science and combinatorics. The three main types of
problems we will discuss are:
  (1) Counting incidences: Given a set (or several sets) of geometric objects
(lines, points, etc..), what is the maximum number of incidences (or
intersections) that can exist between elements in different sets? We will see
several results of this type, such as the Szemeredi-Trotter theorem, over the
reals and over finite fields and discuss their applications in combinatorics
(e.g., in the recent solution of Guth and Katz to Erdos' distance problem) and
in computer science (in explicit constructions of multi-source extractors).
  (2) Kakeya type problems: These problems deal with arrangements of lines that
point in different directions. The goal is to try and understand to what extent
these lines can overlap one another. We will discuss these questions both over
the reals and over finite fields and see how they come up in the theory of
randomness-extractors.
  (3) Sylvester-Gallai type problems: In this type of problems, one is
presented with a configuration of points that contain many `local' dependencies
(e.g., three points on a line) and is asked to derive a bound on the dimension
of the span of all points. We will discuss several recent results of this type,
over various fields, and see their connection to the theory of locally
correctable error-correcting codes.
  Throughout the different parts of the survey, two types of techniques will
make frequent appearance. One is the polynomial method, which uses polynomial
interpolation to impose an algebraic structure on the problem at hand. The
other recurrent techniques will come from the area of additive combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5075</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5075</id><created>2012-08-24</created><updated>2014-02-19</updated><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Exact Byzantine Consensus in Directed Graphs</title><categories>cs.DC</categories><comments>Revised on February 18, 2014 to make major improvements to the
  presentation and related work. Revised September 4, 2012 to add Section 8 on
  example networks. Revised October 10, 2012 to make minor improvements to the
  presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a synchronous point-to-point network of n nodes connected by
directed links, wherein each node has a binary input. This paper proves a tight
necessary and sufficient condition on the underlying communication topology for
achieving Byzantine consensus among these nodes in the presence of up to f
Byzantine faults. We derive a necessary condition, and then we provide a
constructive proof of sufficiency by presenting a Byzantine consensus algorithm
for directed graphs that satisfy the necessary condition.
  Prior work has developed analogous necessary and sufficient conditions for
undirected graphs. It is known that, for undirected graphs, the following two
conditions are together necessary and sufficient [8, 2, 6]: (i) n ? 3f + 1, and
(ii) network connectivity greater than 2f. However, these conditions are not
adequate to completely characterize Byzantine consensus in directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5076</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5076</id><created>2012-08-24</created><authors><author><keyname>Ghaderi</keyname><forenames>Javad</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author></authors><title>Opinion Dynamics in Social Networks: A Local Interaction Game with
  Stubborn Agents</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process by which new ideas, innovations, and behaviors spread through a
large social network can be thought of as a networked interaction game: Each
agent obtains information from certain number of agents in his friendship
neighborhood, and adapts his idea or behavior to increase his benefit. In this
paper, we are interested in how opinions, about a certain topic, form in social
networks. We model opinions as continuous scalars ranging from 0 to 1 with 1(0)
representing extremely positive(negative) opinion. Each agent has an initial
opinion and incurs some cost depending on the opinions of his neighbors, his
initial opinion, and his stubbornness about his initial opinion. Agents
iteratively update their opinions based on their own initial opinions and
observing the opinions of their neighbors. The iterative update of an agent can
be viewed as a myopic cost-minimization response (i.e., the so-called best
response) to the others' actions. We study whether an equilibrium can emerge as
a result of such local interactions and how such equilibrium possibly depends
on the network structure, initial opinions of the agents, and the location of
stubborn agents and the extent of their stubbornness. We also study the
convergence speed to such equilibrium and characterize the convergence time as
a function of aforementioned factors. We also discuss the implications of such
results in a few well-known graphs such as Erdos-Renyi random graphs and
small-world graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5083</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5083</id><created>2012-08-24</created><updated>2013-01-31</updated><authors><author><keyname>Post</keyname><forenames>Ian</forenames></author><author><keyname>Ye</keyname><forenames>Yinyu</forenames></author></authors><title>The simplex method is strongly polynomial for deterministic Markov
  decision processes</title><categories>cs.DS</categories><comments>Minor typo fixes and improvements over version 1. Appeared in SODA
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the simplex method with the highest gain/most-negative-reduced
cost pivoting rule converges in strongly polynomial time for deterministic
Markov decision processes (MDPs) regardless of the discount factor. For a
deterministic MDP with n states and m actions, we prove the simplex method runs
in O(n^3m^2log^2 n) iterations if the discount factor is uniform and
O(n^5m^3log^2 n) iterations if each action has a distinct discount factor.
Previously the simplex method was known to run in polynomial time only for
discounted MDPs where the discount was bounded away from 1 [Ye11].
  Unlike in the discounted case, the algorithm does not greedily converge to
the optimum, and we require a more complex measure of progress. We identify a
set of layers in which the values of primal variables must lie and show that
the simplex method always makes progress optimizing one layer, and when the
upper layer is updated the algorithm makes a substantial amount of progress. In
the case of nonuniform discounts, we define a polynomial number of &quot;milestone&quot;
policies and we prove that, while the objective function may not improve
substantially overall, the value of at least one dual variable is always making
progress towards some milestone, and the algorithm will reach the next
milestone in a polynomial number of steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5092</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5092</id><created>2012-08-24</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Zhao</keyname><forenames>Deli</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Graph Degree Linkage: Agglomerative Clustering on a Directed Graph</title><categories>cs.CV cs.SI stat.ML</categories><comments>Proceedings of European Conference on Computer Vision (ECCV), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple but effective graph-based agglomerative
algorithm, for clustering high-dimensional data. We explore the different roles
of two fundamental concepts in graph theory, indegree and outdegree, in the
context of clustering. The average indegree reflects the density near a sample,
and the average outdegree characterizes the local geometry around a sample.
Based on such insights, we define the affinity measure of clusters via the
product of average indegree and average outdegree. The product-based affinity
makes our algorithm robust to noise. The algorithm has three main advantages:
good performance, easy implementation, and high computational efficiency. We
test the algorithm on two fundamental computer vision problems: image
clustering and object matching. Extensive experiments demonstrate that it
outperforms the state-of-the-arts in both applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5096</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5096</id><created>2012-08-24</created><authors><author><keyname>Mamun</keyname><forenames>Mohammad Saiful Islam</forenames></author><author><keyname>Miyaji</keyname><forenames>Atsuko</forenames></author></authors><title>An Optimized Signature Verification System for Vehicle Ad hoc NETwork</title><categories>cs.CR</categories><comments>8 pages, IEEE WiCOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper1 presents an efficient approach to an existing batch verification
system on Identity based group signature (IBGS) which can be applied to any
Mobile ad hoc network device including Vehicle Ad hoc Networks (VANET). We
propose an optimized way to batch signatures in order to get maximum throughput
from a device in runtime environment. In addition, we minimize the number of
pairing computations in batch verification proposed by B. Qin et al. for large
scale VANET. We introduce a batch scheduling algorithm for batch verification
targeting further minimization the batch computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5124</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5124</id><created>2012-08-25</created><authors><author><keyname>Van Tuan</keyname><forenames>Do</forenames></author><author><keyname>Hien</keyname><forenames>Tran Dang</forenames></author><author><keyname>Van At</keyname><forenames>Pham</forenames></author></authors><title>A Novel Data Hiding Scheme for Binary Images</title><categories>cs.CR cs.GR</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper presents a new scheme for hiding a secret message in binary
images. Given m*n cover image block, the new scheme can conceal as many as
log(m*n +1) bits of data in block, by changing at most one bit in the block.
The hiding ability of the new scheme is the same as Chang et al.'s scheme and
higher than Tseng et al.'s scheme. Additionally, the security of the new scheme
is higher than the two above schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5129</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5129</id><created>2012-08-25</created><updated>2012-09-28</updated><authors><author><keyname>Boja&#x144;czyk</keyname><forenames>Miko&#x142;aj</forenames><affiliation>University of Warsaw</affiliation></author><author><keyname>Segoufin</keyname><forenames>Luc</forenames><affiliation>INRIA</affiliation></author><author><keyname>Straubing</keyname><forenames>Howard</forenames><affiliation>Boston college</affiliation></author></authors><title>Piecewise testable tree languages</title><categories>cs.FL</categories><proxy>LMCS</proxy><acm-class>F.4.3,F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  29, 2012) lmcs:1216</journal-ref><doi>10.2168/LMCS-8(3:26)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a decidable characterization of tree languages that can
be defined by a boolean combination of Sigma_1 sentences. This is a tree
extension of the Simon theorem, which says that a string language can be
defined by a boolean combination of Sigma_1 sentences if and only if its
syntactic monoid is J-trivial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5130</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5130</id><created>2012-08-25</created><updated>2013-02-14</updated><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Value production in a collaborative environment</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>In press: Special Issue of Journal of Statistical Physics:
  Statistical Mechanics and Social Science</comments><journal-ref>Journal of Statistical Physics May 2013, Volume 151, Issue 3-4, pp
  414-439</journal-ref><doi>10.1007/s10955-013-0728-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review some recent endeavors and add some new results to characterize and
understand underlying mechanisms in Wikipedia (WP), the paradigmatic example of
collaborative value production. We analyzed the statistics of editorial
activity in different languages and observed typical circadian and weekly
patterns, which enabled us to estimate the geographical origins of
contributions to WPs in languages spoken in several time zones. Using a
recently introduced measure we showed that the editorial activities have
intrinsic dependencies in the burstiness of events. A comparison of the English
and Simple English WPs revealed important aspects of language complexity and
showed how peer cooperation solved the task of enhancing readability. One of
our focus issues was characterizing the conflicts or edit wars in WPs, which
helped us to automatically filter out controversial pages. When studying the
temporal evolution of the controversiality of such pages we identified typical
patterns and classified conflicts accordingly. Our quantitative analysis
provides the basis of modeling conflicts and their resolution in collaborative
environments and contribute to the understanding of this issue, which becomes
increasingly important with the development of information communication
technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5154</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5154</id><created>2012-08-25</created><updated>2014-08-28</updated><authors><author><keyname>McAllester</keyname><forenames>David</forenames></author><author><keyname>Myllymaki</keyname><forenames>Petri</forenames></author></authors><title>Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2008</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5155</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5155</id><created>2012-08-25</created><updated>2014-08-28</updated><authors><author><keyname>Parr</keyname><forenames>Ronald</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda S.</forenames></author></authors><title>Proceedings of the Twenty-Third Conference on Uncertainty in Artificial
  Intelligence (2007)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2007</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Third Conference on Uncertainty in
Artificial Intelligence, which was held in Vancouver, British Columbia, July 19
- 22 2007.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5159</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5159</id><created>2012-08-25</created><updated>2014-08-28</updated><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>Proceedings of the Twenty-First Conference on Uncertainty in Artificial
  Intelligence (2005)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29
2005.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5160</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5160</id><created>2012-08-25</created><updated>2014-08-28</updated><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Proceedings of the Twenty-Second Conference on Uncertainty in Artificial
  Intelligence (2006)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2006</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Second Conference on Uncertainty in
Artificial Intelligence, which was held in Cambridge, MA, July 13 - 16 2006.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5161</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5161</id><created>2012-08-25</created><updated>2014-08-28</updated><authors><author><keyname>Chickering</keyname><forenames>Max</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph</forenames></author></authors><title>Proceedings of the Twentieth Conference on Uncertainty in Artificial
  Intelligence (2004)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2004</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twentieth Conference on Uncertainty in
Artificial Intelligence, which was held in Banff, Canada, July 7 - 11 2004.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5188</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5188</id><created>2012-08-26</created><updated>2014-11-14</updated><authors><author><keyname>Edwards</keyname><forenames>Katherine</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author></authors><title>A superlocal version of Reed's Conjecture</title><categories>cs.DM math.CO</categories><comments>17 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1109.2112</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reed's well-known $\omega$, $\Delta$, $\chi$ conjecture proposes that every
graph satisfies $\chi \leq \lceil \frac 12(\Delta+1+\omega)\rceil$. The second
author formulated a {\em local strengthening} of this conjecture that considers
a bound supplied by the neighbourhood of a single vertex. Following the idea
that the chromatic number cannot be greatly affected by any particular stable
set of vertices, we propose a further strengthening that considers a bound
supplied by the neighbourhoods of two adjacent vertices. We provide some
fundamental evidence in support, namely that the stronger bound holds in the
fractional relaxation and holds for both quasi-line graphs and graphs with
stability number two. We also conjecture that in the fractional version, we can
push the locality even further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5190</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5190</id><created>2012-08-26</created><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Wang</keyname><forenames>Huaxiong</forenames></author><author><keyname>Zhang</keyname><forenames>Liang Feng</forenames></author></authors><title>On Bringer-Chabanne EPIR Protocol for Polynomial Evaluation</title><categories>cs.CR</categories><comments>23 pages</comments><journal-ref>Journal of Mathematical Crytology, vol. 5, pp.277-301, 2012</journal-ref><doi>10.1515/jmc-2012-0001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extended private information retrieval (EPIR) was defined by \cite{BCPT07} at
CANS'07 and generalized by \cite{BC09} at AFRICACRYPT'09. In the generalized
setting, EPIR allows a user to evaluate a function on a database block such
that the database can learn neither which function has been evaluated nor on
which block the function has been evaluated and the user learns no more
information on the database blocks except for the expected result. An EPIR
protocol for evaluating polynomials over a finite field $L$ was proposed by
Bringer and Chabanne in \cite{BC09}. We show that the protocol does not satisfy
the correctness requirement as they have claimed. In particular, we show that
it does not give the user the expected result with large probability if one of
the coefficients of the polynomial to be evaluated is primitive in $L$ and the
others belong to the prime subfield of $L$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5194</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5194</id><created>2012-08-26</created><updated>2013-03-29</updated><authors><author><keyname>Zhang</keyname><forenames>Liang Feng</forenames></author></authors><title>On the Eigenvalues of Certain Matrices Over $\mathbb{Z}_m$</title><categories>cs.DM math.CO</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $m,n&gt;1$ be integers and $\mathbb{P}_{n,m}$ be the point set of the
projective $(n-1)$-space (defined by [2]) over the ring $\mathbb{Z}_m$of
integers modulo $m$. Let $A_{n,m}=(a_{uv})$ be the matrix with rows and columns
being labeled by elements of $\mathbb{P}_{n,m}$, where $a_{uv}=1$ if the inner
product $&lt; u,v &gt;=0$ and $a_{uv}=0$ otherwise. Let $B_{n,m}=A_{n,m}A_{n,m}^t$.
The eigenvalues of $B_{n,m}$ have been studied by [1, 2, 3], where their
applications in the study of expanders and locally decodable codes were
described. In this paper, we completely determine the eigenvalues of $B_{n,m}$
for general integers $m$ and $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5195</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5195</id><created>2012-08-26</created><authors><author><keyname>Javed</keyname><forenames>Muhammad</forenames></author><author><keyname>Ahmad</keyname><forenames>Bashir</forenames></author><author><keyname>Abbas</keyname><forenames>Zaffar</forenames></author><author><keyname>Nawaz</keyname><forenames>Allah</forenames></author><author><keyname>Abid</keyname><forenames>Muhammad Ali</forenames></author><author><keyname>Ullah</keyname><forenames>Ihsan</forenames></author></authors><title>Decreasing defect rate of test cases by designing and analysis for
  recursive modules of a program structure: Improvement in test cases</title><categories>cs.SE</categories><comments>4 pages</comments><journal-ref>IJCSIS August 2012, Vol. 10 No. 8</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Designing and analysis of test cases is a challenging tasks for tester roles
especially those who are related to test the structure of program. Recently,
Programmers are showing valuable trend towards the implementation of recursive
modules in a program structure. In testing phase of software development life
cycle, test cases help the tester to test the structure and flow of program.
The implementation of well designed test cases for a program leads to reduce
the defect rate and efforts needed for corrective maintenance. In this paper,
author proposed a strategy to design and analyze the test cases for a program
structure of recursive modules. This strategy will definitely leads to
validation of program structure besides reducing the defect rate and corrective
maintenance efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5205</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5205</id><created>2012-08-26</created><updated>2012-12-29</updated><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Monoidal computer I: Basic computability by string diagrams</title><categories>cs.LO math.CT</categories><comments>37 pages, 47 figures, Oxford Workshop on Resources in Security, July
  2012; in this version: fixed typos, added a sentence to the abstract;
  accepted for publication in Information and Computation</comments><msc-class>03D10, 68Q05, 68Q15</msc-class><acm-class>F.1.1; F.1.3; F.4.1</acm-class><journal-ref>Information and Computation (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new model of computation, described in terms of monoidal
categories. It conforms the Church-Turing Thesis, and captures the same
computable functions as the standard models. It provides a succinct categorical
interface to most of them, free of their diverse implementation details, using
the ideas and structures that in the meantime emerged from research in
semantics of computation and programming. The salient feature of the language
of monoidal categories is that it is supported by a sound and complete
graphical formalism, string diagrams, which provide a concrete and intuitive
interface for abstract reasoning about computation. The original motivation and
the ultimate goal of this effort is to provide a convenient high level
programming language for a theory of computational resources, such as one-way
functions, and trapdoor functions, by adopting the methods for hiding the low
level implementation details that emerged from practice. In the present paper,
we make a first step towards this ambitious goal, and sketch a path to reach
it. This path is pursued in three sequel papers, that are in preparation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5207</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5207</id><created>2012-08-26</created><authors><author><keyname>Lawrencenko</keyname><forenames>Serge</forenames></author></authors><title>On the minimum order of a quadrangulation on a given closed 2-manifold</title><categories>math.CO cs.DM math.AT math.GT</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1207.1882</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partial formula is provided to calculate the smallest number of vertices
possible in a quadrangulation on the closed orientable 2-manifold of given
genus. This extends the previously known partial formula due to N. Hartsfield
and G. Ringel [J. Comb. Theory, Ser. B, 1989, 46, 84-95].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5211</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5211</id><created>2012-08-26</created><authors><author><keyname>Garber</keyname><forenames>Dan</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Almost Optimal Sublinear Time Algorithm for Semidefinite Programming</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for approximating semidefinite programs with running
time that is sublinear in the number of entries in the semidefinite instance.
We also present lower bounds that show our algorithm to have a nearly optimal
running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5216</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5216</id><created>2012-08-26</created><updated>2012-12-11</updated><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author><author><keyname>Tonchev</keyname><forenames>Vladimir D.</forenames></author></authors><title>High-rate self-synchronizing codes</title><categories>cs.IT math.CO math.IT</categories><comments>9 pages, no figure, 2 tables. Final accepted version for publication
  in the IEEE Transactions on Information Theory. Material presented in part at
  the International Symposium on Information Theory and its Applications,
  Honolulu, HI USA, October 2012</comments><msc-class>94A45, 05B10, 68R05</msc-class><journal-ref>IEEE Transactions on Information Theory 59 (2013) 2328-2335</journal-ref><doi>10.1109/TIT.2012.2234501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-synchronization under the presence of additive noise can be achieved by
allocating a certain number of bits of each codeword as markers for
synchronization. Difference systems of sets are combinatorial designs which
specify the positions of synchronization markers in codewords in such a way
that the resulting error-tolerant self-synchronizing codes may be realized as
cosets of linear codes. Ideally, difference systems of sets should sacrifice as
few bits as possible for a given code length, alphabet size, and
error-tolerance capability. However, it seems difficult to attain optimality
with respect to known bounds when the noise level is relatively low. In fact,
the majority of known optimal difference systems of sets are for exceptionally
noisy channels, requiring a substantial amount of bits for synchronization. To
address this problem, we present constructions for difference systems of sets
that allow for higher information rates while sacrificing optimality to only a
small extent. Our constructions utilize optimal difference systems of sets as
ingredients and, when applied carefully, generate asymptotically optimal ones
with higher information rates. We also give direct constructions for optimal
difference systems of sets with high information rates and error-tolerance that
generate binary and ternary self-synchronizing codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5243</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5243</id><created>2012-08-26</created><authors><author><keyname>Ubi</keyname><forenames>Jaan</forenames></author></authors><title>General Managers Role in Balancing Subsidiary Between Internal
  Competition and Knowledge Sharing</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work we saw that during the last decades the environment that the MNCs
operate in has changed becoming more volatile and less pacedly growing. In this
environment the MNCs themselves have become more complex and also flexible. We
found that MNCs are essentially three dimensional, that is, they organize
around product, functional and geographical dimensions and exhibit
characteristics that having a common origin can be applied along any one
dimension. Therefore we depicted MNCs as having a divergent, partially
overlapping structural map. On that map there can, for instance, be
functionally oriented Centers Of Excellence, product dimension World Product
Mandates, and for capturing synergy of a big set of operations country
dimension based arrangements. Analyzing the development of organizational
aspects of MNC theories we saw different bodies of work pointing to a similar
direction. We followed developments of concepts Heterarchy, Transnational (and
the related Individualized Corporation), works of interorganizational theories
school (the multicentered MNC), works considering autonomous strategic
decisions, and works originating from subsidiary (host) country research. In
addition to the structural developments mentioned earlier these works also
point to a need to empower the frontline units. Outlining our research problem
the conceptualization of MNCs as operating (competitive) internal markets was
shown to rely on entrepreneurial, initiative taking behavior and result in the
development of the subsidiary. We classified the subsidiaries first selecting
operations substantial enough and then differentiating them based on the
autonomy level, as it has implications for the types of initiatives taken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5247</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5247</id><created>2012-08-26</created><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Faster Clustering via Preprocessing</title><categories>cs.DS cs.CG</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the efficiency of clustering a set of points, when the
encompassing metric space may be preprocessed in advance. In computational
problems of this genre, there is a first stage of preprocessing, whose input is
a collection of points $M$; the next stage receives as input a query set
$Q\subset M$, and should report a clustering of $Q$ according to some
objective, such as 1-median, in which case the answer is a point $a\in M$
minimizing $\sum_{q\in Q} d_M(a,q)$.
  We design fast algorithms that approximately solve such problems under
standard clustering objectives like $p$-center and $p$-median, when the metric
$M$ has low doubling dimension. By leveraging the preprocessing stage, our
algorithms achieve query time that is near-linear in the query size $n=|Q|$,
and is (almost) independent of the total number of points $m=|M|$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5258</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5258</id><created>2012-08-26</created><updated>2012-12-17</updated><authors><author><keyname>Li</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Daniel Yang</forenames></author><author><keyname>Miklau</keyname><forenames>Gerome</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>A Theory of Pricing Private Data</title><categories>cs.CR cs.DB</categories><comments>25 pages, 2 figures. Best Paper Award, to appear in the 16th
  International Conference on Database Theory (ICDT), 2013</comments><journal-ref>ICDT '13 Proceedings of the 16th International Conference on
  Database Theory Pages 33-44, 2013</journal-ref><doi>10.1145/2448496.2448502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personal data has value to both its owner and to institutions who would like
to analyze it. Privacy mechanisms protect the owner's data while releasing to
analysts noisy versions of aggregate query results. But such strict protections
of individual's data have not yet found wide use in practice. Instead, Internet
companies, for example, commonly provide free services in return for valuable
sensitive information from users, which they exploit and sometimes sell to
third parties.
  As the awareness of the value of the personal data increases, so has the
drive to compensate the end user for her private information. The idea of
monetizing private data can improve over the narrower view of hiding private
data, since it empowers individuals to control their data through financial
means.
  In this paper we propose a theoretical framework for assigning prices to
noisy query answers, as a function of their accuracy, and for dividing the
price amongst data owners who deserve compensation for their loss of privacy.
Our framework adopts and extends key principles from both differential privacy
and query pricing in data markets. We identify essential properties of the
price function and micro-payments, and characterize valid solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5268</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5268</id><created>2012-08-26</created><authors><author><keyname>Gr&#xe4;del</keyname><forenames>Erich</forenames></author><author><keyname>V&#xe4;&#xe4;n&#xe4;nen</keyname><forenames>Jouko</forenames></author></authors><title>Dependence and Independence</title><categories>cs.LO math.LO</categories><msc-class>03C80,</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an atomic formula intuitively saying that given variables are
independent from given other variables if a third set of variables is kept
constant. We contrast this with dependence logic. We show that our independence
atom gives rise to a natural logic capable of formalizing basic intuitions
about independence and dependence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5269</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5269</id><created>2012-08-26</created><authors><author><keyname>Tulino</keyname><forenames>Antonia</forenames><affiliation>Shitz</affiliation></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames><affiliation>Shitz</affiliation></author><author><keyname>Verdu'</keyname><forenames>Sergio</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Support Recovery with Sparsely Sampled Free Random Matrices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a Bernoulli-Gaussian complex $n$-vector whose components are $V_i =
X_i B_i$, with $X_i \sim \Cc\Nc(0,\Pc_x)$ and binary $B_i$ mutually independent
and iid across $i$. This random $q$-sparse vector is multiplied by a square
random matrix $\Um$, and a randomly chosen subset, of average size $n p$, $p
\in [0,1]$, of the resulting vector components is then observed in additive
Gaussian noise. We extend the scope of conventional noisy compressive sampling
models where $\Um$ is typically %A16 the identity or a matrix with iid
components, to allow $\Um$ satisfying a certain freeness condition. This class
of matrices encompasses Haar matrices and other unitarily invariant matrices.
We use the replica method and the decoupling principle of Guo and Verd\'u, as
well as a number of information theoretic bounds, to study the input-output
mutual information and the support recovery error rate in the limit of $n \to
\infty$. We also extend the scope of the large deviation approach of Rangan,
Fletcher and Goyal and characterize the performance of a class of estimators
encompassing thresholded linear MMSE and $\ell_1$ relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5270</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5270</id><created>2012-08-26</created><authors><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author></authors><title>The Effects of Limited Channel Knowledge on Cognitive Radio System
  Capacity</title><categories>cs.IT math.IT</categories><doi>10.1109/TVT.2012.2227864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the impact of limited channel knowledge on the secondary user (SU)
in a cognitive radio system. Under a minimum signal-to-interference-and-noise
ratio (SINR) constraint for the primary user (PU) receiver, we determine the SU
capacity under five channel knowledge scenarios. We derive analytical
expressions for the capacity cumulative distribution functions and the
probability of SU blocking as a function of allowable interference. We show
that imperfect knowledge of the PU-PU channel gain by the SU-Tx often prohibits
SU transmission or necessitates a high interference level at the PU. We also
show that errored knowledge of the PU-PU channel is more beneficial than
statistical channel knowledge and imperfect knowledge of the SU-Tx to PU-Rx
link has a limited impact on SU capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5273</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5273</id><created>2012-08-26</created><updated>2014-10-27</updated><authors><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author><author><keyname>Urbanke</keyname><forenames>Ruediger</forenames></author></authors><title>Wave-Like Solutions of General One-Dimensional Spatially Coupled Systems</title><categories>cs.IT math.IT</categories><comments>39 pages, 10 figures. Submitted to IEEE Transactions on Information
  theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the existence of wave-like solutions to spatially coupled
graphical models which, in the large size limit, can be characterized by a
one-dimensional real-valued state. This is extended to a proof of the threshold
saturation phenomenon for all such models, which includes spatially coupled
irregular LDPC codes over the BEC, but also addresses hard-decision decoding
for transmission over general channels, the CDMA multiple-access problem,
compressed sensing, and some statistical physics models.
  For traditional uncoupled iterative coding systems with two components and
transmission over the BEC, the asymptotic convergence behavior is completely
characterized by the EXIT curves of the components. More precisely, the system
converges to the desired fixed point, which is the one corresponding to perfect
decoding, if and only if the two EXIT functions describing the components do
not cross. For spatially coupled systems whose state is one-dimensional a
closely related graphical criterion applies. Now the curves are allowed to
cross, but not by too much. More precisely, we show that the threshold
saturation phenomenon is related to the positivity of the (signed) area
enclosed by two EXIT-like functions associated to the component systems, a very
intuitive and easy-to-use graphical characterization.
  In the spirit of EXIT functions and Gaussian approximations, we also show how
to apply the technique to higher dimensional and even infinite-dimensional
cases. In these scenarios the method is no longer rigorous, but it typically
gives accurate predictions. To demonstrate this application, we discuss
transmission over general channels using both the belief-propagation as well as
the min-sum decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5280</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5280</id><created>2012-08-26</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Farrell</keyname><forenames>Brendan</forenames></author></authors><title>On the Peak-to-Average Power Ratio Reduction Problem for Orthogonal
  Transmission Schemes</title><categories>cs.IT math.IT</categories><comments>Invited/accepted contribution to a special issue of Internet
  Mathematics on wireless communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High peak values of transmission signals in wireless communication systems
lead to wasteful energy consumption and out-of-band radiation. However,
reducing peak values generally comes at the cost some other resource. We
provide a theoretical contribution towards understanding the relationship
between peak value reduction and the resulting cost in information rates. In
particular, we address the relationship between peak values and the proportion
of transmission signals allocated for information transmission when using a
strategy known as tone reservation. We show that when using tone reservation in
both OFDM and DS-CDMA systems, if a Peak-to-Average Power Ratio criterion is
always satisfied, then the proportion of transmission signals that may be
allocated for information transmission must tend to zero. We investigate
properties of these two systems for sets of both finite and infinite
cardinalities. We present properties that OFDM and DS-CDMA share in common as
well as ways in which they fundamentally differ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5281</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5281</id><created>2012-08-26</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Farrell</keyname><forenames>Brendan</forenames></author><author><keyname>Ledoux</keyname><forenames>Michel</forenames></author><author><keyname>Wiese</keyname><forenames>Moritz</forenames></author></authors><title>Expected Supremum of a Random Linear Combination of Shifted Kernels</title><categories>cs.IT math.IT</categories><comments>To appear in the Journal of Fourier Analysis and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the expected supremum of a linear combination of shifts of the
sinc kernel with random coefficients. When the coefficients are Gaussian, the
expected supremum is of order \sqrt{\log n}, where n is the number of shifts.
When the coefficients are uniformly bounded, the expected supremum is of order
\log\log n. This is a noteworthy difference to orthonormal functions on the
unit interval, where the expected supremum is of order \sqrt{n\log n} for all
reasonable coefficient statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5289</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5289</id><created>2012-08-27</created><updated>2012-10-23</updated><authors><author><keyname>Payne</keyname><forenames>Michael S.</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>On the general position subset selection problem</title><categories>math.CO cs.CG</categories><msc-class>52C10</msc-class><journal-ref>SIAM J. Discrete Math. 27:1727-1733, 2013</journal-ref><doi>10.1137/120897493</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f(n,\ell)$ be the maximum integer such that every set of $n$ points in
the plane with at most $\ell$ collinear contains a subset of $f(n,\ell)$ points
with no three collinear. First we prove that if $\ell \leq O(\sqrt{n})$ then
$f(n,\ell)\geq \Omega(\sqrt{\frac{n}{\ln \ell}})$. Second we prove that if
$\ell \leq O(n^{(1-\epsilon)/2})$ then $f(n,\ell) \geq \Omega(\sqrt{n\log_\ell
n})$, which implies all previously known lower bounds on $f(n,\ell)$ and
improves them when $\ell$ is not fixed. A more general problem is to consider
subsets with at most $k$ collinear points in a point set with at most $\ell$
collinear. We also prove analogous results in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5316</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5316</id><created>2012-08-27</created><authors><author><keyname>Magrassi</keyname><forenames>Paolo</forenames></author></authors><title>How Non-linearity will Transform Information Systems</title><categories>cs.CE q-fin.GN</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  One 'problem' with the 21st century world, particularly the economic and
business worlds, is the phenomenal and increasing number of interconnections
between economic agents (consumers, firms, banks, markets, national economies).
This implies that such agents are all interacting and consequently giving raise
to enormous degrees of non-linearity, a.k.a. complexity. Complexity often
brings with it unexpected phenomena, such as chaos and emerging behaviour, that
can become challenges for the survival of economic agents and systems.
Developing econophysics approaches are beginning to apply, to the 'economic
web', methods and models that have been used in physics and/or systems theory
to tackle non-linear domains. The paper gives an account of the research in
progress in this field and shows its implications for enteprise information
systems, anticipating the emergence of software that will allow to reflect the
complexity of the business world, as holistic risk management becomes a mandate
for financial institutions and business organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5317</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5317</id><created>2012-08-27</created><updated>2015-04-29</updated><authors><author><keyname>F&#xfc;l&#xf6;p</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Vogler</keyname><forenames>Heiko</forenames></author></authors><title>Characterizing Weighted MSO for Trees by Branching Transitive Closure
  Logics</title><categories>cs.FL cs.LO</categories><acm-class>F.1.1; F.4.1; F.4.3</acm-class><doi>10.1016/j.tcs.2015.04.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the branching transitive closure operator on weighted monadic
second-order logic formulas where the branching corresponds in a natural way to
the branching inherent in trees. For arbitrary commutative semirings, we prove
that weighted monadic second order logics on trees is equivalent to the
definability by formulas which start with one of the following operators: (i) a
branching transitive closure or (ii) an existential second-order quantifier
followed by one universal first-order quantifier; in both cases the operator is
applied to step-formulas over (a) Boolean first-order logic enriched by modulo
counting or (b) Boolean monadic-second order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5324</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5324</id><created>2012-08-27</created><authors><author><keyname>F&#xfc;l&#xf6;p</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Vogler</keyname><forenames>Heiko</forenames></author></authors><title>Forward and Backward Application of Symbolic Tree Transducers</title><categories>cs.FL</categories><acm-class>F.1.1; F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider symbolic tree automata (sta) and symbolic tree transducers (stt).
We characterize s-recognizable tree languages (which are the tree languages
recognizable by sta) in terms of (classical) recognizable tree languages and
relabelings. We prove that sta and the recently introduced variable tree
automata are incomparable with respect to their recognition power. We define
symbolic regular tree grammars and characterize s-regular tree languages in
terms of regular tree languages and relabelings. As a consequence, we obtain
that s-recognizable tree languages are the same as s-regular tree languages.
  We show that the syntactic composition of two stt computes the composition of
the tree transformations computed by each stt, provided that (1) the first one
is deterministic or the second one is linear and (2) the first one is total or
the second is nondeleting. We consider forward application and backward
application of stt and prove that the backward application of an stt to any
s-recognizable tree language yields an s-recognizable tree language. We give a
linear stt of which the range is not an s-recognizable tree language. We show
that the forward application of simple and linear stt preserves
s-recognizability. As a corollary, we obtain that the type checking problem of
simple and linear stt and the inverse type checking problem of arbitrary stt is
decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5333</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5333</id><created>2012-08-27</created><updated>2012-09-16</updated><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Chira</keyname><forenames>Camelia</forenames></author><author><keyname>Crisan</keyname><forenames>Gloria-C.</forenames></author></authors><title>A hybrid ACO approach to the Matrix Bandwidth Minimization Problem</title><categories>cs.AI cs.NE</categories><comments>This paper has been withdrawn by the author, due to an error</comments><msc-class>68T20</msc-class><journal-ref>Lecture Notes in Computer Science 6076 (2010) 405-412</journal-ref><doi>10.1007/978-3-642-13769-3_49</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of the human society raises more and more difficult endeavors.
For some of the real-life problems, the computing time-restriction enhances
their complexity. The Matrix Bandwidth Minimization Problem (MBMP) seeks for a
simultaneous permutation of the rows and the columns of a square matrix in
order to keep its nonzero entries close to the main diagonal. The MBMP is a
highly investigated P-complete problem, as it has broad applications in
industry, logistics, artificial intelligence or information recovery. This
paper describes a new attempt to use the Ant Colony Optimization framework in
tackling MBMP. The introduced model is based on the hybridization of the Ant
Colony System technique with new local search mechanisms. Computational
experiments confirm a good performance of the proposed algorithm for the
considered set of MBMP instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5340</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5340</id><created>2012-08-27</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Chira</keyname><forenames>Camelia</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author></authors><title>New results of ant algorithms for the Linear Ordering Problem</title><categories>cs.AI cs.NE</categories><comments>5 pages, 5 figures Zbl:06048718</comments><msc-class>68T20</msc-class><journal-ref>An. Univ. Vest Timis., Ser. Mat.-Inform. 48, No. 3, 139-150 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ant-based algorithms are successful tools for solving complex problems. One
of these problems is the Linear Ordering Problem (LOP). The paper shows new
results on some LOP instances, using Ant Colony System (ACS) and the Step-Back
Sensitive Ant Model (SB-SAM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5341</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5341</id><created>2012-08-27</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Chira</keyname><forenames>Camelia</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author><author><keyname>Pop</keyname><forenames>Petrica C.</forenames></author></authors><title>Sensitive Ants in Solving the Generalized Vehicle Routing Problem</title><categories>cs.AI cs.NE</categories><comments>5 pages</comments><msc-class>68T20</msc-class><journal-ref>INT J COMPUT COMMUN, VI(4) (2011) 731-738</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of sensitivity in ant colony systems has been exploited in hybrid
ant-based models with promising results for many combinatorial optimization
problems. Heterogeneity is induced in the ant population by endowing individual
ants with a certain level of sensitivity to the pheromone trail. The variable
pheromone sensitivity within the same population of ants can potentially
intensify the search while in the same time inducing diversity for the
exploration of the environment. The performance of sensitive ant models is
investigated for solving the generalized vehicle routing problem. Numerical
results and comparisons are discussed and analysed with a focus on emphasizing
any particular aspects and potential benefits related to hybrid ant-based
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5345</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5345</id><created>2012-08-27</created><updated>2012-10-23</updated><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Heggernes</keyname><forenames>Pinar</forenames></author><author><keyname>Kratsch</keyname><forenames>Dieter</forenames></author><author><keyname>Villanger</keyname><forenames>Yngve</forenames></author></authors><title>Generating All Minimal Edge Dominating Sets with Incremental-Polynomial
  Delay</title><categories>cs.DS cs.DM math.CO</categories><comments>The incremental-polynomial delay for enumerating minimal dominating
  sets in line graphs have been improved from O(m^4 |L|^2) to O(m^5 |L|)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an arbitrary undirected simple graph G with m edges, we give an algorithm
with running time O(m^4 |L|^2) to generate the set L of all minimal edge
dominating sets of G. For bipartite graphs we obtain a better result; we show
that their minimal edge dominating sets can be enumerated in time O(m^4 |L|).
In fact our results are stronger; both algorithms generate the next minimal
edge dominating set with incremental-polynomial delay O(m^5 |L|) and O(m^4 |L|)
respectively, when L is the set of already generated minimal edge dominating
sets. Our algorithms are tailored for and solve the equivalent problems of
enumerating minimal (vertex) dominating sets of line graphs and line graphs of
bipartite graphs, with incremental-polynomial delay, and consequently in
output-polynomial time. Enumeration of minimal dominating sets in graphs has
very recently been shown to be equivalent to enumeration of minimal
transversals in hypergraphs. The question whether the minimal transversals of a
hypergraph can be enumerated in output-polynomial time is a fundamental and
challenging question in Output-Sensitive Enumeration; it has been open for
several decades and has triggered extensive research in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5365</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5365</id><created>2012-08-27</created><authors><author><keyname>Aly</keyname><forenames>Salah A.</forenames></author></authors><title>A Missing and Found Recognition System for Hajj and Umrah</title><categories>cs.CV cs.CY</categories><comments>website available via http://www.mfhajj.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note describes an integrated recognition system for identifying missing
and found objects as well as missing, dead, and found people during Hajj and
Umrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of
Saudi Arabia. It is assumed that the total estimated number of pilgrims will
reach 20 millions during the next decade. The ultimate goal of this system is
to integrate facial recognition and object identification solutions into the
Hajj and Umrah rituals. The missing and found computerized system is part of
the CrowdSensing system for Hajj and Umrah crowd estimation, management and
safety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5370</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5370</id><created>2012-08-27</created><updated>2013-05-07</updated><authors><author><keyname>Sutherland</keyname><forenames>Andrew V.</forenames></author></authors><title>Isogeny volcanoes</title><categories>math.NT cs.CR</categories><comments>Invited ANTS X paper, minor edits, 18 pages</comments><msc-class>11G07, 11Y16 (Primary) 11G15, 11G20 (Secondary)</msc-class><journal-ref>ANTS X: Proceedings of the Tenth Algorithmic Number Theory
  Symposium, 2012, 507-530</journal-ref><doi>10.2140/obs.2013.1.507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable structure and computationally explicit form of isogeny graphs
of elliptic curves over a finite field has made them an important tool for
computational number theorists and practitioners of elliptic curve
cryptography. This expository paper recounts the theory behind these graphs and
examines several recently developed algorithms that realize substantial (often
dramatic) performance gains by exploiting this theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5373</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5373</id><created>2012-08-27</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author></authors><title>Distributed Pharaoh System for Network Routing</title><categories>cs.AI</categories><comments>4 pages, 4 figures</comments><journal-ref>Automat. Comput. Appl. Math. 16(1-2) (2007) 27-34</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper it is introduced a biobjective ant algorithm for constructing
low cost routing networks. The new algorithm is called the Distributed Pharaoh
System (DPS). DPS is based on AntNet algorithm. The algorithm is using Pharaoh
Ant System (PAS) with an extra-exploration phase and a 'no-entry' condition in
order to improve the solutions for the Low Cost Network Routing problem.
Additionally it is used a cost model for overlay network construction that
includes network traffic demands. The Pharaoh ants (Monomorium pharaonis)
includes negative pheromones with signals concentrated at decision points where
trails fork. The negative pheromones may complement positive pheromone or could
help ants to escape from an unnecessarily long route to food that is being
reinforced by attractive signals. Numerical experiments were made for a random
10-node network. The average node degree of the network tested was 4.0. The
results are encouraging. The algorithm converges to the shortest path while
converging on a low cost overlay routing network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5374</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5374</id><created>2012-08-27</created><updated>2013-02-02</updated><authors><author><keyname>Liu</keyname><forenames>Yen-Cheng</forenames></author><author><keyname>Chen</keyname><forenames>Ching-Wei</forenames></author><author><keyname>Su</keyname><forenames>Yu T.</forenames></author></authors><title>New Constructions of Zero-Correlation Zone Sequences</title><categories>cs.IT math.IT</categories><comments>37 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose three classes of systematic approaches for
constructing zero correlation zone (ZCZ) sequence families. In most cases,
these approaches are capable of generating sequence families that achieve the
upper bounds on the family size ($K$) and the ZCZ width ($T$) for a given
sequence period ($N$).
  Our approaches can produce various binary and polyphase ZCZ families with
desired parameters $(N,K,T)$ and alphabet size. They also provide additional
tradeoffs amongst the above four system parameters and are less constrained by
the alphabet size. Furthermore, the constructed families have nested-like
property that can be either decomposed or combined to constitute smaller or
larger ZCZ sequence sets. We make detailed comparisons with related works and
present some extended properties. For each approach, we provide examples to
numerically illustrate the proposed construction procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5394</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5394</id><created>2012-08-24</created><authors><author><keyname>Ling</keyname><forenames>Amy Poh Ai</forenames></author><author><keyname>Kokichi</keyname><forenames>Sugihara</forenames></author><author><keyname>Masao</keyname><forenames>Mukaidono</forenames></author></authors><title>The Japanese Smart Grid Initiatives, Investments, and Collaborations</title><categories>cs.SY</categories><journal-ref>International Journal of Advanced Science and Applications, 2012,
  Vol. 3, No. 7: pp. 44-54</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A smart grid delivers power around the country and has an intelligent
monitoring system, which not only keeps track of all the energy coming in from
diverse sources but also can detect where energy is needed through a two-way
communication system that collects data about how and when consumers use power.
It is safer in many ways, compared with the current one-directional power
supply system that seems susceptible to either sabotage or natural disasters,
including being more resistant to attack and power outages. In such an
autonomic and advanced-grid environment, investing in a pilot study and knowing
the nation readiness to adopt a smart grid absolves the government of complex
intervention from any failure to bring Japan into the autonomic-grid
environment. This paper looks closely into the concept of the Japanese
government go green effort, the objective of which is to make Japan a leading
nation in environmental and energy sustainability through green innovation,
such as creating a low-carbon society and embracing the natural grid community.
This paper paints a clearer conceptual picture of how Japan smart grid effort
compares with that of the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5412</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5412</id><created>2012-08-27</created><updated>2013-10-03</updated><authors><author><keyname>Marsault</keyname><forenames>Victor</forenames></author><author><keyname>Sakarovitch</keyname><forenames>Jacques</forenames></author></authors><title>On sets of numbers rationally represented in a rational base number
  system</title><categories>cs.FL</categories><journal-ref>5th International Conference, CAI 2013, Porquerolles, France,
  September 3-6, 2013. Proceedings</journal-ref><doi>10.1007/978-3-642-40663-8_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, it is proved that a set of numbers closed under addition and
whose representations in a rational base numeration system is a rational
language is not a finitely generated additive monoid.
  A key to the proof is the definition of a strong combinatorial property on
languages : the bounded left iteration property. It is both an unnatural
property in usual formal language theory (as it contradicts any kind of pumping
lemma) and an ideal fit to the languages defined through rational base number
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5413</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5413</id><created>2012-08-27</created><updated>2012-11-08</updated><authors><author><keyname>Guo</keyname><forenames>Alan</forenames></author><author><keyname>Kopparty</keyname><forenames>Swastik</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>New affine-invariant codes from lifting</title><categories>cs.IT cs.CC math.IT</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore error-correcting codes derived from the &quot;lifting&quot; of
&quot;affine-invariant&quot; codes. Affine-invariant codes are simply linear codes whose
coordinates are a vector space over a field and which are invariant under
affine-transformations of the coordinate space. Lifting takes codes defined
over a vector space of small dimension and lifts them to higher dimensions by
requiring their restriction to every subspace of the original dimension to be a
codeword of the code being lifted. While the operation is of interest on its
own, this work focusses on new ranges of parameters that can be obtained by
such codes, in the context of local correction and testing. In particular we
present four interesting ranges of parameters that can be achieved by such
lifts, all of which are new in the context of affine-invariance and some may be
new even in general. The main highlight is a construction of high-rate codes
with sublinear time decoding. The only prior construction of such codes is due
to Kopparty, Saraf and Yekhanin \cite{KSY}. All our codes are extremely simple,
being just lifts of various parity check codes (codes with one symbol of
redundancy), and in the final case, the lift of a Reed-Solomon code.
  We also present a simple connection between certain lifted codes and lower
bounds on the size of &quot;Nikodym sets&quot;. Roughly, a Nikodym set in
$\mathbb{F}_q^m$ is a set $S$ with the property that every point has a line
passing through it which is almost entirely contained in $S$. While previous
lower bounds on Nikodym sets were roughly growing as $q^m/2^m$, we use our
lifted codes to prove a lower bound of $(1 - o(1))q^m$ for fields of constant
characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5425</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5425</id><created>2012-08-27</created><authors><author><keyname>Arabzadeh</keyname><forenames>Mona</forenames></author><author><keyname>Zamani</keyname><forenames>Morteza Saheb</forenames></author><author><keyname>Sedighi</keyname><forenames>Mehdi</forenames></author><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author></authors><title>Depth-Optimized Reversible Circuit Synthesis</title><categories>quant-ph cs.ET</categories><comments>13 pages, 6 figures, 5 tables; Quantum Information Processing (QINP)
  journal, 2012</comments><journal-ref>Quantum Information Processing: Volume 12, Issue 4 (2013), Page
  1677-1699</journal-ref><doi>10.1007/s11128-012-0482-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, simultaneous reduction of circuit depth and synthesis cost of
reversible circuits in quantum technologies with limited interaction is
addressed. We developed a cycle-based synthesis algorithm which uses negative
controls and limited distance between gate lines. To improve circuit depth, a
new parallel structure is introduced in which before synthesis a set of
disjoint cycles are extracted from the input specification and distributed into
some subsets. The cycles of each subset are synthesized independently on
different sets of ancillae. Accordingly, each disjoint set can be synthesized
by ?different synthesis methods. Our analysis shows that the best worst-case
synthesis cost of reversible circuits in the linear nearest neighbor
architecture is improved by the proposed approach. Our experimental results
reveal the effectiveness of the proposed approach to reduce cost and circuit
depth for several benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5429</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5429</id><created>2012-08-27</created><authors><author><keyname>Matsui</keyname><forenames>Hajime</forenames></author></authors><title>Fast Erasure-and-Error Decoding and Systematic Encoding of a Class of
  Affine Variety Codes</title><categories>cs.IT cs.DM math.IT</categories><comments>6 pages, 2 columns, presented at The 34th Symposium on Information
  Theory and Its Applications (SITA2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a lemma in algebraic coding theory is established, which is
frequently appeared in the encoding and decoding for algebraic codes such as
Reed-Solomon codes and algebraic geometry codes. This lemma states that two
vector spaces, one corresponds to information symbols and the other is indexed
by the support of Grobner basis, are canonically isomorphic, and moreover, the
isomorphism is given by the extension through linear feedback shift registers
from Grobner basis and discrete Fourier transforms. Next, the lemma is applied
to fast unified system of encoding and decoding erasures and errors in a
certain class of affine variety codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5438</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5438</id><created>2012-08-27</created><authors><author><keyname>Ubi</keyname><forenames>Jaan</forenames></author><author><keyname>Liiv</keyname><forenames>Innar</forenames></author><author><keyname>Ubi</keyname><forenames>Evald</forenames></author><author><keyname>Vohandu</keyname><forenames>Leo</forenames></author></authors><title>Data mining the MNC like internal co-opetition duality in a university
  context</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the paper is to quantify the simultaneous competition and
cooperation that takes place in organizations. As the concepts seem to be
dichotomous opposites at first, the term internal coopetition duality is put
forth. Parallels are drawn between coopetitive processes in big multinational
corporations (MNCs) and these taking place in universities, also the structural
solutions used in both are analyzed. Data mining is used while looking at how
specializations inside the university are in competition for better students.
We look at the profiles that students have and find natural divisions between
the specializations, by applying graph theory and modularity algorithms for
community detection. The competitive position of the specializations is evident
in the average grades of the detected communities. The ratio of intercommunity
ties to intracommunity ties (conductance) quantifies the cooperative stance,
though, as students with similar profiles express linkages in the curricula;
and the choices regarding career development undertaken become evident.
Managerial implications discussed include the imperative for actively managing
and financially rewarding the outcomes of the coopetitive duality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5443</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5443</id><created>2012-08-27</created><authors><author><keyname>Lin</keyname><forenames>Bing-Rong</forenames></author><author><keyname>Kifer</keyname><forenames>Daniel</forenames></author></authors><title>A Framework for Extracting Semantic Guarantees from Privacy</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical privacy views privacy definitions as contracts that guide the
behavior of algorithms that take in sensitive data and produce sanitized data.
For most existing privacy definitions, it is not clear what they actually
guarantee.
  In this paper, we propose the first (to the best of our knowledge) framework
for extracting semantic guarantees from privacy definitions. That is, instead
of answering narrow questions such as &quot;does privacy definition Y protect X?&quot;
the goal is to answer the more general question &quot;what does privacy definition Y
protect?&quot;
  The privacy guarantees we can extract are Bayesian in nature and deal with
changes in an attacker's beliefs. The key to our framework is an object we call
the row cone. Every privacy definition has a row cone, which is a convex set
that describes all the ways an attacker's prior beliefs can be turned into
posterior beliefs after observing an output of an algorithm satisfying that
privacy definition.
  The framework can be applied to privacy definitions or even to individual
algorithms to identify the types of inferences they defend against. We
illustrate the use of our framework with analyses of several definitions and
algorithms for which we can derive previously unknown semantics. These include
randomized response, FRAPP, and several algorithms that add integer-valued
noise to their inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5451</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5451</id><created>2012-08-27</created><authors><author><keyname>Tang</keyname><forenames>Zhongwei</forenames></author><author><keyname>Castrodad</keyname><forenames>Alexey</forenames></author><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity
  Analysis from a Single Video</title><categories>cs.CV</categories><msc-class>94A08</msc-class><acm-class>I.4.7; I.5.3; I.6.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework for unsupervised group activity analysis from a single video is
here presented. Our working hypothesis is that human actions lie on a union of
low-dimensional subspaces, and thus can be efficiently modeled as sparse linear
combinations of atoms from a learned dictionary representing the action's
primitives. Contrary to prior art, and with the primary goal of spatio-temporal
action grouping, in this work only one single video segment is available for
both unsupervised learning and analysis without any prior training information.
After extracting simple features at a single spatio-temporal scale, we learn a
dictionary for each individual in the video during each short time lapse. These
dictionaries allow us to compare the individuals' actions by producing an
affinity matrix which contains sufficient discriminative information about the
actions in the scene leading to grouping with simple and efficient tools. With
diverse publicly available real videos, we demonstrate the effectiveness of the
proposed framework and its robustness to cluttered backgrounds, changes of
human appearance, and action variability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5464</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5464</id><created>2012-08-27</created><authors><author><keyname>Sidiropoulos</keyname><forenames>Antonis</forenames></author></authors><title>Finding Communities in Site Web-Graphs and Citation Graphs</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web is a typical example of a social network. One of the most intriguing
features of the Web is its self-organization behavior, which is usually faced
through the existence of communities. The discovery of the communities in a
Web-graph can be used to improve the effectiveness of search engines, for
purposes of prefetching, bibliographic citation ranking, spam detection,
creation of road-maps and site graphs, etc. Correspondingly, a citation graph
is also a social network which consists of communities. The identification of
communities in citation graphs can enhance the bibliography search as well as
the data-mining. In this paper we will present a fast algorithm which can
identify the communities over a given unweighted/undirected graph. This graph
may represent a Web-graph or a citation graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5471</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5471</id><created>2012-08-27</created><authors><author><keyname>Gol</keyname><forenames>Ebru Aydin</forenames></author><author><keyname>Ding</keyname><forenames>Xuchu</forenames></author><author><keyname>Lazar</keyname><forenames>Mircea</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Finite Bisimulations for Switched Linear Systems</title><categories>math.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of constructing a finite bisimulation
quotient for a discrete-time switched linear system in a bounded subset of its
state space. Given a set of observations over polytopic subsets of the state
space and a switched linear system with stable subsystems, the proposed
algorithm generates the bisimulation quotient in a finite number of steps with
the aid of sublevel sets of a polyhedral Lyapunov function. Starting from a
sublevel set that includes the origin in its interior, the proposed algorithm
iteratively constructs the bisimulation quotient for any larger sublevel set.
The bisimulation quotient can then be further used for synthesis of the
switching law and system verification with respect to specifications given as
syntactically co-safe Linear Temporal Logic formulas over the observed
polytopic subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5526</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5526</id><created>2012-08-27</created><authors><author><keyname>Avci</keyname><forenames>Serhat Nazim</forenames></author><author><keyname>Ayanoglu</keyname><forenames>and Ender</forenames></author></authors><title>Coded Path Protection Part 1: Efficient Conversion of Sharing to Coding</title><categories>cs.NI</categories><comments>This is the Part 1 of a two part paper. Simpler version of these two
  papers are published in the proceedings of IEEE ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link failures in wide area networks are common and cause significant data
losses. Mesh-based protection schemes offer high capacity efficiency but they
are slow and require complex signaling. Additionally, real-time
reconfigurations of cross-connects threaten their transmission integrity. On
the other hand, there are other schemes that are proactive. Proactivity results
in higher restoration speed, lower signaling complexity, and higher
transmission integrity. This paper introduces a coding-based proactive
protection scheme, named Coded Path Protection (CPP). In CPP, a backup stream
of the primary data is encoded with other data streams, resulting in capacity
savings. In addition to a systematic approach of building valid coding
structures, this paper presents an optimal and simple capacity placement and
coding group formation algorithm. The algorithm converts the sharing structure
of any solution of a Shared Path Protection (SPP) technique into a coding
structure with minimum extra capacity. We conducted quantitative and
qualitative comparisons of our technique with the SPP. Simulation results
confirm that CPP provides faster link failure recovery than SPP while it incurs
marginal extra capacity beyond that of SPP. In this Part 1 of the paper, we
describe the theory and an algorithm for converting a given SPP solution into a
CPP solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5528</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5528</id><created>2012-08-27</created><authors><author><keyname>Avci</keyname><forenames>Serhat Nazim</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Coded Path Protection Part 2: Design, Implementation, and Performance</title><categories>cs.NI</categories><comments>This is the Part 2 of a two part paper. A simple version of these two
  papers is published in the proceedings of IEEE ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part 1 of this paper, we introduced a coding-based proactive network
protection scheme, named Coded Path Protection (CPP). In CPP, a backup stream
of the primary data is encoded with other data streams, resulting in capacity
savings. In addition to being a systematic approach of building valid coding
structures, CPP is an optimal and simple capacity placement and coding group
formation algorithm. It converts the sharing structure of any solution of a
Shared Path Protection (SPP) technique into a coding structure with minimum
extra capacity. In this Part 2 of the paper, we describe the implementation of
our algorithm using Integer Linear Programming (ILP), its timing and
synchronization requirements, and implementation issues in networks. We present
simulation results which confirm that CPP provides faster link failure recovery
than SPP while it incurs marginal extra capacity beyond that of SPP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5537</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5537</id><created>2012-08-27</created><authors><author><keyname>Boidot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author></authors><title>Planning Random path distributions for ambush games in unstructured
  environments</title><categories>cs.RO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating vehicles in adversarial environments require non-conventional
planning techniques. A two-player, zero-sum non-cooperative game is introduced,
which is solved via a linear program. An extension is proposed to construct
networks displaying good representations of the environment characteristics,
while offering acceptable results for the technique used. Sensitivity of the
solution to the LP solver algorithm is identified. The performances of the
planner are finally assessed by comparison with those of conventional planners.
Results are used to formulate secondary objectives to the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5542</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5542</id><created>2012-08-27</created><authors><author><keyname>Lv</keyname><forenames>Huiwei</forenames></author><author><keyname>Tan</keyname><forenames>Guangming</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Sun</keyname><forenames>Ninghui</forenames></author></authors><title>Compression and Sieve: Reducing Communication in Parallel Breadth First
  Search on Distributed Memory Systems</title><categories>cs.DC cs.DS</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For parallel breadth first search (BFS) algorithm on large-scale distributed
memory systems, communication often costs significantly more than arithmetic
and limits the scalability of the algorithm. In this paper we sufficiently
reduce the communication cost in distributed BFS by compressing and sieving the
messages. First, we leverage a bitmap compression algorithm to reduce the size
of messages before communication. Second, we propose a novel distributed
directory algorithm, cross directory, to sieve the redundant data in messages.
Experiments on a 6,144-core SMP cluster show our algorithm outperforms the
baseline implementation in Graph500 by 2.2 times, reduces its communication
time by 79.0%, and achieves a performance rate of 12.1 GTEPS (billion edge
visits per second)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5554</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5554</id><created>2012-08-28</created><authors><author><keyname>Czibula</keyname><forenames>Gabriela</forenames></author><author><keyname>Crisan</keyname><forenames>Gloria Cerasela</forenames></author><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Czibula</keyname><forenames>Istvan-Gergely</forenames></author></authors><title>Soft Computing approaches on the Bandwidth Problem</title><categories>cs.AI</categories><comments>6 pages, 1 figure; accepted to Informatica</comments><journal-ref>INFORMATICA 24(2) (2013) 169-180</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous
reordering of the rows and the columns of a square matrix such that the nonzero
entries are collected within a band of small width close to the main diagonal.
The MBMP is a NP-complete problem, with applications in many scientific
domains, linear systems, artificial intelligence, and real-life situations in
industry, logistics, information recovery. The complex problems are hard to
solve, that is why any attempt to improve their solutions is beneficent.
Genetic algorithms and ant-based systems are Soft Computing methods used in
this paper in order to solve some MBMP instances. Our approach is based on a
learning agent-based model involving a local search procedure. The algorithm is
compared with the classical Cuthill-McKee algorithm, and with a hybrid genetic
algorithm, using several instances from Matrix Market collection. Computational
experiments confirm a good performance of the proposed algorithms for the
considered set of MBMP instances. On Soft Computing basis, we also propose a
new theoretical Reinforcement Learning model for solving the MBMP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5556</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5556</id><created>2012-08-28</created><authors><author><keyname>Pour</keyname><forenames>Alireza Nemaney</forenames></author><author><keyname>Kholghi</keyname><forenames>Raheleh</forenames></author><author><keyname>Roudsari</keyname><forenames>Soheil Behnam</forenames></author></authors><title>Minimizing the Time of Spam Mail Detection by Relocating Filtering
  System to the Sender Mail Server</title><categories>cs.IR cs.NI</categories><comments>10 pages, 7 figures</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.4, No.2, March 2012</journal-ref><doi>10.5121/ijnsa.2012.4204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsolicited Bulk Emails (also known as Spam) are undesirable emails sent to
massive number of users. Spam emails consume the network resources and cause
lots of security uncertainties. As we studied, the location where the spam
filter operates in is an important parameter to preserve network resources.
Although there are many different methods to block spam emails, most of program
developers only intend to block spam emails from being delivered to their
clients. In this paper, we will introduce a new and efficient approach to
prevent spam emails from being transferred. The result shows that if we focus
on developing a filtering method for spams emails in the sender mail server
rather than the receiver mail server, we can detect the spam emails in the
shortest time consequently to avoid wasting network resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5557</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5557</id><created>2012-08-28</created><authors><author><keyname>Eidkhani</keyname><forenames>Elina</forenames></author><author><keyname>Hajyvahabzadeh</keyname><forenames>Melisa</forenames></author><author><keyname>Mortazavi</keyname><forenames>S. Anahita</forenames></author><author><keyname>Pour</keyname><forenames>Alireza Nemaney</forenames></author></authors><title>CRAW: Combination of Re-Keying and Authentication in Wireless Networks
  for Secure Multicast Increasing Efficiency of Member Join/Leave and Movement</title><categories>cs.CR</categories><comments>22 pages, 16 figures, 7 tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.4, July 2012</journal-ref><doi>10.5121/ijcnc.2012.4407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the number of requests for multicast services through the wireless
networks has been increased. However, for successful deployment, security and
efficiency of content delivery must be provided at first. This paper presents a
new approach for secure multicast in wireless networks. This approach, CRAW
(Combination of Re-keying and Authentication in Wireless networks) combines
member authentication procedure with group key management protocol to provide
an efficient group re-keying process. One-time password is proposed for member
authentication and CKC (Code for Key Calculation) is suggested for group key
management in wireless networks. In fact, the combination of authentication
with group key management in wireless networks results in a simple and secure
mechanism both for authentication and group key management while mobile members
join/leave a group or move inter-area. Simulation results show that CRAW
reduces re-keying overhead at join from O(log2 n+1) to O(1) while security
requirements are saved. Also, CRAW introduces storing a main list to manage
mobile members' location while they move intra-group inter-area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5558</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5558</id><created>2012-08-28</created><authors><author><keyname>Hajyvahabzadeh</keyname><forenames>Melisa</forenames></author><author><keyname>Eidkhani</keyname><forenames>Elina</forenames></author><author><keyname>Mortazavi</keyname><forenames>S. Anahita</forenames></author><author><keyname>Pour</keyname><forenames>Alireza Nemaney</forenames></author></authors><title>An Efficient Group Key Management Using Code for Key Calculation for
  Simultaneous Join/Leave: CKCS</title><categories>cs.CR</categories><comments>18 pages, 16 figures, 4 tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.4, July 2012</journal-ref><doi>10.5121/ijcnc.2012.4417</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient group key management protocol, CKCS (Code
for Key Calculation in Simultaneous join/leave) for simultaneous join/leave in
secure multicast. This protocol is based on logical key hierarchy. In this
protocol, when new members join the group simultaneously, server sends only the
group key for those new members. Then, current members and new members
calculate the necessary keys by node codes and one-way hash function. A node
code is a random number which is assigned to each key to help users calculate
the necessary keys. Again, at leave, the server just sends the new group key to
remaining members. The results show that CKCS reduces computational and
communication overhead, and also message size in simultaneous join/leave.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5569</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5569</id><created>2012-08-28</created><authors><author><keyname>Doomun</keyname><forenames>M. Razvi</forenames></author><author><keyname>Soyjaudah</keyname><forenames>K. M. Sunjiv</forenames></author></authors><title>Route Extrapolation for Source and Destination Camouflage in Wireless Ad
  Hoc Networks</title><categories>cs.NI cs.CR</categories><comments>7 pages, 19th International Conference on Computer Communications and
  Networks (ICCCN 2010), Zurich</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In wireless ad hoc networks, protecting source and destination nodes location
privacy is a challenging task due to malicious traffic analysis and privacy
attacks. Existing solutions, such as incorporating fake source destination
pairs in the network, provide some privacy of real source and destination nodes
against attackers. Moreover, ad hoc networks need stronger privacy protection
against powerful global attacker which has knowledge of overall network
topology and, that can also eavesdrop and visualize network wide data
transmissions. In this paper, we present a novel privacy technique, (EXTROUT)
Route Extrapolation to camouflage the real source and destination nodes along
an extended path in an ad hoc network. We demonstrate that the privacy level
achieved with EXTROUT is higher and more effective against a global attacker,
when compared to fake source destination nodes privacy scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5571</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5571</id><created>2012-08-28</created><authors><author><keyname>Doomun</keyname><forenames>M. Razvi</forenames></author><author><keyname>Soyjaudah</keyname><forenames>KM Sunjiv</forenames></author></authors><title>Modified Temporal Key Integrity Protocol For Efficient Wireless Network
  Security</title><categories>cs.CR</categories><comments>6 pages, International Conference on Security and Cryptography
  (SECRYPT) 2007</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Temporal Key Integrity Protocol (TKIP) is the IEEE TaskGroupi solution for
the security loop holes present in the already widely deployed 802.11 hardware.
It is a set of algorithms that wrap WEP to give the best possible solution
given design constraints such as paucity of the CPU cycles, hardwiring of the
WEP encryption algorithm and software upgrade dependent. Thus, TKIP is
significantly more difficult and challenging to implement and optimise than
WEP. The objective of this research is to examine the cost and benefit of TKIP
security mechanisms and optimise its implementation to reduce security overhead
for better performance. We propose a modified TKIP (MoTKIP) with improved
packet encapsulation and decapsulation procedure that reduces computation and
packet overhead in classic TKIP substantially and optimises total wireless
network throughput rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5580</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5580</id><created>2012-08-28</created><updated>2013-06-14</updated><authors><author><keyname>Eggert</keyname><forenames>Sebastian</forenames></author><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author><author><keyname>Wilke</keyname><forenames>Thomas</forenames></author></authors><title>Noninterference with Local Policies</title><categories>cs.CR</categories><comments>27 pages</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a theory for state-based noninterference in a setting where
different security policies---we call them local policies---apply in different
parts of a given system. Our theory comprises appropriate security definitions,
characterizations of these definitions, for instance in terms of unwindings,
algorithms for analyzing the security of systems with local policies, and
corresponding complexity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5589</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5589</id><created>2012-08-28</created><updated>2013-12-27</updated><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author></authors><title>Hitting all Maximal Independent Sets of a Bipartite Graph</title><categories>cs.CC cs.DS</categories><comments>v3: minor change</comments><journal-ref>Algorithmica, 72/2:359--368, 2015</journal-ref><doi>10.1007/s00453-013-9847-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that given a bipartite graph G with vertex set V and an integer k,
deciding whether there exists a subset of V of size k hitting all maximal
independent sets of G is complete for the class Sigma_2^P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5604</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5604</id><created>2012-08-28</created><updated>2012-08-31</updated><authors><author><keyname>Smarra</keyname><forenames>F.</forenames></author><author><keyname>D'Innocenzo</keyname><forenames>A.</forenames></author><author><keyname>Di Benedetto</keyname><forenames>M. D.</forenames></author></authors><title>Optimal co-design of control, scheduling and routing in multi-hop
  control networks</title><categories>math.OC cs.SY</categories><comments>51st IEEE Conference on Decision and Control, 2012. Accepted for
  publication as regular paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Multi-hop Control Network consists of a plant where the communication
between sensors, actuators and computational units is supported by a (wireless)
multi-hop communication network, and data flow is performed using scheduling
and routing of sensing and actuation data. Given a SISO LTI plant, we will
address the problem of co-designing a digital controller and the network
parameters (scheduling and routing) in order to guarantee stability and
maximize a performance metric on the transient response to a step input, with
constraints on the control effort, on the output overshoot and on the bandwidth
of the communication channel. We show that the above optimization problem is a
polynomial optimization problem, which is generally NP-hard. We provide
sufficient conditions on the network topology, scheduling and routing such that
it is computationally feasible, namely such that it reduces to a convex
optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5616</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5616</id><created>2012-08-28</created><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author></authors><title>Cooperative Cognitive Relaying with Ordered Cognitive Multiple Access</title><categories>cs.NI cs.IT math.IT</categories><comments>in Globecom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a cognitive radio system with two secondary users who can
cooperate with the primary user in relaying its packets to the primary
receiver. In addition to its own queue, each secondary user has a queue to keep
the primary packets that are not received correctly by the primary receiver.
The secondary users accept the unreceived primary packets with a certain
probability and transmit randomly from either of their queues if both are
nonempty. These probabilities are optimized to expand the maximum stable
throughput region of the system. Moreover, we suggest a secondary multiple
access scheme in which one secondary user senses the channel for $\tau$ seconds
from the beginning of the time slot and transmits if the channel is found to be
free. The other secondary user senses the channel over the period $[0,2\tau]$
to detect the possible activity of the primary user and the first-ranked
secondary user. It transmits, if possible, starting after $2\tau$ seconds from
the beginning of the time slot. It compensates for the delayed transmission by
increasing its transmission rate so that it still transmits one packet during
the time slot. We show the potential advantage of this ordered system over the
conventional random access system. We also show the benefit of cooperation in
enhancing the network's throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5620</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5620</id><created>2012-08-28</created><updated>2013-01-30</updated><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Liba</keyname><forenames>Omri</forenames></author><author><keyname>Schiller</keyname><forenames>Elad M.</forenames></author></authors><title>Self-Stabilizing Byzantine Resilient Topology Discovery and Message
  Delivery</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Byzantine resilient algorithms use 2f+1 vertex disjoint paths to
ensure message delivery in the presence of up to f Byzantine nodes. The
question of how these paths are identified is related to the fundamental
problem of topology discovery. Distributed algorithms for topology discovery
cope with a never ending task, dealing with frequent changes in the network
topology and unpredictable transient faults. Therefore, algorithms for topology
discovery should be self-stabilizing to ensure convergence of the topology
information following any such unpredictable sequence of events. We present the
first such algorithm that can cope with Byzantine nodes. Starting in an
arbitrary global state, and in the presence of f Byzantine nodes, each node is
eventually aware of all the other non-Byzantine nodes and their connecting
communication links. Using the topology information, nodes can, for example,
route messages across the network and deliver messages from one end user to
another. We present the first deterministic, cryptographicassumptions- free,
self-stabilizing, Byzantine-resilient algorithms for network topology discovery
and end-to-end message delivery. We also consider the task of r-neighborhood
discovery for the case in which r and the degree of nodes are bounded by
constants. The use of r-neighborhood discovery facilitates polynomial time,
communication and space solutions for the above tasks. The obtained algorithms
can be used to authenticate parties, in particular during the establishment of
private secrets, thus forming public key schemes that are resistant to
man-in-the-middle attacks of the compromised Byzantine nodes. A polynomial and
efficient end-to-end algorithm that is based on the established private secrets
can be employed in between periodical re-establishments of the secrets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5639</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5639</id><created>2012-08-28</created><authors><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author><author><keyname>Rozenblit</keyname><forenames>Michal</forenames></author></authors><title>Convex Integer Optimization by Constantly Many Linear Counterparts</title><categories>math.CO cs.DM cs.DS math.OC</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><journal-ref>Linear Algebra and its Applications 447 (2014) 88-109</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study convex integer maximization problems with composite
objective functions of the form $f(Wx)$, where $f$ is a convex function on
$\R^d$ and $W$ is a $d\times n$ matrix with small or binary entries, over
finite sets $S\subset \Z^n$ of integer points presented by an oracle or by
linear inequalities.
  Continuing the line of research advanced by Uri Rothblum and his colleagues
on edge-directions, we introduce here the notion of {\em edge complexity} of
$S$, and use it to establish polynomial and constant upper bounds on the number
of vertices of the projection $\conv(WS)$ and on the number of linear
optimization counterparts needed to solve the above convex problem.
  Two typical consequences are the following. First, for any $d$, there is a
constant $m(d)$ such that the maximum number of vertices of the projection of
any matroid $S\subset\{0,1\}^n$ by any binary $d\times n$ matrix $W$ is $m(d)$
regardless of $n$ and $S$; and the convex matroid problem reduces to $m(d)$
greedily solvable linear counterparts. In particular, $m(2)=8$. Second, for any
$d,l,m$, there is a constant $t(d;l,m)$ such that the maximum number of
vertices of the projection of any three-index $l\times m\times n$
transportation polytope for any $n$ by any binary $d\times(l\times m\times n)$
matrix $W$ is $t(d;l,m)$; and the convex three-index transportation problem
reduces to $t(d;l,m)$ linear counterparts solvable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5641</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5641</id><created>2012-08-28</created><updated>2013-07-29</updated><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author><author><keyname>Mitrokotsa</keyname><forenames>Aikaterini</forenames></author></authors><title>Near-Optimal Blacklisting</title><categories>cs.CR stat.AP</categories><comments>Submitted to INFOCOM 2014, 10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications involve agents sharing a resource, such as networks or
services. When agents are honest, the system functions well and there is a net
profit. Unfortunately, some agents may be malicious, but it may be hard to
detect them. We consider the intrusion response problem of how to permanently
blacklist agents, in order to maximise expected profit. This is not trivial, as
blacklisting may erroneously expel honest agents. Conversely, while we gain
information by allowing an agent to remain, we may incur a cost due to
malicious behaviour. We present an efficient algorithm (HIPER) for making
near-optimal decisions for this problem. Additionally, we derive three
algorithms by reducing the problem to a Markov decision process (MDP).
Theoretically, we show that HIPER is near-optimal. Experimentally, its
performance is close to that of the full MDP solution, when the (stronger)
requirements of the latter are met.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5649</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5649</id><created>2012-08-28</created><authors><author><keyname>Churbanov</keyname><forenames>A.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>P.</forenames></author></authors><title>Numerical Methods for Solving Convection-Diffusion Problems</title><categories>cs.NA math.NA</categories><msc-class>65M06, 65M08, 76M12, 76M20, 76S05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convection-diffusion equations provide the basis for describing heat and mass
transfer phenomena as well as processes of continuum mechanics. To handle flows
in porous media, the fundamental issue is to model correctly the convective
transport of individual phases. Moreover, for compressible media, the pressure
equation itself is just a time-dependent convection-diffusion equation.
  For different problems, a convection-diffusion equation may be be written in
various forms. The most popular formulation of convective transport employs the
divergent (conservative) form. In some cases, the nondivergent (characteristic)
form seems to be preferable. The so-called skew-symmetric form of convective
transport operators that is the half-sum of the operators in the divergent and
nondivergent forms is of great interest in some applications.
  Here we discuss the basic classes of discretization in space: finite
difference schemes on rectangular grids, approximations on general polyhedra
(the finite volume method), and finite element procedures. The key properties
of discrete operators are studied for convective and diffusive transport. We
emphasize the problems of constructing approximations for convection and
diffusion operators that satisfy the maximum principle at the discrete level
--- they are called monotone approximations.
  Two- and three-level schemes are investigated for transient problems.
Unconditionally stable explicit-implicit schemes are developed for
convection-diffusion problems. Stability conditions are obtained both in
finite-dimensional Hilbert spaces and in Banach spaces depending on the form in
which the convection-diffusion equation is written.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5654</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5654</id><created>2012-08-28</created><updated>2012-08-29</updated><authors><author><keyname>De Vries</keyname><forenames>Christopher M.</forenames></author><author><keyname>Geva</keyname><forenames>Shlomo</forenames></author><author><keyname>Trotman</keyname><forenames>Andrew</forenames></author></authors><title>Document Clustering Evaluation: Divergence from a Random Baseline</title><categories>cs.IR cs.AI</categories><comments>8 pages, 11 figures, WIR2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Divergence from a random baseline is a technique for the evaluation of
document clustering. It ensures cluster quality measures are performing work
that prevents ineffective clusterings from giving high scores to clusterings
that provide no useful result. These concepts are defined and analysed using
intrinsic and extrinsic approaches to the evaluation of document cluster
quality. This includes the classical clusters to categories approach and a
novel approach that uses ad hoc information retrieval. The divergence from a
random baseline approach is able to differentiate ineffective clusterings
encountered in the INEX XML Mining track. It also appears to perform a
normalisation similar to the Normalised Mutual Information (NMI) measure but it
can be applied to any measure of cluster quality. When it is applied to the
intrinsic measure of distortion as measured by RMSE, subtraction from a random
baseline provides a clear optimum that is not apparent otherwise. This approach
can be applied to any clustering evaluation. This paper describes its use in
the context of document clustering evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5659</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5659</id><created>2012-08-28</created><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author></authors><title>Optimal Random Access and Random Spectrum Sensing for an Energy
  Harvesting Cognitive Radio</title><categories>cs.NI cs.IT math.IT</categories><comments>in WiMob 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a secondary user with energy harvesting capability. We design
access schemes for the secondary user which incorporate random spectrum sensing
and random access, and which make use of the primary automatic repeat request
(ARQ) feedback. The sensing and access probabilities are obtained such that the
secondary throughput is maximized under the constraints that both the primary
and secondary queues are stable and that the primary queueing delay is kept
lower than a specified value needed to guarantee a certain quality of service
(QoS) for the primary user. We consider spectrum sensing errors and assume
multipacket reception (MPR) capabilities. Numerical results are presented to
show the enhanced performance of our proposed system over a random access
system, and to demonstrate the benefit of leveraging the primary feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5700</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5700</id><created>2012-08-28</created><authors><author><keyname>Wang</keyname><forenames>Qingsi</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>Dynamic Pricing of Power in Smart-Grid Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the problem of dynamic pricing of power for
smart-grid networks. This is studied within a network utility maximization
(NUM) framework in a deterministic setting with a single provider, multiple
users and a finite horizon. The provider produces power or buys power in a
(deterministic) spot market, and determines a dynamic price to charge the
users. The users then adjust their demand in response to the time-varying
prices. This is typically categorized as the demand response problem, and we
study a progression of related models by focusing on two aspects: 1) the
characterization of the structure of the optimal dynamic prices in the Smart
Grid and the optimal demand and supply under various interaction with a spot
market; 2) a greedy approach to facilitate the solution process of the
aggregate NUM problem and the optimality gap between the greedy solution and
the optimal one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5703</identifier>
 <datestamp>2013-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5703</id><created>2012-08-28</created><updated>2013-08-19</updated><authors><author><keyname>Mallada</keyname><forenames>Enrique</forenames></author><author><keyname>Meng</keyname><forenames>Xiaoqiao</forenames></author><author><keyname>Hack</keyname><forenames>Michel</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Tang</keyname><forenames>Ao</forenames></author></authors><title>Skewless Network Clock Synchronization</title><categories>math.OC cs.SY</categories><comments>to appear in ICNP13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines synchronization of computer clocks connected via a data
network and proposes a skewless algorithm to synchronize them. Unlike existing
solutions, which either estimate and compensate the frequency difference (skew)
among clocks or introduce offset corrections that can generate jitter and
possibly even backward jumps, our algorithm achieves synchronization without
these problems. We first analyze the convergence property of the algorithm and
provide necessary and sufficient conditions on the parameters to guarantee
synchronization. We then implement our solution on a cluster of IBM BladeCenter
servers running Linux and study its performance. In particular, both
analytically and experimentally, we show that our algorithm can converge in the
presence of timing loops. This marks a clear contrast with current standards
such as NTP and PTP, where timing loops are specifically avoided. Furthermore,
timing loops can even be beneficial in our scheme. For example, it is
demonstrated that highly connected subnetworks can collectively outperform
individual clients when the time source has large jitter. It is also
experimentally demonstrated that our algorithm outperforms other
well-established software-based solutions such as the NTPv4 and IBM Coordinated
Cluster Time (IBM CCT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5713</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5713</id><created>2012-08-28</created><authors><author><keyname>Hosangadi</keyname><forenames>Sandeep</forenames></author></authors><title>Distance Measures for Sequences</title><categories>cs.IT cs.DS math.IT</categories><comments>16 PAGES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of sequences, the distance between pairs of them helps us to find
their similarity and derive structural relationship amongst them. For genomic
sequences such measures make it possible to construct the evolution tree of
organisms. In this paper we compare several distance measures and examine a
method that involves circular shifting one sequence against the other for
finding good alignment to minimize Hamming distance. We also use run-length
encoding together with LZ77 to characterize information in a binary sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5738</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5738</id><created>2012-08-02</created><authors><author><keyname>Xu</keyname><forenames>Xiaohua</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author></authors><title>Efficient Construction of Dominating Set in Wireless Networks</title><categories>cs.NI cs.DS</categories><comments>8 pages, 4 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Considering a communication topology of a wireless network modeled by a graph
where an edge exists between two nodes if they are within each other's
communication range. A subset $U$ of nodes is a dominating set if each node is
either in $U$ or adjacent to some node in $U$. Assume each node has a disparate
communication range and is associated with a positive weight, we present a
randomized algorithm to find a min-weight dominating set. Considering any
orientation of the graph where an arc $\overrightarrow{uv}$ exists if the node
$v$ lies in $u$'s communication range. A subset $U$ of nodes is a strongly
dominating set if every node except $U$ has both in-neighbor(s) and
out-neighbor(s) in $U$. We present a polynomial-time algorithm to find a
strongly dominating set of size at most $(2+\epsilon)$ times of the optimum. We
also investigate another related problem called $K$-Coverage. Given are a set
${\cal D}$ of disks with positive weight and a set ${\cal P}$ of nodes. Assume
all input nodes lie below a horizontal line $l$ and all input disks lie above
this line $l$ in the plane. The objective is to find a min-weight subset ${\cal
D}'\subseteq {\cal D}$ of disks such that each node is covered at least $K$
disks in ${\cal D}'$. We propose a novel two-approximation algorithm for this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5740</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5740</id><created>2012-08-28</created><authors><author><keyname>Zaman</keyname><forenames>J K M Sadique Uz</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>A Review Study of NIST Statistical Test Suite: Development of an
  indigenous Computer Package</title><categories>cs.CR</categories><comments>24 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A review study of NIST Statistical Test Suite is undertaken with a motivation
to understand all its test algorithms and to write their C codes independently
without looking at various sites mentioned in the NIST document. All the codes
are tested with the test data given in the NIST document and excellent
agreements have been found. The codes have been put together in a package
executable in MS Windows platform. Based on the package, exhaustive test runs
are executed on three PRNGs, e.g. LCG by Park &amp; Miller, LCG by Knuth and BBSG.
Our findings support the present belief that BBSG is a better PRNG than the
other two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5745</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5745</id><created>2012-08-28</created><authors><author><keyname>Raghunathan</keyname><forenames>Rohit</forenames></author><author><keyname>De</keyname><forenames>Sushovan</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Bayes Networks for Supporting Query Processing Over Incomplete
  Autonomous Databases</title><categories>cs.DB</categories><comments>22 pages, 8 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the information available to lay users through autonomous data sources
continues to increase, mediators become important to ensure that the wealth of
information available is tapped effectively. A key challenge that these
information mediators need to handle is the varying levels of incompleteness in
the underlying databases in terms of missing attribute values. Existing
approaches such as QPIAD aim to mine and use Approximate Functional
Dependencies (AFDs) to predict and retrieve relevant incomplete tuples. These
approaches make independence assumptions about missing values---which
critically hobbles their performance when there are tuples containing missing
values for multiple correlated attributes. In this paper, we present a
principled probabilistic alternative that views an incomplete tuple as defining
a distribution over the complete tuples that it stands for. We learn this
distribution in terms of Bayes networks. Our approach involves
mining/&quot;learning&quot; Bayes networks from a sample of the database, and using it to
do both imputation (predict a missing value) and query rewriting (retrieve
relevant results with incompleteness on the query-constrained attributes, when
the data sources are autonomous). We present empirical studies to demonstrate
that (i) at higher levels of incompleteness, when multiple attribute values are
missing, Bayes networks do provide a significantly higher classification
accuracy and (ii) the relevant possible answers retrieved by the queries
reformulated using Bayes networks provide higher precision and recall than AFDs
while keeping query processing costs manageable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5746</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5746</id><created>2012-08-28</created><authors><author><keyname>Ella</keyname><forenames>Vaignana Spoorthy</forenames></author></authors><title>Sequence Randomization Using Convolutional Codes and Probability
  Functions</title><categories>cs.CR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the use of different transformations for improving
the randomness of sequences. In particular, convolutional codes are used for
increasing the size of a given sequence and then a random mapping function is
used for further randomization. We have shown how such a method can convert
highly correlated sequences into random ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5752</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5752</id><created>2012-08-28</created><authors><author><keyname>Phillips</keyname><forenames>Carolyn L.</forenames></author><author><keyname>Anderson</keyname><forenames>Joshua A.</forenames></author><author><keyname>Chen</keyname><forenames>Elizabeth R.</forenames></author><author><keyname>Glotzer</keyname><forenames>Sharon C.</forenames></author></authors><title>Optimal Fillings - A new spatial subdivision problem related to packing
  and covering</title><categories>math.OC cond-mat.soft cs.CG</categories><comments>38 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present filling as a new type of spatial subdivision problem that is
related to covering and packing. Filling addresses the optimal placement of
overlapping objects lying entirely inside an arbitrary shape so as to cover the
most interior volume. In n-dimensional space, if the objects are polydisperse
n-balls, we show that solutions correspond to sets of maximal n-balls and the
solution space can reduced to the medial axis of a shape. We examine the
structure of the solution space in two dimensions. For the filling of polygons,
we provide detailed descriptions of a heuristic and a genetic algorithm for
finding solutions of maximal discs. We also consider the properties of ideal
distributions of N discs in polygons as N approaches infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5801</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5801</id><created>2012-08-28</created><updated>2012-08-31</updated><authors><author><keyname>Ferreira</keyname><forenames>Nivan</forenames></author><author><keyname>Klosowski</keyname><forenames>James T.</forenames></author><author><keyname>Scheidegger</keyname><forenames>Carlos</forenames></author><author><keyname>Silva</keyname><forenames>Claudio</forenames></author></authors><title>Vector Field k-Means: Clustering Trajectories by Fitting Multiple Vector
  Fields</title><categories>cs.LG</categories><comments>30 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientists study trajectory data to understand trends in movement patterns,
such as human mobility for traffic analysis and urban planning. There is a
pressing need for scalable and efficient techniques for analyzing this data and
discovering the underlying patterns. In this paper, we introduce a novel
technique which we call vector-field $k$-means.
  The central idea of our approach is to use vector fields to induce a
similarity notion between trajectories. Other clustering algorithms seek a
representative trajectory that best describes each cluster, much like $k$-means
identifies a representative &quot;center&quot; for each cluster. Vector-field $k$-means,
on the other hand, recognizes that in all but the simplest examples, no single
trajectory adequately describes a cluster. Our approach is based on the premise
that movement trends in trajectory data can be modeled as flows within multiple
vector fields, and the vector field itself is what defines each of the
clusters. We also show how vector-field $k$-means connects techniques for
scalar field design on meshes and $k$-means clustering.
  We present an algorithm that finds a locally optimal clustering of
trajectories into vector fields, and demonstrate how vector-field $k$-means can
be used to mine patterns from trajectory data. We present experimental evidence
of its effectiveness and efficiency using several datasets, including
historical hurricane data, GPS tracks of people and vehicles, and anonymous
call records from a large phone company. We compare our results to previous
trajectory clustering techniques, and find that our algorithm performs faster
in practice than the current state-of-the-art in trajectory clustering, in some
examples by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5814</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5814</id><created>2012-08-28</created><updated>2013-07-05</updated><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>Minimum Complexity Pursuit for Universal Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nascent field of compressed sensing is founded on the fact that
high-dimensional signals with &quot;simple structure&quot; can be recovered accurately
from just a small number of randomized samples. Several specific kinds of
structures have been explored in the literature, from sparsity and group
sparsity to low-rankness. However, two fundamental questions have been left
unanswered, namely: What are the general abstract meanings of &quot;structure&quot; and
&quot;simplicity&quot;? And do there exist universal algorithms for recovering such
simple structured objects from fewer samples than their ambient dimension? In
this paper, we address these two questions. Using algorithmic information
theory tools such as the Kolmogorov complexity, we provide a unified definition
of structure and simplicity. Leveraging this new definition, we develop and
analyze an abstract algorithm for signal recovery motivated by Occam's
Razor.Minimum complexity pursuit (MCP) requires just O(3\kappa) randomized
samples to recover a signal of complexity \kappa and ambient dimension n. We
also discuss the performance of MCP in the presence of measurement noise and
with approximately simple signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5842</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5842</id><created>2012-08-29</created><updated>2014-06-15</updated><authors><author><keyname>Larkin</keyname><forenames>Kieran G.</forenames></author><author><keyname>Fletcher</keyname><forenames>Peter A.</forenames></author><author><keyname>Hardy</keyname><forenames>Stephen J.</forenames></author></authors><title>Tenacious tagging of images via Mellin monomials</title><categories>cs.CV math.CA</categories><comments>Previous MS rejected as the journal no longer accepts purely image
  processing articles. Reworked with emphasis on the embedded patterns: Mellin
  monomials, which underpin optical rotation and scale invariant pattern
  recognition. Noted visual imperceptibility. Kept animation sequence showing
  how correlation detection survives image distortions. Submitted to an &quot;optics
  and image science&quot; journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for attaching persistent metadata to an image. The
method can be interpreted as a template-based blind watermarking scheme, robust
to common editing operations, namely: cropping, rotation, scaling, stretching,
shearing, compression, printing, scanning, noise, and color removal. Robustness
is achieved through the reciprocity of the embedding and detection invariants.
The embedded patterns are real onedimensional Mellin monomial patterns
distributed over two-dimensions. The embedded patterns are scale invariant and
can be directly embedded in an image by simple pixel addition. Detection
achieves rotation and general affine invariance by signal projection using
implicit Radon transformation. Embedded signals contract to one-dimension in
the two-dimensional Fourier polar domain. The real signals are detected by
correlation with complex Mellin monomial templates. Using a unique template of
4 chirp patterns we detect the affine signature with exquisite sensitivity and
moderate security. The practical implementation achieves efficiencies through
fast Fourier transform (FFT) correspondences such as the projection-slice
theorem, the FFT correlation relation, and fast resampling via the chirp-z
transform. The overall method utilizes orthodox spread spectrum patterns for
the payload and performs well in terms of the classic
robustness-capacity-visibility performance triangle. Tags are entirely
imperceptible with a mean SSIM greater than 0.988 in all cases tested.
Watermarked images survive almost all Stirmark attacks. The method is ideal for
attaching metadata robustly to both digital and analogue images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5848</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5848</id><created>2012-08-29</created><updated>2012-09-25</updated><authors><author><keyname>Doshi</keyname><forenames>Nishant</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author></authors><title>Updating attribute in CP-ABE: A New Approach</title><categories>cs.CR</categories><comments>19 pages; NWC, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Ciphertext-Policy Attribute Based Encryption (CP-ABE), attributes are
attached to the user's secret key and access policy is at-tached to the
ciphertext. If attributes in the secret key of a user satisfy the policy then
only the genuine user can decrypt the ciphertext. However, such scenario also
necessitates periodic updating of the secret key with the changing attributes.
According to our observations, the existing attempts at doing so are not
efficient. In this paper, we propose a newer approach to add, update or delete
the value of particular attribute effi-ciently without the knowledge of the
other attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5855</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5855</id><created>2012-08-29</created><authors><author><keyname>Svorenova</keyname><forenames>Maria</forenames></author><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Barnat</keyname><forenames>Jiri</forenames></author><author><keyname>Cerna</keyname><forenames>Ivana</forenames></author></authors><title>Attraction-Based Receding Horizon Path Planning with Temporal Logic
  Constraints</title><categories>cs.RO</categories><comments>Extended version of CDC 2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal in this paper is to plan the motion of a robot in a partitioned
environment with dynamically changing, locally sensed rewards. We assume that
arbitrary assumptions on the reward dynamics can be given. The robot aims to
accomplish a high-level temporal logic surveillance mission and to locally
optimize the collection of the rewards in the visited regions. These two
objectives often conflict and only a compromise between them can be reached. We
address this issue by taking into consideration a user-defined preference
function that captures the trade-off between the importance of collecting high
rewards and the importance of making progress towards a surveyed region. Our
solution leverages ideas from the automata-based approach to model checking. We
demonstrate the utilization and benefits of the suggested framework in an
illustrative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5870</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5870</id><created>2012-08-29</created><authors><author><keyname>Joshi</keyname><forenames>Shaunak</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>&#x10c;abri&#x107;</keyname><forenames>Danijela</forenames></author><author><keyname>Villasenor</keyname><forenames>John</forenames></author></authors><title>When Channel Bonding is Beneficial for Opportunistic Spectrum Access
  Networks</title><categories>cs.NI</categories><comments>accepted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission over multiple frequency bands combined into one logical channel
speeds up data transfer for wireless networks. On the other hand, the
allocation of multiple channels to a single user decreases the probability of
finding a free logical channel for new connections, which may result in a
network-wide throughput loss. While this relationship has been studied
experimentally, especially in the WLAN configuration, little is known on how to
analytically model such phenomena. With the advent of Opportunistic Spectrum
Access (OSA) networks, it is even more important to understand the
circumstances in which it is beneficial to bond channels occupied by primary
users with dynamic duty cycle patterns. In this paper we propose an analytical
framework which allows the investigation of the average channel throughput at
the medium access control layer for OSA networks with channel bonding enabled.
We show that channel bonding is generally beneficial, though the extent of the
benefits depend on the features of the OSA network, including OSA network size
and the total number of channels available for bonding. In addition, we show
that performance benefits can be realized by adaptively changing the number of
bonded channels depending on network conditions. Finally, we evaluate channel
bonding considering physical layer constraints, i.e. throughput reduction
compared to the theoretical throughput of a single virtual channel due to a
transmission power limit for any bonding size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5875</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5875</id><created>2012-08-29</created><authors><author><keyname>Rahi</keyname><forenames>Priyanka</forenames><affiliation>Department of Computer Science H.P. University, Shimla, H.P., India</affiliation></author></authors><title>Business Intelligence: A Rapidly Growing Option through Web Mining</title><categories>cs.OH</categories><comments>8 pages, 2 figure, 2 tables. arXiv admin note: text overlap with
  arXiv:cs/0405030 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web is a popular and interactive medium to distribute
information in this scenario. The web is huge, diverse, ever changing, widely
disseminated global information service center. We are familiar with terms like
e-commerce, e-governance, e-market, e-finance, e-learning, e-banking etc. for
an organization it is new challenge to maintain direct contact with customers
because of the rapid growth in e-commerce, e-publishing and electronic service
delivery. To deal with this there is need of intelligent marketing strategies
and CRM (customer relationship management) i.e. the effective way of
integrating enterprise applications in real time. Web mining is the vast field
that helps to understand various concepts of different fields. Web usage mining
techniques are attempted to reason about different materialized issues of
Business Intelligence which include marketing expertise as domain knowledge and
are specifically designed for electronic commerce purposes. To this end, the
chapter provides an introduction to the field of Web mining and examines
existing as well as potential Web mining applications applicable for different
business function, like marketing, human resources, and fiscal administration.
Suggestions for improving information technology infrastructure are made, which
can help businesses interested in Web mining hit the ground running.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5889</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5889</id><created>2012-08-29</created><updated>2013-03-14</updated><authors><author><keyname>Jana</keyname><forenames>Raj K.</forenames></author><author><keyname>Snider</keyname><forenames>Gregory L.</forenames></author><author><keyname>Jena</keyname><forenames>Debdeep</forenames></author></authors><title>Resonant Clocking Circuits for Reversible Computation</title><categories>cs.OH</categories><comments>This paper has been withdrawn by the author due to other reasons</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mechanism for the reduction of dynamic energy dissipation in the computing
circuit is described. The resonant circuit with controlled switches conserves
the stored energy by recovering upto 90% of energy that would be otherwise lost
during logic state transitions. This energy-conserving approach preserves
thermodynamic entropy, ideally preventing heat generation in the system. This
approach is used in a proposed resonant clocking and logic application without
dynamic energy dissipation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5894</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5894</id><created>2012-08-29</created><updated>2012-08-30</updated><authors><author><keyname>Petra</keyname><forenames>Stefania</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Average Case Recovery Analysis of Tomographic Compressive Sensing</title><categories>math.NA cs.IT math.IT</categories><msc-class>65F22, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reconstruction of three-dimensional sparse volume functions from few
tomographic projections constitutes a challenging problem in image
reconstruction and turns out to be a particular instance problem of compressive
sensing. The tomographic measurement matrix encodes the incidence relation of
the imaging process, and therefore is not subject to design up to small
perturbations of non-zero entries. We present an average case analysis of the
recovery properties and a corresponding tail bound to establish weak
thresholds, in excellent agreement with numerical experiments. Our result
improve the state-of-the-art of tomographic imaging in experimental fluid
dynamics by a factor of three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5895</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5895</id><created>2012-08-29</created><updated>2012-09-20</updated><authors><author><keyname>Thamsborg</keyname><forenames>Jacob</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Birkedal</keyname><forenames>Lars</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Yang</keyname><forenames>Hongseok</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Two for the Price of One: Lifting Separation Logic Assertions</title><categories>cs.PL</categories><proxy>LMCS</proxy><acm-class>F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  21, 2012) lmcs:997</journal-ref><doi>10.2168/LMCS-8(3:22)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, data abstraction has been studied in the context of separation
logic, with noticeable practical successes: the developed logics have enabled
clean proofs of tricky challenging programs, such as subject-observer patterns,
and they have become the basis of efficient verification tools for Java
(jStar), C (VeriFast) and Hoare Type Theory (Ynot). In this paper, we give a
new semantic analysis of such logic-based approaches using Reynolds's
relational parametricity. The core of the analysis is our lifting theorems,
which give a sound and complete condition for when a true implication between
assertions in the standard interpretation entails that the same implication
holds in a relational interpretation. Using these theorems, we provide an
algorithm for identifying abstraction-respecting client-side proofs; the proofs
ensure that clients cannot distinguish two appropriately-related module
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5907</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5907</id><created>2012-08-29</created><updated>2013-05-13</updated><authors><author><keyname>Birmel&#xe9;</keyname><forenames>Etienne</forenames></author></authors><title>Choose Outsiders First: a mean 2-approximation random algorithm for
  covering problems</title><categories>cs.DS cs.CC</categories><comments>8 pages The paper has been withdrawn due to an error in the proof</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A high number of discrete optimization problems, including Vertex Cover, Set
Cover or Feedback Vertex Set, can be unified into the class of covering
problems. Several of them were shown to be inapproximable by deterministic
algorithms. This article proposes a new random approach, called Choose
Outsiders First, which consists in selecting randomly ele- ments which are
excluded from the cover. We show that this approach leads to random outputs
which mean size is at most twice the optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5909</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5909</id><created>2012-08-29</created><updated>2012-09-16</updated><authors><author><keyname>Parys</keyname><forenames>Pawel</forenames><affiliation>Warsaw University</affiliation></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames><affiliation>LaBRI</affiliation></author></authors><title>Weak Alternating Timed Automata</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.1.1, F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  19, 2012) lmcs:1214</journal-ref><doi>10.2168/LMCS-8(3:18)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternating timed automata on infinite words are considered. The main result
is a characterization of acceptance conditions for which the emptiness problem
for these automata is decidable. This result implies new decidability results
for fragments of timed temporal logics. It is also shown that, unlike for MITL,
the characterisation remains the same even if no punctual constraints are
allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5913</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5913</id><created>2012-08-29</created><updated>2013-05-29</updated><authors><author><keyname>Kramer</keyname><forenames>Simon</forenames></author></authors><title>Logic of Negation-Complete Interactive Proofs (Formal Theory of
  Epistemic Deciders)</title><categories>math.LO cs.CR cs.DC cs.LO cs.MA</categories><comments>Expanded Introduction. Added Footnote 4. Corrected Corollary 3 and 4.
  Continuation of arXiv:1208.1842</comments><journal-ref>Electronic Notes in Theoretical Computer Science, Volume 300, 21
  January 2014, Pages 47-70</journal-ref><doi>10.1016/j.entcs.2013.12.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We produce a decidable classical normal modal logic of internalised
negation-complete and thus disjunctive non-monotonic interactive proofs (LDiiP)
from an existing logical counterpart of non-monotonic or instant interactive
proofs (LiiP). LDiiP internalises agent-centric proof theories that are
negation-complete (maximal) and consistent (and hence strictly weaker than, for
example, Peano Arithmetic) and enjoy the disjunction property (like
Intuitionistic Logic). In other words, internalised proof theories are
ultrafilters and all internalised proof goals are definite in the sense of
being either provable or disprovable to an agent by means of disjunctive
internalised proofs (thus also called epistemic deciders). Still, LDiiP itself
is classical (monotonic, non-constructive), negation-incomplete, and does not
have the disjunction property. The price to pay for the negation completeness
of our interactive proofs is their non-monotonicity and non-communality (for
singleton agent communities only). As a normal modal logic, LDiiP enjoys a
standard Kripke-semantics, which we justify by invoking the Axiom of Choice on
LiiP's and then construct in terms of a concrete oracle-computable function.
LDiiP's agent-centric internalised notion of proof can also be viewed as a
negation-complete disjunctive explicit refinement of standard KD45-belief, and
yields a disjunctive but negation-incomplete explicit refinement of
S4-provability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5915</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5915</id><created>2012-08-13</created><authors><author><keyname>Boudol</keyname><forenames>G&#xe9;rard</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Petri</keyname><forenames>Gustavo</forenames><affiliation>Purdue University</affiliation></author><author><keyname>Serpette</keyname><forenames>Bernard</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Relaxed Operational Semantics of Concurrent Programming Languages</title><categories>cs.PL cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2012, arXiv:1208.2440</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 89, 2012, pp. 19-33</journal-ref><doi>10.4204/EPTCS.89.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel, operational framework to formally describe the semantics
of concurrent programs running within the context of a relaxed memory model.
Our framework features a &quot;temporary store&quot; where the memory operations issued
by the threads are recorded, in program order. A memory model then specifies
the conditions under which a pending operation from this sequence is allowed to
be globally performed, possibly out of order. The memory model also involves a
&quot;write grain,&quot; accounting for architectures where a thread may read a write
that is not yet globally visible. Our formal model is supported by a software
simulator, allowing us to run litmus tests in our semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5919</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5919</id><created>2012-08-29</created><authors><author><keyname>Mulgrew</keyname><forenames>Bernard</forenames></author></authors><title>The Stationary Phase Approximation, Time-Frequency Decomposition and
  Auditory Processing</title><categories>cs.IT cs.SD math.IT</categories><comments>Submitted to IEEE Trans Signal Processing 14th Aug 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principle of stationary phase (PSP) is re-examined in the context of
linear time-frequency (TF) decomposition using Gaussian, gammatone and
gammachirp filters at uniform, logarithmic and cochlear spacings in frequency.
This necessitates consideration of the use the PSP on non-asymptotic integrals
and leads to the introduction of a test for phase rate dominance. Regions of
the TF plane that pass the test and don't contain stationary phase points
contribute little or nothing to the final output. Analysis values that lie in
these regions can thus be set to zero, i.e. sparsity. In regions of the TF
plane that fail the test or are in the vicinity of stationary phase points,
synthesis is performed in the usual way. A new interpretation of the location
parameters associated with the synthesis filters leads to: (i) a new method for
locating stationary phase points in the TF plane; (ii) a test for phase rate
dominance in that plane. Together this is a TF stationary phase approximation
(TFSFA) for both analysis and synthesis. The stationary phase regions of
several elementary signals are identified theoretically and examples of
reconstruction given. An analysis of the TF phase rate characteristics for the
case of two simultaneous tones predicts and quantifies a form of simultaneous
masking similar to that which characterizes the auditory system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5933</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5933</id><created>2012-08-29</created><authors><author><keyname>Cousineau</keyname><forenames>Denis</forenames></author><author><keyname>Doligez</keyname><forenames>Damien</forenames></author><author><keyname>Lamport</keyname><forenames>Leslie</forenames></author><author><keyname>Merz</keyname><forenames>Stephan</forenames></author><author><keyname>Ricketts</keyname><forenames>Daniel</forenames></author><author><keyname>Vanzetto</keyname><forenames>Hern&#xe1;n</forenames></author></authors><title>TLA+ Proofs</title><categories>cs.SE cs.LO</categories><comments>A shorter version of this article appeared in the proceedings of the
  conference Formal Methods 2012 (FM 2012, Paris, France, Springer LNCS 7436,
  pp. 147-154)</comments><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TLA+ is a specification language based on standard set theory and temporal
logic that has constructs for hierarchical proofs. We describe how to write
TLA+ proofs and check them with TLAPS, the TLA+ Proof System. We use Peterson's
mutual exclusion algorithm as a simple example to describe the features of
TLAPS and show how it and the Toolbox (an IDE for TLA+) help users to manage
large, complex proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5946</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5946</id><created>2012-08-29</created><authors><author><keyname>Chan</keyname><forenames>Siu On</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author></authors><title>On extracting common random bits from correlated sources on large
  alphabets</title><categories>cs.IT math.IT</categories><comments>15 pages, 1 figure</comments><msc-class>68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose Alice and Bob receive strings $X=(X_1,...,X_n)$ and $Y=(Y_1,...,Y_n)$
each uniformly random in $[s]^n$ but so that $X$ and $Y$ are correlated . For
each symbol $i$, we have that $Y_i = X_i$ with probability $1-\eps$ and
otherwise $Y_i$ is chosen independently and uniformly from $[s]$.
  Alice and Bob wish to use their respective strings to extract a uniformly
chosen common sequence from $[s]^k$ but without communicating. How well can
they do? The trivial strategy of outputting the first $k$ symbols yields an
agreement probability of $(1 - \eps + \eps/s)^k$. In a recent work by Bogdanov
and Mossel it was shown that in the binary case where $s=2$ and $k = k(\eps)$
is large enough then it is possible to extract $k$ bits with a better agreement
probability rate. In particular, it is possible to achieve agreement
probability $(k\eps)^{-1/2} \cdot 2^{-k\eps/(2(1 - \eps/2))}$ using a random
construction based on Hamming balls, and this is optimal up to lower order
terms.
  In the current paper we consider the same problem over larger alphabet sizes
$s$ and we show that the agreement probability rate changes dramatically as the
alphabet grows. In particular we show no strategy can achieve agreement
probability better than $(1-\eps)^k (1+\delta(s))^k$ where $\delta(s) \to 0$ as
$s \to \infty$. We also show that Hamming ball based constructions have {\em
much lower} agreement probability rate than the trivial algorithm as $s \to
\infty$. Our proofs and results are intimately related to subtle properties of
hypercontractive inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5956</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5956</id><created>2012-08-29</created><authors><author><keyname>Pratt</keyname><forenames>Vaughan R.</forenames></author></authors><title>A combinatorial analysis of the average time for open-address hash
  coding insertion</title><categories>math.CO cs.DS</categories><comments>December 1973, revised 1974</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analysing a well-known hash-coding method, Knuth gave an exact expression
for the average number of rejections encountered by players of a variant of
musical chairs. We study a variant more closely related to musical chairs
itself and deduce the same expression by a purely combinatorial approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5959</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5959</id><created>2012-08-29</created><updated>2013-05-12</updated><authors><author><keyname>Adcock</keyname><forenames>Ben</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author><author><keyname>Poon</keyname><forenames>Clarice</forenames></author></authors><title>On optimal wavelet reconstructions from Fourier samples: linearity and
  universality of the stable sampling rate</title><categories>math.NA cs.IT math.IT</categories><msc-class>94A20, 42C40, 65T60, 41A65, 46C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of computing wavelet coefficients of
compactly supported functions from their Fourier samples. For this, we use the
recently introduced framework of generalized sampling. Our first result
demonstrates that using generalized sampling one obtains a stable and accurate
reconstruction, provided the number of Fourier samples grows linearly in the
number of wavelet coefficients recovered. For the class of Daubechies wavelets
we derive the exact constant of proportionality.
  Our second result concerns the optimality of generalized sampling for this
problem. Under some mild assumptions we show that generalized sampling cannot
be outperformed in terms of approximation quality by more than a constant
factor. Moreover, for the class of so-called perfect methods, any attempt to
lower the sampling ratio below a certain critical threshold necessarily results
in exponential ill-conditioning. Thus generalized sampling provides a
nearly-optimal solution to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5963</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5963</id><created>2012-08-29</created><updated>2012-10-08</updated><authors><author><keyname>Aalto</keyname><forenames>Daniel</forenames></author><author><keyname>Huhtala</keyname><forenames>Antti</forenames></author><author><keyname>Kivel&#xe4;</keyname><forenames>Atle</forenames></author><author><keyname>Malinen</keyname><forenames>Jarmo</forenames></author><author><keyname>Palo</keyname><forenames>Pertti</forenames></author><author><keyname>Saunavaara</keyname><forenames>Jani</forenames></author><author><keyname>Vainio</keyname><forenames>Martti</forenames></author></authors><title>How far are vowel formants from computed vocal tract resonances?</title><categories>math.DS cs.SD</categories><comments>13 pages, 1 figure, 3 tables</comments><msc-class>37N25, 92C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare numerically computed resonances of the human vocal tract with
formants that have been extracted from speech during vowel pronunciation. The
geometry of the vocal tract has been obtained by MRI from a male subject, and
the corresponding speech has been recorded simultaneously. The resonances are
computed by solving the Helmholtz partial differential equation with the Finite
Element Method (FEM).
  Despite a rudimentary exterior space acoustics model, i.e., the Dirichlet
boundary condition at the mouth opening, the computed resonance structure
differs from the measured formant structure by $\approx$ 0.7 semitones for [i]
and [u] having small mouth opening area, and by $\approx$ 3 semitones for
vowels [a] and [ae] that have a larger mouth opening. The contribution of the
possibly open velar port has not been taken into considaration at all which
adds the discrepancy for [a] in the present data set. We conclude that by
improving the exterior space model and properly treating the velar port
opening, it is possible to computationally attain four lowest vowel formants
with an error less than a semitone. The corresponding wave equation model on
MRI-produced vocal tract geometries is expected to have a comparable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5980</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5980</id><created>2012-08-29</created><authors><author><keyname>D'Antoni</keyname><forenames>Loris</forenames></author></authors><title>In the Maze of Data Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data languages the positions of strings and trees carry a label from a
finite alphabet and a data value from an infinite alphabet. Extensions of
automata and logics over finite alphabets have been defined to recognize data
languages, both in the string and tree cases. In this paper we describe and
compare the complexity and expressiveness of such models to understand which
ones are better candidates as regular models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5991</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5991</id><created>2012-08-29</created><updated>2012-09-25</updated><authors><author><keyname>Doshi</keyname><forenames>Nishant</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author></authors><title>Constant Ciphertext Length in CP-ABE</title><categories>cs.CR</categories><comments>12 pages; NWC, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ciphertext policy attribute based encryption (CP-ABE) is a technique in which
user with secret key containing attributes, only able to decrypt the message if
the attributes in the policy match with the attributes in secret key. The
existing methods that use reasonably computable decryption policies produce the
ciphertext of size at least linearly varying with the number of attributes with
additional pairing operations during encryption and decryption. In this paper,
we propose a scheme in which ciphertext remains constant in length,
irrespective of the number of attributes. Our scheme works for a threshold
case: the number of attributes in a policy must be a subset of attributes in a
secret key. The security of propose scheme is based on Decisional Bilinear
Diffie-Hellman (DBDH) problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.5997</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.5997</id><created>2012-08-29</created><authors><author><keyname>Ibrahim</keyname><forenames>Heba Ezzat</forenames></author><author><keyname>Badr</keyname><forenames>Sherif M.</forenames></author><author><keyname>Shaheen</keyname><forenames>Mohamed A.</forenames></author></authors><title>Phases vs. Levels using Decision Trees for Intrusion Detection Systems</title><categories>cs.CR</categories><comments>7 pages; (IJCSIS) International Journal of Computer Science and
  Information Security, Vol. 10, No. 8, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of computers and the networks that connect them is increasingly
becoming of great significance. Intrusion detection system is one of the
security defense tools for computer networks. This paper compares two different
model Approaches for representing intrusion detection system by using decision
tree techniques. These approaches are Phase-model approach and Level-model
approach. Each model is implemented by using two techniques, New Attacks and
Data partitioning techniques. The experimental results showed that Phase
approach has higher classification rate in both New Attacks and Data
Partitioning techniques than Level approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6025</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6025</id><created>2012-08-11</created><authors><author><keyname>Habib</keyname><forenames>Md. Tarek</forenames></author><author><keyname>Faisal</keyname><forenames>Rahat Hossain</forenames></author><author><keyname>Rokonuzzaman</keyname><forenames>M.</forenames></author></authors><title>Feasibility of Genetic Algorithm for Textile Defect Classification Using
  Neural Network</title><categories>cs.NE</categories><comments>20 pages, 11 figures, 6 tables; International Journal of Artificial
  Intelligence &amp; Applications (IJAIA),AIRCC, July 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The global market for textile industry is highly competitive nowadays.
Quality control in production process in textile industry has been a key factor
for retaining existence in such competitive market. Automated textile
inspection systems are very useful in this respect, because manual inspection
is time consuming and not accurate enough. Hence, automated textile inspection
systems have been drawing plenty of attention of the researchers of different
countries in order to replace manual inspection. Defect detection and defect
classification are the two major problems that are posed by the research of
automated textile inspection systems. In this paper, we perform an extensive
investigation on the applicability of genetic algorithm (GA) in the context of
textile defect classification using neural network (NN). We observe the effect
of tuning different network parameters and explain the reasons. We empirically
find a suitable NN model in the context of textile defect classification. We
compare the performance of this model with that of the classification models
implemented by others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6028</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6028</id><created>2012-08-13</created><authors><author><keyname>Ulker</keyname><forenames>Sadik</forenames></author></authors><title>Design of Low Noise Amplifiers Using Particle Swarm Optimization</title><categories>cs.NE</categories><comments>8 pages, 4 figures</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  vol 3, no 4, July 2012, 99-106</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper presents a work on the design of low noise microwave
amplifiers using particle swarm optimization (PSO) technique. Particle Swarm
Optimization is used as a method that is applied to a single stage amplifier
circuit to meet two criteria: desired gain and desired low noise. The aim is to
get the best optimized design using the predefined constraints for gain and low
noise values. The code is written to apply the algorithm to meet the desired
goals and the obtained results are verified using different simulators. The
results obtained show that PSO can be applied very efficiently for this kind of
design problems with multiple constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6037</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6037</id><created>2012-08-29</created><authors><author><keyname>El-Hamied</keyname><forenames>Sara Shaker Abed</forenames></author><author><keyname>Saleh</keyname><forenames>Ahmed Abou El-Fotouh</forenames></author><author><keyname>Asem</keyname><forenames>Aziza</forenames></author></authors><title>Survey on Using GIS in Evacuation Planning</title><categories>cs.CY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural crises form a big threat on environment; these crises mean the loss
of enterprises and individuals, and therefore losses in the sum total of
community development. Management to these crises is required through a crisis
management plan to control the crises before, during, and after the event. One
of the most needed things to consider during preparing the crisis management
plan is preparing the evacuation plan in order to transfer people from the
incident place to a safe place; this must be done quickly and carefully.
Because of the geographic nature of the evacuation process, Geographical
Information System (GIS) has been used widely and effectively for over 20 years
in the field of crisis management in general and in evacuation planning in
particular. This paper provides an overview about evacuation process and the
basic concepts of GIS systems. The paper also demonstrates the importance of
evacuation planning and how GIS systems used in other studies to assists in
evacuation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6051</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6051</id><created>2012-08-29</created><authors><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>Lower Bounds on Information Dissemination in Dynamic Networks</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study lower bounds on information dissemination in adversarial dynamic
networks. Initially, k pieces of information (henceforth called tokens) are
distributed among n nodes. The tokens need to be broadcast to all nodes through
a synchronous network in which the topology can change arbitrarily from round
to round provided that some connectivity requirements are satisfied.
  If the network is guaranteed to be connected in every round and each node can
broadcast a single token per round to its neighbors, there is a simple token
dissemination algorithm that manages to deliver all k tokens to all the nodes
in O(nk) rounds. Interestingly, in a recent paper, Dutta et al. proved an
almost matching Omega(n + nk/log n) lower bound for deterministic
token-forwarding algorithms that are not allowed to combine, split, or change
tokens in any way. In the present paper, we extend this bound in different
ways.
  If nodes are allowed to forward b &lt; k tokens instead of only one token in
every round, a straight-forward extension of the O(nk) algorithm disseminates
all k tokens in time O(nk/b). We show that for any randomized token-forwarding
algorithm, Omega(n + nk/(b^2 log n log log n)) rounds are necessary. If nodes
can only send a single token per round, but we are guaranteed that the network
graph is c-vertex connected in every round, we show a lower bound of
Omega(nk/(c log^{3/2} n)), which almost matches the currently best O(nk/c)
upper bound. Further, if the network is T-interval connected, a notion that
captures connection stability over time, we prove that Omega(n + nk/(T^2 log
n)) rounds are needed. The best known upper bound in this case manages to solve
the problem in O(n + nk/T) rounds. Finally, we show that even if each node only
needs to obtain a delta-fraction of all the tokens for some delta in [0,1],
Omega(nk delta^3 log n) are still required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6057</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6057</id><created>2012-08-29</created><authors><author><keyname>Wang</keyname><forenames>Po T.</forenames></author><author><keyname>King</keyname><forenames>Christine E.</forenames></author><author><keyname>Chui</keyname><forenames>Luis A.</forenames></author><author><keyname>Do</keyname><forenames>An H.</forenames></author><author><keyname>Nenadic</keyname><forenames>Zoran</forenames></author></authors><title>Self-paced brain-computer interface control of ambulation in a virtual
  reality environment</title><categories>cs.HC</categories><comments>20 pages, 7 figures, link to video supplementary material
  (http://youtu.be/GXmovT3BxEo)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Spinal cord injury (SCI) often leaves affected individuals unable
to ambulate. Electroencephalogramme (EEG) based brain-computer interface (BCI)
controlled lower extremity prostheses may restore intuitive and able-body-like
ambulation after SCI. To test its feasibility, the authors developed and tested
a novel EEG-based, data-driven BCI system for intuitive and self-paced control
of the ambulation of an avatar within a virtual reality environment (VRE).
  Approach: Eight able-bodied subjects and one with SCI underwent the following
10-min training session: subjects alternated between idling and walking
kinaesthetic motor imageries (KMI) while their EEG were recorded and analysed
to generate subject-specific decoding models. Subjects then performed a
goal-oriented online task, repeated over 5 sessions, in which they utilised the
KMI to control the linear ambulation of an avatar and make 10 sequential stops
at designated points within the VRE.
  Main results: The average offline training performance across subjects was
77.2 +/- 9.5%, ranging from 64.3% (p = 0.00176) to 94.5% (p = 6.26*10^-23),
with chance performance being 50%. The average online performance was 8.4 +/-
1.0 (out of 10) successful stops and 303 +/- 53 sec completion time (perfect =
211 sec). All subjects achieved performances significantly different than those
of random walk (p &lt; 0.05) in 44 of the 45 online sessions.
  Significance: By using a data-driven machine learning approach to decode
users' KMI, this BCIVRE system enabled intuitive and purposeful self-paced
control of ambulation after only a 10-minute training. The ability to achieve
such BCI control with minimal training indicates that the implementation of
future BCI-lower extremity prosthesis systems may be feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6061</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6061</id><created>2012-08-29</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author><author><keyname>James</keyname><forenames>Matthew R.</forenames></author></authors><title>Robust Stability of Quantum Systems with a Nonlinear Coupling Operator</title><categories>quant-ph cs.SY math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.2676</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of robust stability for a class of uncertain
quantum systems subject to unknown perturbations in the system coupling
operator. A general stability result is given for a class of perturbations to
the system coupling operator. Then, the special case of a nominal linear
quantum system is considered with non-linear perturbations to the system
coupling operator. In this case, a robust stability condition is given in terms
of a scaled strict bounded real condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6063</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6063</id><created>2012-08-29</created><authors><author><keyname>Singh</keyname><forenames>Anurag</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Nonlinear spread of rumor and inoculation strategies in the nodes with
  degree dependent tie strength in complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>27 pages 15 figures</comments><journal-ref>Acta Physica Polonica B Vol. 44, No. 1, January 2013, page 5</journal-ref><doi>10.5506/APhysPolB.44.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In earlier rumor spreading models, at each time step nodes contact all of
their neighbors. In more realistic scenario it is possible that a node may
contact only some of its neighbors to spread the rumor. Therefore it is must in
real world complex networks, the classic rumor spreading model need to be
modified to consider the dependence of rumor spread rate on the degree of the
spreader and the informed nodes. We have given a modified rumor spreading model
to accommodate these facts. This new model, has been studied for rumor
spreading in complex networks in this work. Nonlinear rumor spread exponent
$\alpha$ and degree dependent tie strength exponent $\beta$ in any complex
network gives rumor threshold as some finite value. In the present work, the
modified rumor spreading model has been studied in scale free networks. It is
also found that if $ \alpha $ and $ \beta $ parameters are tuned to appropriate
value, the rumor threshold becomes independent of network size. In any social
network, rumors can spread may have undesirable effect. One of the possible
solutions to control rumor spread, is to inoculate a certain fraction of nodes
against rumors. The inoculation can be done randomly or in a targeted fashion.
We have used modified rumor spreading model over scale free networks to
investigate the efficacy of inoculation. Random and targeted inoculation
schemes have been applied. It has been observed that rumor threshold in random
inoculation scheme is greater than the rumor threshold in the model without any
inoculation scheme. But random inoculation is not that much effective. The
rumor threshold in targeted inoculation is very high than the rumor threshold
in the random inoculation in suppressing the rumor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6064</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6064</id><created>2012-08-29</created><authors><author><keyname>Rehman</keyname><forenames>Obaid Ur</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Fidan</keyname><forenames>Bar&#x131;&#x15f;</forenames></author></authors><title>Minimax Linear Quadratic Gaussian Control of Nonlinear MIMO System with
  Time Varying Uncertainties</title><categories>cs.SY</categories><comments>7 pages 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a robust nonlinear control scheme is proposed for a nonlinear
multi-input multi-output (MIMO) system subject to bounded time varying
uncertainty which satisfies a certain integral quadratic constraint condition.
The scheme develops a robust feedback linarization approach which uses standard
feedback linearization approach to linearize the nominal nonlinear dynamics of
the uncertain nonlinear system and linearizes the nonlinear time varying
uncertainties at an arbitrary point using the mean value theorem. This approach
transforms uncertain nonlinear MIMO systems into an equivalent MIMO linear
uncertain system model with unstructured uncertainty. Finally, a robust minimax
linear quadratic Gaussian (LQG) control design is proposed for the linearized
model. The scheme guarantees the internal stability of the closed loop system
and provides robust performance. In order to illustrate the effectiveness of
this approach, the proposed method is applied to a tracking control problem for
an air-breathing hypersonic flight vehicle (AHFV).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6067</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6067</id><created>2012-08-29</created><updated>2013-04-23</updated><authors><author><keyname>Javdani</keyname><forenames>Shervin</forenames></author><author><keyname>Klingensmith</keyname><forenames>Matthew</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Pollard</keyname><forenames>Nancy S.</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha S.</forenames></author></authors><title>Efficient Touch Based Localization through Submodularity</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many robotic systems deal with uncertainty by performing a sequence of
information gathering actions. In this work, we focus on the problem of
efficiently constructing such a sequence by drawing an explicit connection to
submodularity. Ideally, we would like a method that finds the optimal sequence,
taking the minimum amount of time while providing sufficient information.
Finding this sequence, however, is generally intractable. As a result, many
well-established methods select actions greedily. Surprisingly, this often
performs well. Our work first explains this high performance -- we note a
commonly used metric, reduction of Shannon entropy, is submodular under certain
assumptions, rendering the greedy solution comparable to the optimal plan in
the offline setting. However, reacting online to observations can increase
performance. Recently developed notions of adaptive submodularity provide
guarantees for a greedy algorithm in this online setting. In this work, we
develop new methods based on adaptive submodularity for selecting a sequence of
information gathering actions online. In addition to providing guarantees, we
can capitalize on submodularity to attain additional computational speedups. We
demonstrate the effectiveness of these methods in simulation and on a robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6070</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6070</id><created>2012-08-29</created><updated>2012-11-19</updated><authors><author><keyname>Khodakarami</keyname><forenames>Hamid</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>Link Adaptation with Untrusted Relay Assignment: Design and Performance
  Analysis</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a link adaptation and untrusted relay assignment (LAURA)
framework for efficient and reliable wireless cooperative communications with
physical layer security is proposed. Using sharp channel codes in different
transmission modes, reliability for the destination and security in the
presence of untrusted relays (low probability of interception) are provided
through rate and power allocation. Within this framework, several schemes are
designed for highly spectrally efficient link adaptation and relay selection,
which involve different levels of complexity and channel state information
requirement. Analytical and simulation performance evaluation of the proposed
LAURA schemes are provided, which demonstrates the effectiveness of the
presented designs. The results indicate that power adaptation at the source
plays a critical role in spectral efficiency performance. Also, it is shown
that relay selection based on the signal to noise ratio of the source to relays
channels provides an interesting balance of performance and complexity within
the proposed LAURA framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6092</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6092</id><created>2012-08-30</created><updated>2014-10-11</updated><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>One-Way Reversible and Quantum Finite Automata with Advice</title><categories>quant-ph cs.CC cs.FL</categories><comments>A4, 10pt, 1 figure, 31 pages. This is a complete version of an
  extended abstract appeared in the Proceedings of the 6th International
  Conference on Language and Automata Theory and Applications (LATA 2012),
  March 5-9, 2012, A Coruna, Spain, Lecture Notes in Computer Science,
  Springer-Verlag, Vol.7183, pp.526-537, 2012</comments><journal-ref>Information and Computation, vol.239, pp.122-148, 2014</journal-ref><doi>10.1016/j.ic.2014.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the characteristic features of reversible and quantum computations
in the presence of supplementary external information, known as advice. In
particular, we present a simple, algebraic characterization of languages
recognized by one-way reversible finite automata augmented with deterministic
advice. With a further elaborate argument, we prove a similar but slightly
weaker result for bounded-error one-way quantum finite automata with advice.
Immediate applications of those properties lead to containments and separations
among various language families when they are assisted by appropriately chosen
advice. We further demonstrate the power and limitation of randomized advice
and quantum advice when they are given to one-way quantum finite automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6094</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6094</id><created>2012-08-30</created><updated>2012-12-05</updated><authors><author><keyname>Wang</keyname><forenames>Jiadong</forenames></author><author><keyname>Dolecek</keyname><forenames>Lara</forenames></author><author><keyname>Wesel</keyname><forenames>Richard</forenames></author></authors><title>The Cycle Consistency Matrix Approach to Absorbing Sets in Separable
  Circulant-Based LDPC Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For LDPC codes operating over additive white Gaussian noise channels and
decoded using message-passing decoders with limited precision, absorbing sets
have been shown to be a key factor in error floor behavior. Focusing on this
scenario, this paper introduces the cycle consistency matrix (CCM) as a
powerful analytical tool for characterizing and avoiding absorbing sets in
separable circulant-based (SCB) LDPC codes. SCB codes include a wide variety of
regular LDPC codes such as array-based LDPC codes as well as many common
quasi-cyclic codes. As a consequence of its cycle structure, each potential
absorbing set in an SCB LDPC code has a CCM, and an absorbing set can be
present in an SCB LDPC code only if the associated CCM has a nontrivial null
space.
  CCM-based analysis can determine the multiplicity of an absorbing set in an
SCB code and CCM-based constructions avoid certain small absorbing sets
completely. While these techniques can be applied to an SCB code of any rate,
lower-rate SCB codes can usually avoid small absorbing sets because of their
higher variable node degree. This paper focuses attention on the high-rate
scenario in which the CCM constructions provide the most benefit. Simulation
results demonstrate that under limited-precision decoding the new codes have
steeper error-floor slopes and can provide one order of magnitude of
improvement in the low FER region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6096</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6096</id><created>2012-08-30</created><authors><author><keyname>Javaid</keyname><forenames>Nadeem</forenames></author></authors><title>M-ATTEMPT: A New Energy-Efficient Routing Protocol in Wireless Body Area
  Sensor Networks</title><categories>cs.NI</categories><comments>To be Submitted in a Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an energy efficient routing algorithm for heterogeneous
Wireless Body Area Sensor Networks (WBASNs). A prototype is defined for
employing heterogeneous sensors on human body. Direct communication is used for
real-time traffic (critical data) and on-demand data while multi-hop
communication is used for normal data delivery in this proposed routing
algorithm. One of the prime challenges in WBASNs is sensing of heat generated
by implanted sensor nodes. The proposed routing algorithm is thermal-aware
which sense the link Hot-spot and routes the data away from these links.
Continuous mobility of human body causes disconnection between previous
established links. We introduce mobility support and energy-management to
overcome the problem of disconnection due to continuous mobility of human body.
MATLAB simulations of proposed routing algorithm are performed for lifetime and
reliability in comparison with multi-hop communication. The results show that
the proposed routing algorithm has less energy consumption and more reliable as
compared to multi-hop communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6106</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6106</id><created>2012-08-30</created><authors><author><keyname>Balliu</keyname><forenames>Musard</forenames></author><author><keyname>Dam</keyname><forenames>Mads</forenames></author><author><keyname>Guernic</keyname><forenames>Gurvan Le</forenames></author></authors><title>Epistemic Temporal Logic for Information Flow Security</title><categories>cs.CR cs.LO cs.MA</categories><comments>Published in PLAS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal epistemic logic is a well-established framework for expressing
agents knowledge and how it evolves over time. Within language-based security
these are central issues, for instance in the context of declassification. We
propose to bring these two areas together. The paper presents a computational
model and an epistemic temporal logic used to reason about knowledge acquired
by observing program outputs. This approach is shown to elegantly capture
standard notions of noninterference and declassification in the literature as
well as information flow properties where sensitive and public data intermingle
in delicate ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6109</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6109</id><created>2012-08-30</created><authors><author><keyname>Bochkarev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Shevlyakova</keyname><forenames>Anna V.</forenames></author><author><keyname>Solovyev</keyname><forenames>Valery D.</forenames></author></authors><title>Average word length dynamics as indicator of cultural changes in society</title><categories>cs.CL</categories><comments>16 pages, 9 figures</comments><msc-class>91F20</msc-class><acm-class>J.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamics of average length of words in Russian and English is analysed in the
article. Words belonging to the diachronic text corpus Google Books Ngram and
dated back to the last two centuries are studied. It was found out that average
word length slightly increased in the 19th century, and then it was growing
rapidly most of the 20th century and started decreasing over the period from
the end of the 20th - to the beginning of the 21th century. Words which
contributed mostly to increase or decrease of word average length were
identified. At that, content words and functional words are analysed
separately. Long content words contribute mostly to word average length of
word. As it was shown, these words reflect the main tendencies of social
development and thus, are used frequently. Change of frequency of personal
pronouns also contributes significantly to change of average word length. The
other parameters connected with average length of word were also analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6112</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6112</id><created>2012-08-30</created><updated>2013-01-15</updated><authors><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author><author><keyname>Chen</keyname><forenames>Zhenghong</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Generic Regular Decompositions for Generic Zero-Dimensional Systems</title><categories>cs.SC</categories><comments>Accepted by Science China: Information Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two new concepts, generic regular decomposition and
regular-decomposition-unstable (RDU) variety for generic zero-dimensional
systems, are introduced in this paper and an algorithm is proposed for
computing a generic regular decomposition and the associated RDU variety of a
given generic zero-dimensional system simultaneously. The solutions of the
given system can be expressed by finitely many zero-dimensional regular chains
if the parameter value is not on the RDU variety.
  The so called weakly relatively simplicial decomposition plays a crucial role
in the algorithm, which is based on the theories of subresultant chains.
Furthermore, the algorithm can be naturally adopted to compute a non-redundant
Wu's decomposition and the decomposition is stable at any parameter value that
is not on the RDU variety. The algorithm has been implemented with Maple 15 and
experimented with a number of benchmarks from the literature. Empirical results
are also presented to show the good performance of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6119</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6119</id><created>2012-08-30</created><updated>2012-11-21</updated><authors><author><keyname>Zhang</keyname><forenames>Fuguo</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author></authors><title>Improving information filtering via network manipulation</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 pages, 5 figures</comments><journal-ref>Europhysics Letters 100, 58005 (2012)</journal-ref><doi>10.1209/0295-5075/100/58005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender system is a very promising way to address the problem of
overabundant information for online users. Though the information filtering for
the online commercial systems received much attention recently, almost all of
the previous works are dedicated to design new algorithms and consider the
user-item bipartite networks as given and constant information. However, many
problems for recommender systems such as the cold-start problem (i.e. low
recommendation accuracy for the small degree items) are actually due to the
limitation of the underlying user-item bipartite networks. In this letter, we
propose a strategy to enhance the performance of the already existing
recommendation algorithms by directly manipulating the user-item bipartite
networks, namely adding some virtual connections to the networks. Numerical
analyses on two benchmark data sets, MovieLens and Netflix, show that our
method can remarkably improve the recommendation performance. Specifically, it
not only improve the recommendations accuracy (especially for the small degree
items), but also help the recommender systems generate more diverse and novel
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6122</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6122</id><created>2012-08-30</created><updated>2012-09-05</updated><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author></authors><title>Source normalized indicators of citation impact: An overview of
  different approaches and an empirical comparison</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different scientific fields have different citation practices. Citation-based
bibliometric indicators need to normalize for such differences between fields
in order to allow for meaningful between-field comparisons of citation impact.
Traditionally, normalization for field differences has usually been done based
on a field classification system. In this approach, each publication belongs to
one or more fields and the citation impact of a publication is calculated
relative to the other publications in the same field. Recently, the idea of
source normalization was introduced, which offers an alternative approach to
normalize for field differences. In this approach, normalization is done by
looking at the referencing behavior of citing publications or citing journals.
In this paper, we provide an overview of a number of source normalization
approaches and we empirically compare these approaches with a traditional
normalization approach based on a field classification system. We also pay
attention to the issue of the selection of the journals to be included in a
normalization for field differences. Our analysis indicates a number of
problems of the traditional classification-system-based normalization approach,
suggesting that source normalization approaches may yield more accurate
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6125</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6125</id><created>2012-08-30</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>Bounded-Contention Coding for Wireless Networks in the High SNR Regime</title><categories>cs.NI cs.DC cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient communication in wireless networks is typically challenged by the
possibility of interference among several transmitting nodes. Much important
research has been invested in decreasing the number of collisions in order to
obtain faster algorithms for communication in such networks.
  This paper proposes a novel approach for wireless communication, which
embraces collisions rather than avoiding them, over an additive channel. It
introduces a coding technique called Bounded-Contention Coding (BCC) that
allows collisions to be successfully decoded by the receiving nodes into the
original transmissions and whose complexity depends on a bound on the
contention among the transmitters.
  BCC enables deterministic local broadcast in a network with n nodes and at
most a transmitters with information of l bits each within O(a log n + al) bits
of communication with full-duplex radios, and O((a log n + al)(log n)) bits,
with high probability, with half-duplex radios. When combined with random
linear network coding, BCC gives global broadcast within O((D + a + log n)(a
log n + l)) bits, with high probability. This also holds in dynamic networks
that can change arbitrarily over time by a worst-case adversary. When no bound
on the contention is given, it is shown how to probabilistically estimate it
and obtain global broadcast that is adaptive to the true contention in the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6134</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6134</id><created>2012-08-30</created><authors><author><keyname>Qin</keyname><forenames>Zhihui</forenames></author><author><keyname>He</keyname><forenames>Guanglei</forenames></author></authors><title>The Period of the subtraction games</title><categories>cs.GT</categories><comments>5 pages, 2 figures, 4 tables</comments><msc-class>91A46, 91A05</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subtraction games is a class of impartial combinatorial games, They with
finite subtraction sets are known to have periodic nim-sequences. So people try
to find the regular of the games. But for specific of Sprague-Grundy Theory, it
is too difficult to find, they obtained some conclusions just by simple
observing. This paper used PTFN algorithm to analyze the period of the
Subtraction games. It is more suitable than Sprague-Grundy Theory, and this
paper obtained four conclusions by PTFN algorithm . This algorithm provide a
new direction to study the subtraction games' period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6137</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6137</id><created>2012-08-30</created><authors><author><keyname>Kumar</keyname><forenames>Deepak</forenames></author><author><keyname>Prasad</keyname><forenames>M N Anil</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A G</forenames></author></authors><title>Benchmarking recognition results on word image datasets</title><categories>cs.CV</categories><comments>16 pages, 4 figures</comments><acm-class>I.7; I.7.5; I.4.6; I.4.8; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have benchmarked the maximum obtainable recognition accuracy on various
word image datasets using manual segmentation and a currently available
commercial OCR. We have developed a Matlab program, with graphical user
interface, for semi-automated pixel level segmentation of word images. We
discuss the advantages of pixel level annotation. We have covered five
databases adding up to over 3600 word images. These word images have been
cropped from camera captured scene, born-digital and street view images. We
recognize the segmented word image using the trial version of Nuance Omnipage
OCR. We also discuss, how the degradations introduced during acquisition or
inaccuracies introduced during creation of word images affect the recognition
of the word present in the image. Word images for different kinds of
degradations and correction for slant and curvy nature of words are also
discussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,
Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,
88.5% and 86.7% respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6140</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6140</id><created>2012-08-30</created><authors><author><keyname>Afanasyeva</keyname><forenames>N.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>P.</forenames></author><author><keyname>Vasil'eva</keyname><forenames>M.</forenames></author></authors><title>Unconditionally stable schemes for non-stationary convection-diffusion
  equations</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convection-diffusion problem are the base for continuum mechanics. The main
features of these problems are associated with an indefinite operator the
problem. In this work we construct unconditionally stable scheme for
non-stationary convection-diffusion equations, which are based on use of new
variables. Also, we consider these equations in the form of
convection-diffusion-reaction and construct unconditionally stable schemes when
explicit-implicit approximations are used with splitting of the reaction
operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6152</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6152</id><created>2012-08-30</created><updated>2012-10-29</updated><authors><author><keyname>Bouajjani</keyname><forenames>Ahmed</forenames></author><author><keyname>Derevenetc</keyname><forenames>Egor</forenames></author><author><keyname>Meyer</keyname><forenames>Roland</forenames></author></authors><title>Checking Robustness against TSO</title><categories>cs.PL</categories><msc-class>68Q60</msc-class><acm-class>D.2.4; D.1.3; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms for checking and enforcing robustness of concurrent
programs against the Total Store Ordering (TSO) memory model. A program is
robust if all its TSO computations correspond to computations under the
Sequential Consistency (SC) semantics.
  We provide a complete characterization of non-robustness in terms of
so-called attacks: a restricted form of (harmful) out-of-program-order
executions. Then, we show that detecting attacks can be parallelized, and can
be solved using state reachability queries under SC semantics in a suitably
instrumented program obtained by a linear size source-to-source translation.
Importantly, the construction is valid for an arbitrary number of addresses and
an arbitrary number of parallel threads, and it is independent from the data
domain and from the size of store buffers in the TSO semantics. In particular,
when the data domain is finite and the number of addresses is fixed, we obtain
decidability and complexity results for robustness, even for an arbitrary
number of threads.
  As a second contribution, we provide an algorithm for computing an optimal
set of fences that enforce robustness. We consider two criteria of optimality:
minimization of program size and maximization of its performance. The
algorithms we define are implemented, and we successfully applied them to
analyzing and correcting several concurrent algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6157</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6157</id><created>2012-08-30</created><updated>2013-02-11</updated><authors><author><keyname>Mirshahvalad</keyname><forenames>Atieh</forenames></author><author><keyname>Beauchesne</keyname><forenames>Olivier H.</forenames></author><author><keyname>Archambault</keyname><forenames>Eric</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Resampling effects on significance analysis of network clustering and
  ranking</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures</comments><doi>10.1371/journal.pone.0053943</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection helps us simplify the complex configuration of networks,
but communities are reliable only if they are statistically significant. To
detect statistically significant communities, a common approach is to resample
the original network and analyze the communities. But resampling assumes
independence between samples, while the components of a network are inherently
dependent. Therefore, we must understand how breaking dependencies between
resampled components affects the results of the significance analysis. Here we
use scientific communication as a model system to analyze this effect. Our
dataset includes citations among articles published in journals in the years
1984-2010. We compare parametric resampling of citations with non-parametric
article resampling. While citation resampling breaks link dependencies, article
resampling maintains such dependencies. We find that citation resampling
underestimates the variance of link weights. Moreover, this underestimation
explains most of the differences in the significance analysis of ranking and
clustering. Therefore, when only link weights are available and article
resampling is not an option, we suggest a simple parametric resampling scheme
that generates link-weight variances close to the link-weight variances of
article resampling. Nevertheless, when we highlight and summarize important
structural changes in science, the more dependencies we can maintain in the
resampling scheme, the earlier we can predict structural change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6172</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6172</id><created>2012-08-30</created><updated>2012-09-16</updated><authors><author><keyname>Bojanczyk</keyname><forenames>Mikolaj</forenames><affiliation>University of Warsaw</affiliation></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Straubing</keyname><forenames>Howard</forenames><affiliation>Boston College</affiliation></author></authors><title>Wreath Products of Forest Algebras, with Applications to Tree Logics</title><categories>cs.LO cs.FL</categories><proxy>LMCS</proxy><acm-class>F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  19, 2012) lmcs:1215</journal-ref><doi>10.2168/LMCS-8(3:19)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the recently developed theory of forest algebras to find algebraic
characterizations of the languages of unranked trees and forests definable in
various logics. These include the temporal logics CTL and EF, and first-order
logic over the ancestor relation. While the characterizations are in general
non-effective, we are able to use them to formulate necessary conditions for
definability and provide new proofs that a number of languages are not
definable in these logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6188</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6188</id><created>2012-08-14</created><authors><author><keyname>Shayda</keyname><forenames>Dara O.</forenames></author></authors><title>G2 Matrix Manifold: A Software Construct</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ensemble of symbolic, numeric and graphic computations developed to
construct the Octonionic and compact G2 structures in Mathematica 8.0.
Cayley-Dickenson Construction symbolically applied from Reals to Octonions.
Baker- Campbell-Hausdorff formula (BCH) in bracket form verified for Octonions.
Algorithms for both exponentiation and logarithm of Octonions developed.
Exclusive validity of vector Product verified for 0, 1, 3 and 7 dimensions.
Symbolic exponential computations carried out for two distinct g2 basis(s) and
arbitrary precision BCH for G2 was coded. Example and counter-example Maximal
Torus for G2 was uncovered. Densely coiled shapes of actions of G2 rendered.
Kolmogorov Complexity for BCH investigated and upper bounds computed:
Complexity of non-commutative non- associative algebraic expression is at most
the Complexity of corresponding commutative associative algebra plus K(BCH).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6189</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6189</id><created>2012-08-30</created><authors><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Papamanthou</keyname><forenames>Charalampos</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>Preserving Link Privacy in Social Network Based Systems</title><categories>cs.CR cs.SI</categories><comments>16 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing body of research leverages social network based trust relationships
to improve the functionality of the system. However, these systems expose
users' trust relationships, which is considered sensitive information in
today's society, to an adversary.
  In this work, we make the following contributions. First, we propose an
algorithm that perturbs the structure of a social graph in order to provide
link privacy, at the cost of slight reduction in the utility of the social
graph. Second we define general metrics for characterizing the utility and
privacy of perturbed graphs. Third, we evaluate the utility and privacy of our
proposed algorithm using real world social graphs. Finally, we demonstrate the
applicability of our perturbation algorithm on a broad range of secure systems,
including Sybil defenses and secure routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6198</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6198</id><created>2012-08-30</created><authors><author><keyname>Mandal</keyname><forenames>Sayonnha</forenames></author><author><keyname>Macdonald</keyname><forenames>Gregory</forenames></author><author><keyname>Rifai</keyname><forenames>Mayssaa El</forenames></author><author><keyname>Punekar</keyname><forenames>Nikhil</forenames></author><author><keyname>Zamani</keyname><forenames>Farnaz</forenames></author><author><keyname>Chen</keyname><forenames>Yuhua</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author><author><keyname>Verma</keyname><forenames>Pramode K.</forenames></author><author><keyname>Huck</keyname><forenames>Robert C</forenames></author><author><keyname>Sluss</keyname><forenames>James</forenames></author></authors><title>Implementation of Secure Quantum Protocol using Multiple Photons for
  Communication</title><categories>cs.CR quant-ph</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents the implementation of a quantum cryptography protocol for
secure communication between servers in the cloud. As computing power
increases, classical cryptography and key management schemes based on
computational complexity become increasingly susceptible to brute force and
cryptanalytic attacks. Current implementations of quantum cryptography are
based on the BB84 protocol, which is susceptible to siphoning attacks on the
multiple photons emitted by practical laser sources. The three-stage protocol,
whose implementation is described in this paper, is a departure from
conventional practice and it obviates some of the known vulnerabilities of the
current implementations of quantum cryptography. This paper presents an
implementation of the three-stage quantum communication protocol in free-space.
To the best of the authors' knowledge, this is the first implementation of a
quantum protocol where multiple photons can be used for secure communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6205</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6205</id><created>2012-08-30</created><authors><author><keyname>Sato</keyname><forenames>Aki-Hiro</forenames></author><author><keyname>Umeno</keyname><forenames>Ken</forenames></author></authors><title>Correlational properties of two-dimensional solvable chaos on the unit
  circle</title><categories>nlin.CD cs.CR</categories><comments>18 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates correlational properties of two-dimensional chaotic
maps on the unit circle. We give analytical forms of higher-order covariances.
We derive the characteristic function of their simultaneous and lagged ergodic
densities. We found that these characteristic functions are described by three
types of two-dimensional Bessel functions. Higher-order covariances between x
and y and those between y and y show non-positive values. Asymmetric features
between cosine and sine functions are elucidated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6231</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6231</id><created>2012-08-30</created><authors><author><keyname>Ermi&#x15f;</keyname><forenames>Beyza</forenames></author><author><keyname>Acar</keyname><forenames>Evrim</forenames></author><author><keyname>Cemgil</keyname><forenames>A. Taylan</forenames></author></authors><title>Link Prediction via Generalized Coupled Tensor Factorisation</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study deals with the missing link prediction problem: the problem of
predicting the existence of missing connections between entities of interest.
We address link prediction using coupled analysis of relational datasets
represented as heterogeneous data, i.e., datasets in the form of matrices and
higher-order tensors. We propose to use an approach based on probabilistic
interpretation of tensor factorisation models, i.e., Generalised Coupled Tensor
Factorisation, which can simultaneously fit a large class of tensor models to
higher-order tensors/matrices with com- mon latent factors using different loss
functions. Numerical experiments demonstrate that joint analysis of data from
multiple sources via coupled factorisation improves the link prediction
performance and the selection of right loss function and tensor model is
crucial for accurately predicting missing links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6247</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6247</id><created>2012-08-30</created><updated>2012-09-17</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author></authors><title>Solving Quadratic Equations via PhaseLift when There Are About As Many
  Equations As Unknowns</title><categories>cs.IT math.IT math.NA</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note shows that we can recover a complex vector x in C^n exactly from on
the order of n quadratic equations of the form |&lt;a_i, x&gt;|^2 = b_i, i = 1, ...,
m, by using a semidefinite program known as PhaseLift. This improves upon
earlier bounds in [3], which required the number of equations to be at least on
the order of n log n. We also demonstrate optimal recovery results from noisy
quadratic measurements; these results are much sharper than previously known
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6255</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6255</id><created>2012-08-30</created><updated>2013-02-04</updated><authors><author><keyname>Mones</keyname><forenames>Enys</forenames></author></authors><title>Hierarchy in directed random networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><journal-ref>Phys. Rev. E 87(2): 022817 (2013)</journal-ref><doi>10.1103/PhysRevE.87.022817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the theory and application of complex networks have been
quickly developing in a markable way due to the increasing amount of data from
real systems and to the fruitful application of powerful methods used in
statistical physics. Many important characteristics of social or biological
systems can be described by the study of their underlying structure of
interactions. Hierarchy is one of these features that can be formulated in the
language of networks. In this paper we present some (qualitative) analytic
results on the hierarchical properties of random network models with zero
correlations and also investigate, mainly numerically, the effects of different
type of correlations. The behavior of hierarchy is different in the absence and
the presence of the giant components. We show that the hierarchical structure
can be drastically different if there are one-point correlations in the
network. We also show numerical results suggesting that hierarchy does not
change monotonously with the correlations and there is an optimal level of
non-zero correlations maximizing the level of hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6264</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6264</id><created>2012-08-30</created><authors><author><keyname>Prus</keyname><forenames>Vladimir</forenames></author></authors><title>The Boost.Build System</title><categories>cs.SE</categories><comments>Proceedings of the 3rd Spring Young Researchers' Colloquium on
  Software Engineering (SYRCoSE) 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boost.Build is a new build system with unique approach to portability. This
paper discusses the underlying requirements, the key design decisions, and the
lessons learned during several years of development. We also review other
contemporary build systems, and why they fail to meet the same requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6268</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6268</id><created>2012-08-30</created><updated>2013-02-24</updated><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author></authors><title>Authorship Identification in Bengali Literature: a Comparative Analysis</title><categories>cs.CL cs.IR</categories><comments>9 pages, 5 tables, 4 pictures</comments><journal-ref>Chakraborty, T., Authorship Identification in Bengali Literature:
  a Comparative Analysis, Proceedings of COLING 2012: Demonstration Papers,
  December, 2012, pp. 41-50</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stylometry is the study of the unique linguistic styles and writing behaviors
of individuals. It belongs to the core task of text categorization like
authorship identification, plagiarism detection etc. Though reasonable number
of studies have been conducted in English language, no major work has been done
so far in Bengali. In this work, We will present a demonstration of authorship
identification of the documents written in Bengali. We adopt a set of
fine-grained stylistic features for the analysis of the text and use them to
develop two different models: statistical similarity model consisting of three
measures and their combination, and machine learning model with Decision Tree,
Neural Network and SVM. Experimental results show that SVM outperforms other
state-of-the-art methods after 10-fold cross validations. We also validate the
relative importance of each stylistic feature to show that some of them remain
consistently significant in every model used in this experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6269</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6269</id><created>2012-08-30</created><authors><author><keyname>Katebi</keyname><forenames>Hadi</forenames></author><author><keyname>Sakallah</keyname><forenames>Karem A.</forenames></author><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author></authors><title>Conflict Anticipation in the Search for Graph Automorphisms</title><categories>cs.DS math.GR</categories><comments>15 pages, 9 Figures, 1 Table, Int'l Conf. on Logic for Programming,
  Artificial Intelligence and Reasoning (LPAR)</comments><msc-class>68R10</msc-class><journal-ref>H. Katebi, K. A. Sakallah and I. L. Markov, &quot;Conflict Anticipation
  in the Search for Graph Automorphisms&quot; in Proc. Int'l Conf. on Logic for
  Programming, Artificial Intelligence and Reasoning (LPAR), pp. 243-257,
  Merida, Venezuela, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective search for graph automorphisms allows identifying symmetries in
many discrete structures, ranging from chemical molecules to microprocessor
circuits. Using this type of structure can enhance visualization as well as
speed up computational optimization and verification. Competitive algorithms
for the graph automorphism problem are based on efficient partition refinement
augmented with group-theoretic pruning techniques. In this paper, we improve
prior algorithms for the graph automorphism problem by introducing simultaneous
refinement of multiple partitions, which enables the anticipation of future
conflicts in search and leads to significant pruning, reducing overall
runtimes. Empirically, we observe an exponential speedup for the family of
Miyazaki graphs, which have been shown to impede leading graph-automorphism
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6271</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6271</id><created>2012-08-30</created><authors><author><keyname>Katebi</keyname><forenames>Hadi</forenames></author><author><keyname>Sakallah</keyname><forenames>Karem A.</forenames></author><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author></authors><title>Graph Symmetry Detection and Canonical Labeling: Differences and
  Synergies</title><categories>cs.DS math.GR</categories><comments>15 pages, 10 figures, 1 table, Turing-100</comments><msc-class>68R10</msc-class><journal-ref>H. Katebi, K. A. Sakallah and I. L. Markov, &quot;Graph Symmetry
  Detection and Canonical Labeling: Differences and Synergies'' in Proc.
  Turing-100, EPIC vol. 10, pp. 181-195, Manchester, UK, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetries of combinatorial objects are known to complicate search
algorithms, but such obstacles can often be removed by detecting symmetries
early and discarding symmetric subproblems. Canonical labeling of combinatorial
objects facilitates easy equivalence checking through quick matching. All
existing canonical labeling software also finds symmetries, but the fastest
symmetry-finding software does not perform canonical labeling. In this work, we
contrast the two problems and dissect typical algorithms to identify their
similarities and differences. We then develop a novel approach to canonical
labeling where symmetries are found first and then used to speed up the
canonical labeling algorithms. Empirical results show that this approach
outperforms state-of-the-art canonical labelers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6273</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6273</id><created>2012-08-30</created><authors><author><keyname>Tutuncuoglu</keyname><forenames>Kaya</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Communicating Using an Energy Harvesting Transmitter: Optimum Policies
  Under Energy Storage Losses</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications, August
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, short-term throughput optimal power allocation policies are
derived for an energy harvesting transmitter with energy storage losses. In
particular, the energy harvesting transmitter is equipped with a battery that
loses a fraction of its stored energy. Both single user, i.e. one
transmitter-one receiver, and the broadcast channel, i.e., one
transmitter-multiple receiver settings are considered, initially with an
infinite capacity battery. It is shown that the optimal policies for these
models are threshold policies. Specifically, storing energy when harvested
power is above an upper threshold, retrieving energy when harvested power is
below a lower threshold, and transmitting with the harvested energy in between
is shown to maximize the weighted sum-rate. It is observed that the two
thresholds are related through the storage efficiency of the battery, and are
nondecreasing during the transmission. The results are then extended to the
case with finite battery capacity, where it is shown that a similar
double-threshold structure arises but the thresholds are no longer monotonic. A
dynamic program that yields an optimal online power allocation is derived, and
is shown to have a similar double-threshold structure. A simpler online policy
is proposed and observed to perform close to the optimal policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6279</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6279</id><created>2012-08-30</created><updated>2013-04-08</updated><authors><author><keyname>Ai</keyname><forenames>Albert</forenames></author><author><keyname>Lapanowski</keyname><forenames>Alex</forenames></author><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>One-bit compressed sensing with non-Gaussian measurements</title><categories>cs.IT math.IT</categories><comments>20 pages, streamlined proofs, improved error bounds</comments><msc-class>94A12 (Primary) 60D05, 90C25 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In one-bit compressed sensing, previous results state that sparse signals may
be robustly recovered when the measurements are taken using Gaussian random
vectors. In contrast to standard compressed sensing, these results are not
extendable to natural non-Gaussian distributions without further assumptions,
as can be demonstrated by simple counter-examples. We show that approximately
sparse signals that are not extremely sparse can be accurately reconstructed
from single-bit measurements sampled according to a sub-gaussian distribution,
and the reconstruction comes as the solution to a convex program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6289</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6289</id><created>2012-08-29</created><authors><author><keyname>Aguilar</keyname><forenames>Jeffrey</forenames></author><author><keyname>Lesov</keyname><forenames>Alex</forenames></author><author><keyname>Wiesenfeld</keyname><forenames>Kurt</forenames></author><author><keyname>Goldman</keyname><forenames>Daniel I.</forenames></author></authors><title>Lift-off dynamics in a simple jumping robot</title><categories>physics.class-ph cs.RO nlin.CD</categories><comments>4 pages, 4 figures, Physical Review Letters, in press (2012)</comments><doi>10.1103/PhysRevLett.109.174301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study vertical jumping in a simple robot comprising an actuated
mass-spring arrangement. The actuator frequency and phase are systematically
varied to find optimal performance. Optimal jumps occur above and below (but
not at) the robot's resonant frequency $f_0$. Two distinct jumping modes
emerge: a simple jump which is optimal above $f_0$ is achievable with a squat
maneuver, and a peculiar stutter jump which is optimal below $f_0$ is generated
with a counter-movement. A simple dynamical model reveals how optimal lift-off
results from non-resonant transient dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6308</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6308</id><created>2012-08-30</created><updated>2013-02-15</updated><authors><author><keyname>Liu</keyname><forenames>Jia</forenames></author><author><keyname>Xia</keyname><forenames>Cathy H.</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author><author><keyname>Sherali</keyname><forenames>Hanif D.</forenames></author></authors><title>Distributed Cross-Layer Optimization in Wireless Networks: A
  Second-Order Approach</title><categories>cs.NI cs.DC cs.IT cs.SY math.IT math.OC</categories><comments>This paper is going to appear in IEEE INFOCOM 2013, Turin, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the rapidly growing scale and heterogeneity of wireless networks, the
design of distributed cross-layer optimization algorithms have received
significant interest from the networking research community. So far, the
standard distributed cross-layer approach in the literature is based on
first-order Lagrangian dual decomposition and the subgradient method, which
suffers a slow convergence rate. In this paper, we make the first known attempt
to develop a distributed Newton's method, which is second-order and enjoys a
quadratic convergence rate. However, due to interference in wireless networks,
the Hessian matrix of the cross-layer problem has an non-separable structure.
As a result, developing a distributed second-order algorithm is far more
challenging than its counterpart for wireline networks. Our main results in
this paper are two-fold: i) For a special network setting where all links
mutually interfere, we derive decentralized closed-form expressions to compute
the Hessian inverse; ii) For general wireless networks where the interference
relationships are arbitrary, we propose a distributed iterative matrix
splitting scheme for the Hessian inverse. These results successfully lead to a
new theoretical framework for cross-layer optimization in wireless networks.
More importantly, our work contributes to an exciting second-order paradigm
shift in wireless networks optimization theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6310</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6310</id><created>2012-08-16</created><authors><author><keyname>Topalova</keyname><forenames>Irina</forenames></author></authors><title>Automated Marble Plate Classification System Based On Different Neural
  Network Input Training Sets and PLC Implementation</title><categories>cs.NE cs.LG</categories><comments>7 pages, 11 figures, 1 table, (IJARAI) International Journal of
  Advanced Research in Artificial Intelligence, Vol. 1, No. 2, 2012;
  ISSN:2165-4069(Online), ISSN:2165-4050 (Print)</comments><journal-ref>(IJARAI) International Journal of Advanced Research in Artificial
  Intelligence, Vol. 1, No. 2, 2012, 50-56</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The process of sorting marble plates according to their surface texture is an
important task in the automated marble plate production. Nowadays some
inspection systems in marble industry that automate the classification tasks
are too expensive and are compatible only with specific technological equipment
in the plant. In this paper a new approach to the design of an Automated Marble
Plate Classification System (AMPCS),based on different neural network input
training sets is proposed, aiming at high classification accuracy using simple
processing and application of only standard devices. It is based on training a
classification MLP neural network with three different input training sets:
extracted texture histograms, Discrete Cosine and Wavelet Transform over the
histograms. The algorithm is implemented in a PLC for real-time operation. The
performance of the system is assessed with each one of the input training sets.
The experimental test results regarding classification accuracy and quick
operation are represented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6318</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6318</id><created>2012-08-30</created><updated>2013-12-12</updated><authors><author><keyname>Kuptsov</keyname><forenames>Dmitriy</forenames></author><author><keyname>Nechaev</keyname><forenames>Boris</forenames></author><author><keyname>Lukyanenko</keyname><forenames>Andrey</forenames></author><author><keyname>Gurtov</keyname><forenames>Andrei</forenames></author></authors><title>How Penalty Leads to Improvement: a Measurement Study of Wireless
  Backoff</title><categories>cs.NI</categories><comments>14 pages, 21 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite much theoretical work, different modifications of backoff protocols
in 802.11 networks lack empirical evidence demonstrating their real-life
performance. To fill the gap we have set out to experiment with performance of
exponential backoff by varying its backoff factor. Despite the satisfactory
results for throughput, we have witnessed poor fairness manifesting in severe
capture effect. The design of standard backoff protocol allows already
successful nodes to remain successful, giving little chance to those nodes that
failed to capture the channel in the beginning. With this at hand, we ask a
conceptual question: Can one improve the performance of wireless backoff by
introducing a mechanism of self-penalty, when overly successful nodes are
penalized with big contention windows? Our real-life measurements using
commodity hardware demonstrate that in many settings such mechanism not only
allows to achieve better throughput, but also assures nearly perfect fairness.
We further corroborate these results with simulations and an analytical model.
Finally, we present a backoff factor selection protocol which can be
implemented in access points to enable deployment of the penalty backoff
protocol to consumer devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6322</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6322</id><created>2012-08-30</created><authors><author><keyname>B&#xfc;sing</keyname><forenames>Christina</forenames></author><author><keyname>D'Andreagiovanni</keyname><forenames>Fabio</forenames></author></authors><title>New results about multi-band uncertainty in Robust Optimization</title><categories>math.OC cs.DS cs.NA</categories><comments>15 pages. The present paper is a revised version of the one appeared
  in the Proceedings of SEA 2012</comments><msc-class>90C05, 90C35, 90C57, 90C90</msc-class><journal-ref>Proc. of the 11th Symposium on Experimental Algorithms - SEA 2012,
  LNCS 7276 (Springer, Heidelberg, 2012) pp. 63-74</journal-ref><doi>10.1007/978-3-642-30850-5_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;The Price of Robustness&quot; by Bertsimas and Sim represented a breakthrough in
the development of a tractable robust counterpart of Linear Programming
Problems. However, the central modeling assumption that the deviation band of
each uncertain parameter is single may be too limitative in practice:
experience indeed suggests that the deviations distribute also internally to
the single band, so that getting a higher resolution by partitioning the band
into multiple sub-bands seems advisable. The critical aim of our work is to
close the knowledge gap about the adoption of a multi-band uncertainty set in
Robust Optimization: a general definition and intensive theoretical study of a
multi-band model are actually still missing. Our new developments have been
also strongly inspired and encouraged by our industrial partners, which have
been interested in getting a better modeling of arbitrary distributions, built
on historical data of the uncertainty affecting the considered real-world
problems. In this paper, we study the robust counterpart of a Linear
Programming Problem with uncertain coefficient matrix, when a multi-band
uncertainty set is considered. We first show that the robust counterpart
corresponds to a compact LP formulation. Then we investigate the problem of
separating cuts imposing robustness and we show that the separation can be
efficiently operated by solving a min-cost flow problem. Finally, we test the
performance of our new approach to Robust Optimization on realistic instances
of a Wireless Network Design Problem subject to uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6324</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6324</id><created>2012-08-30</created><updated>2013-10-22</updated><authors><author><keyname>Klimann</keyname><forenames>Ines</forenames><affiliation>LIAFA</affiliation></author></authors><title>The finiteness of a group generated by a 2-letter invertible-reversible
  Mealy automaton is decidable</title><categories>cs.FL math.GR</categories><proxy>ccsd</proxy><journal-ref>30th International Symposium on Theoretical Aspects of Computer
  Science (STACS 2013), Kiel : Germany (2013)</journal-ref><doi>10.4230/LIPIcs.STACS.2013.502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a semigroup generated by a reversible two-state Mealy automaton
is either finite or free of rank 2. This fact leads to the decidability of
finiteness for groups generated by two-state or two-letter
invertible-reversible Mealy automata and to the decidability of freeness for
semigroups generated by two-state invertible-reversible Mealy automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6326</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6326</id><created>2012-08-30</created><authors><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Pisces: Anonymous Communication Using Social Networks</title><categories>cs.CR cs.NI cs.SI</categories><comments>15 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The architectures of deployed anonymity systems such as Tor suffer from two
key problems that limit user's trust in these systems. First, paths for
anonymous communication are built without considering trust relationships
between users and relays in the system. Second, the network architecture relies
on a set of centralized servers. In this paper, we propose Pisces, a
decentralized protocol for anonymous communications that leverages users'
social links to build circuits for onion routing. We argue that such an
approach greatly improves the system's resilience to attackers.
  A fundamental challenge in this setting is the design of a secure process to
discover peers for use in a user's circuit. All existing solutions for secure
peer discovery leverage structured topologies and cannot be applied to
unstructured social network topologies. In Pisces, we discover peers by using
random walks in the social network graph with a bias away from highly connected
nodes to prevent a few nodes from dominating the circuit creation process. To
secure the random walks, we leverage the reciprocal neighbor policy: if
malicious nodes try to exclude honest nodes during peer discovery so as to
improve the chance of being selected, then honest nodes can use a tit-for-tat
approach and reciprocally exclude the malicious nodes from their routing
tables. We describe a fully decentralized protocol for enforcing this policy,
and use it to build the Pisces anonymity system.
  Using theoretical modeling and experiments on real-world social network
topologies, we show that (a) the reciprocal neighbor policy mitigates active
attacks that an adversary can perform, (b) our decentralized protocol to
enforce this policy is secure and has low overhead, and (c) the overall
anonymity provided by our system significantly outperforms existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6335</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6335</id><created>2012-08-30</created><authors><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Mallik</keyname><forenames>Sushmit</forenames></author><author><keyname>Johar</keyname><forenames>Ravdeep</forenames></author></authors><title>Comparative Study and Optimization of Feature-Extraction Techniques for
  Content based Image Retrieval</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Applications 52(20):35-42, 2012</journal-ref><doi>10.5120/8320-1959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query
by Image Content (QBIC), is to help users to retrieve relevant images based on
their contents. CBIR technologies provide a method to find images in large
databases by using unique descriptors from a trained image. The image
descriptors include texture, color, intensity and shape of the object inside an
image. Several feature-extraction techniques viz., Average RGB, Color Moments,
Co-occurrence, Local Color Histogram, Global Color Histogram and Geometric
Moment have been critically compared in this paper. However, individually these
techniques result in poor performance. So, combinations of these techniques
have also been evaluated and results for the most efficient combination of
techniques have been presented and optimized for each class of image query. We
also propose an improvement in image retrieval performance by introducing the
idea of Query modification through image cropping. It enables the user to
identify a region of interest and modify the initial query to refine and
personalize the image retrieval results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6338</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6338</id><created>2012-08-30</created><authors><author><keyname>Watanabe</keyname><forenames>Sumio</forenames></author></authors><title>A Widely Applicable Bayesian Information Criterion</title><categories>cs.LG stat.ML</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A statistical model or a learning machine is called regular if the map taking
a parameter to a probability distribution is one-to-one and if its Fisher
information matrix is always positive definite. If otherwise, it is called
singular. In regular statistical models, the Bayes free energy, which is
defined by the minus logarithm of Bayes marginal likelihood, can be
asymptotically approximated by the Schwarz Bayes information criterion (BIC),
whereas in singular models such approximation does not hold.
  Recently, it was proved that the Bayes free energy of a singular model is
asymptotically given by a generalized formula using a birational invariant, the
real log canonical threshold (RLCT), instead of half the number of parameters
in BIC. Theoretical values of RLCTs in several statistical models are now being
discovered based on algebraic geometrical methodology. However, it has been
difficult to estimate the Bayes free energy using only training samples,
because an RLCT depends on an unknown true distribution.
  In the present paper, we define a widely applicable Bayesian information
criterion (WBIC) by the average log likelihood function over the posterior
distribution with the inverse temperature $1/\log n$, where $n$ is the number
of training samples. We mathematically prove that WBIC has the same asymptotic
expansion as the Bayes free energy, even if a statistical model is singular for
and unrealizable by a statistical model. Since WBIC can be numerically
calculated without any information about a true distribution, it is a
generalized version of BIC onto singular statistical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6342</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6342</id><created>2012-08-30</created><authors><author><keyname>Hughes</keyname><forenames>Dominic J. D.</forenames></author></authors><title>Is Wolfram and Cook's (2,5) Turing machine really universal?</title><categories>cs.FL cs.LO math.LO</categories><comments>13-page draft. Languished untouched since 2007. Seek co-author to dot
  'i's and cross 't's. Email if interested</comments><msc-class>03D10, 68Q05</msc-class><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wolfram [2, p. 707] and Cook [1, p. 3] claim to prove that a (2,5) Turing
machine (2 states, 5 symbols) is universal, via a universal cellular automaton
known as Rule 110. The first part of this paper points out a critical gap in
their argument. The second part bridges the gap, thereby giving what appears to
be the first proof of universality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6357</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6357</id><created>2012-08-30</created><authors><author><keyname>Razaviyayn</keyname><forenames>Meisam</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Linear Transceiver Design for a MIMO Interfering Broadcast Channel
  Achieving Max-Min Fairness</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of linear transceiver design to achieve max-min
fairness in a downlink MIMO multicell network. This problem can be formulated
as maximizing the minimum rate among all the users in an interfering broadcast
channel (IBC). In this paper we show that when the number of antennas is at
least two at each of the transmitters and the receivers, the min rate
maximization problem is NP-hard in the number of users. Moreover, we develop a
low-complexity algorithm for this problem by iteratively solving a sequence of
convex subproblems, and establish its global convergence to a stationary point
of the original minimum rate maximization problem. Numerical simulations show
that this algorithm is efficient in achieving fairness among all the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6360</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6360</id><created>2012-08-30</created><authors><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Transmission Mode Selection for Downlink Coordinated Multi-Point Systems</title><categories>cs.NI</categories><comments>7 pages, 6 figures, accepted as IEEE TVT correspondence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated multi-point (CoMP) transmission has been widely recognized as a
spectrally efficient technique in future cellular systems. To exploit the
abundant patial resources provided by the cooperating base stations, however,
considerable training overhead is required to acquire the channel information.
To avoid the extra overhead outweighing the cooperative gain, we propose a
method that allows each user to select transmission mode between coherent CoMP
and Non-CoMP. We first analyze the average throughput of each user under CoMP
and Non-CoMP transmission after taking into account the downlink training
overhead. A closed-form mode selection rule is then developed, which depends on
the user location and system settings, i.e, the number of cooperating base
stations and transmit antennas, training overhead and cell-edge signal to noise
ratio. Simulation results show that the proposed downlink transmission mode
selection method achieves higher throughput than CoMP for cell-center users and
than Non-CoMP for cell-edge users after accounting for the overhead. As a
by-product, the backhaul load is also reduced significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6363</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6363</id><created>2012-08-30</created><authors><author><keyname>Kazakovtsev</keyname><forenames>Lev</forenames></author></authors><title>Access Points Placement as a Discrete Optimization Problem</title><categories>cs.NI cs.DM</categories><comments>Presented at 2008 ICTP-ITU School on New Perspectives on Wireless
  Networking (Trieste, Italy) 10 pages, 5 figures; ICTP e-prints Archive, 2008,
  http://eprints.ictp.it/494/01/AP_Placement.pdf</comments><msc-class>68M10 (Primary) 90B80, 90C27 (Secondary)</msc-class><acm-class>C.2.1; G.2.1; I.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a method of searching of the direction of a
wireless network development (the places of new access points or base stations
etc.) optimized with criteria of coverage of important territories and minimum
cost of equipment and additional needed infrastructure which does not need the
execution of special field testings and determination of the exact geometry of
elements of RF-propagation medium and their RF absorbing properties but takes
into account the minimum accessible information obtained from built-in
measuring instruments of wireless hardware and approximate data of the medium
elements shape. The problem of search of a disposition and types of the
infrastructure elements of the growing network is formulated as a multicriteria
discrete constrained optimization problem solvable with variant probability
method [1]. The problem of a medium RF-propagation properties modeling is also
formulated and solved as a discrete optimization task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6376</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6376</id><created>2012-08-31</created><authors><author><keyname>Durand</keyname><forenames>Fabien</forenames><affiliation>LAMFA</affiliation></author><author><keyname>Leroy</keyname><forenames>Julien</forenames><affiliation>LAMFA</affiliation></author><author><keyname>Richomme</keyname><forenames>Gw&#xe9;na&#xeb;l</forenames><affiliation>LIRMM</affiliation></author></authors><title>Towards a statement of the S-adic conjecture through examples</title><categories>cs.DM math.CO</categories><comments>25</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $S$-adic conjecture claims that there exists a condition $C$ such that a
sequence has a sub-linear complexity if and only if it is an $S$-adic sequence
satisfying Condition $C$ for some finite set $S$ of morphisms. We present an
overview of the factor complexity of $S$-adic sequences and we give some
examples that either illustrate some interesting properties or that are
counter-examples to what could be believed to be &quot;a good Condition $C$&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6379</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6379</id><created>2012-08-31</created><authors><author><keyname>Long</keyname><forenames>Jean-Alexandre</forenames><affiliation>TIMC</affiliation></author><author><keyname>Hungr</keyname><forenames>Nikolai</forenames><affiliation>TIMC</affiliation></author><author><keyname>Baumann</keyname><forenames>Michael</forenames><affiliation>TIMC</affiliation></author><author><keyname>Descotes</keyname><forenames>Jean-Luc</forenames><affiliation>TIMC</affiliation></author><author><keyname>Bolla</keyname><forenames>Michel</forenames><affiliation>TIMC</affiliation></author><author><keyname>Giraud</keyname><forenames>Jean-Yves</forenames><affiliation>TIMC</affiliation></author><author><keyname>Rambeaud</keyname><forenames>Jean-Jacques</forenames><affiliation>TIMC</affiliation></author><author><keyname>Troccaz</keyname><forenames>Jocelyne</forenames><affiliation>TIMC</affiliation></author></authors><title>Development of a Novel Robot for Transperineal Needle Based
  Interventions: Focal Therapy, Brachytherapy and Prostate Biopsies</title><categories>cs.RO physics.med-ph</categories><proxy>ccsd</proxy><journal-ref>Journal of Urology 188 (2012) 1369-1374</journal-ref><doi>10.1016/j.juro.2012.06.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: We report what is to our knowledge the initial experience with a new
3-dimensional ultrasound robotic system for prostate brachytherapy assistance,
focal therapy and prostate biopsies. Its ability to track prostate motion
intraoperatively allows it to manage motions and guide needles to predefined
targets. Materials and Methods: A robotic system was created for transrectal
ultrasound guided needle implantation combined with intraoperative prostate
tracking. Experiments were done on 90 targets embedded in a total of 9 mobile,
deformable, synthetic prostate phantoms. Experiments involved trying to insert
glass beads as close as possible to targets in multimodal anthropomorphic
imaging phantoms. Results were measured by segmenting the inserted beads in
computerized tomography volumes of the phantoms. Results: The robot reached the
chosen targets in phantoms with a median accuracy of 2.73 mm and a median
prostate motion of 5.46 mm. Accuracy was better at the apex than at the base
(2.28 vs 3.83 mm, p &lt;0.001), and similar for horizontal and angled needle
inclinations (2.7 vs 2.82 mm, p = 0.18). Conclusions: To our knowledge this
robot for prostate focal therapy, brachytherapy and targeted prostate biopsies
is the first system to use intraoperative prostate motion tracking to guide
needles into the prostate. Preliminary experiments show its ability to reach
targets despite prostate motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6388</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6388</id><created>2012-08-31</created><authors><author><keyname>Long</keyname><forenames>Jean-Alexandre</forenames><affiliation>TIMC</affiliation></author><author><keyname>Tostain</keyname><forenames>Jacques</forenames><affiliation>TIMC</affiliation></author><author><keyname>Lanchon</keyname><forenames>Cecilia</forenames><affiliation>TIMC</affiliation></author><author><keyname>Voros</keyname><forenames>Sandrine</forenames><affiliation>TIMC</affiliation></author><author><keyname>Medici</keyname><forenames>Maud</forenames><affiliation>TIMC</affiliation></author><author><keyname>Descotes</keyname><forenames>Jean-Luc</forenames><affiliation>TIMC</affiliation></author><author><keyname>Troccaz</keyname><forenames>Jocelyne</forenames><affiliation>TIMC</affiliation></author><author><keyname>Cinquin</keyname><forenames>Philippe</forenames><affiliation>TIMC</affiliation></author><author><keyname>Rambeaud</keyname><forenames>Jean-Jacques</forenames><affiliation>TIMC</affiliation></author><author><keyname>Moreau-Gaudry</keyname><forenames>Alexandre</forenames><affiliation>TIMC</affiliation></author></authors><title>First Clinical Experience in Urologic Surgery with a Novel Robotic
  Lightweight Laparoscope Holder</title><categories>cs.RO physics.med-ph</categories><comments>Journal of Endourology and Part B, Videourology (2012) epub ahead of
  print</comments><proxy>ccsd</proxy><doi>10.1089/end.2012.0357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: To report the feasibility and the safety of a surgeon-controlled
robotic endoscope holder in laparoscopic surgery. Materials and methods: From
March 2010 to September 2010, 20 patients were enrolled prospectively to
undergo a laparoscopic surgery using an innovative robotic endoscope holder.
Two surgeons performed 6 adrenalectomies, 4 sacrocolpopexies, 5 pyeloplasties,
4 radical prostatectomies and 1 radical nephrectomy. Demographic data, overall
set-up time, operative time, number of assistants needed were reviewed.
Surgeon's satisfaction regarding the ergonomics was assessed using a ten point
scale. Postoperative clinical outcomes were reviewed at day 1 and 1 month
postoperatively. Results: The per-protocol analysis was performed on 17
patients for whom the robot was effectively used for surgery. Median age was 63
years, 10 patients were female (59%). Median BMI was 26.8. Surgical procedures
were completed with the robot in 12 cases (71 %). Median number of surgical
assistant was 0. Overall set-up time with the robot was 19 min, operative time
was 130 min) during which the robot was used 71% of the time. Mean hospital
stay was 6.94 days $\pm$ 2.3. Median score regarding the easiness of use was 7.
Median pain level was 1.5/10 at day 1 and 0 at 1 month postoperatively. Open
conversion was needed in 1 case (6 %) and 4 minor complications occurred in 2
patients (12%). Conclusion: This use of this novel robotic laparoscope holder
is safe, feasible and it provides a good comfort to the surgeon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6389</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6389</id><created>2012-08-31</created><authors><author><keyname>Benmoussa</keyname><forenames>Yahia</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Aoul</keyname><forenames>Yassine Hadjadj</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Lagadec</keyname><forenames>Lo&#xef;c</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Benazzouz</keyname><forenames>Djamel</forenames><affiliation>LMSS</affiliation></author></authors><title>Behavioral Systel Level Power Consumption Modeling of Mobile Video
  Streaming applications</title><categories>cs.MM</categories><comments>Colloque du GDR SoC SiP, Paris : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the use of mobile applications and terminals faces fundamental
challenges related to energy constraint. This is due to the limited battery
lifetime as compared to the increasing hardware evolution. Video streaming is
one of the most energy consuming applications in a mobile system because of its
intensive use of bandwidth, memory and processing power. In this work, we aim
to propose a methodology for building and validating a high level global power
consumption model including a hardware and software elements. Our approach is
based on exploiting the interactions between power consumption sub-models of
standalone systems in the perspective to build more accurate global model. The
interactions are studied within the exclusive context of video streaming
applications that are one of the most used mobile applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6390</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6390</id><created>2012-08-31</created><authors><author><keyname>Olivier</keyname><forenames>Pierre</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Senn</keyname><forenames>Eric</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>Performance Evaluation of Flash File Systems</title><categories>cs.OS</categories><comments>Colloque du GDR SoC-SiP, Paris : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, flash memory are strongly used in the embedded system domain. NAND
flash memories are the building block of main secondary storage systems. Such
memories present many benefits in terms of data density, I/O performance, shock
resistance and power consumption. Nevertheless, flash does not come without
constraints: the write / erase granularity asymmetry and the limited lifetime
bring the need for specific management. This can be done through the operating
system using dedicated Flash File Systems (FFSs). In this document, we present
general concepts about FFSs, and implementations example that are JFFS2, YAFFS2
and UBIFS, the most commonly used flash file systems. Then we give performance
evaluation results for these FFSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6391</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6391</id><created>2012-08-31</created><authors><author><keyname>Olivier</keyname><forenames>Pierre</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Senn</keyname><forenames>Eric</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>On Benchmarking Embedded Linux Flash File Systems</title><categories>cs.OS cs.PF</categories><comments>Embed With Linux, Lorient : France (2012)</comments><proxy>ccsd</proxy><journal-ref>ACM SIGBED Review 9(2) 43-47 9, 2 (2012) 43-47</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its attractive characteristics in terms of performance, weight and
power consumption, NAND flash memory became the main non volatile memory (NVM)
in embedded systems. Those NVMs also present some specific
characteristics/constraints: good but asymmetric I/O performance, limited
lifetime, write/erase granularity asymmetry, etc. Those peculiarities are
either managed in hardware for flash disks (SSDs, SD cards, USB sticks, etc.)
or in software for raw embedded flash chips. When managed in software, flash
algorithms and structures are implemented in a specific flash file system
(FFS). In this paper, we present a performance study of the most widely used
FFSs in embedded Linux: JFFS2, UBIFS,and YAFFS. We show some very particular
behaviors and large performance disparities for tested FFS operations such as
mounting, copying, and searching file trees, compression, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6406</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6406</id><created>2012-08-31</created><authors><author><keyname>Kedia</keyname><forenames>Piyus</forenames></author><author><keyname>Bansal</keyname><forenames>Sorav</forenames></author><author><keyname>Deshpande</keyname><forenames>Deepak</forenames></author><author><keyname>Iyer</keyname><forenames>Sreekanth</forenames></author></authors><title>Building Resilient Cloud Over Unreliable Commodity Infrastructure</title><categories>cs.OS cs.DC</categories><comments>Oral presentation at IEEE &quot;Cloud Computing for Emerging Markets&quot;,
  Oct. 11-12, 2012, Bangalore, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing has emerged as a successful computing paradigm for
efficiently utilizing managed compute infrastructure such as high speed
rack-mounted servers, connected with high speed networking, and reliable
storage. Usually such infrastructure is dedicated, physically secured and has
reliable power and networking infrastructure. However, much of our idle compute
capacity is present in unmanaged infrastructure like idle desktops, lab
machines, physically distant server machines, and laptops. We present a scheme
to utilize this idle compute capacity on a best-effort basis and provide high
availability even in face of failure of individual components or facilities.
  We run virtual machines on the commodity infrastructure and present a cloud
interface to our end users. The primary challenge is to maintain availability
in the presence of node failures, network failures, and power failures. We run
multiple copies of a Virtual Machine (VM) redundantly on geographically
dispersed physical machines to achieve availability. If one of the running
copies of a VM fails, we seamlessly switchover to another running copy. We use
Virtual Machine Record/Replay capability to implement this redundancy and
switchover. In current progress, we have implemented VM Record/Replay for
uniprocessor machines over Linux/KVM and are currently working on VM
Record/Replay on shared-memory multiprocessor machines. We report initial
experimental results based on our implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6408</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6408</id><created>2012-08-31</created><updated>2012-10-03</updated><authors><author><keyname>Misra</keyname><forenames>Janardan</forenames></author></authors><title>Java Source-code Clustering: Unifying Syntactic and Semantic Features</title><categories>cs.SE</categories><comments>This paper has been withdrawn to rectify crucial mistakes in
  authoring details</comments><acm-class>D.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a companion draft to the paper 'Software Clustering: Unifying
Syntactic and Semantic Features', 19th Working Conference on Reverse
Engineering (WCRE 2012). It discusses software clustering process in detail,
which appeared in the paper in an abridged form. It also contains certain
additional process steps which were not covered in the WCRE paper. The
clustering process is described for applications with Java source-code.
However, as argued in the WCRE paper, it can be seamlessly adapted to many
other programming paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6412</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6412</id><created>2012-08-31</created><authors><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>Joo</keyname><forenames>Hyun-Seung</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>Adaptive Generation Method of OFDM Signals in SLM Schemes for
  Low-complexity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many selected mapping (SLM) schemes to reduce the peak-to-average
power ratio (PAPR) of orthogonal frequency division multiplexing (OFDM)
signals. Beginning with the conventional SLM scheme, there have been proposed
many low-complexity SLM schemes including Lim's, Wang's, and Baxely's SLM
schemes typically. In this paper, we propose an adaptive generation (AG) method
of OFDM signals in SLM schemes. By generating the alternative OFDM signals
adaptively, unnecessary computational complexity of SLM schemes can be removed
without any degradation of their PAPR reduction performance. In this paper, we
apply the AG method to various SLM schemes which are the conventional SLM
scheme and its low-complexity versions such as Lim's, Wang's, and Baxely's SLM
schemes. Of course, the AG method can be applied to most of existing SLM
schemes easily. The numerical results show that the AG method can reduce their
computational complexity substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6416</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6416</id><created>2012-08-31</created><updated>2013-07-12</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Relational Databases and Bell's Theorem</title><categories>cs.LO cs.DB quant-ph</categories><comments>19 pages. To appear in Festschrift for Peter Buneman</comments><journal-ref>In Search of Elegance in the Theory and Practice of Computation:
  Essays dedicated to Peter Buneman, ed. V. Tannen, L. Wong, L. Libkin, W. Fan,
  W.C. Tan and M. Fourman, Springer, pages 13-35, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aim in this paper is to point out a surprising formal connection, between
two topics which seem on face value to have nothing to do with each other:
relational database theory, and the study of non-locality and contextuality in
the foundations of quantum mechanics. We shall show that there is a remarkably
direct correspondence between central results such as Bell's theorem in the
foundations of quantum mechanics, and questions which arise naturally and have
been well-studied in relational database theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6421</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6421</id><created>2012-08-31</created><authors><author><keyname>Ojha</keyname><forenames>Muneendra</forenames></author></authors><title>A Novel Service Oriented Model for Query Identification and Solution
  Development using Semantic Web and Multi Agent System</title><categories>cs.MA cs.SE</categories><comments>7 pages, 4 figures, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 35(9):12-18, 2011</journal-ref><doi>10.5120/4428-6165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to develop service model architecture by merging
multi-agentsystems and semantic web technology. The proposed architecture works
in two stages namely, Query Identification and Solution Development. A person
referred to as customer will submit the problem details or requirements which
will be referred to as a query. Anyone who can provide a service will need to
register with the registrar module of the architecture. Services can be
anything ranging from expert consultancy in the field of agriculture to
academic research, from selling products to manufacturing goods, from medical
help to legal issues or even providing logistics. Query submitted by customer
is first parsed and then iteratively understood with the help of domain experts
and the customer to get a precise set of properties. Query thus identified will
be solved again with the help of intelligent agent systems which will search
the semantic web for all those who can find or provide a solution. A workable
solution workflow is created and then depending on the requirements, using the
techniques of negotiation or auctioning, solution is implemented to complete
the service for customer. This part is termed as solution development. In this
service oriented architecture, we first try to analyze the complex set of user
requirements then try to provide best possible solution in an optimized way by
combining better information searches through semantic web and better workflow
provisioning using multi agent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6428</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6428</id><created>2012-08-31</created><authors><author><keyname>Olivier</keyname><forenames>Pierre</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>A Hardware Time Manager Implementation for the Xenomai Real-Time Kernel
  of Embedded Linux</title><categories>cs.OS cs.AR cs.PF</categories><comments>Embed With Linux (EWiLi) workshop, Lorient : France (2012)</comments><proxy>ccsd</proxy><journal-ref>ACM SIGBED Review 9(2) 38-42 9, 2 (2012) 38-42</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the use of embedded operating systems in different embedded
projects is subject to a tremendous growth. Embedded Linux is becoming one of
those most popular EOSs due to its modularity, efficiency, reliability, and
cost. One way to make it hard real-time is to include a real-time kernel like
Xenomai. One of the key characteristics of a Real-Time Operating System (RTOS)
is its ability to meet execution time deadlines deterministically. So, the more
precise and flexible the time management can be, the better it can handle
efficiently the determinism for different embedded applications. RTOS time
precision is characterized by a specific periodic interrupt service controlled
by a software time manager. The smaller the period of the interrupt, the better
the precision of the RTOS, the more it overloads the CPU, and though reduces
the overall efficiency of the RTOS. In this paper, we propose to drastically
reduce these overheads by migrating the time management service of Xenomai into
a configurable hardware component to relieve the CPU. The hardware component is
implemented in a Field Programmable Gate Array coupled to the CPU. This work
was achieved in a Master degree project where students could apprehend many
fields of embedded systems: RTOS programming, hardware design, performance
evaluation, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6444</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6444</id><created>2012-08-31</created><updated>2013-05-23</updated><authors><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Zasada</keyname><forenames>Stefan J.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Survey of Multiscale and Multiphysics Applications and Communities</title><categories>cs.OH physics.soc-ph</categories><comments>12 pages, 6 figures, 2 tables. Accepted by CiSE (with a constrained
  number of references; these were put in a separate literature list)</comments><acm-class>D.0; D.2.11; D.2.12; I.6; I.6.3; I.6.5; I.6.8; J.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiscale and multiphysics applications are now commonplace, and many
researchers focus on combining existing models to construct combined multiscale
models. Here we present a concise review of multiscale applications and their
source communities. We investigate the prevalence of multiscale projects in the
EU and the US, review a range of coupling toolkits they use to construct
multiscale models and identify areas where collaboration between disciplines
could be particularly beneficial. We conclude that multiscale computing has
become increasingly popular in recent years, that different communities adopt
very different approaches to constructing multiscale simulations, and that
simulations on a length scale of a few metres and a time scale of a few hours
can be found in many of the multiscale research domains. Communities may
receive additional benefit from sharing methods that are geared towards these
scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6454</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6454</id><created>2012-08-31</created><authors><author><keyname>Venkatramanan</keyname><forenames>Srinivasan</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>Spread of Influence and Content in Mobile Opportunistic Networks</title><categories>cs.SI cs.MA cs.SY</categories><comments>16 pages, 19 Figures. arXiv admin note: substantial text overlap with
  arXiv:1107.5851</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting in which a single item of content (such as a song or a
video clip) is disseminated in a population of mobile nodes by opportunistic
copying when pairs of nodes come in radio contact. We propose and study models
that capture the joint evolution of the population of nodes interested in the
content (referred to as destinations), and the population of nodes that possess
the content. The evolution of interest in the content is captured using an
influence spread model and the content spread occurs via epidemic copying.
Nodes not yet interested in the content are called relays; the influence spread
process converts relays into destinations. We consider the decentralized
setting, where interest in the content and the spread of the content evolve by
pairwise interactions between the mobiles. We derive fluid limits for the joint
evolution models and obtain optimal policies for copying to relay nodes in
order to deliver content to a desired fraction of destinations. We prove that a
time-threshold policy is optimal while copying to relays. We then provide
insights into the effects of various system parameters on the co-evolution
model through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6464</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6464</id><created>2012-08-31</created><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author></authors><title>Bayesian compressed sensing with new sparsity-inducing prior</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Bayesian learning (SBL) is a popular approach to sparse signal
recovery in compressed sensing (CS). In SBL, the signal sparsity information is
exploited by assuming a sparsity-inducing prior for the signal that is then
estimated using Bayesian inference. In this paper, a new sparsity-inducing
prior is introduced and efficient algorithms are developed for signal recovery.
The main algorithm is shown to produce a sparser solution than existing SBL
methods while preserving their desirable properties. Numerical simulations with
one-dimensional synthetic signals and two-dimensional images verify our
analysis and show that for sparse signals the proposed algorithm outperforms
its SBL peers in both the signal recovery accuracy and computational speed. Its
improved performance is also demonstrated in comparison with other
state-of-the-art methods in CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6465</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6465</id><created>2012-08-31</created><authors><author><keyname>Kazakovtsev</keyname><forenames>Lev</forenames></author></authors><title>Parallel Random Search Algorithm of Constrained Pseudo-Boolean
  Optimization for Some Distinctive Large-Scale Problems</title><categories>cs.DC cs.MS</categories><comments>In: Advanced School in High Performance and GRID Computing - Concepts
  and Applications, 2009 (ICTP, Trieste, Italy); ICTP Open Access Archive,
  2009, http://eprints.ictp.it/508/</comments><msc-class>90C27 (Primary) 65Y05, 90C29, 90C27 (Secondary)</msc-class><acm-class>G.2.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an approach to the parallelizing of the algorithms
realizing the modified probability changigng method with adaptation and partial
rollback procedure for constrained pseudo-Boolean optimization problems.
Existing optimization algorithms are adapted for the shared memory and clusters
(PVM library). The parallel efficiency is estimated for the lagre-scale
non-linear pseudo-Boolean optimization problems with linear constraints.
Initially designed for unconstrained optimization, the probability changing
method (MIVER) allows us finding the approximate solution of different linear
and non-linear pseudo-Boolean optimization problems with constraints. Although,
in case of large-scale problems, the computational demands are also very high
and the precision of the result depends on the time spent. In case of the
constrained optimization problem, even the search of any permissibly solution
can take very large computational resources. The rapid development of the
parallel processor systems which are often implemented even in the computer
systems designed for home use allows to reduce significantly the time spent to
find the acceptable solution with a speed-up close to ideal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6483</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6483</id><created>2012-08-31</created><updated>2012-10-10</updated><authors><author><keyname>Denielou</keyname><forenames>Pierre-Malo</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Bejleri</keyname><forenames>Andi</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Hu</keyname><forenames>Raymond</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Parameterised Multiparty Session Types</title><categories>cs.LO</categories><comments>LMCS 2012</comments><proxy>LMCS</proxy><acm-class>F.3.3, D.1.3, F.1.1, F.1.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  11, 2012) lmcs:924</journal-ref><doi>10.2168/LMCS-8(4:6)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many application-level distributed protocols and parallel algorithms, the
set of participants, the number of messages or the interaction structure are
only known at run-time. This paper proposes a dependent type theory for
multiparty sessions which can statically guarantee type-safe, deadlock-free
multiparty interactions among processes whose specifications are parameterised
by indices. We use the primitive recursion operator from G\&quot;odel's System T to
express a wide range of communication patterns while keeping type checking
decidable. To type individual distributed processes, a parameterised global
type is projected onto a generic generator which represents a class of all
possible end-point types. We prove the termination of the type-checking
algorithm in the full system with both multiparty session types and recursive
types. We illustrate our type theory through non-trivial programming and
verification examples taken from parallel algorithms and Web services usecases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6493</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6493</id><created>2012-08-31</created><updated>2013-04-24</updated><authors><author><keyname>Sasane</keyname><forenames>Amol</forenames></author></authors><title>Shannon's sampling theorem in a distributional setting</title><categories>math.FA cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to an error in a
  claim about singular supports in the proof</comments><msc-class>41A05 (Primary) 46F05, 94A12 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical Shannon sampling theorem states that a signal f with Fourier
transform F in L^2(R) having its support contained in (-\pi,\pi) can be
recovered from the sequence of samples (f(n))_{n in Z} via f(t)=\sum_{n in Z}
f(n) (sin(\pi (t -n)))/(\pi (t-n)) (t in R). In this article we prove a
generalization of this result under the assumption that F is a compactly
supported distribution with its support contained in (-\pi,\pi).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6501</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6501</id><created>2012-08-31</created><updated>2012-09-21</updated><authors><author><keyname>Guo</keyname><forenames>Mingyu</forenames></author><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author></authors><title>False-name-proofness with Bid Withdrawal</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a more powerful variant of false-name manipulation in Internet
auctions: an agent can submit multiple false-name bids, but then, once the
allocation and payments have been decided, withdraw some of her false-name
identities (have some of her false-name identities refuse to pay). While these
withdrawn identities will not obtain the items they won, their initial presence
may have been beneficial to the agent's other identities. We define a mechanism
to be false-name-proof with withdrawal (FNPW) if the aforementioned
manipulation is never beneficial. FNPW is a stronger condition than
false-name-proofness (FNP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6516</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6516</id><created>2012-08-31</created><authors><author><keyname>Salmon</keyname><forenames>Joseph</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author></authors><title>A two-stage denoising filter: the preprocessed Yaroslavsky filter</title><categories>cs.CV math.ST stat.TH</categories><acm-class>I.4.3; I.4.10; I.5.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a simple image noise removal method which combines a
preprocessing step with the Yaroslavsky filter for strong numerical, visual,
and theoretical performance on a broad class of images. The framework developed
is a two-stage approach. In the first stage the image is filtered with a
classical denoising method (e.g., wavelet or curvelet thresholding). In the
second stage a modification of the Yaroslavsky filter is performed on the
original noisy image, where the weights of the filters are governed by pixel
similarities in the denoised image from the first stage. Similar prefiltering
ideas have proved effective previously in the literature, and this paper
provides theoretical guarantees and important insight into why prefiltering can
be effective. Empirically, this simple approach achieves very good performance
for cartoon images, and can be computed much more quickly than current
patch-based denoising algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6523</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6523</id><created>2012-08-31</created><authors><author><keyname>Reininghaus</keyname><forenames>Jan</forenames><affiliation>Institute for Science and Technology Austria</affiliation></author><author><keyname>G&#xfc;nther</keyname><forenames>David</forenames><affiliation>MPI for Informatics Germany</affiliation></author><author><keyname>Hotz</keyname><forenames>Ingrid</forenames><affiliation>Zuse-Insitute Berlin Germany</affiliation></author><author><keyname>Weinkauf</keyname><forenames>Tino</forenames><affiliation>MPI for Informatics Germany</affiliation></author><author><keyname>Seidel</keyname><forenames>Hans Peter</forenames><affiliation>MPI for Informatics Germany</affiliation></author></authors><title>Combinatorial Gradient Fields for 2D Images with Empirically Convergent
  Separatrices</title><categories>cs.CV cs.CG cs.DM</categories><comments>17 pages, 7 figures</comments><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient probabilistic method that computes
combinatorial gradient fields for two dimensional image data. In contrast to
existing algorithms, this approach yields a geometric Morse-Smale complex that
converges almost surely to its continuous counterpart when the image resolution
is increased. This approach is motivated using basic ideas from probability
theory and builds upon an algorithm from discrete Morse theory with a strong
mathematical foundation. While a formal proof is only hinted at, we do provide
a thorough numerical evaluation of our method and compare it to established
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1208.6589</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1208.6589</id><created>2012-08-31</created><authors><author><keyname>Temme</keyname><forenames>Kristan</forenames></author><author><keyname>Wocjan</keyname><forenames>Pawel</forenames></author></authors><title>Efficient Computation of the Permanent of Block Factorizable Matrices</title><categories>cs.DM cs.DS quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm for computing the permanent for matrices of
size N that can written as a product of L block diagonal matrices with blocks
of size at most 2. For fixed L, the time and space resources scale linearly in
N, with a prefactor that scales exponentially in L. This class of matrices
contains banded matrices with banded inverse. We show that such a factorization
into a product of block diagonal matrices gives rise to a circuit acting on a
Hilbert space with a tensor product structure and that the permanent is equal
to the transition amplitude of this circuit and a product basis state. In this
correspondence, a block diagonal matrix gives rise to one layer of the circuit,
where each block to a gate acting either on a single tensor component or on two
adjacent tensor components. This observation allows us to adopt matrix product
states, a computational method from condensed matter physics and quantum
information theory used to simulate quantum systems, to evaluate the transition
amplitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0001</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0001</id><created>2012-08-30</created><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>An Improved Bound for the Nystrom Method for Large Eigengap</title><categories>cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an improved bound for the approximation error of the Nystr\&quot;{o}m
method under the assumption that there is a large eigengap in the spectrum of
kernel matrix. This is based on the empirical observation that the eigengap has
a significant impact on the approximation error of the Nystr\&quot;{o}m method. Our
approach is based on the concentration inequality of integral operator and the
theory of matrix perturbation. Our analysis shows that when there is a large
eigengap, we can improve the approximation error of the Nystr\&quot;{o}m method from
$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ is
the size of the kernel matrix, and $m$ is the number of sampled columns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0029</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0029</id><created>2012-08-31</created><updated>2012-09-05</updated><authors><author><keyname>Purpura</keyname><forenames>Stephen</forenames></author><author><keyname>Hillard</keyname><forenames>Dustin</forenames></author><author><keyname>Hubenthal</keyname><forenames>Mark</forenames></author><author><keyname>Walsh</keyname><forenames>Jim</forenames></author><author><keyname>Golder</keyname><forenames>Scott</forenames></author><author><keyname>Smith</keyname><forenames>Scott</forenames></author></authors><title>Statistically adaptive learning for a general class of cost functions
  (SA L-BFGS)</title><categories>cs.LG stat.ML</categories><comments>7 pages, 2 tables</comments><report-no>version 0.05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system that enables rapid model experimentation for tera-scale
machine learning with trillions of non-zero features, billions of training
examples, and millions of parameters. Our contribution to the literature is a
new method (SA L-BFGS) for changing batch L-BFGS to perform in near real-time
by using statistical tools to balance the contributions of previous weights,
old training examples, and new training examples to achieve fast convergence
with few iterations. The result is, to our knowledge, the most scalable and
flexible linear learning system reported in the literature, beating standard
practice with the current best system (Vowpal Wabbit and AllReduce). Using the
KDD Cup 2012 data set from Tencent, Inc. we provide experimental results to
verify the performance of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0036</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0036</id><created>2012-08-31</created><authors><author><keyname>Allen</keyname><forenames>Robert B.</forenames></author></authors><title>Supporting Structured Browsing for Full-Text Scientific Research Reports</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific research is highly structured and some of that structure is
reflected in research reports. Traditional scientific research reports are
yielding to interactive documents which expose their internal structure and are
richly linked to other materials. In these changes, there are opportunities to
take advantage of the structure in scientific research reports which previously
have not been systematically captured. Thus, we explore ways of capturing more
of the structure of research in reports about the research and we use that
structure to support the development of a new generation of document browsers
which include novel interaction widgets. We apply the browsers incorporating
the conceptual modeling framework to full-text research reports from the Public
Library of Science (PLoS). In addition, we describe the application of
model-oriented constructs to facilitating highly interlinked digital libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0047</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0047</id><created>2012-08-31</created><updated>2013-12-02</updated><authors><author><keyname>Mohanty</keyname><forenames>Kaniska</forenames></author><author><keyname>Vaze</keyname><forenames>Chinmay S.</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The Degrees of Freedom Region of the MIMO Interference Channel with
  Hybrid CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) region of the two-user MIMO (multiple-input
multiple-output) interference channel is established under a new model termed
as hybrid CSIT. In this model, one transmitter has delayed channel state
information (CSI) and the other transmitter has instantaneous CSIT, of incoming
channel matrices at the respective unpaired receivers, and neither transmitter
has any knowledge of the incoming channel matrices of its respective paired
receiver. The DoF region for hybrid CSIT, and consequently that of
$2\times2\times3^{5}$ CSIT models, is completely characterized, and a new
achievable scheme based on a combination of transmit beamforming and
retrospective interference alignment is developed. Conditions are obtained on
the numbers of antennas at each of the four terminals such that the DoF region
under hybrid CSIT is equal to that under (a) global and instantaneous CSIT and
(b) global and delayed CSIT, with the remaining cases resulting in a DoF region
with hybrid CSIT that lies somewhere in between the DoF regions under the
instantaneous and delayed CSIT settings. Further synergistic benefits accruing
from switching between the two hybrid CSIT models are also explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0053</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0053</id><created>2012-09-01</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Pal</keyname><forenames>Moumita</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author></authors><title>A Session Based Blind Watermarking Technique within the NROI of Retinal
  Fundus Images for Authentication Using DWT, Spread Spectrum and Harris Corner
  Detection</title><categories>cs.CV cs.CY</categories><comments>9 pages, 10 figures</comments><journal-ref>International Journal of Modern Engineering Research
  (IJMER),Vol.2, Issue.3,May-June 2012 pp-749-757,ISSN: 2249-6645</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital Retinal Fundus Images helps to detect various ophthalmic diseases by
detecting morphological changes in optical cup, optical disc and macula.
Present work proposes a method for the authentication of medical images based
on Discrete Wavelet Transformation (DWT) and Spread Spectrum. Proper selection
of the Non Region of Interest (NROI) for watermarking is crucial, as the area
under concern has to be the least required portion conveying any medical
information. Proposed method discusses both the selection of least impact area
and the blind watermarking technique. Watermark is embedded within the
High-High (HH) sub band. During embedding, watermarked image is dispersed
within the band using a pseudo random sequence and a Session key. Watermarked
image is extracted using the session key and the size of the image. In this
approach the generated watermarked image having an acceptable level of
imperceptibility and distortion is compared to the Original retinal image based
on Peak Signal to Noise Ratio (PSNR) and correlation value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0054</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0054</id><created>2012-09-01</created><authors><author><keyname>Bhattacharya</keyname><forenames>Tanmay</forenames></author><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Chaudhuri</keyname><forenames>S. R. Bhadra</forenames></author></authors><title>A Novel Session Based Dual Steganographic Technique Using DWT and Spread
  Spectrum</title><categories>cs.CR</categories><comments>5 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1208.0803</comments><journal-ref>International Journal of Modern Engineering Research
  (IJMER),Vol.1, Dec 2011, Issue1, pp-157-161,ISSN: 2249-6645</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed a DWT based Steganographic technique. Cover image is
decomposed into four sub bands using DWT. Two secret images are embedded within
the HL and HH sub bands respectively. During embedding secret images are
dispersed within each band using a pseudo random sequence and a Session key.
Secret images are extracted using the session key and the size of the images.
In this approach the stego image generated is of acceptable level of
imperceptibility and distortion compared to the cover image and the overall
security is high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0056</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0056</id><created>2012-09-01</created><authors><author><keyname>Juba</keyname><forenames>Brendan</forenames></author></authors><title>Learning implicitly in reasoning in PAC-Semantics</title><categories>cs.AI cs.DS cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of answering queries about formulas of propositional
logic based on background knowledge partially represented explicitly as other
formulas, and partially represented as partially obscured examples
independently drawn from a fixed probability distribution, where the queries
are answered with respect to a weaker semantics than usual -- PAC-Semantics,
introduced by Valiant (2000) -- that is defined using the distribution of
examples. We describe a fairly general, efficient reduction to limited versions
of the decision problem for a proof system (e.g., bounded space treelike
resolution, bounded degree polynomial calculus, etc.) from corresponding
versions of the reasoning problem where some of the background knowledge is not
explicitly given as formulas, only learnable from the examples. Crucially, we
do not generate an explicit representation of the knowledge extracted from the
examples, and so the &quot;learning&quot; of the background knowledge is only done
implicitly. As a consequence, this approach can utilize formulas as background
knowledge that are not perfectly valid over the distribution---essentially the
analogue of agnostic learning here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0057</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0057</id><created>2012-09-01</created><authors><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Anchoring Bias in Online Voting</title><categories>physics.data-an cs.IR physics.soc-ph</categories><comments>5 pages, 4 tables, 5 figures</comments><journal-ref>EPL 100 (2012) 68002</journal-ref><doi>10.1209/0295-5075/100/68002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting online with explicit ratings could largely reflect people's
preferences and objects' qualities, but ratings are always irrational, because
they may be affected by many unpredictable factors like mood, weather, as well
as other people's votes. By analyzing two real systems, this paper reveals a
systematic bias embedding in the individual decision-making processes, namely
people tend to give a low rating after a low rating, as well as a high rating
following a high rating. This so-called \emph{anchoring bias} is validated via
extensive comparisons with null models, and numerically speaking, the extent of
bias decays with interval voting number in a logarithmic form. Our findings
could be applied in the design of recommender systems and considered as
important complementary materials to previous knowledge about anchoring effects
on financial trades, performance judgements, auctions, and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0061</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0061</id><created>2012-09-01</created><authors><author><keyname>Bogana</keyname><forenames>S.</forenames></author></authors><title>Compensation of IQ-Imbalance and Phase Noise in MIMO-OFDM Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrading effect of RF impairments on the performance of wireless
communication systems is more pronounced in MIMO-OFDM transmission. Two of the
most common impairments that significantly limit the performance of MIMO-OFDM
transceivers are IQ-imbalance and phase noise. Low-complexity estimation and
compensation techniques that can jointly remove the effect of these impairments
are highly desirable. In this paper, we propose a simple joint estimation and
compensation technique to estimate channel, phase noise and IQ-imbalance
parameters in MIMO-OFDM systems under multipath slow fading channels. A
subcarrier multiplexed preamble structure to estimate the channel and
impairment parameters with minimum overhead is introduced and used in the
estimation of IQ-imbalance parameters as well as the initial estimation of
effective channel matrix including common phase error (CPE). We then use a
novel tracking method based on the second order statistics of the inter-carrier
interference (ICI) and noise to update the effective channel matrix throughout
an OFDM frame. Simulation results for a variety of scenarios show that the
proposed low-complexity estimation and compensation technique can efficiently
improve the performance of MIMO-OFDM systems in terms of bit-error-rate (BER).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0082</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0082</id><created>2012-09-01</created><updated>2013-06-21</updated><authors><author><keyname>Houshmand</keyname><forenames>Monireh</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Recursive quantum convolutional encoders are catastrophic: A simple
  proof</title><categories>quant-ph cs.IT math.IT</categories><comments>15 pages, 1 figure. v2: accepted into IEEE Transactions on
  Information Theory with minor modifications. arXiv admin note: text overlap
  with arXiv:1105.0649</comments><journal-ref>IEEE Transactions on Information Theory vol. 59, no. 10, pages
  6724-6731 (October 2013)</journal-ref><doi>10.1109/TIT.2013.2272932</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poulin, Tillich, and Ollivier discovered an important separation between the
classical and quantum theories of convolutional coding, by proving that a
quantum convolutional encoder cannot be both non-catastrophic and recursive.
Non-catastrophicity is desirable so that an iterative decoding algorithm
converges when decoding a quantum turbo code whose constituents are quantum
convolutional codes, and recursiveness is as well so that a quantum turbo code
has a minimum distance growing nearly linearly with the length of the code,
respectively. Their proof of the aforementioned theorem was admittedly &quot;rather
involved,&quot; and as such, it has been desirable since their result to find a
simpler proof. In this paper, we furnish a proof that is arguably simpler. Our
approach is group-theoretic---we show that the subgroup of memory states that
are part of a zero physical-weight cycle of a quantum convolutional encoder is
equivalent to the centralizer of its &quot;finite-memory&quot; subgroup (the subgroup of
memory states which eventually reach the identity memory state by identity
operator inputs for the information qubits and identity or Pauli-Z operator
inputs for the ancilla qubits). After proving that this symmetry holds for any
quantum convolutional encoder, it easily follows that an encoder is
non-recursive if it is non-catastrophic. Our proof also illuminates why this
no-go theorem does not apply to entanglement-assisted quantum convolutional
encoders---the introduction of shared entanglement as a resource allows the
above symmetry to be broken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0089</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0089</id><created>2012-09-01</created><updated>2014-01-08</updated><authors><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Woodard</keyname><forenames>Ryan</forenames></author></authors><title>Estimating the historical and future probabilities of large terrorist
  events</title><categories>physics.data-an cs.LG physics.soc-ph stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOAS614 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS614</report-no><journal-ref>Annals of Applied Statistics 2013, Vol. 7, No. 4, 1838-1865</journal-ref><doi>10.1214/12-AOAS614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantities with right-skewed distributions are ubiquitous in complex social
systems, including political conflict, economics and social networks, and these
systems sometimes produce extremely large events. For instance, the 9/11
terrorist events produced nearly 3000 fatalities, nearly six times more than
the next largest event. But, was this enormous loss of life statistically
unlikely given modern terrorism's historical record? Accurately estimating the
probability of such an event is complicated by the large fluctuations in the
empirical distribution's upper tail. We present a generic statistical algorithm
for making such estimates, which combines semi-parametric models of tail
behavior and a nonparametric bootstrap. Applied to a global database of
terrorist events, we estimate the worldwide historical probability of observing
at least one 9/11-sized or larger event since 1968 to be 11-35%. These results
are robust to conditioning on global variations in economic development,
domestic versus international events, the type of weapon used and a truncated
history that stops at 1998. We then use this procedure to make a data-driven
statistical forecast of at least one similar event over the next decade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0113</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0113</id><created>2012-09-01</created><authors><author><keyname>Keramat</keyname><forenames>Sue</forenames></author><author><keyname>Xie</keyname><forenames>Zue</forenames></author></authors><title>Using Space-Time trellis Codes For AF Relay Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the analysis and design of space-time trellis codes (STTCs) for a
cooperative relay channel operating in amplify-and-forward (AF) mode assuming
the source and destination nodes are equipped with multiple antennas but the
relay node has single antenna. We derive a pairwise error probability (PEP)
expression for the performance of STTCs in this type of channels. A simple
upper-bound on PEP is then derived and is maximized to find the optimum STTCs.
We show that the designed STTCs based on the derived criterion achieve full
diversity in the AF relay channels especially at high signal-to-noise-ratios
(SNRs). The maximum achievable diversity in relay channels with single-antenna
relay is bounded by $\min(M,N)$ where $M$ and $N$ are respectively the number
of antennas in source and destination nodes. Simulation results confirm that
the proposed codes achieve the maximum diversity and also provide an appealing
coding gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0125</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0125</id><created>2012-09-01</created><updated>2013-08-16</updated><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author></authors><title>A History of Cluster Analysis Using the Classification Society's
  Bibliography Over Four Decades</title><categories>cs.DL cs.LG stat.ML</categories><comments>23 pages, 9 figures</comments><msc-class>62H30</msc-class><acm-class>I.5.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Classification Literature Automated Search Service, an annual
bibliography based on citation of one or more of a set of around 80 book or
journal publications, ran from 1972 to 2012. We analyze here the years 1994 to
2011. The Classification Society's Service, as it was termed, has been produced
by the Classification Society. In earlier decades it was distributed as a
diskette or CD with the Journal of Classification. Among our findings are the
following: an enormous increase in scholarly production post approximately
2000; a very major increase in quantity, coupled with work in different
disciplines, from approximately 2004; and a major shift also from cluster
analysis in earlier times having mathematics and psychology as disciplines of
the journals published in, and affiliations of authors, contrasted with, in
more recent times, a &quot;centre of gravity&quot; in management and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0126</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0126</id><created>2012-09-01</created><authors><author><keyname>J.</keyname><forenames>Joshi Hardik</forenames></author><author><keyname>Jyoti</keyname><forenames>Pareek</forenames></author></authors><title>Evaluation of some Information Retrieval models for Gujarati Ad hoc
  Monolingual Tasks</title><categories>cs.IR</categories><comments>6 pages, Some text in Gujarati Language</comments><journal-ref>&quot;Joshi, H.; Pareek, J.; &quot;,Evaluation of some Information Retrieval
  Models for Gujarati Ad hoc Monolingual Tasks,VNSGU Journal of Science and
  Technology,3,2,176-181,2012,&quot;Veer Narmad South Gujarat University, Surat&quot;</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper describes the work towards Gujarati Ad hoc Monolingual Retrieval
task for widely used Information Retrieval (IR) models. We present an indexing
baseline for the Gujarati Language represented by Mean Average Precision (MAP)
values. Our objective is to obtain a relative picture of a better IR model for
Gujarati Language. Results show that Classical IR models like Term Frequency
Inverse Document Frequency (TF_IDF) performs better when compared to few recent
probabilistic IR models. The experiments helped to identify the outperforming
IR models for Gujarati Language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0127</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0127</id><created>2012-09-01</created><updated>2012-09-24</updated><authors><author><keyname>El-Yaniv</keyname><forenames>Ran</forenames></author><author><keyname>Faynburd</keyname><forenames>Alexandra</forenames></author></authors><title>Autoregressive short-term prediction of turning points using support
  vector regression</title><categories>cs.LG cs.CE cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is concerned with autoregressive prediction of turning points in
financial price sequences. Such turning points are critical local extrema
points along a series, which mark the start of new swings. Predicting the
future time of such turning points or even their early or late identification
slightly before or after the fact has useful applications in economics and
finance. Building on recently proposed neural network model for turning point
prediction, we propose and study a new autoregressive model for predicting
turning points of small swings. Our method relies on a known turning point
indicator, a Fourier enriched representation of price histories, and support
vector regression. We empirically examine the performance of the proposed
method over a long history of the Dow Jones Industrial average. Our study shows
that the proposed method is superior to the previous neural network model, in
terms of trading performance of a simple trading application and also exhibits
a quantifiable advantage over the buy-and-hold benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0129</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0129</id><created>2012-09-01</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author></authors><title>A stronger structure theorem for excluded topological minors</title><categories>math.CO cs.DM</categories><comments>33 pages, 1 figure</comments><msc-class>05C75 (Primary) 05C83, 05C10 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grohe and Marx proved that if G does not contain H as a topological minor,
then there exist constants g=O(|V(H)|^4), D and t depending only on H such that
G is a clique sum of graphs that either contain at most t vertices of degree
greater than D or almost embed in some surface of genus at most g. We
strengthen this result, giving a more precise description of the latter kind of
basic graphs of the decomposition - we only allow graphs that (almost) embed in
ways that are impossible for H (similarly to the structure theorem for minors,
where only graphs almost embedded in surfaces in that H does not embed are
allowed). This enables us to give structural results for graphs avoiding a
fixed graph as an immersion and for graphs with bounded infinity-admissibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0135</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0135</id><created>2012-09-01</created><authors><author><keyname>Cherlopalle</keyname><forenames>Deepthi</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Goldbach Triples and Key Distribution</title><categories>cs.CR</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the use of the number of Goldbach triples, or the
number of three prime partitions of an odd number, for use in the generation
and distribution of cryptographic keys. In addition to presenting randomness
properties of these triples, which turn out to be similar to that of prime
partitions of even numbers, we explore the question of restricted partition
sets. We propose a protocol for key distribution that is based on these
numbers. Two of the three partitions of the randomly chosen number serve as
cover to send the third number to the two parties that wish to communicate with
each other. This third number can serve as session key and the original number
of which it is a partition can be used for audit purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0136</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0136</id><created>2012-09-01</created><updated>2012-09-05</updated><authors><author><keyname>Ulusoy</keyname><forenames>Alphan</forenames></author><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Incremental Control Synthesis in Probabilistic Environments with
  Temporal Logic Constraints</title><categories>cs.RO cs.LO</categories><comments>Extended version of the CDC 2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a method for optimal control synthesis of a plant
that interacts with a set of agents in a graph-like environment. The control
specification is given as a temporal logic statement about some properties that
hold at the vertices of the environment. The plant is assumed to be
deterministic, while the agents are probabilistic Markov models. The goal is to
control the plant such that the probability of satisfying a syntactically
co-safe Linear Temporal Logic formula is maximized. We propose a
computationally efficient incremental approach based on the fact that temporal
logic verification is computationally cheaper than synthesis. We present a
case-study where we compare our approach to the classical non-incremental
approach in terms of computation time and memory usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0160</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0160</id><created>2012-09-02</created><updated>2015-11-28</updated><authors><author><keyname>Huynh</keyname><forenames>Tony</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author><author><keyname>Verdian-Rizi</keyname><forenames>Maryam</forenames></author></authors><title>Strongly even-cycle decomposable graphs</title><categories>math.CO cs.DM</categories><comments>19 pages, 12 figures</comments><msc-class>05C45, 05C76, 05C75</msc-class><doi>10.1002/jgt.22018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is strongly even-cycle decomposable if the edge set of every
subdivision with an even number of edges can be partitioned into cycles of even
length. We prove that several fundamental composition operations that preserve
the property of being Eulerian also yield strongly even-cycle decomposable
graphs. As an easy application of our theorems, we give an exact
characterization of the set of strongly even-cycle decomposable cographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0167</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0167</id><created>2012-09-02</created><updated>2012-11-07</updated><authors><author><keyname>Bazarghan</keyname><forenames>M.</forenames></author><author><keyname>Jaberi</keyname><forenames>Y.</forenames></author><author><keyname>Amandi</keyname><forenames>R.</forenames></author><author><keyname>Abedi</keyname><forenames>M.</forenames></author></authors><title>Automatic ECG Beat Arrhythmia Detection</title><categories>cs.NE</categories><comments>17 pages, 6 figures and 5 tables, Submitted to the BMC Bioinformatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: In recent years automated data analysis techniques have drawn
great attention and are used in almost every field of research including
biomedical. Artificial Neural Networks (ANNs) are one of the Computer- Aided-
Diagnosis tools which are used extensively by advances in computer hardware
technology. The application of these techniques for disease diagnosis has made
great progress and is widely used by physicians. An Electrocardiogram carries
vital information about heart activity and physicians use this signal for
cardiac disease diagnosis which was the great motivation towards our study.
Methods: In this study we are using Probabilistic Neural Networks (PNN) as an
automatic technique for ECG signal analysis along with a Genetic Algorithm
(GA). As every real signal recorded by the equipment can have different
artifacts, we need to do some preprocessing steps before feeding it to the ANN.
Wavelet transform is used for extracting the morphological parameters and
median filter for data reduction of the ECG signal. The subset of morphological
parameters are chosen and optimized using GA. We had two approaches in our
investigation, the first one uses the whole signal with 289 normalized and
de-noised data points as input to the ANN. In the second approach after
applying all the preprocessing steps the signal is reduced to 29 data points
and also their important parameters extracted to form the ANN input with 35
data points. Results: The outcome of the two approaches for 8 types of
arrhythmia shows that the second approach is superior than the first one with
an average accuracy of %99.42.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0194</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0194</id><created>2012-09-02</created><authors><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Sheffer</keyname><forenames>Adam</forenames></author></authors><title>Counting Plane Graphs: Cross-Graph Charging Schemes</title><categories>cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study cross-graph charging schemes for graphs drawn in the plane. These
are charging schemes where charge is moved across vertices of different graphs.
Such methods have been recently applied to obtain various properties of
triangulations that are embedded over a fixed set of points in the plane. We
show how this method can be generalized to obtain results for various other
types of graphs that are embedded in the plane. Specifically, we obtain a new
bound of $O^*(187.53^N)$ (where the $O^*()$ notation hides polynomial factors)
for the maximum number of crossing-free straight-edge graphs that can be
embedded over any specific set of $N$ points in the plane (improving upon the
previous best upper bound $207.85^N$ in Hoffmann et al.). We also derive upper
bounds for numbers of several other types of plane graphs (such as connected
and bi-connected plane graphs), and obtain various bounds on expected
vertex-degrees in graphs that are uniformly chosen from the set of all
crossing-free straight-edge graphs that can be embedded over a specific point
set.
  We then show how to apply the cross-graph charging-scheme method for graphs
that allow certain types of crossings. Specifically, we consider graphs with no
set of $k$ pairwise-crossing edges (more commonly known as $k$-quasi-planar
graphs). For $k=3$ and $k=4$, we prove that, for any set $S$ of $N$ points in
the plane, the number of graphs that have a straight-edge $k$-quasi-planar
embedding over $S$ is only exponential in $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0196</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0196</id><created>2012-09-02</created><updated>2013-01-09</updated><authors><author><keyname>Herrera</keyname><forenames>Roberto H.</forenames></author><author><keyname>Van der Baan</keyname><forenames>Mirko</forenames></author></authors><title>Short-time homomorphic wavelet estimation</title><categories>physics.geo-ph cs.CV physics.data-an</categories><comments>13 pages, 5 figures. 2012 J. Geophys. Eng. 9 674</comments><journal-ref>J. Geophys. Eng. 9 674, 2012</journal-ref><doi>10.1088/1742-2132/9/6/674</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful wavelet estimation is an essential step for seismic methods like
impedance inversion, analysis of amplitude variations with offset and full
waveform inversion. Homomorphic deconvolution has long intrigued as a
potentially elegant solution to the wavelet estimation problem. Yet a
successful implementation has proven difficult. Associated disadvantages like
phase unwrapping and restrictions of sparsity in the reflectivity function
limit its application. We explore short-time homomorphic wavelet estimation as
a combination of the classical homomorphic analysis and log-spectral averaging.
The introduced method of log-spectral averaging using a short-term Fourier
transform increases the number of sample points, thus reducing estimation
variances. We apply the developed method on synthetic and real data examples
and demonstrate good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0219</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0219</id><created>2012-09-02</created><updated>2012-10-06</updated><authors><author><keyname>Levnaji&#x107;</keyname><forenames>Zoran</forenames></author></authors><title>Dynamical networks reconstructed from time series</title><categories>physics.data-an cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>Proceeding of the Conference on Information Technology and
  Information Society, Dolenjske Toplice, Slovenia, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel method of reconstructing dynamical networks from empirically measured
time series is proposed. By examining the variable--derivative correlation of
network node pairs, we derive a simple equation that directly yields the
adjacency matrix, assuming the intra-network interaction functions to be known.
We illustrate the method on a simple example, and discuss the dependence of the
reconstruction precision on the properties of time series. Our method is
applicable to any network, allowing for reconstruction precision to be
maximized, and errors to be estimated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0229</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0229</id><created>2012-09-02</created><updated>2013-09-30</updated><authors><author><keyname>Huang</keyname><forenames>Qingqing</forenames></author><author><keyname>Roozbehani</keyname><forenames>Mardavij</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A</forenames></author></authors><title>Efficiency-Risk Tradeoffs in Dynamic Oligopoly Markets - with
  application to electricity markets</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine in an abstract framework, how a tradeoff between
efficiency and robustness arises in different dynamic oligopolistic market
architectures. We consider a market in which there is a monopolistic resource
provider and agents that enter and exit the market following a random process.
Self-interested and fully rational agents dynamically update their resource
consumption decisions over a finite time horizon, under the constraint that the
total resource consumption requirements are met before each individual's
deadline. We then compare the statistics of the stationary aggregate demand
processes induced by the non-cooperative and cooperative load scheduling
schemes. We show that although the non-cooperative load scheduling scheme leads
to an efficiency loss - widely known as the &quot;price of anarchy&quot; - the stationary
distribution of the corresponding aggregate demand process has a smaller tail.
This tail, which corresponds to rare and undesirable demand spikes, is
important in many applications of interest. On the other hand, when the agents
can cooperate with each other in optimizing their total cost, a higher market
efficiency is achieved at the cost of a higher probability of demand spikes. We
thus posit that the origins of endogenous risk in such systems may lie in the
market architecture, which is an inherent characteristic of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0236</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0236</id><created>2012-09-02</created><updated>2013-03-11</updated><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Purkayastha</keyname><forenames>Punarbasu</forenames></author><author><keyname>Wang</keyname><forenames>Chengmin</forenames></author></authors><title>Cross-Bifix-Free Codes Within a Constant Factor of Optimality</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cross-bifix-free code is a set of words in which no prefix of any length of
any word is the suffix of any word in the set. Cross-bifix-free codes arise in
the study of distributed sequences for frame synchronization. We provide a new
construction of cross-bifix-free codes which generalizes the construction in
Bajic (2007) to longer code lengths and to any alphabet size. The codes are
shown to be nearly optimal in size. We also establish new results on Fibonacci
sequences, that are used in estimating the size of the cross-bifix-free codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0237</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0237</id><created>2012-09-02</created><updated>2013-07-11</updated><authors><author><keyname>Coifman</keyname><forenames>Ronald R.</forenames></author><author><keyname>Hirn</keyname><forenames>Matthew J.</forenames></author></authors><title>Bi-stochastic kernels via asymmetric affinity functions</title><categories>math.CA cs.IT math.IT math.PR math.SP</categories><comments>5 pages. v2: Expanded upon the first paragraph of subsection 2.1. v3:
  Minor changes and edits. v4: Edited comments and added DOI</comments><journal-ref>Applied and Computational Harmonic Analysis, volume 35, number 1,
  pages 177-180, July, 2013</journal-ref><doi>10.1016/j.acha.2013.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short letter we present the construction of a bi-stochastic kernel p
for an arbitrary data set X that is derived from an asymmetric affinity
function {\alpha}. The affinity function {\alpha} measures the similarity
between points in X and some reference set Y. Unlike other methods that
construct bi-stochastic kernels via some convergent iteration process or
through solving an optimization problem, the construction presented here is
quite simple. Furthermore, it can be viewed through the lens of out of sample
extensions, making it useful for massive data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0245</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0245</id><created>2012-09-03</created><updated>2013-07-11</updated><authors><author><keyname>Coifman</keyname><forenames>Ronald R.</forenames></author><author><keyname>Hirn</keyname><forenames>Matthew J.</forenames></author></authors><title>Diffusion maps for changing data</title><categories>math.CA cs.IT math.IT math.PR math.SP</categories><comments>38 pages. 9 figures. To appear in Applied and Computational Harmonic
  Analysis. v2: Several minor changes beyond just typos. v3: Minor typo
  corrected, added DOI</comments><journal-ref>Applied and Computational Harmonic Analysis, Volume 36, Issue 1,
  January 2014, Pages 79-107</journal-ref><doi>10.1016/j.acha.2013.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Laplacians and related nonlinear mappings into low dimensional spaces
have been shown to be powerful tools for organizing high dimensional data. Here
we consider a data set X in which the graph associated with it changes
depending on some set of parameters. We analyze this type of data in terms of
the diffusion distance and the corresponding diffusion map. As the data changes
over the parameter space, the low dimensional embedding changes as well. We
give a way to go between these embeddings, and furthermore, map them all into a
common space, allowing one to track the evolution of X in its intrinsic
geometry. A global diffusion distance is also defined, which gives a measure of
the global behavior of the data over the parameter space. Approximation
theorems in terms of randomly sampled data are presented, as are potential
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0249</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0249</id><created>2012-09-03</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>Robopinion: Opinion Mining Framework Inspired by Autonomous Robot
  Navigation</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data association methods are used by autonomous robots to find matches
between the current landmarks and the new set of observed features. We seek a
framework for opinion mining to benefit from advancements in autonomous robot
navigation in both research and development
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0263</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0263</id><created>2012-09-03</created><updated>2013-02-19</updated><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Yao</keyname><forenames>Penghui</forenames></author></authors><title>A strong direct product theorem in terms of the smooth rectangle bound</title><categories>cs.CC</categories><comments>12 pages, version 3, improved parameters in the main result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strong direct product theorem states that, in order to solve k instances of
a problem, if we provide less than k times the resource required to compute one
instance, then the probability of overall success is exponentially small in k.
In this paper, we consider the model of two-way public-coin communication
complexity and show a strong direct product theorem for all relations in terms
of the smooth rectangle bound, introduced by Jain and Klauck as a generic lower
bound method in this model. Our result therefore uniformly implies a strong
direct product theorem for all relations for which an (asymptotically) optimal
lower bound can be provided using the smooth rectangle bound, for example Inner
Product, Greater-Than, Set-Disjointness, Gap-Hamming Distance etc. Our result
also implies near optimal direct product results for several important
functions and relations used to show exponential separations between classical
and quantum communication complexity, for which near optimal lower bounds are
provided using the rectangle bound, for example by Raz [1999], Gavinsky [2008]
and Klartag and Regev [2011]. In fact we are not aware of any relation for
which it is known that the smooth rectangle bound does not provide an optimal
lower bound. This lower bound subsumes many of the other lower bound methods,
for example the rectangle bound (a.k.a the corruption bound), the smooth
discrepancy bound (a.k.a the \gamma_2 bound) which in turn subsumes the
discrepancy bound, the subdistribution bound and the conditional min-entropy
bound.
  We show our result using information theoretic arguments. A key tool we use
is a sampling protocol due to Braverman [2012], in fact a modification of it
used by Kerenidis, Laplante, Lerays, Roland and Xiao [2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0286</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0286</id><created>2012-09-03</created><authors><author><keyname>Sen</keyname><forenames>Nilanjan</forenames></author><author><keyname>Banerjee</keyname><forenames>Indrajit</forenames></author></authors><title>CAWS - Security Algorithms for Wireless Sensor Networks: A Cellular
  Automata Based Approach</title><categories>cs.CR</categories><comments>Proceedings of &quot;All India Seminar on Role of ICT in Improving Quality
  of Life&quot; on March 26-27, 2010 organized by The Institution of Engineers
  (India) and Bengal Engineering and Science University, Shibpur</comments><journal-ref>Proceedings of &quot;All India Seminar on Role of ICT in Improving
  Quality of Life&quot;, Dated on March 26-27, 2010; pp: 81-88</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security in the Wireless Sensor Networks (WSN) is a very challenging task
because of their dissimilarities with the conventional wireless networks. The
related works so far have been done have tried to solve the problem keeping in
the mind the constraints of WSNs. In this paper we have proposed a set of
cellular automata based security algorithms (CAWS) which consists of CAKD, a
Cellular Automata (CA) based key management algorithm and CASC, a CA based
secure data communication algorithm, which require very small amount of memory
as well as simple computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0296</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0296</id><created>2012-09-03</created><authors><author><keyname>Levy</keyname><forenames>Tal</forenames></author><author><keyname>Cohen</keyname><forenames>Guy</forenames></author><author><keyname>Rabani</keyname><forenames>Eran</forenames></author></authors><title>Simulating Lattice Spin Models on Graphics Processing Units</title><categories>physics.comp-ph cs.OH</categories><journal-ref>Journal of Chemical Theory and Computation 6, 11, 3293-3301, 2010</journal-ref><doi>10.1021/ct100385b</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice spin models are useful for studying critical phenomena and allow the
extraction of equilibrium and dynamical properties. Simulations of such systems
are usually based on Monte Carlo (MC) techniques, and the main difficulty is
often the large computational effort needed when approaching critical points.
In this work, it is shown how such simulations can be accelerated with the use
of NVIDIA graphics processing units (GPUs) using the CUDA programming
architecture. We have developed two different algorithms for lattice spin
models, the first useful for equilibrium properties near a second-order phase
transition point and the second for dynamical slowing down near a glass
transition. The algorithms are based on parallel MC techniques, and speedups
from 70- to 150-fold over conventional single-threaded computer codes are
obtained using consumer-grade hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0308</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0308</id><created>2012-09-03</created><authors><author><keyname>Ojha</keyname><forenames>Muneendra</forenames></author></authors><title>Optimizing Supply Chain Management using Gravitational Search Algorithm
  and Multi Agent System</title><categories>cs.MA cs.AI</categories><comments>11 pages, 3 figures, &quot;Published in Conference of Soft Computing and
  Problem Solving 2011, Springer&quot;</comments><journal-ref>Proceedings of the International Conference on SocProS 2011, AISC
  130, pp. 481</journal-ref><doi>10.1007/978-81-322-0487-9_47</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supply chain management is a very dynamic operation research problem where
one has to quickly adapt according to the changes perceived in environment in
order to maximize the benefit or minimize the loss. Therefore we require a
system which changes as per the changing requirements. Multi agent system
technology in recent times has emerged as a possible way of efficient solution
implementation for many such complex problems. Our research here focuses on
building a Multi Agent System (MAS), which implements a modified version of
Gravitational Search swarm intelligence Algorithm (GSA) to find out an optimal
strategy in managing the demand supply chain. We target the grains distribution
system among various centers of Food Corporation of India (FCI) as application
domain. We assume centers with larger stocks as objects of greater mass and
vice versa. Applying Newtonian law of gravity as suggested in GSA, larger
objects attract objects of smaller mass towards itself, creating a virtual
grain supply source. As heavier object sheds its mass by supplying some to the
one in demand, it loses its gravitational pull and thus keeps the whole system
of supply chain per-fectly in balance. The multi agent system helps in
continuous updation of the whole system with the help of autonomous agents
which react to the change in environment and act accordingly. This model also
reduces the communication bottleneck to greater extents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0317</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0317</id><created>2012-09-03</created><authors><author><keyname>Latu</keyname><forenames>Guillaume</forenames><affiliation>IRFM, INRIA Lorraine / IECN / LSIIT / IRMA</affiliation></author><author><keyname>Grandgirard</keyname><forenames>Virginie</forenames><affiliation>IRFM</affiliation></author><author><keyname>Abiteboul</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>IRFM</affiliation></author><author><keyname>Bergot</keyname><forenames>Morgane</forenames><affiliation>INRIA Lorraine / IECN / LSIIT / IRMA</affiliation></author><author><keyname>Crouseilles</keyname><forenames>Nicolas</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Garbet</keyname><forenames>Xavier</forenames><affiliation>IRFM</affiliation></author><author><keyname>Ghendrih</keyname><forenames>Philippe</forenames><affiliation>IRFM</affiliation></author><author><keyname>Mehrenberger</keyname><forenames>Michel</forenames><affiliation>INRIA Lorraine / IECN / LSIIT / IRMA</affiliation></author><author><keyname>Sarazin</keyname><forenames>Yanick</forenames><affiliation>IRFM</affiliation></author><author><keyname>Sellama</keyname><forenames>Hocine</forenames><affiliation>INRIA Lorraine / IECN / LSIIT / IRMA</affiliation></author><author><keyname>Sonnendr&#xfc;cker</keyname><forenames>Eric</forenames><affiliation>INRIA Lorraine / IECN / LSIIT / IRMA, IRMA</affiliation></author><author><keyname>Zarzoso</keyname><forenames>David</forenames><affiliation>IRFM</affiliation></author></authors><title>Accuracy of unperturbed motion of particles in a gyrokinetic
  semi-Lagrangian code</title><categories>cs.NA math.AP math.NA</categories><comments>No. RR-8054 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inaccurate description of the equilibrium can yield to spurious effects in
gyrokinetic turbulence simulations. Also, the Vlasov solver and time
integration schemes impact the conservation of physical quantities, especially
in long-term simulations. Equilibrium and Vlasov solver have to be tuned in
order to preserve constant states (equilibrium) and to provide good
conservation property along time (mass to begin with). Several illustrative
simple test cases are given to show typical spurious effects that one can
observes for poor settings. We explain why Forward Semi-Lagrangian scheme bring
us some benefits. Some toroidal and cylindrical GYSELA runs are shown that use
FSL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0320</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0320</id><created>2012-09-03</created><updated>2012-09-04</updated><authors><author><keyname>Borri</keyname><forenames>Alessandro</forenames></author><author><keyname>Pola</keyname><forenames>Giordano</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Maria D.</forenames></author></authors><title>Integrated Symbolic Design of Unstable Nonlinear Networked Control
  Systems</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1203.1069</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research area of Networked Control Systems (NCS) has been the topic of
intensive study in the last decade. In this paper we give a contribution to
this research line by addressing symbolic control design of (possibly unstable)
nonlinear NCS with specifications expressed in terms of automata. We first
derive symbolic models that are shown to approximate the given NCS in the sense
of (alternating) approximate simulation. We then address symbolic control
design with specifications expressed in terms of automata. We finally derive
efficient algorithms for the synthesis of the proposed symbolic controllers
that cope with the inherent computational complexity of the problem at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0341</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0341</id><created>2012-09-03</created><authors><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Draief</keyname><forenames>Moez</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Structural Analysis of Viral Spreading Processes in Social and
  Communication Networks Using Egonets</title><categories>math.OC cs.DM cs.SI cs.SY physics.soc-ph</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how the behavior of viral spreading processes is influenced by local
structural properties of the network over which they propagate. For a wide
variety of spreading processes, the largest eigenvalue of the adjacency matrix
of the network plays a key role on their global dynamical behavior. For many
real-world large-scale networks, it is unfeasible to exactly retrieve the
complete network structure to compute its largest eigenvalue. Instead, one
usually have access to myopic, egocentric views of the network structure, also
called egonets. In this paper, we propose a mathematical framework, based on
algebraic graph theory and convex optimization, to study how local structural
properties of the network constrain the interval of possible values in which
the largest eigenvalue must lie. Based on this framework, we present a
computationally efficient approach to find this interval from a collection of
egonets. Our numerical simulations show that, for several social and
communication networks, local structural properties of the network strongly
constrain the location of the largest eigenvalue and the resulting spreading
dynamics. From a practical point of view, our results can be used to dictate
immunization strategies to tame the spreading of a virus, or to design network
topologies that facilitate the spreading of information virally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0359</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0359</id><created>2012-09-03</created><updated>2012-09-28</updated><authors><author><keyname>Heussner</keyname><forenames>Alexander</forenames><affiliation>LaBRI, Universit&#xe9; de Bordeaux, CNRS, France</affiliation></author><author><keyname>Leroux</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>LaBRI, Universit&#xe9; de Bordeaux, CNRS, France</affiliation></author><author><keyname>Muscholl</keyname><forenames>Anca</forenames><affiliation>LaBRI, Universit&#xe9; de Bordeaux, CNRS, France</affiliation></author><author><keyname>Sutre</keyname><forenames>Gr&#xe9;goire</forenames><affiliation>LaBRI, Universit&#xe9; de Bordeaux, CNRS, France</affiliation></author></authors><title>Reachability Analysis of Communicating Pushdown Systems</title><categories>cs.LO cs.FL</categories><proxy>LMCS</proxy><acm-class>D.2.4, F.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  27, 2012) lmcs:921</journal-ref><doi>10.2168/LMCS-8(3:23)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reachability analysis of recursive programs that communicate
asynchronously over reliable FIFO channels calls for restrictions to ensure
decidability. Our first result characterizes communication topologies with a
decidable reachability problem restricted to eager runs (i.e., runs where
messages are either received immediately after being sent, or never received).
The problem is EXPTIME-complete in the decidable case. The second result is a
doubly exponential time algorithm for bounded context analysis in this setting,
together with a matching lower bound. Both results extend and improve previous
work from La Torre et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0365</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0365</id><created>2012-09-03</created><updated>2015-08-31</updated><authors><author><keyname>Pacher</keyname><forenames>Christoph</forenames></author><author><keyname>Abidin</keyname><forenames>Aysajan</forenames></author><author><keyname>Lor&#xfc;nser</keyname><forenames>Thomas</forenames></author><author><keyname>Peev</keyname><forenames>Momtchil</forenames></author><author><keyname>Ursin</keyname><forenames>Rupert</forenames></author><author><keyname>Zeilinger</keyname><forenames>Anton</forenames></author><author><keyname>Larsson</keyname><forenames>Jan-&#xc5;ke</forenames></author></authors><title>Attacks on quantum key distribution protocols that employ non-ITS
  authentication</title><categories>quant-ph cs.CR</categories><comments>34 pages</comments><journal-ref>Quantum Information Processing, 15(1), 327-362, 2016</journal-ref><doi>10.1007/s11128-015-1160-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate how adversaries with unbounded computing resources can break
Quantum Key Distribution (QKD) protocols which employ a particular message
authentication code suggested previously. This authentication code, featuring
low key consumption, is not Information-Theoretically Secure (ITS) since for
each message the eavesdropper has intercepted she is able to send a different
message from a set of messages that she can calculate by finding collisions of
a cryptographic hash function. However, when this authentication code was
introduced it was shown to prevent straightforward Man-In-The-Middle (MITM)
attacks against QKD protocols.
  In this paper, we prove that the set of messages that collide with any given
message under this authentication code contains with high probability a message
that has small Hamming distance to any other given message. Based on this fact
we present extended MITM attacks against different versions of BB84 QKD
protocols using the addressed authentication code; for three protocols we
describe every single action taken by the adversary. For all protocols the
adversary can obtain complete knowledge of the key, and for most protocols her
success probability in doing so approaches unity.
  Since the attacks work against all authentication methods which allow to
calculate colliding messages, the underlying building blocks of the presented
attacks expose the potential pitfalls arising as a consequence of non-ITS
authentication in QKD-postprocessing. We propose countermeasures, increasing
the eavesdroppers demand for computational power, and also prove necessary and
sufficient conditions for upgrading the discussed authentication code to the
ITS level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0366</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0366</id><created>2012-09-03</created><updated>2016-02-03</updated><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Lidicky</keyname><forenames>Bernard</forenames></author><author><keyname>Mohar</keyname><forenames>Bojan</forenames></author><author><keyname>Postle</keyname><forenames>Luke</forenames></author></authors><title>5-list-coloring planar graphs with distant precolored vertices</title><categories>math.CO cs.DM</categories><comments>53 pages, 9 figures version 2: addresses suggestions by reviewers</comments><msc-class>05C15 (Primary) 05C10 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We answer positively the question of Albertson asking whether every planar
graph can be $5$-list-colored even if it contains precolored vertices, as long
as they are sufficiently far apart from each other. In order to prove this
claim, we also give bounds on the sizes of graphs critical with respect to
5-list coloring. In particular, if G is a planar graph, H is a connected
subgraph of G and L is an assignment of lists of colors to the vertices of G
such that |L(v)| &gt;= 5 for every v in V(G)-V(H) and G is not L-colorable, then G
contains a subgraph with O(|H|^2) vertices that is not L-colorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0368</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0368</id><created>2012-09-03</created><authors><author><keyname>Villa</keyname><forenames>Silvia</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Mosci</keyname><forenames>Sofia</forenames></author><author><keyname>Verri</keyname><forenames>Alessandro</forenames></author></authors><title>Proximal methods for the latent group lasso penalty</title><categories>math.OC cs.LG stat.ML</categories><comments>4 figures</comments><msc-class>65K10, 90C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a regularized least squares problem, with regularization by
structured sparsity-inducing norms, which extend the usual $\ell_1$ and the
group lasso penalty, by allowing the subsets to overlap. Such regularizations
lead to nonsmooth problems that are difficult to optimize, and we propose in
this paper a suitable version of an accelerated proximal method to solve them.
We prove convergence of a nested procedure, obtained composing an accelerated
proximal method with an inner algorithm for computing the proximity operator.
By exploiting the geometrical properties of the penalty, we devise a new active
set strategy, thanks to which the inner iteration is relatively fast, thus
guaranteeing good computational performances of the overall algorithm. Our
approach allows to deal with high dimensional problems without pre-processing
for dimensionality reduction, leading to better computational and prediction
performances with respect to the state-of-the art methods, as shown empirically
both on toy and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0375</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0375</id><created>2012-09-03</created><updated>2013-01-03</updated><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Tuma</keyname><forenames>Vojtech</forenames></author></authors><title>A dynamic data structure for counting subgraphs in sparse graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>27 pages, no figures</comments><msc-class>68P05 (Primary) 05C75 (Secondary)</msc-class><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a dynamic data structure representing a graph G, which allows
addition and removal of edges from G and can determine the number of
appearances of a graph of a bounded size as an induced subgraph of G. The
queries are answered in constant time. When the data structure is used to
represent graphs from a class with bounded expansion (which includes planar
graphs and more generally all proper classes closed on topological minors, as
well as many other natural classes of graphs with bounded average degree), the
amortized time complexity of updates is polylogarithmic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0377</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0377</id><created>2012-09-03</created><updated>2014-06-27</updated><authors><author><keyname>Yue</keyname><forenames>Man-Chung</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author></authors><title>A Perturbation Inequality for the Schatten-$p$ Quasi-Norm and Its
  Applications to Low-Rank Matrix Recovery</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish the following perturbation result concerning the
singular values of a matrix: Let $A,B \in \mathbb{R}^{m\times n}$ be given
matrices, and let $f:\mathbb{R}_+\rightarrow\mathbb{R}_+$ be a concave function
satisfying $f(0)=0$. Then, we have $$ \sum_{i=1}^{\min\{m,n\}} \big|
f(\sigma_i(A)) - f(\sigma_i(B)) \big| \le \sum_{i=1}^{\min\{m,n\}}
f(\sigma_i(A-B)), $$ where $\sigma_i(\cdot)$ denotes the $i$--th largest
singular value of a matrix. This answers an open question that is of interest
to both the compressive sensing and linear algebra communities. In particular,
by taking $f(\cdot)=(\cdot)^p$ for any $p \in (0,1]$, we obtain a perturbation
inequality for the so--called Schatten $p$--quasi--norm, which allows us to
confirm the validity of a number of previously conjectured conditions for the
recovery of low--rank matrices via the popular Schatten $p$--quasi--norm
heuristic. We believe that our result will find further applications,
especially in the study of low--rank matrix recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0378</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0378</id><created>2012-09-03</created><updated>2013-03-28</updated><authors><author><keyname>Dam&#xe1;sio</keyname><forenames>C. V.</forenames></author><author><keyname>Analyti</keyname><forenames>A.</forenames></author><author><keyname>Antoniou</keyname><forenames>G.</forenames></author></authors><title>Provenance for SPARQL queries</title><categories>cs.DB</categories><comments>22 pages, extended version of the ISWC 2012 paper including proofs</comments><journal-ref>The Semantic Web - ISWC 2012 - 11th International Semantic Web
  Conference, Part I, LNCS, Vol. 7649, ISBN 978-3-642-35175-4, pp. 625-640,
  Springer Berlin Heidelberg, November 2012</journal-ref><doi>10.1007/978-3-642-35176-1_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining trust of data available in the Semantic Web is fundamental for
applications and users, in particular for linked open data obtained from SPARQL
endpoints. There exist several proposals in the literature to annotate SPARQL
query results with values from abstract models, adapting the seminal works on
provenance for annotated relational databases. We provide an approach capable
of providing provenance information for a large and significant fragment of
SPARQL 1.1, including for the first time the major non-monotonic constructs
under multiset semantics. The approach is based on the translation of SPARQL
into relational queries over annotated relations with values of the most
general m-semiring, and in this way also refuting a claim in the literature
that the OPTIONAL construct of SPARQL cannot be captured appropriately with the
known abstract models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0410</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0410</id><created>2012-09-03</created><authors><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Valle</keyname><forenames>Eduardo</forenames></author><author><keyname>Mariano</keyname><forenames>Nathan</forenames></author><author><keyname>Torres</keyname><forenames>Ricardo</forenames></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><suffix>Jr</suffix></author><author><keyname>Saltz</keyname><forenames>Joel H.</forenames></author></authors><title>Approximate Similarity Search for Online Multimedia Services on
  Distributed CPU-GPU Platforms</title><categories>cs.MM cs.DB cs.DC</categories><comments>25 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Similarity search in high-dimentional spaces is a pivotal operation found a
variety of database applications. Recently, there has been an increase interest
in similarity search for online content-based multimedia services. Those
services, however, introduce new challenges with respect to the very large
volumes of data that have to be indexed/searched, and the need to minimize
response times observed by the end-users. Additionally, those users dynamically
interact with the systems creating fluctuating query request rates, requiring
the search algorithm to adapt in order to better utilize the underline hardware
to reduce response times. In order to address these challenges, we introduce
hypercurves, a flexible framework for answering approximate k-nearest neighbor
(kNN) queries for very large multimedia databases, aiming at online
content-based multimedia services. Hypercurves executes on hybrid CPU--GPU
environments, and is able to employ those devices cooperatively to support
massive query request rates. In order to keep the response times optimal as the
request rates vary, it employs a novel dynamic scheduler to partition the work
between CPU and GPU. Hypercurves was throughly evaluated using a large database
of multimedia descriptors. Its cooperative CPU--GPU execution achieved
performance improvements of up to 30x when compared to the single CPU-core
version. The dynamic work partition mechanism reduces the observed query
response times in about 50% when compared to the best static CPU--GPU task
partition configuration. In addition, Hypercurves achieves superlinear
scalability in distributed (multi-node) executions, while keeping a high
guarantee of equivalence with its sequential version --- thanks to the proof of
probabilistic equivalence, which supported its aggressive parallelization
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0424</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0424</id><created>2012-09-03</created><authors><author><keyname>Mercure</keyname><forenames>Jean-Francois</forenames></author></authors><title>On the changeover timescales of technology transitions and induced
  efficiency changes: an overarching theory</title><categories>math.DS cs.SI physics.soc-ph q-fin.GN</categories><comments>18 pages, 5 figures, submitted to Technology Forecasting and Social
  Change</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general theory that aims at explaining timescales
observed empirically in technology transitions and predicting those of future
transitions. This framework is used further to derive a theory for exploring
the dynamics that underlie the complex phenomenon of irreversible and path
dependent price or policy induced efficiency changes. Technology transitions
are known to follow patterns well described by logistic functions, which should
more rigorously be modelled mathematically using the Lotka-Volterra family of
differential equations, originally developed to described the population growth
of competing species. The dynamic evolution of technology has also been
described theoretically using evolutionary dynamics similar to that observed in
nature. The theory presented here joins both approaches and presents a
methodology for predicting changeover time constants in order to describe real
systems of competing technologies. The problem of price or policy induced
efficiency changes becomes naturally explained using this framework. Examples
of application are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0430</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0430</id><created>2012-09-03</created><updated>2013-04-24</updated><authors><author><keyname>Mishra</keyname><forenames>B.</forenames></author><author><keyname>Meyer</keyname><forenames>G.</forenames></author><author><keyname>Bonnabel</keyname><forenames>S.</forenames></author><author><keyname>Sepulchre</keyname><forenames>R.</forenames></author></authors><title>Fixed-rank matrix factorizations and Riemannian low-rank optimization</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of learning a linear regression model whose
parameter is a large fixed-rank non-symmetric matrix, we consider the
optimization of a smooth cost function defined on the set of fixed-rank
matrices. We adopt the geometric framework of optimization on Riemannian
quotient manifolds. We study the underlying geometries of several well-known
fixed-rank matrix factorizations and then exploit the Riemannian quotient
geometry of the search space in the design of a class of gradient descent and
trust-region algorithms. The proposed algorithms generalize our previous
results on fixed-rank symmetric positive semidefinite matrices, apply to a
broad range of applications, scale to high-dimensional problems and confer a
geometric basis to recent contributions on the learning of fixed-rank
non-symmetric matrices. We make connections with existing algorithms in the
context of low-rank matrix completion and discuss relative usefulness of the
proposed framework. Numerical experiments suggest that the proposed algorithms
compete with the state-of-the-art and that manifold optimization offers an
effective and versatile framework for the design of machine learning algorithms
that learn a fixed-rank matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0435</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0435</id><created>2012-09-03</created><updated>2012-10-11</updated><authors><author><keyname>Barrett</keyname><forenames>Jonathan</forenames></author><author><keyname>Colbeck</keyname><forenames>Roger</forenames></author><author><keyname>Kent</keyname><forenames>Adrian</forenames></author></authors><title>Unconditionally secure device-independent quantum key distribution with
  only two devices</title><categories>quant-ph cs.CR</categories><comments>11 pages, minor changes</comments><journal-ref>Physical Review A 86, 062326 (2012)</journal-ref><doi>10.1103/PhysRevA.86.062326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-independent quantum key distribution is the task of using
uncharacterized quantum devices to establish a shared key between two users. If
a protocol is secure regardless of the device behaviour, it can be used to
generate a shared key even if the supplier of the devices is malicious. To
date, all device-independent quantum key distribution protocols that are known
to be secure require separate isolated devices for each entangled pair, which
is a significant practical limitation. We introduce a protocol that requires
Alice and Bob to have only one device each. Although inefficient, our protocol
is unconditionally secure against an adversarial supplier limited only by
locally enforced signalling constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0442</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0442</id><created>2012-09-03</created><authors><author><keyname>Varsaki</keyname><forenames>Eleni E.</forenames></author><author><keyname>Gizani</keyname><forenames>Nectaria A. B.</forenames></author><author><keyname>Fotopoulos</keyname><forenames>Vassilis</forenames></author><author><keyname>Skodras</keyname><forenames>Athanassios N.</forenames></author></authors><title>Alternative Astronomical FITS imaging</title><categories>astro-ph.IM astro-ph.CO cs.MM physics.data-an</categories><comments>2 pages, conference, The 11th Asian-Pacific Regional IAU Meeting
  2011, NARIT Conference Series, Vol. 1, S. Komonjinda, Y. Kovalev, and D.
  Ruffolo, eds</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomical radio maps are presented mainly in FITS format. Astronomical
Image Processing Software (AIPS) uses a set of tables attached to the output
map to include all sorts of information concerning the production of the image.
However this information together with information on the flux and noise of the
map is lost as soon as the image of the radio source in fits or other format is
extracted from AIPS. This information would have been valuable to another
astronomer who just uses NED, for example, to download the map. In the current
work, we show a method of data hiding inside the radio map, which can be
preserved under transformations, even for example while the format of the map
is changed from fits to other lossless available image formats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0444</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0444</id><created>2012-09-03</created><updated>2012-09-04</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Seuret</keyname><forenames>Alexandre</forenames></author></authors><title>Affine characterizations of minimum and mode-dependent dwell-times for
  uncertain linear switched systems</title><categories>math.OC cs.SY math.CA math.DS</categories><comments>7 pages, 1 figure, To appear in IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An alternative approach for minimum and mode-dependent dwell-time
characterization for switched systems is derived. The proposed technique is
related to Lyapunov looped-functionals, a new type of functionals leading to
stability conditions affine in the system matrices, unlike standard results for
minimum dwell-time. These conditions are expressed as infinite-dimensional LMIs
which can be solved using recent polynomial optimization techniques such as
sum-of-squares. The specific structure of the conditions is finally utilized in
order to derive dwell-time stability results for uncertain switched systems.
Several examples illustrate the efficiency of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0488</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0488</id><created>2012-09-03</created><authors><author><keyname>Kober</keyname><forenames>Jens</forenames></author><author><keyname>Peters</keyname><forenames>Jan</forenames></author></authors><title>Learning Prioritized Control of Motor Primitives</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tasks in robotics can be decomposed into sub-tasks that are performed
simultaneously. In many cases, these sub-tasks cannot all be achieved jointly
and a prioritization of such sub-tasks is required to resolve this issue. In
this paper, we discuss a novel learning approach that allows to learn a
prioritized control law built on a set of sub-tasks represented by motor
primitives. The primitives are executed simultaneously but have different
priorities. Primitives of higher priority can override the commands of the
conflicting lower priority ones. The dominance structure of these primitives
has a significant impact on the performance of the prioritized control law. We
evaluate the proposed approach with a ball bouncing task on a Barrett WAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0490</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0490</id><created>2012-09-03</created><updated>2012-09-08</updated><authors><author><keyname>Rahmati</keyname><forenames>Ahmad</forenames></author><author><keyname>Shepard</keyname><forenames>Clayton</forenames></author><author><keyname>Tossell</keyname><forenames>Chad</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Kortum</keyname><forenames>Philip</forenames></author></authors><title>Practical Context Awareness: Measuring and Utilizing the Context
  Dependency of Mobile Usage</title><categories>cs.HC</categories><report-no>Technical Report 2012-08-31, Rice University</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context information brings new opportunities for efficient and effective
applications and services on mobile devices. A wide range of research has
exploited context dependency, i.e., the relations between context(s) and the
outcome, to achieve significant, quantified, performance gains for a variety of
applications. These works often have to deal with the challenges of multiple
sources of context that can lead to a sparse training data set, and the
challenge of energy hungry context sensors. Often, they address these
challenges in an application specific and ad-hoc manner. We liberate mobile
application designers and researchers from these burdens by providing a
methodical approach to these challenges. In particular, we 1) define and
measure the context-dependency of three fundamental types of mobile usage in an
application agnostic yet practical manner, which can provide clear insight into
the performance of potential ap-plication. 2) Address the challenge of data
sparseness when dealing with multiple and different sources of context in a
systematic manner. 3) Present SmartContext to address the energy challenge by
automatically selecting among context sources while ensuring the minimum
accuracy for each estimation event is met.
  Our analysis and findings are based on usage and context traces collected in
real-life settings from 24 iPhone users over a period of one year. We present
findings regarding the context dependency of the three principal types of
mobile usage; visited websites, phone calls, and app usage. Yet, our
methodology and the lessons we learn can be readily extended to other
context-dependent mobile usage and system resources as well. Our findings guide
the development of context aware systems, and highlight the challenges and
expectations regarding the context dependency of mobile usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0491</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0491</id><created>2012-09-03</created><updated>2013-04-18</updated><authors><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Valaee</keyname><forenames>Shahrokh</forenames></author></authors><title>Coding Opportunity Densification Strategies for Instantly Decodable
  Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we aim to identify the strategies that can maximize and
monotonically increase the density of the coding opportunities in instantly
decodable network coding (IDNC).Using the well-known graph representation of
IDNC, first derive an expression for the exact evolution of the edge set size
after the transmission of any arbitrary coded packet. From the derived
expressions, we show that sending commonly wanted packets for all the receivers
can maximize the number of coding opportunities. Since guaranteeing such
property in IDNC is usually impossible, this strategy does not guarantee the
achievement of our target. Consequently, we further investigate the problem by
deriving the expectation of the edge set size evolution after ignoring the
identities of the packets requested by the different receivers and considering
only their numbers. We then employ this expected expression to show that
serving the maximum number of receivers having the largest numbers of missing
packets and erasure probabilities tends to both maximize and monotonically
increase the expected density of coding opportunities. Simulation results
justify our theoretical findings. Finally, we validate the importance of our
work through two case studies showing that our identified strategy outperforms
the step-by-step service maximization solution in optimizing both the IDNC
completion delay and receiver goodput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0514</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0514</id><created>2012-09-03</created><authors><author><keyname>Belavkin</keyname><forenames>Roman V.</forenames></author><author><keyname>Channon</keyname><forenames>Alastair</forenames></author><author><keyname>Aston</keyname><forenames>Elizabeth</forenames></author><author><keyname>Aston</keyname><forenames>John</forenames></author><author><keyname>Krasovec</keyname><forenames>Rok</forenames></author><author><keyname>Knight</keyname><forenames>Christopher G.</forenames></author></authors><title>Monotonicity of Fitness Landscapes and Mutation Rate Control</title><categories>q-bio.PE cs.IT cs.NE math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The typical view in evolutionary biology is that mutation rates are
minimised. Contrary to that view, studies in combinatorial optimisation and
search have shown a clear advantage of using variable mutation rates as a
control parameter to optimise the performance of evolutionary algorithms.
Ronald Fisher's work is the basis of much biological theory in this area. He
used Euclidean geometry of continuous, infinite phenotypic spaces to study the
relation between mutation size and expected fitness of the offspring. Here we
develop a general theory of optimal mutation rate control that is based on the
alternative geometry of discrete and finite spaces of DNA sequences. We define
the monotonic properties of fitness landscapes, which allows us to relate
fitness to the topology of genotypes and mutation size. First, we consider the
case of a perfectly monotonic fitness landscape, in which the optimal mutation
rate control functions can be derived exactly or approximately depending on
additional constraints of the problem. Then we consider the general case of
non-monotonic landscapes. We use the ideas of local and weak monotonicity to
show that optimal mutation rate control functions exist in any such landscape
and that they resemble control functions in a monotonic landscape at least in
some neighbourhood of a fitness maximum. Generally, optimal mutation rates
increase when fitness decreases, and the increase of mutation rate is more
rapid in landscapes that are less monotonic (more rugged). We demonstrate these
relationships by obtaining and analysing approximately optimal mutation rate
control functions in 115 complete landscapes of binding scores between DNA
sequences and transcription factors. We discuss the relevance of these findings
to living organisms, including the phenomenon of stress-induced mutagenesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0516</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0516</id><created>2012-09-03</created><updated>2013-02-15</updated><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>When is Metric Temporal Logic Expressively Complete?</title><categories>cs.LO</categories><comments>Submitted to ICALP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A seminal result of Kamp is that over the reals Linear Temporal Logic (LTL)
has the same expressive power as first-order logic with binary order relation &lt;
and monadic predicates. A key question is whether there exists an analogue of
Kamp's theorem for Metric Temporal Logic (MTL) -- a generalization of LTL in
which the Until and Since modalities are annotated with intervals that express
metric constraints. Hirshfeld and Rabinovich gave a negative answer, showing
that first-order logic with binary order relation &lt; and unary function +1 is
strictly more expressive than MTL with integer constants. However, a recent
result of Hunter, Ouaknine and Worrell shows that with rational timing
constants, MTL has the same expressive power as first-order logic, giving a
positive answer. In this paper we generalize these results by giving a precise
characterization of those sets of constants for which MTL and first-order logic
have the same expressive power. We also show that full first-order
expressiveness can be recovered with the addition of counting modalities,
strongly supporting the assertion of Hirshfeld and Rabinovich that Q2MLO is one
of the most expressive decidable fragments of FO(&lt;,+1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0518</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0518</id><created>2012-09-03</created><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author></authors><title>The expressiveness of MTL with counting</title><categories>cs.LO</categories><comments>Preliminary version - proof notes only</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that MTL with integer endpoints is unable to express all of
monadic first-order logic of order and metric (FO(&lt;,+1)). Indeed, MTL is unable
to express the counting modalities $C_n$ that assert a properties holds $n$
times in the next time interval. We show that MTL with the counting modalities,
MTL+C, is expressively complete for FO(&lt;,+1). This result strongly supports the
assertion of Hirshfeld and Rabinovich that Q2MLO is the most expressive
decidable fragments of FO(&lt;,+1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0521</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0521</id><created>2012-09-03</created><authors><author><keyname>Delalleau</keyname><forenames>Olivier</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Efficient EM Training of Gaussian Mixtures with Missing Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data-mining applications, we are frequently faced with a large fraction of
missing entries in the data matrix, which is problematic for most discriminant
machine learning algorithms. A solution that we explore in this paper is the
use of a generative model (a mixture of Gaussians) to compute the conditional
expectation of the missing variables given the observed variables. Since
training a Gaussian mixture with many different patterns of missing values can
be computationally very expensive, we introduce a spanning-tree based algorithm
that significantly speeds up training in these conditions. We also observe that
good results can be obtained by using the generative model to fill-in the
missing values for a separate discriminant learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0532</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0532</id><created>2012-09-04</created><updated>2012-10-09</updated><authors><author><keyname>Zhang</keyname><forenames>Shuai</forenames></author><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author></authors><title>Controlling the Error Floor in LDPC Decoding</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures, Submitted to IEEE Trans. Comm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The error floor of LDPC is revisited as an effect of dynamic message behavior
in the so-called absorption sets of the code. It is shown that if the signal
growth in the absorption sets is properly balanced by the growth of
set-external messages, the error floor can be lowered to essentially
arbitrarily low levels. Importance sampling techniques are discussed and used
to verify the analysis, as well as to discuss the impact of iterations and
message quantization on the code performance in the ultra-low BER (error floor)
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0533</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0533</id><created>2012-09-04</created><updated>2013-05-01</updated><authors><author><keyname>Harvey</keyname><forenames>David</forenames></author></authors><title>A subquadratic algorithm for computing the n-th Bernoulli number</title><categories>math.NT cs.DS cs.NA</categories><comments>few minor changes, to appear in Mathematics of Computation</comments><msc-class>11B68, 11Y55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm that computes the n-th Bernoulli number in n^(4/3
+ o(1)) bit operations. This improves on previous algorithms that had
complexity n^(2 + o(1)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0537</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0537</id><created>2012-09-04</created><authors><author><keyname>Zhang</keyname><forenames>Chen</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Wei</keyname><forenames>Guo</forenames></author></authors><title>One-sided Precoder Designs on Manifolds for Interference Alignment</title><categories>cs.IT math.IT</categories><comments>10 pages, 9 figures, submitted to Internation Journal of
  Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is a technique recently shown to achieve the
maximum degrees of freedom (DoF) of $K$-user interference channel. In this
paper, we focus on the precoder designs on manifolds for IA. By restricting the
optimization only at the transmitters' side, it will alleviate the overhead
induced by alternation between the forward and reverse links significantly.
Firstly a classical steepest descent (SD) algorithm in multi-dimensional
complex space is proposed to achieve feasible IA. Then we reform the
optimization problem on Stiefel manifold, and propose a novel SD algorithm
based on this manifold with lower dimensions. Moreover, aiming at further
reducing the complexity, Grassmann manifold is introduced to derive
corresponding algorithm for reaching the perfect IA. Numerical simulations show
that the proposed algorithms on manifolds have better convergence performance
and higher system capacity than previous methods, also achieve the maximum DoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0550</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0550</id><created>2012-09-04</created><authors><author><keyname>Bokhari</keyname><forenames>Fawaz</forenames></author><author><keyname>Zaruba</keyname><forenames>Gergely</forenames></author></authors><title>On the Use of Smart Ants for Efficient Routing in Wireless Mesh Networks</title><categories>cs.NI</categories><comments>18 pages, 6 figures</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN). Vol.
  4, No. 2, pp. 117-134, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing in wireless mesh networks (WMNs) has been an active area of research
for the last several years. In this paper, we address the problem of packet
routing for efficient data forwarding in wireless mesh networks (WMNs) with the
help of smart ants acting as intelligent agents. The aim of this paper is to
study the use of such biologically inspired agents to effectively route the
packets in WMNs. In particular, we propose AntMesh, a distributed
interference-aware data forwarding algorithm which enables the use of smart
ants to probabilistically and concurrently perform the routing and data
forwarding in order to stochastically solve a dynamic network routing problem.
AntMesh belongs to the class of routing algorithms inspired by the behaviour of
real ants which are known to find a shortest path between their nest and a food
source. In addition, AntMesh has the capability to effectively utilize the
space/channel diversity typically common in multi radio WMNs and to discover
high throughput paths with less inter-flow and intra-flow interference while
conventional wireless network routing protocols fail to do so. We implement our
smart ant-based routing algorithm in ns-2 and carry out extensive evaluation.
We demonstrate the stability of AntMesh in terms of how quickly it adapts
itself to the changing dynamics or load on the network. We tune the parameters
of AntMesh algorithm to study the effect on its performance in terms of the
routing load and end-to-end delay and have tested its performance under various
network scenarios particularly fixed nodes mesh networks and also on mobile WMN
scenarios. The results obtained show AntMesh's advantages that make it a
valuable candidate to operate in mesh networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0570</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0570</id><created>2012-09-04</created><authors><author><keyname>Ibing</keyname><forenames>Andreas</forenames></author></authors><title>On Side Channel Cryptanalysis and Sequential Decoding</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach for side channel cryptanalysis with iterative
approximate Bayesian inference, based on sequential decoding methods.
Reliability information about subkey hypotheses is generated in the form of
likelihoods, and sets of subkey hypothesis likelihoods are optimally combined
into key bit log likelihood ratios. The redundancy of expanded keys in
multi-round cryptographic schemes is exploited to correct round key estimation
errors. This is achieved by sequential decoding, where subkey candidates are
sorted by a probabilistic path metric and iteratively extended. The M-algorithm
is presented as a concrete implementation example with deterministic run-time
behaviour. The resulting algorithm contains previous hard decision differential
analysis as special case for single-round analysis and M=1, and is strictly
more accurate otherwise. The trade-off between estimation accuracy and
complexity is scalable by parameter choice. The proposed algorithm is
simulatively shown in an example scenario to reduce the number of required side
channel traces compared to standard differential analysis by a factor of two
when run with reasonable complexity, for the whole investigated signal-to-noise
ratio range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0571</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0571</id><created>2012-09-04</created><updated>2012-10-17</updated><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Stainer</keyname><forenames>Am&#xe9;lie</forenames></author><author><keyname>Sutre</keyname><forenames>Gr&#xe9;goire</forenames></author></authors><title>Reachability of Communicating Timed Processes</title><categories>cs.LO</categories><comments>Extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the reachability problem for communicating timed processes, both in
discrete and dense time. Our model comprises automata with local timing
constraints communicating over unbounded FIFO channels. Each automaton can only
access its set of local clocks; all clocks evolve at the same rate. Our main
contribution is a complete characterization of decidable and undecidable
communication topologies, for both discrete and dense time. We also obtain
complexity results, by showing that communicating timed processes are at least
as hard as Petri nets; in the discrete time, we also show equivalence with
Petri nets. Our results follow from mutual topology-preserving reductions
between timed automata and (untimed) counter automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0572</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0572</id><created>2012-09-04</created><updated>2012-11-01</updated><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>In-place associative integer sorting</title><categories>cs.DS</categories><comments>25 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.3668, arXiv:1210.1771, arXiv:1209.1942, arXiv:1209.4714</comments><msc-class>68P05, 68P10</msc-class><acm-class>E.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A novel integer value-sorting technique is proposed replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms.
It requires only constant amount of additional memory. The technique is
inspired from one of the ordinal theories of &quot;serial order in behavior&quot; and
explained by the analogy with the three main stages in the formation and
retrieval of memory in cognitive neuroscience namely (i) practicing, (ii)
storing and (iii) retrieval.
  Although not suitable for integer rank-sorting where the problem is to put an
array of elements into ascending or descending order by their numeric keys,
each of which is an integer, the technique seems to be efficient and applicable
to rank-sorting, as well as other problems such as hashing, searching, element
distinction, succinct data structures, gaining space, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0578</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0578</id><created>2012-09-04</created><authors><author><keyname>D&#xed;ez</keyname><forenames>Alicia</forenames></author><author><keyname>Tapiador</keyname><forenames>Antonio</forenames></author></authors><title>Social Cheesecake: An UX-driven designed interface for managing contacts</title><categories>cs.HC cs.SI</categories><comments>Preprint of IADIS WWW/Internet 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Social network management interfaces should consider separation of contexts
and tie strength. This paper shows the design process upon building the Social
Cheesecake, an interface that addresses both issues. Paper and screen
prototyping were used in the design process. Paper prototype interactions
helped to explore the metaphors in the domain, while screen prototype
consolidated the model. The prototype was finally built using HTML5 and
Javascript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0579</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0579</id><created>2012-09-04</created><updated>2015-06-11</updated><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author></authors><title>Flip Distance Between Triangulations of a Simple Polygon is NP-Complete</title><categories>cs.CG</categories><comments>Accepted version</comments><journal-ref>Discrete and Computational Geometry (DCG), 54(2), September 2015,
  pp. 368-389</journal-ref><doi>10.1007/s00454-015-9709-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let T be a triangulation of a simple polygon. A flip in T is the operation of
removing one diagonal of T and adding a different one such that the resulting
graph is again a triangulation. The flip distance between two triangulations is
the smallest number of flips required to transform one triangulation into the
other. For the special case of convex polygons, the problem of determining the
shortest flip distance between two triangulations is equivalent to determining
the rotation distance between two binary trees, a central problem which is
still open after over 25 years of intensive study. We show that computing the
flip distance between two triangulations of a simple polygon is NP-complete.
This complements a recent result that shows APX-hardness of determining the
flip distance between two triangulations of a planar point set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0598</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0598</id><created>2012-09-04</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Di Bartolomeo</keyname><forenames>Marco</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author></authors><title>Implementing a Partitioned 2-page Book Embedding Testing Algorithm</title><categories>cs.DS</categories><comments>21 pages, 11 figures, published at 20th International Symposyum on
  Graph Drawing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a book embedding the vertices of a graph are placed on the &quot;spine&quot; of a
&quot;book&quot; and the edges are assigned to &quot;pages&quot; so that edges on the same page do
not cross. In the Partitioned 2-page Book Embedding problem egdes are
partitioned into two sets E_1 and E_2, the pages are two, the edges of E_1 are
assigned to page 1, and the edges of E_2 are assigned to page 2. The problem
consists of checking if an ordering of the vertices exists along the spine so
that the edges of each page do not cross. Hong and Nagamochi give an
interesting and complex linear time algorithm for tackling Partitioned 2-page
Book Embedding based on SPQR-trees. We show an efficient implementation of this
algorithm and show its effectiveness by performing a number of experimental
tests. Because of the relationships between Partitioned 2-page Book Embedding
and clustered planarity we yield as a side effect an implementation of a
clustered planarity testing where the graph has exactly two clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0616</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0616</id><created>2012-09-04</created><authors><author><keyname>Bouzarkouna</keyname><forenames>Zyed</forenames><affiliation>IFPEN</affiliation></author><author><keyname>Ding</keyname><forenames>Didier Yu</forenames><affiliation>IFPEN</affiliation></author><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Well Placement Optimization under Uncertainty with CMA-ES Using the
  Neighborhood</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>13th European Conference on the Mathematics of Oil Recovery ECMOR
  2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the well placement problem, as well as in other field development
optimization problems, geological uncertainty is a key source of risk affecting
the viability of field development projects. Well placement problems under
geological uncertainty are formulated as optimization problems in which the
objective function is evaluated using a reservoir simulator on a number of
possible geological realizations. In this paper, we present a new approach to
handle geological uncertainty for the well placement problem with a reduced
number of reservoir simulations. The proposed approach uses already simulated
well configurations in the neighborhood of each well configuration for the
objective function evaluation. We use thus only one single reservoir simulation
performed on a randomly chosen realization together with the neighborhood to
estimate the objective function instead of using multiple simulations on
multiple realizations. This approach is combined with the stochastic optimizer
CMA-ES. The proposed approach is shown on the benchmark reservoir case PUNQ-S3
to be able to capture the geological uncertainty using a smaller number of
reservoir simulations. This approach is compared to the reference approach
using all the possible realizations for each well configuration, and shown to
be able to reduce significantly the number of reservoir simulations (around
80%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0622</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0622</id><created>2012-09-04</created><authors><author><keyname>Nishadh</keyname><forenames>K. A.</forenames></author><author><keyname>Azeez</keyname><forenames>P. A</forenames></author></authors><title>Sensor Webs for Environmental research</title><categories>cs.SY cs.NI</categories><comments>6 pages, 1 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ongoing massive global environmental changes and the past learnings have
highlighted the urgency and importance of further detailed understanding of the
earth system and implementation of social ecological sustainability measures in
a much more effective and transparent manner. This short communication discuss
the potential of sensor webs in addressing those research challenges,
highlighting it in the context of air pollution issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0643</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0643</id><created>2012-09-04</created><updated>2012-09-28</updated><authors><author><keyname>Gawlitza</keyname><forenames>Thomas Martin</forenames><affiliation>The University of Sydney</affiliation></author><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>CNRS/VERIMAG</affiliation></author></authors><title>Invariant Generation through Strategy Iteration in Succinctly
  Represented Control Flow Graphs</title><categories>cs.PL</categories><comments>35 pages, conference version published at ESOP 2011, this version is
  a CoRR version of our submission to Logical Methods in Computer Science</comments><proxy>LMCS</proxy><acm-class>D.2.4, F.3.1 (D.2.1, D.2.4, D.3.1, E.1), F.3.2 (D.3.1)</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  30, 2012) lmcs:689</journal-ref><doi>10.2168/LMCS-8(3:29)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing numerical invariants of programs, for
instance bounds on the values of numerical program variables. More
specifically, we study the problem of performing static analysis by abstract
interpretation using template linear constraint domains. Such invariants can be
obtained by Kleene iterations that are, in order to guarantee termination,
accelerated by widening operators. In many cases, however, applying this form
of extrapolation leads to invariants that are weaker than the strongest
inductive invariant that can be expressed within the abstract domain in use.
Another well-known source of imprecision of traditional abstract interpretation
techniques stems from their use of join operators at merge nodes in the control
flow graph. The mentioned weaknesses may prevent these methods from proving
safety properties. The technique we develop in this article addresses both of
these issues: contrary to Kleene iterations accelerated by widening operators,
it is guaranteed to yield the strongest inductive invariant that can be
expressed within the template linear constraint domain in use. It also eschews
join operators by distinguishing all paths of loop-free code segments. Formally
speaking, our technique computes the least fixpoint within a given template
linear constraint domain of a transition relation that is succinctly expressed
as an existentially quantified linear real arithmetic formula. In contrast to
previously published techniques that rely on quantifier elimination, our
algorithm is proved to have optimal complexity: we prove that the decision
problem associated with our fixpoint problem is in the second level of the
polynomial-time hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0652</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0652</id><created>2012-09-04</created><updated>2012-09-18</updated><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author><author><keyname>Cheng</keyname><forenames>Lizhi</forenames></author></authors><title>Necessary and sufficient conditions of solution uniqueness in $\ell_1$
  minimization</title><categories>cs.IT math.IT math.NA math.OC</categories><comments>6 pages; revised version; submitted</comments><journal-ref>Journal of Optimization Theory and Applications 164(1), 109-122,
  2015</journal-ref><doi>10.1007/s10957-014-0581-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the solutions to various convex $\ell_1$ minimization
problems are \emph{unique} if and only if a common set of conditions are
satisfied. This result applies broadly to the basis pursuit model, basis
pursuit denoising model, Lasso model, as well as other $\ell_1$ models that
either minimize $f(Ax-b)$ or impose the constraint $f(Ax-b)\leq\sigma$, where
$f$ is a strictly convex function. For these models, this paper proves that,
given a solution $x^*$ and defining $I=\supp(x^*)$ and $s=\sign(x^*_I)$, $x^*$
is the unique solution if and only if $A_I$ has full column rank and there
exists $y$ such that $A_I^Ty=s$ and $|a_i^Ty|_\infty&lt;1$ for $i\not\in I$. This
condition is previously known to be sufficient for the basis pursuit model to
have a unique solution supported on $I$. Indeed, it is also necessary, and
applies to a variety of other $\ell_1$ models. The paper also discusses ways to
recognize unique solutions and verify the uniqueness conditions numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0654</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0654</id><created>2012-09-04</created><updated>2013-10-30</updated><authors><author><keyname>Gonzalez</keyname><forenames>Adriana</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>Christophe</forenames></author><author><keyname>Antoine</keyname><forenames>Philippe</forenames></author></authors><title>Compressive Optical Deflectometric Tomography: A Constrained
  Total-Variation Minimization Approach</title><categories>cs.CV math.OC</categories><comments>37 pages, 51 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Deflectometric Tomography (ODT) provides an accurate characterization
of transparent materials whose complex surfaces present a real challenge for
manufacture and control. In ODT, the refractive index map (RIM) of a
transparent object is reconstructed by measuring light deflection under
multiple orientations. We show that this imaging modality can be made
&quot;compressive&quot;, i.e., a correct RIM reconstruction is achievable with far less
observations than required by traditional Filtered Back Projection (FBP)
methods. Assuming a cartoon-shape RIM model, this reconstruction is driven by
minimizing the map Total-Variation under a fidelity constraint with the
available observations. Moreover, two other realistic assumptions are added to
improve the stability of our approach: the map positivity and a frontier
condition. Numerically, our method relies on an accurate ODT sensing model and
on a primal-dual minimization scheme, including easily the sensing operator and
the proposed RIM constraints. We conclude this paper by demonstrating the power
of our method on synthetic and experimental data under various compressive
scenarios. In particular, the compressiveness of the stabilized ODT problem is
demonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of
360 incident light angles for moderately noisy sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0663</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0663</id><created>2012-09-04</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Heindel</keyname><forenames>Tobias</forenames></author><author><keyname>Mazza</keyname><forenames>Damiano</forenames></author><author><keyname>Varacca</keyname><forenames>Daniele</forenames></author></authors><title>Computational Complexity of Interactive Behaviors</title><categories>cs.CC cs.LO</categories><comments>17 pages</comments><acm-class>F.1.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of computational complexity focuses on functions and, hence,
studies programs whose interactive behavior is reduced to a simple
question/answer pattern. We propose a broader theory whose ultimate goal is
expressing and analyzing the intrinsic difficulty of fully general interactive
behaviors. To this extent, we use standard tools from concurrency theory,
including labelled transition systems (formalizing behaviors) and their
asynchronous extension (providing causality information). Behaviors are
implemented by means of a multiprocessor machine executing CCS-like processes.
The resulting theory is shown to be consistent with the classical definitions:
when we restrict to functional behaviors (i.e., question/answer patterns), we
recover several standard computational complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0676</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0676</id><created>2012-09-04</created><authors><author><keyname>Urgaonkar</keyname><forenames>Rahul</forenames></author><author><keyname>Ramanathan</keyname><forenames>Ram</forenames></author><author><keyname>Redi</keyname><forenames>Jason</forenames></author><author><keyname>Tetteh</keyname><forenames>William N.</forenames></author></authors><title>Channel Assignment in Dense MC-MR Wireless Networks: Scaling Laws and
  Algorithms</title><categories>cs.NI cs.IT cs.PF math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate optimal channel assignment algorithms that maximize per node
throughput in dense multichannel multi-radio (MC-MR) wireless networks.
Specifically, we consider an MC-MR network where all nodes are within the
transmission range of each other. This situation is encountered in many
real-life settings such as students in a lecture hall, delegates attending a
conference, or soldiers in a battlefield. In this scenario, we show that
intelligent assignment of the available channels results in a significantly
higher per node throughput. We first propose a class of channel assignment
algorithms, parameterized by T (the number of transceivers per node), that can
achieve $\Theta(1/N^{1/T})$ per node throughput using $\Theta(TN^{1-1/T})$
channels. In view of practical constraints on $T$, we then propose another
algorithm that can achieve $\Theta(1/(\log_2 N)^2)$ per node throughput using
only two transceivers per node. Finally, we identify a fundamental relationship
between the achievable per node throughput, the total number of channels used,
and the network size under any strategy. Using analysis and simulations, we
show that our algorithms achieve close to optimal performance at different
operating points on this curve. Our work has several interesting implications
on the optimal network design for dense MC-MR wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0679</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0679</id><created>2012-09-04</created><authors><author><keyname>Carmi</keyname><forenames>Paz</forenames></author><author><keyname>Chaitman-Yerushalmi</keyname><forenames>Lilach</forenames></author></authors><title>Minimum Weight Euclidean t-spanner is NP-Hard</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set P of points in the plane, an Euclidean t-spanner for P is a
geometric graph that preserves the Euclidean distances between every pair of
points in P up to a constant factor t. The weight of a geometric graph refers
to the total length of its edges. In this paper we show that the problem of
deciding whether there exists an Euclidean t-spanner, for a given set of points
in the plane, of weight at most w is NP-hard for every real constant t &gt; 1,
both whether planarity of the t-spanner is required or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0680</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0680</id><created>2012-09-04</created><updated>2016-02-18</updated><authors><author><keyname>Grigore</keyname><forenames>Radu</forenames></author><author><keyname>Tzevelekos</keyname><forenames>Nikos</forenames></author></authors><title>History-Register Automata</title><categories>cs.PL cs.FL</categories><comments>LMCS (improved version of FoSSaCS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs with dynamic allocation are able to create and use an unbounded
number of fresh resources, such as references, objects, files, etc. We propose
History-Register Automata (HRA), a new automata-theoretic formalism for
modelling such programs. HRAs extend the expressiveness of previous approaches
and bring us to the limits of decidability for reachability checks. The
distinctive feature of our machines is their use of unbounded memory sets
(histories) where input symbols can be selectively stored and compared with
symbols to follow. In addition, stored symbols can be consumed or deleted by
reset. We show that the combination of consumption and reset capabilities
renders the automata powerful enough to imitate counter machines, and yields
closure under all regular operations apart from complementation. We moreover
examine weaker notions of HRAs which strike different balances between
expressiveness and effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0684</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0684</id><created>2012-09-04</created><authors><author><keyname>Zarmehri</keyname><forenames>Mohammad Nozari</forenames></author><author><keyname>Aguiar</keyname><forenames>Ana</forenames></author></authors><title>Data Gathering for Sensing Applications in Vehicular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to use Vehicular ad hoc networks (VANET) as the infrastructure for
an urban cyber-physical system for gathering up-to-date data about a city, like
traffic conditions or environmental parameters. In this context, it is critical
to design a data collection protocol that enables retrieving the data from the
vehicles in almost real-time in an efficient way for urban scenarios.
  We propose Back off-based Per-hop Forwarding (BPF), a broadcast-based
receiver-oriented protocol that uses the destination location information to
select the forwarding order among the nodes receiving the packet. BFP does not
require nodes to exchange periodic messages with their neighbors communicating
their locations to keep a low management message overhead. It uses geographic
information about the final destination node in the header of each data packet
to route it in a hop-by-hop basis. It takes advantage of redundant forwarding
to increase packet delivery to a destination, what is more critical in an urban
scenario than in a highway, where the road topology does not represent a
challenge for forwarding.
  We evaluate the performance of the BPF protocol using ns-3 and a Manhattan
grid topology and compare it with well-known broadcast suppression techniques.
Our results show that BPF achieves significantly higher packet delivery rates
at a reduced redundancy cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0687</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0687</id><created>2012-09-04</created><authors><author><keyname>Armando</keyname><forenames>Alessandro</forenames></author><author><keyname>Merlo</keyname><forenames>Alessio</forenames></author><author><keyname>Verderame</keyname><forenames>Luca</forenames></author></authors><title>Security Issues in the Android Cross-Layer Architecture</title><categories>cs.CR cs.OS</categories><comments>7 pages, double column, 4 figures</comments><acm-class>D.4.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of Android has been recently challenged by the discovery of a
number of vulnerabilities involving different layers of the Android stack. We
argue that such vulnerabilities are largely related to the interplay among
layers composing the Android stack. Thus, we also argue that such interplay has
been underestimated from a security point-of-view and a systematic analysis of
the Android interplay has not been carried out yet. To this aim, in this paper
we provide a simple model of the Android cross-layer interactions based on the
concept of flow, as a basis for analyzing the Android interplay. In particular,
our model allows us to reason about the security implications associated with
the cross-layer interactions in Android, including a recently discovered
vulnerability that allows a malicious application to make Android devices
totally unresponsive. We used the proposed model to carry out an empirical
assessment of some flows within the Android cross-layered architecture. Our
experiments indicate that little control is exercised by the Android Security
Framework (ASF) over cross-layer interactions in Android. In particular, we
observed that the ASF lacks in discriminating the originator of a flow and
sensitive security issues arise between the Android stack and the Linux kernel,
thereby indicating that the attack surface of the Android platform is wider
than expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0700</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0700</id><created>2012-09-04</created><authors><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author></authors><title>A Simple Test on 2-Vertex- and 2-Edge-Connectivity</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing a graph on 2-vertex- and 2-edge-connectivity are two fundamental
algorithmic graph problems. For both problems, different linear-time algorithms
with simple implementations are known. Here, an even simpler linear-time
algorithm is presented that computes a structure from which both the 2-vertex-
and 2-edge-connectivity of a graph can be easily &quot;read off&quot;. The algorithm
computes all bridges and cut vertices of the input graph in the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0715</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0715</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>The Synthesis and Analysis of Stochastic Switching Circuits</title><categories>cs.IT math.IT</categories><comments>2 columns, 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic switching circuits are relay circuits that consist of stochastic
switches called pswitches. The study of stochastic switching circuits has
widespread applications in many fields of computer science, neuroscience, and
biochemistry. In this paper, we discuss several properties of stochastic
switching circuits, including robustness, expressibility, and probability
approximation.
  First, we study the robustness, namely, the effect caused by introducing an
error of size \epsilon to each pswitch in a stochastic circuit. We analyze two
constructions and prove that simple series-parallel circuits are robust to
small error perturbations, while general series-parallel circuits are not.
Specifically, the total error introduced by perturbations of size less than
\epsilon is bounded by a constant multiple of \epsilon in a simple
series-parallel circuit, independent of the size of the circuit.
  Next, we study the expressibility of stochastic switching circuits: Given an
integer q and a pswitch set S=\{\frac{1}{q},\frac{2}{q},...,\frac{q-1}{q}\},
can we synthesize any rational probability with denominator q^n (for arbitrary
n) with a simple series-parallel stochastic switching circuit? We generalize
previous results and prove that when q is a multiple of 2 or 3, the answer is
yes. We also show that when q is a prime number larger than 3, the answer is
no.
  Probability approximation is studied for a general case of an arbitrary
pswitch set S=\{s_1,s_2,...,s_{|S|}\}. In this case, we propose an algorithm
based on local optimization to approximate any desired probability. The
analysis reveals that the approximation error of a switching circuit decreases
exponentially with an increasing circuit size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0724</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0724</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Chen</keyname><forenames>Ho-Lin</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Synthesis of Stochastic Flow Networks</title><categories>cs.IT cs.NE math.IT math.PR</categories><comments>2 columns, 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic flow network is a directed graph with incoming edges (inputs)
and outgoing edges (outputs), tokens enter through the input edges, travel
stochastically in the network, and can exit the network through the output
edges. Each node in the network is a splitter, namely, a token can enter a node
through an incoming edge and exit on one of the output edges according to a
predefined probability distribution. Stochastic flow networks can be easily
implemented by DNA-based chemical reactions, with promising applications in
molecular computing and stochastic computing. In this paper, we address a
fundamental synthesis question: Given a finite set of possible splitters and an
arbitrary rational probability distribution, design a stochastic flow network,
such that every token that enters the input edge will exit the outputs with the
prescribed probability distribution.
  The problem of probability transformation dates back to von Neumann's 1951
work and was followed, among others, by Knuth and Yao in 1976. Most existing
works have been focusing on the &quot;simulation&quot; of target distributions. In this
paper, we design optimal-sized stochastic flow networks for &quot;synthesizing&quot;
target distributions. It shows that when each splitter has two outgoing edges
and is unbiased, an arbitrary rational probability \frac{a}{b} with a\leq b\leq
2^n can be realized by a stochastic flow network of size n that is optimal.
Compared to the other stochastic systems, feedback (cycles in networks)
strongly improves the expressibility of stochastic flow networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0726</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0726</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>A Universal Scheme for Transforming Binary Algorithms to Generate Random
  Bits from Loaded Dice</title><categories>cs.IT math.IT math.PR</categories><comments>2 columns, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a universal scheme for transforming an arbitrary
algorithm for biased 2-face coins to generate random bits from the general
source of an m-sided die, hence enabling the application of existing algorithms
to general sources. In addition, we study approaches of efficiently generating
a prescribed number of random bits from an arbitrary biased coin. This
contrasts with most existing works, which typically assume that the number of
coin tosses is fixed, and they generate a variable number of random bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0730</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0730</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Streaming Algorithms for Optimal Generation of Random Bits</title><categories>cs.IT cs.DS math.IT math.PR</categories><comments>2 columns, 13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating random bits from a source of biased coins (the biased is unknown)
is a classical question that was originally studied by von Neumann. There are a
number of known algorithms that have asymptotically optimal information
efficiency, namely, the expected number of generated random bits per input bit
is asymptotically close to the entropy of the source. However, only the
original von Neumann algorithm has a `streaming property' - it operates on a
single input bit at a time and it generates random bits when possible, alas, it
does not have an optimal information efficiency.
  The main contribution of this paper is an algorithm that generates random bit
streams from biased coins, uses bounded space and runs in expected linear time.
As the size of the allotted space increases, the algorithm approaches the
information-theoretic upper bound on efficiency. In addition, we discuss how to
extend this algorithm to generate random bit streams from m-sided dice or
correlated sources such as Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0732</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0732</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Linear Transformations for Randomness Extraction</title><categories>cs.IT cs.CR math.IT math.PR</categories><comments>2 columns, 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-efficient approaches for extracting randomness from imperfect
sources have been extensively studied, but simpler and faster ones are required
in the high-speed applications of random number generation. In this paper, we
focus on linear constructions, namely, applying linear transformation for
randomness extraction. We show that linear transformations based on sparse
random matrices are asymptotically optimal to extract randomness from
independent sources and bit-fixing sources, and they are efficient (may not be
optimal) to extract randomness from hidden Markov sources. Further study
demonstrates the flexibility of such constructions on source models as well as
their excellent information-preserving capabilities. Since linear
transformations based on sparse random matrices are computationally fast and
can be easy to implement using hardware like FPGAs, they are very attractive in
the high-speed applications. In addition, we explore explicit constructions of
transformation matrices. We show that the generator matrices of primitive BCH
codes are good choices, but linear transformations based on such matrices
require more computational time due to their high densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0734</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0734</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Efficiently Extracting Randomness from Imperfect Stochastic Processes</title><categories>cs.IT cs.CR math.IT math.PR</categories><comments>2 columns, 16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of extracting a prescribed number of random bits by
reading the smallest possible number of symbols from non-ideal stochastic
processes. The related interval algorithm proposed by Han and Hoshi has
asymptotically optimal performance; however, it assumes that the distribution
of the input stochastic process is known. The motivation for our work is the
fact that, in practice, sources of randomness have inherent correlations and
are affected by measurement's noise. Namely, it is hard to obtain an accurate
estimation of the distribution. This challenge was addressed by the concepts of
seeded and seedless extractors that can handle general random sources with
unknown distributions. However, known seeded and seedless extractors provide
extraction efficiencies that are substantially smaller than Shannon's entropy
limit. Our main contribution is the design of extractors that have a variable
input-length and a fixed output length, are efficient in the consumption of
symbols from the source, are capable of generating random bits from general
stochastic processes and approach the information theoretic upper bound on
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0735</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0735</id><created>2012-08-31</created><authors><author><keyname>Veberic</keyname><forenames>Darko</forenames></author></authors><title>Lambert W Function for Applications in Physics</title><categories>cs.MS cs.NA physics.comp-ph</categories><comments>9 pages, 12 figures. Extended version of version of arXiv:1003.1628</comments><journal-ref>Computer Physics Communications 183 (2012) 2622-2628</journal-ref><doi>10.1016/j.cpc.2012.07.008</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Lambert W(x) function and its possible applications in physics are
presented. The actual numerical implementation in C++ consists of Halley's and
Fritsch's iterations with initial approximations based on branch-point
expansion, asymptotic series, rational fits, and continued-logarithm recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0736</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0736</id><created>2012-09-04</created><updated>2012-12-02</updated><authors><author><keyname>Murai</keyname><forenames>Fabricio</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author></authors><title>On Set Size Distribution Estimation and the Characterization of Large
  Networks via Sampling</title><categories>math.ST cs.IT cs.SI math.IT stat.TH</categories><report-no>Technical Report UM-CS-2012-023v2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the set size distribution estimation problem, where
elements are randomly sampled from a collection of non-overlapping sets and we
seek to recover the original set size distribution from the samples. This
problem has applications to capacity planning, network theory, among other
areas. Examples of real-world applications include characterizing in-degree
distributions in large graphs and uncovering TCP/IP flow size distributions on
the Internet. We demonstrate that it is hard to estimate the original set size
distribution. The recoverability of original set size distributions presents a
sharp threshold with respect to the fraction of elements that remain in the
sets. If this fraction remains below a threshold, typically half of the
elements in power-law and heavier-than-exponential-tailed distributions, then
the original set size distribution is unrecoverable. We also discuss practical
implications of our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0738</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0738</id><created>2012-09-04</created><updated>2014-06-16</updated><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Romera-Paredes</keyname><forenames>Bernardino</forenames></author></authors><title>Sparse coding for multitask and transfer learning</title><categories>cs.LG stat.ML</categories><comments>International Conference on Machine Learning 2013</comments><msc-class>68Q32, 68T05, 97C30, 46N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of sparse coding and dictionary learning in the
context of multitask and transfer learning. The central assumption of our
learning method is that the tasks parameters are well approximated by sparse
linear combinations of the atoms of a dictionary on a high or infinite
dimensional space. This assumption, together with the large quantity of
available data in the multitask and transfer learning settings, allows a
principled choice of the dictionary. We provide bounds on the generalization
error of this approach, for both settings. Numerical experiments on one
synthetic and two real datasets show the advantage of our method over single
task learning, a previous method based on orthogonal and dense representation
of the tasks and a related method learning task grouping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0740</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0740</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames><affiliation>Andrew</affiliation></author><author><keyname>Anxiao</keyname><affiliation>Andrew</affiliation></author><author><keyname>Jiang</keyname></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Nonuniform Codes for Correcting Asymmetric Errors in Data Storage</title><categories>cs.IT math.IT</categories><comments>2 columns, 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of asymmetric error correcting codes is a topic that was
studied extensively, however, the existing approach for code construction
assumes that every codeword should tolerate $t$ asymmetric errors. Our main
observation is that in contrast to symmetric errors, asymmetric errors are
content dependent. For example, in Z-channels, the all-1 codeword is prone to
have more errors than the all-0 codeword. This motivates us to develop
nonuniform codes whose codewords can tolerate different numbers of asymmetric
errors depending on their Hamming weights. The idea in a nonuniform codes'
construction is to augment the redundancy in a content-dependent way and
guarantee the worst case reliability while maximizing the code size. In this
paper, we first study nonuniform codes for Z-channels, namely, they only suffer
one type of errors, say 1 to 0. Specifically, we derive their upper bounds,
analyze their asymptotic performances, and introduce two general constructions.
Then we extend the concept and results of nonuniform codes to general binary
asymmetric channels, where the error probability for each bit from 0 to 1 is
smaller than that from 1 to 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0741</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0741</id><created>2012-09-04</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Optimal Coordinated Beamforming in the Multicell Downlink with
  Transceiver Impairments</title><categories>cs.IT math.IT</categories><comments>Published at Global Telecommunications Conference (GLOBECOM 2012), 6
  pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical wireless transceivers suffer from a variety of impairments that
distort the transmitted and received signals. Their degrading impact is
particularly evident in modern systems with multiuser transmission, high
transmit power, and low-cost devices, but their existence is routinely ignored
in the optimization literature for multicell transmission. This paper provides
a detailed analysis of coordinated beamforming in the multicell downlink. We
solve two optimization problems under a transceiver impairment model and derive
the structure of the optimal solutions. We show numerically that these
solutions greatly reduce the impact of impairments, compared with beamforming
developed for ideal transceivers. Although the so-called multiplexing gain is
zero under transceiver impairments, we show that the gain of multiplexing can
be large at practical SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0744</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0744</id><created>2012-09-04</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames><affiliation>Andrew</affiliation></author><author><keyname>Anxiao</keyname><affiliation>Andrew</affiliation></author><author><keyname>Jiang</keyname></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Balanced Modulation for Nonvolatile Memories</title><categories>cs.IT math.IT</categories><comments>2 columns, 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a practical writing/reading scheme in nonvolatile
memories, called balanced modulation, for minimizing the asymmetric component
of errors. The main idea is to encode data using a balanced error-correcting
code. When reading information from a block, it adjusts the reading threshold
such that the resulting word is also balanced or approximately balanced.
Balanced modulation has suboptimal performance for any cell-level distribution
and it can be easily implemented in the current systems of nonvolatile
memories. Furthermore, we studied the construction of balanced error-correcting
codes, in particular, balanced LDPC codes. It has very efficient encoding and
decoding algorithms, and it is more efficient than prior construction of
balanced error-correcting codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0748</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0748</id><created>2012-09-04</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Trott</keyname><forenames>Lowell</forenames></author></authors><title>Force-Directed Graph Drawing Using Social Gravity and Scaling</title><categories>cs.CG cs.SI physics.soc-ph</categories><comments>GD2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Force-directed layout algorithms produce graph drawings by resolving a system
of emulated physical forces. We present techniques for using social gravity as
an additional force in force-directed layouts, together with a scaling
technique, to produce drawings of trees and forests, as well as more complex
social networks. Social gravity assigns mass to vertices in proportion to their
network centrality, which allows vertices that are more graph-theoretically
central to be visualized in physically central locations. Scaling varies the
gravitational force throughout the simulation, and reduces crossings relative
to unscaled gravity. In addition to providing this algorithmic framework, we
apply our algorithms to social networks produced by Mark Lombardi, and we show
how social gravity can be incorporated into force-directed Lombardi-style
drawings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0756</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0756</id><created>2012-09-04</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Ohrimenko</keyname><forenames>Olga</forenames></author><author><keyname>Tamassia</keyname><forenames>Roberto</forenames></author></authors><title>Data-Oblivious Graph Drawing Model and Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study graph drawing in a cloud-computing context where data is stored
externally and processed using a small local working storage. We show that a
number of classic graph drawing algorithms can be efficiently implemented in
such a framework where the client can maintain privacy while constructing a
drawing of her graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0781</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0781</id><created>2012-09-04</created><updated>2012-12-17</updated><authors><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>World citation and collaboration networks: uncovering the role of
  geography in science</title><categories>physics.soc-ph cs.DL cs.SI physics.data-an</categories><comments>Published version. 9 pages, 5 figures + Appendix, The world citation
  and collaboration networks at both city and country level are available at
  http://becs.aalto.fi/~rajkp/datasets.html</comments><journal-ref>Scientific Reports 2, 902 (2012)</journal-ref><doi>10.1038/srep00902</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern information and communication technologies, especially the Internet,
have diminished the role of spatial distances and territorial boundaries on the
access and transmissibility of information. This has enabled scientists for
closer collaboration and internationalization. Nevertheless, geography remains
an important factor affecting the dynamics of science. Here we present a
systematic analysis of citation and collaboration networks between cities and
countries, by assigning papers to the geographic locations of their authors'
affiliations. The citation flows as well as the collaboration strengths between
cities decrease with the distance between them and follow gravity laws. In
addition, the total research impact of a country grows linearly with the amount
of national funding for research &amp; development. However, the average impact
reveals a peculiar threshold effect: the scientific output of a country may
reach an impact larger than the world average only if the country invests more
than about 100,000 USD per researcher annually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0785</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0785</id><created>2012-09-04</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Thed N.</forenames></author><author><keyname>Visser</keyname><forenames>Martijn S.</forenames></author></authors><title>Some modifications to the SNIP journal impact indicator</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SNIP (source normalized impact per paper) indicator is an indicator of
the citation impact of scientific journals. The indicator, introduced by Henk
Moed in 2010, is included in Elsevier's Scopus database. The SNIP indicator
uses a source normalized approach to correct for differences in citation
practices between scientific fields. The strength of this approach is that it
does not require a field classification system in which the boundaries of
fields are explicitly defined. In this paper, a number of modifications that
will be made to the SNIP indicator are explained, and the advantages of the
resulting revised SNIP indicator are pointed out. It is argued that the
original SNIP indicator has some counterintuitive properties, and it is shown
mathematically that the revised SNIP indicator does not have these properties.
Empirically, the differences between the original SNIP indicator and the
revised one turn out to be relatively small, although some systematic
differences can be observed. Relations with other source normalized indicators
proposed in the literature are discussed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0800</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0800</id><created>2012-09-04</created><updated>2012-09-25</updated><authors><author><keyname>Holtmann</keyname><forenames>Michael</forenames><affiliation>RWTH Aachen, Lehrstuhl f&#xfc;r Informatik 7, Germany</affiliation></author><author><keyname>Kaiser</keyname><forenames>Lukasz</forenames><affiliation>RWTH Aachen, Mathematische Grundlagen der Informatik, Germany</affiliation></author><author><keyname>Thomas</keyname><forenames>Wolfgang</forenames><affiliation>RWTH Aachen, Lehrstuhl f&#xfc;r Informatik 7, Germany</affiliation></author></authors><title>Degrees of Lookahead in Regular Infinite Games</title><categories>cs.FL</categories><comments>LMCS submission</comments><proxy>LMCS</proxy><acm-class>D.2.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  27, 2012) lmcs:922</journal-ref><doi>10.2168/LMCS-8(3:24)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study variants of regular infinite games where the strict alternation of
moves between the two players is subject to modifications. The second player
may postpone a move for a finite number of steps, or, in other words, exploit
in his strategy some lookahead on the moves of the opponent. This captures
situations in distributed systems, e.g. when buffers are present in
communication or when signal transmission between components is deferred. We
distinguish strategies with different degrees of lookahead, among them being
the continuous and the bounded lookahead strategies. In the first case the
lookahead is of finite possibly unbounded size, whereas in the second case it
is of bounded size. We show that for regular infinite games the solvability by
continuous strategies is decidable, and that a continuous strategy can always
be reduced to one of bounded lookahead. Moreover, this lookahead is at most
doubly exponential in the size of a given parity automaton recognizing the
winning condition. We also show that the result fails for non-regular
gamesxwhere the winning condition is given by a context-free omega-language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0802</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0802</id><created>2012-09-04</created><authors><author><keyname>Borges</keyname><forenames>Nerio</forenames></author></authors><title>A sufficient condition for first order non-definability of arrowing
  problems</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We here present a sufficient condition for general arrowing problems to be
non definable in first order logic, based in well known tools of finite model
theory e.g. Hanf's Theorem and known concepts in finite combinatorics, like
senders and determiners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0811</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0811</id><created>2012-09-04</created><authors><author><keyname>Wang</keyname><forenames>Yongqiang</forenames></author><author><keyname>Doyle</keyname><forenames>Francis J.</forenames><suffix>III</suffix></author></authors><title>Exponential synchronization rate of Kuramoto oscillators in the presence
  of a pacemaker</title><categories>cs.SY nlin.AO</categories><comments>Accepted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exponential synchronization rate is addressed for Kuramoto oscillators in
the presence of a pacemaker. When natural frequencies are identical, we prove
that synchronization can be ensured even when the phases are not constrained in
an open half-circle, which improves the existing results in the literature. We
derive a lower bound on the exponential synchronization rate, which is proven
to be an increasing function of pacemaker strength, but may be an increasing or
decreasing function of local coupling strength. A similar conclusion is
obtained for phase locking when the natural frequencies are non-identical. An
approach to trapping phase differences in an arbitrary interval is also given,
which ensures synchronization in the sense that synchronization error can be
reduced to an arbitrary level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0814</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0814</id><created>2012-09-04</created><authors><author><keyname>Wang</keyname><forenames>Yongqiang</forenames></author><author><keyname>Nunez</keyname><forenames>Felipe</forenames></author><author><keyname>Doyle</keyname><forenames>Francis J.</forenames><suffix>III</suffix></author></authors><title>Increasing sync rate of pulse-coupled oscillators via phase response
  function design: theory and application to wireless networks</title><categories>cs.SY nlin.AO</categories><comments>Accepted to IEEE Transactions on Control Systems Technology</comments><doi>10.1109/TCST.2012.2205254</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the synchronization rate of weakly connected
pulse-coupled oscillators (PCOs). We prove that besides coupling strength, the
phase response function is also a determinant of synchronization rate. Inspired
by the result, we propose to increase the synchronization rate of PCOs by
designing the phase response function. This has important significance in
PCO-based clock synchronization of wireless networks. By designing the phase
response function, synchronization rate is increased even under a fixed
transmission power. Given that energy consumption in synchronization is
determined by the product of synchronization time and transformation power, the
new strategy reduces energy consumption in clock synchronization. QualNet
experiments confirm the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0830</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0830</id><created>2012-09-04</created><authors><author><keyname>Bruckdorfer</keyname><forenames>Till</forenames></author><author><keyname>Cornelsen</keyname><forenames>Sabine</forenames></author><author><keyname>Gutwenger</keyname><forenames>Carsten</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Montecchiani</keyname><forenames>Fabrizio</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Progress on Partial Edge Drawings</title><categories>cs.CG</categories><comments>16 pages, 12 figures</comments><msc-class>05C62, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a new way of avoiding crossings in straight-line drawings of
non-planar graphs has been investigated. The idea of partial edge drawings
(PED) is to drop the middle part of edges and rely on the remaining edge parts
called stubs. We focus on a symmetric model (SPED) that requires the two stubs
of an edge to be of equal length. In this way, the stub at the other endpoint
of an edge assures the viewer of the edge's existence. We also consider an
additional homogeneity constraint that forces the stub lengths to be a given
fraction $\delta$ of the edge lengths ($\delta$-SHPED). Given length and
direction of a stub, this model helps to infer the position of the opposite
stub.
  We show that, for a fixed stub--edge length ratio $\delta$, not all graphs
have a $\delta$-SHPED. Specifically, we show that $K_{241}$ does not have a
1/4-SHPED, while bandwidth-$k$ graphs always have a $\Theta(1/\sqrt{k})$-SHPED.
We also give bounds for complete bipartite graphs. Further, we consider the
problem \textsc{MaxSPED} where the task is to compute the SPED of maximum total
stub length that a given straight-line drawing contains. We present an
efficient solution for 2-planar drawings and a 2-approximation algorithm for
the dual problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0832</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0832</id><created>2012-09-04</created><authors><author><keyname>Hoy</keyname><forenames>Darrell</forenames></author><author><keyname>Jain</keyname><forenames>Kamal</forenames></author><author><keyname>Wilkens</keyname><forenames>Christopher A.</forenames></author></authors><title>Coopetitive Ad Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A single advertisement often benefits many parties, for example, an ad for a
Samsung laptop benefits Microsoft. We study this phenomenon in search
advertising auctions and show that standard solutions, including the status quo
ignorance of mutual benefit and a benefit-aware Vickrey-Clarke-Groves
mechanism, perform poorly. In contrast, we show that an appropriate first-price
auction has nice equilibria in a single-slot ad auction --- all equilibria that
satisfy a natural cooperative envy-freeness condition select the
welfare-maximizing ad and satisfy an intuitive lower-bound on revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0835</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0835</id><created>2012-09-04</created><updated>2012-09-18</updated><authors><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Xu</keyname><forenames>Wenchang</forenames></author><author><keyname>Huang</keyname><forenames>Ling</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Stefanov</keyname><forenames>Emil</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>Evolution of Social-Attribute Networks: Measurements, Modeling, and
  Implications using Google+</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>14 pages, 19 figures. will appear in IMC'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding social network structure and evolution has important
implications for many aspects of network and system design including
provisioning, bootstrapping trust and reputation systems via social networks,
and defenses against Sybil attacks. Several recent results suggest that
augmenting the social network structure with user attributes (e.g., location,
employer, communities of interest) can provide a more fine-grained
understanding of social networks. However, there have been few studies to
provide a systematic understanding of these effects at scale. We bridge this
gap using a unique dataset collected as the Google+ social network grew over
time since its release in late June 2011. We observe novel phenomena with
respect to both standard social network metrics and new attribute-related
metrics (that we define). We also observe interesting evolutionary patterns as
Google+ went from a bootstrap phase to a steady invitation-only stage before a
public release. Based on our empirical observations, we develop a new
generative model to jointly reproduce the social structure and the node
attributes. Using theoretical analysis and empirical evaluations, we show that
our model can accurately reproduce the social and attribute structure of real
social networks. We also demonstrate that our model provides more accurate
predictions for practical application contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0841</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0841</id><created>2012-09-04</created><updated>2015-01-17</updated><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Yu</keyname><forenames>Zhiding</forenames></author><author><keyname>Tang</keyname><forenames>Huajin</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Constructing the L2-Graph for Robust Subspace Learning and Subspace
  Clustering</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the framework of graph-based learning, the key to robust subspace
clustering and subspace learning is to obtain a good similarity graph that
eliminates the effects of errors and retains only connections between the data
points from the same subspace (i.e., intra-subspace data points). Recent works
achieve good performance by modeling errors into their objective functions to
remove the errors from the inputs. However, these approaches face the
limitations that the structure of errors should be known prior and a complex
convex problem must be solved. In this paper, we present a novel method to
eliminate the effects of the errors from the projection space (representation)
rather than from the input space. We first prove that $\ell_1$-, $\ell_2$-,
$\ell_{\infty}$-, and nuclear-norm based linear projection spaces share the
property of Intra-subspace Projection Dominance (IPD), i.e., the coefficients
over intra-subspace data points are larger than those over inter-subspace data
points. Based on this property, we introduce a method to construct a sparse
similarity graph, called L2-Graph. The subspace clustering and subspace
learning algorithms are developed upon L2-Graph. Experiments show that L2-Graph
algorithms outperform the state-of-the-art methods for feature extraction,
image clustering, and motion segmentation in terms of accuracy, robustness, and
time efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0846</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0846</id><created>2012-09-04</created><updated>2013-05-06</updated><authors><author><keyname>Wang</keyname><forenames>Michael.</forenames></author><author><keyname>Jiang</keyname><forenames>C.</forenames></author></authors><title>Discovery Signal Design and Its Application to Peer-to-Peer
  Communications in OFDMA Cellular Networks</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1112.1990, arXiv:1207.0557
  add reference in page 5 add text in page 5 for explaination</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper proposes a unique discovery signal as an enabler of peer-to-peer
(P2P) communication which overlays a cellular network and shares its resources.
Applying P2P communication to cellular network has two key issues: 1.
Conventional ad hoc P2P connections may be unstable since stringent resource
and interference coordination is usually difficult to achieve for ad hoc P2P
communications; 2. The large overhead required by P2P communication may offset
its gain. We solve these two issues by using a special discovery signal to aid
cellular network-supervised resource sharing and interference management
between cellular and P2P connections. The discovery signal, which facilitates
efficient neighbor discovery in a cellular system, consists of un-modulated
tones transmitted on a sequence of OFDM symbols. This discovery signal not only
possesses the properties of high power efficiency, high interference tolerance,
and freedom from near-far effects, but also has minimal overhead. A practical
discovery-signal-based P2P in an OFDMA cellular system is also proposed.
Numerical results are presented which show the potential of improving local
service and edge device performance in a cellular network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0851</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0851</id><created>2012-09-04</created><authors><author><keyname>Saboori</keyname><forenames>Ehsan</forenames></author><author><keyname>Mohammadi</keyname><forenames>Shahriar</forenames></author><author><keyname>Parsazad</keyname><forenames>Shafigh</forenames></author></authors><title>A new scheduling algorithm for server farms load balancing</title><categories>cs.DC</categories><comments>4 Pages</comments><journal-ref>2010 2nd International Conference on Industrial and Information
  Systems (IIS), vol.1, no., pp.417-420, 2010</journal-ref><doi>10.1109/INDUSIS.2010.5565821</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new scheduling algorithm to distribute jobs in server
farm systems. The proposed algorithm overcomes the starvation caused by SRPT
(Shortest Remaining Processing Time). This algorithm is used in process
scheduling in operating system approach. The algorithm was developed to be used
in dispatcher scheduling. This algorithm is non-preemptive discipline, similar
to SRPT, in which the priority of each job depends on its estimated run time,
and also the amount of time it has spent on waiting. Tasks in the servers are
served in order of priority to optimize the system response time. The
experiments show that the mean round around time is reduced in the server farm
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0852</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0852</id><created>2012-09-04</created><authors><author><keyname>Saboori</keyname><forenames>Ehsan</forenames></author><author><keyname>Parsazad</keyname><forenames>Shafigh</forenames></author><author><keyname>Sanatkhani</keyname><forenames>Yasaman</forenames></author></authors><title>Automatic firewall rules generator for anomaly detection systems with
  Apriori algorithm</title><categories>cs.AI</categories><comments>4 Pages</comments><journal-ref>2010 3rd International Conference on Advanced Computer Theory and
  Engineering (ICACTE), vol.6, no., pp.V6-57-V6-60, 2010</journal-ref><doi>10.1109/ICACTE.2010.5579365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network intrusion detection systems have become a crucial issue for computer
systems security infrastructures. Different methods and algorithms are
developed and proposed in recent years to improve intrusion detection systems.
The most important issue in current systems is that they are poor at detecting
novel anomaly attacks. These kinds of attacks refer to any action that
significantly deviates from the normal behaviour which is considered intrusion.
This paper proposed a model to improve this problem based on data mining
techniques. Apriori algorithm is used to predict novel attacks and generate
real-time rules for firewall. Apriori algorithm extracts interesting
correlation relationships among large set of data items. This paper illustrates
how to use Apriori algorithm in intrusion detection systems to cerate a
automatic firewall rules generator to detect novel anomaly attack. Apriori is
the best-known algorithm to mine association rules. This is an innovative way
to find association rules on large scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0853</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0853</id><created>2012-09-04</created><authors><author><keyname>Saboori</keyname><forenames>Ehsan</forenames></author><author><keyname>Parsazad</keyname><forenames>Shafigh</forenames></author><author><keyname>Sadeghi</keyname><forenames>Anoosheh</forenames></author></authors><title>Improving the K-means algorithm using improved downhill simplex search</title><categories>cs.LG</categories><comments>4 Pages</comments><journal-ref>2010 2nd International Conference on Software Technology and
  Engineering (ICSTE), vol.2, no., pp.V2-350-V2-354, 3-5 Oct. 2010</journal-ref><doi>10.1109/ICSTE.2010.5608792</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-means algorithm is one of the well-known and most popular clustering
algorithms. K-means seeks an optimal partition of the data by minimizing the
sum of squared error with an iterative optimization procedure, which belongs to
the category of hill climbing algorithms. As we know hill climbing searches are
famous for converging to local optimums. Since k-means can converge to a local
optimum, different initial points generally lead to different convergence
cancroids, which makes it important to start with a reasonable initial
partition in order to achieve high quality clustering solutions. However, in
theory, there exist no efficient and universal methods for determining such
initial partitions. In this paper we tried to find an optimum initial
partitioning for k-means algorithm. To achieve this goal we proposed a new
improved version of downhill simplex search, and then we used it in order to
find an optimal result for clustering approach and then compare this algorithm
with Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved Genetic
K-Means (IGKM) and k-means algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0863</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0863</id><created>2012-09-05</created><updated>2014-01-20</updated><authors><author><keyname>Lee</keyname><forenames>Chang-Hun</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Hun</forenames></author><author><keyname>Tahk</keyname><forenames>Min-Jea</forenames></author></authors><title>Agile Missile Controller Based on Adaptive Nonlinear Backstepping
  Control</title><categories>cs.SY</categories><comments>25 pages, 10 figures, 2 tables. The modified and improved version of
  this work has been accepted for publication in Transactions of the Japan
  Society for Aeronautical and Space Sciences</comments><journal-ref>Transactions of the Japan Society for Aeronautical and Space
  Sciences, Vol. 57, No. 1, 2014, pp. 9-20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a nonlinear adaptive autopilot design for agile missile
systems. In advance of the autopilot design, an investigation of the agile turn
maneuver, based on the trajectory optimization, is performed to determine state
behaviors during the agile turn phase. This investigation shows that there
exist highly nonlinear, rapidly changing dynamics and aerodynamic
uncertainties. To handle of these difficulties, we propose a longitudinal
autopilot for angle-of-attack tracking based on backstepping control
methodology in conjunction with the time-delay adaptation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0864</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0864</id><created>2012-09-05</created><updated>2013-10-17</updated><authors><author><keyname>Lee</keyname><forenames>Chang-Hun</forenames></author><author><keyname>Seo</keyname><forenames>Min-Guk</forenames></author><author><keyname>Tahk</keyname><forenames>Min-Jea</forenames></author><author><keyname>Lee</keyname><forenames>Jin-Ik</forenames></author><author><keyname>Jun</keyname><forenames>Byung-Eul</forenames></author></authors><title>Missile Acceleration Controller Design using PI and Time-Delay Adaptive
  Feedback Linearization Methodology</title><categories>cs.SY</categories><comments>A condensed version of this draft has been submitted to European
  Control Conference (ECC) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A straight forward application of feedback linearization to the missile
autopilot design for acceleration control may be limited due to the nonminimum
characteristics and the model uncertainties. As a remedy, this paper presents a
cascade structure of an acceleration controller based on approximate feedback
linearization methodology with a time-delay adaptation scheme. The inner loop
controller is constructed by applying feedback linearization to the approximate
system which is a minimum phase system and provides the desired acceleration
signal caused by the angle-of-attack. This controller is augmented by the
time-delay adaptive law and the outer loop PI (proportional-integral)
controller in order to adaptively compensate for feedback linearization error
because of model uncertainty and in order to track the desired acceleration
signal. The performance of the proposed method is examined through numerical
simulations. Moreover, the proposed controller is tested by using an intercept
scenario in 6DOF nonlinear simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0871</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0871</id><created>2012-09-05</created><authors><author><keyname>Mena</keyname><forenames>Adria Alcala</forenames></author><author><keyname>Rossello</keyname><forenames>Francesc</forenames></author></authors><title>Ternary graph isomorphism in polynomial time, after Luks</title><categories>cs.DM cs.DS q-bio.PE</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph isomorphism problem has a long history in mathematics and computer
science, with applications in computational chemistry and biology, and it is
believed to be neither solvable in polynomial time nor NP-complete. E. Luks
proposed in 1982 the best algorithm so far for the solution of this problem,
which moreover runs in polynomial time if an upper bound for the degrees of the
nodes in the graphs is taken as a constant. Unfortunately, Luks' algorithm is
purely theoretical, very difficult to use in practice, and, in particular, we
have not been able to find any implementation of it in the literature. The main
goal of this paper is to present an efficient implementation of this algorithm
for ternary graphs in the SAGE system, as well as an adaptation to fully
resolved rooted phylogenetic networks on a given set of taxa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0875</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0875</id><created>2012-09-05</created><updated>2013-03-25</updated><authors><author><keyname>Roland</keyname><forenames>Michael</forenames></author></authors><title>Applying recent secure element relay attack scenarios to the real world:
  Google Wallet Relay Attack</title><categories>cs.CR</categories><comments>Technical report, 29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report explains recent developments in relay attacks on contactless
smartcards and secure elements. It further reveals how these relay attacks can
be applied to the Google Wallet. Finally, it gives an overview of the
components and results of a successful attempt to relay an EMV Mag-Stripe
transaction between a Google Wallet device and an external card emulator over a
wireless network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0880</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0880</id><created>2012-09-05</created><authors><author><keyname>Blum</keyname><forenames>Christian</forenames></author><author><keyname>Schmid</keyname><forenames>Verena</forenames></author><author><keyname>Baumgartner</keyname><forenames>Lukas</forenames></author></authors><title>On Solving the Oriented Two-Dimensional Bin Packing Problem under Free
  Guillotine Cutting: Exploiting the Power of Probabilistic Solution
  Construction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-dimensional bin packing problems are highly relevant combinatorial
optimization problems. They find a large number of applications, for example,
in the context of transportation or warehousing, and for the cutting of
different materials such as glass, wood or metal. In this work we deal with the
oriented two-dimensional bin packing problem under free guillotine cutting. In
this specific problem a set of oriented rectangular items is given which must
be packed into a minimum number of bins of equal size. The first algorithm
proposed in this work is a randomized multi-start version of a constructive
one-pass heuristic from the literature. Additionally we propose the use of this
randomized one-pass heuristic within an evolutionary algorithm. The results of
the two proposed algorithms are compared to the best approaches from the
literature. In particular the evolutionary algorithm compares very favorably to
current state-of-the-art approaches. The optimal solution for 4 previously
unsolved instances could be found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0911</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0911</id><created>2012-09-05</created><authors><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Jin</keyname><forenames>Xiaolong</forenames></author></authors><title>Conquering the rating bound problem in neighborhood-based collaborative
  filtering: a function recovery approach</title><categories>cs.IR cs.AI cs.HC</categories><comments>10 pages, 4 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important tool for information filtering in the era of socialized web,
recommender systems have witnessed rapid development in the last decade. As
benefited from the better interpretability, neighborhood-based collaborative
filtering techniques, such as item-based collaborative filtering adopted by
Amazon, have gained a great success in many practical recommender systems.
However, the neighborhood-based collaborative filtering method suffers from the
rating bound problem, i.e., the rating on a target item that this method
estimates is bounded by the observed ratings of its all neighboring items.
Therefore, it cannot accurately estimate the unobserved rating on a target
item, if its ground truth rating is actually higher (lower) than the highest
(lowest) rating over all items in its neighborhood. In this paper, we address
this problem by formalizing rating estimation as a task of recovering a scalar
rating function. With a linearity assumption, we infer all the ratings by
optimizing the low-order norm, e.g., the $l_1/2$-norm, of the second derivative
of the target scalar function, while remaining its observed ratings unchanged.
Experimental results on three real datasets, namely Douban, Goodreads and
MovieLens, demonstrate that the proposed approach can well overcome the rating
bound problem. Particularly, it can significantly improve the accuracy of
rating estimation by 37% than the conventional neighborhood-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0913</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0913</id><created>2012-09-05</created><authors><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author></authors><title>Structuring Relevant Feature Sets with Multiple Model Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is one of the most prominent learning tasks, especially in
high-dimensional datasets in which the goal is to understand the mechanisms
that underly the learning dataset. However most of them typically deliver just
a flat set of relevant features and provide no further information on what kind
of structures, e.g. feature groupings, might underly the set of relevant
features. In this paper we propose a new learning paradigm in which our goal is
to uncover the structures that underly the set of relevant features for a given
learning problem. We uncover two types of features sets, non-replaceable
features that contain important information about the target variable and
cannot be replaced by other features, and functionally similar features sets
that can be used interchangeably in learned models, given the presence of the
non-replaceable features, with no change in the predictive performance. To do
so we propose a new learning algorithm that learns a number of disjoint models
using a model disjointness regularization constraint together with a constraint
on the predictive agreement of the disjoint models. We explore the behavior of
our approach on a number of high-dimensional datasets, and show that, as
expected by their construction, these satisfy a number of properties. Namely,
model disjointness, a high predictive agreement, and a similar predictive
performance to models learned on the full set of relevant features. The ability
to structure the set of relevant features in such a manner can become a
valuable tool in different applications of scientific knowledge discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0935</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0935</id><created>2012-09-05</created><authors><author><keyname>Saraf</keyname><forenames>Sanchit</forenames></author><author><keyname>Sourabh</keyname><forenames>Sumit</forenames></author></authors><title>Characterizing Successful Formulas: the Multi-agent Case</title><categories>cs.MA cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterization of successful formulas in Public Announcement Logic (PAL) is
a well known open problem in Dynamic Epistemic Logic. Recently, Holliday and
ICard have given a complete characterization for the single agent case.
However, the problem for the multi-agent case is open. This paper gives a
partial solution to the problem, characterizing the subclass of the language
consisting of unary operators, and discusses methods to give a complete
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0940</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0940</id><created>2012-09-05</created><authors><author><keyname>Hyvernat</keyname><forenames>Pierre</forenames><affiliation>LAMA</affiliation></author></authors><title>A Linear Category of Polynomial Diagrams</title><categories>cs.LO math.CT</categories><comments>20 pages</comments><proxy>ccsd</proxy><journal-ref>Mathematical Structures in Computer Science 24, 1 (2014) e240104</journal-ref><doi>10.1017/S0960129512001016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a categorical model for intuitionistic linear logic where objects
are polynomial diagrams and morphisms are simulation diagrams. The
multiplicative structure (tensor product and its adjoint) can be defined in any
locally cartesian closed category, whereas the additive (product and coproduct)
and exponential Tensor-comonoid comonad) structures require additional
properties and are only developed in the category Set, where the objects and
morphisms have natural interpretations in terms of games, simulation and
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0941</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0941</id><created>2012-09-05</created><updated>2013-05-14</updated><authors><author><keyname>Oger</keyname><forenames>B&#xe9;r&#xe9;nice</forenames><affiliation>ICJ</affiliation></author></authors><title>Decorated hypertrees</title><categories>math.CO cs.DM</categories><comments>nombre de pages : 36</comments><proxy>ccsd</proxy><journal-ref>Journal of Combinatorial Theory, Series A (2013) Vol. 120,
  n&amp;deg;7, 1871-1905</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  C. Jensen, J. McCammond and J. Meier have used weighted hypertrees to compute
the Euler characteristic of a subgroup of the automorphism group of a free
product. Weighted hypertrees also appear in the study of the homology of the
hypertree poset. We link them to decorated hypertrees after a general study on
decorated hypertrees, which we enumerate using box trees.---C. Jensen, J.
McCammond et J. Meier ont utilis\'e des hyperarbres pond\'er\'es pour calculer
la caract\'eristique d'Euler d'un sous-groupe du groupe des automorphismes d'un
produit libre. Un autre type d'hyperarbres pond\'er\'es appara\^it aussi dans
l'\'etude de l'homologie du poset des hyperarbres. Nous \'etudions les
hyperarbres d\'ecor\'es puis les comptons \`a l'aide de la notion d'arbre en
bo\^ite avant de les relier aux hyperarbres pond\'er\'es.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0943</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0943</id><created>2012-09-05</created><authors><author><keyname>Coudert</keyname><forenames>David</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Hogie</keyname><forenames>Luc</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Lancin</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Papadimitriou</keyname><forenames>Dimitri</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>P&#xe9;rennes</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Tahiri</keyname><forenames>Issam</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author></authors><title>Feasibility study on distributed simulations of BGP</title><categories>cs.NI</categories><comments>26th ACM/IEEE/SCS Workshop on Principles of Advanced and Distributed
  Simulation (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Autonomous System (AS) topology of the Internet (up to 61k ASs) is
growing at a rate of about 10% per year. The Border Gateway Protocol (BGP)
starts to show its limits in terms of the number of routing table entries it
can dynamically process and control. Due to the increasing routing information
processing and storage, the same trend is observed for routing model simulators
such as DRMSim specialized in large-scale simulations of routing models.
Therefore, DRMSim needs enhancements to support the current size of the
Internet topology and its evolution (up to 100k ASs). To this end, this paper
proposes a feasibility study of the extension of DRMSim so as to support the
Distributed Parallel Discrete Event paradigm. We first detail the possible
distribution models and their associated communication overhead. Then, we
analyze this overhead by executing BGP on a partitioned topology according to
different scenarios. Finally, we conclude on the feasibility of such a
simulator by computing the expected additional time required by a distributed
simulation of BGP compared to its sequential simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0948</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0948</id><created>2012-09-05</created><authors><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author></authors><title>Teaching cloud computing: a software engineering perspective</title><categories>cs.DC cs.SE</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short papers discusses the issues of teaching cloud computing from a
software engineering rather than a business perspective. It discusses what
topics might be covered in a senior course on cloud software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0960</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0960</id><created>2012-09-05</created><updated>2013-09-30</updated><authors><author><keyname>Blatt</keyname><forenames>Markus</forenames></author><author><keyname>Ippisch</keyname><forenames>Olaf</forenames></author><author><keyname>Bastian</keyname><forenames>Peter</forenames></author></authors><title>A Massively Parallel Algebraic Multigrid Preconditioner based on
  Aggregation for Elliptic Problems with Heterogeneous Coefficients</title><categories>math.NA cs.DC cs.MS cs.NA</categories><comments>22 pages, 1 figure</comments><msc-class>65F08, 65N08, 65N55, 65Y05</msc-class><acm-class>F.2.1; G.1.3; G.1.8; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a massively parallel algebraic multigrid method based on
non-smoothed aggregation. It is especially suited for solving heterogeneous
elliptic problems as it uses a greedy heuristic algorithm for the aggregation
that detects changes in the coefficients and prevents aggregation across them.
Using decoupled aggregation on each process with data agglomeration onto fewer
processes on the coarse level, it weakly scales well in terms of both total
time to solution and time per iteration to nearly 300,000 cores. Because of
simple piecewise constant interpolation between the levels, its memory
consumption is low and allows solving problems with more than 100,000,000,000
degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0967</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0967</id><created>2012-09-05</created><authors><author><keyname>Suel</keyname><forenames>Travis Z.</forenames></author></authors><title>KeyAuth: Bringing Public-key Authentication to the Masses</title><categories>cs.CR cs.HC</categories><comments>6 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passwords are a fragile, inadequate, and insecure tool for authenticating
users, and are especially fraught with problems when used to secure access to
network resources and services. In many cases, passwords provide a false sense
of security. Creating passwords which are both secure (i.e., hard for attackers
to guess) and easy for humans to remember is, at best, a paradoxical task
because these two criteria are diametrically opposed. Fortunately, a far more
secure and user-friendly alternative is available. Public-key cryptography
provides a means of both identifying and authenticating users without the need
for passwords. KeyAuth is a generic and universal implementation of public-key
authentication aimed at supplanting password-based authentication and
significantly improving the security of network accessible resources by
enhancing the usability of frequently used authentication mechanisms. KeyAuth
is an application-, language-, operating system-, and protocol-independent
public-key authentication service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0997</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0997</id><created>2012-09-05</created><authors><author><keyname>Shchekotykhin</keyname><forenames>Kostyantyn</forenames></author><author><keyname>Fleiss</keyname><forenames>Philipp</forenames></author><author><keyname>Rodler</keyname><forenames>Patrick</forenames></author><author><keyname>Friedrich</keyname><forenames>Gerhard</forenames></author></authors><title>Direct computation of diagnoses for ontology debugging</title><categories>cs.AI</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern ontology debugging methods allow efficient identification and
localization of faulty axioms defined by a user while developing an ontology.
The ontology development process in this case is characterized by rather
frequent and regular calls to a reasoner resulting in an early user awareness
of modeling errors. In such a scenario an ontology usually includes only a
small number of conflict sets, i.e. sets of axioms preserving the faults. This
property allows efficient use of standard model-based diagnosis techniques
based on the application of hitting set algorithms to a number of given
conflict sets. However, in many use cases such as ontology alignment the
ontologies might include many more conflict sets than in usual ontology
development settings, thus making precomputation of conflict sets and
consequently ontology diagnosis infeasible. In this paper we suggest a
debugging approach based on a direct computation of diagnoses that omits
calculation of conflict sets. Embedded in an ontology debugger, the proposed
algorithm is able to identify diagnoses for an ontology which includes a large
number of faults and for which application of standard diagnosis methods fails.
The evaluation results show that the approach is practicable and is able to
identify a fault in adequate time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.0999</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.0999</id><created>2012-09-05</created><authors><author><keyname>Vilanova</keyname><forenames>Anna</forenames></author><author><keyname>Preim</keyname><forenames>Bernhard</forenames></author><author><keyname>van Pelt</keyname><forenames>Roy</forenames></author><author><keyname>Gasteiger</keyname><forenames>Rocco</forenames></author><author><keyname>Neugebauer</keyname><forenames>Mathias</forenames></author><author><keyname>Wischgoll</keyname><forenames>Thomas</forenames></author></authors><title>Visual Exploration of Simulated and Measured Blood Flow</title><categories>cs.GR cs.CV</categories><comments>20 pages book chapter of Dagstuhl Seminar 09251 &quot;Scientific
  Visualization 2011&quot; book http://www.dagstuhl.de/09251</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphology of cardiovascular tissue is influenced by the unsteady behavior of
the blood flow and vice versa. Therefore, the pathogenesis of several
cardiovascular diseases is directly affected by the blood-flow dynamics.
Understanding flow behavior is of vital importance to understand the
cardiovascular system and potentially harbors a considerable value for both
diagnosis and risk assessment. The analysis of hemodynamic characteristics
involves qualitative and quantitative inspection of the blood-flow field.
Visualization plays an important role in the qualitative exploration, as well
as the definition of relevant quantitative measures and its validation. There
are two main approaches to obtain information about the blood flow: simulation
by computational fluid dynamics, and in-vivo measurements. Although research on
blood flow simulation has been performed for decades, many open problems remain
concerning accuracy and patient-specific solutions. Possibilities for real
measurement of blood flow have recently increased considerably by new
developments in magnetic resonance imaging which enable the acquisition of 3D
quantitative measurements of blood-flow velocity fields. This chapter presents
the visualization challenges for both simulation and real measurements of
unsteady blood-flow fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1003</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1003</id><created>2012-09-05</created><updated>2014-05-30</updated><authors><author><keyname>Arag&#xf3;n</keyname><forenames>Alejandro M.</forenames></author></authors><title>A C++11 implementation of arbitrary-rank tensors for high-performance
  computing</title><categories>cs.MS</categories><comments>21 pages, 6 figures, 1 table</comments><msc-class>97N80, 97N60</msc-class><acm-class>D.1.5; D.2.3; D.3.3; G.4</acm-class><journal-ref>Computer Physics Communications, 185(6): 1681 - 1696, 2014</journal-ref><doi>10.1016/j.cpc.2014.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses an efficient implementation of tensors of arbitrary
rank by using some of the idioms introduced by the recently published C++ ISO
Standard (C++11). With the aims at providing a basic building block for
high-performance computing, a single Array class template is carefully crafted,
from which vectors, matrices, and even higher-order tensors can be created. An
expression template facility is also built around the array class template to
provide convenient mathematical syntax. As a result, by using templates, an
extra high-level layer is added to the C++ language when dealing with algebraic
objects and their operations, without compromising performance. The
implementation is tested running on both CPU and GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1007</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1007</id><created>2012-09-05</created><updated>2014-04-29</updated><authors><author><keyname>Velner</keyname><forenames>Yaron</forenames></author></authors><title>Finite-Memory Strategy Synthesis for Robust Multidimensional Mean-Payoff
  Objectives</title><categories>cs.LO cs.GT</categories><comments>Accepted for CSL-LICS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-player games on graphs provide the mathematical foundation for the study
of reactive systems. In the quantitative framework, an objective assigns a
value to every play, and the goal of player 1 is to minimize the value of the
objective. In this framework, there are two relevant synthesis problems to
consider: the quantitative analysis problem is to compute the minimal (or
infimum) value that player 1 can assure, and the boolean analysis problem asks
whether player 1 can assure that the value of the objective is at most $\nu$
(for a given threshold $\nu$). Mean-payoff expression games are played on a
multidimensional weighted graph. An atomic mean-payoff expression objective is
the mean-payoff value (the long-run average weight) of a certain dimension, and
the class of mean-payoff expressions is the closure of atomic mean-payoff
expressions under the algebraic operations of $\MAX,\MIN$, numerical complement
and $\SUM$. In this work, we study for the first time the strategy synthesis
problems for games with robust quantitative objectives, namely, games with
mean-payoff expression objectives. While in general, optimal strategies for
these games require infinite-memory, in synthesis we are typically interested
in the construction of a finite-state system. Hence, we consider games in which
player 1 is restricted to finite-memory strategies, and our main contribution
is as follows. We prove that for mean-payoff expressions, the quantitative
analysis problem is computable, and the boolean analysis problem is
inter-reducible with Hilbert's tenth problem over rationals --- a fundamental
long-standing open problem in computer science and mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1011</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1011</id><created>2012-09-05</created><authors><author><keyname>Spivak</keyname><forenames>David I.</forenames></author></authors><title>Kleisli Database Instances</title><categories>cs.DB math.CT</categories><msc-class>18C20, 68P15, 68Q65</msc-class><acm-class>E.1; H.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We use monads to relax the atomicity requirement for data in a database.
Depending on the choice of monad, the database fields may contain generalized
values such as lists or sets of values, or they may contain exceptions such as
various types of nulls. The return operation for monads ensures that any
ordinary database instance will count as one of these generalized instances,
and the bind operation ensures that generalized values behave well under joins
of foreign key sequences. Different monads allow for vastly different types of
information to be stored in the database. For example, we show that classical
concepts like Markov chains, graphs, and finite state automata are each
perfectly captured by a different monad on the same schema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1013</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1013</id><created>2012-09-05</created><updated>2012-10-06</updated><authors><author><keyname>Kuniavsky</keyname><forenames>Sergey</forenames></author></authors><title>Mechanisms and Behavior Prediction</title><categories>cs.GT</categories><comments>The paper has been withdrawn due to lack of references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses the possibility of predicting human behavior in a
mechanism. Such a mechanism will have certain properties, which are defined and
discussed here. Here it is shown that, unfortunately, certain property
combinations are not possible. The impossibility result implies that either the
such mechanism will not be finite or it would never be fully known. In both
cases such a mechanism is inapplicable to fully predict human behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1032</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1032</id><created>2012-09-05</created><updated>2012-10-17</updated><authors><author><keyname>Hu</keyname><forenames>Donglin</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author></authors><title>On Scalable Video Streaming over Cognitive Radio Cellular and Ad Hoc
  Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video content delivery over wireless networks is expected to grow drastically
in the coming years. In this paper, we investigate the challenging problem of
video over cognitive radio (CR) networks. Although having high potential, this
problem brings about a new level of technical challenges. After reviewing
related work, we first address the problem of video over infrastructure-based
CR networks, and then extend the problem to video over non-infrastructure-based
ad hoc CR networks. We present formulations of cross-layer optimization
problems as well as effective algorithms to solving the problems. The proposed
algorithms are analyzed with respect to their optimality and validate with
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1033</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1033</id><created>2012-09-05</created><updated>2013-05-01</updated><authors><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Fan</keyname><forenames>Hongqi</forenames></author><author><keyname>Lu</keyname><forenames>Zaiqi</forenames></author><author><keyname>Fu</keyname><forenames>Qiang</forenames></author></authors><title>The Annealing Sparse Bayesian Learning Algorithm</title><categories>cs.IT cs.LG math.IT</categories><comments>The update equation in the annealing process was too empirical for
  practical usage. This paper need to be revised in order to be printed on the
  arxiv.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a two-level hierarchical Bayesian model and an
annealing schedule to re-enable the noise variance learning capability of the
fast marginalized Sparse Bayesian Learning Algorithms. The performance such as
NMSE and F-measure can be greatly improved due to the annealing technique. This
algorithm tends to produce the most sparse solution under moderate SNR
scenarios and can outperform most concurrent SBL algorithms while pertains
small computational load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1040</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1040</id><created>2012-09-05</created><authors><author><keyname>Mena</keyname><forenames>Adria Alcala</forenames></author></authors><title>Trivalent Graph isomorphism in polynomial time</title><categories>cs.DM cs.DS</categories><comments>48 pages. It is a Master Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It's important to design polynomial time algorithms to test if two graphs are
isomorphic at least for some special classes of graphs.
  An approach to this was presented by Eugene M. Luks(1981) in the work
\textit{Isomorphism of Graphs of Bounded Valence Can Be Tested in Polynomial
Time}. Unfortunately, it was a theoretical algorithm and was very difficult to
put into practice. On the other hand, there is no known implementation of the
algorithm, although Galil, Hoffman and Luks(1983) shows an improvement of this
algorithm running in $O(n^3 \log n)$.
  The two main goals of this master thesis are to explain more carefully the
algorithm of Luks(1981), including a detailed study of the complexity and, then
to provide an efficient implementation in SAGE system. It is divided into four
chapters plus an appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1045</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1045</id><created>2012-09-05</created><authors><author><keyname>Suarjaya</keyname><forenames>I. Made Agus Dwi</forenames></author></authors><title>A New Algorithm for Data Compression Optimization</title><categories>cs.DS</categories><comments>4 pages</comments><msc-class>68W01</msc-class><acm-class>F.2.0</acm-class><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Volume 3 Issue 8, 2012, 14-17</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People tend to store a lot of files inside theirs storage. When the storage
nears it limit, they then try to reduce those files size to minimum by using
data compression software. In this paper we propose a new algorithm for data
compression, called j-bit encoding (JBE). This algorithm will manipulates each
bit of data inside file to minimize the size without losing any data after
decoding which is classified to lossless compression. This basic algorithm is
intended to be combining with other data compression algorithms to optimize the
compression ratio. The performance of this algorithm is measured by comparing
combination of different data compression algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1048</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1048</id><created>2012-08-15</created><authors><author><keyname>Ojha</keyname><forenames>Varun Kumar</forenames></author><author><keyname>Dutta</keyname><forenames>Paramartha</forenames></author><author><keyname>Saha</keyname><forenames>Hiranmay</forenames></author></authors><title>Performance Analysis Of Neuro Genetic Algorithm Applied On Detecting
  Proportion Of Components In Manhole Gas Mixture</title><categories>cs.NE cs.CV</categories><comments>16 pages,11 figures</comments><doi>10.5121/ijaia.2012.3406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents performance analysis of a real valued neuro genetic
algorithm applied for the detection of proportion of the gases found in manhole
gas mixture. The neural network (NN) trained using genetic algorithm (GA) leads
to concept of neuro genetic algorithm, which is used for implementing an
intelligent sensory system for the detection of component gases present in
manhole gas mixture Usually a manhole gas mixture contains several toxic gases
like Hydrogen Sulfide, Ammonia, Methane, Carbon Dioxide, Nitrogen Oxide, and
Carbon Monoxide. A semiconductor based gas sensor array used for sensing
manhole gas components is an integral part of the proposed intelligent system.
It consists of many sensor elements, where each sensor element is responsible
for sensing particular gas component. Multiple sensors of different gases used
for detecting gas mixture of multiple gases, results in cross-sensitivity. The
cross-sensitivity is a major issue and the problem is viewed as pattern
recognition problem. The objective of this article is to present performance
analysis of the real valued neuro genetic algorithm which is applied for
multiple gas detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1055</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1055</id><created>2012-09-05</created><authors><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author><author><keyname>Kempe</keyname><forenames>Julia</forenames></author></authors><title>Hardness of approximation for quantum problems</title><categories>quant-ph cs.CC</categories><comments>21 pages, 1 figure, extended abstract appeared in Proceedings of the
  39th International Colloquium on Automata, Languages and Programming (ICALP),
  pages 387-398, Springer, 2012</comments><journal-ref>Quantum Information &amp; Computation 14 (5 &amp; 6): 517-540, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polynomial hierarchy plays a central role in classical complexity theory.
Here, we define a quantum generalization of the polynomial hierarchy, and
initiate its study. We show that not only are there natural complete problems
for the second level of this quantum hierarchy, but that these problems are in
fact hard to approximate. Using these techniques, we also obtain hardness of
approximation for the class QCMA. Our approach is based on the use of
dispersers, and is inspired by the classical results of Umans regarding
hardness of approximation for the second level of the classical polynomial
hierarchy [Umans, FOCS 1999]. The problems for which we prove hardness of
approximation for include, among others, a quantum version of the Succinct Set
Cover problem, and a variant of the local Hamiltonian problem with hybrid
classical-quantum ground states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1060</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1060</id><created>2012-09-05</created><updated>2014-08-03</updated><authors><author><keyname>Nguyen</keyname><forenames>Philon</forenames></author></authors><title>Combinatorial Spaces And Order Topologies</title><categories>cs.CC cs.DM</categories><comments>24 pages, 6 figures, submitted for acceptance in journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An archetypal problem discussed in computer science is the problem of
searching for a given number in a given set of numbers. Other than sequential
search, the classic solution is to sort the list of numbers and then apply
binary search. The binary search problem has a complexity of O(logN) for a list
of N numbers while the sorting problem cannot be better than O(N) on any
sequential computer following the usual assumptions. Whenever the problem of
deciding partial order can be done in O(1), a variation of the problem on some
bounded list of numbers is to apply binary search without resorting to sort.
The overall complexity of the problem is then O(log R) for some radius R. A
logarithmic upper-bound for finite encodings is shown. Also, the topology of
orderings can provide efficient algorithms for search problems in combinatorial
spaces. The main characteristic of those spaces is that they have typical
exponential space complexities. The factorial case describes an order topology
that can be illustrated using the combinatorial polytope . When a known order
topology can be combined to a given formulation of a search problem, the
resulting search problem has a polylogarithmic complexity. This logarithmic
complexity can then become useful in combinatorial search by providing a
logarithmic break-down. These algorithms can be termed as the class of search
algorithms that do not require read and are equivalent to the class of
logarithmically recursive functions. Also, the notion of order invariance is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1064</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1064</id><created>2012-09-05</created><updated>2013-08-25</updated><authors><author><keyname>Song</keyname><forenames>Zhao</forenames></author><author><keyname>Dogandzic</keyname><forenames>Aleksandar</forenames></author></authors><title>A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals
  from Compressive Samples</title><categories>stat.ML cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2277833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian expectation-maximization (EM) algorithm for
reconstructing Markov-tree sparse signals via belief propagation. The
measurements follow an underdetermined linear model where the
regression-coefficient vector is the sum of an unknown approximately sparse
signal and a zero-mean white Gaussian noise with an unknown variance. The
signal is composed of large- and small-magnitude components identified by
binary state variables whose probabilistic dependence structure is described by
a Markov tree. Gaussian priors are assigned to the signal coefficients given
their state variables and the Jeffreys' noninformative prior is assigned to the
noise variance. Our signal reconstruction scheme is based on an EM iteration
that aims at maximizing the posterior distribution of the signal and its state
variables given the noise variance. We construct the missing data for the EM
iteration so that the complete-data posterior distribution corresponds to a
hidden Markov tree (HMT) probabilistic graphical model that contains no loops
and implement its maximization (M) step via a max-product algorithm. This EM
algorithm estimates the vector of state variables as well as solves iteratively
a linear system of equations to obtain the corresponding signal estimate. We
select the noise variance so that the corresponding estimated signal and state
variables obtained upon convergence of the EM iteration have the largest
marginal posterior distribution. We compare the proposed and existing
state-of-the-art reconstruction methods via signal and image reconstruction
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1073</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1073</id><created>2012-09-05</created><authors><author><keyname>Radonjic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Vujicic</keyname><forenames>Vladimir</forenames></author></authors><title>Reply to 'Comments on Integer SEC-DED codes for low power
  communications'</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a reply to the comments on 'Integer SEC-DED codes for low power
communications'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1075</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1075</id><created>2012-09-05</created><authors><author><keyname>Mohammadian</keyname><forenames>Maisam</forenames></author></authors><title>A New Mechanism For Mutual Authentication In SIP</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The greatest threat in the new generation network which is called ngn is
unsafe authentication. Communication between new servers in ngn world is done
based on Session Initiation Protocol. SIP is an application layer control
operating on top of a transport protocol which allows creating modifying and
terminating sessions among more agents. For authentication SIP relies on HTTP
Digest by default the client is authenticated to the SIP proxy server called
one way authentication because in this approach we can authenticate client to
server and the client cant do any authentication in server side. In this paper
we propose a mutual authentication mechanism that is not based on HTTP Digest
and then we implement our method in IMS and start to do authentication client
to server is done in first step and server to client next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1076</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1076</id><created>2012-09-05</created><authors><author><keyname>Tsianos</keyname><forenames>Konstantinos I.</forenames></author><author><keyname>Lawlor</keyname><forenames>Sean</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>Communication/Computation Tradeoffs in Consensus-Based Distributed
  Optimization</title><categories>cs.DC</categories><comments>10 Pages, 3 Figures, Appearing at NIPS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the scalability of consensus-based distributed optimization
algorithms by considering two questions: How many processors should we use for
a given problem, and how often should they communicate when communication is
not free? Central to our analysis is a problem-specific value $r$ which
quantifies the communication/computation tradeoff. We show that organizing the
communication among nodes as a $k$-regular expander graph (Reingold, Vadhan,
and Wigderson, 2002) yields speedups, while when all pairs of nodes communicate
(as in a complete graph), there is an optimal number of processors that depends
on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach
a fixed level of accuracy, by communicating less and less frequently as the
computation progresses. Experiments on a real cluster solving metric learning
and non-smooth convex minimization tasks demonstrate strong agreement between
theory and practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1077</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1077</id><created>2012-09-05</created><authors><author><keyname>Canas</keyname><forenames>Guillermo D.</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>Learning Probability Measures with respect to Optimal Transport Metrics</title><categories>cs.LG stat.ML</categories><comments>13 pages, 2 figures. Advances in Neural Information Processing
  Systems, NIPS 2012</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating, in the sense of optimal transport
metrics, a measure which is assumed supported on a manifold embedded in a
Hilbert space. By establishing a precise connection between optimal transport
metrics, optimal quantization, and learning theory, we derive new probabilistic
bounds for the performance of a classic algorithm in unsupervised learning
(k-means), when used to produce a probability measure derived from the data. In
the course of the analysis, we arrive at new lower bounds, as well as
probabilistic upper bounds on the convergence rate of the empirical law of
large numbers, which, unlike existing bounds, are applicable to a wide class of
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1082</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1082</id><created>2012-09-05</created><updated>2013-05-14</updated><authors><author><keyname>Bjorklund</keyname><forenames>Andreas</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Kowalik</keyname><forenames>Lukasz</forenames></author></authors><title>Constrained Multilinear Detection and Generalized Graph Motifs</title><categories>cs.DS</categories><comments>Journal submission, 18 pages. The conference version (see
  http://drops.dagstuhl.de/opus/volltexte/2013/3919/pdf/7.pdf) was presented at
  STACS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new algebraic sieving technique to detect constrained
multilinear monomials in multivariate polynomial generating functions given by
an evaluation oracle. As applications of the technique, we show an
$O^*(2^k)$-time polynomial space algorithm for the $k$-sized Graph Motif
problem. We also introduce a new optimization variant of the problem, called
Closest Graph Motif and solve it within the same time bound. The Closest Graph
Motif problem encompasses several previously studied optimization variants,
like Maximum Graph Motif, Min-Substitute Graph Motif, and Min-Add Graph Motif.
Finally, we provide a piece of evidence that our result might be essentially
tight: the existence of an $O^*((2-\epsilon)^k)$-time algorithm for the Graph
Motif problem implies an $O((2-\epsilon')^n)$-time algorithm for Set Cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1086</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1086</id><created>2012-09-05</created><updated>2014-09-29</updated><authors><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames></author></authors><title>Robustness and Generalization for Metric Learning</title><categories>cs.LG stat.ML</categories><comments>16 pages, to appear in Neurocomputing</comments><journal-ref>Neurocomputing,151(1):259-267, 2015</journal-ref><doi>10.1016/j.neucom.2014.09.044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric learning has attracted a lot of interest over the last decade, but the
generalization ability of such methods has not been thoroughly studied. In this
paper, we introduce an adaptation of the notion of algorithmic robustness
(previously introduced by Xu and Mannor) that can be used to derive
generalization bounds for metric learning. We further show that a weak notion
of robustness is in fact a necessary and sufficient condition for a metric
learning algorithm to generalize. To illustrate the applicability of the
proposed framework, we derive generalization results for a large family of
existing metric learning algorithms, including some sparse formulations that
are not covered by previous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1114</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1114</id><created>2012-09-05</created><authors><author><keyname>Thomas</keyname><forenames>Jean</forenames></author><author><keyname>Hansson</keyname><forenames>Anders</forenames></author></authors><title>Speed Tracking of a Linear Induction Motor - Enumerative Nonlinear Model
  Predictive Control</title><categories>cs.SY math.OC</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct torque control is considered as one of the most efficient techniques
for speed and/or position tracking control of induction motor drives. However,
this control scheme has several drawbacks: the switching frequency may exceed
the maximum allowable switching frequency of the inverters, and the ripples in
current and torque, especially at low speed tracking, may be too large. In this
paper we propose a new approach that overcomes these problems. The suggested
controller is a model predictive controller which directly controls the
inverter switches. It is easy to implement in real time and it outperforms all
previous approaches. Simulation results show that the new approach has as good
tracking properties as any other scheme, and that it reduces the average
inverter switching frequency about 95% as compared to classical direct torque
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1121</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1121</id><created>2012-09-05</created><updated>2013-02-19</updated><authors><author><keyname>Canas</keyname><forenames>Guillermo D.</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>Learning Manifolds with K-Means and K-Flats</title><categories>cs.LG stat.ML</categories><comments>19 pages, 2 figures; Advances in Neural Information Processing
  Systems, NIPS 2012</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating a manifold from random samples. In
particular, we consider piecewise constant and piecewise linear estimators
induced by k-means and k-flats, and analyze their performance. We extend
previous results for k-means in two separate directions. First, we provide new
results for k-means reconstruction on manifolds and, secondly, we prove
reconstruction bounds for higher-order approximation (k-flats), for which no
known results were previously available. While the results for k-means are
novel, some of the technical tools are well-established in the literature. In
the case of k-flats, both the results and the mathematical tools are new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1122</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1122</id><created>2012-09-05</created><authors><author><keyname>Drakopoulos</keyname><forenames>Kimon</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Tsitsiklis</keyname><forenames>John</forenames></author></authors><title>On Learning with Finite Memory</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an infinite collection of agents who make decisions,
sequentially, about an unknown underlying binary state of the world. Each
agent, prior to making a decision, receives an independent private signal whose
distribution depends on the state of the world. Moreover, each agent also
observes the decisions of its last K immediate predecessors. We study
conditions under which the agent decisions converge to the correct value of the
underlying state. We focus on the case where the private signals have bounded
information content and investigate whether learning is possible, that is,
whether there exist decision rules for the different agents that result in the
convergence of their sequence of individual decisions to the correct state of
the world. We first consider learning in the almost sure sense and show that it
is impossible, for any value of K. We then explore the possibility of
convergence in probability of the decisions to the correct state. Here, a
distinction arises: if K equals 1, learning in probability is impossible under
any decision rule, while for K greater or equal to 2, we design a decision rule
that achieves it. We finally consider a new model, involving forward looking
strategic agents, each of which maximizes the discounted sum (over all agents)
of the probabilities of a correct decision. (The case, studied in previous
literature, of myopic agents who maximize the probability of their own decision
being correct is an extreme special case.) We show that for any value of K, for
any equilibrium of the associated Bayesian game, and under the assumption that
each private signal has bounded information content, learning in probability
fails to obtain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1123</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1123</id><created>2012-09-05</created><authors><author><keyname>Sabau</keyname><forenames>Serban</forenames></author><author><keyname>Martins</keyname><forenames>Nuno C.</forenames></author></authors><title>Stabilizability and Norm-Optimal Control Design subject to Sparsity
  Constraints</title><categories>cs.SY math.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider that a linear time-invariant (LTI) plant is given and that we wish
to design a stabilizing controller for it. Admissible controllers are LTI and
must comply with a pre-selected sparsity pattern. The sparsity pattern is
assumed to be quadratically invariant (QI) with respect to the plant, which,
from prior results, guarantees that there is a convex parametrization of all
admissible stabilizing controllers provided that an initial admissible stable
stabilizing controller is provided. This paper addresses the previously
unsolved problem of determining necessary and sufficient conditions for the
existence of an admissible stabilizing controller. The main idea is to cast the
existence of such a controller as the feasibility of an exact model-matching
problem with stability restrictions, which can be tackled using existing
methods. Furthermore, we show that, when it exists, the solution of the
model-matching problem can be used to compute an admissible stabilizing
controller. This method also leads to a convex parametrization that may be
viewed as an extension of Youla's classical approach so as to incorporate
sparsity constraints. Applications of this parametrization on the design of
norm-optimal controllers via convex methods are also explored. An illustrative
example is provided, and a special case is discussed for which the exact model
matching problem has a unique and easily computable solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1125</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1125</id><created>2012-09-05</created><authors><author><keyname>Slimi</keyname><forenames>Jamel</forenames></author><author><keyname>Ammar</keyname><forenames>Anis Ben</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>Video Data Visualization System: Semantic Classification And
  Personalization</title><categories>cs.IR cs.CV cs.MM</categories><comments>graphics</comments><doi>10.5121/ijcga.2012.2201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper an intelligent video data visualization tool, based
on semantic classification, for retrieving and exploring a large scale corpus
of videos. Our work is based on semantic classification resulting from semantic
analysis of video. The obtained classes will be projected in the visualization
space. The graph is represented by nodes and edges, the nodes are the keyframes
of video documents and the edges are the relation between documents and the
classes of documents. Finally, we construct the user's profile, based on the
interaction with the system, to render the system more adequate to its
references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1128</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1128</id><created>2012-09-05</created><authors><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>Capacity achieving multiwrite WOM codes</title><categories>cs.IT cs.CC math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give an explicit construction of a capacity achieving family
of binary t-write WOM codes for any number of writes t, that have a polynomial
time encoding and decoding algorithms. The block length of our construction is
N=(t/\epsilon)^{O(t/(\delta\epsilon))} when \epsilon is the gap to capacity and
encoding and decoding run in time N^{1+\delta}. This is the first deterministic
construction achieving these parameters. Our techniques also apply to larger
alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1139</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1139</id><created>2012-09-05</created><updated>2013-01-12</updated><authors><author><keyname>Cizelj</keyname><forenames>Igor</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Control of Noisy Differential-Drive Vehicles from Time-Bounded Temporal
  Logic Specifications</title><categories>cs.RO</categories><comments>Technical Report. arXiv admin note: text overlap with arXiv:1207.1280</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of controlling a noisy differential drive mobile robot
such that the probability of satisfying a specification given as a Bounded
Linear Temporal Logic (BLTL) formula over a set of properties at the regions in
the environment is maximized. We assume that the vehicle can determine its
precise initial position in a known map of the environment. However, inspired
by practical limitations, we assume that the vehicle is equipped with noisy
actuators and, during its motion in the environment, it can only measure the
angular velocity of its wheels using limited accuracy incremental encoders.
Assuming the duration of the motion is finite, we map the measurements to a
Markov Decision Process (MDP). We use recent results in Statistical Model
Checking (SMC) to obtain an MDP control policy that maximizes the probability
of satisfaction. We translate this policy to a vehicle feedback control
strategy and show that the probability that the vehicle satisfies the
specification in the environment is bounded from below by the probability of
satisfying the specification on the MDP. We illustrate our method with
simulations and experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1150</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1150</id><created>2012-09-05</created><authors><author><keyname>Yu</keyname><forenames>Changtao</forenames></author></authors><title>On dually flat Randers metrics</title><categories>math.DG cs.IT math.IT</categories><comments>10 pages</comments><msc-class>53B40, 53C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I will show how to use beta-deformations to deal with dual
flatness of Randers metrics. beta-deformations is a new method in
Riemann-Finsler geometry, it is introduced by the author(see arxiv:1209.0845).
Later on I will provide more applications of the new kind of deformations in
Finsler geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1154</identifier>
 <datestamp>2013-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1154</id><created>2012-09-05</created><updated>2013-09-04</updated><authors><author><keyname>Lee</keyname><forenames>Chang-Hun</forenames></author><author><keyname>Tahk</keyname><forenames>Min-Jea</forenames></author><author><keyname>Lee</keyname><forenames>Jin-Ik</forenames></author></authors><title>Generalized Formulation of Weighted Optimal Guidance Laws with Impact
  Angle Constraint</title><categories>cs.SY</categories><comments>This work has been accepted for publication in IEEE Transactions on
  Aerospace and Electronic Systems (in press)</comments><journal-ref>IEEE Transactions on Aerospace and Electronic Systems, Vol. 49,
  No. 2, 2013, pp. 1317-1322</journal-ref><doi>10.1109/TAES.2013.6494416</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to investigate the generalized formulation of
weighted optimal guidance laws with impact angle constraint. From the
generalized formulation, we explicitly find the feasible set of weighting
functions that lead to analytical forms of weighted optimal guidance laws. This
result has potential significance because it can provide additional degrees of
freedom in designing a guidance law that accomplishes the specified guidance
objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1176</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1176</id><created>2012-09-06</created><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Edwards</keyname><forenames>Katherine</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author></authors><title>Edge-colouring eight-regular planar graphs</title><categories>cs.DM math.CO</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was conjectured by the third author in about 1973 that every $d$-regular
planar graph (possibly with parallel edges) can be $d$-edge-coloured, provided
that for every odd set $X$ of vertices, there are at least $d$ edges between
$X$ and its complement. For $d = 3$ this is the four-colour theorem, and the
conjecture has been proved for all $d\le 7$, by various authors. Here we prove
it for $d = 8$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1180</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1180</id><created>2012-09-06</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Dall'Anese</keyname><forenames>Emiliano</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Distributed Optimal Beamformers for Cognitive Radios Robust to Channel
  Uncertainties</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures, accepted by IEEE Transactions on Signal
  Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 60, issue 12, pp.
  6495-6508, Dec. 2012</journal-ref><doi>10.1109/TSP.2012.2218240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through spatial multiplexing and diversity, multi-input multi-output (MIMO)
cognitive radio (CR) networks can markedly increase transmission rates and
reliability, while controlling the interference inflicted to peer nodes and
primary users (PUs) via beamforming. The present paper optimizes the design of
transmit- and receive-beamformers for ad hoc CR networks when CR-to-CR channels
are known, but CR-to-PU channels cannot be estimated accurately. Capitalizing
on a norm-bounded channel uncertainty model, the optimal beamforming design is
formulated to minimize the overall mean-square error (MSE) from all data
streams, while enforcing protection of the PU system when the CR-to-PU channels
are uncertain. Even though the resultant optimization problem is non-convex,
algorithms with provable convergence to stationary points are developed by
resorting to block coordinate ascent iterations, along with suitable convex
approximation techniques. Enticingly, the novel schemes also lend themselves
naturally to distributed implementations. Numerical tests are reported to
corroborate the analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1181</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1181</id><created>2012-09-06</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Roy</keyname><forenames>Anamitra Bardhan</forenames></author><author><keyname>Pal</keyname><forenames>Moumita</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author></authors><title>FCM Based Blood Vessel Segmentation Method for Retinal Images</title><categories>cs.CV</categories><comments>5 pages,3figures</comments><journal-ref>International Journal of Computer Science and Network
  (IJCSN),Volume 1, Issue 3, June 2012,ISSN 2277-5420</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation of blood vessels in retinal images provides early diagnosis of
diseases like glaucoma, diabetic retinopathy and macular degeneration. Among
these diseases occurrence of Glaucoma is most frequent and has serious ocular
consequences that can even lead to blindness, if it is not detected early. The
clinical criteria for the diagnosis of glaucoma include intraocular pressure
measurement, optic nerve head evaluation, retinal nerve fiber layer and visual
field defects. This form of blood vessel segmentation helps in early detection
for ophthalmic diseases, and potentially reduces the risk of blindness. The
low-contrast images at the retina owing to narrow blood vessels of the retina
are difficult to extract. These low contrast images are, however useful in
revealing certain systemic diseases. Motivated by the goals of improving
detection of such vessels, this present work proposes an algorithm for
segmentation of blood vessels and compares the results between expert
ophthalmologist hand-drawn ground-truths and segmented image(i.e. the output of
the present work).Sensitivity, specificity, positive predictive value (PPV),
positive likelihood ratio (PLR) and accuracy are used to evaluate overall
performance.It is found that this work segments blood vessels successfully with
sensitivity, specificity, PPV, PLR and accuracy of 99.62%, 54.66%, 95.08%,
219.72 and 95.03%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1198</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1198</id><created>2012-09-06</created><updated>2012-12-20</updated><authors><author><keyname>Chang</keyname><forenames>Yaotsu</forenames></author><author><keyname>Lee</keyname><forenames>Chong-Dao</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author></authors><title>Multivariate Interpolation Formula over Finite Fields and Its
  Applications in Coding Theory</title><categories>cs.IT math.IT</categories><comments>11 pages. This work is supported by the grant of the NSFC no.10990011
  and the Tsinghua National Lab. of Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multivariate interpolation formula (MVIF) over finite fields is presented
by using the proposed Kronecker delta function. The MVIF can be applied to
yield polynomial relations over the base field among homogeneous symmetric
rational functions. Besides the property that all the coefficients are coming
from the base field, there is also a significant one on the degrees of the
obtained polynomial; namely, the degree of each term satisfies certain
condition. Next, for any cyclic codes the unknown syndrome representation can
also be provided by the proposed MVIF and also has the same properties. By
applying the unknown syndrome representation and the Berlekamp-Massey
algorithm, one-step decoding algorithms can be developed to determine the error
locator polynomials for arbitrary cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1224</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1224</id><created>2012-09-06</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sheli Sinha</forenames></author></authors><title>Wavelet Based Normal and Abnormal Heart Sound Identification using
  Spectrogram Analysis</title><categories>cs.CV</categories><comments>7 pages, 13 figures</comments><journal-ref>International Journal of Computer Science &amp; Engineering Technology
  (IJCSET), Vol. 3 No. 6 June 2012, ISSN : 2229-3345</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work proposes a computer-aided normal and abnormal heart sound
identification based on Discrete Wavelet Transform (DWT), it being useful for
tele-diagnosis of heart diseases. Due to the presence of Cumulative Frequency
components in the spectrogram, DWT is applied on the spectro-gram up to n level
to extract the features from the individual approximation components. One
dimensional feature vector is obtained by evaluating the Row Mean of the
approximation components of these spectrograms. For this present approach, the
set of spectrograms has been considered as the database, rather than raw sound
samples. Minimum Euclidean distance is computed between feature vector of the
test sample and the feature vectors of the stored samples to identify the heart
sound. By applying this algorithm, almost 82% of accuracy was achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1236</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1236</id><created>2012-09-06</created><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Coordination of autonomic functionalities in communications networks</title><categories>cs.NI cs.SY</categories><comments>submitted to INFOCOM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future communication networks are expected to feature autonomic (or
self-organizing) mechanisms to ease deployment (self-configuration), tune
parameters automatically (self-optimization) and repair the network
(self-healing). Self-organizing mechanisms have been designed as stand-alone
entities, even though multiple mechanisms will run in parallel in operational
networks. An efficient coordination mechanism will be the major enabler for
large scale deployment of self-organizing networks. We model self-organizing
mechanisms as control loops, and study the conditions for stability when
running control loops in parallel. Based on control theory and Lyapunov
stability, we propose a coordination mechanism to stabilize the system, which
can be implemented in a distributed fashion. The mechanism remains valid in the
presence of measurement noise via stochastic approximation. Instability and
coordination in the context of wireless networks are illustrated with two
examples and the influence of network geometry is investigated. We are
essentially concerned with linear systems, and the applicability of our results
for non-linear systems is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1246</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1246</id><created>2012-09-06</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Domingo</keyname><forenames>Albert</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Spectrum Sensing with USRP-E110</title><categories>cs.NI</categories><comments>5th International Workshop on Multiple Access Communications,
  11/2012, Dublin, Ireland, (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sensing is one of the key topics towards the implementation of
future wireless services like SuperWiFi. This new wireless proposal aims at
using the freed spectrum resulting from the analog-to-digital transition of TV
channels for wireless data transmission (UHF TV White Spaces). The benefits
range from better building penetration to longer distances when compared to the
set of IEEE 802.11 standards. Nevertheless, the effective use of the available
spectrum is subject to strict regulation that prohibits unlicensed users to
interfere with incumbents (like wireless microphones). Cognitive Radios (CR)
and dynamic spectrum allocation are suggested to cope with this problem. These
techniques consist on frequency sweeps of the TV-UHF band to detect White
Spaces that could be used for SuperWiFi transmissions. In this paper we develop
and implement algorithms from GNURadio in the Ettus USRP-E110 to build a
standalone White Spaces detector that can be consulted from a centralized
location via IP networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1248</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1248</id><created>2012-09-06</created><authors><author><keyname>Kaminski</keyname><forenames>Mark</forenames></author><author><keyname>Smolka</keyname><forenames>Gert</forenames></author></authors><title>Correctness of an Incremental and Worst-Case Optimal Decision Procedure
  for Modal Logic with Eventualities</title><categories>cs.LO</categories><comments>11 Feb 2011, 18 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple theory explaining the construction and the correctness of
an incremental and worst-case optimal decision procedure for modal logic with
eventualities. The procedure gives an abstract account of important aspects of
Gor\'e and Widmann's PDL prover. Starting from an input formula, the procedure
grows a Pratt-style graph tableau until the tableau proves or disproves the
satisfiability of the formula. The procedure provides a basis for practical
provers since satisfiability and unsatisfiability of formulas can often be
determined with small tableaux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1260</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1260</id><created>2012-09-06</created><updated>2013-01-03</updated><authors><author><keyname>Ye</keyname><forenames>Fred Y.</forenames></author><author><keyname>Yu</keyname><forenames>Susan S.</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The Triple Helix of University-Industry-Government Relations at the
  Country Level, and Its Dynamic Evolution under the Pressures of Globalization</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using data from the Web of Science (WoS), we analyze the mutual information
among university, industrial, and governmental addresses (U-I-G) at the country
level for a number of countries. The dynamic evolution of the Triple Helix can
thus be compared among developed and developing nations in terms of
cross-sectorial co-authorship relations. The results show that the Triple-Helix
interactions among the three subsystems U-I-G become less intensive over time,
but unequally for different countries. We suggest that globalization erodes
local Triple-Helix relations and thus can be expected to increase
differentiation in national systems since the mid-1990s. This effect of
globalization is more pronounced in developed countries than in developing
ones. In the dynamic analysis, we focus on a more detailed comparison between
China and the USA. The Chinese Academy of the (Social) Sciences changes
increasingly from a public research institute to an academic one, and this has
a measurable effect on China's position in the globalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1291</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1291</id><created>2012-09-06</created><authors><author><keyname>Vaze</keyname><forenames>Chinmay S.</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The degrees of freedom of MIMO networks with full-duplex receiver
  cooperation but no CSIT</title><categories>cs.IT math.IT</categories><comments>This work was presented at the Workshop on Interference in Wireless
  Networks, Boston University, June 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of whether the degrees of freedom (DoF) of multi-user networks
can be enhanced even under isotropic fading and no channel state information
(or output feedback) at the transmitters (CSIT) is investigated. Toward this
end, the two-user MIMO (multiple-input, multiple-output) broadcast and
interference channels are studied with no side-information whatsoever at the
transmitters and with receivers equipped with full-duplex radios. The
full-duplex feature allows for receiver cooperation because each receiver, in
addition to receiving the signals sent by the transmitters, can also
simultaneously transmit a signal in the same band to the other receiver. Unlike
the case of MIMO networks with CSIT and full-duplex receivers, for which DoF
are known, it is shown that for MIMO networks with no CSIT, full-duplex
receiver cooperation is beneficial to such an extent that even the DoF region
is enhanced. Indeed, for important classes of two-user MIMO broadcast and
interference channels, defined by certain relationships on numbers of antennas
at different terminals, the exact DoF regions are established. The key to
achieving DoF-optimal performance for such networks are new retro-cooperative
interference alignment schemes. Their optimality is established via the DoF
analysis of certain genie-aided or enhanced version of those networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1295</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1295</id><created>2012-09-06</created><authors><author><keyname>Zhou</keyname><forenames>Bo</forenames></author><author><keyname>Song</keyname><forenames>Qiankun</forenames></author></authors><title>Period Distribution of Inversive Pseudorandom Number Generators Over
  Finite Fields</title><categories>cs.IT math.IT</categories><comments>13 pages, 2 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:1208.2488</comments><msc-class>11B50, 11K45, 12E20, 94A55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on analyzing the period distribution of the inversive
pseudorandom number generators (IPRNGs) over finite field $({\rm
Z}_{N},+,\times)$, where $N&gt;3$ is a prime. The sequences generated by the
IPRNGs are transformed to 2-dimensional linear feedback shift register (LFSR)
sequences. By employing the generating function method and the finite field
theory, the period distribution is obtained analytically. The analysis process
also indicates how to choose the parameters and the initial values such that
the IPRNGs fit specific periods. The analysis results show that there are many
small periods if $N$ is not chosen properly. The experimental examples show the
effectiveness of the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1300</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1300</id><created>2012-08-18</created><authors><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Input Scheme for Hindi Using Phonetic Mapping</title><categories>cs.CL</categories><comments>Proceedings of National Conference on ICT: Theory, Practice and
  Applications. SPSU Press. Organized by Sir Padampat Singhania University,
  Udaipur. Sponsored by CSIR, New Delhi. March, 2010</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Written Communication on Computers requires knowledge of writing text for the
desired language using Computer. Mostly people do not use any other language
besides English. This creates a barrier. To resolve this issue we have
developed a scheme to input text in Hindi using phonetic mapping scheme. Using
this scheme we generate intermediate code strings and match them with
pronunciations of input text. Our system show significant success over other
input systems available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1301</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1301</id><created>2012-08-18</created><authors><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Evaluation of Computational Grammar Formalisms for Indian Languages</title><categories>cs.CL</categories><comments>Proc. of International Conference in Computer Engineering and
  Technology, 2012, Organized by Jodhpur Institute of Engineering and
  Technology, Jodhpur. Sponsored by IEEE, USA and Institution of Engineers
  (India), Kolkatta</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Natural Language Parsing has been the most prominent research area since the
genesis of Natural Language Processing. Probabilistic Parsers are being
developed to make the process of parser development much easier, accurate and
fast. In Indian context, identification of which Computational Grammar
Formalism is to be used is still a question which needs to be answered. In this
paper we focus on this problem and try to analyze different formalisms for
Indian languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1317</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1317</id><created>2012-09-06</created><updated>2014-02-03</updated><authors><author><keyname>Kostina</keyname><forenames>Victoria</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Lossy joint source-channel coding in the finite blocklength regime</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, vol. 59, no. 5, pp.
  2545-2575, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper finds new tight finite-blocklength bounds for the best achievable
lossy joint source-channel code rate, and demonstrates that joint
source-channel code design brings considerable performance advantage over a
separate one in the non-asymptotic regime. A joint source-channel code maps a
block of $k$ source symbols onto a length$-n$ channel codeword, and the
fidelity of reproduction at the receiver end is measured by the probability
$\epsilon$ that the distortion exceeds a given threshold $d$. For memoryless
sources and channels, it is demonstrated that the parameters of the best joint
source-channel code must satisfy $nC - kR(d) \approx \sqrt{nV + k \mathcal
V(d)} Q(\epsilon)$, where $C$ and $V$ are the channel capacity and channel
dispersion, respectively; $R(d)$ and $\mathcal V(d)$ are the source
rate-distortion and rate-dispersion functions; and $Q$ is the standard Gaussian
complementary cdf. Symbol-by-symbol (uncoded) transmission is known to achieve
the Shannon limit when the source and channel satisfy a certain probabilistic
matching condition. In this paper we show that even when this condition is not
satisfied, symbol-by-symbol transmission is, in some cases, the best known
strategy in the non-asymptotic regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1318</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1318</id><created>2012-09-06</created><authors><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author></authors><title>Finding and Recommending Scholarly Articles</title><categories>cs.IR astro-ph.IM cs.DL physics.soc-ph</categories><comments>14 pages, part of the forthcoming MIT book &quot;Bibliometrics and Beyond:
  Metrics-Based Evaluation of Scholarly Research&quot; edited by Blaise Cronin and
  Cassidy R. Sugimoto</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The rate at which scholarly literature is being produced has been increasing
at approximately 3.5 percent per year for decades. This means that during a
typical 40 year career the amount of new literature produced each year
increases by a factor of four. The methods scholars use to discover relevant
literature must change. Just like everybody else involved in information
discovery, scholars are confronted with information overload. Two decades ago,
this discovery process essentially consisted of paging through abstract books,
talking to colleagues and librarians, and browsing journals. A time-consuming
process, which could even be longer if material had to be shipped from
elsewhere. Now much of this discovery process is mediated by online scholarly
information systems. All these systems are relatively new, and all are still
changing. They all share a common goal: to provide their users with access to
the literature relevant to their specific needs. To achieve this each system
responds to actions by the user by displaying articles which the system judges
relevant to the user's current needs. Recently search systems which use
particularly sophisticated methodologies to recommend a few specific papers to
the user have been called &quot;recommender systems&quot;. These methods are in line with
the current use of the term &quot;recommender system&quot; in computer science. We do not
adopt this definition, rather we view systems like these as components in a
larger whole, which is presented by the scholarly information systems
themselves. In what follows we view the recommender system as an aspect of the
entire information system; one which combines the massive memory capacities of
the machine with the cognitive abilities of the human user to achieve a
human-machine synergy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1322</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1322</id><created>2012-09-06</created><authors><author><keyname>Qardaji</keyname><forenames>Wahbeh</forenames></author><author><keyname>Yang</keyname><forenames>Weining</forenames></author><author><keyname>Li</keyname><forenames>Ninghui</forenames></author></authors><title>Differentially Private Grids for Geospatial Data</title><categories>cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of constructing a differentially private
synopsis for two-dimensional datasets such as geospatial datasets. The current
state-of-the-art methods work by performing recursive binary partitioning of
the data domains, and constructing a hierarchy of partitions. We show that the
key challenge in partition-based synopsis methods lies in choosing the right
partition granularity to balance the noise error and the non-uniformity error.
We study the uniform-grid approach, which applies an equi-width grid of a
certain size over the data domain and then issues independent count queries on
the grid cells. This method has received no attention in the literature,
probably due to the fact that no good method for choosing a grid size was
known. Based on an analysis of the two kinds of errors, we propose a method for
choosing the grid size. Experimental results validate our method, and show that
this approach performs as well as, and often times better than, the
state-of-the-art methods. We further introduce a novel adaptive-grid method.
The adaptive grid method lays a coarse-grained grid over the dataset, and then
further partitions each cell according to its noisy count. Both levels of
partitions are then used in answering queries over the dataset. This method
exploits the need to have finer granularity partitioning over dense regions
and, at the same time, coarse partitioning over sparse regions. Through
extensive experiments on real-world datasets, we show that this approach
consistently and significantly outperforms the uniform-grid method and other
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1323</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1323</id><created>2012-09-06</created><authors><author><keyname>Yu</keyname><forenames>Sheng</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>An Empirical Study of How Users Adopt Famous Entities</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users of social networking services construct their personal social networks
by creating asymmetric and symmetric social links. Users usually follow friends
and selected famous entities that include celebrities and news agencies. In
this paper, we investigate how users follow famous entities. We statically and
dynamically analyze data within a huge social networking service with a
manually classified set of famous entities. The results show that the in-degree
of famous entities does not fit to power-law distribution. Conversely, the
maximum number of famous followees in one category for each user shows
power-law property. To our best knowledge, there is no research work on this
topic with human-chosen famous entity dataset in real life. These findings
might be helpful in microblogging marketing and user classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1327</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1327</id><created>2012-09-06</created><authors><author><keyname>Kruchten</keyname><forenames>Philippe</forenames></author></authors><title>The frog and the octopus: a conceptual model of software development</title><categories>cs.SE</categories><acm-class>K.6.3; D.2.9</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a conceptual model of software development that encompasses all
approaches: traditional or agile, light and heavy, for large and small
development efforts. The model identifies both the common aspects in all
software development, i.e., elements found in some form or another in each and
every software development project (Intent, Product, People, Work, Time,
Quality, Risk, Cost, Value), as well as the variable part, i.e., the main
factors that cause the very wide variations we can find in the software
development world (Size, Age, Criticality, Architecture stability, Business
model, Governance, Rate of change, Geographic distribution). We show how the
model can be used as an explanatory theory of software development, as a tool
for analysis of practices, techniques, processes, as the basis for curriculum
design or for software process adoption and improvement, and to support
empirical research on software development methods. This model is also proposed
as a way to depolarize the debate on agile methods versus the
rest-of-the-world: a unified model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1351</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1351</id><created>2012-09-06</created><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Meloni</keyname><forenames>Sandro</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Emergence of influential spreaders in modified rumor models</title><categories>physics.soc-ph cs.SI</categories><comments>14 Pages, 6 figures, accepted for publication in Journal of
  Statistical Physics</comments><journal-ref>J. Stat Phys 151, 383 (2013)</journal-ref><doi>10.1007/s10955-012-0595-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The burst in the use of online social networks over the last decade has
provided evidence that current rumor spreading models miss some fundamental
ingredients in order to reproduce how information is disseminated. In
particular, recent literature has revealed that these models fail to reproduce
the fact that some nodes in a network have an influential role when it comes to
spread a piece of information. In this work, we introduce two mechanisms with
the aim of filling the gap between theoretical and experimental results. The
first model introduces the assumption that spreaders are not always active
whereas the second model considers the possibility that an ignorant is not
interested in spreading the rumor. In both cases, results from numerical
simulations show a higher adhesion to real data than classical rumor spreading
models. Our results shed some light on the mechanisms underlying the spreading
of information and ideas in large social systems and pave the way for more
realistic diffusion models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1358</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1358</id><created>2012-09-05</created><authors><author><keyname>Maurer</keyname><forenames>Alexandre</forenames><affiliation>LIP6, LINCS</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author></authors><title>On Byzantine Broadcast in Loosely Connected Networks</title><categories>cs.DC cs.CR</categories><comments>14</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-33651-5_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reliably broadcasting information in a multihop
asynchronous network that is subject to Byzantine failures. Most existing
approaches give conditions for perfect reliable broadcast (all correct nodes
deliver the authentic message and nothing else), but they require a highly
connected network. An approach giving only probabilistic guarantees (correct
nodes deliver the authentic message with high probability) was recently
proposed for loosely connected networks, such as grids and tori. Yet, the
proposed solution requires a specific initialization (that includes global
knowledge) of each node, which may be difficult or impossible to guarantee in
self-organizing networks - for instance, a wireless sensor network, especially
if they are prone to Byzantine failures. In this paper, we propose a new
protocol offering guarantees for loosely connected networks that does not
require such global knowledge dependent initialization. In more details, we
give a methodology to determine whether a set of nodes will always deliver the
authentic message, in any execution. Then, we give conditions for perfect
reliable broadcast in a torus network. Finally, we provide experimental
evaluation for our solution, and determine the number of randomly distributed
Byzantine failures than can be tolerated, for a given correct broadcast
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1359</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1359</id><created>2012-09-06</created><authors><author><keyname>Varma</keyname><forenames>Vineeth S</forenames></author><author><keyname>Elayoubi</keyname><forenames>Salah E</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>A Flow Level Perspective on Base Station Power Allocation in Green
  Networks</title><categories>cs.NI</categories><comments>Accepted for Valuetools 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel power allocation mechanism which allows one
to optimize the energy-efficiency of base stations operating in the downlink.
The energy-efficiency refers to the amount of bits that can be transmitted by
the base station per unit of energy consumed. This work studies the impact of
flow-level dynamics on the energy efficiency of base stations, by considering
user arrivals and departures. Our proposed power allocation scheme optimizes
the energyefficiency, accounting for the dynamic nature of users (referred to
as the global energy-efficiency). We emphasize our numerical results that study
the influence of the radio conditions, transmit power and the user traffic on
the energy-efficiency in an LTE compliant framework. Finally, we show that the
power allocation scheme that considers traffic dynamics, is significantly
different from the power allocation scheme when the number of users is
considered as constant, and that it has a better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1360</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1360</id><created>2012-09-06</created><updated>2012-09-14</updated><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>Multiclass Learning with Simplex Coding</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss a novel framework for multiclass learning, defined
by a suitable coding/decoding strategy, namely the simplex coding, that allows
to generalize to multiple classes a relaxation approach commonly used in binary
classification. In this framework, a relaxation error analysis can be developed
avoiding constraints on the considered hypotheses class. Moreover, we show that
in this setting it is possible to derive the first provably consistent
regularized method with training/tuning complexity which is independent to the
number of classes. Tools from convex analysis are introduced that can be used
beyond the scope of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1361</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1361</id><created>2012-09-06</created><authors><author><keyname>Varma</keyname><forenames>Vineeth S</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Elayoubi</keyname><forenames>Salah E</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Cross-Layer Design for Green Power Control</title><categories>cs.NI</categories><comments>Presented in ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new energy efficiency metric which allows one to
optimize the performance of a wireless system through a novel power control
mechanism. The proposed metric possesses two important features. First, it
considers the whole power of the terminal and not just the radiated power.
Second, it can account for the limited buffer memory of transmitters which
store arriving packets as a queue and transmit them with a success rate that is
determined by the transmit power and channel conditions. Remarkably, this
metric is shown to have attractive properties such as quasi-concavity with
respect to the transmit power and a unique maximum, allowing to derive an
optimal power control scheme. Based on analytical and numerical results, the
influence of the packet arrival rate, the size of the queue, and the
constraints in terms of quality of service are studied. Simulations show that
the proposed cross-layer approach of power control may lead to significant
gains in terms of transmit power compared to a physical layer approach of green
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1362</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1362</id><created>2012-09-06</created><authors><author><keyname>Gagarin</keyname><forenames>Andrei</forenames></author><author><keyname>Zverovich</keyname><forenames>Vadim</forenames></author></authors><title>The bondage number of graphs on topological surfaces and Teschner's
  conjecture</title><categories>math.CO cs.DM</categories><comments>21 pages; Original version from January 2012</comments><msc-class>05C69, 05C10, 57M15, 90B10, 90B25</msc-class><journal-ref>Discrete Math. 313 (2013), no. 6, pp. 796-808</journal-ref><doi>10.1016/j.disc.2012.12.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bondage number of a graph is the smallest number of its edges whose
removal results in a graph having a larger domination number. We provide
constant upper bounds for the bondage number of graphs on topological surfaces,
improve upper bounds for the bondage number in terms of the maximum vertex
degree and the orientable and non-orientable genera of the graph, and show
tight lower bounds for the number of vertices of graphs 2-cell embeddable on
topological surfaces of a given genus. Also, we provide stronger upper bounds
for graphs with no triangles and graphs with the number of vertices larger than
a certain threshold in terms of the graph genera. This settles Teschner's
Conjecture in positive for almost all graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1380</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1380</id><created>2012-09-06</created><updated>2013-05-01</updated><authors><author><keyname>Malloy</keyname><forenames>Matthew L.</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author></authors><title>The Sample Complexity of Search over Multiple Populations</title><categories>cs.IT math.IT stat.ML</categories><comments>To appear, IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the sample complexity of searching over multiple
populations. We consider a large number of populations, each corresponding to
either distribution P0 or P1. The goal of the search problem studied here is to
find one population corresponding to distribution P1 with as few samples as
possible. The main contribution is to quantify the number of samples needed to
correctly find one such population. We consider two general approaches:
non-adaptive sampling methods, which sample each population a predetermined
number of times until a population following P1 is found, and adaptive sampling
methods, which employ sequential sampling schemes for each population. We first
derive a lower bound on the number of samples required by any sampling scheme.
We then consider an adaptive procedure consisting of a series of sequential
probability ratio tests, and show it comes within a constant factor of the
lower bound. We give explicit expressions for this constant when samples of the
populations follow Gaussian and Bernoulli distributions. An alternative
adaptive scheme is discussed which does not require full knowledge of P1, and
comes within a constant factor of the optimal scheme. For comparison, a lower
bound on the sampling requirements of any non-adaptive scheme is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1399</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1399</id><created>2012-09-06</created><authors><author><keyname>MacCormick</keyname><forenames>John</forenames></author></authors><title>Video Chat with Multiple Cameras</title><categories>cs.MM cs.HC</categories><comments>49 pages, 14 figures</comments><acm-class>H.5.1; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dominant paradigm for video chat employs a single camera at each end of
the conversation, but some conversations can be greatly enhanced by using
multiple cameras at one or both ends. This paper provides the first rigorous
investigation of multi-camera video chat, concentrating especially on the
ability of users to switch between views at either end of the conversation. A
user study of 23 individuals analyzes the advantages and disadvantages of
permitting a user to switch between views at a remote location. Benchmark
experiments employing up to four webcams simultaneously demonstrate that
multi-camera video chat is feasible on consumer hardware. The paper also
presents the design of MultiCam, a software package permitting multi-camera
video chat. Some important trade-offs in the design of MultiCam are discussed,
and typical usage scenarios are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1402</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1402</id><created>2012-09-06</created><updated>2013-01-28</updated><authors><author><keyname>Adhikary</keyname><forenames>Ansuman</forenames></author><author><keyname>Nam</keyname><forenames>Junyoung</forenames></author><author><keyname>Ahn</keyname><forenames>Jae-Young</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Joint Spatial Division and Multiplexing</title><categories>cs.IT math.IT</categories><comments>10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Joint Spatial Division and Multiplexing (JSDM), an approach to
multiuser MIMO downlink that exploits the structure of the correlation of the
channel vectors in order to allow for a large number of antennas at the base
station while requiring reduced-dimensional Channel State Information at the
Transmitter (CSIT). This allows for significant savings both in the downlink
training and in the CSIT feedback from the user terminals to the base station,
thus making the use of a large number of base station antennas potentially
suitable also for Frequency Division Duplexing (FDD) systems, for which
uplink/downlink channel reciprocity cannot be exploited. JSDM forms the
multiuser MIMO downlink precoder by concatenating a pre-beamforming matrix,
which depends only on the channel second-order statistics, with a classical
multiuser precoder, based on the instantaneous knowledge of the resulting
reduced dimensional effective channels. We prove a simple condition under which
JSDM incurs no loss of optimality with respect to the full CSIT case. For
linear uniformly spaced arrays, we show that such condition is closely
approached when the number of antennas is large. For this case, we use Szego
asymptotic theory of large Toeplitz matrices to design a DFT-based
pre-beamforming scheme requiring only coarse information about the users angles
of arrival and angular spread. Finally, we extend these ideas to the case of a
two-dimensional base station antenna array, with 3-dimensional beamforming,
including multiple beams in the elevation angle direction. We provide
guidelines for the pre-beamforming optimization and calculate the system
spectral efficiency under proportional fairness and maxmin fairness criteria,
showing extremely attractive performance. Our numerical results are obtained
via an asymptotic random matrix theory tool known as deterministic equivalent
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1406</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1406</id><created>2012-09-06</created><updated>2013-06-25</updated><authors><author><keyname>Conrad</keyname><forenames>Patrick R.</forenames></author><author><keyname>Marzouk</keyname><forenames>Youssef M.</forenames></author></authors><title>Adaptive Smolyak Pseudospectral Approximations</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial approximations of computationally intensive models are central to
uncertainty quantification. This paper describes an adaptive method for
non-intrusive pseudospectral approximation, based on Smolyak's algorithm with
generalized sparse grids. We rigorously analyze and extend the non-adaptive
method proposed in [6], and compare it to a common alternative approach for
using sparse grids to construct polynomial approximations, direct quadrature.
Analysis of direct quadrature shows that O(1) errors are an intrinsic property
of some configurations of the method, as a consequence of internal aliasing. We
provide precise conditions, based on the chosen polynomial basis and quadrature
rules, under which this aliasing error occurs. We then establish theoretical
results on the accuracy of Smolyak pseudospectral approximation, and show that
the Smolyak approximation avoids internal aliasing and makes far more effective
use of sparse function evaluations. These results are applicable to broad
choices of quadrature rule and generalized sparse grids. Exploiting this
flexibility, we introduce a greedy heuristic for adaptive refinement of the
pseudospectral approximation. We numerically demonstrate convergence of the
algorithm on the Genz test functions, and illustrate the accuracy and
efficiency of the adaptive approach on a realistic chemical kinetics problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1411</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1411</id><created>2012-09-06</created><updated>2013-04-08</updated><authors><author><keyname>Song</keyname><forenames>Chaoming</forenames></author><author><keyname>Wang</keyname><forenames>Dashun</forenames></author><author><keyname>Barabasi</keyname><forenames>Albert-Laszlo</forenames></author></authors><title>Connections between Human Dynamics and Network Science</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of large-scale data on human behavior has
catalyzed simultaneous advances in network theory, capturing the scaling
properties of the interactions between a large number of individuals, and human
dynamics, quantifying the temporal characteristics of human activity patterns.
These two areas remain disjoint, each pursuing as separate lines of inquiry.
Here we report a series of generic relationships between the quantities
characterizing these two areas by demonstrating that the degree and link weight
distributions in social networks can be expressed in terms of the dynamical
exponents characterizing human activity patterns. We test the validity of these
theoretical predictions on datasets capturing various facets of human
interactions, from mobile calls to tweets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1421</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1421</id><created>2012-09-06</created><authors><author><keyname>Jacquet</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Linden</keyname><forenames>Isabelle</forenames></author><author><keyname>Staicu</keyname><forenames>Mihail-Octavian</forenames></author></authors><title>Blackboard Rules for Coordinating Context-aware Applications in Mobile
  Ad Hoc Networks</title><categories>cs.MA</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 91, 2012, pp. 63-78</journal-ref><doi>10.4204/EPTCS.91.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to improvements in wireless communication technologies and increasing
computing power in hand-held devices, mobile ad hoc networks are becoming an
ever-more present reality. Coordination languages are expected to become
important means in supporting this type of interaction. To this extent we argue
the interest of the Bach coordination language as a middleware that can handle
and react to context changes as well as cope with unpredictable physical
interruptions that occur in opportunistic network connections. More concretely,
our proposal is based on blackboard rules that model declaratively the actions
to be taken once the blackboard content reaches a predefined state, but also
that manage the engagement and disengagement of hosts and transient sharing of
blackboards. The idea of reactiveness has already been introduced in previous
work, but as will be appreciated by the reader, this article presents a new
perspective, more focused on a declarative setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1422</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1422</id><created>2012-09-06</created><authors><author><keyname>Jongmans</keyname><forenames>Sung-Shik T. Q.</forenames></author><author><keyname>Clarke</keyname><forenames>Dave</forenames></author><author><keyname>Proen&#xe7;a</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>A Procedure for Splitting Processes and its Application to Coordination</title><categories>cs.PL</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 91, 2012, pp. 79-96</journal-ref><doi>10.4204/EPTCS.91.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a procedure for splitting processes in a process algebra with
multi-actions (a subset of the specification language mCRL2). This splitting
procedure cuts a process into two processes along a set of actions A: roughly,
one of these processes contains no actions from A, while the other process
contains only actions from A. We state and prove a theorem asserting that the
parallel composition of these two processes equals the original process under
appropriate synchronization.
  We apply our splitting procedure to the process algebraic semantics of the
coordination language Reo: using this procedure and its related theorem, we
formally establish the soundness of splitting Reo connectors along the
boundaries of their (a)synchronous regions in implementations of Reo. Such
splitting can significantly improve the performance of connectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1423</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1423</id><created>2012-09-06</created><updated>2012-12-17</updated><authors><author><keyname>Gonz&#xe1;lez-Avella</keyname><forenames>J. C.</forenames></author><author><keyname>Cosenza</keyname><forenames>M. G.</forenames></author><author><keyname>Miguel</keyname><forenames>M. San</forenames></author></authors><title>A model for cross-cultural reciprocal interactions through mass media</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.CD</categories><comments>8 pages and 7 figures</comments><journal-ref>PLOS One 7(12):e51035 2012</journal-ref><doi>10.1371/journal.pone.0051035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of cross-cultural interactions through mass media
in a model where two populations of social agents, each with its own internal
dynamics, get information about each other through reciprocal global
interactions. As the agent dynamics, we employ Axelrod's model for social
influence. The global interaction fields correspond to the statistical mode of
the states of the agents and represent mass media messages on the cultural
trend originating in each population. Several phases are found in the
collective behavior of either population depending on parameter values: two
homogeneous phases, one having the state of the global field acting on that
population, and the other consisting of a state different from that reached by
the applied global field; and a disordered phase. In addition, the system
displays nontrivial effects: (i) the emergence of a largest minority group of
appreciable size sharing a state different from that of the applied global
field; (ii) the appearance of localized ordered states for some values of
parameters when the entire system is observed, consisting of one population in
a homogeneous state and the other in a disordered state. This last situation
can be considered as a social analogue to a chimera state arising in globally
coupled populations of oscillators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1424</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1424</id><created>2012-09-06</created><updated>2015-09-20</updated><authors><author><keyname>Nekouei</keyname><forenames>Ehsan</forenames></author><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Multiuser Diversity for the Cognitive Uplink with Generalized Fading and
  Reduced Primary's Cooperation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive multiple access networks, feedback is an important mechanism to
convey secondary transmitter primary base station (STPB) channel gains from the
primary base station (PBS) to the secondary base station (SBS). This paper
investigates the optimal sum-rate capacity scaling laws for cognitive multiple
access networks in feedback limited communication scenarios. First, an
efficient feedback protocol called $K$-smallest channel gains ($K$-SCGs)
feedback protocol is proposed in which the PBS feeds back the $\K$ smallest out
of $N$ STPB channel gains to the SBS. Second, the sum-rate performance of the
$K$-SCG feedback protocol is studied for three network types when transmission
powers of secondary users (SUs) are optimally allocated. The network types
considered are total-power-and-interference-limited (TPIL),
interference-limited (IL) and individual-power-and-interference-limited (IPIL)
networks. For each network type studied, we provide a sufficient condition on
$\K$ such that the $K$-SCG feedback protocol is {\em asymptotically} optimal in
the sense that the secondary network sum-rate scaling behavior under the
$K$-SCG feedback protocol is the same with that under the full-feedback
protocol. We allow distributions of
secondary-transmitter-secondary-base-station (STSB), and STPB channel power
gains to belong to a fairly general class of distributions called class
$\mathcal{C}$-distributions that includes commonly used fading models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1425</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1425</id><created>2012-09-06</created><authors><author><keyname>Xin</keyname><forenames>Reynold S.</forenames></author></authors><title>The End of an Architectural Era for Analytical Databases</title><categories>cs.DB cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Traditional enterprise warehouse solutions center around an analytical
database system that is monolithic and inflexible: data needs to be extracted,
transformed, and loaded into the rigid relational form before analysis. It
takes years of sophisticated planning to provision and deploy a warehouse;
adding new hardware resources to an existing warehouse is an equally lengthy
and daunting task.
  Additionally, modern data analysis employs statistical methods that go well
beyond the typical roll-up and drill-down capabilities provided by warehouse
systems. Although it is possible to implement such methods using a combination
of SQL and UDFs, query engines in relational databases are ill-suited for
these.
  The Hadoop ecosystem introduces a suite of tools for data analytics that
overcome some of the problems of traditional solutions. These systems, however,
forgo years of warehouse research. Memory is significantly underutilized in
Hadoop clusters, and execution engine is naive compared with its relational
counterparts.
  It is time to rethink the design of data warehouse systems and take the best
from both worlds. The new generation of warehouse systems should be modular,
high performance, fault-tolerant, easy to provision, and designed to support
both SQL query processing and machine learning applications.
  This paper references the Shark system developed at Berkeley as an initial
attempt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1426</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1426</id><created>2012-09-06</created><updated>2013-04-06</updated><authors><author><keyname>Nekouei</keyname><forenames>Ehsan</forenames></author><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Power Control and Multiuser Diversity for the Distributed Cognitive
  Uplink</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies optimum power control and sum-rate scaling laws for the
distributed cognitive uplink. It is first shown that the optimum distributed
power control policy is in the form of a threshold based water-filling power
control. Each secondary user executes the derived power control policy in a
distributed fashion by using local knowledge of its direct and interference
channel gains such that the resulting aggregate (average) interference does not
disrupt primary's communication. Then, the tight sum-rate scaling laws are
derived as a function of the number of secondary users $N$ under the optimum
distributed power control policy. The fading models considered to derive
sum-rate scaling laws are general enough to include Rayleigh, Rician and
Nakagami fading models as special cases. When transmissions of secondary users
are limited by both transmission and interference power constraints, it is
shown that the secondary network sum-rate scales according to
$\frac{1}{\e{}n_h}\log\logp{N}$, where $n_h$ is a parameter obtained from the
distribution of direct channel power gains. For the case of transmissions
limited only by interference constraints, on the other hand, the secondary
network sum-rate scales according to $\frac{1}{\e{}\gamma_g}\logp{N}$, where
$\gamma_g$ is a parameter obtained from the distribution of interference
channel power gains. These results indicate that the distributed cognitive
uplink is able to achieve throughput scaling behavior similar to that of the
centralized cognitive uplink up to a pre-log multiplier $\frac{1}{\e{}}$,
whilst primary's quality-of-service requirements are met. The factor
$\frac{1}{\e{}}$ can be interpreted as the cost of distributed implementation
of the cognitive uplink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1428</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1428</id><created>2012-09-06</created><authors><author><keyname>Winikoff</keyname><forenames>Michael</forenames></author></authors><title>Challenges and Directions for Engineering Multi-agent Systems</title><categories>cs.MA cs.SE</categories><comments>Dagstuhl seminar 12342</comments><report-no>DPA-12342</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this talk I review where we stand regarding the engineering of multi-agent
systems. There is both good news and bad news. The good news is that over the
past decade we've made considerable progress on techniques for engineering
multi-agent systems: we have good, usable methodologies, and mature tools.
Furthermore, we've seen a wide range of demonstrated applications, and have
even begun to quantify the advantages of agent technology. However, industry
involvement in AAMAS appears to be declining (as measured by industry
sponsorship of the conference), and industry affiliated attendants at AAMAS
2012 were few (1-2%). Furthermore, looking at the applications of agents being
reported at recent AAMAS, usage of Agent Oriented Software Engineering (AOSE)
and of Agent Oriented Programming Languages (AOPLs) is quite limited. This
observation is corroborated by the results of a 2008 survey by Frank and
Virginia Dignum. Based on these observations, I make five recommendations: (1)
Re-engage with industry; (2) Stop designing AOPLs and AOSE methodologies ...
and instead ... (3) Move to the &quot;macro&quot; level: develop techniques for designing
and implementing interaction, integrate micro (single cognitive agent) and
macro (MAS) design and implementation; (4) Develop techniques for the Assurance
of MAS; and (5) Re-engage with the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1432</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1432</id><created>2012-09-06</created><authors><author><keyname>Latella</keyname><forenames>D.</forenames><affiliation>CNR -- Istituto di Scienza e Tecnologie dell'Informazione 'A. Faedo'</affiliation></author><author><keyname>Massink</keyname><forenames>M.</forenames><affiliation>CNR -- Istituto di Scienza e Tecnologie dell'Informazione 'A. Faedo'</affiliation></author><author><keyname>de Vink</keyname><forenames>E. P.</forenames><affiliation>Technische Universiteit Eindhoven and Centrum Wiskunde Informatica</affiliation></author></authors><title>Bisimulation of Labeled State-to-Function Transition Systems of
  Stochastic Process Languages</title><categories>cs.LO</categories><comments>In Proceedings ACCAT 2012, arXiv:1208.4301</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 93, 2012, pp. 23-43</journal-ref><doi>10.4204/EPTCS.93.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labeled state-to-function transition systems, FuTS for short, admit multiple
transition schemes from states to functions of finite support over general
semirings. As such they constitute a convenient modeling instrument to deal
with stochastic process languages. In this paper, the notion of bisimulation
induced by a FuTS is proposed and a correspondence result is proven stating
that FuTS-bisimulation coincides with the behavioral equivalence of the
associated functor. As generic examples, the concrete existing equivalences for
the core of the process algebras ACP, PEPA and IMC are related to the
bisimulation of specific FuTS, providing via the correspondence result
coalgebraic justification of the equivalences of these calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1433</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1433</id><created>2012-09-06</created><authors><author><keyname>Diskin</keyname><forenames>Zinovy</forenames><affiliation>McMaster University/The University of Waterloo</affiliation></author><author><keyname>Maibaum</keyname><forenames>Tom</forenames><affiliation>McMaster University</affiliation></author></authors><title>Category Theory and Model-Driven Engineering: From Formal Semantics to
  Design Patterns and Beyond</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ACCAT 2012, arXiv:1208.4301</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 93, 2012, pp. 1-21</journal-ref><doi>10.4204/EPTCS.93.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a hidden intrigue in the title. CT is one of the most abstract
mathematical disciplines, sometimes nicknamed &quot;abstract nonsense&quot;. MDE is a
recent trend in software development, industrially supported by standards,
tools, and the status of a new &quot;silver bullet&quot;. Surprisingly, categorical
patterns turn out to be directly applicable to mathematical modeling of
structures appearing in everyday MDE practice. Model merging, transformation,
synchronization, and other important model management scenarios can be seen as
executions of categorical specifications.
  Moreover, the paper aims to elucidate a claim that relationships between CT
and MDE are more complex and richer than is normally assumed for &quot;applied
mathematics&quot;. CT provides a toolbox of design patterns and structural
principles of real practical value for MDE. We will present examples of how an
elementary categorical arrangement of a model management scenario reveals
deficiencies in the architecture of modern tools automating the scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1434</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1434</id><created>2012-09-06</created><authors><author><keyname>Markovski</keyname><forenames>Jasen</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Communicating Processes with Data for Supervisory Coordination</title><categories>cs.SY</categories><comments>In Proceedings FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 91, 2012, pp. 97-111</journal-ref><doi>10.4204/EPTCS.91.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We employ supervisory controllers to safely coordinate high-level
discrete(-event) behavior of distributed components of complex systems.
Supervisory controllers observe discrete-event system behavior, make a decision
on allowed activities, and communicate the control signals to the involved
parties. Models of the supervisory controllers can be automatically synthesized
based on formal models of the system components and a formalization of the safe
coordination (control) requirements. Based on the obtained models, code
generation can be used to implement the supervisory controllers in software, on
a PLC, or an embedded (micro)processor. In this article, we develop a process
theory with data that supports a model-based systems engineering framework for
supervisory coordination. We employ communication to distinguish between the
different flows of information, i.e., observation and supervision, whereas we
employ data to specify the coordination requirements more compactly, and to
increase the expressivity of the framework. To illustrate the framework, we
remodel an industrial case study involving coordination of maintenance
procedures of a printing process of a high-tech Oce printer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1435</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1435</id><created>2012-09-06</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>Harald</forenames></author><author><keyname>Wolter</keyname><forenames>Uwe</forenames></author><author><keyname>L&#xf6;we</keyname><forenames>Michael</forenames></author></authors><title>Characterizing Van Kampen Squares via Descent Data</title><categories>cs.DM cs.LO</categories><comments>In Proceedings ACCAT 2012, arXiv:1208.4301</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 93, 2012, pp. 61-81</journal-ref><doi>10.4204/EPTCS.93.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categories in which cocones satisfy certain exactness conditions w.r.t.
pullbacks are subject to current research activities in theoretical computer
science. Usually, exactness is expressed in terms of properties of the pullback
functor associated with the cocone. Even in the case of non-exactness,
researchers in model semantics and rewriting theory inquire an elementary
characterization of the image of this functor. In this paper we will
investigate this question in the special case where the cocone is a cospan,
i.e. part of a Van Kampen square. The use of Descent Data as the dominant
categorical tool yields two main results: A simple condition which
characterizes the reachable part of the above mentioned functor in terms of
liftings of involved equivalence relations and (as a consequence) a necessary
and sufficient condition for a pushout to be a Van Kampen square formulated in
a purely algebraic manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1436</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1436</id><created>2012-09-06</created><authors><author><keyname>Sch&#xf6;lzel</keyname><forenames>Hanna</forenames><affiliation>Institut f&#xfc;r Softwaretechnik und Theoretische Informatik, Technische Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Ehrig</keyname><forenames>Hartmut</forenames><affiliation>Institut f&#xfc;r Softwaretechnik und Theoretische Informatik, Technische Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Maximova</keyname><forenames>Maria</forenames><affiliation>Institut f&#xfc;r Softwaretechnik und Theoretische Informatik, Technische Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Garbriel</keyname><forenames>Karsten</forenames><affiliation>Institut f&#xfc;r Softwaretechnik und Theoretische Informatik, Technische Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Hermann</keyname><forenames>Frank</forenames><affiliation>University of Luxembourg, Interdisciplinary Centre for Security, Reliability and Trust</affiliation></author></authors><title>Satisfaction, Restriction and Amalgamation of Constraints in the
  Framework of M-Adhesive Categories</title><categories>cs.LO</categories><comments>In Proceedings ACCAT 2012, arXiv:1208.4301</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 93, 2012, pp. 83-104</journal-ref><doi>10.4204/EPTCS.93.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application conditions for rules and constraints for graphs are well-known in
the theory of graph transformation and have been extended already to M-adhesive
transformation systems. According to the literature we distinguish between two
kinds of satisfaction for constraints, called general and initial satisfaction
of constraints, where initial satisfaction is defined for constraints over an
initial object of the base category. Unfortunately, the standard definition of
general satisfaction is not compatible with negation in contrast to initial
satisfaction.
  Based on the well-known restriction of objects along type morphisms, we study
in this paper restriction and amalgamation of application conditions and
constraints together with their solutions. In our main result, we show
compatibility of initial satisfaction for positive constraints with restriction
and amalgamation, while general satisfaction fails in general.
  Our main result is based on the compatibility of composition via pushouts
with restriction, which is ensured by the horizontal van Kampen property in
addition to the vertical one that is generally satisfied in M-adhesive
categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1450</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1450</id><created>2012-09-07</created><authors><author><keyname>Schwartz</keyname><forenames>Yannick</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Varoquaux</keyname><forenames>Ga&#xeb;l</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author></authors><title>On spatial selectivity and prediction across conditions with fMRI</title><categories>stat.ML cs.LG</categories><comments>PRNI 2012 : 2nd International Workshop on Pattern Recognition in
  NeuroImaging, London : United Kingdom (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers in functional neuroimaging mostly use activation coordinates to
formulate their hypotheses. Instead, we propose to use the full statistical
images to define regions of interest (ROIs). This paper presents two machine
learning approaches, transfer learning and selection transfer, that are
compared upon their ability to identify the common patterns between brain
activation maps related to two functional tasks. We provide some preliminary
quantification of these similarities, and show that selection transfer makes it
possible to set a spatial scale yielding ROIs that are more specific to the
context of interest than with transfer learning. In particular, selection
transfer outlines well known regions such as the Visual Word Form Area when
discriminating between different visual tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1476</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1476</id><created>2012-09-07</created><authors><author><keyname>Barankai</keyname><forenames>Norbert</forenames></author><author><keyname>Fekete</keyname><forenames>Attila</forenames></author><author><keyname>Vattay</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>The effect of network structure on phase transitions in queuing networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>12 pages, 7 figures</comments><doi>10.1103/PhysRevE.86.066111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, De Martino et al have presented a general framework for the study
of transportation phenomena on complex networks. One of their most significant
achievements was a deeper understanding of the phase transition from the
uncongested to the congested phase at a critical traffic load. In this paper,
we also study phase transition in transportation networks using a discrete time
random walk model. Our aim is to establish a direct connection between the
structure of the graph and the value of the critical traffic load. Applying
spectral graph theory, we show that the original results of De Martino et al
showing that the critical loading depends only on the degree sequence of the
graph -- suggesting that different graphs with the same degree sequence have
the same critical loading if all other circumstances are fixed -- is valid only
if the graph is dense enough. For sparse graphs, higher order corrections,
related to the local structure of the network, appear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1479</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1479</id><created>2012-09-07</created><authors><author><keyname>Haerter</keyname><forenames>Jan O.</forenames></author><author><keyname>Jamtveit</keyname><forenames>Bjorn</forenames></author><author><keyname>Mathiesen</keyname><forenames>Joachim</forenames></author></authors><title>Communication dynamics in finite capacity social networks</title><categories>physics.soc-ph cs.SI</categories><comments>Physical Review Letter, accepted (5 pages, 4 figures)</comments><acm-class>J.4; E.1</acm-class><journal-ref>Phys. Rev. Lett. 109, 168701 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.168701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In communication networks structure and dynamics are tightly coupled. The
structure controls the flow of information and is itself shaped by the
dynamical process of information exchanged between nodes. In order to reconcile
structure and dynamics, a generic model, based on the local interaction between
nodes, is considered for the communication in large social networks. In
agreement with data from a large human organization, we show that the flow is
non-Markovian and controlled by the temporal limitations of individuals. We
confirm the versatility of our model by predicting simultaneously the
degree-dependent node activity, the balance between information input and
output of nodes and the degree distribution. Finally, we quantify the
limitations to network analysis when it is based on data sampled over a finite
period of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1481</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1481</id><created>2012-09-07</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Krauthammer</keyname><forenames>Michael</forenames></author></authors><title>Image Mining from Gel Diagrams in Biomedical Publications</title><categories>cs.IR q-bio.QM</categories><journal-ref>Proceedings of the 5th International Symposium on Semantic Mining
  in Biomedicine (SMBM 2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authors of biomedical publications often use gel images to report
experimental results such as protein-protein interactions or protein
expressions under different conditions. Gel images offer a way to concisely
communicate such findings, not all of which need to be explicitly discussed in
the article text. This fact together with the abundance of gel images and their
shared common patterns makes them prime candidates for image mining endeavors.
We introduce an approach for the detection of gel images, and present an
automatic workflow to analyze them. We are able to detect gel segments and
panels at high accuracy, and present first results for the identification of
gene names in these images. While we cannot provide a complete solution at this
point, we present evidence that this kind of image mining is feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1482</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1482</id><created>2012-09-07</created><authors><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author><author><keyname>Shulman</keyname><forenames>Haya</forenames></author></authors><title>Unilateral Antidotes to DNS Cache Poisoning</title><categories>cs.CR</categories><journal-ref>SecureComm 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate defenses against DNS cache poisoning focusing on mechanisms
that can be readily deployed unilaterally by the resolving organisation,
preferably in a single gateway or a proxy. DNS poisoning is (still) a major
threat to Internet security; determined spoofing attackers are often able to
circumvent currently deployed antidotes such as port randomisation. The
adoption of DNSSEC, which would foil DNS poisoning, remains a long-term
challenge. We discuss limitations of the prominent resolver-only defenses,
mainly port and IP randomisation, 0x20 encoding and birthday protection. We
then present two new (unilateral) defenses: the sandwich antidote and the NAT
antidote. The defenses are simple, effective and efficient, and can be
implemented in a gateway connecting the resolver to the Internet. The sandwich
antidote is composed of two phases: poisoning-attack detection and then
prevention. The NAT antidote adds entropy to DNS requests by switching the
resolver's IP address to a random address (belonging to the same autonomous
system). Finally, we show how to implement the birthday protection mechanism in
the gateway, thus allowing to restrict the number of DNS requests with the same
query to 1 even when the resolver does not support this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1483</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1483</id><created>2012-09-07</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Krauthammer</keyname><forenames>Michael</forenames></author></authors><title>Underspecified Scientific Claims in Nanopublications</title><categories>cs.DL cs.IR</categories><journal-ref>In Proceedings of the Web of Linked Entities Workshop (WoLE 2012),
  CEUR Workshop Proceedings, Volume 906, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application range of nanopublications --- small entities of scientific
results in RDF representation --- could be greatly extended if complete formal
representations are not mandatory. To that aim, we present an approach to
represent and interlink scientific claims in an underspecified way, based on
independent English sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1551</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1551</id><created>2012-09-07</created><authors><author><keyname>Chopra</keyname><forenames>Amit K.</forenames></author></authors><title>The Meaning of Requirements and Adaptation</title><categories>cs.SE</categories><comments>12 pages, 1 figure</comments><acm-class>D.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional understanding of stakeholders requirements is that they
express desirable relationships among phenomena in the relevant environment.
Historically, software engineering research has tended to focus more on the
problems of modeling requirements and deriving specifications given
requirements, and much less on the meaning of a requirement itself. I introduce
new concepts that elucidate the meaning of requirements, namely, the designated
set and the falsifiability of requirements.
  By relying on these concepts, I (i) show that the adaptive requirements
approaches, which constitute a lively and growing field in RE, are
fundamentally flawed, (ii) give a sufficient characterization of vague
requirements, and (iii) make the connection between requirements modeling and
the Zave and Jackson sense of engineering. I support my claims with examples
and an extensive discussion of the related literature. Finally, I show how
adaptation can be framed in terms of Zave and Jackson's ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1557</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1557</id><created>2012-09-07</created><updated>2016-01-27</updated><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Boufounos</keyname><forenames>Petros T.</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Learning Model-Based Sparsity via Projected Gradient Descent</title><categories>stat.ML cs.LG math.OC</categories><msc-class>62FXX, 65KXX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several convex formulation methods have been proposed previously for
statistical estimation with structured sparsity as the prior. These methods
often require a carefully tuned regularization parameter, often a cumbersome or
heuristic exercise. Furthermore, the estimate that these methods produce might
not belong to the desired sparsity model, albeit accurately approximating the
true parameter. Therefore, greedy-type algorithms could often be more desirable
in estimating structured-sparse parameters. So far, these greedy methods have
mostly focused on linear statistical models. In this paper we study the
projected gradient descent with non-convex structured-sparse parameter model as
the constraint set. Should the cost function have a Stable Model-Restricted
Hessian the algorithm produces an approximation for the desired minimizer. As
an example we elaborate on application of the main results to estimation in
Generalized Linear Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1558</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1558</id><created>2012-09-07</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Nandi</keyname><forenames>Pradipti</forenames></author><author><keyname>Barman</keyname><forenames>Nilanjana</forenames></author><author><keyname>Das</keyname><forenames>Debolina</forenames></author><author><keyname>Chakraborty</keyname><forenames>Subhabrata</forenames></author></authors><title>A Comparative Study between Moravec and Harris Corner Detection of Noisy
  Images Using Adaptive Wavelet Thresholding Technique</title><categories>cs.CV</categories><comments>8 pages, 13 figures</comments><journal-ref>International Journal of Engineering Research and Applications
  (IJERA) Vol. 2, Issue 1, Jan-Feb 2012, pp.599-606</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a comparative study between Moravec and Harris Corner Detection
has been done for obtaining features required to track and recognize objects
within a noisy image. Corner detection of noisy images is a challenging task in
image processing. Natural images often get corrupted by noise during
acquisition and transmission. As Corner detection of these noisy images does
not provide desired results, hence de-noising is required. Adaptive wavelet
thresholding approach is applied for the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1563</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1563</id><created>2012-09-07</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sayantan</forenames></author><author><keyname>Biswas</keyname><forenames>Shouvik</forenames></author><author><keyname>Roy</keyname><forenames>Anamitra Bardhan</forenames></author><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author></authors><title>Wavelet Based QRS Complex Detection of ECG Signal</title><categories>cs.CV</categories><comments>5 pages, 8 figures, ISSN: 2248-9622</comments><journal-ref>Journal of Engineering Research and Applications (IJERA) Vol. 2,
  Issue 3, 2012, pp.2361-2365</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used to
detect various cardiovascular diseases by measuring and recording the
electrical activity of the heart in exquisite detail. A wide range of heart
condition is determined by thorough examination of the features of the ECG
report. Automatic extraction of time plane features is important for
identification of vital cardiac diseases. This paper presents a
multi-resolution wavelet transform based system for detection 'P', 'Q', 'R',
'S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is an
important minutia of the ECG signal that corresponds to the heartbeat of the
concerned person. Abrupt increase in height of the 'R' wave or changes in the
measurement of the 'R-R' denote various anomalies of human heart. Similarly
'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart and
their peak amplitude also envisages other cardiac diseases. In this proposed
method the 'PQRST' peaks are marked and stored over the entire signal and the
time interval between two consecutive 'R' peaks and other peaks interval are
measured to detect anomalies in behavior of heart, if any. The peaks are
achieved by the composition of Daubeheissub bands wavelet of original ECG
signal. The accuracy of the 'PQRST' complex detection and interval measurement
is achieved up to 100% with high exactitude by processing and thresholding the
original ECG signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1595</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1595</id><created>2012-09-07</created><updated>2014-12-26</updated><authors><author><keyname>Pawlik</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Laso&#x144;</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Trotter</keyname><forenames>William T.</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Triangle-free intersection graphs of line segments with large chromatic
  number</title><categories>math.CO cs.CG cs.DM</categories><comments>Small corrections, bibliography update</comments><msc-class>05C62, 05C15</msc-class><journal-ref>J.Combin.Theory Ser.B 105 (2014) 6-10</journal-ref><doi>10.1016/j.jctb.2013.11.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 1970s, Erdos asked whether the chromatic number of intersection graphs
of line segments in the plane is bounded by a function of their clique number.
We show the answer is no. Specifically, for each positive integer $k$, we
construct a triangle-free family of line segments in the plane with chromatic
number greater than $k$. Our construction disproves a conjecture of Scott that
graphs excluding induced subdivisions of any fixed graph have chromatic number
bounded by a function of their clique number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1597</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1597</id><created>2012-09-07</created><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Adeyeye</keyname><forenames>John O.</forenames></author></authors><title>Sensitivity and block sensitivity of nested canalyzing function</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a recent characterization of nested canalyzing function (NCF), we
obtain the formula of the sensitivity of any NCF. Hence we find that any
sensitivity of NCF is between $\frac{n+1}{2}$ and $n$. Both lower and upper
bounds are tight. We prove that the block sensitivity, hence the $l$-block
sensitivity, is same to the sensitivity. It is well known that monotone
function also has this property. We eventually find all the functions which are
both monotone and nested canalyzing (MNCF). The cardinality of all the MNCF is
also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1628</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1628</id><created>2012-09-06</created><updated>2013-05-14</updated><authors><author><keyname>Merelli</keyname><forenames>Emanuela</forenames><affiliation>School of Science and Technology. CS division. University of Camerino. IT</affiliation></author><author><keyname>Paoletti</keyname><forenames>Nicola</forenames><affiliation>School of Science and Technology. CS division. University of Camerino. IT</affiliation></author><author><keyname>Tesei</keyname><forenames>Luca</forenames><affiliation>School of Science and Technology. CS division. University of Camerino. IT</affiliation></author></authors><title>A multi-level model for self-adaptive systems</title><categories>cs.LO cs.SE</categories><comments>In Proceedings of FOCLASA 2012, arXiv:1208.4327</comments><proxy>EPTCS</proxy><acm-class>F.1.1; D.2; F.3.1</acm-class><journal-ref>EPTCS 91, 2012, pp. 112-126</journal-ref><doi>10.4204/EPTCS.91.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a general multi-level model for self-adaptive systems. A
self-adaptive system is seen as composed by two levels: the lower level
describing the actual behaviour of the system and the upper level accounting
for the dynamically changing environmental constraints on the system. In order
to keep our description as general as possible, the lower level is modelled as
a state machine and the upper level as a second-order state machine whose
states have associated formulas over observable variables of the lower level.
Thus, each state of the second-order machine identifies the set of lower-level
states satisfying the constraints. Adaptation is triggered when a second-order
transition is performed; this means that the current system no longer can
satisfy the current high-level constraints and, thus, it has to adapt its
behaviour by reaching a state that meets the new constraints. The semantics of
the multi-level system is given by a flattened transition system that can be
statically checked in order to prove the correctness of the adaptation model.
To this aim we formalize two concepts of weak and strong adaptability providing
both a relational and a logical characterization. We report that this work
gives a formal computational characterization of multi-level self-adaptive
systems, evidencing the important role that (theoretical) computer science
could play in the emerging science of complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1645</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1645</id><created>2012-09-07</created><authors><author><keyname>Sergeev</keyname><forenames>Igor</forenames></author></authors><title>On additive complexity of a sequence of matrices</title><categories>cs.DS</categories><comments>7 pages, in English; 8 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show new upper and lower bounds for the complexity of implementation of a
sequence of Boolean matrices proposed by Kaski et al. (arXiv:1208.0554) with
additive circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1652</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1652</id><created>2012-09-07</created><authors><author><keyname>Hatton</keyname><forenames>Les</forenames></author></authors><title>Power-laws and the Conservation of Information in discrete token
  systems: Part 2 The role of defect</title><categories>cs.IT math-ph math.IT math.MP q-bio.GN</categories><comments>15 pages, 5 figures, 25 references</comments><acm-class>H.1.1; D.3.3; F.1.1; I.5.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a matching paper (arXiv:1207.5027), I proved that Conservation of Size and
Information in a discrete token based system is overwhelmingly likely to lead
to a power-law component size distribution with respect to the size of its
unique alphabet. This was substantiated to a very high level of significance
using some 55 million lines of source code of mixed provenance. The principle
was also applied to show that average gene length should be constant in an
animal kingdom where the same constraints appear to hold, the implication being
that Conservation of Information plays a similar role in discrete token-based
systems as the Conservation of Energy does in physical systems.
  In this part 2, the role of defect will be explored and a functional
behaviour for defect derived to be consistent with the power-law behaviour
substantiated above.
  This will be supported by further experimental data and the implications
explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1664</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1664</id><created>2012-09-07</created><updated>2013-08-07</updated><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author></authors><title>Explicit tensors of border rank at least 2n-1</title><categories>cs.CC math.AG</categories><comments>8 pages, version 2 contains a study of Griesser's equations</comments><msc-class>68Q17, 14M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For odd n, I write down tensors in C^n\otimes C^n\otimes C^n of border rank
2n-1, showing the non-triviality of the Young-flattening equations of
Landsberg-Ottaviani. I also study the border rank of the tensors of Alexeev et.
al., showing the tensors their tensors T_{2^k}, despite having rank equal to
2^{k+1}-1, have border rank equal to 2^k, the minimum of any concise tensor. I
also study the equations of Griesser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1673</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1673</id><created>2012-09-07</created><authors><author><keyname>Li</keyname><forenames>Shujun</forenames></author><author><keyname>Karrenbauer</keyname><forenames>Andreas</forenames></author><author><keyname>Saupe</keyname><forenames>Dietmar</forenames></author><author><keyname>Kuo</keyname><forenames>C. -C. Jay</forenames></author></authors><title>Recovering Missing Coefficients in DCT-Transformed Images</title><categories>cs.MM cs.CR math.OC</categories><comments>4 pages, 4 figures</comments><journal-ref>Proceedings of 2011 18th IEEE International Conference on Image
  Processing (ICIP 2011), pages 1537-1540, IEEE, 2011</journal-ref><doi>10.1109/ICIP.2011.6115738</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general method for recovering missing DCT coefficients in DCT-transformed
images is presented in this work. We model the DCT coefficients recovery
problem as an optimization problem and recover all missing DCT coefficients via
linear programming. The visual quality of the recovered image gradually
decreases as the number of missing DCT coefficients increases. For some images,
the quality is surprisingly good even when more than 10 most significant DCT
coefficients are missing. When only the DC coefficient is missing, the proposed
algorithm outperforms existing methods according to experimental results
conducted on 200 test images. The proposed recovery method can be used for
cryptanalysis of DCT based selective encryption schemes and other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1678</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1678</id><created>2012-09-07</created><authors><author><keyname>Mamun</keyname><forenames>Mohammad Saiful Islam</forenames></author><author><keyname>Kabir</keyname><forenames>A. F. M Sultanul</forenames></author><author><keyname>Hossen</keyname><forenames>Md. Sakhawat</forenames></author><author><keyname>Khan</keyname><forenames>Md. Razib Hayat</forenames></author></authors><title>Policy based intrusion detection and response system in hierarchical WSN
  architecture</title><categories>cs.CR</categories><comments>5 pages, IEEE International Conference on Wireless
  Communication,Vehicular Technology, Information Theory and Aerospace &amp;
  Electronic Systems Technology, Wireless Vitae'09, May 17-20, 2009. Aalborg,
  Denmark. ISBN 978-1-4244-4067-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, wireless sensor network becomes popular both in civil and
military jobs. However, security is one of the significant challenges for
sensor network because of their deployment in open and unprotected environment.
As cryptographic mechanism is not enough to protect sensor network from
external attacks, intrusion detection system (IDS) needs to be introduced. In
this paper we propose a policy based IDS for hierarchical architecture that
fits the current demands and restrictions of wireless ad hoc sensor network. In
this proposed IDS architecture we followed clustering mechanism to build four
level hierarchical network which enhance network scalability to large
geographical area and use both anomaly and misuse detection techniques for
intrusion detection that concentrates on power saving of sensor nodes by
distributing the responsibility of intrusion detection among different layers.
We also introduce a policy based intrusion response system for hierarchical
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1679</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1679</id><created>2012-09-07</created><updated>2012-10-02</updated><authors><author><keyname>Nabaee</keyname><forenames>Mahdy</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Bayesian Quantized Network Coding via Belief Propagation</title><categories>cs.IT math.IT</categories><comments>submitted for IEEE 2013 International Conference on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an alternative for routing based packet forwarding,
which uses network coding to increase transmission efficiency, in terms of both
compression and error resilience. This non-adaptive encoding is called
quantized network coding, which involves random linear mapping in the real
field, followed by quantization to cope with the finite capacity of the links.
At the gateway node, which collects received quantized network coder packets,
minimum mean squared error decoding is performed, by using belief propagation
in the factor graph representation. Our simulation results show a significant
improvement, in terms of the number of required packets to recover the
messages, which can be interpreted as an embedded distributed source coding for
correlated messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1682</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1682</id><created>2012-09-07</created><authors><author><keyname>Samanta</keyname><forenames>Sovan</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Irregular Bipolar Fuzzy Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define irregular bipolar fuzzy graphs and its various
classifications. Size of regular bipolar fuzzy graphs is derived. The relation
between highly and neighbourly irregular bipolar fuzzy graphs are established.
Some basic theorems related to the stated graphs have also been presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1688</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1688</id><created>2012-09-08</created><updated>2015-11-12</updated><authors><author><keyname>Negahban</keyname><forenames>Sahand</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Rank Centrality: Ranking from Pair-wise Comparisons</title><categories>cs.LG stat.ML</categories><comments>45 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of aggregating pair-wise comparisons to obtain a global ranking
over a collection of objects has been of interest for a very long time: be it
ranking of online gamers (e.g. MSR's TrueSkill system) and chess players,
aggregating social opinions, or deciding which product to sell based on
transactions. In most settings, in addition to obtaining a ranking, finding
`scores' for each object (e.g. player's rating) is of interest for
understanding the intensity of the preferences.
  In this paper, we propose Rank Centrality, an iterative rank aggregation
algorithm for discovering scores for objects (or items) from pair-wise
comparisons. The algorithm has a natural random walk interpretation over the
graph of objects with an edge present between a pair of objects if they are
compared; the score, which we call Rank Centrality, of an object turns out to
be its stationary probability under this random walk. To study the efficacy of
the algorithm, we consider the popular Bradley-Terry-Luce (BTL) model
(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which
each object has an associated score which determines the probabilistic outcomes
of pair-wise comparisons between objects. In terms of the pair-wise marginal
probabilities, which is the main subject of this paper, the MNL model and the
BTL model are identical. We bound the finite sample error rates between the
scores assumed by the BTL model and those estimated by our algorithm. In
particular, the number of samples required to learn the score well with high
probability depends on the structure of the comparison graph. When the
Laplacian of the comparison graph has a strictly positive spectral gap, e.g.
each item is compared to a subset of randomly chosen items, this leads to
dependence on the number of samples that is nearly order-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1695</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1695</id><created>2012-09-08</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Decentralized Stochastic Control with Partial History Sharing: A Common
  Information Approach</title><categories>cs.SY math.OC</categories><comments>37 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general model of decentralized stochastic control called partial history
sharing information structure is presented. In this model, at each step the
controllers share part of their observation and control history with each
other. This general model subsumes several existing models of information
sharing as special cases. Based on the information commonly known to all the
controllers, the decentralized problem is reformulated as an equivalent
centralized problem from the perspective of a coordinator. The coordinator
knows the common information and select prescriptions that map each
controller's local information to its control actions. The optimal control
problem at the coordinator is shown to be a partially observable Markov
decision process (POMDP) which is solved using techniques from Markov decision
theory. This approach provides (a) structural results for optimal strategies,
and (b) a dynamic program for obtaining optimal strategies for all controllers
in the original decentralized problem. Thus, this approach unifies the various
ad-hoc approaches taken in the literature. In addition, the structural results
on optimal control strategies obtained by the proposed approach cannot be
obtained by the existing generic approach (the person-by-person approach) for
obtaining structural results in decentralized problems; and the dynamic program
obtained by the proposed approach is simpler than that obtained by the existing
generic approach (the designer's approach) for obtaining dynamic programs in
decentralized problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1698</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1698</id><created>2012-09-08</created><authors><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Malec</keyname><forenames>David</forenames></author></authors><title>Sequential Auctions of Identical Items with Budget-Constrained Bidders</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study sequential auctions with two budget constrained
bidders and any number of identical items. All prior results on such auctions
consider only two items. We construct a canonical outcome of the auction that
is the only natural equilibrium and is unique under a refinement of subgame
perfect equilibria. We show certain interesting properties of this equilibrium;
for instance, we show that the prices decrease as the auction progresses. This
phenomenon has been observed in many experiments and previous theoretic work
attributed it to features such as uncertainty in the supply or risk averse
bidders. We show that such features are not needed for this phenomenon and that
it arises purely from the most essential features: budget constraints and the
sequential nature of the auction. A little surprisingly we also show that in
this equilibrium one agent wins all his items in the beginning and then the
other agent wins the rest. The major difficulty in analyzing such sequential
auctions has been in understanding how the selling prices of the first few
rounds affect the utilities of the agents in the later rounds. We tackle this
difficulty by identifying certain key properties of the auction and the proof
is via a joint induction on all of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1699</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1699</id><created>2012-09-08</created><authors><author><keyname>Pace</keyname><forenames>Gordon J.</forenames></author><author><keyname>Ravn</keyname><forenames>Anders P.</forenames></author></authors><title>Proceedings Sixth Workshop on Formal Languages and Analysis of
  Contract-Oriented Software</title><categories>cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 94, 2012</journal-ref><doi>10.4204/EPTCS.94</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to negotiate contracts for a wide range of aspects and to provide
services conforming to them is a most pressing need in service-oriented
architectures. High-level models of contracts are making their way into the
area, but application developers are still left to their own devices when it
comes to writing code that will comply with a contract concluded before service
provision. At the programming language level, contracts appear as separate
concerns that crosscut through application logic. Therefore there is a need for
contract analysis tools that extract abstracted models from applications so
they become amenable to formal reasoning using formal language techniques.
  Since its inception, the aim of of FLACOS has been that of bringing together
researchers and practitioners working on language- or application-based
solutions to these problems through the formalization of contracts, the design
of appropriate abstraction mechanisms, and tools and techniques for analysis of
contracts, and analysis, testing and monitoring of conformance to contracts by
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1700</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1700</id><created>2012-09-08</created><authors><author><keyname>Patil</keyname><forenames>V. P.</forenames></author></authors><title>Performance Evaluation of on demand and Table driven Protocol for
  Wireless Ad hoc Network</title><categories>cs.NI</categories><comments>13 pages,4 figures,for international journal publications IJCES</comments><report-no>Paper id #127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Adhoc Network is a wireless network without infrastructure.It is a
kind of wireless adhoc network,and is a self configuring network of mobile
routers connected by wireless links.The routers are free to move randomly and
organize themselves arbitrarily,thus the network's wireless topology may change
rapidly and unpredictably. Such a network may operate in a standalone
fashion,or may be connected to the larger Internet.There are various routing
protocols available for MANET.The most popular ones are DSR,AODV and DSDV.This
paper examines two routing protocols for mobile ad hoc networks :the
Destination Sequenced Distance Vector,the table driven protocol and the Ad hoc
On Demand Distance Vector routing,an On Demand protocol and evaluates both
protocols based on packet delivery fraction, average end to end
delay,throughput and routing overhead while varying pause time.The performance
evaluation has been done by using simulation tool NS2 which is the main
simulator, NAM(Network Animator)and excel graph which is used for preparing the
graphs from the trace files.Simulation revealed that although DSDV perfectly
scales to small networks with low node speeds,AODV is preferred due to its more
efficient use of bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1711</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1711</id><created>2012-09-08</created><authors><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author></authors><title>Programming Languages for Scientific Computing</title><categories>cs.PL cs.CE cs.MS</categories><comments>21 pages</comments><journal-ref>Encyclopedia of Applied and Computational Mathematics, Springer,
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific computation is a discipline that combines numerical analysis,
physical understanding, algorithm development, and structured programming.
Several yottacycles per year on the world's largest computers are spent
simulating problems as diverse as weather prediction, the properties of
material composites, the behavior of biomolecules in solution, and the quantum
nature of chemical compounds. This article is intended to review specfic
languages features and their use in computational science. We will review the
strengths and weaknesses of different programming styles, with examples taken
from widely used scientific codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1716</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1716</id><created>2012-09-08</created><updated>2013-09-03</updated><authors><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Classification of binary systematic codes of small defect</title><categories>cs.IT math.IT</categories><comments>Le Matematiche (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper non-trivial non-linear binary systematic AMDS codes are
classified in terms of their weight distributions, employing only elementary
techniques. In particular, we show that their length and minimum distance
completely determine the weight distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1719</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1719</id><created>2012-09-08</created><authors><author><keyname>Simas</keyname><forenames>Tiago</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Semi-metric networks for recommender systems</title><categories>cs.IR cond-mat.stat-mech cs.SI</categories><journal-ref>2012 IEEE/WIC/ACM International Conference on Web Intelligence and
  Intelligent Agent Technology</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted graphs obtained from co-occurrence in user-item relations lead to
non-metric topologies. We use this semi-metric behavior to issue
recommendations, and discuss its relationship to transitive closure on fuzzy
graphs. Finally, we test the performance of this method against other item- and
user-based recommender systems on the Movielens benchmark. We show that
including highly semi-metric edges in our recommendation algorithms leads to
better recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1721</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1721</id><created>2012-09-08</created><authors><author><keyname>Litvinov</keyname><forenames>G. L.</forenames></author></authors><title>Idempotent and tropical mathematics. Complexity of algorithms and
  interval analysis</title><categories>math.NA cs.DS math.RA</categories><comments>26 pages, submitted to the journal &quot;Computers and Mathematics with
  Applications&quot;. arXiv admin note: substantial text overlap with
  arXiv:1001.4247, arXiv:1005.1247</comments><msc-class>81Q20, 16S80, 65G99, 68Q25, 15A80, 81S99, 65G30, 12K10, 46L65,
  28B10, 28A80, 28A25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A very brief introduction to tropical and idempotent mathematics is
presented. Tropical mathematics can be treated as a result of a dequantization
of the traditional mathematics as the Planck constant tends to zero taking
imaginary values. In the framework of idempotent mathematics usually
constructions and algorithms are more simple with respect to their traditional
analogs. We especially examine algorithms of tropical/idempotent mathematics
generated by a collection of basic semiring (or semifield) operations and other
&quot;good&quot; operations. Every algorithm of this type has an interval version. The
complexity of this interval version coincides with the complexity of the
initial algorithm. The interval version of an algorithm of this type gives
exact interval estimates for the corresponding output data. Algorithms of
linear algebra over idempotent and semirings are examined. In this case, basic
algorithms are polynomial as well as their interval versions. This situation is
very different from the traditional linear algebra, where basic algorithms are
polynomial but the corresponding interval versions are NP-hard and interval
estimates are not exact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1727</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1727</id><created>2012-09-08</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Lugosi</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Bandits with heavy tail</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic multi-armed bandit problem is well understood when the reward
distributions are sub-Gaussian. In this paper we examine the bandit problem
under the weaker assumption that the distributions have moments of order
1+\epsilon, for some $\epsilon \in (0,1]$. Surprisingly, moments of order 2
(i.e., finite variance) are sufficient to obtain regret bounds of the same
order as under sub-Gaussian reward distributions. In order to achieve such
regret, we define sampling strategies based on refined estimators of the mean
such as the truncated empirical mean, Catoni's M-estimator, and the
median-of-means estimator. We also derive matching lower bounds that also show
that the best achievable regret deteriorates when \epsilon &lt;1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1734</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1734</id><created>2012-09-08</created><authors><author><keyname>Mannava</keyname><forenames>Vishnuvardhan</forenames></author><author><keyname>Ramesh</keyname><forenames>T.</forenames></author></authors><title>Load Distribution Composite Design Pattern for Genetic Algorithm-Based
  Autonomic Computing Systems</title><categories>cs.SE cs.DC cs.NE</categories><comments>International Journal on Soft Computing (IJSC), 15 pages, 11 figures</comments><acm-class>D.2.11; D.2.10; D.3.3; I.2.8</acm-class><journal-ref>Vishnuvardhan, Mannava., &amp; Ramesh, T. (2012). Load Distribution
  Composite Design Pattern for Genetic Algorithm-Based Autonomic Computing
  Systems. International Journal on Soft Computing (IJSC), 3(3), 85-99</journal-ref><doi>10.5121/ijsc</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Current autonomic computing systems are ad hoc solutions that are designed
and implemented from the scratch. When designing software, in most cases two or
more patterns are to be composed to solve a bigger problem. A composite design
patterns shows a synergy that makes the composition more than just the sum of
its parts which leads to ready-made software architectures. As far as we know,
there are no studies on composition of design patterns for autonomic computing
domain. In this paper we propose pattern-oriented software architecture for
self-optimization in autonomic computing system using design patterns
composition and multi objective evolutionary algorithms that software designers
and/or programmers can exploit to drive their work. Main objective of the
system is to reduce the load in the server by distributing the population to
clients. We used Case Based Reasoning, Database Access, and Master Slave design
patterns. We evaluate the effectiveness of our architecture with and without
design patterns compositions. The use of composite design patterns in the
architecture and quantitative measurements are presented. A simple UML class
diagram is used to describe the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1738</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1738</id><created>2012-09-08</created><updated>2012-09-19</updated><authors><author><keyname>Fischer</keyname><forenames>Diana</forenames><affiliation>RWTH Aachen</affiliation></author><author><keyname>Kaiser</keyname><forenames>Lukasz</forenames><affiliation>CNRS and LIAFA, Universite Paris Diderot</affiliation></author></authors><title>Model Checking the Quantitative mu-Calculus on Linear Hybrid Systems</title><categories>cs.LO</categories><comments>LMCS submission</comments><proxy>LMCS</proxy><acm-class>D.2.4, F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  20, 2012) lmcs:760</journal-ref><doi>10.2168/LMCS-8(3:21)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the model-checking problem for a quantitative extension of the modal
mu-calculus on a class of hybrid systems. Qualitative model checking has been
proved decidable and implemented for several classes of systems, but this is
not the case for quantitative questions that arise naturally in this context.
Recently, quantitative formalisms that subsume classical temporal logics and
allow the measurement of interesting quantitative phenomena were introduced. We
show how a powerful quantitative logic, the quantitative mu-calculus, can be
model checked with arbitrary precision on initialised linear hybrid systems. To
this end, we develop new techniques for the discretisation of continuous state
spaces based on a special class of strategies in model-checking games and
present a reduction to a class of counter parity games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1739</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1739</id><created>2012-09-08</created><authors><author><keyname>Oksanen</keyname><forenames>Jan</forenames></author><author><keyname>Lund&#xe9;n</keyname><forenames>Jarmo</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Design of Spectrum Sensing Policy for Multi-user Multi-band Cognitive
  Radio Network</title><categories>cs.LG cs.NI</categories><comments>In Proceedings of CISS 2012 Conference, Princeton, NJ, USA, March
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding an optimal sensing policy for a particular access policy and sensing
scheme is a laborious combinatorial problem that requires the system model
parameters to be known. In practise the parameters or the model itself may not
be completely known making reinforcement learning methods appealing. In this
paper a non-parametric reinforcement learning-based method is developed for
sensing and accessing multi-band radio spectrum in multi-user cognitive radio
networks. A suboptimal sensing policy search algorithm is proposed for a
particular multi-user multi-band access policy and the randomized
Chair-Varshney rule. The randomized Chair-Varshney rule is used to reduce the
probability of false alarms under a constraint on the probability of detection
that protects the primary user. The simulation results show that the proposed
method achieves a sum profit (e.g. data rate) close to the optimal sensing
policy while achieving the desired probability of detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1750</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1750</id><created>2012-09-08</created><updated>2013-03-17</updated><authors><author><keyname>Grier</keyname><forenames>Daniel</forenames></author></authors><title>Deciding the Winner of an Arbitrary Finite Poset Game is PSPACE-Complete</title><categories>cs.CC cs.GT</categories><journal-ref>ICALP 2013, Part I, LNCS 7965, 2013, pp 497-503</journal-ref><doi>10.1007/978-3-642-39206-1_42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A poset game is a two-player game played over a partially ordered set (poset)
in which the players alternate choosing an element of the poset, removing it
and all elements greater than it. The first player unable to select an element
of the poset loses. Polynomial time algorithms exist for certain restricted
classes of poset games, such as the game of Nim. However, until recently the
complexity of arbitrary finite poset games was only known to exist somewhere
between NC^1 and PSPACE. We resolve this discrepancy by showing that deciding
the winner of an arbitrary finite poset game is PSPACE-complete. To this end,
we give an explicit reduction from Node Kayles, a PSPACE-complete game in which
players vie to chose an independent set in a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1751</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1751</id><created>2012-09-08</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author><author><keyname>Mart&#xed;n</keyname><forenames>Ferm&#xed;n Moscoso del Prado</forenames></author></authors><title>Information content versus word length in random typing</title><categories>physics.data-an cond-mat.stat-mech cs.CL</categories><journal-ref>Journal of Statistical Mechanics, L12002 (2011)</journal-ref><doi>10.1088/1742-5468/2011/12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been claimed that a linear relationship between a measure of
information content and word length is expected from word length optimization
and it has been shown that this linearity is supported by a strong correlation
between information content and word length in many languages (Piantadosi et
al. 2011, PNAS 108, 3825-3826). Here, we study in detail some connections
between this measure and standard information theory. The relationship between
the measure and word length is studied for the popular random typing process
where a text is constructed by pressing keys at random from a keyboard
containing letters and a space behaving as a word delimiter. Although this
random process does not optimize word lengths according to information content,
it exhibits a linear relationship between information content and word length.
The exact slope and intercept are presented for three major variants of the
random typing process. A strong correlation between information content and
word length can simply arise from the units making a word (e.g., letters) and
not necessarily from the interplay between a word and its context as proposed
by Piantadosi et al. In itself, the linear relation does not entail the results
of any optimization process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1759</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1759</id><created>2012-09-08</created><authors><author><keyname>Ioannou</keyname><forenames>Yani</forenames></author><author><keyname>Taati</keyname><forenames>Babak</forenames></author><author><keyname>Harrap</keyname><forenames>Robin</forenames></author><author><keyname>Greenspan</keyname><forenames>Michael</forenames></author></authors><title>Difference of Normals as a Multi-Scale Operator in Unorganized Point
  Clouds</title><categories>cs.CV</categories><comments>To be published in proceedings of 3DIMPVT 2012</comments><doi>10.1109/3DIMPVT.2012.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel multi-scale operator for unorganized 3D point clouds is introduced.
The Difference of Normals (DoN) provides a computationally efficient,
multi-scale approach to processing large unorganized 3D point clouds. The
application of DoN in the multi-scale filtering of two different real-world
outdoor urban LIDAR scene datasets is quantitatively and qualitatively
demonstrated. In both datasets the DoN operator is shown to segment large 3D
point clouds into scale-salient clusters, such as cars, people, and lamp posts
towards applications in semi-automatic annotation, and as a pre-processing step
in automatic object recognition. The application of the operator to
segmentation is evaluated on a large public dataset of outdoor LIDAR scenes
with ground truth annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1763</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1763</id><created>2012-09-08</created><authors><author><keyname>Abdallah</keyname><forenames>Yara</forenames></author><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>On the Efficiency-vs-Security Tradeoff in the Smart Grid</title><categories>cs.NI</categories><comments>A shorter version appears in IEEE CDC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart grid is envisioned to significantly enhance the efficiency of
energy consumption, by utilizing two-way communication channels between
consumers and operators. For example, operators can opportunistically leverage
the delay tolerance of energy demands in order to balance the energy load over
time, and hence, reduce the total operational cost. This opportunity, however,
comes with security threats, as the grid becomes more vulnerable to
cyber-attacks. In this paper, we study the impact of such malicious
cyber-attacks on the energy efficiency of the grid in a simplified setup. More
precisely, we consider a simple model where the energy demands of the smart
grid consumers are intercepted and altered by an active attacker before they
arrive at the operator, who is equipped with limited intrusion detection
capabilities. We formulate the resulting optimization problems faced by the
operator and the attacker and propose several scheduling and attack strategies
for both parties. Interestingly, our results show that, as opposed to
facilitating cost reduction in the smart grid, increasing the delay tolerance
of the energy demands potentially allows the attacker to force increased costs
on the system. This highlights the need for carefully constructed and robust
intrusion detection mechanisms at the operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1788</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1788</id><created>2012-09-09</created><authors><author><keyname>Moschetti</keyname><forenames>Elsa E.</forenames></author><author><keyname>Palacio</keyname><forenames>M. Gabriela</forenames></author><author><keyname>Picco</keyname><forenames>Mery</forenames></author><author><keyname>Bustos</keyname><forenames>Oscar H.</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author></authors><title>On the Use of Lee's Protocol for Speckle-Reducing Techniques</title><categories>cs.CV</categories><comments>http://www.laar.uns.edu.ar</comments><journal-ref>Latin American Applied Research, volume 36, number 2, pages
  115-121, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two new MAP (Maximum a Posteriori) filters for speckle
noise reduction and a Monte Carlo procedure for the assessment of their
performance. In order to quantitatively evaluate the results obtained using
these new filters, with respect to classical ones, a Monte Carlo extension of
Lee's protocol is proposed. This extension of the protocol shows that its
original version leads to inconsistencies that hamper its use as a general
procedure for filter assessment. Some solutions for these inconsistencies are
proposed, and a consistent comparison of speckle-reducing filters is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1794</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1794</id><created>2012-09-09</created><authors><author><keyname>Aissa</keyname><forenames>Saida</forenames></author><author><keyname>Gouider</keyname><forenames>Mohamed Salah</forenames></author></authors><title>A New Similairty Measure For Spatial Personalization</title><categories>cs.DB</categories><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.4, No.4, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting the relevant information by exploiting the spatial data warehouse
becomes increasingly hard. In fact, because of the enormous amount of data
stored in the spatial data warehouse, the user, usually, don't know what part
of the cube contain the relevant information and what the forthcoming query
should be. As a solution, we propose to study the similarity between the
behaviors of the users, in term of the spatial MDX queries launched on the
system, as a basis to recommend the next relevant MDX query to the current
user. This paper introduces a new similarity measure for comparing spatial MDX
queries. The proposed similarity measure could directly support the development
of spatial personalization approaches. The proposed similarity measure takes
into account the basic components of the similarity assessment models: the
topology, the direction and the distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1797</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1797</id><created>2012-09-09</created><updated>2013-06-05</updated><authors><author><keyname>Menahem</keyname><forenames>Eitan</forenames></author><author><keyname>Schclar</keyname><forenames>Alon</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Securing Your Transactions: Detecting Anomalous Patterns In XML
  Documents</title><categories>cs.CR cs.LG</categories><comments>Journal version (14 pages)</comments><acm-class>C.2.0; K.4.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XML transactions are used in many information systems to store data and
interact with other systems. Abnormal transactions, the result of either an
on-going cyber attack or the actions of a benign user, can potentially harm the
interacting systems and therefore they are regarded as a threat. In this paper
we address the problem of anomaly detection and localization in XML
transactions using machine learning techniques. We present a new XML anomaly
detection framework, XML-AD. Within this framework, an automatic method for
extracting features from XML transactions was developed as well as a practical
method for transforming XML features into vectors of fixed dimensionality. With
these two methods in place, the XML-AD framework makes it possible to utilize
general learning algorithms for anomaly detection. Central to the functioning
of the framework is a novel multi-univariate anomaly detection algorithm,
ADIFA. The framework was evaluated on four XML transactions datasets, captured
from real information systems, in which it achieved over 89% true positive
detection rate with less than a 0.2% false positive rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1800</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1800</id><created>2012-09-09</created><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Tang</keyname><forenames>Ke</forenames></author></authors><title>An Empirical Study of MAUC in Multi-class Problems with Uncertain Cost
  Matrices</title><categories>cs.LG</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cost-sensitive learning relies on the availability of a known and fixed cost
matrix. However, in some scenarios, the cost matrix is uncertain during
training, and re-train a classifier after the cost matrix is specified would
not be an option. For binary classification, this issue can be successfully
addressed by methods maximizing the Area Under the ROC Curve (AUC) metric.
Since the AUC can measure performance of base classifiers independent of cost
during training, and a larger AUC is more likely to lead to a smaller total
cost in testing using the threshold moving method. As an extension of AUC to
multi-class problems, MAUC has attracted lots of attentions and been widely
used. Although MAUC also measures performance of base classifiers independent
of cost, it is unclear whether a larger MAUC of classifiers is more likely to
lead to a smaller total cost. In fact, it is also unclear what kinds of
post-processing methods should be used in multi-class problems to convert base
classifiers into discrete classifiers such that the total cost is as small as
possible. In the paper, we empirically explore the relationship between MAUC
and the total cost of classifiers by applying two categories of post-processing
methods. Our results suggest that a larger MAUC is also beneficial.
Interestingly, simple calibration methods that convert the output matrix into
posterior probabilities perform better than existing sophisticated post
re-optimization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1803</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1803</id><created>2012-09-09</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Secure and Privacy-Preserving Authentication Protocols for Wireless Mesh
  Networks</title><categories>cs.CR cs.NI</categories><comments>32 pages, 10 figures. The work is an extended version of the author's
  previous works submitted in CoRR: arXiv:1107.5538v1 and arXiv:1102.1226v1</comments><journal-ref>Secure and Privacy-Preserving Authentication Protocols for
  Wireless Mesh Networks - Book Chapter in Applied Cryptography and Network
  Security, Editor: Jaydip Sen, pp. 3 - 34, April 2012, published by INTECH,
  Croatia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless mesh networks (WMNs) have emerged as a promising concept to meet the
challenges in next-generation wireless networks such as providing flexible,
adaptive, and reconfigurable architecture while offering cost-effective
solutions to service providers. As WMNs become an increasingly popular
replacement technology for last-mile connectivity to the home networking,
community and neighborhood networking, it is imperative to design efficient and
secure communication protocols for these networks. However, several
vulnerabilities exist in currently existing protocols for WMNs. These security
loopholes can be exploited by potential attackers to launch attack on WMNs. The
absence of a central point of administration makes securing WMNs even more
challenging. The broadcast nature of transmission and the dependency on the
intermediate nodes for multi-hop communications lead to several security
vulnerabilities in WMNs. The attacks can be external as well as internal in
nature. External attacks are launched by intruders who are not authorized users
of the network. For example, an intruding node may eavesdrop on the packets and
replay those packets at a later point of time to gain access to the network
resources. On the other hand, the internal attacks are launched by the nodes
that are part of the WMN. On example of such attack is an intermediate node
dropping packets which it was supposed to forward. This chapter presents a
comprehensive discussion on the current authentication and privacy protection
schemes for WMN. In addition, it proposes a novel security protocol for node
authentication and message confidentiality and an anonymization scheme for
privacy protection of users in WMNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1808</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1808</id><created>2012-09-09</created><authors><author><keyname>Gnewuch</keyname><forenames>Michael</forenames></author></authors><title>Lower Error Bounds for Randomized Multilevel and Changing Dimension
  Algorithms</title><categories>math.NA cs.CC</categories><comments>16 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide lower error bounds for randomized algorithms that approximate
integrals of functions depending on an unrestricted or even infinite number of
variables. More precisely, we consider the infinite-dimensional integration
problem on weighted Hilbert spaces with an underlying anchored decomposition
and arbitrary weights. We focus on randomized algorithms and the randomized
worst case error. We study two cost models for function evaluation which depend
on the number of active variables of the chosen sample points. Multilevel
algorithms behave very well with respect to the first cost model, while
changing dimension algorithms and also dimension-wise quadrature methods, which
are based on a similar idea, can take advantage of the more generous second
cost model. We prove the first non-trivial lower error bounds for randomized
algorithms in these cost models and demonstrate their quality in the case of
product weights. In particular, we show that the randomized changing dimension
algorithms provided in [L. Plaskota, G. W. Wasilkowski, J. Complexity 27
(2011), 505--518] achieve convergence rates arbitrarily close to the optimal
convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1811</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1811</id><created>2012-09-09</created><updated>2012-10-05</updated><authors><author><keyname>Brunelle</keyname><forenames>Justin F.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Evaluating the SiteStory Transactional Web Archive With the ApacheBench
  Tool</title><categories>cs.PF cs.DL</categories><comments>13 pages, Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional Web archives are created by periodically crawling a web site and
archiving the responses from the Web server. Although easy to implement and
common deployed, this form of archiving typically misses updates and may not be
suitable for all preservation scenarios, for example a site that is required
(perhaps for records compliance) to keep a copy of all pages it has served. In
contrast, transactional archives work in conjunction with a Web server to
record all pages that have been served. Los Alamos National Laboratory has
developed SiteSory, an open-source transactional archive written in Java
solution that runs on Apache Web servers, provides a Memento compatible access
interface, and WARC file export features. We used the ApacheBench utility on a
pre-release version of to measure response time and content delivery time in
different environments and on different machines. The performance tests were
designed to determine the feasibility of SiteStory as a production-level
solution for high fidelity automatic Web archiving. We found that SiteStory
does not significantly affect content server performance when it is performing
transactional archiving. Content server performance slows from 0.076 seconds to
0.086 seconds per Web page access when the content server is under load, and
from 0.15 seconds to 0.21 seconds when the resource has many embedded and
changing resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1826</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1826</id><created>2012-09-09</created><authors><author><keyname>Basu</keyname><forenames>Kinjal</forenames></author><author><keyname>Sengupta</keyname><forenames>Debapriya</forenames></author></authors><title>A spatio-spectral hybridization for edge preservation and noisy image
  restoration via local parametric mixtures and Lagrangian relaxation</title><categories>stat.ME cs.CV stat.AP</categories><comments>29 Pages, 13 figures</comments><msc-class>Primary : 62G07, 62H15, 68U10. Secondary : 62P30, 65D10, 65D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a fully unsupervised statistical method for edge
preserving image restoration and compression using a spatial decomposition
scheme. Smoothed maximum likelihood is used for local estimation of edge pixels
from mixture parametric models of local templates. For the complementary smooth
part the traditional L2-variational problem is solved in the Fourier domain
with Thin Plate Spline (TPS) regularization. It is well known that naive
Fourier compression of the whole image fails to restore a piece-wise smooth
noisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as
relative frequency histograms of samples from bi-variate densities where the
sample sizes might be unknown. The set of discontinuities is assumed to be
completely unsupervised Lebesgue-null, compact subset of the plane in the
continuous formulation of the problem. Proposed spatial decomposition uses a
widely used topological concept, partition of unity. The decision on edge pixel
neighborhoods are made based on the multiple testing procedure of Holms.
Statistical summary of the ?final output is decomposed into two layers of
information extraction, one for the subset of edge pixels and the other for the
smooth region. Robustness is also demonstrated by applying the technique on
noisy degradation of clean images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1859</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1859</id><created>2012-09-09</created><authors><author><keyname>King</keyname><forenames>Christine E.</forenames></author><author><keyname>Wang</keyname><forenames>Po T.</forenames></author><author><keyname>Chui</keyname><forenames>Luis A.</forenames></author><author><keyname>Do</keyname><forenames>An H.</forenames></author><author><keyname>Nenadic</keyname><forenames>Zoran</forenames></author></authors><title>Operation of a Brain-Computer Interface Walking Simulator by Users with
  Spinal Cord Injury</title><categories>cs.HC q-bio.NC</categories><comments>17 pages, 7 figures, 5 tables, supplementary video link
  (http://www.youtube.com/watch?v=K4Frq9pwAz8)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Spinal cord injury (SCI) can leave the affected individuals
unable to ambulate. Since there are no restorative treatments for SCI, novel
approaches such as brain-controlled prostheses have been sought. Our recent
studies show that a brain-computer interface (BCI) can be used to control
ambulation within a virtual reality environment (VRE), suggesting that a
BCI-controlled lower extremity prosthesis for ambulation may be feasible.
However, the operability of our BCI has not been tested in a SCI population.
  Methods: Five subjects with paraplegia or tetraplegia due to SCI underwent a
10-min training session in which they alternated between kinesthetic motor
imagery (KMI) of idling and walking while their electroencephalogram (EEG) were
recorded. Subjects then performed a goal-oriented online task, where they
utilized KMI to control the linear ambulation of an avatar and make 10
sequential stops at designated points within the VRE. Multiple online trials
were performed over 5 experimental days.
  Results: Classification accuracy of idling and walking was estimated offline
and ranged from 60.5% (p=0.0176) to 92.3% (p=1.36*10^-20) across subjects and
days. In the online task, all subjects achieved purposeful control with an
average performance of 7.4 +/- 2.3 successful stops in 273 +/- 51 sec (p&lt;0.01).
All subjects maintained purposeful control throughout the study, and their
online performances improved over time.
  Conclusions: The results demonstrate that SCI subjects can purposefully
operate a self-paced BCI walking simulator to complete a goal-oriented
ambulation task. The operation of this BCI system requires short training, is
intuitive, and robust against subject-to-subject and day-to-day
neurophysiological variations. These findings indicate that BCI-controlled
lower extremity prostheses for gait rehabilitation or restoration after SCI may
be feasible in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1873</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1873</id><created>2012-09-09</created><updated>2013-01-30</updated><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Stochastic Dual Coordinate Ascent Methods for Regularized Loss
  Minimization</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) has become popular for solving large scale
supervised machine learning optimization problems such as SVM, due to their
strong theoretical guarantees. While the closely related Dual Coordinate Ascent
(DCA) method has been implemented in various software packages, it has so far
lacked good convergence analysis. This paper presents a new analysis of
Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods
enjoy strong theoretical guarantees that are comparable or better than SGD.
This analysis justifies the effectiveness of SDCA for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1877</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1877</id><created>2012-09-10</created><authors><author><keyname>Kitaeff</keyname><forenames>Vyacheslav V.</forenames></author><author><keyname>Wu</keyname><forenames>Chen</forenames></author><author><keyname>Wicenec</keyname><forenames>Andreas</forenames></author><author><keyname>Cannon</keyname><forenames>Andrew D.</forenames></author><author><keyname>Vinsen</keyname><forenames>Kevin</forenames></author></authors><title>SkuareView: Client-Server Framework for Accessing Extremely Large Radio
  Astronomy Image Data</title><categories>astro-ph.IM cs.DC</categories><comments>8 pages, 1 figure, Astro-HPC '12 Proceedings of the 2012 workshop on
  High-Performance Computing for Astronomy</comments><doi>10.1145/2286976.2286984</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new wide-field radio telescopes, such as: ASKAP, MWA, and SKA; will
produce spectral-imaging data-cubes (SIDC) of unprecedented volume. This
requires new approaches to managing and servicing the data to the end-user. We
present a new integrated framework based on the JPEG2000/ISO/IEC 15444 standard
to address the challenges of working with extremely large SIDC. We also present
the developed j2k software, that converts and encodes FITS image cubes into
JPEG2000 images, paving the way to implementing the pre- sented framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1885</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1885</id><created>2012-09-10</created><authors><author><keyname>Kramer</keyname><forenames>Simon</forenames></author><author><keyname>Sack</keyname><forenames>Joshua</forenames></author></authors><title>Parametric Constructive Kripke-Semantics for Standard Multi-Agent Belief
  and Knowledge (Knowledge As Unbiased Belief)</title><categories>cs.LO cs.AI cs.DC cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose parametric constructive Kripke-semantics for multi-agent
KD45-belief and S5-knowledge in terms of elementary set-theoretic constructions
of two basic functional building blocks, namely bias (or viewpoint) and
visibility, functioning also as the parameters of the doxastic and epistemic
accessibility relation. The doxastic accessibility relates two possible worlds
whenever the application of the composition of bias with visibility to the
first world is equal to the application of visibility to the second world. The
epistemic accessibility is the transitive closure of the union of our doxastic
accessibility and its converse. Therefrom, accessibility relations for common
and distributed belief and knowledge can be constructed in a standard way. As a
result, we obtain a general definition of knowledge in terms of belief that
enables us to view S5-knowledge as accurate (unbiased and thus true)
KD45-belief, negation-complete belief and knowledge as exact KD45-belief and
S5-knowledge, respectively, and perfect S5-knowledge as precise (exact and
accurate) KD45-belief, and all this generically for arbitrary functions of bias
and visibility. Our results can be seen as a semantic complement to previous
foundational results by Halpern et al. about the (un)definability and
(non-)reducibility of knowledge in terms of and to belief, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1887</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1887</id><created>2012-09-10</created><authors><author><keyname>Majumder</keyname><forenames>A.</forenames></author><author><keyname>Jaiswal</keyname><forenames>Sharad</forenames></author><author><keyname>Naidu</keyname><forenames>K. V. M.</forenames></author><author><keyname>Shrivastava</keyname><forenames>Nisheeth</forenames></author></authors><title>Bulk content delivery using co-operating end-nodes with upload/download
  limits</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of optimizing the cost of content delivery in a
cooperative network of caches at end-nodes. The caches could be, for example,
within the computers of users downloading videos from websites (such as
Netflix, Blockbuster etc.), DVRs (such as TiVo, or cable boxes) used as part of
video on demand services or public hot-spots (e.g. Wi-Fi access points with a
cache) deployed over a city to serve content to mobile users. Each cache serves
user requests locally over a medium that incurs no additional costs (i.e. WiFi,
home LAN); if a request is not cached, it must be fetched from another cache or
a central server. In our model, each cache has a tiered back-haul internet
connection, with a usage cap (and fixed per-byte costs thereafter). Redirecting
requests intended for the central server to other caches with unused back-haul
capacity can bring down the network costs. Our goal is to develop a mechanism
to optimally 1) place data into the caches and 2) route requests to caches to
reduce the overall cost of content delivery.
  We develop a multi-criteria approximation based on a LP rounding procedure
that with a small (constant factor) blow-up in storage and upload limits of
each cache, gives a data placement that is within constant factor of the
optimum. Further, to speed up the solution, we propose a technique to cluster
caches into groups, solve the data placement problem within a group, and
combine the results in the rounding phase to get the global solution.Based on
extensive simulations, we show that our schemes perform very well in practice,
giving costs within $5--15$% to the optimal, and reducing the network load at a
central server by as much as 55% with only a marginal blow up in the limits.
Also we demonstrate that our approach out-performs a non-cooperative caching
mechanism by about 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1899</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1899</id><created>2012-09-10</created><authors><author><keyname>Yuming</keyname><forenames>Xu</forenames></author></authors><title>A matrix approach for computing extensions of argumentation frameworks</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1110.1416</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The matrices and their sub-blocks are introduced into the study of
determining various extensions in the sense of Dung's theory of argumentation
frameworks. It is showed that each argumentation framework has its matrix
representations, and the core semantics defined by Dung can be characterized by
specific sub-blocks of the matrix. Furthermore, the elementary permutations of
a matrix are employed by which an efficient matrix approach for finding out all
extensions under a given semantics is obtained. Different from several
established approaches, such as the graph labelling algorithm, Constraint
Satisfaction Problem algorithm, the matrix approach not only put the mathematic
idea into the investigation for finding out various extensions, but also
completely achieve the goal to compute all the extensions needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1905</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1905</id><created>2012-09-10</created><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Coquand</keyname><forenames>Thierry</forenames></author><author><keyname>M&#xf6;rtberg</keyname><forenames>Anders</forenames></author><author><keyname>Siles</keyname><forenames>Vincent</forenames></author></authors><title>Computing Persistent Homology within Coq/SSReflect</title><categories>cs.LO math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology is one of the most active branches of Computational
Algebraic Topology with applications in several contexts such as optical
character recognition or analysis of point cloud data. In this paper, we report
on the formal development of certified programs to compute persistent Betti
numbers, an instrumental tool of persistent homology, using the Coq proof
assistant together with the SSReflect extension. To this aim it has been
necessary to formalize the underlying mathematical theory of these algorithms.
This is another example showing that interactive theorem provers have reached a
point where they are mature enough to tackle the formalization of nontrivial
mathematical theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1910</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1910</id><created>2012-09-10</created><authors><author><keyname>Ishigami</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Kimura</keyname><forenames>Kinji</forenames></author><author><keyname>Nakamura</keyname><forenames>Yoshimasa</forenames></author></authors><title>On Implementation and Evaluation of Inverse Iteration Algorithm with
  compact WY Orthogonalization</title><categories>cs.NA math.NA</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new inverse iteration algorithm that can be used to compute all the
eigenvectors of a real symmetric tri-diagonal matrix on parallel computers is
developed. The modified Gram-Schmidt orthogonalization is used in the classical
inverse iteration. This algorithm is sequential and causes a bottleneck in
parallel computing. In this paper, the use of the compact WY representation is
proposed in the orthogonalization process of the inverse iteration with the
Householder transformation. This change results in drastically reduced
synchronization cost in parallel computing. The new algorithm is evaluated on
both an 8-core and a 32-core parallel computer, and it is shown that the new
algorithm is greatly faster than the classical inverse iteration algorithm in
computing all the eigenvectors of matrices with several thousand dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1911</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1911</id><created>2012-09-10</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Cancellieri</keyname><forenames>Giovanni</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Progressive Differences Convolutional Low-Density Parity-Check Codes</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures. Accepted for publication in IEEE Communications
  Letters. Copyright transferred to IEEE</comments><journal-ref>IEEE Communications Letters, Vol. 16, No. 11, pp. 1848-1851, Nov.
  2012</journal-ref><doi>10.1109/LCOMM.2012.091212.121230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new family of low-density parity-check (LDPC) convolutional
codes that can be designed using ordered sets of progressive differences. We
study their properties and define a subset of codes in this class that have
some desirable features, such as fixed minimum distance and Tanner graphs
without short cycles. The design approach we propose ensures that these
properties are guaranteed independently of the code rate. This makes these
codes of interest in many practical applications, particularly when high rate
codes are needed for saving bandwidth. We provide some examples of coded
transmission schemes exploiting this new class of codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1916</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1916</id><created>2012-09-10</created><updated>2012-09-19</updated><authors><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames><affiliation>Uppsala University</affiliation></author></authors><title>Model-Checking of Ordered Multi-Pushdown Automata</title><categories>cs.LO cs.FL</categories><comments>31 pages</comments><proxy>LMCS</proxy><acm-class>D.2.4, D.3.1, F.4.3, I.2.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (September
  20, 2012) lmcs:871</journal-ref><doi>10.2168/LMCS-8(3:20)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the verification problem of ordered multi-pushdown automata: A
multi-stack extension of pushdown automata that comes with a constraint on
stack transitions such that a pop can only be performed on the first non-empty
stack. First, we show that the emptiness problem for ordered multi-pushdown
automata is in 2ETIME. Then, we prove that, for an ordered multi-pushdown
automata, the set of all predecessors of a regular set of configurations is an
effectively constructible regular set. We exploit this result to solve the
global model-checking which consists in computing the set of all configurations
of an ordered multi-pushdown automaton that satisfy a given w-regular property
(expressible in linear-time temporal logics or the linear-time \mu-calculus).
As an immediate consequence, we obtain an 2ETIME upper bound for the
model-checking problem of w-regular properties for ordered multi-pushdown
automata (matching its lower-bound).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1942</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1942</id><created>2012-09-10</created><updated>2012-09-17</updated><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>Sorting distinct integer keys using in-place associative sort</title><categories>cs.DS</categories><comments>20 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572</comments><msc-class>68P05, 68P10</msc-class><acm-class>E.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In-place associative integer sorting technique was proposed for integer lists
which requires only constant amount of additional memory replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms.
The technique was explained by the analogy with the three main stages in the
formation and retrieval of memory in cognitive neuroscience which are (i)
practicing, (ii) storing and (iii) retrieval.
  In this study, the technique is specialized with two variants one for
read-only integer keys and the other for modifiable integers. Hence, a novel
algorithm is obtained that does not require additional memory other than a
constant amount and sorts faster than all no matter how large is the list
provided that m = O (n logn) where m is the range and n is the number of keys
(or integers).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1943</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1943</id><created>2012-09-10</created><authors><author><keyname>Cantone</keyname><forenames>Domenico</forenames></author><author><keyname>Asmundo</keyname><forenames>Marianna Nicolosi</forenames></author></authors><title>On the satisfiability problem for a 4-level quantified syllogistic and
  some applications to modal logic (extended version)</title><categories>cs.LO</categories><comments>Extended version of a paper to be published in Fundamenta
  Informaticae</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a multi-sorted stratified syllogistic, called 4LQSR, admitting
variables of four sorts and a restricted form of quantification over variables
of the first three sorts, and prove that it has a solvable satisfiability
problem by showing that it enjoys a small model property. Then, we consider the
fragments (4LQSR)^h of 4LQSR, consisting of 4LQSR-formulae whose quantifier
prefixes have length bounded by h &gt; 1 and satisfying certain syntactic
constraints, and prove that each of them has an NP-complete satisfiability
problem. Finally we show that the modal logic K45 can be expressed in 4LQSR^3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1949</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1949</id><created>2012-09-10</created><authors><author><keyname>Elahian</keyname><forenames>Atefeh</forenames></author><author><keyname>Khalili</keyname><forenames>Mehdi</forenames></author><author><keyname>Shokouhi</keyname><forenames>Shahriar Baradaran</forenames></author></authors><title>Improved Robust DWT-Watermarking in YCbCr Color Space</title><categories>cs.CR cs.MM cs.SE</categories><comments>5 Pages, 4 Figures, 3 Tables</comments><msc-class>68U10, 68U20, 65C20, 94A08, 94A24, 94A60, 11T71, 14G50, 68P25, 81P94</msc-class><acm-class>D.4.6; K.6.5; K.4.2</acm-class><journal-ref>Global journal of Computer Application and Technology (GJCAT),
  Vol.1, No.3, 2011, Pages 300-304</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital watermarking is an effective way to protect copyright. In this paper,
a robust watermarking algorithm based on wavelet transformation is proposed
which can confirm the copyright without original image. The wavelet
transformation technique is effective in image analyzing and processing. Thus
the color-image watermark algorithm based on discrete wavelet transformation
(DWT) begins to draw an increasing attention. In the proposed approach, the
watermark Encrypt by Arnold transform and the host image is converted into the
YCbCr color space. Then its Y channel decomposed into wavelet coefficients and
the selected approximation coefficients are quantized and then their least
significant bit of the quantized coefficients is replaced by the Encrypted
watermark using LSB insertion technique. The experimental results show that
watermark embedded by this algorithm is of better robustness and extra
imperceptibility and robustness against wavelet compression compared to the
traditional embedding methods in RGB color space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1960</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1960</id><created>2012-09-10</created><authors><author><keyname>Celebi</keyname><forenames>M. Emre</forenames></author><author><keyname>Kingravi</keyname><forenames>Hassan A.</forenames></author><author><keyname>Vela</keyname><forenames>Patricio A.</forenames></author></authors><title>A Comparative Study of Efficient Initialization Methods for the K-Means
  Clustering Algorithm</title><categories>cs.LG cs.CV</categories><comments>17 pages, 1 figure, 7 tables</comments><acm-class>I.5.3; H.2.8</acm-class><journal-ref>Expert Systems with Applications 40 (2013) 200-210</journal-ref><doi>10.1016/j.eswa.2012.07.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K-means is undoubtedly the most widely used partitional clustering algorithm.
Unfortunately, due to its gradient descent nature, this algorithm is highly
sensitive to the initial placement of the cluster centers. Numerous
initialization methods have been proposed to address this problem. In this
paper, we first present an overview of these methods with an emphasis on their
computational efficiency. We then compare eight commonly used linear time
complexity initialization methods on a large and diverse collection of data
sets using various performance criteria. Finally, we analyze the experimental
results using non-parametric statistical tests and provide recommendations for
practitioners. We demonstrate that popular initialization methods often perform
poorly and that there are in fact strong alternatives to these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1977</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1977</id><created>2012-09-10</created><authors><author><keyname>B&#xf6;cker</keyname><forenames>Sebastian</forenames></author></authors><title>Ten times eighteen</title><categories>cs.DM cs.DS</categories><comments>10 pages</comments><acm-class>G.2.1; F.2.2; K.8.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following simple game: We are given a table with ten slots
indexed one to ten. In each of the ten rounds of the game, three dice are
rolled and the numbers are added. We then put this number into any free slot.
For each slot, we multiply the slot index with the number in this slot, and add
up the products. The goal of the game is to maximize this score. In more
detail, we play the game many times, and try to maximize the sum of scores or,
equivalently, the expected score. We present a strategy to optimally play this
game with respect to the expected score. We then modify our strategy so that we
need only polynomial time and space. Finally, we show that knowing all ten
rolls in advance, results in a relatively small increase in score. Although the
game has a random component and requires a non-trivial strategy to be solved
optimally, this strategy needs only polynomial time and space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.1983</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.1983</id><created>2012-09-10</created><authors><author><keyname>Meyer</keyname><forenames>Frank</forenames></author><author><keyname>Fessant</keyname><forenames>Fran&#xe7;oise</forenames></author><author><keyname>Cl&#xe9;rot</keyname><forenames>Fabrice</forenames></author><author><keyname>Gaussier</keyname><forenames>Eric</forenames></author></authors><title>Toward a New Protocol to Evaluate Recommender Systems</title><categories>cs.IR cs.PF</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1203.4487</comments><acm-class>H.3.3; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approach to analyze the performance and the
added value of automatic recommender systems in an industrial context. We show
that recommender systems are multifaceted and can be organized around 4
structuring functions: help users to decide, help users to compare, help users
to discover, help users to explore. A global off line protocol is then proposed
to evaluate recommender systems. This protocol is based on the definition of
appropriate evaluation measures for each aforementioned function. The
evaluation protocol is discussed from the perspective of the usefulness and
trust of the recommendation. A new measure called Average Measure of Impact is
introduced. This measure evaluates the impact of the personalized
recommendation. We experiment with two classical methods, K-Nearest Neighbors
(KNN) and Matrix Factorization (MF), using the well known dataset: Netflix. A
segmentation of both users and items is proposed to finely analyze where the
algorithms perform well or badly. We show that the performance is strongly
dependent on the segments and that there is no clear correlation between the
RMSE and the quality of the recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2012</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2012</id><created>2012-09-10</created><authors><author><keyname>van Staden</keyname><forenames>Stephan</forenames></author></authors><title>A Framework for Concurrent Imperative Programming</title><categories>cs.PL cs.LO</categories><comments>20 pages</comments><acm-class>D.3.1; F.3.2; F.4.3; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proposed framework provides a general model of concurrent imperative
programming. Programs are modeled as formal languages and concurrency as an
interleaving (or shuffle) operator. This yields a simple and elegant algebra of
programs. The framework supports the views program logic by Dinsdale-Young and
others, which generalizes various type systems and separation logic approaches
to program correctness. It also validates familiar operational calculi in
small-step and big-step flavours. The consistency of the program logic with
respect to the operational rules is established directly and does not use
induction on derivations. In fact the whole framework uses only straightforward
mathematics. Parametric in states, views and basic commands, it can be
instantiated to a variety of concrete languages and settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2035</identifier>
 <datestamp>2013-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2035</id><created>2012-09-10</created><updated>2013-01-10</updated><authors><author><keyname>Perrin</keyname><forenames>Dominique</forenames></author></authors><title>Completely reducible sets</title><categories>cs.FL math.CO</categories><journal-ref>International Journal of Algebra and Computation, 23 (4), 915-942
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the family of rational sets of words, called completely reducible
and which are such that the syntactic representation of their characteristic
series is completely reducible. This family contains, by a result of
Reutenauer, the submonoids generated by bifix codes and, by a result of Berstel
and Reutenauer, the cyclic sets. We study the closure properties of this
family. We prove a result on linear representations of monoids which gives a
generalization of the result concerning the complete reducibility of the
submonoid generated by a bifix code to sets called birecurrent. We also give a
new proof of the result concerning cyclic sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2038</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2038</id><created>2012-09-10</created><authors><author><keyname>Chan</keyname><forenames>Yao-ban</forenames></author><author><keyname>Marckert</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Selig</keyname><forenames>Thomas</forenames></author></authors><title>A natural stochastic extension of the sandpile model on a graph</title><categories>math.CO cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new model of a stochastic sandpile on a graph $G$ containing a
sink. When unstable, a site sends one grain to each of its neighbours
independently with probability $p \in (0,1]$. For $p=1$, this coincides with
the standard Abelian sandpile model. In general, for $p\in(0,1)$, the set of
recurrent configurations of this sandpile model is different from that of the
Abelian sandpile model. We give a characterisation of this set in terms of
orientations of the graph $G$. We also define the lacking polynomial $L_G$ as
the generating function counting this set according to the number of grains,
and show that this polynomial satisfies a recurrence which resembles that of
the Tutte polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2039</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2039</id><created>2012-09-06</created><updated>2013-03-14</updated><authors><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames><affiliation>LTCI</affiliation></author><author><keyname>Vu</keyname><forenames>Thanh-Tung</forenames><affiliation>LTCI</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>LTCI</affiliation></author></authors><title>Modeling energy consumption in cellular networks</title><categories>cs.NI math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new analysis of energy consumption in cellular
networks. We focus on the distribution of energy consumed by a base station for
one isolated cell. We first define the energy consumption model in which the
consumed energy is divided into two parts: The additive part and the broadcast
part. The broadcast part is the part of energy which is oblivious of the number
of mobile stations but depends on the farthest terminal, for instance, the
energy effort necessary to maintain the beacon signal. The additive part is due
to the communication power which depends on both the positions, mobility and
activity of all the users. We evaluate by closed form expressions the mean and
variance of the consumed energy. Our analytic evaluation is based on the
hypothesis that mobiles are distributed according to a Poisson point process.
We show that the two parts of energy are of the same order of magnitude and
that substantial gain can be obtained by power control. We then consider the
impact of mobility on the energy consumption. We apply our model to two case
studies: The first one is to optimize the cell radius from the energetic point
of view, the second one is to dimension the battery of a base station in sites
that do not have access to permanent power supply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2058</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2058</id><created>2012-09-10</created><updated>2012-10-11</updated><authors><author><keyname>Johnson</keyname><forenames>Taylor T.</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author></authors><title>Safe and Stabilizing Distributed Multi-Path Cellular Flows</title><categories>cs.RO cs.DC cs.MA cs.SY</categories><comments>An earlier version of this paper appeared in the 30th IEEE
  International Conference on Distributed Computing Systems (ICDCS 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of distributed traffic control in the partitioned plane,
where the movement of all entities (robots, vehicles, etc.) within each
partition (cell) is coupled. Establishing liveness in such systems is
challenging, but such analysis will be necessary to apply such distributed
traffic control algorithms in applications like coordinating robot swarms and
the intelligent highway system. We present a formal model of a distributed
traffic control protocol that guarantees minimum separation between entities,
even as some cells fail. Once new failures cease occurring, in the case of a
single target, the protocol is guaranteed to self-stabilize and the entities
with feasible paths to the target cell make progress towards it. For multiple
targets, failures may cause deadlocks in the system, so we identify a class of
non-deadlocking failures where all entities are able to make progress to their
respective targets. The algorithm relies on two general principles: temporary
blocking for maintenance of safety and local geographical routing for
guaranteeing progress. Our assertional proofs may serve as a template for the
analysis of other distributed traffic control protocols. We present simulation
results that provide estimates of throughput as a function of entity velocity,
safety separation, single-target path complexity, failure-recovery rates, and
multi-target path complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2066</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2066</id><created>2012-09-10</created><updated>2014-11-16</updated><authors><author><keyname>Reani</keyname><forenames>Avraham</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Data Processing Bounds for Scalar Lossy Source Codes with Side
  Information at the Decoder</title><categories>cs.IT math.IT</categories><comments>35 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce new lower bounds on the distortion of scalar
fixed-rate codes for lossy compression with side information available at the
receiver. These bounds are derived by presenting the relevant random variables
as a Markov chain and applying generalized data processing inequalities a la
Ziv and Zakai. We show that by replacing the logarithmic function with other
functions, in the data processing theorem we formulate, we obtain new lower
bounds on the distortion of scalar coding with side information at the decoder.
The usefulness of these results is demonstrated for uniform sources and the
convex function $Q(t)=t^{1-\alpha}$, $\alpha&gt;1$. The bounds in this case are
shown to be better than one can obtain from the Wyner-Ziv rate-distortion
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2067</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2067</id><created>2012-09-10</created><updated>2013-11-23</updated><authors><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author><author><keyname>Bovik</keyname><forenames>Alan C.</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author></authors><title>A Markov Decision Model for Adaptive Scheduling of Stored Scalable
  Videos</title><categories>cs.MM</categories><comments>14 pages</comments><journal-ref>IEEE Transactions on Circuits and Systems for Video Technology,
  vol.23, no.6, pp.1081-1095, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two scheduling algorithms that seek to optimize the quality of
scalably coded videos that have been stored at a video server before
transmission.} The first scheduling algorithm is derived from a Markov Decision
Process (MDP) formulation developed here. We model the dynamics of the channel
as a Markov chain and reduce the problem of dynamic video scheduling to a
tractable Markov decision problem over a finite state space. Based on the MDP
formulation, a near-optimal scheduling policy is computed that minimize the
mean square error. Using insights taken from the development of the optimal
MDP-based scheduling policy, the second proposed scheduling algorithm is an
online scheduling method that only requires easily measurable knowledge of the
channel dynamics, and is thus viable in practice. Simulation results show that
the performance of both scheduling algorithms is close to a performance upper
bound also derived in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2070</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2070</id><created>2012-08-15</created><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Content-based Multi-media Retrieval Technology</title><categories>cs.MM cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a summary of the content-based Image Retrieval and
Content-based Audio Retrieval, which are two parts of the Content-based
Retrieval. Content-based Retrieval is the retrieval based on the features of
the content. Generally, it is a way to extract features of the media data and
find other data with the similar features from the database automatically.
Content-based Retrieval can not only work on discrete media like texts, but
also can be used on continuous media, such as video and audio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2079</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2079</id><created>2012-09-10</created><updated>2013-02-05</updated><authors><author><keyname>&#x15e;afak</keyname><forenames>Ilg&#x131;n</forenames></author><author><keyname>Akta&#x15f;</keyname><forenames>Emre</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>Ali &#xd6;zg&#xfc;r</forenames></author></authors><title>Error Rate Analysis of GF(q) Network Coded Detect-and-Forward Wireless
  Relay Networks Using Equivalent Relay Channel Models</title><categories>cs.IT math.IT</categories><comments>28 pages, 10 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</comments><doi>10.1109/TWC.2013.051613.121309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates simple means of analyzing the error rate performance
of a general q-ary Galois Field network coded detect-and-forward cooperative
relay network with known relay error statistics at the destination. Equivalent
relay channels are used in obtaining an approximate error rate of the relay
network, from which the diversity order is found. Error rate analyses using
equivalent relay channel models are shown to be closely matched with simulation
results. Using the equivalent relay channels, low complexity receivers are
developed whose performances are close to that of the optimal maximum
likelihood receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2082</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2082</id><created>2012-09-10</created><updated>2014-04-22</updated><authors><author><keyname>Liu</keyname><forenames>Guangcan</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>Blind Image Deblurring by Spectral Properties of Convolution Operators</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of recovering a sharp version of a given
blurry image when the blur kernel is unknown. Previous methods often introduce
an image-independent regularizer (such as Gaussian or sparse priors) on the
desired blur kernel. We shall show that the blurry image itself encodes rich
information about the blur kernel. Such information can be found through
analyzing and comparing how the spectrum of an image as a convolution operator
changes before and after blurring. Our analysis leads to an effective convex
regularizer on the blur kernel which depends only on the given blurry image. We
show that the minimizer of this regularizer guarantees to give good
approximation to the blur kernel if the original image is sharp enough. By
combining this powerful regularizer with conventional image deblurring
techniques, we show how we could significantly improve the deblurring results
through simulations and experiments on real images. In addition, our analysis
and experiments help explaining a widely accepted doctrine; that is, the edges
are good features for deblurring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2086</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2086</id><created>2012-09-10</created><updated>2012-10-19</updated><authors><author><keyname>Hu</keyname><forenames>Donglin</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author></authors><title>On Cooperative Relay Networks with Video Applications</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of cooperative relay in CR networks
for further enhanced network performance. In particular, we focus on the two
representative cooperative relay strategies, and develop optimal spectrum
sensing and $p$-Persistent CSMA for spectrum access. Then, we study the problem
of cooperative relay in CR networks for video streaming. We incorporate
interference alignment to allow transmitters collaboratively send encoded
signals to all CR users. In the cases of a single licensed channel and multiple
licensed channels with channel bonding, we develop an optimal distributed
algorithm with proven convergence and convergence speed. In the case of
multiple channels without channel bonding, we develop a greedy algorithm with
bounded performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2088</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2088</id><created>2012-09-10</created><updated>2012-09-11</updated><authors><author><keyname>Horn</keyname><forenames>Paul</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Spreading Processes and Large Components in Ordered, Directed Random
  Graphs</title><categories>math.CO cs.DM cs.SI</categories><comments>Working paper, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Order the vertices of a directed random graph \math{v_1,...,v_n}; edge
\math{(v_i,v_j)} for \math{i&lt;j} exists independently with probability \math{p}.
This random graph model is related to certain spreading processes on networks.
We consider the component reachable from \math{v_1} and prove existence of a
sharp threshold \math{p^*=\log n/n} at which this reachable component
transitions from \math{o(n)} to \math{\Omega(n)}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2097</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2097</id><created>2012-09-07</created><authors><author><keyname>Kutz</keyname><forenames>Nadja</forenames></author></authors><title>Semantic web applications with regard to math and environment</title><categories>cs.DL cs.IR</categories><comments>17 pages, 2 figures</comments><msc-class>00A-xx, 06-xx, 90C-xx, 65K05, 94A-xx, 94C-xx, 97Bxx</msc-class><acm-class>D.2.6; D.2.12; D.3; E.m; G.m; H.3; H.5; J.2; J.6; K.3.m; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following is an outline of possible strategies in using semantic web
techniques and math with regard to environmental issues. The article uses
concrete examples and applications and provides partially a rather basic
treatment of semantic web techniques and math in order to adress a broader
audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2124</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2124</id><created>2012-09-10</created><authors><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author></authors><title>A measure of total research impact independent of time and discipline</title><categories>astro-ph.IM cs.DL physics.soc-ph</categories><comments>14 pages, 5 figures. PLoS ONE, in press</comments><doi>10.1371/journal.pone.0046428</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authorship and citation practices evolve with time and differ by academic
discipline. As such, indicators of research productivity based on citation
records are naturally subject to historical and disciplinary effects. We
observe these effects on a corpus of astronomer career data constructed from a
database of refereed publications. We employ a simple mechanism to measure
research output using author and reference counts available in bibliographic
databases to develop a citation-based indicator of research productivity. The
total research impact (tori) quantifies, for an individual, the total amount of
scholarly work that others have devoted to his/her work, measured in the volume
of research papers. A derived measure, the research impact quotient (riq), is
an age independent measure of an individual's research ability. We demonstrate
that these measures are substantially less vulnerable to temporal debasement
and cross-disciplinary bias than the most popular current measures. The
proposed measures of research impact, tori and riq, have been implemented in
the Smithsonian/NASA Astrophysics Data System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2131</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2131</id><created>2012-09-10</created><authors><author><keyname>Abhishek</keyname><forenames>Vineet</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>On the Incentive to Deviate in Core Selecting Combinatorial Auctions</title><categories>cs.GT</categories><comments>8 pages, 2 figures. This work was presented at the Workshop on
  Telecom Economics, Engineering and Policy 2012, Krakow, Poland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent spectrum auctions in the United Kingdom, and some proposals for future
auctions of spectrum in the United States, are based on preliminary price
discovery rounds, followed by calculation of final prices for the winning
buyers. For example, the prices could be the projection of Vikrey prices onto
the core of reported prices. The use of Vikrey prices should lead to more
straightforward bidding, but the projection reverses some of the incentive for
bidders to report truthfully. Still, we conjecture that the price paid by a
winning buyer increases no faster than the bid, as in a first price auction. It
would be rather disturbing if the conjecture is false. The conjecture is
established for a buyer interacting with disjoint groups of other buyers in a
star network setting. It is also shown that for any core-selecting payment rule
and any integer w greater than or equal to two, there is a market setting with
w winning buyers such that the price paid by some winning buyer increases at
least (1-1/w) times as fast as the price bid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2137</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2137</id><created>2012-09-10</created><updated>2014-05-15</updated><authors><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author><author><keyname>Boytsov</keyname><forenames>Leonid</forenames></author></authors><title>Decoding billions of integers per second through vectorization</title><categories>cs.IR cs.DB</categories><comments>For software, see https://github.com/lemire/FastPFor, For data, see
  http://boytsov.info/datasets/clueweb09gap/</comments><journal-ref>Software: Practice &amp; Experience 45 (1), 2015</journal-ref><doi>10.1002/spe.2203</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In many important applications -- such as search engines and relational
database systems -- data is stored in the form of arrays of integers. Encoding
and, most importantly, decoding of these arrays consumes considerable CPU time.
Therefore, substantial effort has been made to reduce costs associated with
compression and decompression. In particular, researchers have exploited the
superscalar nature of modern processors and SIMD instructions. Nevertheless, we
introduce a novel vectorized scheme called SIMD-BP128 that improves over
previously proposed vectorized approaches. It is nearly twice as fast as the
previously fastest schemes on desktop processors (varint-G8IU and PFOR). At the
same time, SIMD-BP128 saves up to 2 bits per integer. For even better
compression, we propose another new vectorized scheme (SIMD-FastPFOR) that has
a compression ratio within 10% of a state-of-the-art scheme (Simple-8b) while
being two times faster during decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2138</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2138</id><created>2012-09-10</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Jald&#xe9;n</keyname><forenames>Niklas</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Optimality Properties, Distributed Strategies, and Measurement-Based
  Evaluation of Coordinated Multicell OFDMA Transmission</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Transactions on Signal Processing, 15 pages, 7
  figures. This version corrects typos related to Eq. (4) and Eq. (28)</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 59, no. 12, pp.
  6086-6101, December 2011</journal-ref><doi>10.1109/TSP.2011.2165706</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The throughput of multicell systems is inherently limited by interference and
the available communication resources. Coordinated resource allocation is the
key to efficient performance, but the demand on backhaul signaling and
computational resources grows rapidly with number of cells, terminals, and
subcarriers. To handle this, we propose a novel multicell framework with
dynamic cooperation clusters where each terminal is jointly served by a small
set of base stations. Each base station coordinates interference to neighboring
terminals only, thus limiting backhaul signalling and making the framework
scalable. This framework can describe anything from interference channels to
ideal joint multicell transmission.
  The resource allocation (i.e., precoding and scheduling) is formulated as an
optimization problem (P1) with performance described by arbitrary monotonic
functions of the signal-to-interference-and-noise ratios (SINRs) and arbitrary
linear power constraints. Although (P1) is non-convex and difficult to solve
optimally, we are able to prove: 1) Optimality of single-stream beamforming; 2)
Conditions for full power usage; and 3) A precoding parametrization based on a
few parameters between zero and one. These optimality properties are used to
propose low-complexity strategies: both a centralized scheme and a distributed
version that only requires local channel knowledge and processing. We evaluate
the performance on measured multicell channels and observe that the proposed
strategies achieve close-to-optimal performance among centralized and
distributed solutions, respectively. In addition, we show that multicell
interference coordination can give substantial improvements in sum performance,
but that joint transmission is very sensitive to synchronization errors and
that some terminals can experience performance degradations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2139</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2139</id><created>2012-09-10</created><updated>2013-12-30</updated><authors><author><keyname>Yang</keyname><forenames>Sen</forenames></author><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author><author><keyname>Shen</keyname><forenames>Xiaotong</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Fused Multiple Graphical Lasso</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of estimating multiple graphical
models simultaneously using the fused lasso penalty, which encourages adjacent
graphs to share similar structures. A motivating example is the analysis of
brain networks of Alzheimer's disease using neuroimaging data. Specifically, we
may wish to estimate a brain network for the normal controls (NC), a brain
network for the patients with mild cognitive impairment (MCI), and a brain
network for Alzheimer's patients (AD). We expect the two brain networks for NC
and MCI to share common structures but not to be identical to each other;
similarly for the two brain networks for MCI and AD. The proposed formulation
can be solved using a second-order method. Our key technical contribution is to
establish the necessary and sufficient condition for the graphs to be
decomposable. Based on this key property, a simple screening rule is presented,
which decomposes the large graphs into small subgraphs and allows an efficient
estimation of multiple independent (small) subgraphs, dramatically reducing the
computational cost. We perform experiments on both synthetic and real data; our
results demonstrate the effectiveness and efficiency of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2154</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2154</id><created>2012-09-10</created><authors><author><keyname>Pelechrinis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Prashant</forenames></author><author><keyname>Weiss</keyname><forenames>Martin</forenames></author><author><keyname>Znati</keyname><forenames>Taied</forenames></author></authors><title>Cognitive Radio Networks: Realistic or Not?</title><categories>cs.NI</categories><comments>Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large volume of research has been conducted in the cognitive radio (CR)
area the last decade. However, the deployment of a commercial CR network is yet
to emerge. A large portion of the existing literature does not build on real
world scenarios, hence, neglecting various important interactions of the
research with commercial telecommunication networks. For instance, a lot of
attention has been paid to spectrum sensing as the front line functionality
that needs to be completed in an efficient and accurate manner to enable an
opportunistic CR network architecture. This is necessary to detect the
existence of spectrum holes without which no other procedure can be fulfilled.
However, simply sensing (cooperatively or not) the energy received from a
primary transmitter cannot enable correct dynamic spectrum access. For example,
the low strength of a primary transmitter's signal does not assure that there
will be no interference to a nearby primary receiver. In addition, the presence
of a primary transmitter's signal does not mean that CR network users cannot
access the spectrum since there might not be any primary receiver in the
vicinity. Despite the existing elegant and clever solutions to the DSA problem
no robust, implementable scheme has emerged. In this paper, we challenge the
basic premises of the proposed schemes. We further argue that addressing the
technical challenges we face in deploying robust CR networks can only be
achieved if we radically change the way we design their basic functionalities.
In support of our argument, we present a set of real-world scenarios, inspired
by realistic settings in commercial telecommunications networks, focusing on
spectrum sensing as a basic and critical functionality in the deployment of
CRs. We use these scenarios to show why existing DSA paradigms are not amenable
to realistic deployment in complex wireless environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2163</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2163</id><created>2012-09-10</created><authors><author><keyname>Delano&#xeb;</keyname><forenames>Alexandre</forenames></author><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>Modeling controversies in the press: the case of the abnormal bees'
  death</title><categories>physics.soc-ph cs.CL</categories><comments>22 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The controversy about the cause(s) of abnormal death of bee colonies in
France is investigated through an extensive analysis of the french speaking
press. A statistical analysis of textual data is first performed on the lexicon
used by journalists to describe the facts and to present associated
informations during the period 1998-2010. Three states are identified to
explain the phenomenon. The first state asserts a unique cause, the second one
focuses on multifactor causes and the third one states the absence of current
proof. Assigning each article to one of the three states, we are able to follow
the associated opinion dynamics among the journalists over 13 years. Then, we
apply the Galam sequential probabilistic model of opinion dynamic to those
data. Assuming journalists are either open mind or inflexible about their
respective opinions, the results are reproduced precisely provided we account
for a series of annual changes in the proportions of respective inflexibles.
The results shed a new counter intuitive light on the various pressure supposed
to apply on the journalists by either chemical industries or beekeepers and
experts or politicians. The obtained dynamics of respective inflexibles shows
the possible effect of lobbying, the inertia of the debate and the net
advantage gained by the first whistleblowers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2166</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2166</id><created>2012-09-10</created><updated>2012-12-10</updated><authors><author><keyname>Pritchard</keyname><forenames>David</forenames></author><author><keyname>Vasiga</keyname><forenames>Troy</forenames></author></authors><title>CS Circles: An In-Browser Python Course for Beginners</title><categories>cs.CY cs.HC</categories><comments>To appear in SIGCSE 2013</comments><acm-class>K.3.1; K.3.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computer Science Circles is a free programming website for beginners that is
designed to be fun, easy to use, and accessible to the broadest possible
audience. We teach Python since it is simple yet powerful, and the course
content is well-structured but written in plain language. The website has over
one hundred exercises in thirty lesson pages, plus special features to help
teachers support their students. It is available in both English and French. We
discuss the philosophy behind the course and its design, we describe how it was
implemented, and we give statistics on its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2171</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2171</id><created>2012-09-10</created><authors><author><keyname>Ahn</keyname><forenames>Hee-Kap</forenames></author><author><keyname>Bae</keyname><forenames>Sang Won</forenames></author><author><keyname>Cheong</keyname><forenames>Otfried</forenames></author><author><keyname>Gudmundsson</keyname><forenames>Joachim</forenames></author><author><keyname>Tokuyama</keyname><forenames>Takeshi</forenames></author><author><keyname>Vigneron</keyname><forenames>Antoine</forenames></author></authors><title>A Generalization of the Convex Kakeya Problem</title><categories>cs.CG</categories><comments>14 pages, 9 figures</comments><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of line segments in the plane, not necessarily finite, what is a
convex region of smallest area that contains a translate of each input segment?
This question can be seen as a generalization of Kakeya's problem of finding a
convex region of smallest area such that a needle can be rotated through 360
degrees within this region. We show that there is always an optimal region that
is a triangle, and we give an optimal \Theta(n log n)-time algorithm to compute
such a triangle for a given set of n segments. We also show that, if the goal
is to minimize the perimeter of the region instead of its area, then placing
the segments with their midpoint at the origin and taking their convex hull
results in an optimal solution. Finally, we show that for any compact convex
figure G, the smallest enclosing disk of G is a smallest-perimeter region
containing a translate of every rotated copy of G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2177</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2177</id><created>2012-09-10</created><authors><author><keyname>Harris</keyname><forenames>Kameron Decker</forenames></author></authors><title>On-off Threshold Models of Social Contagion</title><categories>physics.soc-ph cs.SI</categories><comments>Master's Thesis, mathematics, University of Vermont</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study binary state contagion dynamics on a social network where nodes act
in response to the average state of their neighborhood. We model the competing
tendencies of imitation and non-conformity by incorporating an off-threshold
into standard threshold models of behavior. In this way, we attempt to capture
important aspects of fashions and general societal trends. Allowing varying
amounts of stochasticity in both the network and node responses, we find
different outcomes in the random and deterministic versions of the model. In
the limit of a large, dense network, however, we show that these dynamics
coincide. The dynamical behavior of the system ranges from steady state to
chaotic depending on network connectivity and update synchronicity. We
construct a mean field theory for general random networks. In the undirected
case, the mean field theory predicts that the dynamics on the network are a
smoothed version of the average node response dynamics. We compare our theory
to extensive simulations on Poisson random graphs with node responses that
average to the chaotic tent map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2178</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2178</id><created>2012-09-10</created><updated>2013-03-08</updated><authors><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Holder</keyname><forenames>Lawrence B.</forenames></author><author><keyname>Ray</keyname><forenames>Abhik</forenames></author><author><keyname>Chin</keyname><forenames>George</forenames><suffix>Jr.</suffix></author><author><keyname>Feo</keyname><forenames>John T.</forenames></author></authors><title>Continuous Queries for Multi-Relational Graphs</title><categories>cs.DB cs.SI</categories><comments>Withdrawn because for information disclosure considerations</comments><report-no>PNNL-SA-90326</report-no><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Acting on time-critical events by processing ever growing social media or
news streams is a major technical challenge. Many of these data sources can be
modeled as multi-relational graphs. Continuous queries or techniques to search
for rare events that typically arise in monitoring applications have been
studied extensively for relational databases. This work is dedicated to answer
the question that emerges naturally: how can we efficiently execute a
continuous query on a dynamic graph? This paper presents an exact subgraph
search algorithm that exploits the temporal characteristics of representative
queries for online news or social media monitoring. The algorithm is based on a
novel data structure called the Subgraph Join Tree (SJ-Tree) that leverages the
structural and semantic characteristics of the underlying multi-relational
graph. The paper concludes with extensive experimentation on several real-world
datasets that demonstrates the validity of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2179</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2179</id><created>2012-09-10</created><authors><author><keyname>Xu</keyname><forenames>Mingguang</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Downlink Noncoherent Cooperation without Transmitter Phase Alignment</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicell joint processing can mitigate inter-cell interference and thereby
increase the spectral efficiency of cellular systems. Most previous work has
assumed phase-aligned (coherent) transmissions from different base transceiver
stations (BTSs), which is difficult to achieve in practice. In this work, a
noncoherent cooperative transmission scheme for the downlink is studied, which
does not require phase alignment. The focus is on jointly serving two users in
adjacent cells sharing the same resource block. The two BTSs partially share
their messages through a backhaul link, and each BTS transmits a superposition
of two codewords, one for each receiver. Each receiver decodes its own message,
and treats the signals for the other receiver as background noise. With
narrowband transmissions the achievable rate region and maximum achievable
weighted sum rate are characterized by optimizing the power allocation (and the
beamforming vectors in the case of multiple transmit antennas) at each BTS
between its two codewords. For a wideband (multicarrier) system, a dual
formulation of the optimal power allocation problem across sub-carriers is
presented, which can be efficiently solved by numerical methods. Results show
that the proposed cooperation scheme can improve the sum rate substantially in
the low to moderate signal-to-noise ratio (SNR) range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2184</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2184</id><created>2012-09-10</created><authors><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Demmel</keyname><forenames>James</forenames></author><author><keyname>Holtz</keyname><forenames>Olga</forenames></author><author><keyname>Lipshitz</keyname><forenames>Benjamin</forenames></author><author><keyname>Schwartz</keyname><forenames>Oded</forenames></author></authors><title>Graph Expansion Analysis for Communication Costs of Fast Rectangular
  Matrix Multiplication</title><categories>cs.DS cs.CC cs.NA</categories><journal-ref>Design and Analysis of Algorithms Volume 7659, 2012, pp 13-36</journal-ref><doi>10.1007/978-3-642-34862-4_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph expansion analysis of computational DAGs is useful for obtaining
communication cost lower bounds where previous methods, such as geometric
embedding, are not applicable. This has recently been demonstrated for
Strassen's and Strassen-like fast square matrix multiplication algorithms. Here
we extend the expansion analysis approach to fast algorithms for rectangular
matrix multiplication, obtaining a new class of communication cost lower
bounds. These apply, for example to the algorithms of Bini et al. (1979) and
the algorithms of Hopcroft and Kerr (1971). Some of our bounds are proved to be
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2185</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2185</id><created>2012-09-10</created><updated>2013-05-01</updated><authors><author><keyname>Avron</keyname><forenames>Haim</forenames></author><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Toledo</keyname><forenames>Sivan</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>Efficient Dimensionality Reduction for Canonical Correlation Analysis</title><categories>cs.DS math.NA</categories><comments>22 pages. 4 figures. To appear in ICML 2013: The 30th International
  Conference on Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast algorithm for approximate Canonical Correlation Analysis
(CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first
employs a randomized dimensionality reduction transform to reduce the size of
the input matrices, and then applies any CCA algorithm to the new pair of
matrices. The algorithm computes an approximate CCA to the original pair of
matrices with provable guarantees, while requiring asymptotically less
operations than the state-of-the-art exact algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2189</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2189</id><created>2012-09-10</created><authors><author><keyname>Kamyabpour</keyname><forenames>Najmeh</forenames></author><author><keyname>Hoang</keyname><forenames>Doan B.</forenames></author></authors><title>Statistical Analysis to Extract Effective Parameters on Overall Energy
  Consumption of Wireless Sensor Network (WSN)</title><categories>cs.NI</categories><comments>5-pages. This paper has been accepted in PDCAT-2012 conference
  (http://www.pdcat2012.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use statistical tools to analysis dependency between
Wireless Sensor Network (WSN) parameters and overall Energy Consumption (EC).
Our approach has two main phases: profiling, and effective parameter
extraction. In former, a sensor network simulator is re-run 800 times with
different values for eight WSN parameters to profile consumed energy in nodes;
then in latter, three statistical analyses (p-value, linear and non-linear
correlation) are applied to the outcome of profiling phase to extract the most
effective parameters on WSN overall energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2191</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2191</id><created>2012-09-10</created><authors><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author></authors><title>MapReduce is Good Enough? If All You Have is a Hammer, Throw Away
  Everything That's Not a Nail!</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hadoop is currently the large-scale data analysis &quot;hammer&quot; of choice, but
there exist classes of algorithms that aren't &quot;nails&quot;, in the sense that they
are not particularly amenable to the MapReduce programming model. To address
this, researchers have proposed MapReduce extensions or alternative programming
models in which these algorithms can be elegantly expressed. This essay
espouses a very different position: that MapReduce is &quot;good enough&quot;, and that
instead of trying to invent screwdrivers, we should simply get rid of
everything that's not a nail. To be more specific, much discussion in the
literature surrounds the fact that iterative algorithms are a poor fit for
MapReduce: the simple solution is to find alternative non-iterative algorithms
that solve the same problem. This essay captures my personal experiences as an
academic researcher as well as a software engineer in a &quot;real-world&quot; production
analytics environment. From this combined perspective I reflect on the current
state and future of &quot;big data&quot; research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2192</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2192</id><created>2012-09-10</created><authors><author><keyname>Ahmed</keyname><forenames>Imtiaz</forenames></author><author><keyname>Ikhlef</keyname><forenames>Aissa</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Mallik</keyname><forenames>Ranjan K.</forenames></author></authors><title>Power Allocation for Conventional and Buffer-Aided Link Adaptive
  Relaying Systems with Energy Harvesting Nodes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) nodes can play an important role in cooperative
communication systems which do not have a continuous power supply. In this
paper, we consider the optimization of conventional and buffer-aided link
adaptive EH relaying systems, where an EH source communicates with the
destination via an EH decode-and-forward relay. In conventional relaying,
source and relay transmit signals in consecutive time slots whereas in
buffer-aided link adaptive relaying, the state of the source-relay and
relay-destination channels determines whether the source or the relay is
selected for transmission. Our objective is to maximize the system throughput
over a finite number of transmission time slots for both relaying protocols. In
case of conventional relaying, we propose an offline and several online joint
source and relay transmit power allocation schemes. For offline power
allocation, we formulate an optimization problem which can be solved optimally.
For the online case, we propose a dynamic programming (DP) approach to compute
the optimal online transmit power. To alleviate the complexity inherent to DP,
we also propose several suboptimal online power allocation schemes. For
buffer-aided link adaptive relaying, we show that the joint offline
optimization of the source and relay transmit powers along with the link
selection results in a mixed integer non-linear program which we solve
optimally using the spatial branch-and-bound method. We also propose an
efficient online power allocation scheme and a naive online power allocation
scheme for buffer-aided link adaptive relaying. Our results show that link
adaptive relaying provides performance improvement over conventional relaying
at the expense of a higher computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2194</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2194</id><created>2012-09-10</created><updated>2014-12-15</updated><authors><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Cooperative learning in multi-agent systems from intermittent
  measurements</title><categories>math.OC cs.LG cs.MA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of tracking a direction in a decentralized way, we
consider the general problem of cooperative learning in multi-agent systems
with time-varying connectivity and intermittent measurements. We propose a
distributed learning protocol capable of learning an unknown vector $\mu$ from
noisy measurements made independently by autonomous nodes. Our protocol is
completely distributed and able to cope with the time-varying, unpredictable,
and noisy nature of inter-agent communication, and intermittent noisy
measurements of $\mu$. Our main result bounds the learning speed of our
protocol in terms of the size and combinatorial features of the (time-varying)
networks connecting the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2204</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2204</id><created>2012-09-10</created><authors><author><keyname>Svetlova</keyname><forenames>Ekaterina</forenames><affiliation>Karlshochschule International University</affiliation></author><author><keyname>van Elst</keyname><forenames>Henk</forenames><affiliation>Karlshochschule International University</affiliation></author></authors><title>How is non-knowledge represented in economic theory?</title><categories>q-fin.GN cs.AI stat.AP</categories><comments>18 pages, LaTeX2e, hyperlinked references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we address the question of how non-knowledge about future
events that influence economic agents' decisions in choice settings has been
formally represented in economic theory up to date. To position our discussion
within the ongoing debate on uncertainty, we provide a brief review of
historical developments in economic theory and decision theory on the
description of economic agents' choice behaviour under conditions of
uncertainty, understood as either (i) ambiguity, or (ii) unawareness.
Accordingly, we identify and discuss two approaches to the formalisation of
non-knowledge: one based on decision-making in the context of a state space
representing the exogenous world, as in Savage's axiomatisation and some
successor concepts (ambiguity as situations with unknown probabilities), and
one based on decision-making over a set of menus of potential future
opportunities, providing the possibility of derivation of agents' subjective
state spaces (unawareness as situation with imperfect subjective knowledge of
all future events possible). We also discuss impeding challenges of the
formalisation of non-knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2218</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2218</id><created>2012-09-11</created><authors><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Mathew</keyname><forenames>Rogers</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author><author><keyname>Sharma</keyname><forenames>Roohani</forenames></author></authors><title>Product Dimension of Forests and Bounded Treewidth Graphs</title><categories>math.CO cs.DM</categories><comments>12 pages, 3 figures</comments><msc-class>05C05, 05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The product dimension of a graph G is defined as the minimum natural number l
such that G is an induced subgraph of a direct product of l complete graphs. In
this paper we study the product dimension of forests, bounded treewidth graphs
and k-degenerate graphs. We show that every forest on n vertices has a product
dimension at most 1.441logn+3. This improves the best known upper bound of
3logn for the same due to Poljak and Pultr. The technique used in arriving at
the above bound is extended and combined with a result on existence of
orthogonal Latin squares to show that every graph on n vertices with a
treewidth at most t has a product dimension at most (t+2)(logn+1). We also show
that every k-degenerate graph on n vertices has a product dimension at most
\ceil{8.317klogn}+1. This improves the upper bound of 32klogn for the same by
Eaton and Rodl.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2229</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2229</id><created>2012-09-11</created><updated>2014-04-22</updated><authors><author><keyname>Ilik</keyname><forenames>Danko</forenames></author><author><keyname>Nakata</keyname><forenames>Keiko</forenames></author></authors><title>A Direct Version of Veldman's Proof of Open Induction on Cantor Space
  via Delimited Control Operators</title><categories>math.LO cs.LO</categories><acm-class>F.4.1; F.3.3</acm-class><journal-ref>Leibniz International Proceedings in Informatics, 26, 2014</journal-ref><doi>10.4230/LIPIcs.TYPES.2013.188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First, we reconstruct Wim Veldman's result that Open Induction on Cantor
space can be derived from Double-negation Shift and Markov's Principle. In
doing this, we notice that one has to use a countable choice axiom in the proof
and that Markov's Principle is replaceable by slightly strengthening the
Double-negation Shift schema. We show that this strengthened version of
Double-negation Shift can nonetheless be derived in a constructive intermediate
logic based on delimited control operators, extended with axioms for
higher-type Heyting Arithmetic. We formalize the argument and thus obtain a
proof term that directly derives Open Induction on Cantor space by the shift
and reset delimited control operators of Danvy and Filinski.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2234</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2234</id><created>2012-09-11</created><authors><author><keyname>Nataf</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Festor</keyname><forenames>Olivier</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Online Estimation of Battery Lifetime for Wireless Sensors Network</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Battery is a major hardware component of wireless sensor networks. Most of
them have no power supply and are generally deployed for a long time.
Researches have been done on battery physical model and their adaptation for
sensors. We present an implementation on a real sensor operating system and how
architectural constraints have been assumed. Experiments have been made in
order to test the impact of some parameter, as the application throughput, on
the battery lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2237</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2237</id><created>2012-09-11</created><authors><author><keyname>D&#xed;az</keyname><forenames>Gregorio</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author><author><keyname>Llana</keyname><forenames>Luis</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author><author><keyname>Valero</keyname><forenames>Valent&#xed;n</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author><author><keyname>Mateo</keyname><forenames>Jose Antonio</forenames><affiliation>Universidad de Castilla-La Mancha</affiliation></author></authors><title>Conformance Verification of Normative Specifications using C-O Diagrams</title><categories>cs.FL cs.SE</categories><comments>In Proceedings FLACOS 2012, arXiv:1209.1699</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 94, 2012, pp. 1-10</journal-ref><doi>10.4204/EPTCS.94.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  C-O Diagrams have been introduced as a means to have a visual representation
of normative texts and electronic contracts, where it is possible to represent
the obligations, permissions and prohibitions of the different signatories, as
well as what are the penalties in case of not fulfillment of their obligations
and prohibitions. In such diagrams we are also able to represent absolute and
relative timing constrains.
  In this paper we consider a formal semantics for C-O Diagrams based on a
network of timed automata and we present several relations to check the
consistency of a contract in terms of realizability, to analyze whether an
implementation satisfies the requirements defined on its contract, and to
compare several implementations using the executed permissions as criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2238</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2238</id><created>2012-09-11</created><authors><author><keyname>Pace</keyname><forenames>Gordon J.</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Schapachnik</keyname><forenames>Fernando</forenames><affiliation>Universidad de Buenos Aires</affiliation></author></authors><title>Contracts for Interacting Two-Party Systems</title><categories>cs.LO</categories><comments>In Proceedings FLACOS 2012, arXiv:1209.1699</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 94, 2012, pp. 21-30</journal-ref><doi>10.4204/EPTCS.94.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the interrelation of deontic operators in contracts
-- an aspect often neglected when considering only one of the involved parties.
On top of an automata-based semantics we formalise the onuses that obligations,
permissions and prohibitions on one party impose on the other. Such
formalisation allows for a clean notion of contract strictness and a derived
notion of contract conflict that is enriched with issues arising from party
interdependence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2239</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2239</id><created>2012-09-11</created><authors><author><keyname>Zaharieva-Stojanovski</keyname><forenames>Marina</forenames></author><author><keyname>Huisman</keyname><forenames>Marieke</forenames></author><author><keyname>Blom</keyname><forenames>Stefan</forenames></author></authors><title>A History of BlockingQueues</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FLACOS 2012, arXiv:1209.1699</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1</acm-class><journal-ref>EPTCS 94, 2012, pp. 31-35</journal-ref><doi>10.4204/EPTCS.94.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a way to formally specify the behaviour of concurrent
data structures. When specifying concurrent data structures, the main challenge
is to make specifications stable, i.e., to ensure that they cannot be
invalidated by other threads. To this end, we propose to use history-based
specifications: instead of describing method behaviour in terms of the object's
state, we specify it in terms of the object's state history. A history is
defined as a list of state updates, which at all points can be related to the
actual object's state.
  We illustrate the approach on the BlockingQueue hierarchy from the
java.util.concurrent library. We show how the behaviour of the interface
BlockingQueue is specified, leaving a few decisions open to descendant classes.
The classes implementing the interface correctly inherit the specifications. As
a specification language, we use a combination of JML and permission-based
separation logic, including abstract predicates. This results in an abstract,
modular and natural way to specify the behaviour of concurrent queues. The
specifications can be used to derive high-level properties about queues, for
example to show that the order of elements is preserved. Moreover, the approach
can be easily adapted to other concurrent data structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2244</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2244</id><created>2012-09-11</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Visualizations in Exploratory Search: A User Study with Stock Market
  Information</title><categories>cs.HC</categories><comments>To be published in the Proceedings of the I-Know 2012 Conference,
  Special Track TAVA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an approach that integrates interactive
visualizations in the exploratory search process. In this model visualizations
can act as hubs where large amounts of information are made accessible in easy
user interfaces. Through interaction techniques this information can be
combined with related information on the World Wide Web. We applied the new
search concept to the domain of stock market information and conducted a user
study. Participants could use this interface without instructions, could
complete complex tasks like identifying related information items, link
heterogeneous information types and use different interaction techniques to
access related information more easily. In this way, users could quickly
acquire knowledge in an unfamiliar domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2262</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2262</id><created>2012-09-11</created><authors><author><keyname>Berg</keyname><forenames>Ewout van den</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Chinn</keyname><forenames>Garry</forenames></author><author><keyname>Levin</keyname><forenames>Craig</forenames></author><author><keyname>Olcott</keyname><forenames>Peter</forenames></author><author><keyname>Sing-Long</keyname><forenames>Carlos</forenames></author></authors><title>A single-photon sampling architecture for solid-state imaging</title><categories>cs.IT math.IT physics.ins-det</categories><comments>24 pages, 3 figures, 5 tables</comments><doi>10.1073/pnas.1216318110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in solid-state technology have enabled the development of silicon
photomultiplier sensor arrays capable of sensing individual photons. Combined
with high-frequency time-to-digital converters (TDCs), this technology opens up
the prospect of sensors capable of recording with high accuracy both the time
and location of each detected photon. Such a capability could lead to
significant improvements in imaging accuracy, especially for applications
operating with low photon fluxes such as LiDAR and positron emission
tomography.
  The demands placed on on-chip readout circuitry imposes stringent trade-offs
between fill factor and spatio-temporal resolution, causing many contemporary
designs to severely underutilize the technology's full potential. Concentrating
on the low photon flux setting, this paper leverages results from group testing
and proposes an architecture for a highly efficient readout of pixels using
only a small number of TDCs, thereby also reducing both cost and power
consumption. The design relies on a multiplexing technique based on binary
interconnection matrices. We provide optimized instances of these matrices for
various sensor parameters and give explicit upper and lower bounds on the
number of TDCs required to uniquely decode a given maximum number of
simultaneous photon arrivals.
  To illustrate the strength of the proposed architecture, we note a typical
digitization result of a 120x120 photodiode sensor on a 30um x 30um pitch with
a 40ps time resolution and an estimated fill factor of approximately 70%, using
only 161 TDCs. The design guarantees registration and unique recovery of up to
4 simultaneous photon arrivals using a fast decoding algorithm. In a series of
realistic simulations of scintillation events in clinical positron emission
tomography the design was able to recover the spatio-temporal location of 98.6%
of all photons that caused pixel firings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2274</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2274</id><created>2012-09-11</created><authors><author><keyname>Tavoli</keyname><forenames>Reza</forenames></author><author><keyname>Mahmoudi</keyname><forenames>Fariborz</forenames></author></authors><title>PCA-Based Relevance Feedback in Document Image Retrieval</title><categories>cs.IR</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research has been devoted in the past few years to relevance feedback as an
effective solution to improve performance of information retrieval systems.
Relevance feedback refers to an interactive process that helps to improve the
retrieval performance. In this paper we propose the use of relevance feedback
to improve document image retrieval System (DIRS) performance. This paper
compares a variety of strategies for positive and negative feedback. In
addition, feature subspace is extracted and updated during the feedback process
using a Principal Component Analysis (PCA) technique and based on user's
feedback. That is, in addition to reducing the dimensionality of feature
spaces, a proper subspace for each type of features is obtained in the feedback
process to further improve the retrieval accuracy. Experiments show that using
relevance Feedback in DIR achieves better performance than common DIR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2295</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2295</id><created>2012-09-11</created><updated>2012-09-12</updated><authors><author><keyname>Eynard</keyname><forenames>Davide</forenames></author><author><keyname>Glashoff</keyname><forenames>Klaus</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Bronstein</keyname><forenames>Alexander M.</forenames></author></authors><title>Multimodal diffusion geometry by joint diagonalization of Laplacians</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct an extension of diffusion geometry to multiple modalities
through joint approximate diagonalization of Laplacian matrices. This naturally
extends classical data analysis tools based on spectral geometry, such as
diffusion maps and spectral clustering. We provide several synthetic and real
examples of manifold learning, retrieval, and clustering demonstrating that the
joint diffusion geometry frequently better captures the inherent structure of
multi-modal data. We also show that many previous attempts to construct
multimodal spectral clustering can be seen as particular cases of joint
approximate diagonalization of the Laplacians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2308</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2308</id><created>2012-09-11</created><updated>2014-04-04</updated><authors><author><keyname>Ghosh</keyname><forenames>Subir Kumar</forenames></author><author><keyname>Roy</keyname><forenames>Bodhayan</forenames></author></authors><title>Some Results On Point Visibility Graphs</title><categories>cs.CG cs.DM math.CO</categories><msc-class>68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present three necessary conditions for recognizing point
visibility graphs. We show that this recognition problem lies in PSPACE. We
state new properties of point visibility graphs along with some known
properties that are important in understanding point visibility graphs. For
planar point visibility graphs, we present a complete characterization which
leads to a linear time recognition and reconstruction algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2322</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2322</id><created>2012-09-11</created><authors><author><keyname>Puente</keyname><forenames>Javier</forenames></author><author><keyname>de la Fuente</keyname><forenames>David</forenames></author><author><keyname>Lozano</keyname><forenames>Jesus</forenames></author><author><keyname>Gascon</keyname><forenames>Fernando</forenames></author></authors><title>On firm specific characteristics of pharmaceutical generics and
  incentives to permanence under fuzzy conditions</title><categories>cs.AI</categories><journal-ref>International Journal of Applications of Fuzzy Sets(ISSN
  2241-1240) Vol. 1 (2011), 19-37</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to develop a methodology that is useful for
analysing from a microeconomic perspective the incentives to entry, permanence
and exit in the market for pharmaceutical generics under fuzzy conditions. In
an empirical application of our proposed methodology, the potential towards
permanence of labs with different characteristics has been estimated. The case
we deal with is set in an open market where global players diversify into
different national markets of pharmaceutical generics. Risk issues are
significantly important in deterring decision makers from expanding in the
generic pharmaceutical business. However, not all players are affected in the
same way and/or to the same extent. Small, non-diversified generics labs are in
the worse position. We have highlighted that the expected NPV and the number of
generics in the portfolio of a pharmaceutical lab are important variables, but
that it is also important to consider the degree of diversification. Labs with
a higher potential for diversification across markets have an advantage over
smaller labs. We have described a fuzzy decision support system based on the
Mamdani model in order to determine the incentives for a laboratory to remain
in the market both when it is stable and when it is growing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2333</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2333</id><created>2012-09-11</created><authors><author><keyname>Agrawal</keyname><forenames>Manindra</forenames></author><author><keyname>Saha</keyname><forenames>Chandan</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Quasi-polynomial Hitting-set for Set-depth-Delta Formulas</title><categories>cs.CC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We call a depth-4 formula C set-depth-4 if there exists a (unknown) partition
(X_1,...,X_d) of the variable indices [n] that the top product layer respects,
i.e. C(x) = \sum_{i=1}^k \prod_{j=1}^{d} f_{i,j}(x_{X_j}), where f_{i,j} is a
sparse polynomial in F[x_{X_j}]. Extending this definition to any depth - we
call a depth-Delta formula C (consisting of alternating layers of Sigma and Pi
gates, with a Sigma-gate on top) a set-depth-Delta formula if every Pi-layer in
C respects a (unknown) partition on the variables; if Delta is even then the
product gates of the bottom-most Pi-layer are allowed to compute arbitrary
monomials.
  In this work, we give a hitting-set generator for set-depth-Delta formulas
(over any field) with running time polynomial in exp(({Delta}^2 log s)^{Delta -
1}), where s is the size bound on the input set-depth-Delta formula. In other
words, we give a quasi-polynomial time blackbox polynomial identity test for
such constant-depth formulas. Previously, the very special case of Delta=3
(also known as set-multilinear depth-3 circuits) had no known sub-exponential
time hitting-set generator. This was declared as an open problem by Shpilka &amp;
Yehudayoff (FnT-TCS 2010); the model being first studied by Nisan &amp; Wigderson
(FOCS 1995). Our work settles this question, not only for depth-3 but, up to
depth epsilon.log s / loglog s, for a fixed constant epsilon &lt; 1.
  The technique is to investigate depth-Delta formulas via depth-(Delta-1)
formulas over a Hadamard algebra, after applying a `shift' on the variables. We
propose a new algebraic conjecture about the low-support rank-concentration in
the latter formulas, and manage to prove it in the case of set-depth-Delta
formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2340</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2340</id><created>2012-09-11</created><updated>2012-09-27</updated><authors><author><keyname>Khan</keyname><forenames>Zaryab</forenames></author></authors><title>Quadratic time $O(n^2)$ Fully Homomorphic public key encryption
  algorithm based on a unique technique to create p-adic homomorphism from ring
  X to ring Y</title><categories>cs.CR</categories><comments>critical typo in the toy example</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ZK111 is a fully homomorphic public key encryption algorithm which runs in
quadratic time. It's security solely relies upon a very unique 'color-blind'
function which is used to create p-adic ring homomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2341</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2341</id><created>2012-09-11</created><updated>2012-09-18</updated><authors><author><keyname>Balamurali</keyname><forenames>A. R.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Malu</keyname><forenames>Akshat</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author></authors><title>Leveraging Sentiment to Compute Word Similarity</title><categories>cs.IR cs.CL</categories><comments>The paper is available at
  http://subhabrata-mukherjee.webs.com/publications.htm</comments><journal-ref>In Proceedings of The 6th International Global Wordnet Conference
  (GWC 2012), Matsue, Japan, January, 9-13, 2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we introduce a new WordNet based similarity metric, SenSim,
which incorporates sentiment content (i.e., degree of positive or negative
sentiment) of the words being compared to measure the similarity between them.
The proposed metric is based on the hypothesis that knowing the sentiment is
beneficial in measuring the similarity. To verify this hypothesis, we measure
and compare the annotator agreement for 2 annotation strategies: 1) sentiment
information of a pair of words is considered while annotating and 2) sentiment
information of a pair of words is not considered while annotating.
Inter-annotator correlation scores show that the agreement is better when the
two annotators consider sentiment information while assigning a similarity
score to a pair of words. We use this hypothesis to measure the similarity
between a pair of words. Specifically, we represent each word as a vector
containing sentiment scores of all the content words in the WordNet gloss of
the sense of that word. These sentiment scores are derived from a sentiment
lexicon. We then measure the cosine similarity between the two vectors. We
perform both intrinsic and extrinsic evaluation of SenSim and compare the
performance with other widely usedWordNet similarity metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2352</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2352</id><created>2012-09-11</created><updated>2012-09-18</updated><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author></authors><title>Feature Specific Sentiment Analysis for Product Reviews</title><categories>cs.IR cs.CL</categories><comments>The paper is available at
  http://subhabrata-mukherjee.webs.com/publications.htm</comments><journal-ref>COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING, Lecture
  Notes in Computer Science, 2012, Volume 7181/2012, 475-487</journal-ref><doi>10.1007/978-3-642-28604-9_39</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a novel approach to identify feature specific
expressions of opinion in product reviews with different features and mixed
emotions. The objective is realized by identifying a set of potential features
in the review and extracting opinion expressions about those features by
exploiting their associations. Capitalizing on the view that more closely
associated words come together to express an opinion about a certain feature,
dependency parsing is used to identify relations between the opinion
expressions. The system learns the set of significant relations to be used by
dependency parsing and a threshold parameter which allows us to merge closely
associated opinion expressions. The data requirement is minimal as this is a
one time learning of the domain independent parameters. The associations are
represented in the form of a graph which is partitioned to finally retrieve the
opinion expression describing the user specified feature. We show that the
system achieves a high accuracy across all domains and performs at par with
state-of-the-art systems despite its data limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2355</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2355</id><created>2012-09-11</created><updated>2013-07-27</updated><authors><author><keyname>Bottou</keyname><forenames>L&#xe9;on</forenames></author><author><keyname>Peters</keyname><forenames>Jonas</forenames></author><author><keyname>Qui&#xf1;onero-Candela</keyname><forenames>Joaquin</forenames></author><author><keyname>Charles</keyname><forenames>Denis X.</forenames></author><author><keyname>Chickering</keyname><forenames>D. Max</forenames></author><author><keyname>Portugaly</keyname><forenames>Elon</forenames></author><author><keyname>Ray</keyname><forenames>Dipankar</forenames></author><author><keyname>Simard</keyname><forenames>Patrice</forenames></author><author><keyname>Snelson</keyname><forenames>Ed</forenames></author></authors><title>Counterfactual Reasoning and Learning Systems</title><categories>cs.LG cs.AI cs.IR math.ST stat.TH</categories><comments>revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work shows how to leverage causal inference to understand the behavior
of complex learning systems interacting with their environment and predict the
consequences of changes to the system. Such predictions allow both humans and
algorithms to select changes that improve both the short-term and long-term
performance of such systems. This work is illustrated by experiments carried
out on the ad placement system associated with the Bing search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2364</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2364</id><created>2012-09-11</created><updated>2012-12-09</updated><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Performance Modeling for Dense Linear Algebra</title><categories>cs.MS cs.NA cs.PF</categories><comments>3rd International Workshop on Performance Modeling, Benchmarking and
  Simulation of High Performance Computer Systems (PMBS12), International
  Conference for High Performance Computing, Networking, Storage and Analysis
  2012 (SC12)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the behavior of dense linear algebra algorithms is
greatly influenced by factors like target architecture, underlying libraries
and even problem size; because of this, the accurate prediction of their
performance is a real challenge. In this article, we are not interested in
creating accurate models for a given algorithm, but in correctly ranking a set
of equivalent algorithms according to their performance. Aware of the
hierarchical structure of dense linear algebra routines, we approach the
problem by developing a framework for the automatic generation of statistical
performance models for BLAS and LAPACK libraries. This allows us to obtain
predictions through evaluating and combining such models. We demonstrate that
our approach is successful in both single- and multi-core environments, not
only in the ranking of algorithms but also in tuning their parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2368</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2368</id><created>2012-08-31</created><authors><author><keyname>Vyas</keyname><forenames>Shilpan Dineshkumar</forenames></author></authors><title>Impact of E-Banking on Traditional Banking Services</title><categories>cs.CY</categories><journal-ref>International Journal of Computer Science &amp; Communication
  Networks, Vol 2(3), 310-313, June-July 2012, ISSN: 2249-5789</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet banking is changing the banking industry, having the major effects
on banking relationships. Banking is now no longer confined to the branches
were one has to approach the branch in person, to withdraw cash or deposit a
cheque or request a statement of accounts. In true Internet banking, any
inquiry or transaction is processed online without any reference to the branch
(anywhere banking) at any time. Providing Internet banking is increasingly
becoming a &quot;need to have&quot; than a &quot;nice to have&quot; service. The net banking, thus,
now is more of a norm rather than an exception in many developed countries due
to the fact that it is the cheapest way of providing banking services. This
research paper will introduce you to e-banking, giving the meaning, functions,
types, advantages and limitations of e-banking. It will also show the impact of
e-banking on traditional services and finally the result documentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2376</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2376</id><created>2012-09-11</created><updated>2013-07-25</updated><authors><author><keyname>Mishra</keyname><forenames>Rohit</forenames></author><author><keyname>Zeeshan</keyname><forenames>Md</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Two Way Concurrent Buffer System without Deadlock in Various Time Models
  Using Timed Automata</title><categories>cs.CR</categories><comments>24 pages, 19 figures, presented at WICT 2012, Trivandrum, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two way buffer system is a system that exhibits transfer of data using two
buffers concurrently. It includes processes that synchronize to exchange data
with each other along with executing certain delays between these
synchronizations. In existing Tiny Two Way Buffer System, both generators
produce packets in half duplex manner in no time, deterministic time, and non
deterministic time. Analysis of the model for above time options leads the
model in deadlock. The model can be out of the deadlock if timings in the model
is incorporated in alternative fashion. The generators produce packets after a
delay of 10 seconds. However, generator one has an initial shift of 5 seconds
after which it begins sending a packet every 10 seconds. Hence, initial delay
for generator one is 15 seconds and for generator two it is 10 seconds. Due to
this initial shift, both generators produce packets alternatively and is
deadlock free as the packets do not meet at the same time instant. Moreover,
the existing system model is not concurrent and hence takes more time for
packet transfer in every iteration. In this paper we have proposed a model of
buffer system using an additional dummy buffer for transfer of data packets, we
thus checking the model with various time models as no time, deterministic time
and non deterministic time. The results of proposed model under above time
models are in deadlock. We achieve deadlock free situation by introducing
appropriate delay in various buffers of the proposed system, the delay timing
is nondeterministic time. The new approach speeds up the transfer of packets,
as a result the transfer of data becomes concurrent, deadlock free and hence
the model proposed is time efficient. Simulation results shows that the
proposed two way buffer system is fully concurrent and time efficient as
compared to the existing buffer system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2379</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2379</id><created>2012-09-11</created><updated>2012-12-21</updated><authors><author><keyname>Caboara</keyname><forenames>Massimo</forenames></author><author><keyname>Perry</keyname><forenames>John</forenames></author></authors><title>Reducing the size and number of linear programs in a dynamic Gr\&quot;obner
  basis algorithm</title><categories>math.AC cs.SC</categories><comments>11 figures, of which half are algorithms; submitted to journal for
  refereeing, December 2012</comments><msc-class>13P10 (Primary) 68W30, 90C05, 90C10</msc-class><acm-class>F.2.1; I.1.2; G.1.6</acm-class><doi>10.1007/s00200-014-0216-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic algorithm to compute a Gr\&quot;obner basis is nearly twenty years
old, yet it seems to have arrived stillborn; aside from two initial
publications, there have been no published followups. One reason for this may
be that, at first glance, the added overhead seems to outweigh the benefit; the
algorithm must solve many linear programs with many linear constraints. This
paper describes two methods of reducing the cost substantially, answering the
problem effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2382</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2382</id><created>2012-09-08</created><authors><author><keyname>Kordon</keyname><forenames>F.</forenames></author><author><keyname>Linard</keyname><forenames>A.</forenames></author><author><keyname>Buchs</keyname><forenames>D.</forenames></author><author><keyname>Colange</keyname><forenames>M.</forenames></author><author><keyname>Evangelista</keyname><forenames>S.</forenames></author><author><keyname>Fronc</keyname><forenames>L.</forenames></author><author><keyname>Hillah</keyname><forenames>L. M.</forenames></author><author><keyname>Lohmann</keyname><forenames>N.</forenames></author><author><keyname>Paviot-Adet</keyname><forenames>E.</forenames></author><author><keyname>Pommereau</keyname><forenames>F</forenames></author><author><keyname>Rohr</keyname><forenames>C.</forenames></author><author><keyname>Thierry-Mieg</keyname><forenames>Y.</forenames></author><author><keyname>Wimmel</keyname><forenames>H.</forenames></author><author><keyname>Wolf</keyname><forenames>K.</forenames></author></authors><title>Raw Report on the Model Checking Contest at Petri Nets 2012</title><categories>cs.SE</categories><comments>78 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents the results of the Model Checking Contest held at Petri
Nets 2012 in Hambourg. This contest aimed at a fair and experimental evaluation
of the performances of model checking techniques applied to Petri nets. This is
the second edition after a successful one in 2011.
  The participating tools were compared on several examinations (state space
generation and evaluation of several types of formulae - structural,
reachability, LTL, CTL) run on a set of common models (Place/Transition and
Symmetric Petri nets).
  After a short overview of the contest, this paper provides the raw results
from the context, model per model and examination per examination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2388</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2388</id><created>2012-09-11</created><updated>2013-04-29</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>On the Complexity of Bandit and Derivative-Free Stochastic Convex
  Optimization</title><categories>cs.LG math.OC stat.ML</categories><comments>Version appearing in COLT (Conference on Learning Theory) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of stochastic convex optimization with bandit feedback (in the
learning community) or without knowledge of gradients (in the optimization
community) has received much attention in recent years, in the form of
algorithms and performance upper bounds. However, much less is known about the
inherent complexity of these problems, and there are few lower bounds in the
literature, especially for nonlinear functions. In this paper, we investigate
the attainable error/regret in the bandit and derivative-free settings, as a
function of the dimension d and the available number of queries T. We provide a
precise characterization of the attainable performance for strongly-convex and
smooth functions, which also imply a non-trivial lower bound for more general
problems. Moreover, we prove that in both the bandit and derivative-free
setting, the required number of queries must scale at least quadratically with
the dimension. Finally, we show that on the natural class of quadratic
functions, it is possible to obtain a &quot;fast&quot; O(1/T) error rate in terms of T,
under mild assumptions, even without having access to gradients. To the best of
our knowledge, this is the first such rate in a derivative-free stochastic
setting, and holds despite previous results which seem to imply the contrary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2400</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2400</id><created>2012-09-11</created><authors><author><keyname>Delpech</keyname><forenames>Estelle</forenames><affiliation>LINA</affiliation></author><author><keyname>Daille</keyname><forenames>B&#xe9;atrice</forenames><affiliation>LINA</affiliation></author><author><keyname>Morin</keyname><forenames>Emmanuel</forenames><affiliation>LINA</affiliation></author><author><keyname>Lemaire</keyname><forenames>Claire</forenames></author></authors><title>Identification of Fertile Translations in Medical Comparable Corpora: a
  Morpho-Compositional Approach</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>AMTA, San Diego, CA : United States (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines a method for lexicon in the biomedical domain from
comparable corpora. The method is based on compositional translation and
exploits morpheme-level translation equivalences. It can generate translations
for a large variety of morphologically constructed words and can also generate
'fertile' translations. We show that fertile translations increase the overall
quality of the extracted lexicon for English to French translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2408</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2408</id><created>2012-09-11</created><updated>2013-09-22</updated><authors><author><keyname>Forbes</keyname><forenames>Michael A.</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>Quasipolynomial-time Identity Testing of Non-Commutative and Read-Once
  Oblivious Algebraic Branching Programs</title><categories>cs.CC</categories><comments>35 pages; v2 updated with improvements for the bounded-width case,
  results for polynomials of low evaluation dimension, and applications to
  Noether Normalization</comments><journal-ref>54rd Annual IEEE Symposium on Foundations of Computer Science,
  FOCS 2013,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of obtaining deterministic black-box polynomial identity
testing algorithms (PIT) for algebraic branching programs (ABPs) that are
read-once and oblivious. This class has an deterministic white-box polynomial
identity testing algorithm (due to Raz and Shpilka), but prior to this work
there was no known such black-box algorithm.
  The main result of this work gives the first quasi-polynomial sized hitting
sets for size S circuits from this class, when the order of the variables is
known. As our hitting set is of size exp(lg^2 S), this is analogous (in the
terminology of boolean pseudorandomness) to a seed-length of lg^2 S, which is
the seed length of the pseudorandom generators of Nisan and
Impagliazzo-Nisan-Wigderson for read-once oblivious boolean branching programs.
  Our results are stronger for branching programs of bounded width, where we
give a hitting set of size exp(lg^2 S/lglg S), corresponding to a seed length
of lg^2 S/lglg S. This is in stark contrast to the known results for read-once
oblivious boolean branching programs of bounded width, where no pseudorandom
generator (or hitting set) with seed length o(lg^2 S) is known.
  In follow up work, we strengthened a result of Mulmuley, and showed that
derandomizing a particular case of the Noether Normalization Lemma is reducible
to black-box PIT of read-once oblivious ABPs. Using the results of the present
work, this gives a derandomization of Noether Normalization in that case, which
Mulmuley conjectured would difficult due to its relations to problems in
algebraic geometry.
  We also show that several other circuit classes can be black-box reduced to
read-once oblivious ABPs, including set-multilinear ABPs (a generalization of
depth-3 set-multilinear formulas), non-commutative ABPs (generalizing
non-commutative formulas), and (semi-)diagonal depth-4 circuits (as introduced
by Saxena).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2419</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2419</id><created>2012-09-11</created><authors><author><keyname>Noble</keyname><forenames>Charleston</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Brockmann</keyname><forenames>Dirk</forenames></author></authors><title>The role of caretakers in disease dynamics</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.PE</categories><comments>8 pages, 9 figures</comments><doi>10.1007/s10955-013-0787-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in modeling the dynamics of contagion phenomena is
to understand how the structure of social interactions shapes the time course
of a disease. Complex network theory has provided significant advances in this
context. However, awareness of an epidemic in a population typically yields
behavioral changes that correspond to changes in the network structure on which
the disease evolves. This feedback mechanism has not been investigated in
depth. For example, one would intuitively expect susceptible individuals to
avoid other infecteds. However, doctors treating patients or parents tending
sick children may also increase the amount of contact made with an infecteds,
in an effort to speed up recovery but also exposing themselves to higher risks
of infection. We study the role of these caretaker links in an adaptive network
models where individuals react to a disease by increasing or decreasing the
amount of contact they make with infected individuals. We find that pure
avoidance, with only few caretaker links, is the best strategy for curtailing
an SIS disease in networks that possess a large topological variability. In
more homogeneous networks, disease prevalence is decreased for low
concentrations of caretakers whereas a high prevalence emerges if caretaker
concentration passes a well defined critical value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2423</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2423</id><created>2012-09-11</created><authors><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Reply to recent scepticism about the foundations of quantum cryptography</title><categories>quant-ph cs.CR</categories><comments>1 page + references and footnotes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a series of recent papers, Hirota and Yuen claim to have identified a
fundamental flaw in the theory underlying quantum cryptography, which would
invalidate existing security proofs. In this short note, we sketch their
argument and show that their conclusion is unjustified --- it originates from a
confusion between necessary and sufficient criteria for secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2433</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2433</id><created>2012-09-11</created><updated>2012-10-06</updated><authors><author><keyname>Risk</keyname><forenames>James</forenames></author></authors><title>Correlations between Google search data and Mortality Rates</title><categories>stat.AP cs.IR</categories><comments>4 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by correlations recently discovered between Google search data and
financial markets, we show correlations between Google search data mortality
rates. Words with negative connotations may provide for increased mortality
rates, while words with positive connotations may provide for decreased
mortality rates, and so statistical methods were employed to determine to
investigate further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2434</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2434</id><created>2012-09-11</created><authors><author><keyname>Jamieson</keyname><forenames>Kevin G.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Query Complexity of Derivative-Free Optimization</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides lower bounds on the convergence rate of Derivative Free
Optimization (DFO) with noisy function evaluations, exposing a fundamental and
unavoidable gap between the performance of algorithms with access to gradients
and those with access to only function evaluations. However, there are
situations in which DFO is unavoidable, and for such situations we propose a
new DFO algorithm that is proved to be near optimal for the class of strongly
convex objective functions. A distinctive feature of the algorithm is that it
uses only Boolean-valued function comparisons, rather than function
evaluations. This makes the algorithm useful in an even wider range of
applications, such as optimization based on paired comparisons from human
subjects, for example. We also show that regardless of whether DFO is based on
noisy function evaluations or Boolean-valued function comparisons, the
convergence rate is the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2457</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2457</id><created>2012-09-11</created><authors><author><keyname>Talamo</keyname><forenames>Maurizio</forenames></author><author><keyname>Galinium</keyname><forenames>Maulahikmah</forenames></author><author><keyname>Schunck</keyname><forenames>Christian H.</forenames></author><author><keyname>Arcieri</keyname><forenames>Franco</forenames></author></authors><title>Interleaving Command Sequences: a Threat to Secure Smartcard
  Interoperability</title><categories>cs.CR</categories><comments>6 pages; published in the 10th WSEAS International Conference on
  Information Security and Privacy (ISP 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasingly widespread use of smartcards for a variety of sensitive
applications, including digital signatures, creates the need to ensure and
possibly certify the secure interoperability of these devices. Standard
certification criteria, in particular the Common Criteria, define security
requirements but do not sufficiently address the problem of interoperability.
Here we consider the interoperability problem which arises when various
applications interact with different smartcards through a middleware. In such a
situation it is possible that a smartcard of type S receives commands that were
supposed to be executed on a different smartcard of type S'. Such &quot;external
commands&quot; can interleave with the commands that were supposed to be executed on
S. We experimentally demonstrate this problem with a Common Criteria certified
digital signature process on a commercially available smartcard. Importantly,
in some of these cases the digital signature processes terminate without
generating an error message or warning to the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2471</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2471</id><created>2012-09-11</created><authors><author><keyname>Weifan</keyname><forenames>Wang</forenames></author><author><keyname>Qiaojun</keyname><forenames>Shu</forenames></author><author><keyname>Yiqiao</keyname><forenames>Wang</forenames></author></authors><title>Every 4-regular graph is acyclically edge-6-colorable</title><categories>math.CO cs.DM</categories><comments>24 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that
no bichromatic cycles are produced. The acyclic chromatic index $a'(G)$ of $G$
is the smallest integer $k$ such that $G$ has an acyclic edge coloring using
$k$ colors. Fiam${\rm \check{c}}$ik (1978) and later Alon, Sudakov and Zaks
(2001) conjectured that $a'(G)\le \Delta + 2$ for any simple graph $G$ with
maximum degree $\Delta$. Basavaraju and Chandran (2009) showed that every graph
$G$ with $\Delta=4$, which is not 4-regular, satisfies the conjecture. In this
paper, we settle the 4-regular case, i.e., we show that every 4-regular graph
$G$ has $a'(G)\le 6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2476</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2476</id><created>2012-09-11</created><updated>2013-08-15</updated><authors><author><keyname>Silva</keyname><forenames>Filipi Nascimento</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Local Dimension of Complex Networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>15 pages, 7 figures. A working manuscript, all suggestions are
  welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality is one of the most important properties of complex physical
systems. However, only recently this concept has been considered in the context
of complex networks. In this paper we further develop the previously introduced
definitions of dimension in complex networks by presenting a new method to
characterize the dimensionality of individual nodes. The methodology consists
in obtaining patterns of dimensionality at different scales for each node,
which can be used to detect regions with distinct dimensional structures as
well as borders. We also apply this technique to power grid networks, showing,
quantitatively, that the continental European power grid is substantially more
planar than the network covering the western states of US, which present
topological dimension higher than their intrinsic embedding space dimension.
Local dimension also successfully revealed how distinct regions of network
topologies spreads along the degrees of freedom when it is embedded in a metric
space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2486</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2486</id><created>2012-09-11</created><updated>2013-02-20</updated><authors><author><keyname>Wang</keyname><forenames>Baiyang</forenames></author></authors><title>On sampling social networking services</title><categories>stat.AP cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article aims at summarizing the existing methods for sampling social
networking services and proposing a faster confidence interval for related
sampling methods. It also includes comparisons of common network sampling
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2493</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2493</id><created>2012-09-12</created><updated>2012-09-18</updated><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author></authors><title>WikiSent : Weakly Supervised Sentiment Analysis Through Extractive
  Summarization With Wikipedia</title><categories>cs.IR cs.CL</categories><comments>The paper is available at
  http://subhabrata-mukherjee.webs.com/publications.htm</comments><journal-ref>Lecture Notes in Computer Science Volume 7523, 2012, pp 774-793</journal-ref><doi>10.1007/978-3-642-33460-3_55</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes a weakly supervised system for sentiment analysis in the
movie review domain. The objective is to classify a movie review into a
polarity class, positive or negative, based on those sentences bearing opinion
on the movie alone. The irrelevant text, not directly related to the reviewer
opinion on the movie, is left out of analysis. Wikipedia incorporates the world
knowledge of movie-specific features in the system which is used to obtain an
extractive summary of the review, consisting of the reviewer's opinions about
the specific aspects of the movie. This filters out the concepts which are
irrelevant or objective with respect to the given movie. The proposed system,
WikiSent, does not require any labeled data for training. The only weak
supervision arises out of the usage of resources like WordNet, Part-of-Speech
Tagger and Sentiment Lexicons by virtue of their construction. WikiSent
achieves a considerable accuracy improvement over the baseline and has a better
or comparable accuracy to the existing semi-supervised and unsupervised systems
in the domain, on the same dataset. We also perform a general movie review
trend analysis using WikiSent to find the trend in movie-making and the public
acceptance in terms of movie genre, year of release and polarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2495</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2495</id><created>2012-09-12</created><updated>2012-09-18</updated><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Malu</keyname><forenames>Akshat</forenames></author><author><keyname>Balamurali</keyname><forenames>A. R.</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author></authors><title>TwiSent: A Multistage System for Analyzing Sentiment in Twitter</title><categories>cs.IR cs.CL</categories><comments>The paper is available at
  http://subhabrata-mukherjee.webs.com/publications.htm</comments><journal-ref>In Proceedings of The 21st ACM Conference on Information and
  Knowledge Management (CIKM), 2012 as a poster</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present TwiSent, a sentiment analysis system for Twitter.
Based on the topic searched, TwiSent collects tweets pertaining to it and
categorizes them into the different polarity classes positive, negative and
objective. However, analyzing micro-blog posts have many inherent challenges
compared to the other text genres. Through TwiSent, we address the problems of
1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies
in the text in the form of incorrect spellings, nonstandard abbreviations,
slangs etc., 3) Entity specificity in the context of the topic searched and 4)
Pragmatics embedded in text. The system performance is evaluated on manually
annotated gold standard data and on an automatically annotated tweet set based
on hashtags. It is a common practise to show the efficacy of a supervised
system on an automatically annotated dataset. However, we show that such a
system achieves lesser classification accurcy when tested on generic twitter
dataset. We also show that our system performs much better than an existing
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2501</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2501</id><created>2012-09-12</created><authors><author><keyname>Doreswamy</keyname><forenames>Hemanth K. S</forenames></author></authors><title>Performance Evaluation of Predictive Classifiers For Knowledge Discovery
  From Engineering Materials Data Sets</title><categories>cs.LG</categories><comments>Volume 3,No 3,March 2011 7 pages, 6 tables, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are
successively applied on materials informatics to classify the engineering
materials into different classes for the selection of materials that suit the
input design specifications. Here, the classifiers are analyzed individually
and their performance evaluation is analyzed with confusion matrix predictive
parameters and standard measures, the classification results are analyzed on
different class of materials. Comparison of classifiers has found that naive
Bayesian classifier is more accurate and better than the C4.5 DTC. The
knowledge discovered by the naive bayesian classifier can be employed for
decision making in materials selection in manufacturing industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2503</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2503</id><created>2012-09-12</created><updated>2014-09-10</updated><authors><author><keyname>Pongr&#xe1;cz</keyname><forenames>Lajos L.</forenames></author></authors><title>A greedy approximation algorithm for the longest path problem in
  undirected graphs</title><categories>cs.DS</categories><comments>6 pages, 3 figures Withdrawn due to an error in search subroutine,
  2nd figure and time complexity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In graph theory, the longest path problem is the problem of finding a simple
path of maximum length in a given graph. For some small classes of graphs, the
problem can be solved in polynomial time [2, 4], but it remains NP-hard on
general graphs, since it includes the Hamiltonian path problem as a special
case [3]. Motivated by finding a simple, quick algorithm for finding long paths
in large graphs, in this paper we show a greedy algorithm with a time
complexity of O(n^2 (n+m)), where n is the number of the vertices and m is the
number of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2505</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2505</id><created>2012-09-12</created><authors><author><keyname>Noghabi</keyname><forenames>Hossein Sharifi</forenames></author><author><keyname>Askar</keyname><forenames>Arash Ghazi</forenames></author><author><keyname>Boustani</keyname><forenames>Arash</forenames></author><author><keyname>Moghani</keyname><forenames>Arash</forenames></author><author><keyname>Zanjani</keyname><forenames>Motahareh Bahrami</forenames></author></authors><title>Implementing A Greedy Chain Routing Technique With Spread Spectrum On
  Grid-Based WSNs</title><categories>cs.NI</categories><comments>8 pages, 3 figures, 1 table</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 4, August 2012 page 195-202</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSN) are set of energy-limited sensors, which
recently have been point of interest due to their vast applications. One of the
efficient ways to consume energy in these networks is to utilize optimal
routing protocols. In this approach, we proposed a greedy hierarchical
chain-based routing method, named, PGC (stands for Persian Greedy Chain) which
route the network applying Spread Spectrum codes as a mask given to the grid
cells. Due to similarities between the proposed method in this article and
LEACH protocol, we compare this routing protocol with the proposed model from
diverse aspects in the simulation section such as remaining energy and being
fault tolerant and reliable. The results prove that presented method is more
robust and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2507</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2507</id><created>2012-09-12</created><authors><author><keyname>Sreenivasa</keyname><forenames>B. C.</forenames></author><author><keyname>Prakash</keyname><forenames>G. C. Bhanu</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. V.</forenames></author></authors><title>Comparative analysis of ADTCP and M-ADTCP: Congestion Control Techniques
  for improving TCP performance over Ad-hoc Networks</title><categories>cs.NI</categories><comments>9 pages 4 figures</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT): Vol. 2, No. 4, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the occurrence of congestion in a Mobile Ad-hoc Network (MANET)
is a major task. The inbuilt congestion control techniques of existing
Transmission Control Protocol (TCP) designed for wired networks do not handle
the unique properties of shared wireless multi-hop link. There are several
approaches proposed for detecting and overcoming the congestion in the mobile
ad-hoc network. In this paper we present a Modified AD-hoc Transmission Control
Protocol (M-ADTCP) method where the receiver detects the probable current
network status and transmits this information to the sender as feedback. The
sender behavior is altered appropriately. The proposed technique is also
compatible with standard TCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2508</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2508</id><created>2012-09-12</created><authors><author><keyname>Hizem</keyname><forenames>Moez</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Acquisition probability of multi-user UWB systems in the presence of a
  novel synchronization approach</title><categories>cs.NI cs.PF</categories><comments>9 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, to synchronize Ultra Wideband (UWB) systems in ad-hoc
multi-user environments, we propose a new timing acquisition approach for
achieving a good performance despite the difficulties to get there.
Synchronization constraints are caused by the ultra-short emitted waveforms
nature of UWB signals. Used in [1, 2] for single-user environments, our timing
acquisition approach is based on two successive stages or floors. Extended for
multi-user environments, the used algorithm is a combination between coarse
synchronization based on timing with dirty templates (TDT) acquisition scheme
and a new fine synchronization scheme developed in [3-6] which conduct to an
improved estimate of timing offset. In this work, we develop and test this
method in both data-aided (DA) and non-data-aided (NDA) modes. Simulation
results and comparisons are also given to confirm performance improvement of
our approach (in terms of mean square error and acquisition probability)
compared to the original TDT algorithm in multi-user environments, especially
in the NDA mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2512</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2512</id><created>2012-09-12</created><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author></authors><title>Maximum Weight Independent Sets in Odd-Hole-Free Graphs Without Dart or
  Without Bull</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Weight Independent Set (MWIS) Problem on graphs with vertex
weights asks for a set of pairwise nonadjacent vertices of maximum total
weight. Being one of the most investigated and most important problems on
graphs, it is well known to be NP-complete and hard to approximate. The
complexity of MWIS is open for hole-free graphs (i.e., graphs without induced
subgraphs isomorphic to a chordless cycle of length at least five). By applying
clique separator decomposition as well as modular decomposition, we obtain
polynomial time solutions of MWIS for odd-hole- and dart-free graphs as well as
for odd-hole- and bull-free graphs (dart and bull have five vertices, say
$a,b,c,d,e$, and dart has edges $ab,ac,ad,bd,cd,de$, while bull has edges
$ab,bc,cd,be,ce$). If the graphs are hole-free instead of odd-hole-free then
stronger structural results and better time bounds are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2515</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2515</id><created>2012-09-12</created><authors><author><keyname>Rehna</keyname><forenames>V. J.</forenames></author><author><keyname>Kumar</keyname><forenames>M. K. Jeya</forenames></author></authors><title>Wavelet Based Image Coding Schemes : A Recent Survey</title><categories>cs.CV</categories><comments>18 pages, 7 figures, journal</comments><journal-ref>International Journal on Soft Computing (IJSC) Vol.3, No.3, August
  2012, 101-118</journal-ref><doi>10.5121/ijsc.2012.3308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of new and powerful algorithms have been developed for image
compression over the years. Among them the wavelet-based image compression
schemes have gained much popularity due to their overlapping nature which
reduces the blocking artifacts that are common phenomena in JPEG compression
and multiresolution character which leads to superior energy compaction with
high quality reconstructed images. This paper provides a detailed survey on
some of the popular wavelet coding techniques such as the Embedded Zerotree
Wavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, the
Set Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Coding
with Optimized Truncation (EBCOT) algorithm. Other wavelet-based coding
techniques like the Wavelet Difference Reduction (WDR) and the Adaptive Scanned
Wavelet Difference Reduction (ASWDR) algorithms, the Space Frequency
Quantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder
(EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run
(SR) coding and the recent Geometric Wavelet (GW) coding are also discussed.
Based on the review, recommendations and discussions are presented for
algorithm development and implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2516</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2516</id><created>2012-09-12</created><authors><author><keyname>Khatibi</keyname><forenames>Elham</forenames></author><author><keyname>Ibrahim</keyname><forenames>Roliana</forenames></author></authors><title>Efficient Indicators to Evaluate the Status of Software Development
  Effort Estimation inside the Organizations</title><categories>cs.SE</categories><comments>10 pages</comments><journal-ref>International Journal of Managing Information
  Technology,(4):3,2012, 23-32</journal-ref><doi>10.5121/ijmit.2012.4303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development effort is an undeniable part of the project management which
considerably influences the success of project. Inaccurate and unreliable
estimation of effort can easily lead to the failure of project. Due to the
special specifications, accurate estimation of effort in the software projects
is a vital management activity that must be carefully done to avoid from the
unforeseen results. However numerous effort estimation methods have been
proposed in this field, the accuracy of estimates is not satisfying and the
attempts continue to improve the performance of estimation methods. Prior
researches conducted in this area have focused on numerical and quantitative
approaches and there are a few research works that investigate the root
problems and issues behind the inaccurate effort estimation of software
development effort. In this paper, a framework is proposed to evaluate and
investigate the situation of an organization in terms of effort estimation. The
proposed framework includes various indicators which cover the critical issues
in field of software development effort estimation. Since the capabilities and
shortages of organizations for effort estimation are not the same, the proposed
indicators can lead to have a systematic approach in which the strengths and
weaknesses of organizations in field of effort estimation are discovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2531</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2531</id><created>2012-09-12</created><authors><author><keyname>Bond</keyname><forenames>Mike</forenames></author><author><keyname>Choudary</keyname><forenames>Omar</forenames></author><author><keyname>Murdoch</keyname><forenames>Steven J.</forenames></author><author><keyname>Skorobogatov</keyname><forenames>Sergei</forenames></author><author><keyname>Anderson</keyname><forenames>Ross</forenames></author></authors><title>Chip and Skim: cloning EMV cards with the pre-play attack</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EMV, also known as &quot;Chip and PIN&quot;, is the leading system for card payments
worldwide. It is used throughout Europe and much of Asia, and is starting to be
introduced in North America too. Payment cards contain a chip so they can
execute an authentication protocol. This protocol requires point-of-sale (POS)
terminals or ATMs to generate a nonce, called the unpredictable number, for
each transaction to ensure it is fresh. We have discovered that some EMV
implementers have merely used counters, timestamps or home-grown algorithms to
supply this number. This exposes them to a &quot;pre-play&quot; attack which is
indistinguishable from card cloning from the standpoint of the logs available
to the card-issuing bank, and can be carried out even if it is impossible to
clone a card physically (in the sense of extracting the key material and
loading it into another card). Card cloning is the very type of fraud that EMV
was supposed to prevent. We describe how we detected the vulnerability, a
survey methodology we developed to chart the scope of the weakness, evidence
from ATM and terminal experiments in the field, and our implementation of
proof-of-concept attacks. We found flaws in widely-used ATMs from the largest
manufacturers. We can now explain at least some of the increasing number of
frauds in which victims are refused refunds by banks which claim that EMV cards
cannot be cloned and that a customer involved in a dispute must therefore be
mistaken or complicit. Pre-play attacks may also be carried out by malware in
an ATM or POS terminal, or by a man-in-the-middle between the terminal and the
acquirer. We explore the design and implementation mistakes that enabled the
flaw to evade detection until now: shortcomings of the EMV specification, of
the EMV kernel certification process, of implementation testing, formal
analysis, or monitoring customer complaints. Finally we discuss
countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2541</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2541</id><created>2012-09-12</created><authors><author><keyname>Demirel</keyname><forenames>G&#xfc;ven</forenames></author><author><keyname>Gross</keyname><forenames>Thilo</forenames></author></authors><title>Absence of epidemic thresholds in a growing adaptive network</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>23 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of social contact networks strongly influences the dynamics of
epidemic diseases. In particular the scale-free structure of real-world social
networks allows unlikely diseases with low infection rates to spread and become
endemic. However, in particular for potentially fatal diseases, also the impact
of the disease on the social structure cannot be neglected, leading to a
complex interplay. Here, we consider the growth of a network by preferential
attachment from which nodes are simultaneously removed due to an SIR epidemic.
We show that increased infectiousness increases the prevalence of the disease
and simultaneously causes a transition from scale-free to exponential topology.
Although a transition to a degree distribution with finite variance takes
place, the network still exhibits no epidemic threshold in the thermodynamic
limit. We illustrate these results using agent-based simulations and
analytically tractable approximation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2542</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2542</id><created>2012-09-12</created><authors><author><keyname>Zhao</keyname><forenames>Shancheng</forenames></author><author><keyname>Lu</keyname><forenames>Zhifei</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>Joint Detection/Decoding Algorithms for Nonbinary LDPC Codes over ISI
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the application of nonbinary low-density
parity-check (NB-LDPC) codes to binary input inter-symbol interference (ISI)
channels. Two low-complexity joint detection/decoding algorithms are proposed.
One is referred to as max-log-MAP/X-EMS algorithm, which is implemented by
exchanging soft messages between the max-log-MAP detector and the extended
min-sum (EMS) decoder. The max-log-MAP/X-EMS algorithm is applicable to general
NB-LDPC codes. The other one, referred to as Viterbi/GMLGD algorithm, is
designed in particular for majority-logic decodable NB-LDPC codes. The
Viterbi/GMLGD algorithm works in an iterative manner by exchanging
hard-decisions between the Viterbi detector and the generalized majority-logic
decoder(GMLGD). As a by-product, a variant of the original EMS algorithm is
proposed, which is referred to as \mu-EMS algorithm. In the \mu-EMS algorithm,
the messages are truncated according to an adaptive threshold, resulting in a
more efficient algorithm. Simulations results show that the max-log-MAP/X-EMS
algorithm performs as well as the traditional iterative detection/decoding
algorithm based on the BCJR algorithm and the QSPA, but with lower complexity.
The complexity can be further reduced for majority-logic decodable NB-LDPC
codes by executing the Viterbi/GMLGD algorithm with a performance degradation
within one dB. Simulation results also confirm that the \mu-EMS algorithm
requires lower computational loads than the EMS algorithm with a fixed
threshold. These algorithms provide good candidates for trade-offs between
performance and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2548</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2548</id><created>2012-09-12</created><authors><author><keyname>Nandy</keyname><forenames>Sudarshan</forenames></author><author><keyname>Sarkar</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author></authors><title>Training a Feed-forward Neural Network with Artificial Bee Colony Based
  Backpropagation Method</title><categories>cs.NE cs.AI</categories><comments>14 Pages, 11 figures</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 4, 2012, 33-46</journal-ref><doi>10.5121/ijcsit.2012.4404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Back-propagation algorithm is one of the most widely used and popular
techniques to optimize the feed forward neural network training. Nature
inspired meta-heuristic algorithms also provide derivative-free solution to
optimize complex problem. Artificial bee colony algorithm is a nature inspired
meta-heuristic algorithm, mimicking the foraging or food source searching
behaviour of bees in a bee colony and this algorithm is implemented in several
applications for an improved optimized outcome. The proposed method in this
paper includes an improved artificial bee colony algorithm based
back-propagation neural network training method for fast and improved
convergence rate of the hybrid neural network learning method. The result is
analysed with the genetic algorithm based back-propagation method, and it is
another hybridized procedure of its kind. Analysis is performed over standard
data sets, reflecting the light of efficiency of proposed method in terms of
convergence speed and rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2550</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2550</id><created>2012-09-12</created><authors><author><keyname>Nayak</keyname><forenames>Pinki</forenames></author><author><keyname>Agarwal</keyname><forenames>Rekha</forenames></author><author><keyname>Verma</keyname><forenames>Seema</forenames></author></authors><title>Energy aware routing scheme for mobile ad hoc network using variable
  range transmission</title><categories>cs.NI</categories><comments>11 pages</comments><journal-ref>International Journal of Ad hoc, Sensor &amp; Ubiquitous Computing
  (IJASUC) Vol.3, No.4, 2012, 53-63</journal-ref><doi>10.5121/ijasuc.2012.3406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a new energy aware routing scheme which uses variable
transmission range. The protocol has been incorporated along with the route
discovery procedure of AODV as a case study. Both the protocols are simulated
using Network Simulator and comparisons are made to analyze their performance
based on energy consumption, network lifetime and number of alive nodes metrics
for different network scenarios. The results show that EAR makes effective node
energy utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2553</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2553</id><created>2012-09-12</created><authors><author><keyname>Malathi</keyname><forenames>S.</forenames></author><author><keyname>Sridhar</keyname><forenames>S.</forenames></author></authors><title>Optimization of fuzzy analogy in software cost estimation using
  linguistic variables</title><categories>cs.SE</categories><comments>14 pages, 8 figures; Journal of Systems and Software, 2011. arXiv
  admin note: text overlap with arXiv:1112.3877 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important objectives of software engineering community has
been the increase of useful models that beneficially explain the development of
life cycle and precisely calculate the effort of software cost estimation. In
analogy concept, there is deficiency in handling the datasets containing
categorical variables though there are innumerable methods to estimate the
cost. Due to the nature of software engineering domain, generally project
attributes are often measured in terms of linguistic values such as very low,
low, high and very high. The imprecise nature of such value represents the
uncertainty and vagueness in their elucidation. However, there is no efficient
method that can directly deal with the categorical variables and tolerate such
imprecision and uncertainty without taking the classical intervals and numeric
value approaches. In this paper, a new approach for optimization based on fuzzy
logic, linguistic quantifiers and analogy based reasoning is proposed to
improve the performance of the effort in software project when they are
described in either numerical or categorical data. The performance of this
proposed method exemplifies a pragmatic validation based on the historical NASA
dataset. The results were analyzed using the prediction criterion and indicates
that the proposed method can produce more explainable results than other
machine learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2557</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2557</id><created>2012-09-12</created><authors><author><keyname>Ojha</keyname><forenames>Gaurav</forenames></author><author><keyname>Tak</keyname><forenames>Gaurav Kumar</forenames></author></authors><title>A novel approach against E-mail attacks derived from user-awareness
  based techniques</title><categories>cs.CR</categories><comments>16 pages, 12 figures, International Journal of Information Technology
  Convergence and Services (IJITCS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large part of modern day communications are carried out through the medium
of E-mails, especially corporate communications. More and more people are using
E-mail for personal uses too. Companies also send notifications to their
customers in E-mail. In fact, in the Multinational business scenario E-mail is
the most convenient and sought-after method of communication. Important
features of E-mail such as its speed, reliability, efficient storage options
and a large number of added facilities make it highly popular among people from
all sectors of business and society. But being largely popular has its negative
aspects too. E-mails are the preferred medium for a large number of attacks
over the internet. Some of the most popular attacks over the internet include
spams, and phishing mails. Both spammers and phishers utilize E-mail services
quite efficiently in spite of a large number of detection and prevention
techniques already in place. Very few methods are actually good in
detection/prevention of spam/phishing related mails but they have higher false
positives. These techniques are implemented at the server and in addition to
giving higher number of false positives, they add to the processing load on the
server. This paper outlines a novel approach to detect not only spam, but also
scams, phishing and advertisement related mails. In this method, we overcome
the limitations of server-side detection techniques by utilizing some
intelligence on the part of users. Keywords parsing, token separation and
knowledge bases are used in the background to detect almost all E-mail attacks.
The proposed methodology, if implemented, can help protect E-mail users from
almost all kinds of unwanted mails with enhanced efficiency, reduced number of
false positives while not increasing the load on E-mail servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2582</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2582</id><created>2012-09-12</created><authors><author><keyname>Mishra</keyname><forenames>Mina</forenames></author><author><keyname>Mankar</keyname><forenames>V. H.</forenames></author></authors><title>Hybrid Message-Embedded Cipher Using Logistic Map</title><categories>cs.CR</categories><comments>10pages,6 figures. arXiv admin note: substantial text overlap with
  arXiv:1208.1900</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proposed hybrid message embedded scheme consists of hill cipher combined
with message embedded chaotic scheme. Message-embedded scheme using non-linear
feedback shift register as non-linear function and 1-D logistic map as chaotic
map is modified, analyzed and tested for avalanche property and strength
against known plaintext attack and brute-force attack. Parameter of logistic
map acts as a secret key. As we know that the minimum key space to resist
brute-force attack is 2100, and it is observed from analysis that key space of
the discussed method is lesser than 2100. But the identifiability test
concludes that the scheme consists of identifiable keys which are sufficient
condition to resist brute-force attack for chaotic ciphers. A complete file can
be encrypted and decrypted successfully by the method that assures security
against brute force attack. It is also concluded that the scheme has an average
key sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2602</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2602</id><created>2012-09-12</created><authors><author><keyname>Staicu</keyname><forenames>Stefan</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Internal joint forces in dynamics of a 3-PRP planar parallel robot</title><categories>cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:0904.0058</comments><proxy>ccsd</proxy><journal-ref>Proceedings of the Romanian Academy, Series A 13, 3 (2012) 235-242</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive matrix relations for the complete dynamics of a 3-PRP planar
parallel robot are established in this paper. Three identical planar legs
connecting to the moving platform are located in the same vertical plane.
Knowing the motion of the platform, we develop first the inverse kinematical
problem and determine the positions, velocities and accelerations of the robot.
Further, the inverse dynamic problem is solved using an approach based on the
principle of virtual work. Finally, some graphs of simulation for the input
powers of three actuators and the internal joint forces are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2612</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2612</id><created>2012-09-12</created><updated>2012-09-12</updated><authors><author><keyname>Wenfeng</keyname><forenames>Feng</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author><author><keyname>Junhao</keyname><forenames>Yan</forenames></author></authors><title>Effect of interaction strength on the evolution of cooperation</title><categories>cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative behaviors are ubiquitous in nature,which is a puzzle to
evolutionary biology,because the defector always gains more benefit than the
cooperator,thus,the cooperator should decrease and vanish over time.This
typical &quot;prisoners' dilemma&quot; phenomenon has been widely researched in recent
years.The interaction strength between cooperators and defectors is introduced
in this paper(in human society,it can be understood as the tolerance of
cooperators).We find that only when the maximum interaction strength is between
two critical values,the cooperator and defector can coexist,otherwise, 1) if it
is greater than the upper value,the cooperator will vanish, 2) if it is less
than the lower value,a bistable state will appear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2614</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2614</id><created>2012-09-12</created><authors><author><keyname>Venkataramana</keyname><forenames>K.</forenames></author><author><keyname>Padmavathamma</keyname><forenames>M.</forenames></author></authors><title>A threshold secure data sharing scheme for federated clouds</title><categories>cs.DC</categories><comments>8 pages, 3 Figures, International Journal of Research in Computer
  Science 2012. arXiv admin note: text overlap with arXiv:1003.3920 by other
  authors</comments><doi>10.7815/ijorcs.25.2012.044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing allows users to view computing in a new direction, as it uses
the existing technologies to provide better IT services at low-cost. To offer
high QOS to customers according SLA, cloud services broker or cloud service
provider uses individual cloud providers that work collaboratively to form a
federation of clouds. It is required in applications like Real-time online
interactive applications, weather research and forecasting etc., in which the
data and applications are complex and distributed. In these applications secret
data should be shared, so secure data sharing mechanism is required in
Federated clouds to reduce the risk of data intrusion, the loss of service
availability and to ensure data integrity. So In this paper we have proposed
zero knowledge data sharing scheme where Trusted Cloud Authority (TCA) will
control federated clouds for data sharing where the secret to be exchanged for
computation is encrypted and retrieved by individual cloud at the end. Our
scheme is based on the difficulty of solving the Discrete Logarithm problem
(DLOG) in a finite abelian group of large prime order which is NP-Hard. So our
proposed scheme provides data integrity in transit, data availability when one
of host providers are not available during the computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2617</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2617</id><created>2012-09-12</created><updated>2012-10-10</updated><authors><author><keyname>L&#xf3;pez-Fraguas</keyname><forenames>Francisco J.</forenames></author><author><keyname>Martin-Martin</keyname><forenames>Enrique</forenames></author><author><keyname>Rodr&#xed;guez-Hortal&#xe1;</keyname><forenames>Juan</forenames></author><author><keyname>S&#xe1;nchez-Hern&#xe1;ndez</keyname><forenames>Jaime</forenames></author></authors><title>Rewriting and narrowing for constructor systems with call-time choice
  semantics</title><categories>cs.PL</categories><comments>89 pages, 6 figures. To appear in Theory and Practice of Logic
  Programming (TPLP). Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-confluent and non-terminating constructor-based term rewrite systems are
useful for the purpose of specification and programming. In particular,
existing functional logic languages use such kind of rewrite systems to define
possibly non-strict non-deterministic functions. The semantics adopted for
non-determinism is call-time choice, whose combination with non-strictness is a
non trivial issue, addressed years ago from a semantic point of view with the
Constructor-based Rewriting Logic (CRWL), a well-known semantic framework
commonly accepted as suitable semantic basis of modern functional logic
languages. A drawback of CRWL is that it does not come with a proper notion of
one-step reduction, which would be very useful to understand and reason about
how computations proceed. In this paper we develop thoroughly the theory for
the first order version of let-rewriting, a simple reduction notion close to
that of classical term rewriting, but extended with a let-binding construction
to adequately express the combination of call-time choice with non-strict
semantics. Let-rewriting can be seen as a particular textual presentation of
term graph rewriting. We investigate the properties of let-rewriting, most
remarkably their equivalence with respect to a conservative extension of the
CRWL-semantics coping with let-bindings, and we show by some case studies that
having two interchangeable formal views (reduction/semantics) of the same
language is a powerful reasoning tool. After that, we provide a notion of
let-narrowing which is adequate for call-time choice as proved by soundness and
completeness results of let-narrowing with respect to let-rewriting. Moreover,
we relate those let-rewriting and let-narrowing relations (and hence CRWL) with
ordinary term rewriting and narrowing (..)
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2620</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2620</id><created>2012-09-12</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Lloyd</keyname><forenames>John W.</forenames></author><author><keyname>Ng</keyname><forenames>Kee Siong</forenames></author><author><keyname>Uther</keyname><forenames>William T. B.</forenames></author></authors><title>Probabilities on Sentences in an Expressive Logic</title><categories>cs.LO cs.AI cs.LG math.LO math.PR</categories><comments>52 LaTeX pages, 64 definiton/theorems/etc, presented at conference
  Progic 2011 in New York</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated reasoning about uncertain knowledge has many applications. One
difficulty when developing such systems is the lack of a completely
satisfactory integration of logic and probability. We address this problem
directly. Expressive languages like higher-order logic are ideally suited for
representing and reasoning about structured knowledge. Uncertain knowledge can
be modeled by using graded probabilities rather than binary truth-values. The
main technical problem studied in this paper is the following: Given a set of
sentences, each having some probability of being true, what probability should
be ascribed to other (query) sentences? A natural wish-list, among others, is
that the probability distribution (i) is consistent with the knowledge base,
(ii) allows for a consistent inference procedure and in particular (iii)
reduces to deductive logic in the limit of probabilities being 0 and 1, (iv)
allows (Bayesian) inductive reasoning and (v) learning in the limit and in
particular (vi) allows confirmation of universally quantified
hypotheses/sentences. We translate this wish-list into technical requirements
for a prior probability and show that probabilities satisfying all our criteria
exist. We also give explicit constructions and several general
characterizations of probabilities that satisfy some or all of the criteria and
various (counter) examples. We also derive necessary and sufficient conditions
for extending beliefs about finitely many sentences to suitable probabilities
over all sentences, and in particular least dogmatic or least biased ones. We
conclude with a brief outlook on how the developed theory might be used and
approximated in autonomous reasoning agents. Our theory is a step towards a
globally consistent and empirically satisfactory unification of probability and
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2635</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2635</id><created>2012-09-12</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author></authors><title>TCP/IP communication between two USRP-E110</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short report intends to provide an overview of the procedure and
statistics of establishing a TCP/IP link between two USRP-E110. The testings
are performed using an example GNURadio code and the networking protocol stack
provided by the Linux operating system embedded in the USRP-E110.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2641</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2641</id><created>2012-09-12</created><authors><author><keyname>Wang</keyname><forenames>Haibin</forenames></author></authors><title>C-PASS-PC: A Cloud-driven Prototype of Multi-Center Proactive
  Surveillance System for Prostate Cancer</title><categories>cs.CE</categories><journal-ref>IJCSIT 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently there are many clinical trials using paper case report forms as the
primary data collection tool. Cloud Computing platforms provide big potential
for increasing efficiency through a web-based data collection interface,
especially for large-scale multi-center trials. Traditionally, clinical and
biological data for multi-center trials are stored in one dedicated,
centralized database system running at a data coordinating center (DCC). This
paper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive
surveillance system for prostate cancer. The prototype is developed in PHP,
JQuery and CSS with an Oracle backend in a local Web server and database server
and deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The
deploying process is fast and easy to follow. The C-PASS-PC prototype can be
accessed through an SSL-enabled web browser. Our approach proves the concept
that cloud computing platforms such as GAE is a suitable and flexible solution
in the near future for multi-center clinical trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2647</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2647</id><created>2012-09-12</created><authors><author><keyname>Liu</keyname><forenames>Jason T.</forenames></author></authors><title>Shadow Theory, data model design for data integration</title><categories>cs.DB</categories><comments>85 pages, 31 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For data integration in information ecosystems, semantic heterogeneity is a
known difficulty. In this paper, we propose Shadow Theory as the philosophical
foundation to address this issue. It is based on the notion of shadows in
Plato's Allegory of the Cave. What we can observe are just shadows, and
meanings of shadows are mental entities that only exist in viewers' cognitive
structures. With enterprise customer data integration example, we proposed six
design principles and algebra to support required operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2650</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2650</id><created>2012-09-12</created><authors><author><keyname>Balloni</keyname><forenames>Antonio J.</forenames></author><author><keyname>de Azevedo</keyname><forenames>Adalberto Mantovani Martiniano</forenames></author><author><keyname>Silveira</keyname><forenames>Marco Antonio</forenames></author></authors><title>Sociotechnical Management Model for Governance of an Ecosystem</title><categories>cs.OH</categories><comments>11 pages, 6 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This is an opinion paper regarding a proposal of a model for a Ecosystemm
Governance. In the globalized world the importance of Information Systems (IS)
and Information Technology (IT) become increasingly relevant regarding the
requirements imposed by competition. Both the knowledge of the business as the
rapid flow of information are fundamental for a enterprise decision making.
Whereas the basic definition of IT = hardware + software, i.e., tools that has
been used to create, store and disseminate data and information in the creation
of knowledge, and IS = IT + People + procedures that collect, process and
disseminate the information to support decision making, coordination, control,
analysis and visualization in the organization [01], it makes implicit the
understanding of IS is essential to create competitive companies, to manage
global corporations and provide customers with products and services of value.
In this work we are correlating IS with the governance of management of an
ecosystem. Yet, as IT is redefining the foundations of business, then the
customer service, operations, strategies of product marketing and its
distribution and even the knowledge management (KM) depends very much, or
sometimes even completely, on the IS. The IT and its costs have become a part
of day-to-day business [02]. In order to meet this complexity of business
needs, today is not possible to disregard the IT and its available resources,
which makes very dificult to draw up IS. Therefore, the perspective view of the
Sociotehcnical Aspects of an IS are directly concerned with governance and the
model proposed regarding an ecosystem. Finally, whereas the summary above, the
main objective of this opinion paper is to propose the guidelines for a
Sociotechnical Management Model of Governance for an Ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2657</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2657</id><created>2012-09-12</created><authors><author><keyname>Rebollo-Neira</keyname><forenames>Laura</forenames></author><author><keyname>Bowley</keyname><forenames>James</forenames></author></authors><title>Sparse Representation of Astronomical Images</title><categories>math-ph cs.CV math.MP</categories><comments>Software to implement the approach is available on
  http://www.nonlinear-approx.info/examples/node1.html</comments><doi>10.1364/JOSAA.30.000758</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation of astronomical images is discussed. It is shown that a
significant gain in sparsity is achieved when particular mixed dictionaries are
used for approximating these types of images with greedy selection strategies.
Experiments are conducted to confirm: i)Effectiveness at producing sparse
representations. ii)Competitiveness, with respect to the time required to
process large images.The latter is a consequence of the suitability of the
proposed dictionaries for approximating images in partitions of small
blocks.This feature makes it possible to apply the effective greedy selection
technique Orthogonal Matching Pursuit, up to some block size. For blocks
exceeding that size a refinement of the original Matching Pursuit approach is
considered. The resulting method is termed Self Projected Matching Pursuit,
because is shown to be effective for implementing, via Matching Pursuit itself,
the optional back-projection intermediate steps in that approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2659</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2659</id><created>2012-09-12</created><authors><author><keyname>D&#xe1;vila-Nicanor</keyname><forenames>Leticia</forenames></author><author><keyname>Mej&#xed;a-Alvarez</keyname><forenames>Pedro</forenames></author></authors><title>Reliability improvement with PSP of Web-based software application</title><categories>cs.SE</categories><comments>17 pages, 7 figures,IEEE Fourth International Conference on Quality
  Software QSIC 04</comments><journal-ref>Computer Science &amp; Engineering: An International Journal
  (CSEIJ),Vol.2, No.4, 2012, 9-25</journal-ref><doi>10.5121/cseij.2012.2402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In diverse industrial and academic environments, the quality of the software
has been evaluated using different analytic studies. The contribution of the
present work is focused on the development of a methodology in order to improve
the evaluation and analysis of the reliability of web-based software
applications. The Personal Software Process (PSP) was introduced in our
methodology for improving the quality of the process and the product. The
Evaluation + Improvement (Ei) process is performed in our methodology to
evaluate and improve the quality of the software system. We tested our
methodology in a web-based software system and used statistical modeling theory
for the analysis and evaluation of the reliability. The behavior of the system
under ideal conditions was evaluated and compared against the operation of the
system executing under real conditions. The results obtained demonstrated the
effectiveness and applicability of our methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2660</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2660</id><created>2012-09-12</created><authors><author><keyname>Gentile</keyname><forenames>Antonio A.</forenames></author></authors><title>Review of strategies for a comprehensive simulation in sputtering
  devices</title><categories>cs.CE physics.plasm-ph</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of sputtering facilities, at the moment, is mainly pursued
through experimental tests, or simply by expertise in the field, and relies
much less on numerical simulation of the process environment. This leads to
great efforts and empirically, roughly optimized solutions: in fact, the
simulation of these devices, at the state of art, is quite good in predicting
the behavior of single steps of the overall deposition process, but it seems
still ahead a full integration among the tools simulating the various phenomena
involved in a sputter. We summarize here the techniques and codes already
available for problems of interest in sputtering facilities, and we try to
outline the possible features of a comprehensive simulation framework. This
framework should be able to integrate the single paradigms, dealing with
aspects going from the plasma environment up to the distribution and properties
of the deposited film, not only on the surface of the substrate, but also on
the walls of the process chamber.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2664</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2664</id><created>2012-09-12</created><authors><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>A Plan For Curating &quot;Obsolete Data or Resources&quot;</title><categories>cs.DL</categories><comments>Position paper for the UNC/NSF Workshop &quot;Curating for Quality:
  Ensuring Data Quality to Enable New Science&quot;, September 10-11, 2012</comments><acm-class>H.3.7</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Our cultural discourse is increasingly carried in the web. With the initial
emergence of the web many years ago, there was a period where conventional
mediums (e.g., music, movies, books, scholarly publications) were primary and
the web was a supplementary channel. This has now changed, where the web is
often the primary channel, and other publishing mechanisms, if present at all,
supplement the web. Unfortunately, the technology for publishing information on
the web always outstrips our technology for preservation. My concern is less
that we will lose data of known importance (e.g., scientific data, census
data), but rather that we will lose data that we do not yet know is important.
In this paper I review some of the issues and, where appropriate, proposed
solutions for increasing the archivability of the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2672</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2672</id><created>2012-09-12</created><authors><author><keyname>Shi</keyname><forenames>Feng</forenames></author><author><keyname>Wu</keyname><forenames>Xuebin</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>New Crosstalk Avoidance Codes Based on a Novel Pattern Classification</title><categories>cs.IT math.IT</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crosstalk delay associated with global on-chip interconnects becomes more
severe in deep submicron technology, and hence can greatly affect the overall
system performance. Based on a delay model proposed by Sotiriadis et al.,
transition patterns over a bus can be classified according to their delays.
Using this classification, crosstalk avoidance codes (CACs) have been proposed
to alleviate the crosstalk delays by restricting the transition patterns on a
bus. In this paper, we first propose a new classification of transition
patterns, and then devise a new family of CACs based on this classification. In
comparison to the previous classification, our classification has more classes
and the delays of its classes do not overlap, both leading to more accurate
control of delays. Our new family of CACs includes some previously proposed
codes as well as new codes with reduced delays and improved throughput. Thus,
this new family of crosstalk avoidance codes provides a wider variety of
tradeoffs between bus delay and efficiency. Finally, since our analytical
approach to the classification and CACs treats the technology-dependent
parameters as variables, our approach can be easily adapted to a wide variety
of technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2673</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2673</id><created>2012-09-12</created><updated>2012-09-24</updated><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Conditional validity of inductive conformal predictors</title><categories>cs.LG</categories><comments>23 pages, 9 figures, 2 tables; to appear in the ACML 2012 Proceedings</comments><report-no>OCMNS05</report-no><msc-class>68T05, 62G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal predictors are set predictors that are automatically valid in the
sense of having coverage probability equal to or exceeding a given confidence
level. Inductive conformal predictors are a computationally efficient version
of conformal predictors satisfying the same property of validity. However,
inductive conformal predictors have been only known to control unconditional
coverage probability. This paper explores various versions of conditional
validity and various ways to achieve them using inductive conformal predictors
and their modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2678</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2678</id><created>2012-09-12</created><updated>2013-02-27</updated><authors><author><keyname>Kehagias</keyname><forenames>Athanasios</forenames></author><author><keyname>Pitsoulis</keyname><forenames>Leonidas</forenames></author></authors><title>Bad Communities with High Modularity</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>Significantly improved version of the paper, with the help of L.
  Pitsoulis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss some problematic aspects of Newman's modularity
function QN. Given a graph G, the modularity of G can be written as QN = Qf
-Q0, where Qf is the intracluster edge fraction of G and Q0 is the expected
intracluster edge fraction of the null model, i.e., a randomly connected graph
with same expected degree distribution as G. It follows that the maximization
of QN must accomodate two factors pulling in opposite directions: Qf favors a
small number of clusters and Q0 favors many balanced (i.e., with approximately
equal degrees) clusters. In certain cases the Q0 term can cause overestimation
of the true cluster number; this is the opposite of the well-known under
estimation effect caused by the &quot;resolution limit&quot; of modularity. We illustrate
the overestimation effect by constructing families of graphs with a &quot;natural&quot;
community structure which, however, does not maximize modularity. In fact, we
prove that we can always find a graph G with a &quot;natural clustering&quot; V of G and
another, balanced clustering U of G such that (i) the pair (G; U) has higher
modularity than (G; V) and (ii) V and U are arbitrarily different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2681</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2681</id><created>2012-09-11</created><authors><author><keyname>Colombo</keyname><forenames>Christian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Francalanza</keyname><forenames>Adrian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Grima</keyname><forenames>Ian</forenames><affiliation>University of Malta</affiliation></author></authors><title>Simplifying Contract-Violating Traces</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FLACOS 2012, arXiv:1209.1699</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 94, 2012, pp. 11-20</journal-ref><doi>10.4204/EPTCS.94.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contract conformance is hard to determine statically, prior to the deployment
of large pieces of software. A scalable alternative is to monitor for contract
violations post-deployment: once a violation is detected, the trace
characterising the offending execution is analysed to pinpoint the source of
the offence. A major drawback with this technique is that, often, contract
violations take time to surface, resulting in long traces that are hard to
analyse. This paper proposes a methodology together with an accompanying tool
for simplifying traces and assisting contract-violation debugging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2684</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2684</id><created>2012-09-12</created><authors><author><keyname>Berlingerio</keyname><forenames>Michele</forenames></author><author><keyname>Koutra</keyname><forenames>Danai</forenames></author><author><keyname>Eliassi-Rad</keyname><forenames>Tina</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>NetSimile: A Scalable Approach to Size-Independent Network Similarity</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of k networks, possibly with different sizes and no overlaps in
nodes or edges, how can we quickly assess similarity between them, without
solving the node-correspondence problem? Analogously, how can we extract a
small number of descriptive, numerical features from each graph that
effectively serve as the graph's &quot;signature&quot;? Having such features will enable
a wealth of graph mining tasks, including clustering, outlier detection,
visualization, etc.
  We propose NetSimile -- a novel, effective, and scalable method for solving
the aforementioned problem. NetSimile has the following desirable properties:
(a) It gives similarity scores that are size-invariant. (b) It is scalable,
being linear on the number of edges for &quot;signature&quot; vector extraction. (c) It
does not need to solve the node-correspondence problem. We present extensive
experiments on numerous synthetic and real graphs from disparate domains, and
show NetSimile's superiority over baseline competitors. We also show how
NetSimile enables several mining tasks such as clustering, visualization,
discontinuity detection, network transfer learning, and re-identification
across networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2688</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2688</id><created>2012-09-12</created><authors><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekr</keyname><forenames>Faramarz</forenames></author></authors><title>Molecular Communication Between Two Populations of Bacteria</title><categories>cs.IT math.IT q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication is an expanding body of research. Recent advances in
biology have encouraged using genetically engineered bacteria as the main
component in the molecular communication. This has stimulated a new line of
research that attempts to study molecular communication among bacteria from an
information-theoretic point of view. Due to high randomness in the individual
behavior of the bacterium, reliable communication between two bacteria is
almost impossible. Therefore, we recently proposed that a population of
bacteria in a cluster is considered as a node capable of molecular transmission
and reception. This proposition enables us to form a reliable node out of many
unreliable bacteria. The bacteria inside a node sense the environment and
respond accordingly. In this paper, we study the communication between two
nodes, one acting as the transmitter and the other as the receiver. We consider
the case in which the information is encoded in the concentration of molecules
by the transmitter. The molecules produced by the bacteria in the transmitter
node propagate in the environment via the diffusion process. Then, their
concentration sensed by the bacteria in the receiver node would decode the
information. The randomness in the communication is caused by both the error in
the molecular production at the transmitter and the reception of molecules at
the receiver. We study the theoretical limits of the information transfer rate
in such a setup versus the number of bacteria per node. Finally, we consider
M-ary modulation schemes and study the achievable rates and their error
probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2693</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2693</id><created>2012-09-12</created><authors><author><keyname>Ortner</keyname><forenames>Ronald</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author><author><keyname>Auer</keyname><forenames>Peter</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Regret Bounds for Restless Markov Bandits</title><categories>cs.LG math.OC stat.ML</categories><comments>In proceedings of The 23rd International Conference on Algorithmic
  Learning Theory (ALT 2012)</comments><journal-ref>Proceedings of ALT, Lyon, France, LNCS 7568, pp.214-228, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the restless Markov bandit problem, in which the state of each
arm evolves according to a Markov process independently of the learner's
actions. We suggest an algorithm that after $T$ steps achieves
$\tilde{O}(\sqrt{T})$ regret with respect to the best policy that knows the
distributions of all arms. No assumptions on the Markov chains are made except
that they are irreducible. In addition, we show that index-based policies are
necessarily suboptimal for the considered problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2696</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2696</id><created>2012-09-12</created><authors><author><keyname>Dundar</keyname><forenames>Aysegul</forenames></author><author><keyname>Jin</keyname><forenames>Jonghoon</forenames></author><author><keyname>Culurciello</keyname><forenames>Eugenio</forenames></author></authors><title>Visual Tracking with Similarity Matching Ratio</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to visual tracking: Similarity Matching
Ratio (SMR). The traditional approach of tracking is minimizing some measures
of the difference between the template and a patch from the frame. This
approach is vulnerable to outliers and drastic appearance changes and an
extensive study is focusing on making the approach more tolerant to them.
However, this often results in longer, corrective algo- rithms which do not
solve the original problem. This paper proposes a novel approach to the
definition of the tracking problems, SMR, which turns the differences into a
probability measure. Only pixel differences below a threshold count towards
deciding the match, the rest are ignored. This approach makes the SMR tracker
robust to outliers and points that dramaticaly change appearance. The SMR
tracker is tested on challenging video sequences and achieved state-of-the-art
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2713</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2713</id><created>2012-09-12</created><updated>2013-06-03</updated><authors><author><keyname>Magnin</keyname><forenames>Lo&#xef;ck</forenames></author><author><keyname>Roland</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>Explicit relation between all lower bound techniques for quantum query
  complexity</title><categories>quant-ph cs.CC</categories><comments>20 pages. v2: typos corrected</comments><journal-ref>Proceedings of the 30th International Symposium on Theoretical
  Aspects of Computer Science (STACS 2013) p. 434--445</journal-ref><doi>10.4230/LIPIcs.STACS.2013.434</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polynomial method and the adversary method are the two main techniques to
prove lower bounds on quantum query complexity, and they have so far been
considered as unrelated approaches. Here, we show an explicit reduction from
the polynomial method to the multiplicative adversary method. The proof goes by
extending the polynomial method from Boolean functions to quantum state
generation problems. In the process, the bound is even strengthened. We then
show that this extended polynomial method is a special case of the
multiplicative adversary method with an adversary matrix that is independent of
the function. This new result therefore provides insight on the reason why in
some cases the adversary method is stronger than the polynomial method. It also
reveals a clear picture of the relation between the different lower bound
techniques, as it implies that all known techniques reduce to the
multiplicative adversary method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2717</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2717</id><created>2012-09-12</created><authors><author><keyname>Ulker</keyname><forenames>Ezgi Deniz</forenames></author><author><keyname>Ulker</keyname><forenames>Sadik</forenames></author></authors><title>Comparison Study for Clonal Selection Algorithm and Genetic Algorithm</title><categories>cs.NE</categories><comments>12 pages, 12 figures, 2 tables</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 4, August 2012 pp 107-118</journal-ref><doi>10.5121/ijcsit.2012.4410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two metaheuristic algorithms namely Artificial Immune Systems (AIS) and
Genetic Algorithms are classified as computational systems inspired by
theoretical immunology and genetics mechanisms. In this work we examine the
comparative performances of two algorithms. A special selection algorithm,
Clonal Selection Algorithm (CLONALG), which is a subset of Artificial Immune
Systems, and Genetic Algorithms are tested with certain benchmark functions. It
is shown that depending on type of a function Clonal Selection Algorithm and
Genetic Algorithm have better performance over each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2724</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2724</id><created>2012-09-12</created><authors><author><keyname>Biernacki</keyname><forenames>Arkadiusz</forenames></author></authors><title>Simulating Performance of a BitTorrent-Based P2P TV System</title><categories>cs.NI</categories><journal-ref>Computer Networks, pp. 448-458, 2011, Springer Berlin Heidelberg</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a prototype of a simulation framework and some
ideas which are to be used to study performance of a P2P TV system in a
controllable and adjustable environment. We created a simplified model
describing live video distribution in a P2P TV system. Using the model we
analyse how some of the system parameters influence its behaviour. We present
the preliminary results obtained at different granularity levels of
measurements, describing the macroscopic system performance as well as the
performance of its individual components.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="35000" completeListSize="102538">1122234|36001</resumptionToken>
</ListRecords>
</OAI-PMH>
